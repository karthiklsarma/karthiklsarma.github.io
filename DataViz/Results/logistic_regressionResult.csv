"V1","V2","V3","V4"
"0.113960576459638","0.0539687072220866","   837","<p>I am creating multiple logistic regression models using lrm from Harrell's Design package in R.  One model I would like to make is the model with no predictors.  For example, I want to predict a constant c such that: </p>

<pre><code>logit(Y) ~ c
</code></pre>

<p>I know I how to compute c (divide the number of ""1""s by the total), what I would like is to use <code>lrm</code> so I can manipulate it as a model in a consistent way with the other models I am making.  Is this possible, and if so how?  </p>

<p>I have tried so far:</p>

<pre><code>library(Design)
data(mtcars)
lrm(am ~ 1, data=mtcars)
</code></pre>

<p>which gives the error:</p>

<pre><code>Error in dimnames(stats) &lt;- list(names(cof), c(""Coef"", ""S.E."", ""Wald Z"",  :
    length of 'dimnames' [1] not equal to array extent
</code></pre>

<p>and I have tried:</p>

<pre><code>lrm(am ~ ., data=mtcars)
</code></pre>

<p>But this uses all the predictors, rather then none of the predictors.</p>
"
"0.1176979772673","0.0557386411433294","  1432","<p>In answering <a href=""http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur"">this</a> question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?</p>
"
"NaN","NaN","  2234","<p>I would like as many algorithms that perform the same task as logistic regression.  That is  algorithms/models that can give a prediction to a binary response (Y) with some explanatory variable (X).</p>

<p>I would be glad if after you name the algorithm, if you would also show how to implement it in R.  Here is a code that can be updated with other models:</p>

<pre><code>set.seed(55)
n &lt;- 100
x &lt;- c(rnorm(n), 1+rnorm(n))
y &lt;- c(rep(0,n), rep(1,n))
r &lt;- glm(y~x, family=binomial)
plot(y~x)
abline(lm(y~x),col='red',lty=2)
xx &lt;- seq(min(x), max(x), length=100)
yy &lt;- predict(r, data.frame(x=xx), type='response')
lines(xx,yy, col='blue', lwd=5, lty=2)
title(main='Logistic regression with the ""glm"" function')
</code></pre>
"
"NaN","NaN","  3531","<p>I would like to perform reversible jump on a network model, but before arriving there, I'm wondering if there are any R packages which support reversible jump for a user specified generalized linear model or spatial-GLM?</p>

<p>Something as simple as an RJMCMC procedure (in R) for the selection of predictors in a logistic regression would be a nice place for me to start?  Does such a function exist?</p>

<p>Through googling, I've only found <a href=""http://cran.r-project.org/web/packages/RJaCGH/index.html"" rel=""nofollow"">RJaCGH</a> which appears to be a bit more complicated (and application specific) than I was hoping for.</p>
"
"0.131590338991954","0.0934765429274634","  3841","<p>I have two years of data which looks basically like this:</p>

<p>Date   <strong><em>_</em>__<em></strong>    Violence Y/N? _</em>  Number of patients</p>

<p>1/1/2008    <strong><em>_</em>___<em></strong>    0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 11</p>

<p>2/1/2008 <strong><em>_</em>__<em>_</em></strong>       0  <strong><em>_</em>__<em>_</em>__<em>_</em>__</strong> 11</p>

<p>3/1/2008 <strong><em>_</em>____</strong><em>1  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>4/1/2008 <strong><em>_</em>____</strong><em>0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>...</p>

<p>31/12/2009_<strong><em>_</em>__</strong>      0_<strong><em>_</em>__<em>_</em>__<em>_</em>__</strong>                 14</p>

<p>i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.</p>

<p>We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individualâ€™s probability of violence goes up when there are more patients on the ward.</p>

<p>I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesnâ€™t get above .1 at any lag, although this is above the â€œsignificantâ€ blue dashed line that R draws for me).</p>

<p>Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).</p>

<p>I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation â€œlme4 is not set up to deal with autoregressive structuresâ€. Even if it were, Iâ€™m not sure I grasp how to write the code anyway.</p>

<p>Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).</p>

<p>So Iâ€™m a bit stuck and lost to be honest. Any help gratefully received!</p>
"
"0.0657951694959769","0.0623176952849756","  4830","<p>Full Disclosure: This is homework. I've included a link to the dataset ( <a href=""http://www.bertelsen.ca/R/logistic-regression.sav"">http://www.bertelsen.ca/R/logistic-regression.sav</a> )</p>

<p>My goal is to maximize the prediction of loan defaulters in this data set.  </p>

<p>Every model that I have come up with so far, predicts >90% of non-defaulters, but &lt;40% of defaulters making the classification efficiency overall ~80%. So, I wonder if there are interaction effects between the variables? Within a logistic regression, other than testing each possible combination is there a way to identify potential interaction effects? Or alternatively a way to boost the efficiency of classification of defaulters. </p>

<p>I'm stuck, any recommendations would be helpful in your choice of words, R-code or SPSS syntax. </p>

<p>My primary variables are outlined in the following histogram and scatterplot (with the exception of the dichotomous variable)</p>

<p>A description of the primary variables: </p>

<pre><code>age: Age in years
employ: Years with current employer
address: Years at current address
income: Household income in thousands
debtinc: Debt to income ratio (x100)
creddebt: Credit card debt in thousands
othdebt: Other debt in thousands
default: Previously defaulted (dichotomous, yes/no, 0/1)
ed: Level of education (No HS, HS, Some College, College, Post-grad)
</code></pre>

<p>Additional variables are just transformations of the above. I also tried converting a few of the continuous variables into categorical variables and implementing them in the model, no luck there. </p>

<p>If you'd like to pop it into R, quickly, here it is: </p>

<pre><code>## R Code
df &lt;- read.spss(file=""http://www.bertelsen.ca/R/logistic-regression.sav"", use.value.labels=T, to.data.frame=T)
</code></pre>

<p><img src=""http://i.stack.imgur.com/aVqtZ.jpg"" alt=""alt text"">
<img src=""http://i.stack.imgur.com/VQJDg.jpg"" alt=""alt text""></p>
"
"NaN","NaN","  4832","<p>In SPSS output there is a pretty little classification table available when you perform a logistic regression, is the same possible with R? If so, how?</p>

<p><img src=""http://i.stack.imgur.com/yFS6P.jpg"" alt=""alt text""></p>
"
"0.0930484210398471","0.0440652649239232","  5293","<p>My name is Tuhin.
I came up with a couple of questions when I was doing an
analysis in R.</p>

<p>I did a logistic regression analysis in R and tried to check
how good the model fits the data.</p>

<p>But, I got stuck as I could not get the pseudo R square value
for the model which could give me some idea about the variation
explained by the model.</p>

<p>Could you please guide me on how to achieve this value (pseudo
R square for Logistic regression analysis).
It would also be helpful if you could show me a way to get the
Hosmer Lemeshow statistic for the model as well. I found out a
user defined function to do it, but if there is a quicker way
possible, it would be really helpful.</p>

<p>I would be very grateful if you can provide me the answers to
my queries.</p>

<p>Eagerly waiting for your response.</p>

<p>Regards</p>
"
"0.149209419390598","0.0824385620013739","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.107443061870051","0.0636027314143481","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"NaN","NaN","  5647","<p>I am fitting a conditional logistic regression model with 1:4 controls using <code>R</code>. I wish to obtain <code>AIC</code> from the model. How can I extract the appropriate parameters based on the object <code>m</code>?</p>

<pre><code>library(survival)
m&lt;-clogit(cc~exp+ factor1+ factor2 + strata(stratum),data=data1)
</code></pre>
"
"0.0465242105199235","0.0220326324619616","  5912","<p>A recent question about <a href=""http://stats.stackexchange.com/questions/2234/alternatives-to-logistic-regression-in-r"">alternatives to logistic regression in R</a> yielded a variety of answers including randomForest, gbm, rpart, bayesglm, and generalized additive models.  What are the practical and interpretation differences between these methods and logistic regression?  What assumptions do they make (or not make) relative to logistic regression?  Are the suitable for hypothesis testing?  Etc.</p>
"
"NaN","NaN","  6122","<p>I have almost two questions. I need a single covariate logistic regression (LR) for each of my variables. Should I do it manually in SPSS, selecting each variable and do logistic regression? Is there a ""for each"" cycle to do it? I should switch to R language to have what I want. </p>

<p>In the multivariables (multi covariates) LR, could I have missing values?</p>

<p>Thanks!!</p>
"
"0.0657951694959769","0.0311588476424878","  6206","<p>I made a logistic regression model using glm in R.  I have two independent variables.  How can I plot the decision boundary of my model in the scatter plot of the two variables.  For example, how can I plot a figure like:
<a href=""http://onlinecourses.science.psu.edu/stat557/node/55"">http://onlinecourses.science.psu.edu/stat557/node/55</a></p>

<p>Thanks.</p>
"
"NaN","NaN","  6412","<p>What is your favorite free tool on Linux for multivariate logistic regression?</p>

<p>Possibilities I've seen:</p>

<ul>
<li><a href=""http://www.r-project.org"" rel=""nofollow"">R</a> (see <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">paper</a>).  <a href=""http://stackoverflow.com/questions/3439248/logistic-regression-in-r-sas-like-output"">This question</a> says use <a href=""http://cran.r-project.org/package=Design"" rel=""nofollow"">design</a>.</li>
<li>Can you use <a href=""http://docs.scipy.org/doc/scipy/reference/stats.html#statistical-functions"" rel=""nofollow"">SciPy</a>?</li>
</ul>

<p>Other choices?</p>

<p>Do people have experience with large data?</p>
"
"0.161164592805076","0.0763232776972177","  6505","<p>Suppose I am going to do a univariate logistic regression on several independent variables, like this:</p>

<pre><code>mod.a &lt;- glm(x ~ a, data=z, family=binominal(""logistic""))
mod.b &lt;- glm(x ~ b, data=z, family=binominal(""logistic""))
</code></pre>

<p>I did a model comparison (likelihood ratio test) to see if the model is better than the null model by this command</p>

<pre><code>1-pchisq(mod.a$null.deviance-mod.a$deviance, mod.a$df.null-mod.a$df.residual)
</code></pre>

<p>Then I built another model with all variables in it</p>

<pre><code>mod.c &lt;- glm(x ~ a+b, data=z, family=binomial(""logistic""))
</code></pre>

<p>In order to see if the variable is statistically significant in the multivariate model, I used the <code>lrtest</code> command from <code>epicalc</code></p>

<pre><code>lrtest(mod.c,mod.a) ### see if variable b is statistically significant after adjustment of a
lrtest(mod.c,mod.b) ### see if variable a is statistically significant after adjustment of b
</code></pre>

<p>I wonder if the <code>pchisq</code> method and the <code>lrtest</code> method are equivalent for doing loglikelihood test? As I dunno how to use <code>lrtest</code> for univate logistic model.</p>
"
"0.0986927542439653","0.0623176952849756","  6562","<p>I have read an <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">article</a> from Christopher Manning, and saw an interesting code for collapsing categorical variable in an logistic regression model:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class == 1), family=binomial(""logit""))
</code></pre>

<p>Does the <code>I(class == 1)</code> means that the <code>class</code> variable has been recoded into either it is <code>1</code> or it is not <code>1</code>?</p>

<p>After that, I am thinking of modifying it a bit:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class %in% C(1,2)), family=binomial(""logit""))
</code></pre>

<p>I am planning to merge the variable <code>class</code> from <code>c(1,2,3,4)</code> into two groups, one group contains <code>c(1,2)</code>, another group contains <code>c(3,4)</code>, can the code above give me the result I want?</p>

<p>Thanks.</p>
"
"0.169882397145875","0.0965421584050956","  7551","<p>I am running an ordinal logistic regression in R and running into trouble when I include dummy variables.  My model works great with my first set of predictors.  Next I want to add dummy variables for each of the years represented in my dataset.  </p>

<p>I created the dummy variables with <code>car:recode</code> in this manner (one statement like this for each of the 11 years)</p>

<pre><code>fsd$admityear2000 &lt;- recode(fsd$ApplicationYear ,""2000=1;else=0"")
</code></pre>

<p>The lrm model is specified as follows</p>

<pre><code>library(Design)
ddist&lt;- datadist(fsd)
options(datadist='ddist')
m4 &lt;- lrm(Outcome ~ relGPA + mcAvgGPA + Interview_Z + WorkHistory_years + GMAT + UGI_Gourman + admityear1999 + admityear2000 + admityear2001 + admityear2002 + admityear2003 + admityear2004 + admityear2005 + admityear2006 + admityear2007 + admityear2008 + admityear2009, data=fsd)
</code></pre>

<p>(sorry for all of the other random variables, but I don't want to introduce confusion by changing my code)</p>

<p>I get the error</p>

<pre><code>singular information matrix in lrm.fit (rank= 22 ).  Offending variable(s):
admityear2009 admityear2000 admityear1999 
Error in lrm(Outcome ~ relGPA + mcAvgGPA + Interview_Z + WorkHistory_years +  : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>I understand that including all options of a dummy variable over-defines the model, but I get the error whether I include all 11 years or just 10.</p>

<p>I found a suggestion <a href=""http://r.789695.n4.nabble.com/Singular-information-matrix-in-lrm-fit-td869221.html"" rel=""nofollow"">here</a> to set the penalty parameter of <code>lrm</code> to a <em>small positive value</em>.  Setting it to 1 or 5 changes the error such that it only names one of the variables as offending.  The error doesn't go away even with <code>penalty=100</code>.</p>

<p>I'm pretty new to R, but loving the freedom so far.  Thanks for any help!</p>

<p><strong>Responses and Lessons</strong></p>

<ul>
<li>Factors are awesome and I can't believe I didn't notice them earlier. Man that cleans up my code a lot.  Thanks!</li>
<li>My DV, 'Outcome' is indeed ordinal and after making it a factor(), I also made it ordered().</li>
<li>The str() command is also awesome and this is what my data now looks like (with some of the non-relevant variables omitted)</li>
</ul>

<p>output:</p>

<pre><code>str(fsd)     
Outcome      : Ord.factor w/ 3 levels ""0""&lt;""1""&lt;""2""
relGPA       : num  
mcAvgGPA     : num 
admitschool  : Factor w/ 4 levels ""1"",""2"",""3"",""4""
appyear      : Factor w/ 11 levels ""1999"",""2000"",..
</code></pre>

<ul>
<li>both lrm() and polr() now run successfully, and they both deal with appyear by dropping some values of the factor.  lrm() drops 1999, 2000, and 2001 while polr() just drops 1999 and 2000.  lrm() gives no warnings while polr() says ""design appears to be rank-deficient, so dropping some coefs.""  This is an improvement, but I still don't understand why more than one value needs to be dropped. xtabs shows that there isn't full seperation right?</li>
</ul>

<p>output:</p>

<pre><code>xtabs(~fsd$appyear + fsd$Outcome)
           fsd$Outcome
fsd$appyear    0    1    2
       1999 1207  123  418
       2000 1833  246  510
       2001 1805  294  553
       2002 1167  177  598
       2003 4070  158 1076
       2004 2803  106 1138
       2005 3749  513 2141
       2006 4429  519 2028
       2007 6134  670 1947
       2008 7446  662 1994
       2009 4411   86 1118
</code></pre>
"
"0.153522062157279","0.0934765429274634","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"NaN","NaN","  8303","<p>I am fitting a binomial family glm in R, and I have a whole troupe of explanatory variables, and I need to find the best (R-squared as a measure is fine). Short of writing a script to loop through random different combinations of the explanatory variables and then recording which performs the best, I really dont know what to do. And the <code>leaps</code> function from package <strong><a href=""http://cran.r-project.org/web/packages/leaps/index.html"">leaps</a></strong> does not seem to do logistic regression.</p>

<p>Any help or suggestions would be greatly appreciated
Leendert   </p>
"
"0.162834736819732","0.0771142136168656","  8511","<p>Christopher Manning's <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">writeup on logistic regression in R</a> shows a logistic regression in R as follows:</p>

<pre><code>ced.logr &lt;- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)
</code></pre>

<p>Some output:</p>

<pre><code>&gt; summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -1.31827    0.12221 -10.787 &lt; 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 &lt; 2e-16
followsV       0.53408    0.05660   9.436 &lt; 2e-16
factor(class)2 1.27045    0.10320  12.310 &lt; 2e-16
factor(class)3 1.04805    0.10355  10.122 &lt; 2e-16
factor(class)4 1.37425    0.10155  13.532 &lt; 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4
</code></pre>

<p>He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.</p>

<p>However, how much variance does the model account for?  A <a href=""http://www.ats.ucla.edu/stat/stata/output/old/lognoframe.htm"" rel=""nofollow"">Stata page on logistic regression</a> says:</p>

<blockquote>
  <p>Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the ""constant-only"" model and L1 is the log likelihood for the full model with constant and predictors. </p>
</blockquote>

<p>I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.</p>

<p>Can someone verify how one actually computes the pseudo-R^2 in R using this example?</p>
"
"0.0379868588198793","0.0359791381480577","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.0657951694959769","0.0311588476424878","  9027","<p>I have two logistic regression models in R made with <code>glm()</code>.  They both use the same variables, but were made using different subsets of a matrix.  Is there an easy way to get an average model which gives the means of the coefficients and then use this with the predict() function?</p>

<p>[ sorry if this type of question should be posted on a programming site let me know and I'll post it there ]</p>

<p>Thanks</p>
"
"0.147122471584125","0.0696733014291618"," 10316","<p>I'm working on a multiple logistic regression in R using <code>glm</code>. The predictor variables are continuous and categorical. An extract of the summary of the model shows the following:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   2.451e+00  2.439e+00   1.005   0.3150
Age           5.747e-02  3.466e-02   1.658   0.0973 .
BMI          -7.750e-02  7.090e-02  -1.093   0.2743
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Confidence intervals:</p>

<pre><code>                  2.5 %       97.5 %
(Intercept)  0.10969506 1.863217e+03
Age          0.99565783 1.142627e+00
BMI          0.80089276 1.064256e+00
...
</code></pre>

<p>Odd ratios:</p>

<pre><code>                 Estimate Std. Error   z value Pr(&gt;|z|)
(Intercept)  1.159642e+01  11.464683 2.7310435 1.370327
Age          1.059155e+00   1.035269 5.2491658 1.102195
B            9.254228e-01   1.073477 0.3351730 1.315670
...
</code></pre>

<p>The first output shows that $Age$ is significant. However, the confidence interval for $Age$ includes the value 1 and the odds ratio for $Age$ is very close to 1. What does the significant p-value from the first output mean? Is $Age$ a predictor of the outcome or not?</p>
"
"0.139572631559771","0.0881305298478463"," 10986","<p>I am trying to find some help with something that is called an ""Adjusted Analysis"" (or also Covariate Adjusted Logistic Regression);  a typical response has been that I might just want multivariable logistic regression, but this is not quite what I am looking for. The trouble I have is with what exactly an ""adjusted"" analysis is.</p>

<p>As an example, I have at my disposal a software suite that performs this type of adjusted analysis. We have some genes and various clinical variables from patients; what the method seems to do is adjust the p-values of the genes. But I can't figure out why, or how.  So I am trying to move outside of this software suite to truly understand what the underlying mathematics of this statistical technique is. </p>

<p>When I've posted this question in <a href=""http://stackoverflow.com/questions/6061305/using-r-for-covariate-adjusted-logistic-regression"">other places</a> the response has been that I should just take more courses in statistics. So while acknowledging my short comings, I would like to please ask if anyone can point me in a somewhat correct direction. I have been trying to find resources to help however I think I am not posing my question correctly enough.  As an aside I have a background in computer science and  more recently I am branching into biostatistics and I don't like using black box software so I would eventually like to re-implement this technique in R.</p>

<p>Thank you for any help that can be offered. Please let me know if there is a way I can pose my question clearer.</p>
"
"0.1359059177167","0.0884969785380043"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.05884898863365","0.0418039808574971"," 11178","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio"">Logistic Regression in R (Odds Ratio)</a>  </p>
</blockquote>



<p>I need to do a logistic regression in R. My response variable is <code>surv=0</code>; <code>surv=1</code> and I have about 18 predictor variables.</p>

<p>After reading my model, I got the table of Coefficients below and I need to go through some steps, which I am not familiar with, until I get to the odds ratios.</p>

<p>This is my first time to do a logistic regression in R and your help would be appreciated.</p>

<pre><code>Call:
glm(formula = surv ~ as.factor(tdate) + as.factor(line) + as.factor(wt) + 
    as.factor(crump) + as.factor(pind) + as.factor(pcscore) + 
    as.factor(ptem) + as.factor(pshiv) + as.factor(pincis) + 
    as.factor(presp) + as.factor(pmtone) + as.factor(pscolor) + 
    as.factor(ppscore) + as.factor(pmstain) + as.factor(pbse) + 
    as.factor(psex) + as.factor(pgf), family = binomial(link = ""logit""), 
    data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9772  -0.5896  -0.4419  -0.3154   2.8264  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.59796    0.27024  -2.213 0.026918 *  
as.factor(tdate)2009-09-08  0.43918    0.19876   2.210 0.027130 *  
as.factor(tdate)2009-09-11  0.27613    0.20289   1.361 0.173514    
as.factor(tdate)2009-09-15  0.58733    0.19232   3.054 0.002259 ** 
as.factor(tdate)2009-09-18  0.52823    0.20605   2.564 0.010360 *  
as.factor(tdate)2009-09-22  0.45661    0.19929   2.291 0.021954 *  
as.factor(tdate)2009-09-25 -0.09189    0.21740  -0.423 0.672526    
as.factor(tdate)2009-09-29 -0.15696    0.28369  -0.553 0.580076    
as.factor(tdate)2010-01-26  1.39260    0.21049   6.616 3.69e-11 ***
as.factor(tdate)2010-01-29  1.67827    0.21099   7.954 1.80e-15 ***
as.factor(tdate)2010-02-02  1.35442    0.21292   6.361 2.00e-10 ***
as.factor(tdate)2010-02-05  1.36856    0.21439   6.383 1.73e-10 ***
as.factor(tdate)2010-02-09  1.18159    0.21951   5.383 7.33e-08 ***
as.factor(tdate)2010-02-12  1.40457    0.22001   6.384 1.73e-10 ***
as.factor(tdate)2010-02-16  1.01063    0.21783   4.639 3.49e-06 ***
as.factor(tdate)2010-02-19  1.54992    0.21535   7.197 6.14e-13 ***
as.factor(tdate)2010-02-23  0.85695    0.33968   2.523 0.011641 *  
as.factor(line)2           -0.26311    0.07257  -3.625 0.000288 ***
as.factor(line)5            0.06766    0.11162   0.606 0.544387    
as.factor(line)6           -0.30409    0.12130  -2.507 0.012176 *  
as.factor(wt)2             -0.33904    0.10708  -3.166 0.001544 ** 
as.factor(wt)3             -0.28976    0.13217  -2.192 0.028359 *  
as.factor(wt)4             -0.50470    0.16264  -3.103 0.001915 ** 
as.factor(wt)5             -0.74870    0.20067  -3.731 0.000191 ***
as.factor(crump)2           0.07537    0.10751   0.701 0.483280    
as.factor(crump)3          -0.14050    0.13217  -1.063 0.287768    
as.factor(crump)4          -0.20131    0.16689  -1.206 0.227724    
as.factor(crump)5          -0.23963    0.20778  -1.153 0.248803    
as.factor(pind)2           -0.29893    0.10752  -2.780 0.005434 ** 
as.factor(pind)3           -0.40828    0.12436  -3.283 0.001027 ** 
as.factor(pind)4           -0.73021    0.14947  -4.885 1.03e-06 ***
as.factor(pind)5           -0.68878    0.17650  -3.902 9.52e-05 ***
as.factor(pcscore)2        -0.52667    0.13606  -3.871 0.000108 ***
as.factor(ptem)2           -0.72600    0.08964  -8.099 5.52e-16 ***
as.factor(ptem)3           -0.79145    0.10503  -7.536 4.86e-14 ***
as.factor(ptem)4           -0.89956    0.10331  -8.707  &lt; 2e-16 ***
as.factor(ptem)5           -0.90181    0.10721  -8.412  &lt; 2e-16 ***
as.factor(pshiv)2           0.25236    0.07713   3.272 0.001068 ** 
as.factor(pincis)2          0.02327    0.07216   0.323 0.747041    
as.factor(presp)2           0.43746    0.11598   3.772 0.000162 ***
as.factor(pmtone)2          0.34515    0.11178   3.088 0.002016 ** 
as.factor(pscolor)2         0.53469    0.26851   1.991 0.046443 *  
as.factor(ppscore)2         0.25664    0.08751   2.933 0.003361 ** 
as.factor(pmstain)2        -0.48619    0.84408  -0.576 0.564611    
as.factor(pbse)2           -0.28248    0.07335  -3.851 0.000117 ***
as.factor(psex)2           -0.18240    0.06385  -2.857 0.004280 ** 
as.factor(pgf)12            0.10329    0.14314   0.722 0.470554    
as.factor(pgf)21           -0.06481    0.10772  -0.602 0.547388    
as.factor(pgf)22            0.39584    0.12740   3.107 0.001890 ** 
as.factor(pgf)31            0.18820    0.10082   1.867 0.061936 .  
as.factor(pgf)32            0.39662    0.13963   2.841 0.004504 ** 
as.factor(pgf)41            0.09178    0.10413   0.881 0.378106    
as.factor(pgf)42            0.21056    0.14906   1.413 0.157787    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7812.9  on 8714  degrees of freedom
Residual deviance: 6797.4  on 8662  degrees of freedom
  (418 observations deleted due to missingness)
AIC: 6903.4

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.107443061870051","0.0636027314143481"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.0930484210398471","0.0881305298478463"," 11679","<p>I have a nested-case control study that I have been using for analysis. At the end of my work I have deduced a set of variables that I use later to to classify new cases. One example of a simple classifier I am using is a naive Bayes, which will output simply a probability. </p>

<p>So here is my question:</p>

<p>Could I make my probabilities reflect the real world? In my specific example, the condition that I am testing for has a prevalence of 33% in my study, but a it has a population prevalence of only 10%.  Bayes factors have been suggested to me as a way to achieve this, however I am little unsure how to set up the problem. </p>

<p>As an example I have seen a Bayes factor as a logit between the true vs. study prevalence of the outcome. The classifier however was a logistic regression, and in that case the Bayes factor was just added to the linear predictors. I think the example there was very specific, and perhaps an inappropriate method for probabilities of a naive Bayes. Instead what I did was add the logit Bayes factor to the logged probabilities, but I am also not convinced this is right either. I also think a simpler solution would be to use Bayes theorem directly, but there I am not sure how to represented my study vs.population prevalences. The method below isn't quite right, but gets at what I want:</p>

<pre><code>        p_final = classier_posterior*(population_prev)/(study_prev)
</code></pre>

<p>I should contextualize that I use the probabilities to establish a threshold for classification down stream.</p>
"
"0","0.0311588476424878"," 12219","<p>My logistic regression model seems to identify successes very well (about 85% - 94%), but fails to identify the failures (only identifying 18% - 32% correctly). </p>

<p>I have thought of weighting the success and failures differently, but I am not sure of how to do this R. I have tried:</p>

<pre><code>Fr &lt;- ifelse (y==0, 2, 1)
model &lt;- glm(y~factor(x1) + factor(x2), weights=Fr, 
                                        family = ""binomial"", 
                                        data = data)
</code></pre>

<p>but did not get any improvement this way.</p>
"
"NaN","NaN"," 12554","<p>What parameterization to <code>glmnet</code> will give the same results as <code>glm</code>?  (I'm mainly interested in logistic and linear regressions, if that matters.)</p>
"
"NaN","NaN"," 12743","<p>I would like to know how to estimate a population average model of a hierarchical logistic regression using <code>R</code> package <code>geepack</code>.</p>

<p>The <code>Stata</code> code is: </p>

<pre><code>xtlogit dep ind1 ind2 ind3, i(ind4) pa
</code></pre>

<p>I would like to reproduce this in <code>R</code> using <code>geepack</code> or any other method.</p>
"
"0.189934294099397","0.107937414444173"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.0465242105199235","0.0440652649239232"," 13469","<p>Tools such as random forests or adaboost are powerful at solving cross-sectional binary logistic problems or prediction problems where there are many weak learners. But can these tools be adapted to solve panel regression problems? </p>

<p>One could naively introduce a time index as an independent variable but all this does is to provide an additional degree of freedom to the fitting algorithm. What we would like is a solution that allows information from period T-1 to have bearing on period T. </p>

<p>If there is not a straightforward way to do this using these algorithms, is there an alternative algorithm that can perform a panel regression making use of the information in both the cross-section and time-series?</p>
"
"0.138865930150177","0.0939474604818017"," 14206","<p>I am using SVM to predict diabetes. I am using the <a href=""http://www.cdc.gov/BRFSS/"">BRFSS</a> data set for this purpose. The data set has the dimensions of $432607 \times 136$ and is skewed. The percentage of <code>Y</code>s in the target variable is $11\%$ while the <code>N</code>s constitute the remaining $89\%$.</p>

<p>I am using only <code>15</code> out of <code>136</code> independent variables from the data set. One of the reasons for reducing the data set was to have more training samples when rows containing <code>NA</code>s are omitted.</p>

<p>These <code>15</code> variables were selected after running statistical methods such as random trees, logistic regression and finding out which variables are significant from the resulting models. For example, after running logistic regression we used <code>p-value</code> to order the most significant variables.</p>

<p>Is my method of doing variable selection correct? Any suggestions to is greatly welcome. </p>

<p>The following is my <code>R</code> implementation. </p>

<pre><code>library(e1071) # Support Vector Machines

#--------------------------------------------------------------------
# read brfss file (huge 135 MB file)
#--------------------------------------------------------------------
y &lt;- read.csv(""http://www.hofroe.net/stat579/brfss%2009/brfss-2009-clean.csv"")
indicator &lt;- c(""DIABETE2"", ""GENHLTH"", ""PERSDOC2"", ""SEX"", ""FLUSHOT3"", ""PNEUVAC3"", 
    ""X_RFHYPE5"", ""X_RFCHOL"", ""RACE2"", ""X_SMOKER3"", ""X_AGE_G"", ""X_BMI4CAT"", 
    ""X_INCOMG"", ""X_RFDRHV3"", ""X_RFDRHV3"", ""X_STATE"");
target &lt;- ""DIABETE2"";
diabetes &lt;- y[, indicator];

#--------------------------------------------------------------------
# recode DIABETE2
#--------------------------------------------------------------------
x &lt;- diabetes$DIABETE2;
x[x &gt; 1]  &lt;- 'N';
x[x != 'N']  &lt;- 'Y';
diabetes$DIABETE2 &lt;- x; 
rm(x);

#--------------------------------------------------------------------
# remove NA
#--------------------------------------------------------------------
x &lt;- na.omit(diabetes);
diabetes &lt;- x;
rm(x);

#--------------------------------------------------------------------
# reproducible research 
#--------------------------------------------------------------------
set.seed(1612);
nsamples &lt;- 1000; 
sample.diabetes &lt;- diabetes[sample(nrow(diabetes), nsamples), ]; 

#--------------------------------------------------------------------
# split the dataset into training and test
#--------------------------------------------------------------------
ratio &lt;- 0.7;
train.samples &lt;- ratio*nsamples;
train.rows &lt;- c(sample(nrow(sample.diabetes), trunc(train.samples)));

train.set  &lt;- sample.diabetes[train.rows, ];
test.set   &lt;- sample.diabetes[-train.rows, ];

train.result &lt;- train.set[ , which(names(train.set) == target)];
test.result  &lt;- test.set[ , which(names(test.set) == target)];

#--------------------------------------------------------------------
# SVM 
#--------------------------------------------------------------------
formula &lt;- as.formula(factor(DIABETE2) ~ . );
svm.tune &lt;- tune.svm(formula, data = train.set, 
    gamma = 10^(-3:0), cost = 10^(-1:1));
svm.model &lt;- svm(formula, data = train.set, 
    kernel = ""linear"", 
    gamma = svm.tune$best.parameters$gamma, 
    cost  = svm.tune$best.parameters$cost);

#--------------------------------------------------------------------
# Confusion matrix
#--------------------------------------------------------------------
train.pred &lt;- predict(svm.model, train.set);
test.pred  &lt;- predict(svm.model, test.set);
svm.table &lt;- table(pred = test.pred, true = test.result);
print(svm.table);
</code></pre>

<p>I ran with $1000$ (training = $700$ and test = $300$) samples since it is faster in my laptop. The confusion matrix for the test data ($300$ samples)  I get is quite bad.</p>

<pre><code>    true
pred   N   Y
   N 262  38
   Y   0   0
</code></pre>

<p>I need to improve my prediction for the <code>Y</code> class. In fact, I need to be as accurate as possible with <code>Y</code> even if I perform poorly with <code>N</code>. Any suggestions to improve the accuracy of classification would be greatly appreciated.</p>
"
"0.0930484210398471","0.0440652649239232"," 14546","<p>I have run the following logistic regression:</p>

<pre><code>glm(formula = DecisionasReceiver ~ L1 + L2 + L3, 
  family = binomial(""logit""), data = lue)
</code></pre>

<p>where L1 L2 and L3 code for differences in condition of no.GREEN.
L1: 1,-1,0,0 : is there a difference in DecisionasReceiver as no.GREEN changes from 1 to 2?</p>

<p>L2: 0,1,-1,0: is there a difference in DecisionasReceiver as no.GREEN changes from 2 to 3?</p>

<p>L2: 0,0,1,-1: is there a difference in DecisionasReceiver as no.GREEN changes from 3 to 4?</p>

<p>And I'm running this regression both for the cases where MessageReceived is BLUE and for MessageReceived RED. </p>

<p>I have the following output:</p>

<pre><code>   Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9535     0.3659  -2.606  0.00916 ** 
L1            2.2753     0.5406   4.209 2.56e-05 ***
L2            3.1234     0.7318   4.268 1.97e-05 ***
L3            1.9369     0.8134   2.381  0.01726 *  
</code></pre>

<p>Looking at my graph it seems strange that the coefficients are positive and that the intercept is -0.953. How exactly should I interpret these results in the light of the graph? </p>

<p><img src=""http://dl.dropbox.com/u/22681355/graph.png"" alt=""""></p>
"
"0.0930484210398471","0.0440652649239232"," 15307","<p>I am curious about the consequences of changing the order of the explanatory variables in a binary logistic regression. In a recent series of logistic regressions I ran in SPSS, I found that changing the order of the explanatory variables ($z, y, x$ instead of $x, y, z$) resulted in different coefficient values and significance levels. Investigating further, I got the same results in <code>R</code> using the same orders. Clearly, shifting the predictors around matters in terms of the resultsâ€”my question is how? (And yes, I checked to make sure I wasn't entering the variables stepwise.)</p>
"
"0.138865930150177","0.103342206529982"," 15577","<p>I'm trying to run a zero-inflated regression for a continuous response variable in R. I'm aware of a gamlss implementation, but I'd really like to try out this algorithm by Dale McLerran that is conceptually a bit more straightforward. Unfortunately, the code is in SAS and I'm not sure how to re-write it for something like nlme. </p>

<p>The code is as follows:</p>

<pre><code>proc nlmixed data=mydata;
  parms b0_f=0 b1_f=0 
        b0_h=0 b1_h=0 
        log_theta=0;


  eta_f = b0_f + b1_f*x1 ;
  p_yEQ0 = 1 / (1 + exp(-eta_f));


  eta_h = b0_h + b1_h*x1;
  mu    = exp(eta_h);
  theta = exp(log_theta);
  r = mu/theta;


  if y=0 then
     ll = log(p_yEQ0);
  else
     ll = log(1 - p_yEQ0)
          - lgamma(theta) + (theta-1)*log(y) - theta*log(r) - y/r;


  model y ~ general(ll);
  predict (1 - p_yEQ0)*mu out=expect_zig;
  predict r out=shape;
  estimate ""scale"" theta;
run;
</code></pre>

<p>From: <a href=""http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779"" rel=""nofollow"">http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779</a></p>

<p><strong>ADD:</strong></p>

<p>Note: There are no mixed effects present here - only fixed.</p>

<p>The advantage to this fitting is that (even though the coefficients are the same as if you separately fit a logistic regression to P(y=0) and a gamma error regression with log link to E(y | y>0)) you can estimate the combined function E(y) which includes the zeroes. One can predict this value in SAS (with a CI) using the line <code>predict (1 - p_yEQ0)*mu</code> .</p>

<p>Further, one is able to write custom contrast statements to test the significance of predictor variables on E(y). For example, here is another version of the SAS code I have used:</p>

<pre><code>proc nlmixed data=TestZIG;
      parms b0_f=0 b1_f=0 b2_f=0 b3_f=0
            b0_h=0 b1_h=0 b2_h=0 b3_h=0
            log_theta=0;


        if gifts = 1 then x1=1; else x1 =0;
        if gifts = 2 then x2=1; else x2 =0;
        if gifts = 3 then x3=1; else x3 =0;


      eta_f = b0_f + b1_f*x1 + b2_f*x2 + b3_f*x3;
      p_yEQ0 = 1 / (1 + exp(-eta_f));

      eta_h = b0_h + b1_h*x1 + b2_h*x2 + b3_h*x3;
      mu    = exp(eta_h);
      theta = exp(log_theta);
      r = mu/theta;

      if amount=0 then
         ll = log(p_yEQ0);
      else
         ll = log(1 - p_yEQ0)
              - lgamma(theta) + (theta-1)*log(amount) -                      theta*log(r) - amount/r;

      model amount ~ general(ll);
      predict (1 - p_yEQ0)*mu out=expect_zig;
      estimate ""scale"" theta;
    run; 
</code></pre>

<p>Then to estimate ""gift1"" versus ""gift2"" (b1 versus b2) we can write this estimate statement:</p>

<pre><code>estimate ""gift1 versus gift 2"" 
 (1-(1 / (1 + exp(-b0_f -b1_f))))*(exp(b0_h + b1_h)) - (1-(1 / (1 + exp(-b0_f -b2_f))))*(exp(b0_h + b2_h)) ; 
</code></pre>

<p>Can R do this?</p>
"
"0.131590338991954","0.0934765429274634"," 15623","<p>I am completely new to R, just downloaded and installed it today. I am familiar with SAS and Stata; I am using R because I have found out that in survey regression analysis, R is capable of using data that have stratum with one PSU. However, I cannot figure out how to write the code at all.</p>

<p>Here is what I have done so far: read a Stata dataset and save the .RData file. I have also put in the MASS, pscl, and survey (for svyglm) packages.</p>

<p>Here's what I need to do:
1) I am using survey data, so I have a ""weight"" variable, a ""strata"" variable, and a ""PSU"" variable. I need to incorporate those; I know how to use svyset in Stata, but no idea in R.
2) I have stratum with singleton PSUs. I need to use an option called survey.lonely.psu I believe, and I have no idea where to even begin with that. This is the reason why I am using R instead of Stata as I do not want to collapse stratum or delete observations.
3) The types of regression models I have to run: survey negative binomial, survey zero-inflated negative binomial (need to also determine the predictors of zeros), survey logistic, and survey OLS regression.
4)I also really can't make much sense in R of how to write the model in R code. In Stata, I can simply write the model as:</p>

<p>svy: nbreg dependent_var independent_var1 independent_var2 independent_var3</p>

<p>I can't figure out how to do that at all in R.</p>

<p>Any and all help will be greatly appreciated.</p>
"
"0.113960576459638","0.0359791381480577"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.0657951694959769","0.0311588476424878"," 17052","<p>I have the summary of a logistic regression output in R.  I used training data to make the model. </p>

<ul>
<li>How do I test the logistic regression model developed on the training data on the data left out?</li>
</ul>

<p>My naive guess is to create a function then run each test same through that (not even sure how do pull that) but I have to imagine there's a better way.  </p>
"
"0.0465242105199235","0.0440652649239232"," 17461","<p>I'm studying a data set in R using both regression trees (tree and rpart functions) and logistic regression. I'm finding explanatory variables in the regression which are significant, but when I fit a tree those variables are not used as splits. What does this imply about the predictive ability of the results?</p>
"
"0.119027940128723","0.0939474604818017"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"NaN","NaN"," 19082","<p>I have a logistic regression model with several categorical explanatory variables and one interaction term (between two binary variables, named A and B). I know how to calculate the odds ratios for the different levels of A and B (for A=1, e.g., I need to add the coeff for A and coeff for A*B, then exponentiate), but how do I get a confidence interval for this OR? I need to do this in R please, this is the only statistical package I have access to.</p>
"
"0.161164592805076","0.0763232776972177"," 19469","<p>Here is an example: I have a set of observations of different individuals from lots of different families of grasses:</p>

<pre><code>individual#, Fam, Genus, Factor1(3 levels), Factor2(7 levels), Factor3(5 levels), Response1(3 levels), Response2(3 levels)
</code></pre>

<p>What I am hoping to discover is whether the frequency of occurrences of Response1 and 2 are linked to family groups, and whether Factors 1 - 3 (things like soil type, sun exposure etc) have an impact. </p>

<p>Example: </p>

<pre><code>family,  resp1a,  resp1b,   resp1c 
1,       14%(20), 16%(24),  67%(98),  Total N = 147  
2,       38%(98), 86%(220), 48%(123), Total N = 256
...
</code></pre>

<p>First, I need to see whether these differences in responses between families is significant (chi-squared?). Secondly, I need to see if one of the 3 factors has an effect on the response.</p>

<p>Now it seems in my basic understanding, that if the response(s) were continuous measurement, ANOVA/MANOVA would work. Easy-peasy. However, since everything is discreet categories (including the independent and dependent variables) I can't do this. Additionally, since the responses are not mutually exclusive, this seems to violate an assumption of the log-linear model.</p>

<p>I've scoured, and keep bouncing around between Multinomial Logistic Regression, or just independent Chi-Square tests, or... hell I don't know anymore.</p>

<p>And yes, I am trying to swim before learning to float.</p>

<p>Oh, and this is all happening in R.</p>
"
"0.0930484210398471","0.0771142136168656"," 19869","<p>I'm running a predictive model to predict the probability of winning a certain item based on the price that I bid (other factors also). After running the model (ols) in R, I wanted to account for all the variables in my model and develop a graph highlighting the 'predicted probabilities' regarding the primary variables I'm concerned about. So want to have a line graph showing the probability of winning on the y axis, and the bid on the x axis. The following data would result in a graph which shows that the probability of winning decreases as the bid increases.</p>

<pre><code>Bid                  8  6      4
Probability Winning 30% 22%    18%
</code></pre>

<ol>
<li>Are predicted probabilities only relevant for logistic regression models or can be equally relevant for linear regression models?</li>
<li>What is the reasoning and logic behind going from a model to a probability curve which would show the 'trend' in one variable as predicted by another, while accounting for all other factors.</li>
</ol>

<p>Sorry for the elementary question, I'm just a little clueless.
Thanks for the help!</p>
"
"0.0930484210398471","0.0440652649239232"," 19895","<p>I've run an ordered logistic regression model in R with Zelig and am looking to calculate predicted probabilities. Zelig has a series of simple one line commands to generate the information I want on first differences and so forth. Unfortunately, I keep getting an error when running the zelig function and was wondering if there was a quick alternative for generating predicted probabilities for a ordered logit in R.</p>

<p>For what it's worth, here's the error from my Zelig code.</p>

<pre><code>&gt; x.out &lt;- setx(mod, credit=1)
Error in dta[complete.cases(mf), names(dta) %in% vars, drop = FALSE] : 
  incorrect number of dimensions
</code></pre>

<p>I just need an alternative solution that I can use to generate the probabilities.</p>
"
"0.107443061870051","0.0763232776972177"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.0657951694959769","0.0311588476424878"," 20523","<p>What is the difference between <a href=""https://en.wikipedia.org/wiki/Logistic_regression"">Logit</a> and <a href=""https://en.wikipedia.org/wiki/Probit_model"">Probit model</a>?</p>

<p>I'm more interested here in knowing when to use logistic regression, and when to use Probit.</p>

<p>If there is any literature which defines it using <a href=""http://en.wikipedia.org/wiki/R_%28programming_language%29"">R</a>, that would be helpful as well.</p>
"
"0.1176979772673","0.0696733014291618"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"NaN","NaN"," 20854","<p>I have read from <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">here</a> and understand how to calculate the estimated logit from a fitted logistic regression model, but how to work on the confidence interval? As it involved a variance-covariance matrix and I think it is better to have a program to do the calculation, rather then doing it by myself.</p>

<p>Thanks.</p>

<h3>Edit 01</h3>

<p>I have added a script here:</p>

<pre><code>chdage.dummy &lt;- data.frame(chd=c(rep(1,50),rep(0,50)),
                           race=c(rep(""white"",5),rep(""black"",20),rep(""hispanic"",15),rep(""other"",10),
                                  rep(""white"",20),rep(""black"",10),rep(""hispanic"",10),rep(""other"",10)),
                           stringsAsFactors=FALSE)
chdage.dummy[,""race""] &lt;- factor(chdage.dummy[,""race""],levels=c(""white"",""black"",""hispanic"",""other""))
chdage.lr.02 &lt;- glm(chd~race,data=chdage.dummy,family=""binomial"")
predict(chdage.lr.02,newdata=data.frame(race=""white""))
</code></pre>

<p><code>predict</code> function can give me an estimate, but I can't use <code>confint</code> outside <code>predict</code>, so what can I do?</p>
"
"0.0759737176397586","0.0539687072220866"," 20864","<p>I am learning logistic regression modeling from the book <em>Applied Logistic Regression</em> by Hosmer.</p>

<p>I need to create a plot named ""create univariable smoothed scatterplot on logit scale"", something like this one (<a href=""http://www.ats.ucla.edu/stat/SAS/examples/alr2/hlch4sas.htm"" rel=""nofollow"">Figure 4.2 page 107</a>):
<img src=""http://i.stack.imgur.com/1HRT2.gif"" alt=""enter image description here""></p>

<p>Can anyone help? Thanks.</p>

<h3>edit 01</h3>

<p>Thanks for all the answers. I tried creating the plot, and discover that <code>smooth.spline</code> created the graph by using percentage (<code>nrow((df[df[,""dfree""]==1,]))/nrow(df)</code>), rather than logit (<code>log(nrow(df[df[,""dfree""]==1,])/nrow(df[df[,""dfree""]==0,]))</code>) when creating the y-axis. Yes, the graph looks similar, but I wonder if we can create an exact copy of that? Thanks.</p>

<p>One more thing, when using logit, some of the logit value in the data.frame is <code>-Inf</code> and <code>Inf</code>, which is not allowed in <code>smooth.spline</code>.</p>

<h3>edit 02</h3>

<p>I have further question concerning my edit:
What if I manually dropped all <code>Inf</code> and <code>-Inf</code> in my data.frame and do <code>smooth.spline</code> on that data.frame? Is it appropriate?</p>
"
"0.1176979772673","0.0696733014291618"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0986927542439653","0.0623176952849756"," 21177","<p>So I hear the term 'testing a model' used a lot. There seems to be a variety of ways to find whether a model has explanatory power (R^2) or if there is an overall significant relationship between the variables (g-test). However, let's say I have data which I acquire each day. Let's say I ran a logistic regression on the past three days of day. Now, I want to know if that model has any explanatory power in explaining data from the newest day. So does this model still hold up if I look at just the newest day of data? I'm just wondering if there's a better way to test a model this way than by simply running that same model on the new dataset? </p>
"
"0.080582296402538","0.0763232776972177"," 22392","<p>I am learning logistic regression modeling using the book ""Applied Logistic Regression"" by Hosmer.</p>

<p>In chpaters, he suggested using Fractional Polynomials for fitting continuous variable which does not seems to be related to logit in linear fashion. I tried the <code>mfp</code> package and can give exactly the same verbose as the book. </p>

<p>But I don't know how to write the transformed variable based on the output of fractional polynomials. The book only shows example of the transformed variable when $J=2$ with $p_1=0$ and $p_2=-0.5$ (page 101) and when $J=2$ with $p_1=2$ and $p_2=2$ (page 101), But what about the others? Currently my case is $J=2$ with $p_1=-1$ and $p_2=-1$.</p>

<p>I know little about fractional polynomials and the book seems not giving sufficient hits on this part. Can anyone refer me to some place which I can know how to write the polynomial? Thanks.</p>
"
"0.0465242105199235","0.0440652649239232"," 22462","<p>Let's say that I am putting together a logistic regression model where I am predicting 
something (y) based on the day of the week. However, the model needs to account for each single day.</p>

<p>Therefore, instead of:</p>

<pre><code>y = B0 + B1*(day)
</code></pre>

<p>Where day is a categorical variable with 7 levels.</p>

<p>It would be:</p>

<pre><code>y = B0 + B1*(monday) + B2*(tuesday) + B3*(wednesday) + ... + B7*(sunday)
</code></pre>

<p>I'm basically thinking that each day needs a separate coefficient because each 
has a different affect on y. However, I think each will need to be a dummy variable 
so that for monday, 1 is for monday, and 0 for not monday, and so forth. </p>

<p>I'm just curious if there is a statistical logic to doing it the second way 
with separate days? What's the best way to do this?</p>
"
"0.0657951694959769","0.0311588476424878"," 22716","<p>I am currently playing around with the MNIST dataset (<a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow"">http://yann.lecun.com/exdb/mnist/</a>) in R. The training set size is 60000x748 and it seems to drain all my memory even when constructing simple models like logistic regression.</p>

<p>My question is: how do you guys usually deal with big datasets in R?</p>

<p>And tangent: is it feasible to break the dataset into smaller chunks, construct a model on each, then perform a weighted average on the predicted values?</p>
"
"0.0986927542439653","0.0623176952849756"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.0746047096952991","0.0824385620013739"," 24251","<p>Complex survey data is that typically found produced by the National Center for Health Statistics (NCHS) or the NSLY; it typically contains information on PSU, strata, and weights. To make nationally representative samples, one would traditionally perform a weighted regression that accounts for the sampling design by Taylor linearization (i.e. the survey analog to Huber-White errors). </p>

<p>I'm interested in matched analyses (e.g., King's <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">MatchIt program</a>) as a manner in which to improve causal inference. What remains unclear from a first look is: (1) what criteria should be used to determine when matched analyses are appropriate with complex survey data; and (2) how such matched analyses ought to account for weights and/or survey sampling. </p>

<p>My understanding of (1) is that there is nothing different about these analyses than any other, but that it might/must improve inference and efficiency when the number of matched cases is small. As regards (2), my understanding is that common recommendations suggest including weights, and not the sampling design, in the matching (e.g., a weighted logistic regression to develop propensity scores) and not the later causal inference. </p>

<p>Should the sampling structure (e.g., PSU, strata) not be taken into account? Any references, suggestions, confirmations, or contradictions of what is above would be welcomed. </p>
"
"0.113960576459638","0.107937414444173"," 24365","<p>I am using the mlogit package in R to run a multinomial logistic regression on pooled discrete choice data collected using two different questionnaire formats. I want to test whether the format had a significant effect on choices. When I run the basic model I get a result. But when I run the same model with a dummy variable indicating which format the respondents saw, I get an error: ""Error in solve.default(H, g[!fixed]) : Lapack routine dgesv: system is exactly singular""</p>

<p>I was able to replicate the error using Train's Electricity dataset in the mlogit package, setting a dummy based on whether the respondent ID was odd or even:</p>

<pre><code>library(mlogit)
data(""Electricity"", package = ""mlogit"")
Electr &lt;- mlogit.data(Electricity, id = ""id"", choice = ""choice"", 
                      varying = 3:26, shape = ""wide"", sep = """")
Electr$odd.dummy &lt;- ifelse(Electr$id %% 2 == 0, 0, 1) # As example, set dummy if ID is odd
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas | 0, data=Electr)) # Basic model
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas + odd.dummy | 0, data=Electr)) # Basic + dummy
summary(mlogit(choice ~ odd.dummy | 0, data=Electr)) # Only dummy
</code></pre>

<p>As with my data, the first model runs, but the second two are singular.</p>

<p>I understand that a result will be singular if there is perfect colinearity between variables, but I don't see how this is the case here.  Respondents were randomly assigned to one format or the other, and the underlying experimental design was the same in both formats, so there shouldn't be any colinearity between the dummy and the other variables.</p>

<p>I would be grateful if someone could explain why adding the dummy leads to a singular result, and even more grateful if they could suggest a solution to avoid it.</p>
"
"0.109489780290272","0.11234482285936"," 24442","<p>I'm running into troubles fitting a polytomous logistic regression model using grouped data. The data are of the form (dput at bottom):</p>

<pre><code>&gt; head(alligator)
    lake  sex  size    food count
1 Hancock male small    fish     7
2 Hancock male small  invert     1
3 Hancock male small reptile     0
4 Hancock male small    bird     0
5 Hancock male small   other     5
6 Hancock male large    fish     4
</code></pre>

<p>And I've tried to fit the model with <code>vglm()</code> from package VGAM:</p>

<pre><code>&gt; result &lt;- vglm(food~lake+size+sex, data=alligator, fam=multinomial, weights=count)
Error in if (max(abs(ycounts - round(ycounts))) &gt; smallno) warning(""converting 'ycounts' to integer in @loglikelihood"") : 
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  96 elements replaced by 1.819e-12
</code></pre>

<p>It was also suggested to look at <code>mlogit()</code> from package <code>globaltest</code> (on Bioconductor), but it does not appear to support grouped data. It obviously doesn't support the <code>weights</code> parameter, but I can't find where the equivalent parameter is documented:</p>

<pre><code>source(""http://bioconductor.org/biocLite.R"")
biocLite(""globaltest"")

result &lt;- mlogit(food~lake+size+sex, weights=count, data=alligator)
Error in mlogit(food ~ lake + size + sex, weights = count, data = alligator) : 
  unused argument(s) (weights = count)
</code></pre>

<p>If anyone could put me down the right path, I'd appreciate it!</p>

<pre><code>&gt; dput(alligator)
structure(list(lake = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""George"", ""Hancock"", 
""Oklawaha"", ""Trafford""), class = ""factor""), sex = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""female"", 
""male""), class = ""factor""), size = structure(c(2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""large"", 
""small""), class = ""factor""), food = structure(c(2L, 3L, 5L, 1L, 
4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 
2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 
3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 
5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 
1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L), .Label = c(""bird"", 
""fish"", ""invert"", ""other"", ""reptile""), class = ""factor""), count = c(7L, 
1L, 0L, 0L, 5L, 4L, 0L, 0L, 1L, 2L, 16L, 3L, 2L, 2L, 3L, 3L, 
0L, 1L, 2L, 3L, 2L, 2L, 0L, 0L, 1L, 13L, 7L, 6L, 0L, 0L, 3L, 
9L, 1L, 0L, 2L, 0L, 1L, 0L, 1L, 0L, 3L, 7L, 1L, 0L, 1L, 8L, 6L, 
6L, 3L, 5L, 2L, 4L, 1L, 1L, 4L, 0L, 1L, 0L, 0L, 0L, 13L, 10L, 
0L, 2L, 2L, 9L, 0L, 0L, 1L, 2L, 3L, 9L, 1L, 0L, 1L, 8L, 1L, 0L, 
0L, 1L)), .Names = c(""lake"", ""sex"", ""size"", ""food"", ""count""), class = ""data.frame"", row.names = c(NA, 
-80L))
</code></pre>
"
"0.0930484210398471","0.0440652649239232"," 24962","<p>When I run multinomial logistic regression with some of the explanatory variables as categorical, my algo (glm) turns them in binary variables, automatically. For examples if one categorical variable X has three values a, b anc c, then my output shows cofficient and t-values for x.a, x.b and x.c. </p>

<p>But in fact I want t-value at the level of x itself so that I can see if variable X is significant or not in determination of dependent variable. Can you please suggest some way so that I can see output directly at the level of X?</p>
"
"0.0930484210398471","0.0440652649239232"," 24975","<p>I am working on an assignment involving a logistic regression model, where I need to plot the pearson standardized residuals against one of the predictors. Here's the basic setup:</p>

<pre><code>model &lt;- glm(outcome ~ predictor1 + predictor2, family=binomial(logit))
res &lt;- residuals(model, ""pearson"")
</code></pre>

<p>When looking at the residuals' distribution, I see something totally different than my colleagues who use Stata (using predict and rstandard). Their residuals are more or less normal, whereas in mine there is a gap in the values (not a singe residual is between -0.05 and 1.15). That does make sense in the context of logistic regression, especially that the maximum predicted probability is not so high (38%). </p>

<p>I'd like to understand what's happening here... What is Stata doing that R isn't, with those residuals? </p>
"
"0.0379868588198793","0.0179895690740289"," 25282","<p>I want to make a nested logistic regression in R with the package <a href=""http://cran.r-project.org/web/packages/mlogit/index.html"" rel=""nofollow"">mlogit</a>.</p>

<p>I would like to test how producer's decision to enter organisations (14 organisations) or not is affected by different factors.</p>

<p>Producer can be in specific organisation and other according to year. I was advised to use year as separate variables in columns. This is OK. But for my different organizations will it be the same? So if my producer appear in my database several times (6 times) in row, isn't it redundant? </p>

<p>Have you an Idea of the struture of database that I can adopt?</p>

<p>Here is how my data looks like in R:</p>

<pre><code>   Year   Organisation   Member AGE.Member ...
1  2005 Organisation 1 Member 1         37
2  2005 Organisation 1 Member 2         32
3  2005 Organisation 3 Member 3         32
4  2005 Organisation 4 Member 4         35
5  2005 Organisation 2 Member 5         33
6  2005 Organisation 3 Member 6         33

'data.frame':   18 obs. of  4 variables:
 $ ANNEE       : int  2005 2005 2005 2005 2005 2005 2005 2005 2005 2006 2006 2006...
     $ Organisation: Factor w/ 4 levels ""Organisation 1"",..: 1 1 3 4 2 3 2 3 3 2 ...
 $ Member      : Factor w/ 9 levels &quot;Member 1&quot;,&quot;Member 2&quot;,..: 1 2 3 4 5 6 7 8 9 1 ...
     $ AGE.Member  : int  37 32 32 35 33 33 32 32 33 37 ...
</code></pre>
"
"NaN","NaN"," 25389","<p>Let's say that I have an object of class <code>glm</code> (corresponding to a logistic regression model) and I'd like to turn the predicted probabilities given by <code>predict.glm</code> using the argument <code>type=""response""</code> into binary responses, i.e. $Y=1$ or $Y=0$. What's the quickest &amp; most canonical way to do this in R? </p>

<p>While, again, I'm aware of <code>predict.glm</code>, I don't know where exactly the cutoff value $P(Y_i=1|\hat X_{i})$ lives -- and I guess this is my main stumbling block here.</p>
"
"0.0882734829504749","0.0557386411433294"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.20385887657505","0.120677698006369"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.0379868588198793","0.0539687072220866"," 26178","<p>I want to create a classification table regarding an ordinal response variable with three levels but I don't know how to do it. Searching on the site I fell on the question posted by Brandon Bertelsen that covers only the case of the binary logistic regression (link at the end of the post).Does anyone knows how I can create such that table in my case?</p>

<p>I don't know if it is important but I used the <code>rms</code> package to run the olr and using the <code>predict(fit,type=""fitted.ind"")</code> command I get the next table with probability for each case</p>

<pre><code>      grade=1    grade=2   grade=3
1  0.08042197 0.28380601 0.6357720
2  0.08086877 0.28475584 0.6343754
3  0.41472656 0.40802584 0.1772476
4  0.39680650 0.41484517 0.1883483
5  0.25402385 0.43644283 0.3095333
6  0.13539881 0.37098177 0.4936194
7  0.12591996 0.35959459 0.5144855
8  0.50489952 0.36489760 0.1302029
9          NA         NA        NA
10 0.34757283 0.42969971 0.2227275
11 0.24690054 0.43539812 0.3177013
12 0.17325212 0.40529586 0.4214520
13 0.45795712 0.38900855 0.1530343
14 0.03594015 0.16033637 0.8037235
15         NA         NA        NA
16 0.50188652 0.36653955 0.1315739
17 0.48710163 0.37441720 0.1384812
18 0.38094725 0.42028884 0.1987639
19 0.04134659 0.17894428 0.7797091
20 0.12844729 0.36275605 0.5087967
21 0.23991274 0.43410413 0.3259831
22 0.20506362 0.42316514 0.3717712
23 0.45457929 0.39061326 0.1548075
24         NA         NA        NA
25 0.31269786 0.43606610 0.2512360
26 0.20905830 0.42483513 0.3661066
27 0.05240710 0.21353381 0.7340591
28 0.26569967 0.43759072 0.2967096
29 0.21258621 0.42621415 0.3611996
30 0.11407246 0.34347156 0.5424560
31 0.34656138 0.42993750 0.2235011
32 0.01813256 0.08978609 0.8920813
33 0.44034224 0.39716470 0.1624931
34 0.12213714 0.35468488 0.5231780
35 0.40888783 0.41032190 0.1807903
36 0.33901842 0.43161582 0.2293658
37 0.13275554 0.36793345 0.4993110
38 0.32091057 0.43492411 0.2441653
39 0.45984161 0.38810515 0.1520532
40 0.55550665 0.33564053 0.1088528
41 0.02812293 0.13122652 0.8406505
42 0.46250424 0.38681892 0.1506768
43 0.07352751 0.26852580 0.6579467
44 0.04330967 0.18541327 0.7712771
45 0.45457929 0.39061326 0.1548075
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r"">Logistic Regression: Classification Tables a la SPSS in R</a></p>
"
"0.1176979772673","0.0557386411433294"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.124341182825498","0.0824385620013739"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.124341182825498","0.0706616245726062"," 26652","<p>I have large dataset (around 2 million records and 300 features) with a lot of missing data. Most of the independent variables are categorical (some of these variables have more than 40 valid values). The outcome is either Y or N. The Y outcome is a rare event: around 98% of outcomes are N. </p>

<p>I'm supposed to fit a logistic regression model to these data. I took random sample of them, keeping the same distribution. I am working in R, but I'm new to both R and logistic regression modeling and I have some questions:</p>

<ol>
<li><p>I applied <code>factor</code> to the outcome.  Do I need to apply it on every categorical independent variable?  I have more than 200 variables, some of them have only 2 valid values while others have 40!  Will it affect the size of the data? </p></li>
<li><p>Is there any advice about attribute selection? Should it be done before fitting the logistic regression model or after, depending on the results?</p></li>
<li><p>Is it recommended to take biased sample data where the outcome Y is more than the original distribution in the large data?</p></li>
<li><p>There are fields like <code>userId</code>, <code>groupId</code>, etc. What type of data do we consider these to be? How to deal with them?</p></li>
<li><p>What other predictive models are suitable for this kind of data?</p></li>
</ol>
"
"0.0328975847479884","0.0467382714637317"," 26762","<p>I'm reviewing a paper which has the following biological experiment. A device is used to expose cells to varying amounts of fluid shear stress. As greater shear stress is applied to the cells, more of them start to detach from the substrate. At each level of shear stress, they count the cells that remain attached, and since they know the total number of cells that were attached at the beginning, they can calculate a fractional attachment (or detachment).</p>

<p>If you plot the adherent fraction vs. shear stress, the result is a logistic curve. In theory, each individual cell is a single observation, but obviously there are thousands or tens of thousand of cells, so the data set would be gigantic, if it was set up in the usual way (with each row being an observation).</p>

<p>So, naturally, my question (as stated in the title) should make sense now. How do we do a logistic regression using the fractional outcome as the D.V.? Is there some automatic transform that can be done in glm?</p>

<p>Along the same lines, if there were potentially 3 or more (fractional) measurements, how would one do this for a multinomial logistic regression?</p>
"
"0.157934513826448","0.0883924339314693"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.0657951694959769","0.0623176952849756"," 27001","<p>I need to generate simulated data for a conditional logistic regression using <code>R</code>.</p>

<p>The code I wrote appears correct, but something is wrong.  After generating my data, betas, and labels, I use <code>glm()</code> to attempt to estimate the betas.  If everything is correct, the estimated betas from <code>glm</code> should be very close to the true betas.  However, <code>glm()</code> fails to converge.  </p>

<p>Below is my code.  Can anyone point out what I'm doing wrong?</p>

<p>Thanks!</p>

<pre><code>library(survival)

# Initialize parameters
k &lt;- 5
n &lt;- 10
g &lt;- 20

p &lt;- rep(0, n*g)
y &lt;- rep(0, n*g)

# Create the group index
GI &lt;- vector(mode=""numeric"")
for(group in 1:g){
    GI &lt;- c(GI, rep(group,n))
}

# generate the data
betas &lt;- matrix(rnorm(k), ncol=1)
x &lt;- matrix(rnorm(n*g*k), ncol=k)

# calculate the true labels
for(group in 1:g){
    p[GI==group] &lt;- exp(x[GI==group,] %*% betas) / sum( exp(x[GI==group,] %*% betas) )
    y[n*(group-1)+which.max(p[GI==group])] &lt;- 1
}

cl &lt;- clogit(y ~x + strata(GI))
</code></pre>
"
"NaN","NaN"," 27297","<p>I'm using the <code>logistf</code> package in R to perform Firth logistic regression on an unbalanced dataset. I have a logistf object:</p>

<pre><code>fit = logistf(a~b)
</code></pre>

<p>Is there a <code>predict()</code> function like on that's used in the <code>lm</code> class to predict probabilities for future data points? Or do I have to manually input the estimated parameters from the Firth regression.</p>
"
"0.0882734829504749","0.0696733014291618"," 27400","<p>I'm reading A. Agresti (2007), <em><a href=""http://rads.stackoverflow.com/amzn/click/0471226181"">An Introduction to Categorical Data Analysis</a></em>, 2nd. edition, and am not sure if I understand this paragraph (p.106, 4.2.1) correctly (although it should be easy):</p>

<blockquote>
  <p>In Table 3.1 on snoring and heart disease in the previous chapter, 254
  subjects reported snoring every night, of whom 30 had heart disease.
  If the data file has grouped binary data, a line in the data file
  reports these data as 30 cases of heart disease out of a sample size
  of 254. If the data file has ungrouped binary data, each line in the
  data file refers to a separate subject, so 30 lines contain a 1 for
  heart disease and 224 lines contain a 0 for heart disease. The ML
  estimates and SE values are the same for either type of data file.</p>
</blockquote>

<p>Transforming a set of ungrouped data (1 dependent, 1 independent) would take more then ""a line"" to include all the information!? </p>

<p>In the following example a (unrealistic!) simple data set is created and a logistic regression model is build. </p>

<p>How would grouped data actually look like (variable tab?)? How could the same model be build using grouped data? </p>

<pre><code>&gt; dat = data.frame(y=c(0,1,0,1,0), x=c(1,1,0,0,0))
&gt; dat
  y x
1 0 1
2 1 1
3 0 0
4 1 0
5 0 0
&gt; tab=table(dat)
&gt; tab
   x
y   0 1
  0 2 1
  1 1 1
&gt; mod1=glm(y~x, data=dat, family=binomial())
</code></pre>
"
"0.174077655955698","0.15310018657398"," 27830","<p>In a previous post Iâ€™ve wondered how to <a href=""http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as"">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=""http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281"">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.
The formula is simple:</p>

<p>$logit(y)=log(\frac{y-y_{min}}{y_{max}-y})$</p>

<p>To avoid log(0) and division by 0 you extend the range by a small value, $\epsilon$. This gives an environment that respects the boundaries of the score. </p>

<p>The problem is that any $\beta$ will be in the logit scale and that makes doesnâ€™t make any sense unless transformed back into the regular scale but that means that the $\beta$ will be non-linear. For graphing purposes this doesnâ€™t matter but not with more $\beta$:s this will be very inconvenient. </p>

<p>My question:</p>

<p><strong>How do you suggest to report a logit $\beta$ without reporting the full span?</strong></p>

<hr>

<h2>Implementation example</h2>

<p>For testing the implementation Iâ€™ve written a simulation based on this basic function:</p>

<p>$outcome=\beta_0+\beta_1* xtest^3+\beta_2*sex$</p>

<p>Where $\beta_0 = 0$, $\beta_1 = 0.5$ and $\beta_2 = 1$. Since there is a ceiling in scores Iâ€™ve set any outcome value above 4 and any below -1 to the max value.</p>

<h3>Simulate the data</h3>

<pre><code>set.seed(10)
intercept &lt;- 0
beta1 &lt;- 0.5
beta2 &lt;- 1
n = 1000
xtest &lt;- rnorm(n,1,1)
gender &lt;- factor(rbinom(n, 1, .4), labels=c(""Male"", ""Female""))
random_noise  &lt;- runif(n, -1,1)

# Add a ceiling and a floor to simulate a bound score
fake_ceiling &lt;- 4
fake_floor &lt;- -1

# Just to give the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)

# Simulate the predictor
linpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == ""Female"") + random_noise
# Remove some extremes
linpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |
    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA
#limit the interval and give a ceiling and a floor effect similar to scores
linpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling
linpred[linpred &lt; fake_floor] &lt;- fake_floor
</code></pre>

<p>To plot the above:</p>

<pre><code>library(ggplot2)
# Just to give all the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)
qplot(y=linpred, x=xtest, col=gender, ylab=""Outcome"")
</code></pre>

<p>Gives this image:</p>

<p><img src=""http://i.stack.imgur.com/luZGu.png"" alt=""Scatterplot from simulation""></p>

<h3>The regressions</h3>

<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>

<pre><code>library(rms)

# Regular linear regression
fit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)
boot_fit_lm &lt;- bootcov(fit_lm, B=500)
p &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
lm_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# Quantile regression regular
fit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)
boot_rq &lt;- bootcov(fit_rq, B=500)
# A little disturbing warning:
# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique

p &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
rq_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# The logit transformations
logit_fn &lt;- function(y, y_min, y_max, epsilon)
    log((y-(y_min-epsilon))/(y_max+epsilon-y))


antilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)
    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/
        (1+exp(antiy))


epsilon &lt;- .0001
y_min &lt;- min(linpred, na.rm=T)
y_max &lt;- max(linpred, na.rm=T)
logit_linpred &lt;- logit_fn(linpred, 
                          y_min=y_min,
                          y_max=y_max,
                          epsilon=epsilon)

fit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)
boot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)


p &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))

# Change back to org. scale
transformed_p &lt;- p
transformed_p$yhat &lt;- antilogit_fn(p$yhat,
                                    y_min=y_min,
                                    y_max=y_max,
                                    epsilon=epsilon)
transformed_p$lower &lt;- antilogit_fn(p$lower, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)
transformed_p$upper &lt;- antilogit_fn(p$upper, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)

logit_rq_plot &lt;- plot.Predict(transformed_p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)
</code></pre>

<h3>The plots</h3>

<p>To compare with the base function Iâ€™ve added this code:</p>

<pre><code>library(lattice)
# Calculate the true lines
x &lt;- seq(min(xtest), max(xtest), by=.1)
y &lt;- beta1*x^3+intercept
y_female &lt;- y + beta2
y[y &gt; fake_ceiling] &lt;- fake_ceiling
y[y &lt; fake_floor] &lt;- fake_floor
y_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling
y_female[y_female &lt; fake_floor] &lt;- fake_floor

tr_df &lt;- data.frame(x=x, y=y, y_female=y_female)
true_line_plot &lt;- xyplot(y  + y_female ~ x, 
                         data=tr_df,
                         type=""l"", 
                         xlim=my_xlim, 
                         ylim=my_ylim, 
                         ylab=""Outcome"", 
                         auto.key = list(
                           text = c(""Male"","" Female""),
                           columns=2))


# Just for making pretty graphs with the comparison plot
compareplot &lt;- function(regr_plot, regr_title, true_plot){
  print(regr_plot, position=c(0,0.5,1,1), more=T)
  trellis.focus(""toplevel"")
  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)
  trellis.unfocus()
  print(true_plot, position=c(0,0,1,.5), more=F)
  trellis.focus(""toplevel"")
  panel.text(0.3, .65, ""True line"", cex = 1.2, font = 2)
  trellis.unfocus()
}

compareplot(lm_plot, ""Linear regression"", true_line_plot)
compareplot(rq_plot, ""Quantile regression"", true_line_plot)
compareplot(logit_rq_plot, ""Logit - Quantile regression"", true_line_plot)
</code></pre>

<p><img src=""http://i.stack.imgur.com/74Uid.png"" alt=""Linear regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/xHRtF.png"" alt=""Quantile regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/XfLy8.png"" alt=""Logistic quantile regression for bounded outcome""></p>

<h3>The contrast output</h3>

<p>Now I've tried to get the contrast and it's almost ""right"" but it varies along the span as expected:</p>

<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), 
+                              xtest=c(-1:1)), 
+          FUN=function(x)antilogit_fn(x, epsilon))
   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)
   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  
   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  
   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  
*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  
   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  
*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
</code></pre>
"
"0.0657951694959769","0.0311588476424878"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.0930484210398471","0.0440652649239232"," 29406","<p>I have the following linear model:</p>

<p>$$w^*=\text{arg min}_w\sum_{i=1}^N \bigg(Y_i-\sum_{j=1}^M X_{i,j}\times w_j\bigg)^2$$</p>

<p>Let $T \in N^*$ and $e_i=|Y_i-\sum_{j=1}^M X_{i,j}\times w_j|$. </p>

<p>It's possible using logistic regression to predict which errors will be less than $T$ (i.e., $e_i&lt;T$) and greater or equal with $T$ (i.e., $e_i \ge T$)?</p>

<p>Here is more information to make the question clearer:</p>

<p>$N$ represent the number of observations. My data has the following property: the histogram of errors using multiple linear regression has a Laplace distribution. My data come from digital images represented on 8 bits. The $Y_i$ are current pixels and $X_{ij}$ are neighborhoods pixels. I want to predict which pixels produce errors less than $T$. I want to know what R functions can I use to make a test? $T$ is not very large, it has the values between 1 and 15 in general.</p>
"
"0.113960576459638","0.107937414444173"," 29653","<p>The likelihood ratio (a.k.a. deviance) $G^2$ statistic and lack-of-fit (or goodness-of-fit) test is fairly straightforward to obtain for a logistic regression model (fit using the <code>glm(..., family = binomial)</code> function) in R. However, it can be easy to have some cell counts end up low enough that the test is unreliable. One way to verify the reliability of the likelihood ratio test for lack of fit is to compare its test statistic and <em>P</em>-value to those of Pearson's chi square (or $\chi^2$) lack-of-fit test.</p>

<p>Neither the <code>glm</code> object nor its <code>summary()</code> method report the test statistic for Pearson's chi square test for lack of fit. In my search, the only thing I came up with is the <code>chisq.test()</code> function (in the <code>stats</code> package): its documentation says ""<code>chisq.test</code> performs chi-squared contingency table tests and goodness-of-fit tests."" However, the documentation is sparse on how to perform such tests:</p>

<blockquote>
  <p>If <code>x</code> is a matrix with one row or column, or if <code>x</code> is a vector and <code>y</code> is not given, then a <em>goodness-of-fit</em> test is performed (<code>x</code> is treated as a one-dimensional contingency table). The entries of <code>x</code> must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in <code>p</code>, or are all equal if <code>p</code> is not given.</p>
</blockquote>

<p>I'd imagine that you could use the <code>y</code> component of the <code>glm</code> object for the <code>x</code> argument of <code>chisq.test</code>. However, you can't use the <code>fitted.values</code> component of the <code>glm</code> object for the <code>p</code> argument of <code>chisq.test</code>, because you'll get an error: ""<code>probabilities must sum to 1.</code>""</p>

<p>How can I (in R) at least calculate the Pearson $\chi^2$ test statistic for lack of fit without having to run through the steps manually?</p>
"
"0.0657951694959769","0.0311588476424878"," 30406","<p>I am trying to determine if simple probabilities will work for my problem or if it will be better to use (and learn about) more sophisticated methods like logistic regression.</p>

<p>The response variable in this problem is a binary response (0, 1).  I have a number of predictor variables that are all categorical and unordered.  I am trying to determine which combinations of the predictor variables yield the highest proportion of 1's.  Do I need logistic regression?  How would it be an advantage to just calculating proportions in my sample set for each combination of the categorical predictors?</p>
"
"0","0.0311588476424878"," 31294","<p>I'm trying to do a logistic regression on some data. </p>

<p>Here's a simplified version of the situation: </p>

<p>I'm trying to predict student success based on their history, etc. One of my predictors is the percentage of the courses they've passed in the past. If they haven't taken any courses, I don't want to set that to zero, because that's obviously different from having failed all their courses. Right now, these cases are set as NaN, but when I use the glm function in R, I get the following error: </p>

<blockquote>
  <p>Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  : 
    NA/NaN/Inf in foreign function call (arg 1)</p>
</blockquote>

<p><strong>How do I predict performance for individuals who haven't taken any courses yet?</strong></p>
"
"NaN","NaN"," 31299","<p>I have a dependent variable made up of 3 categories and 14 binary predictor variables.</p>

<p>I have tried using <code>mlogit</code> and <code>nnet/multinom</code> packages in R. </p>

<p><strong>Is there a better approach than multinomial logistic regression for this particular scenario?</strong></p>
"
"0.201604912253002","0.13219579477177"," 31592","<p>I am a bit puzzled about the behavior of uncorrelated predictors in logistic regression. 
As in OLS, I thought that if two predictors (<code>rv1</code> and <code>rv2</code>) are uncorrelated, then the regression weights of <code>rv1</code> will not change from a regression that only includes <code>rv1</code> to one that includes <code>rv1</code> and <code>rv2</code>. 
However, it seems to be the case that this is not true in logistic regression and coefficients change between the two regression models, even if the predictors are uncorrelated.</p>

<p>I have pasted some R syntax below that demonstrates this behavior.</p>

<p>Why is this the case and how do the regression weights from the two regressions (the one with only rv1 and the other one with rv1 and rv2) relate to each other? Is there a way to know what the regression weight of rv1 will be if one knows the regression weight of rv1 in the regression that includes both predictors?</p>

<p>Thanks!
P.S. This post is crossposted at another unrelated stat answer site.</p>

<pre><code>library(MASS)

#generate lots of data (a little bit weird data handling, I know)
n &lt;- 10000
rdta &lt;- as.data.frame(mvrnorm(n=n,c(0,0),matrix(c(1,0,0,1),2,2),empirical=TRUE))
names(rdta) &lt;- c(""rv1"",""rv2"")

#confirm that preds are uncorrelated
cov(rdta$rv1,rdta$rv2)

rv1 &lt;- rdta$rv1
    rv2 &lt;- rdta$rv2

rv1ry &lt;- 1
rv2ry &lt;- 1

#generate binary data from known regression coefficients
ylinp &lt;- (1 / (1+exp(-(-1 + rv1*rv1ry + rv2*rv2ry))))
y &lt;- rbinom(n,1,ylinp) 
glm(y~rv1+rv2,family=binomial(link='logit'))
glm(y~rv1,family=binomial(link='logit'))
glm(y~rv2,family=binomial(link='logit'))

#confirm that OLS regression works as expected (regression weights do not change)
rv1y &lt;- .222
rv2y &lt;- .333
y &lt;- rv1y * rv1 + rv2y * rv2 + rnorm(n,0,.5)
lm(y~rv1+rv2)
lm(y~rv1)
lm(y~rv2)   
</code></pre>

<p>I am not sure if it is expected to also paste relevant output here, but here goes:
OLS results</p>

<blockquote>
  <p>lm(y~rv1+rv2)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1 + rv2)

Coefficients:
(Intercept)          rv1          rv2  
 0.001096     0.220051     0.333072  
</code></pre>

<blockquote>
  <p>lm(y~rv1)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1)

Coefficients:
(Intercept)          rv1  
 0.001096     0.220051  
</code></pre>

<blockquote>
  <p>lm(y~rv2)  </p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv2)

Coefficients:
(Intercept)          rv2  
 0.001096     0.333072  
</code></pre>

<p>Logistic regression results</p>

<blockquote>
  <p>glm(ry~rv1+rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1 + rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1          rv2  
     -1.001        1.916        2.469  
</code></pre>

<blockquote>
  <p>glm(ry~rv1,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1  
    -0.5495       1.0535  
</code></pre>

<blockquote>
  <p>glm(ry~rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv2  
-0.6538       1.6140  
</code></pre>
"
"0.0379868588198793","0.0359791381480577"," 31724","<p>Let's say I have a logistic regression model which predicts whether a consumer will buy an item based on about 10 consumer characteristics. </p>

<p>$$\begin{array}{rcl}Buy &amp;=&amp; B_0 + B_1\times Gender + B_2\times CreditType + B_3\times Education + B_4\times OwnsHome \\\phantom{Buy} &amp;&amp; + B_5\times CarMake + B_6\times CarYear + B_7\times State + B_8\times Income + B_9\times Insurance \\ \phantom{Buy} &amp;&amp;+ B_{10}\times CarAccidents\end{array} $$</p>

<ol>
<li><p>Is there ever an issue with including too many predictors in a logistic regression model? I'm not talking about insignificant variables or ones that may be related, but just the sheer number of variables included in a model. </p></li>
<li><p>With a larger number of predictors, how should one present the regression results in a meaningful manner? Is it just a matter of plotting the probability curve for $Y=1$, or are there ""better"" ways of doing this. I'd be doing this in R, so any help on that end would be appreciated.</p></li>
</ol>
"
"0.113960576459638","0.0539687072220866"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0","0"," 32239","<p>As described in Merlo et al (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16537344"" rel=""nofollow"">J Epidem Comm Health 2006</a>), the 95% credible interval for MOR is calculated using MCMC. MOR is defined as $\exp(\sqrt{2\sigma^2}\times 0.675)$, where $\sigma$ is the level-2 variance of the random intercept $u$ from a null model of a hierarchical logistic regression.  </p>

<p>Does anyone have an idea of how to write a program for an Markov chain Monte Carlo to calculate the standard error of the  median odds ratio (MOR) using <a href=""http://cran.r-project.org/web/packages/rjags/index.html"" rel=""nofollow"">rjags</a>?<br>
My dependent variable is outcome(alive/dead) and the clustering (level2)variable is Hospital. There are 140 hospitals and would like to see variations in outcome between hospitals. Other risk factors will be included later as independent level1 variables.</p>
"
"0.0379868588198793","0.0539687072220866"," 32839","<p>I have a response variable with 2 categories and $500$ predictor variables. The $500$ coefficient $a_1, a_2, a_3, \ldots, a_{500}$ ranges $(-1, 1)$. Positive $a_i$ indicates category A; negative $a_i$ indicates category B. The larger the coefficient, the stronger it indicates its correspondent category. I get the coefficient from a researcher, who score each coefficient from -1 to 1 based on the importance and influence in classification.</p>

<p>To classify an object that have attributes $x_1, x_2, x_3,\ldots,x_{500}$, I am thinking of using logistic regression. But I do not know how to deal with continuous data (the range of the coefficient is continuous from -1 to 1). Is logistic regression viable? </p>

<p>If not, will someone help with other methods and post your code? I prefer using R.</p>
"
"0.113960576459638","0.0539687072220866"," 33151","<p>I have a dataset (<code>data.mrsa</code>) about the MRSA prevalence of elderly in long term care facilities (LTCF) with the following information:</p>

<ul>
<li>mrsa_result: MRSA result of recruited elderly (positive VS negative)</li>
<li>age: residents' age</li>
<li>ltcf: UID for each LTCF (we sampled 30 out of 1000 LTCFs)</li>
<li>ltcf_type: type of LTCF (private VS non-private)</li>
</ul>

<p>I have a multi-level logistic regression model like the one below:</p>

<pre><code>fit2 &lt;- glmer(mrsa_result ~ age + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>I know I am trying to find out the effect of <code>age</code> on the log-odds of <code>mrsa_result</code>, having the <code>ltcf</code> on the 2nd level gives me a wider CI on the lod-odds.</p>

<p>Now I want to add the <code>ltcf_type</code> into the model, I think this should be a fixed effect, as there can only be private and non-private, but <code>itcf_type</code> should be considered as 2nd level data, right? As this describe the type of LTCF, not the type of elderly.</p>

<p>I am puzzled on where should I put the term <code>ltcf_type</code> into my model, I wonder which of the following lines is correct:</p>

<pre><code>fit2a &lt;- glmer(mrsa_result ~ age + ltcf_type + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
fit2b &lt;- glmer(mrsa_result ~ age + (1|ltcf + ltcf_type), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>Thanks.</p>
"
"0.0986927542439653","0.0467382714637317"," 33857","<p>I'm trying to calculate logistic regression coefficients by defining the log-likelihood function and using maximum likelihood.</p>

<p>In some cases when the initial (start) values I gave to the maximum likelihood were not correct I got wrong results for the logistic regression (different from the ones I get when using <code>glm</code> for example).</p>

<p>Given the input data and y values, what should be the optimum initial values for logistic regression (or, in other words, what are the values that are being used in <code>glm</code>)?</p>
"
"0.161164592805076","0.0763232776972177"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.119027940128723","0.103342206529982"," 34319","<p>Let's say I have the following logistic regression models:</p>

<pre><code> df=data.frame(income=c(5,5,3,3,6,5),
                  won=c(0,0,1,1,1,0),
                  age=c(18,18,23,50,19,39),
                  home=c(0,0,1,0,0,1))

&gt; md1 = glm(factor(won) ~ income + age + home, 
+           data=df, family=binomial(link=""logit""))
&gt; md2 = glm(factor(won) ~ factor(income) + factor(age) + factor(home), 
+           data=df, family=binomial(link=""logit""))
&gt; summary(md1)

Call:
glm(formula = factor(won) ~ income + age + home, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
      1        2        3        4        5        6  
-1.0845  -1.0845   0.8017   0.4901   1.7298  -0.8017  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.784832   6.326264   0.756    0.449
income      -1.027049   1.056031  -0.973    0.331
age          0.007102   0.097759   0.073    0.942
home        -0.896802   2.252894  -0.398    0.691

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178  on 5  degrees of freedom
Residual deviance: 6.8700  on 2  degrees of freedom
AIC: 14.87

Number of Fisher Scoring iterations: 4

&gt; summary(md2)

Call:
glm(formula = factor(won) ~ factor(income) + factor(age) + factor(home), 
    family = binomial(link = ""logit""), data = df)

Deviance Residuals: 
         1           2           3           4           5           6  
-6.547e-06  -6.547e-06   6.547e-06   6.547e-06   6.547e-06  -6.547e-06  

Coefficients: (3 not defined because of singularities)
                  Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      2.457e+01  1.310e+05       0        1
factor(income)5 -4.913e+01  1.605e+05       0        1
factor(income)6 -2.573e-30  1.853e+05       0        1
factor(age)19           NA         NA      NA       NA
factor(age)23   -1.383e-30  1.853e+05       0        1
factor(age)39   -3.479e-14  1.605e+05       0        1
factor(age)50           NA         NA      NA       NA
factor(home)1           NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178e+00  on 5  degrees of freedom
Residual deviance: 2.5720e-10  on 1  degrees of freedom
AIC: 10
</code></pre>

<p>So depending on the mode of the predictors, R produced different outputs. For factors, R splits out the coefficients into separate categories for the levels, but not for the model with numeric predictors. I'm wondering about a couple things.</p>

<ol>
<li><p>Is it ever useful to have the response categories expressed as individual rows?</p></li>
<li><p>To express the general regression equation, how does one go from a model with the categories expressed in an individual equation to an equation with a single B_i. So, for example, if gender has two coefficients, 3.5 for Male and 2.3 for Female, how does one use that in an equation such that (besides converting them into numeric values):</p></li>
</ol>

<p>Y = B0 + B1 (Gender)</p>
"
"0.0379868588198793","0.0539687072220866"," 34549","<p>I understand that there is a function in R called <code>poly()</code> that can generate orthogonal polynomials--useful for applying on input variables before running a predictive model.</p>

<p>My question is that what is the role of categorical variables when we generate polynomials? Are they to be excluded?</p>

<h2>Update:</h2>

<p>Dan, Thank you for your kind response. I'm not sure I understand it completely - let me explain the query in more detail. I'm trying to run logistic regression using glmnet on <a href=""http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls"" rel=""nofollow"">Titanic dataset</a>. <BR/> Let us assume shortened set of columns:<ul>* class(factor with three levels 1, 2 ,3), <br/>* sex(factor: male, female), <br/>* Age (integer), <br/>*survived(factor &amp; target variable 0 or 1).</ul> The questions is it meaningful to create polynomial features based on these factors? e.g. class. If yes could you pls explain what it means? <BR> I've seen examples with numeric input variables, where one can pass the entire input set to the poly() function and get polynomial features as output. <br/> Your response is highly appreciated.</p>
"
"0.109658615826628","0.0934765429274634"," 34930","<p>I have a large set of data for 37 different clinical units (all oncology) in their respective 37 hospitals. There are two specific outcome variables that I need to analyse:</p>

<p>First, drug usage for specific drugs types and classes (aggregated drugs) that are expressed as a rate â€“  DDD (Defined Daily Doses) per 100 patient days. There are individual patient drug use figures for this set.</p>

<p>Question1: Which regression approach should I take? From what I can gather I can use a Poisson regression model. IF there is overdispersion in the outcome I could resort to a negative binomial model.</p>

<p>Second: I have antibiotic resistance data that is expressed as proportion in the range 0 â€“ 1.These are not available as individual patient data points but aggregated to each of the 37 hospitals.</p>

<p>Question 2: Again, which approach? From what I have read I can use a logistic regression model. I have been advised by another statistician to initiall use a logit model and then use a probit model and compare goodness of fit for each model.</p>

<p>Does this sound like a reasonable approach? Is there a specific text that you could direct me to in order to upgrade my basic regression modelling skills. I will be using R for the analysis.</p>

<p>Thanks in advance.</p>
"
"0.0882734829504749","0.0696733014291618"," 34997","<p>I want to do an ordinal logistic regression in R without the proportionality odds assumption. I know this can be done directly using <code>vglm()</code> function in <code>R</code> by setting <code>parallel=FALSE</code>.</p>

<p>But my problem is how to fix a particular set of coefficients in this regression setup? For example, say the dependent variable $Y$ is discrete and ordinal and can take values $Y = 1$, $2$, or $3$. If the regressors are $X_{1}$ and $X_{2}$, then the regression equations are</p>

<p>$$ \begin{aligned} 
{\rm logit} \big( P(Y \leq 1) \big) &amp;= \alpha_{1} + \beta_{11}X_{1} + \beta_{12}X_{2} \\
{\rm logit}\big(P(Y \leq 2) \big) &amp;= \alpha_{2} + \beta_{21}X_{1} + \beta_{22}X_{2} 
\end{aligned} $$</p>

<p>I want to set $\beta_{11}$ and $\beta_{22}$ to $1$. Please let me know how can I achieve this. Also if <code>R</code> can't do this, could you also please let me know if I can achieve this in any other statistical software?</p>
"
"NaN","NaN"," 35071","<p>Fitting a logistic regression using <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> ends with </p>

<pre><code>Error in mer_finalize(ans) : Downdated X'X is not positive definite. 
</code></pre>

<p>A likely cause of this error is apparently rank deficiency. What is rank deficiency, and how should I address it?</p>
"
"0.164487923739942","0.116845678659329"," 35940","<p>This question is in response to an answer given by @Greg Snow in regards to a <a href=""http://stats.stackexchange.com/questions/35918/logistic-regression-the-standard-deviation-used-in-glmpower"">question</a> I asked concerning power analysis with logistic regression and SAS <code>Proc GLMPOWER</code>.</p>

<p>If I am designing an experiment and will analze the results in a factorial logistic regression, how can I use <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">simulation</a> ( and <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"">here</a> ) to conduct a power analysis?</p>

<p>Here is a simple example where there are two variables, the first takes on three possible values {0.03, 0.06, 0.09} and the second is a dummy indicator {0,1}. For each we estimate the response rate for each combination (# of responders / number of people marketed to). Further, we wish to have 3 times as many of the first combination of factors as the others (which can be considered equal) because this first combination is our tried and true version. This is a setup like given in the SAS course mentioned in the linked question.</p>

<p><img src=""http://i.stack.imgur.com/4LIvh.jpg"" alt=""enter image description here""></p>

<p>The model that will be used to analyze the results will be a logistic regression, with main effects and interaction (response is 0 or 1). </p>

<pre><code>mod &lt;- glm(response ~ Var1 + Var2 + I(Var1*Var2))
</code></pre>

<p>How can I simulate a data set to use with this model to conduct a power analysis?</p>

<p>When I run this through SAS <code>Proc GLMPOWER</code> (using <code>STDDEV =0.05486016</code>
 which corresponds to <code>sqrt(p(1-p))</code> where p is the weighted average of the shown response rates):  </p>

<pre class=""lang-sas prettyprint-override""><code>data exemplar;
  input Var1 $ Var2 $ response weight;
  datalines;
    3 0 0.0025  3
    3 1 0.00395 1
    6 0 0.003   1
    6 1 0.0042  1
    9 0 0.0035  1
    9 1 0.002   1;
run;

proc glmpower data=exemplar;
  weight weight;
  class Var1 Var2;
  model response = Var1 | Var2;
  power
    power=0.8
    ntotal=.
    stddev=0.05486016;
run;
</code></pre>

<p>Note: <code>GLMPOWER</code> only will use class (nominal) variables so 3, 6, 9 above are treated as characters and could have been low, mid and high or any other three strings. When the real analysis is conducted, Var1 will be used a numeric (and we will include a polynomial term Var1*Var1) to account for any curvature.</p>

<p>The output from SAS is </p>

<p><img src=""http://i.stack.imgur.com/T44tM.png"" alt=""enter image description here""></p>

<p>So we see that we need 762,112 as our sample size (Var2 main effect is the hardest to estimate) with power equal to 0.80 and alpha equal to 0.05. We would allocate these so that 3 times as many were the baseline combination (i.e. 0.375 * 762112) and the remainder just fall equally into the other 5 combinations.</p>
"
"0.0994729462603988","0.0824385620013739"," 37411","<p>Suppose I'm building a logistic regression classifier that predicts whether someone is married or single. (1 = married, 0 = single) I want to choose a point on the precision-recall curve that gives me at least 75% precision, so I want to choose thresholds $t_1$ and $t_2$, so that:</p>

<ul>
<li>If the output of my classifier is greater than $t_1$, I output ""married"".</li>
<li>If the output is below $t_2$, I output ""single"".</li>
<li>If the output is in between, I output ""I don't know"".</li>
</ul>

<p>A couple questions:</p>

<ol>
<li>I think under the standard definition of precision, precision will be measuring the precision of the married class alone (i.e., precision = # times I correctly predict married / total # times I predict married). However, what I really want to do is measure the overall precision (i.e., the total # times I correctly predict married or single / total # times I predict married or single). Is this an okay thing to do? If not, what should I be doing?</li>
<li>Is there a way to calculate this ""overall"" precision/recall curve in R (e.g., using the ROCR package or some other library)? I'm currently using the ROCR package, but it seems to only give me the single-class-at-a-time precision/recall.</li>
</ol>
"
"0.0759737176397586","0.0539687072220866"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"NaN","NaN"," 37830","<p>I've been experimenting with the <code>rfe</code> function in the <code>caret</code> package to do logistic regression with feature selection. I used the <code>lmFuncs</code> functions with the following <code>rfeContol</code> :</p>

<p><code>ctrl &lt;- rfeControl(functions = lmFuncs,
                     method = 'cv',
                     rerank=TRUE,
                     saveDetails=TRUE,
                     verbose = TRUE,
                     returnResamp = ""all"",
                     number=100)</code></p>

<p>Below is the structure of the <code>rfe</code> call:</p>

<p><code>fit.rfe=rfe(df.preds,df.depend, metric='RMSE',sizes=c(5,10,15,20), rfeControl=ctrl)</code></p>

<p><code>df.preds</code> is a data frame of inputs to the model. <code>df.depend</code> is a vector of 1 or 0 corresponding to each row in <code>df.preds</code> to indicate response.</p>

<p>The resulting model accessed in from the <code>fit</code> object in the <code>rfe</code> object is of class <code>lm</code> and produces predicted values of less than zero and greater than 1 when I use the following code with the <code>predict</code> function:</p>

<p><code>predict(fit.rfe$fit,df,type='response')</code></p>

<p>Given I'm expecting this to be a logistic, all predicted values should greater than zero and less than one. </p>

<p>Any help will be appreciated.</p>
"
"0.0657951694959769","0.0311588476424878"," 38541","<p>I used the functions from this <a href=""http://www.math.mcmaster.ca/peter/s4f03/s4f03_0607/rochl.html"" rel=""nofollow"">link</a> for creating ROC curve for logistic regression model. Since the object produced by <code>glmer</code> in <code>lme4</code> package is a S4 object (as far as I know) and the function from the link cannot handle it.</p>

<p>I wonder if there are similar functions for creating ROC curve for multi-level logistic regression model in R.</p>
"
"0.182482967150453","0.11234482285936"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"NaN","NaN"," 40603","<p>I am running a logistic regression in R and am attempting to determine if multicollinearity is a problem with my model.<br>
 When I run <code>vif()</code> on my final model, I get <code>GVIF</code> and <code>GVIF^1/(2*Df)</code> columns. From what I have read <code>GVIF^1/(2*Df)</code> is what I should use to assess muticollinearity, but I have been unable to determine what values I should use as a cut-off point. </p>

<p>Any help would be greatly appreciated.</p>
"
"0.113960576459638","0.0539687072220866"," 40739","<p>The <code>bild</code> package appears to be an excellent package for serial binary responses.  But it is for discrete time.  I would like to specify a smooth function of time for the odds ratio connection of the current response Y with binary responses measured at earlier times, or at least a first-order Markov version of this.  I believe this is called alternating logistic regression.  Does anyone know of an R package that handles continuous time, i.e., measurement times can be at any follow-up time?  I don't need random effects in the model. </p>
"
"0.164234670435408","0.11234482285936"," 41390","<p>I am looking for a test similar to a 2-way ANOVA that would work on a binary response variable. My response variable is survival of plant seedlings (alive or dead).  My explanatory variables are Treatment (3 treatment groups) and Site (3 sites).  </p>

<p>First, I would like to know whether Treatment, Site and their interaction have a significant effect on survival.  Second, if either Treatment or Site is significant, I would like to test all pairs of treatment groups or sites to know which pairs of levels are significantly different, as I would normally do with an ANOVA.</p>

<p>I have considered several options:</p>

<ol>
<li><p>Transform the response variable, for example through an arcsin transformation, and then perform an ANOVA. This does not work on my data because at one of the sites I measure 100% survival.  Therefore there is 0 variability at this site and no transformation will change that.</p></li>
<li><p>Logistic regression with Treatment and Site recoded as dummy variables.  The results do not seem to give me a test of significance of Treatment, Site and interaction term -  Instead, I get the relative importance of each treatment group and each site separately.  Furthermore, it seems that I cannot test all the pairs of treatment groups or sites, I can only compare one ""baseline"" or ""default"" group to each of the two remaining groups.</p></li>
<li><p>Chi-square test on each explanatory variable separately.  This has the obvious drawback of not being able to test the interaction term.  Also I suspect that I am omitting important information if I am comparing survival across the 3 treatment groups without taking into account that this survival data is grouped in 3 different sites.  Does this bias the results?</p></li>
</ol>

<p>Can anyone recommend a different test or what the best approach would be in my case?</p>

<p><strong>UPDATE:</strong> Logistic regression can in fact give a test of significance of each independent variable.  In R, I discovered I can use glm to contruct a model and then the anova function to extract p-values for each IV:</p>

<pre><code>mymodel &lt;- glm(Survival ~ Treatment*Site, data=survivaldata, family=""binomial"")
anova(mymodel, test=""Chisq"")
</code></pre>
"
"0.0930484210398471","0.0440652649239232"," 41660","<p>I'm modeling a set of outcome data the depends on two parameters:</p>

<ol>
<li>time, T</li>
<li>-100 &lt; A &lt; 100</li>
</ol>

<p>I've done logistic regression using R with the command:</p>

<pre><code>model &lt;- glm(Outcome ~ A + T, family = ""binomial"", data = myData)
</code></pre>

<p>My expectation (the only thing that makes sense) is that when A &lt; 0, the fit probability should be an increasing function of time approaching 0.5, while when A > 0 it should be a decreasing function of time approaching 0.5.</p>

<p>However, the fit I get is that A &lt; 0, A > 0, and A = 0 all are increasing functions of time.  They in fact appear to be the same curve just shifted (ie same ""shape"").</p>

<p>What am I doing incorrectly?  Any suggestions?</p>
"
"0.109658615826628","0.0830902603799674"," 41670","<p>I have a dataset in which each row belongs to one of 8 categories. I'm running a logistic regrssion on it using R. I created dummy variables for each of these categories. In my logistic regression model I know one of these dummies need to be excluded to not fall for the dummy variable trap. However the model keeps kicking out the same dummy-variable, even if I exclude another. I thought, maybe this dummy is kicked out due to other variables (other than the other dummy variables), but after using only the category-dummies to estimate the model, that particular variable still results in NA. </p>

<p>Why is this? Am I missing something?</p>

<p>Example: (I've removed all other variables and just included the first 100 rows of the dependent variable + the category dummies)</p>

<pre><code> structure(list(buy = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), cat1 = c(0L, 1L, 0L, 
1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 
0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L), cat2 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), cat3 = c(1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
cat4 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L), cat5 = c(0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), cat6 = c(0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L), cat7 = c(0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L), cat8 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L)), .Names = c(""buy"", ""cat1"", ""cat2"", ""cat3"", ""cat4"", 
""cat5"", ""cat6"", ""cat7"", ""cat8""), row.names = c(NA, 100L), class = ""data.frame"")
</code></pre>

<p>For the logistic regression I use:</p>

<pre><code>mylogit &lt;- glm(formula= buy ~ cat1 + cat2 + cat4 + cat5 + cat6 + cat7 + cat8, family = binomial, data=df)
</code></pre>

<p>As shown in the output of this model, cat7 is NA</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -2.6626     0.5972  -4.459 8.24e-06 ***
cat1          -17.9035  2769.0186  -0.006    0.995    
cat2          -17.9035 17730.3699  -0.001    0.999    
cat4            1.0531     1.2476   0.844    0.399    
cat5          -17.9035 17730.3699  -0.001    0.999    
cat6          -17.9035  8865.1850  -0.002    0.998    
cat7                NA         NA      NA       NA    
cat8          -17.9035 17730.3699  -0.001    0.999
</code></pre>

<p>However, 1 of the dummies should be excluded. If I exclude for instance cat 6 by leaving it out of the equation or by using '- cat6' the results for cat7 are still NA. </p>
"
"0.264274989057364","0.164975235956687"," 43040","<p>I need some guidance related to regression model verification using validation data. 
I am new to R-tool &amp; statistics and trying my best to learn. I did search on internet too but I couldn't get a final answer to my questions. 
Actually I have a lot of questions, I may try my best to explain the problems:
I am experimenting with network packets and R-tool.
I have captured some packets from a network using a custom made packet sniffer in java. The sniffer will capture some packets and save the information of packet header like: tcp window size, tcp sequence numbers, date-time, ip header length, ip time to live etc... in a csv file.</p>

<p>Also the sniffer will add category number to each csv file so that we can know which packet belongs to which category. I created 9 different categories saved in 9 different csv files.
Now I extracted 1000 observation from each of the csv files and created a data set named ""alldata"".</p>

<p>Then I created training data set and validation data set from ""alldata"" data set.</p>

<p>Now I want to perform linear regression, logistic regression, decision tree analysis, cluster analysis etc on this ""alldata"" data set.</p>

<p>So my plan was to use training data set to create models and then later use validation data set to verify my models. </p>

<p>Category will be my target variable in any case. I want to predict the category from other independent variables.</p>

<ol>
<li><p>My first confusion is that after I created scatter plot of category with other independent variables and I don't see any linear relationship between them. Moreover I even don't know what relation exists between category and independent variables. From scatter plots it seems to me that there is no specific relation between category and other independent variables(except date_time it is bit linear to category). Am I doing the correct interpretation ?
Here are some of the plots:
<a href=""http://imageshack.us/photo/my-images/211/tcpdport.png/"" rel=""nofollow"">plot 1</a>
<a href=""http://imageshack.us/photo/my-images/547/tcpchksum.png/"" rel=""nofollow"">plot 2</a></p></li>
<li><p>I think doing linear regression won't make any sense now after having a look at scatter plots. Is this correct assumption?</p></li>
<li><p>Although I tried to do make some regression models with training data set, but the R-square values for all the models is quite low (for example like 0.00019, 0.0035, 0.018 etc. ) 
So can I assume that these models are not good due to very low r-square vales?</p></li>
<li><p>As logistic regression is used when we have target variables having only two values 1 or 0, or some probabilities between 0.0 - 1.0.
This means performing logistic regression is not possible for this type of data set.
Is my assumption true?</p></li>
<li><p>My main question was how to verify a model created with training data set by using validation data set?
Please let me know the commands and the procedure.
Please let me know if I am doing this in wrong way or if you can suggest me a better way to do this whole work. I think if someone could please clear my doubts then I may ask further more questions.</p></li>
</ol>

<p>If you don't understand my problem we can discuss in more detail
I look forward for your replies.
Thank you!</p>

<hr>

<p>@Wayne</p>

<p>Hello thanks for the reply, but the thing is for each category I have almost same range of values of independent variables like(tcpheader, ipttl, iplen). For example iptype is only having two values 6 and 17. So most of the categories are having iptype value of 6 &amp; 17.
So it is also same is for tcpheader, tcp sequence number, tcp acknowledgement number etc. I don't think there is any way to distinguish a particular packet based on these independent variables. Only the independent variable that can be helpful is time.
But when I created a model with time it had good r-squared value but the regression line equation doesn't predict category with any value of date_time.
I don't understand this behaviour.</p>

<p>Thanks.</p>
"
"0.0882734829504749","0.0696733014291618"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0","0"," 43551","<p>I am running a multinomial logistic regression using the <code>mlogit</code> package and <code>mlogit</code> function in R.  Now I need to check for outliers for the model.</p>

<p>Is there any approach or function in R for testing outliers in an <code>mlogit</code> model?</p>
"
"NaN","NaN"," 43785","<p>If I have a set of continuous predictors $X$ and a binary outcome $Y$ and I wanted to build a predictive model of $P(Y|X)$, I would start with a logistic regression model.</p>

<p>However, in my particular case, my $Y$ isn't binary, it's continuous between 0 and 1.  Is there a similar Generalized Linear Model that can be applied in this case?  My optimistic/naive attempt in R reveals that </p>

<pre><code>set.seed(123)
df &lt;- data.frame(y=runif(8), x1=rnorm(8), x2=rnorm(8))
mod &lt;- glm(y ~ ., data=df, family=binomial('logit'))

# Warning message:
# In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!

rbind(yhat=predict(mod, newdata=df), y=df$y)
#              1         2          3         4         5          6         7           8
# yhat 0.7461449 0.4869853 -0.1092115 1.9854276 0.8328304 -1.3708688 1.0150934 -0.03496334
# y    0.2875775 0.7883051  0.4089769 0.8830174 0.9404673  0.0455565 0.5281055  0.89241904
</code></pre>

<p>Note that some of the predictions are outside of $(0,1)$.  Any suggestions?</p>
"
"0.107443061870051","0.0763232776972177"," 44359","<p>So I have data from a randomized blind trial of 1mg of nicotine gum on dual n-back working memory scores; I analyzed them as usual with a t-test and found a small increase in means but a large increase in standard deviations on a f-test! Strange. I also have data for each day on mood/productivity that day on a 1-5 scale.</p>

<p>I wondered: is nicotine following an inverse U-curve, where it causes higher scores on the worser days (1-3) and lower scores on the better days (3-5)? I look around and it seems I want a multinomial logistic regression comparing the placebo &amp; active days.</p>

<p>I enter the data &amp; load <code>mlogit</code>:</p>

<pre><code>nicotine &lt;- read.table(stdin(),header=TRUE)
day      active mp score
20120824 1      3  35.2
20120827 0      5  37.2
20120828 0      3  37.6
20120830 1      3  37.75
20120831 1      2  37.75
20120902 0      2  36.0
20120905 0      5  36.0
20120906 1      5  37.25
20120910 0      5  49.2
20120911 1      3  36.8
20120912 0      3  44.6
20120913 0      5  38.4
20120915 0      5  43.8
20120916 0      2  39.6
20120918 0      3  49.6
20120919 0      4  38.4
20120923 0      5  36.2
20120924 0      5  45.4
20120925 1      3  43.8
20120926 0      4  36.4
20120929 1      3  43.8
20120930 1      3  36.0
20121001 1      3  46.0
20121002 0      4  45.0
20121008 0      2  34.6
20121009 1      3  45.2
20121012 0      5  37.8
20121013 0      4  37.2
20121016 0      4  40.2
20121020 1      3  39.0
20121021 0      3  41.2
20121022 0      3  42.2
20121024 0      5  40.4
20121029 1      2  41.4
20121031 1      3  38.4
20121101 1      5  43.8
20121102 0      3  48.2
20121103 1      5  40.6

library(mlogit)
Nicotine &lt;- mlogit.data(nicotine,shape=""wide"", choice=""mp"")
mlogit(score ~ (active + mp)^2, Nicotine)
Error in solve.default(H, g[!fixed]) : 
  Lapack routine dgesv: system is exactly singular
Calls: mlogit ... mlogit.optim -&gt; as.vector -&gt; solve -&gt; solve.default
</code></pre>

<p>The error also happens even with the simplest call I can think of:</p>

<pre><code>mlogit(score ~ active, Nicotine)
Error in solve.default(H, g[!fixed]) : 
  Lapack routine dgesv: system is exactly singular
Calls: mlogit ... mlogit.optim -&gt; as.vector -&gt; solve -&gt; solve.default
</code></pre>

<p>Reading the documentation for <code>mlogit</code> didn't much help, and look at the other questions having the same error, they're different enough I can't tell whether they apply or not.</p>

<p>Thank you for your assistance.</p>
"
"0.0465242105199235","0.0220326324619616"," 44780","<p>I have some data similar to this</p>

<pre><code>died  age  hospital
0     75     AA
0     88     AA
1     81     AA
0     77     AA
1     65     AA
0     41     AA
0     66     BA
1     81     BA
0     82     BA
1     64     BA
0     65     BA
1     52     BA
</code></pre>

<p>I was asked to calculate ""age adjusted mortality rates"" for each hospital. There are around 150 hospitals and approx 1000 patients (observations) per hospital. Each row in the data concerns a particular patient.</p>

<p>I was told how this could be done in Stata:</p>

<ul>
<li>Perform logistic regression of <code>died</code> on <code>age</code>.</li>
<li>Use the <code>predict</code> function to get patient-level probabilities of death.</li>
<li>Summarise the patient-level probabilities by hospital to get the mortality rates for each hospital.</li>
</ul>

<p>However, I am using R.</p>

<p>Is this the correct approach ? Are the alternatives ? Can I do the same thing in R with <code>glm</code> and <code>predict</code> ? </p>

<p><strong>Edit:</strong></p>

<p>I should perhaps add that there are several other variables that are going to be adjusted for in the model. I have shown age above, just for simplicity.</p>
"
"0.05884898863365","0.0557386411433294"," 44998","<p>I am trying to perform multinomial logistic regression on my data which is as below(just the header).
<img src=""http://i.stack.imgur.com/LLZjK.jpg"" alt=""enter image description here""></p>

<p>""category"" is my target variable and all other variables are independent variables. category has values like 1,2,3,4,5,6,7,8,9. My main motive is to predict category from independent variables.
Here is summary of my data set.
<img src=""http://i.stack.imgur.com/hXuNg.jpg"" alt=""summary""></p>

<p>I used following command to perform multinomial logistic regression:</p>

<pre><code>&gt; mod=multinom(category~hlen+iplen+ipttl+iptype+tcpsport+tcpdport+tcpsec+tcpack+tcpwindow+tcpchksum+date_time, data=train)
</code></pre>

<p><img src=""http://i.stack.imgur.com/bZMWw.jpg"" alt=""command""></p>

<p>I got the following output but i don't know how to interpret it? 
<img src=""http://i.stack.imgur.com/c2QQR.jpg"" alt=""enter image description here"">
What should be the starting point or is there other way to do so? (For example I know how to interpret linear and logistic regression output to create regression equations)</p>

<p>Please right click any image and select view to see it clearly.
Thank you,</p>
"
"0.116310526299809","0.0881305298478463"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.0930484210398471","0.0440652649239232"," 45723","<p>I am a student in biology, currently finishing my master in Behavior, evolution and conservation. I have been in the Swiss national park for my field work and I have some trouble to analyze my data.</p>

<p>So basically what I have is a lot of factors and what I need to do is a multinomial logistic regression :</p>

<p>Factors :</p>

<ul>
<li>Behaviour (4 states - Moving, Feeding, Resting, Runing)</li>
<li>Age (Of the individual)</li>
<li>Temperature</li>
<li>Valley (where the chamois was, 2 different choices)</li>
<li>Year (of the observation)</li>
<li>Month (of the observation)</li>
<li>Kid (if it has a kid or not)</li>
<li>Individual (The number written on the tag he had on his ear)</li>
</ul>

<p>What I want to do is to check what factors influence the Behavior.</p>

<p>I tried several things with R but can't use the <code>mlogit</code> package which is the one I have been told to use.</p>

<p>I have also tried in JMP, now the problem I have here is that I just clicked on ""Fit Y in function of X"" and selected my response variable and my factors and bam, I had results, but to be honest this seems very simple and I was wondering if I am not missing something somewhere.</p>

<p>Edit : Here is what R returns when I try the <code>mlogit</code> function :</p>

<pre><code>mlogit(Behavior~Age+Temp+Valley+Individual+Year+Month, Merge)
Error in `row.names&lt;-.data.frame`(`*tmp*`, value = value) : 
  invalid 'row.names' length
</code></pre>

<p>Can anyone here that can help me either with R or with JMP?</p>
"
"0.0465242105199235","0.0220326324619616"," 45754","<p>I have the following output from a logistic regression model.</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode10000:14849  0.0019334  0.0009006   2.147 0.031807 *  
our_bid:zipcode14850:19699  0.0022905  0.0009514   2.407 0.016064 *  
our_bid:zipcode19700:29999 -0.0009483  0.0008583  -1.105 0.269231    
our_bid:zipcode30000:31999 -0.0016309  0.0011028  -1.479 0.139161    
our_bid:zipcode32000:34999  0.0016241  0.0007856   2.067 0.038688 *  
our_bid:zipcode35000:42999  0.0023549  0.0008541   2.757 0.005831 ** 
our_bid:zipcode43000:49999  0.0007096  0.0008104   0.876 0.381286    
our_bid:zipcode50000:59999  0.0006533  0.0009269   0.705 0.480942    
our_bid:zipcode60000:69999  0.0030564  0.0008169   3.742 0.000183 ***
our_bid:zipcode7000:9999   -0.0027419  0.0012699  -2.159 0.030847 *  
our_bid:zipcode70000:79999  0.0013243  0.0007809   1.696 0.089921 .  
our_bid:zipcode80000:89999  0.0038726  0.0008006   4.837 1.32e-06 ***
our_bid:zipcode90000:96999  0.0038746  0.0007817   4.957 7.18e-07 ***
our_bid:zipcode97000:99820  0.0009085  0.0010044   0.905 0.365726    
---
</code></pre>

<p>I am using these coefficients to draw the predicted probabilities such that.</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>I realize that interpreting these interaction terms can be challenging. However, I generate the main regression equation and use that to formulate the probability curve. However, I'm not sure how to make sense of any of the ""our_bid:zipcode"" variables? </p>

<p>What about if my model output was: (instead saving zipcode as a factor, I make it a continuous variable)</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode             0.0019334  0.0009006   2.147 0.031807 *  
</code></pre>

<p>Would interpretation being easier with this approach? Keeping with the log-odds, how can I make sense of the log-odds effect that this model expresses for the interaction term?</p>
"
"0.0882734829504749","0.0557386411433294"," 46205","<p>I have a large amount of vegetation data that has been broken down into 13 habitat classes. I am trying to determine which vegetation tends to fall into or is absent from which habitat with any sort of significance. I have been put onto running a multinomial logistic regression, specifically using glmnet (as I have approximately 200 variables, and only about 260 observations).</p>

<p>Running cv.glmnet using the code:</p>

<pre><code>cv&lt;-cv.glmnet(data,Class,family=""multinomial"",nfolds=50,standardize=FALSE)
</code></pre>

<p>I get a list of numbers that I am struggling to understand, however I found the code:</p>

<pre><code>coef(cv, s=cv$lambda.1se)
</code></pre>

<p>Which returns the coefficients for each variable for each habitat class for the lambda that is 1 SE larger than the minimum Lambda value (which as far as I can tell the generally accepted lambda value).</p>

<pre><code>(Intercept)                                              0.7914263664   
Salix                                                    0.0000000000  
Mash                                                     0.0000000000   
Pin                                                      0.0000000000   
Choke                                                    .          
Betula                                                   0.0025260258   
Ideae                                                    0.0000000000   
Leather                                                  0.0000000000
</code></pre>

<p>What I'm wondering, using these coefficients, is it possible to state that those values with the largest magnitude (either closest to -1 and +1) are the most important in defining that class, which those close to 0 are unimportant, and those with periods were removed during the cv.glmnet. So in this case the plant ""Betula"" would be more influential than all others, and ""Choke"" was so uninfluential that it was removed? Also, no idea what intercept means, but I imagine I can find that one on my own.</p>
"
"0.166038918045633","0.135818268070785"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.0930484210398471","0.0440652649239232"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"NaN","NaN"," 46661","<p>I'm using the R package <a href=""http://cran.r-project.org/web/packages/ltm/index.html"" rel=""nofollow"">ltm</a> to create a 2-parameter logistic regression.</p>

<p>The input matrix is sparse - many users have taken a small subset of the items in the item bank.</p>

<p>For some of my data sets i'm running into this error:</p>

<pre><code>Error in if (any(ind &lt;- pr == 0)) pr[ind] &lt;- sqrt(.Machine$double.eps) : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Not sure what the issue is.  Doesn't repro on most of my data sets.  Any thoughts?</p>
"
"0.132954005869578","0.0989426299071587"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.0930484210398471","0.0440652649239232"," 47306","<p>I have a dataset with one binary target variable called â€œtargetâ€ and many many factors â€œF1â€, F2â€â€¦ â€œF200â€. Iâ€™m trying to come up with code to fit 200 single factor logistic regression models and return the model summaries. Here is what I have so far</p>

<pre><code>single &lt;- function(df, factor){
    s &lt;- summary(glm(as.formula(paste('target ~ ', factor, sep='')), family=binomial, data=df));
return(s);
}
single(data, c(""F1"", ""F2"", ""F2""));
</code></pre>

<p>but this gives me only a summary of the first model. Am I missing something obvious here?</p>
"
"0.113960576459638","0.0539687072220866"," 47348","<p>I am trying to run a logistic regression in R on my data where my independent variables are 13 continuous variables and my dependent variable is binary.  I want to segment my data so that I train on the first 80% and test on the last 20%.  I have a total of 3750 rows of data so I utilize the first 3000 for training.  I have written the following:  </p>

<pre><code>mydata&lt;-totaldata[1:3000,2:15]
mylogit&lt;-glm(mydata$TARGET ~ mydata$VAR1+mydata$VAR2+mydata$VAR3+mydata$VAR4+ #$
                             mydata$VAR5+mydata$VAR6+mydata$VAR7+mydata$VAR8+
                             mydata$VAR9+mydata$VAR10+mydata$VAR11+mydata$VAR12+
                             mydata$VAR13, family=""binomial"")

predictdata=totaldata[3001:3751,3:15]
in_frame&lt;-data.frame(predictdata)
predictions=predict(mylogit,in_frame,type=""response"")
</code></pre>

<p>However I get the following warning message: 
Warning message:
'newdata' had 751 rows but variable(s) found have 3000 rows </p>

<p>Then when I look at predictions there are 3000 predictions not the 751 that I wanted.  What can I do to fix this?</p>
"
"0.0759737176397586","0.0539687072220866"," 47795","<p>I am currently carrying out an investigation to find if certain factors such as playing home or away or position of a footballer affects overall pass completion using logistic regression. I am using R to compute my data. In my current section in which I am trying to analyse uses the data of every player to convey a general conclusion to whether or not the position of a player affects the successfulness of pass completion. </p>

<p>so far I have computed:</p>

<pre><code>test.logit &lt;- glm( cbind(Total.Successful.Passes.All,Total.Unsuccessful.Passes.All) ~
                   as.factor(Position.Id), data=passes.data, family = ""binomial"")

summary(test.logit)
</code></pre>

<p>and my output was:</p>

<pre><code>Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    

(Intercept)              0.28482    0.01256   22.67   &lt;2e-16 

as.factor(Position.Id)2  0.99768    0.01438   69.38   &lt;2e-16 

as.factor(Position.Id)4  1.06679    0.01398   76.29   &lt;2e-16 

as.factor(Position.Id)6  0.68090    0.01652   41.23   &lt;2e-16 

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 32638  on 10269  degrees of freedom
Residual deviance: 26499  on 10266  degrees of freedom

AIC: 60422

Number of Fisher Scoring iterations: 4
</code></pre>

<p>the intercept is goalkeepers,position.Id 2 is for a defender, 4 = midfielder and 6 = striker</p>

<p>Is this a good set of results to come to a conclusion? and with the large deviances?</p>
"
"0.0657951694959769","0"," 47802","<p>This isn't as easy to Google as some other things as, to be clear, I'm not talking about logistic regression in the sense of using regression to predict categorical variables.</p>

<p>I'm talking about fitting a logistic growth curve to given data points. To be specific, $x$ is a given year from 1958 to 2012 and $y$ is the estimated global CO2 ppm (parts per million of carbon dioxide) in November of year $x$.</p>

<p>Right now it's accelerating but it's got to level off at some point. So I want a logistic curve.</p>

<p>I haven't found a relatively straightforward way to do this yet.</p>
"
"NaN","NaN"," 48243","<p>Is there any function in R for doing variable selection (backward elimination) in a multiple logistic regression using restricted cubic splines like <strong>mvrs</strong> procedure for STATA?</p>
"
"0.164487923739942","0.116845678659329"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.151947435279517","0.107937414444173"," 48410","<p>I'm modeling the effect of a categorical predictor on a binary dependent variable using logistic regression. I'm comparing models with/without the predictor using a likelihood-ratio test.</p>

<p>Two categories of the predictor are associated with values of 1 only (no 0s) for the dependent variable. Regression coefficients for these categories (expressed as changes in log(odds) compared to a reference category) are very large and highly suspicious, as this reference category is always associated with response values of 1 (but for one case), and I would thus expect regression coefficients close to 0 for these two categories. Comparisons between the reference category and other categories having more balanced distribution of 1 and 0s matches what I'm expecting from visual inspection of the data. 
Removing cases associated with these two 'problematic' categories does not change the logLikelihood of the models, but because it changes the number of parameters it affects the results of the likelihood ratio test.</p>

<p>Models are fitted using the glm function with binomial family and logit link in R.</p>

<p>My question therefore is: what model (or procedure) should I use to: </p>

<p>(1) test the global significance of the effect of the predictor on the dependent variable? Should I keep data from the 'problematic' categories in the model or not before conducted the likelihood ratio test?</p>

<p>(2) compare these two 'problematic' categories with others?</p>

<p>Any hint appreciated,</p>
"
"0.192069377013354","0.142935514895074"," 48415","<p>I come to you today because I face a huge problem that I cannot explain.</p>

<p>I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing</p>

<pre><code>    mlogit &lt;- mlogit.data(Merge, choice = ""Choice"", shape = ""long"", alt.var = ""Comp"", 
                          drop.index = TRUE)
</code></pre>

<p>on my Merge data.</p>

<p>which gives me the following:</p>

<pre><code>                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice
    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE
    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE
    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE
    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE
    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE
    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
</code></pre>

<p>then I ran my regression :</p>

<pre><code>m1 &lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)
</code></pre>

<p>and it gave me significant results :</p>

<pre><code>                          Estimate  Std. Error  t-value  Pr(&gt;|t|)    
    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***
    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &lt; 2.2e-16 ***
    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &lt; 2.2e-16 ***
    M:Temp              1.5371e-02  9.8680e-04  15.5764 &lt; 2.2e-16 ***
    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &lt; 2.2e-16 ***
    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***
    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &lt; 2.2e-16 ***
    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***
    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &lt; 2.2e-16 ***
    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***
    R:Age               1.9465e-02  1.6479e-03  11.8119 &lt; 2.2e-16 ***
    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***
    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** 
    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &lt; 2.2e-16 ***
    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***
    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &lt; 2.2e-16 ***
    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***
    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &lt; 2.2e-16 ***
</code></pre>

<p>those results were in line with what I expected to find in literature so I was quite happy.</p>

<p>My next step was to plot my results and here is when I have some trouble.</p>

<p>First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.</p>

<p>I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.</p>

<p>So I wanted to use the <code>predict()</code> function but I don't know how to do that. I was hoping to get some help here.</p>
"
"0.0465242105199235","0.0440652649239232"," 48651","<p>I am looking for a Least Angle Regression (LAR) packages in R or MATLAB which can be used for <strong>classification</strong> problems.</p>

<p>The only package that I currently know which fits this description is <a href=""http://cran.r-project.org/web/packages/glmpath/index.html"" rel=""nofollow"">glmpath</a>. The issue with this package is that it is a little old and somewhat limited in its scope (I am forced to rely on logistic regression for classification problems model).</p>

<p>I am wondering if anyone knows of other packages that allow me to run LAR on different types of classification models, such as Support Vector Machines (see <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.391&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">The Entire Regularization Path for the Support Vector Machine</a>). </p>

<p>The ideal package would allow me to run LAR-type algorithms for different types of classification models and also provide a function that can produce the full regularization path. </p>
"
"0.1176979772673","0.0557386411433294"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.197385508487931","0.0934765429274634"," 49497","<p>I have a dataset I'm working on that has some co-variate shift between the training set and the test set.  I'm trying to build a predictive model to predict an outcome, using the training set.  So far my best model is a random forest.</p>

<p>How can I deal with the shifted distributions in the training vs. test set?  I've come across 2 possible solutions that I've been able to implement myself:</p>

<ol>
<li>Remove the shifted variables.  This is sub-optimal, but helps prevent my model from over fitting the training set.</li>
<li>Use a logistic regression to predict whether a given observation is from the test set (after balancing the classes), predict ""test set probabilities"" for the training set, and then boostrap sample the training set, using the probabilities for sampling.  Then fit the final model on the new training set.</li>
</ol>

<p>Both 1 and 2 are pretty easy to implement, but neither one satisfies me, as #1 omits variables that might be relevant, and #2 uses a logistic regression, when my final model is tree-based.  Furthermore, #2 takes a few paragraphs of custom code, and I worry that my implementation may not be correct.</p>

<p>What are the standard methods for dealing with covariate shift?  Are there any packages in R (or another language) that implement these methods?</p>

<p>/edit: It seems like ""kernel mean matching"" is another approach I could take.  I've found lots of academic papers on the subject, but no one seems to have published any code.  I'm going to try to implement this on my own, and will post the code as an answer to this question when I do.</p>
"
"0.0986927542439653","0.0623176952849756"," 49549","<p>I am attempting some variable reduction before I perform a logistic regression.  I am quite interested in using <code>Hmisc::varclus</code> in R.  However, I am having some difficulty interpreting the output.  As far as I can tell, the (tree) plot produced using <code>varclus</code> is the only built-in way to get information on the groups created by the procedure.  </p>

<p>My main question involves knowing at what level of the hierarchy to select the clusters to be used for variable reduction?  I read that of a rule-of-thumb to keep a cluster is if its rho (for Spearman's) is at least 0.30.  Would this be evaluated visually from the plot?  At what value of rho would the cut-off be made to separate the tree into <em>final</em> clusters? Again, is this to be done visually?   </p>

<p>Perhaps my internet searching skills are lacking, but I am having difficulty finding information on this procedure in general and more specifically in R. Is there a good <em>beginner's</em> article on variable clustering that I am missing that spells out the fundamentals?  Are there additional commands for <code>varclus</code> in R to help with final cluster decisions besides examining the tree visually?</p>

<pre><code># varclust example in R using mtcars data
mtcn &lt;- data.matrix(mtcars)
clust &lt;- varclus(mtcn)
clust
plot(clust)
</code></pre>
"
"0.0657951694959769","0.0311588476424878"," 49714","<p>I have recently begun to read about bayesian statistics and I am playing around with the R2WinBUGS package. I'm trying to fit a logistic regression to the spam data (that can be found on the webpage of the elements of statistical learning) using R and WinBUGS. My approach was to first divide the data into 80% training and 20% testing sets. I can fit the model using the 80% set but I dont know how to write WinBugs code to predict on new observations (say the 20% test set) and I wonder if this approach to study model/classification precisicion make sense in a Bayesian Approach?</p>
"
"NaN","NaN"," 49916","<p>I was trying to create some test data for logistic regression and I found this post <a href=""http://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression"">How to simulate artificial data for logistic regression?</a></p>

<p>It is a nice answer but it creates only continuous variables. What about a categorical variable x3 with 5 levels (A B C D E) associated with y for the same example as in the link?</p>
"
"0.0465242105199235","0.0440652649239232"," 51046","<p>One way to find accuracy of the logistic regression model using 'glm' is to find AUC plot. How to check the same for regression model found with continuous response variable (family = 'gaussian')?</p>

<p>What methods are used to check how well does my regression model fit the data?</p>
"
"0.116310526299809","0.0881305298478463"," 51152","<p>I've been trying to use the fastbw function from the rms package in R to perform logistic regression with backward selection, with p-values as exclusion criterion (I am well aware of the arguments against using p-values for this as opposed to e.g. AIC). However, the results are not in agreement with what I would get if I perform the backward selection manually, as fastbw often drops more factors in comparison. The results also seem to depend on the number of factors considered, even with the option </p>

<pre><code>type=""individual"".
</code></pre>

<p>I created some simple example data in order to prove my point, which give the following result:</p>

<pre><code>&gt; fastbw(lrm(y~x1+x2+x3+x4),rule=""p"",type=""individual"")

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x3      0.37   1    0.5412 0.37     1    0.5412 -1.63
 x1      1.82   1    0.1771 2.20     2    0.3336 -1.80
 x4      2.58   1    0.1082 4.78     3    0.1889 -1.22
 x2      3.56   1    0.0591 8.34     4    0.0799  0.34

[...]

Factors in Final Model

None
</code></pre>

<p>I.e., x2 is dropped as the last of the factors considered, resulting in a model without factors. However, if I consider x2 only, I get the following result. </p>

<pre><code>&gt; fastbw(lrm(y~x2),rule=""p"",type=""individual"")

No Factors Deleted

Factors in Final Model

[1] x2
</code></pre>

<p>The same is true if I do the backward selection manually, as x2 considered separately has a p-value of 0.045. What might cause this behavior? Since x2 is the last remaining variable in the backward selection, the results shouldn't depend on associations with other model covariates.</p>
"
"0.0930484210398471","0.0440652649239232"," 51412","<p>I'm performing a multiple logistic regression with a large amount of columns. I want to set all the variables over a certain p value to have 0 coefficients, then test the model on the test data. </p>

<p>Since there's so many variables I'm using, it's impractical to manually go in and set the coefficients. So I'm looking for something like the following.</p>

<pre><code>log_results &lt;- glm(formula, data, family);
log_results_sig &lt;- get_sig_only(log_results, p value threshold);
</code></pre>

<p>This way I can use the results with the predict() function, and export the results easily for use in another program. </p>

<p>Additionally, if anyone knows a way to automatically extract the significant variables and refit with them, I would appreciate that too.</p>

<p>Thanks.</p>
"
"0.153522062157279","0.0934765429274634"," 51464","<p>Background: For a project, I am fitting a conditional logit model where I have 5 control cases for every realized case. To do that I use the <code>clogit()</code> function in the package <code>survival</code>. I wanted to graph interactions with the <code>effects</code> package by John Fox et al. It turns out that this package can't handle <code>clogit</code> objects (output of <code>clogit()</code>). </p>

<p>As I believed I remembered that conditional logit were a special case of GLM, I thought the clever/lazy way to get my interaction plots would be to refit the model using a fixed effects glm and then use <code>effect()</code>.
The documentation of <code>clogit</code> seemed to confirm my intuition:</p>

<blockquote>
  <p>It turns out that the logliklihood for a conditional logistic regresson model = loglik from a Cox model with a particular data structure. [...]
  When a well tested Cox model routine is available many packages use this â€˜trickâ€™ rather than writing a new software routine from scratch, and this is what the clogit routine does. </p>
  
  <p>In detail, a stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph.</p>
</blockquote>

<p>Based on this description, it seems that I should be able to reproduce the stratification achieved through <code>strata()</code> by using a random intercept for each case/control group with <code>1|group</code> in <code>lmer()</code>. However, when I try, the results of <code>clogit</code> and <code>lmer</code> differ. One thing is that I probably have the wrong likelihood function. I don't really know how to specify this in <code>lmer</code> but more important, I am wondering what else I am missing. </p>

<p>I wonder whether I am completely wrong or somewhat on the right track but missing some pieces? What I would like is to understand what are the difference in terms of how the model is fitted between a conditional logit and a regular one (I understand that might be quite a long answer, so a book reference would be a great start). The my usual references for regression (Gelman and Hill, 2007; Mills 2011) are somewhat silent on the subject.</p>
"
"0.0379868588198793","0.0539687072220866"," 51786","<p>Does anyone know what exact data cleaning steps one need to undertake in order to clean data for a logit regression (not a logistic regression)?</p>

<p>I have only time variables, meaning year and month, as my independent variables, and I am using R.</p>

<p>A logit regression is simply a normal linear regression where the DV have been transformed with the following formula:</p>

<blockquote>
  <p><code>logit(y) = ln(y/(1-y)</code> for </p>
</blockquote>

<p>An example:</p>

<blockquote>
  <p>3 of 12 people gets cured from taking a pill in period 3 ->
  <code>ln(0.25/(1-0.25)</code></p>
  
  <p>5 of 25 people gets cured taking a pill in period 5 ->
  <code>ln(0.20/(1-0.20)</code></p>
</blockquote>

<p>One can use the logit transformation if you have ratios and in many papers and books it is closely related to the logistic regression.</p>
"
"0.200731263865498","0.11234482285936"," 52206","<p>I have a data set which consists of binomial proportions, let's say the success rate of converting a customer depending on the advertisement, the customer age, and various other factors.</p>

<p>For some common combinations of covariates, I have a lot of data, and therefore the binomial proportion of successes has low variance. For rare combinations of covariates, however, I have very little data, and therefore the variance of the proportion is high.</p>

<p>The magnitude of differences is very large, for example I might have 1 million trials for some combinations of covariates, and only 50 for others. However, I want to include ALL data in my model and weight it appropriately to get the best model fit.</p>

<p>I've tried to use R to do binomial (logistic) regression using a generalized linear model:</p>

<pre><code>lrfit &lt;- glm ( cbind(converted,not_converted) ~ advertisement + age, family = binomial)
</code></pre>

<p>This is a good start because it automatically weights the observations by the number of trials.</p>

<p>However, it's not good enough. Here's why: Let's say you have some observations with 100,000 trials and others with 1,000,000 trials. If you weight by number of trials the latter group is going to receive 10 times the weight. This seems nonsensical, however, because both observations are easily precise enough to receive equal treatment in the model. Clearly you want to penalize groups with only 10 or 100 trials, but as the number of trials gets larger, the weight should stop increasing.</p>

<p>Since in weighted least squares the reciprocal of the error variance is used as the weight, my idea would be to use calculate the posterior variance of the proportion (using Jeffrey's prior), then add some constant term to it (this will make sure the variance stops increasing at a certain number of trials) and then use the reciprocal of the sum as the weight.</p>

<p>Is this approach reasonable? Am I missing something? Can someone give me more information about this method?</p>
"
"0.147122471584125","0.0696733014291618"," 52475","<p>I have been running some binomial logistic regressions in R on a data set and I realised that the p-values of the estimated coefficients are not computed based upon a Normal distribution. For e.g. I have the following result from the glm() function:</p>

<pre><code>    Coefficients:

                                   Estimate Std. Error z value Pr(&gt;|z|) 
    (Intercept)                    -1.6127     0.1124 -14.347  &lt; 2e-16 ***
    relevel(Sex, ""M"")F             -0.3126     0.1410  -2.216   0.0267 *  
    relevel(Feed, ""Bottle"")Both    -0.1725     0.2056  -0.839   0.4013    
    relevel(Feed, ""Bottle"")Breast  -0.6693     0.1530  -4.374 1.22e-05 ***
</code></pre>

<p>I previously thought that the beta hats (estimated coefficients) are asymptotically normal and the p-value should be calculated using the normal distribution. But it seems from the p-values here that a t-distribution is used to compute them (I think). </p>

<p>I know that the asymptotic condition does not hold when sample size is small but if so, what is the process that causes the estimator to be distributed with a Students distribution? And how do I find out the degrees of freedom for such a distribution?</p>
"
"0.0657951694959769","0.0311588476424878"," 52785","<p>I'm only a linguist, so my knowledge of statistics is very basic.</p>

<p>I fitted a logistic regression model with R (with <code>lrm(formula, y=T, x=T)</code>), and when I use the option <code>validate(lrm)</code>, I get some statistics I don't really understand.</p>

<pre><code>index.orig   training   test optimism index.corrected     n
Dxy           0.5984   0.6112  0.5461   0.0651          0.5333 40
R2            0.3258   0.3676  0.2929   0.0747          0.2511 40
Intercept     0.0000   0.0000 -0.0105   0.0105         -0.0105 40
Slope         1.0000   1.0000  0.8427   0.1573          0.8427 40
Emax          0.0000   0.0000  0.0399   0.0399          0.0399 40
D             0.2713   0.3176  0.2394   0.0782          0.1931 40
U            -0.0177  -0.0177  0.0092  -0.0269          0.0092 40
Q             0.2890   0.3353  0.2302   0.1051          0.1839 40
B             0.1864   0.1772  0.1972  -0.0201          0.2064 40
g             1.4632   1.6642  1.3460   0.3182          1.1449 40
gp            0.2840   0.3011  0.2703   0.0308          0.2532 40
</code></pre>

<p>I don't really understand most of that. I think <code>R2</code> and <code>Dxy</code> are supposed to be statistics of how good the predictors are, but I'm not sure how I should interpret the values, does the corrected <code>Dyx = 0.651</code> mean that there is a strong correlation, while the corrected <code>R2 = 0.0747</code> means that the correlation is very weak? I think the model is overfitted, but I'm not sure if I'm right.</p>

<p>Also, the other statistics are totally strange to me. What are <code>Emax, D, U, Q, B, g</code>, and <code>gp</code>?</p>
"
"0.0379868588198793","0.0179895690740289"," 55240","<p>I'm working on a data set modeling road kills (0 = random point, 1 = road kill) as a function of a number of habitat variables.  Following Hosmer and Lemeshow, I've examined each continuous predictor variable for linearity, and a couple appear nonlinear.  I'd like to try a fractional polynomial transformation for each, also following Hosmer and Lemeshow, and have looked at the R package mfp, but I'm having trouble coming up with (and understanding) the R code that will correctly transform the variable.  Can anyone suggest R code that would help me accomplish the concepts on p. 101 - 102 of Hosmer and Lemeshow's Applied Logistic Regression (2000).  Thanks!</p>
"
"0.131590338991954","0.0934765429274634"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.0882734829504749","0.0696733014291618"," 56440","<p>So I'm working with logistic regression models in R. Though I'm still new to statistics I feel like I got a bit of an understanding for regression models by now, but there's still something that bothers me:</p>

<p>Looking at the linked picture, you see the summary R prints for an example model I created. The model is trying to predict, if an email in the dataset will be refound or not (binary variable <code>isRefound</code>) and the dataset contains two variables closely related to <code>isRefound</code> , namely <code>next24</code> and <code>next7days</code> - these are also binary and tell if a mail will be clicked in the next 24hrs / next 7 days from the current point in the logs.</p>

<p>The high p-value should indicate, that the impact this variable has on the model prediction is pretty random, isn't it? 
Based on this I don't understand why the precision of the models predictions drops below 10% when these two variables are left out of the calculation formula. If these variables show such a low significance, why does removing them from the model have such a big impact?</p>

<p>Best regards and thanks in advance,
Rickyfox</p>

<p><img src=""http://i.stack.imgur.com/oiCrN.png"" alt=""enter image description here""></p>

<hr>

<h2>EDIT:</h2>

<p>First I removed only next24, which should yield a low impact because it's coef is pretty small. As expected, little changed - not gonna upload a pic for that. </p>

<p>Removing next7days tho had a big impact on the model: AIC 200k up, precision down to 16% and recall down to 73%</p>

<p><img src=""http://i.stack.imgur.com/583nx.png"" alt=""enter image description here""></p>
"
"0.161164592805076","0.0763232776972177"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0379868588198793","0.0539687072220866"," 56590","<p>I'm reading the technical manual for a <a href=""http://www.nwea.org/sites/www.nwea.org/files/resources/NJ_2011_LinkingStudy.pdf"" rel=""nofollow"">linking study</a> between two assessments.  It's pretty clear that the table is model output from a fitted logistic regression equation.  Here's what pass odds look like on test 2 as a function of score on test 1 (RIT score):  </p>

<p><img src=""http://i.stack.imgur.com/8lwbx.png"" alt=""enter image description here""></p>

<p>It seems silly to use a lookup table that rounds to 5 when the model that made that table could give a better estimate.  But how do I recreate that equation from this output? </p>

<p>I have a good sense of how I would fit this model if I had the raw data, but I'm not sure what to do here.  Not <code>glm(family=binomial)</code> because the data I have is are odds ratios, not pass / no pass (i.e., 1s and 0s), right?</p>

<p>Here's the data:</p>

<pre><code>PASS &lt;- c(0, 0, 0, 0.01, 0.01, 0.01, 0.02, 0.04, 0.06, 0.1, 0.15, 0.23, 
0.33, 0.45, 0.57, 0.69, 0.79, 0.86, 0.91, 0.94, 0.96, 0.98, 0.99, 
0.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)

RIT &lt;- c(120L, 125L, 130L, 135L, 140L, 145L, 150L, 155L, 160L, 165L, 
170L, 175L, 180L, 185L, 190L, 195L, 200L, 205L, 210L, 215L, 220L, 
225L, 230L, 235L, 240L, 245L, 250L, 255L, 260L, 265L, 270L, 275L, 
280L, 285L, 290L, 295L, 300L)
</code></pre>
"
"0.0328975847479884","0.0467382714637317"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"NaN","NaN"," 56729","<p>I hope this question is not off topic on CV:</p>

<p>I am currently fitting some different models (naive bayes, logistic regression...) in R which I up to now thought of as prototypes for a later Java implementation of the same models. </p>

<p>Now I came across multiple ways of executing R code from Java. </p>

<p>I am sure people were in a similar situation before, and I was wondering if you stick to Java libraries (or any other programming language) for solving statistical problems, or whether you execute R code from your programming environment? Since R is such a rich language for statistical problems, I do not see much motivation for using Java libraries for solving such tasks.</p>
"
"0.0759737176397586","0.0539687072220866"," 56871","<p>I have a dataset from a bank with demographic data and one variable telling if the customer is a good customer or not (binary variable). I would like to do prediction on if the customer is good or not based on this demographic data.</p>

<p>I managed to do it with a logistic regression, but would like now to compare the result (classification rate) with neural networks. </p>

<p>I found 2 functions from different packages doing that:
- nnet()
- neuralnet()
But those functions seem to be conceived for numerical dependent variables.</p>

<p>Thus my question: is there a possibility to use these functions for a categorical numerical variable (by estimating a posteriori probabilities for instance) or is there another function doing that?</p>

<p>Thanks a lot!</p>

<p>Robin</p>
"
"0.145643816250884","0.0985329278164293"," 57036","<p>I am using logistic regression to analyze some categorical data (binary response variable and categorical -- mostly binary -- predictor variables). For my model, I have something like <code>A ~  B</code> and a hypothesis that the respondent's <code>B</code> has some explanatory power over the choice of <code>A</code>. When I run this regression, only the intercept shows a p-value within the range of statistical significance. </p>

<p>However, I have some other variable <code>C</code> that assesses some pre-existing conditions for each respondent. When I run a logit regression on <code>A ~ B + C</code>, <code>C</code> has a very low p-value (statistically significant). That is to say, the pre-existing preferences that each respondent has, as reflected by <code>C</code> appear to have an effect on their choice in <code>A</code>. </p>

<p>My question then, is whether or not it is appropriate to add an interaction term for <code>B*C</code> to my regression in this case. When I run the logit regression <code>A ~ B * C</code> (or the equivalent to <code>A ~ B + C + B:C</code>), both <code>B</code> and <code>C</code> and the interaction term <code>B:C</code> have high statistical significance (low p-values). Is this statistically valid? Does it make sense for something to become statistically significant when an interaction term is added to the model?</p>
"
"0.0657951694959769","0.0311588476424878"," 57342","<p>I have created a logistic regression model in R. The dependent variable is binary whether or not a landscape transitioned to another cover class denoted by 1s and 0s, and the independent or predictor variables are some characteristics of the landscapes such as elevation, slope, distance to urban areas, etc. I need to illustrate the difference between predictor variables in terms of their importance or their effect on the dependent variable (land cover change). I was wondering if there is a way to rank and plot variable importance in logistic regression in R. Any help is greatly appreciated!</p>
"
"0.0759737176397586","0.0539687072220866"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"0.0379868588198793","0.0359791381480577"," 58151","<p>I'm using the well-known USC Burns Survival dataset to explore logistic regression in R.</p>

<p>The independent variable is burn area, and the outcome is binary survival (yes/no).</p>

<p>In the documentation, burn areas are grouped as the 'midpoint of set intervals', and taken as a log ie.</p>

<pre><code>log(area + 1)
</code></pre>

<p>for sample datapoints like (highest/lowest):</p>

<pre><code>1.35    yes
2.35    no
</code></pre>

<p>Medically, burns are usually specifed in terms of surface area % of total body eg. 40%, or alternatively, as an estimate of surface area in square meters. My question is: how does this relate to the dataset independent variable ie. what does '1.35' actually mean in terms of % body surface area burnt? What would eg. 30% burns become in the dataset, using the 'midpoint of set interval'?</p>

<p>Thanks guys</p>

<p>Ref: <a href=""http://statmaster.sdu.dk/courses/st111/data/index.html#burns"" rel=""nofollow"">http://statmaster.sdu.dk/courses/st111/data/index.html#burns</a></p>
"
"0.139572631559771","0.0881305298478463"," 58315","<p>I want to find the most important predictors for a binomial dependent variable out of a set of more than 43,000 independent variables (These form the columns of my input dataset). The number of observations is more than 45,000 (these form the rows of my input dataset). Most of the independent variables are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. Here is some code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"", type.measure = ""class"")
betacoeff = as.matrix(fit$glmnet.fit$beta[,ncol(fit$glmnet.fit$beta)])
</code></pre>

<p>betacoeff returns the betas for all the independent variables. I am thinking of showing the predictors corresponding to the top 50 betas as the most important predictors. 
My questions are:</p>

<ol>
<li><p>glmnet picks one good predictor out of a bunch of highly correlated good predictors. So I am not sure how much I can rely on the betas returned by the above model run.</p></li>
<li><p>Should I manually sample the data (say 10 times) and each time run the above model, get the list of predictors with the top betas and then find those which are present in all 10 repetitions? Is there any standard way of doing this? What is the standard way of sampling in this case?</p></li>
<li><p>My other question is about cvm (cross validation error) returned by the above model. Since I use type.measure = ""class"", cvm gives the misclassification error for different values of lambda. How do I report the misclassification error for the entire model? Is it the cvm corresponding to lambda.min?</p></li>
</ol>
"
"0.0657951694959769","0.0311588476424878"," 58403","<p>I have a predictor variable of type factor which contains the date of birth. 
From your experience (when dealing with logistic regression), what is the best way of treating date of birth (or similar types)?
As a continuos variable or maybe or maybe as a categorical variable which stores different age segments? </p>
"
"0.0930484210398471","0.0440652649239232"," 58772","<p>In testing the parallel regression assumption in ordinal logistic regression I find there are several approaches. I've used both the graphical approach (as detailed in HarrellÂ´s book) and the approach detailed using the  <a href=""http://cran.r-project.org/web/packages/ordinal/vignettes/clm_tutorial.pdf"" rel=""nofollow"">ordinal package</a> in R. </p>

<p>However I would also like to run the Brant test (from Stata) for both the individual variables and also for the total model. I've looked around but cannot find it implemented in R. </p>

<p><strong>Is there an implementation of the Brant test in R?</strong></p>
"
"0.131590338991954","0.0623176952849756"," 59311","<p>I'm working on a behavoural scorecard modelling exercise, and many of the decisions taken to date have been based on the experience of a consulting credit analyst (whose experience software-wise is SAS) as I am primarily in BI. So far I have:</p>

<ul>
<li>a linux pc with 32gb of ram and an i7 processor</li>
<li>an observation window  </li>
<li>~90 potential characteristics </li>
<li>a binary outcome</li>
</ul>

<p>In <strong>R</strong>, I have</p>

<ol>
<li>loaded the dataset (225k obs of 88 vars, 1 outcome)</li>
<li>split the dataset up based on the recommendations/examples in the package <strong>caret</strong> i.e. predictors and outcomes split up (150k obs in training sample)</li>
<li>removed any variables showing a high degree of correlation (caret::findCorrelation)</li>
<li>cut all continuous variables into categorical intervals </li>
<li>reduced the number of variables based on near zero variance, missing values, and low information value (IV) (150k obs of 48 vars)</li>
<li>tried bestglm::bestglm, caret::train (with glm and glmnet), FWDselect::selection, FWDselect::qselection but eventually had to interrupt each of these due to not completing after 4 hours of 100% CPU usage</li>
<li>used FactoMineR:MCA to perform a multiple correspondence analysis (on predictors only)</li>
</ol>

<p>What I would like to do is have a selection of logistic regression models for say 4, 8, 12, and 16 variables that are the most predictive models at each point.  I'm not sure if I'm going in the correct direction here with MCA as I've mainly been simply trying to find something that works in a timely fashion for reducing my variables further or going directly to variable selection steps.</p>

<p>I would appreciate any advice on how to do any of step 6 better, whether 7 makes sense and what step 8 should be. </p>

<p>Thanks,</p>

<p>Steph</p>

<p>PS Design decisions up to 6&amp;7 can't be revised so please, no telling me off for them! </p>
"
"0.0657951694959769","0.0467382714637317"," 60087","<p>Is it viable to do several binary logistic regressions instead of doing a multinomial regression? From this question: <a href=""http://stats.stackexchange.com/questions/52104/multinomial-logistic-regression-vs-binary-logistic-regression"">Multinomial logistic regression vs binary logistic regression</a> I see that the multinomial regression might have lower standard errors. </p>

<p>However, the package I would like to utilize has not been generalized to multinomial regression (<code>ncvreg</code>: <a href=""http://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf</a>) and so I was wondering if I could simply do several binary logistic regressions instead.</p>
"
"0.0465242105199235","0.0440652649239232"," 60109","<p>I would like to understand what the following code is doing. The person who wrote the code no longer works here and it is almost completely undocumented. I was asked to investigate it by someone who thinks ""<em>it's a bayesian logistic regression model</em>""</p>

<pre><code>bglm &lt;- function(Y,X) {
    # Y is a vector of binary responses
    # X is a design matrix

    fit &lt;- glm.fit(X,Y, family = binomial(link = logit))
    beta &lt;- coef(fit)
    fs &lt;- summary.glm(fit)
    M &lt;- t(chol(fs$cov.unscaled))
    betastar &lt;- beta + M %*% rnorm(ncol(M))
    p &lt;- 1/(1 + exp(-(X %*% betastar)))
    return(runif(length(p)) &lt;= p)
}
</code></pre>

<p>I can see that it fits a logistic model, takes the transpose of the Cholseky factorisation of the estimated covariance matrix, post-multiplies this by a vector of draws from $N(0,1)$ and is then added to the model estimates. This is then premultiplied by the design matrix, the inverse logit of this is taken, compared with a vector of draws from $U(0,1)$ and the resulting binary vector returned. But what does all this <strong><em>mean</em></strong> statistically ?</p>
"
"0.109658615826628","0.0934765429274634"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.113960576459638","0.0539687072220866"," 60817","<p>I am having trouble interpreting the z values for categorical variables in logistic regression. In the example below I have a categorical variable with 3 classes and according to the z value, CLASS2 might be relevant while the others are not. </p>

<p>But now what does this mean?</p>

<p>That I could merge the other classes to one? <br>
That the whole variable might not be a good predictor?</p>

<p>This is just an example and the actual z values here are not from a real problem, I just have difficulties about their interpretation.  </p>

<pre><code>           Estimate    Std. Error  z value Pr(&gt;|z|)    
CLASS0     6.069e-02  1.564e-01   0.388   0.6979    
CLASS1     1.734e-01  2.630e-01   0.659   0.5098    
CLASS2     1.597e+00  6.354e-01   2.514   0.0119 *  
</code></pre>
"
"0","0.0440652649239232"," 60958","<p>it happened to me that in a logistic regression in R with <code>glm</code> the Fisher scoring iterations in the output are less than the iterations selected with the argument <code>control=glm.control(maxit=25)</code> in <code>glm</code> itself.</p>

<p>I see this as the effect of divergence in the iteratively reweighted least
squares algorithm behind <code>glm</code>. </p>

<p>My question is: under which criteria does <code>glm</code> stop the iterations and provides with a partial output? I was thinking about something like ""when the new coefficients-old coefficients &lt; epsilon, then STOP"". Is this the case? If not, what does make <code>glm</code> stop?
Thanks,
Avitus</p>
"
"0.148039131365948","0.124635390569951"," 61138","<p>I am trying to calculate the marginal effects of a multinomial logistic regression. To do this I use the <code>mlogit</code> package and the <code>effects()</code> function.</p>

<p>Here is how the procedure works (source : <code>effects()</code> function of <code>mlogit</code> package) :</p>

<pre><code>data(""Fishing"", package = ""mlogit"")
Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = ""wide"", choice = ""mode"")
m &lt;- mlogit(mode ~ price | income | catch, data = Fish)
# compute a data.frame containing the mean value of the covariates in the sample
z &lt;- with(Fish, data.frame(price = tapply(price, index(m)$alt, mean), 
	catch = tapply(catch, index(m)$alt, mean), 
income = mean(income)))
# compute the marginal effects (the second one is an elasticity
effects(m, covariate = ""income"", data = z)
effects(m, covariate = ""price"", type = ""rr"", data = z)
effects(m, covariate = ""catch"", type = ""ar"", data = z)
</code></pre>

<p>I have no problem with first step (<code>mlogit.data()</code> function). I think my problem is in the specification of the multinomial regression.</p>

<p>My regression (for example with three variables) is on the form: <code>Y ~ 0 | X1 + X2 + X3</code>. When I try to estimate the marginal effects for a model with 2 variables, there is no problem, however for 3 variables R console returns me the following error: ""Error in if (rhs% in% c (1, 3)) {: argument is of length zero "" (translation from error in R console in french).</p>

<p>To understand what is my problem I tried to perform a multinomial regression of similar shape on the dataset ""Fishing"", i.e.,: <code>mode ~ 0 | income + price + catch</code> (even if this form has no ""economic"" sense.) Again the R console returns me the same error for 3 variables but manages to estimate these effects for a model with two variables.</p>

<p>This leads me to think that my problem really comes from the specification of my multinomial regression.  Do you know how I could find a solution to my problem? Or could you suggest another logit multinomial regression form ?</p>

<p>Thank you for your help :)</p>
"
"0.0657951694959769","0.0311588476424878"," 61144","<p>I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (<a href=""http://www.miraibio.com/blog/2010/08/the-4-parameter-logistic-4pl-nonlinear-regression-model/"" rel=""nofollow"">reference</a>) is often used for regression these data following this function:
$$
F(x) = \left(\frac{A-D}{1+(x/C)^B}\right) + D 
$$
How can I do this in <code>R</code>? I want to get the $A$, $B$, $C$ and $D$ values and plot the curve.</p>

<p>PS. If I have some data, how can I use the calculated function $F(x)$ to get the value? I mean how do I go from ""data -> F(x) -> value""?</p>
"
"0.1176979772673","0.0557386411433294"," 61344","<p>In a paper by <a href=""http://www.ncbi.nlm.nih.gov/pubmed/23628224"" rel=""nofollow"">Faraklas et al</a>, the researchers create a Necrotizing Soft-Tissue Infection Mortality Risk Calculator. They use logistic regression to create a model with mortality from necrotizing soft-tissue infection as the main outcome and then calculate the area under the curve (AUC). They use the bootstrap method to find the ""bootstrap optimism-corrected ROC area.""</p>

<p>If I were to do this in <code>R</code>, how would it look like? The code I have been toying with looks something like below:</p>

<pre><code>library(boot)
library(ROCR)

auc_calc &lt;- function(data, indices, outcomes) {
  d &lt;- data[indices,]
  # Using glm for logistic regression
  # Do I recreate the glm model for each dataset?
  fit &lt;- glm(outcomes[indices,] ~ X1 + X2 + X3, data=d, family=binomial)
  fit.predict &lt;- predict(fit, type=""response"")

  # Using ROCR to calculate AUC
  pred &lt;- prediction(fit.predict, outcomes[indices,])
  perf &lt;- performance(pred, ""auc"")

  # Returning the AUC
  return(perf@y.values[[1]])
}

boot.results &lt;- boot(data=my.data, statistic=auc_calc, R=10000, outcomes=my.outcomes)
</code></pre>

<p>Is this correct? Or am I doing something wrong - namely should I be passing in a glm model rather than recalculating it each time? As always thanks for the help.</p>
"
"0","0.0311588476424878"," 61845","<p>I am trying to run a Cox regression on a sample 2,000,000 row dataset as follows using only R. This is a direct translation of a PHREG in SAS. The sample is representative of the structure of the original dataset.</p>

<pre><code>##
library(survival)

### Replace 100000 by 2,000,000

test &lt;- data.frame(start=runif(100000,1,100), stop=runif(100000,101,300), censor=round(runif(100000,0,1)), testfactor=round(runif(100000,1,11)))

test$testfactorf &lt;- as.factor(test$testfactor)
summ &lt;- coxph(Surv(start,stop,censor) ~ relevel(testfactorf, 2), test)

# summary(summ)
##

user  system elapsed 
9.400   0.090   9.481 
</code></pre>

<p>The main challenge is in the compute time for the original dataset (2m rows). As far as I understand, in SAS this could take up to 1 day, ... but at least it finishes.</p>

<ul>
<li><p>Running the example with only 100,000 observations take only 9 seconds. Thereafter the time increases almost quadratically for every 100,000 increment in the number of observations.</p></li>
<li><p>I have not found any means to parallelize the operation (e.g., we can leverage a 48-core machine if this was possible)</p></li>
<li><p>Neither <code>biglm</code> nor any package from Revolution Analytics is available for Cox regression, and so I cannot leverage those.</p></li>
</ul>

<p><strong>Is there a means to represent this in terms of a logistic regression (for which there are packages in Revolution) or if there are any other alternatives to this problem?</strong> I know that they are fundamentally different, but it's the closest I can assume as a possibility given the circumstances. </p>
"
"0.0657951694959769","0.0623176952849756"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.164487923739942","0.116845678659329"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0657951694959769","0.0311588476424878"," 62225","<p>I have paired data (GWAS case/control study) and I have heard using conditional logistic regression or generalized linear mixed models (GLMM) is appropriate. Which should I use in this case? Why would you use one over the other. More importantly can you guys point me towards resources for doing these methods in <code>R</code>? I'm finding a lot of material for <code>SAS</code>, which I do not prefer. I can provide more details if necessary.  </p>
"
"0.107443061870051","0.0636027314143481"," 62307","<p>I used regular logistic regression on my dataset and got a few significant hits. However, since the data is 1:1 case-control matched data I decided to try using conditional logistic regression (<code>clogit()</code> in the <code>survival</code> package of <code>R</code>). </p>

<p>However, none of the results are significant any more. Is this normal? I was hoping to gain power by switching to conditional logistic regression. </p>

<pre><code> m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>This is a GWAS (genome wide association study). <code>PHENO</code> is either 1 (control) or 2 (case). <code>x</code> is number of copies of the minor allele (0, 1, or 2) in the SNP of interest, <code>COVAR[,1]</code> is a race covariate and <code>COVAR[,2]</code> is the matching covariate. </p>

<p>This test is performed for each SNP (~300,000) and the resulting p-values are adjusted for FDR. </p>

<p>Perhaps the matching of the cases and controls was not very good?</p>

<p>Matching: case and control pairs had: 1) same sex. 2) BMI > threshold. 3) same self-reported ethnicity. 4) lived within 50 miles of procurement site. </p>

<p>Perhaps I should use GLMM?</p>
"
"0","0.0311588476424878"," 62483","<p>I've got a conditional logistic regression setup using <code>clogit</code> in <code>R</code> like this: </p>

<pre><code>m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>I wanted to try doing GLMM analysis in <code>R</code>. I'm a little confused on the syntax for the <code>lme4</code> package in <code>R</code>. <code>COVAR[,2]</code> is the matching variable in my data. Can someone explain to me the difference between some of these statements:</p>

<pre><code>m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (x|COVAR[,2]) )
m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (1|COVAR[,2]) )
</code></pre>

<p>Which one is more appropriate? </p>
"
"0","0.0440652649239232"," 62503","<p>Say I want to do a logistic regression on whether it snows on a given month of the year and day of the week.  I'd have 12 months and 7 days.  My understanding is that this would translate into 17 dummies (11 for months and 6 for days).  </p>

<p>First off is this correct?  Second, how would I interpret regression results (i.e. assuming I dropped January, what's the p-value for January)?</p>
"
"NaN","NaN"," 62919","<p>@Dmitrij Celov posted an answer to the question <a href=""http://stats.stackexchange.com/questions/8303/how-to-do-logistic-regression-subset-selection"">How to do logistic regression subset selection</a> saying that <a href=""http://www.jstor.org/discover/10.2307/2531779?uid=3739256&amp;uid=2&amp;uid=4&amp;sid=21102415674961"" rel=""nofollow"">this paper</a> was a good reference. I was wondering if anyone knows if any of these authors ever created an R package based on it.</p>
"
"0.05884898863365","0.0696733014291618"," 63436","<p>I sometimes have to vectorise the Huber weights from a robust regression and use them in a lm.
Recently I've had to do something similar for a logistic model but I'm slightly worried because I don't get very similar results</p>

<pre><code>library(robustbase)
data(vaso)
ROB &lt;- glmrob(Y ~Volume+Rate, family=binomial(""logit""), data=vaso)
ROB
glm(Y ~Volume+Rate,data=vaso,family=binomial(""logit""))
glm(Y ~Volume+Rate,data=vaso,weights=ROB$w.r,family=binomial(""logit""))
</code></pre>

<p>The coefficients from the weighted glm are more similar to the robust regression than the unweighted glm, but is there a way to make them the same? I can get the same results with a robust (rlm) and weighted lm but this doesn't seem to be the case with glm. I haven't looked at the glm robust regression in detail so what I'm asking may be impossible...</p>

<p>Thanks for your help</p>
"
"0.0930484210398471","0.0440652649239232"," 63494","<p>In <code>R</code> I have a categorical variable that I performed logistic regression on and got the following result:</p>

<pre><code>glm(formula = mortality ~ SMOKE, family = binomial, data = c.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2155  -0.2155  -0.2155  -0.1860   2.8515  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.0483     0.3189 -12.694   &lt;2e-16 ***
SMOKEN        0.2968     0.3559   0.834    0.404    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 492.45  on 2369  degrees of freedom
Residual deviance: 491.72  on 2368  degrees of freedom
AIC: 495.72

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Is the value for the intercept the same as <code>SMOKEY</code> (has a history of smoking)?</p>
"
"0.174077655955698","0.0824385620013739"," 63566","<p>I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.</p>

<p>My data-frame in short: A subject has one property, <code>is_frisian</code> which is either 0 or 1 depending on the subject. And it is tested for two conditions, <code>person</code> and <code>condition</code>. The measurement variable is <code>error</code>, which is either 0 or 1.</p>

<p>My mixed linear model in R is:</p>

<pre><code>&gt; model &lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)
</code></pre>

<p>However, the residuals plot of this model gives an unexpected (?) result.</p>

<p><img src=""http://i.stack.imgur.com/nz2KY.png"" alt=""Residuals lmer model""></p>

<p>I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?</p>

<p>The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable <code>error</code> is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?</p>

<p><strong>UPDATE:</strong></p>

<pre><code>&gt; model &lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')
&gt; binnedplot(fitted(model),resid(model))
</code></pre>

<p>Gives the following result:</p>

<p><img src=""http://i.stack.imgur.com/XMXFx.png"" alt=""binned residual plot""></p>

<p><strong>FINAL EDIT:</strong></p>

<p>The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), <a href=""https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/"" rel=""nofollow"">see here at statisticssolutions.com</a></p>
"
"0.175453785322605","0.0830902603799674"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.1176979772673","0.0696733014291618"," 64242","<p>I have the following toy data:</p>

<pre><code>x &lt;- structure(c(2L, 2L, 3L, 1L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 2L, 3L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 3L, 3L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")

y &lt;- structure(c(2L, 2L, 3L, 1L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
2L, 3L, 2L, 1L, 2L, 2L, 3L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 1L, 2L, 
2L, 3L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 3L, 3L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 3L, 3L, 1L, 3L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 2L, 1L, 
3L, 2L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 1L, 3L, 
3L, 3L, 2L, 2L, 3L, 3L, 2L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")

z &lt;- structure(c(1L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 3L, 1L, 1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 1L, 1L, 
1L, 3L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 3L, 1L, 3L, 
3L, 1L, 1L, 1L, 3L, 1L, 3L, 3L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 3L, 
3L, 1L, 1L, 1L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")
</code></pre>

<p>I have replaced the 1's in <code>x</code> with 2's in <code>z</code> and vice versa. Now when I do an ordered logistic regression in R with <code>polr</code> (library MASS), I get the following coefficients: </p>

<pre><code>f1 &lt;- polr(y~x,Hess=TRUE)
f2 &lt;- polr(y~z,Hess=TRUE)

coef(summary(f1))
       Value Std. Error  t value
x2  25.95727  0.3028808 85.70127
x3  30.21524  0.5463144 55.30742
1|2 24.02167  0.3480269 69.02246
2|3 27.77068  0.3432316 80.90944

coef(summary(f2))
         Value   Std. Error       t value
z2  -21.495979 6.530398e-10 -3.291680e+10
z3    4.257964 8.119540e-01  5.244095e+00
1|2  -1.935599 3.567345e-01 -5.425880e+00
2|3   1.813399 3.411874e-01  5.314964e+00
</code></pre>

<p>It seems that something is not correct. Why relabeling the levels is changing dramatically the estimates for the SEs?</p>
"
"0.174077655955698","0.0824385620013739"," 64788","<p>I performed multivariate logistic regression with the dependent variable <code>Y</code> being death at a nursing home within a certain period of entry and got the following results (note if the variables starts in <code>A</code> it is a continuous value while those starting in <code>B</code> are categorical):</p>

<pre><code>Call:
glm(Y ~ A1 + B2 + B3 + B4 + B5 + A6 + A7 + A8 + A9, data=mydata, family=binomial)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0728  -0.2167  -0.1588  -0.1193   3.7788  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  20.048631  6.036637   3.321 0.000896 ***
A1           0.051167   0.016942   3.020 0.002527 ** 
B2          -0.664940   0.304299  -2.185 0.028878 *  
B3          -2.825281   0.633072  -4.463 8.09e-06 ***
B4          -2.547931   0.957784  -2.660 0.007809 ** 
B5          -2.862460   1.385118  -2.067 0.038774 *  
A6          -0.129808   0.041286  -3.144 0.001666 ** 
A7           0.020016   0.009456   2.117 0.034276 *  
A8          -0.707924   0.253396  -2.794 0.005210 ** 
A9           0.003453   0.001549   2.229 0.025837 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 485.10  on 2206  degrees of freedom
Residual deviance: 417.28  on 2197  degrees of freedom
AIC: 437.28

Number of Fisher Scoring iterations: 7

 (Intercept)           A1           B2           B3           B4           B5           A6           A7           A8           A9 
5.093426e+08 1.052499e+00 5.143045e-01 5.929197e-02 7.824340e-02 5.712806e-02 8.782641e-01 1.020218e+00 4.926657e-01 1.003459e+00 

                   2.5 %       97.5 %
(Intercept) 3.703525e+03 7.004944e+13
A1          1.018123e+00 1.088035e+00
B2          2.832698e-01 9.337710e-01
B3          1.714448e-02 2.050537e-01
B4          1.197238e-02 5.113460e-01
B5          3.782990e-03 8.627079e-01
A6          8.099945e-01 9.522876e-01
A7          1.001484e+00 1.039302e+00
A8          2.998207e-01 8.095488e-01
A9          1.000416e+00 1.006510e+00
</code></pre>

<p>As you can see, all of the variables are ""significant"" in that their p values are below the usual threshold of 0.05. However looking at the coefficients, I'm not quite sure what to make of these results. It seems that although these variables contribute to the model, looking at the odds ratios, they don't seem to really seem to have much predictive power. Of note, when I calculated the AUC, I got approximately 0.8. </p>

<p>Can I say that this model is better at predicting against mortality (e.g. predicting that seniors will live past the prescribed period) compared to predicting for mortality?</p>
"
"0.0657951694959769","0.0467382714637317"," 65174","<p>I am doing a logistic regression analysis using the glm command in R. It is to identify causes of valve narrowing beyond a certain threshold; 0=no narrowing, 1=narrowed. One of my variables is the size of a medical device that is implanted (range 25-36mm). Sometimes the device isn't implanted and I've left this as a blank field, but of course this is interpreted as a missing field. Not implanting the device seems to have a significant effect using Chi-sq analysis, and the size of the device has a significant effect using a t-test. How do I get around this in a linear regression model?</p>

<p>To make it more complicated I actually have two different makes of the device: ""C"" and ""D"" with sizes 25-36mm, another device without a size ""S"" and then no device ""N"". Can it all be entered together or is it best to analyze separately outside of regression?</p>

<p>What effect does the ""missingness"" have on various other variables that are in the analysis?</p>

<p>Please &amp; thankyou</p>
"
"0.147122471584125","0.0696733014291618"," 65244","<p>I'm curious about how to understand the accuracy of my model which I computed with <code>glm( family = binomial(logit) )</code>.</p>

<p>In some articles it is mentioned that we should perform chisq test with residual deviance with it's DoF. 
When I call summary() of my glm module.
<strong>""Residual deviance: 9109.9 on 99993 degrees of freedom""</strong> 
Therefore when I perform pchisq test with these inputs: <strong>1-pchisq(9110, 99993)</strong> it returns 1.</p>

<p>Hence it is much more greater than our significance level. So we are curious about why does it return 1, is it a perfect model ?</p>

<p>In addition to these, here's the output of my Logistic Regression Model</p>

<pre><code>Logistic Regression Model

lrm(formula = bool.revenue.all.time ~ level + building.count + 
    gold.spent + npc + friends + post.count, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
level           0.0756 0.0193   3.92 &lt;0.0001 
building.count  0.0698 0.0091   7.64 &lt;0.0001 
gold.spent      0.0020 0.0002  11.05 &lt;0.0001 
npc             0.0172 0.0057   3.03 0.0024  
friends         0.0304 0.0045   6.82 &lt;0.0001 
post.count     -0.0132 0.0042  -3.17 0.0015 
</code></pre>

<p>This is validation with bootstrap's output</p>

<pre><code>  index.orig training   test optimism index.corrected    n
Dxy           0.5511   0.5500 0.5506  -0.0006          0.5518 1000
R2            0.1469   0.1469 0.1465   0.0005          0.1465 1000
Intercept     0.0000   0.0000 0.0002  -0.0002          0.0002 1000
Slope         1.0000   1.0000 0.9997   0.0003          0.9997 1000
Emax          0.0000   0.0000 0.0001   0.0001          0.0001 1000
D             0.0149   0.0149 0.0148   0.0000          0.0148 1000
U             0.0000   0.0000 0.0000   0.0000          0.0000 1000
Q             0.0149   0.0149 0.0148   0.0001          0.0148 1000
B             0.0086   0.0086 0.0086   0.0000          0.0086 1000
g             1.1410   1.1381 1.1365   0.0016          1.1394 1000
gp            0.0111   0.0111 0.0111   0.0000          0.0111 1000
</code></pre>

<p>And this is the output of my calibration curve:</p>

<pre><code>n=100000   Mean absolute error=0.002   Mean squared error=5e-05
0.9 Quantile of absolute error=0.002
</code></pre>

<p><img src=""http://i.stack.imgur.com/2AUEX.png"" alt=""Calibration Curve""></p>

<p>Thanks.</p>
"
"0.0379868588198793","0.0539687072220866"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.0759737176397586","0.0539687072220866"," 65384","<p>I'm analyzing users' in-game data in order to model whether they're going to be paid user or not. </p>

<p>Here's my model:</p>

<pre><code>Logistic Regression Model

lrm(formula = becomePaid ~ x1 + x2 + 
    x3 + x4 + x5 + x6, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
x1              0.0756 0.0193   3.92 &lt;0.0001 
x2              0.0698 0.0091   7.64 &lt;0.0001 
x3              0.0020 0.0002  11.05 &lt;0.0001 
x4              0.0172 0.0057   3.03 0.0024  
x5              0.0304 0.0045   6.82 &lt;0.0001 
x6             -0.0132 0.0042  -3.17 0.0015  
</code></pre>

<p>And in my model, I created couple of use cases such as:</p>

<pre><code>    test1   test2       
x1  8           9
x2  10          10
x3  250        250
x4  6           6
x5  2           2
x6  0           1
</code></pre>

<p>Then the probability of user test1 is to turn out to be a paid user is %.07 and % 0.84 for test2.</p>

<p>However I want to calculate the cumulative probabilities such as users whose' x1 values are greater than 8, x2 values are between 10 and 20 and so on.</p>

<p>Is there any way to calculate this ?</p>

<p>Thanks ! </p>
"
"0.124341182825498","0.0824385620013739"," 65463","<p>This is a question regarding using logistic regression, and relating it to gaussian distribution or a binomial distribution.  </p>

<pre><code>model&lt;-glm(target~ x1, data=data, type='response', family='binomial')
model&lt;-glm(target~ x1, data=data, type='response')  #defaults to gaussian
</code></pre>

<p>My understanding of binomial is that it is </p>

<pre><code>theta=chance of success
z=trails ending in success
k=trials ending in failure
(theta^z)*(1-theta)^k
</code></pre>

<p>And something Gaussian is </p>

<pre><code>theta = standard deviation
x = success
u = mean
Y = [ 1/Ïƒ * sqrt(2Ï€) ] * e -(x - Î¼)2/2Ïƒ2 
</code></pre>

<p>So I understand how to do GLM with R, I kind of understand what binomial and gaussian means, but I have no understanding of how you relate binomial or gaussian to logistic regression, and how binomial and gaussian are different in this context.</p>

<p>Question 1- Can someone explain the intuition behind how ""family='binomial'"" is used when building a model with GLM?</p>

<p>Question 2- Given that the shapes of a binomial distribution and a gaussian distribution look very much the same (they both peak in the middle and gradually go down towards the ends), how does choosing either binomial or guassian lead to different models built from GLM?</p>

<p>thanks!!</p>
"
"0.0882734829504749","0.0696733014291618"," 65548","<p>Here is the kind of data I have:</p>

<p>I have two predictor variables: </p>

<p>1) discrete non-ordinal --> c('a','b','c') </p>

<p>2) discrete ordinal --> c(10,100,200,500)</p>

<p>Response variable: Proportion of TRUE over a list of TRUE/FALSE. If the list is of length 3, my variable can take only 4 values. But not all my values come from the same list's length. And moreover the lists are globally long ! So it is discrete proportions but can take more than 100 values.</p>

<p>Here is an example (resp is a subset of my data):</p>

<pre><code>pred_1 = rep(c(10,20,50,100),30)
pred_2 = rep(c('a','b','c'),40)

resp = c(0.08666667, 0.04000000, 0.13333333, 0.04666667, 0.50000000, 0.04000000, 0.02666667, 0.24666667, 0.15333333, 0.04000000, 0.06666667, 0.06666667, 0.03333333,
    0.04000000, 0.26000000, 0.04000000, 0.04000000, 1.00000000, 0.28666667, 0.03333333, 0.06666667, 0.15333333, 0.06666667, 0.28000000, 0.35333333, 0.06000000,
    0.06000000, 0.05333333, 0.96666667, 0.06666667, 0.03333333, 0.22000000, 0.04666667, 0.04666667, 0.05333333, 0.05333333, 0.05333333, 0.08000000, 0.48666667,
    0.08666667, 0.02666667, 0.21333333, 0.45333333, 0.04666667, 0.36000000, 0.06666667, 0.04000000, 0.06000000, 0.07333333, 0.06000000, 0.04000000, 0.04666667,
    0.30000000, 0.08666667, 0.07333333, 0.06666667, 0.29333333, 0.36000000, 0.17333333, 0.04000000, 0.09333333, 0.11333333, 0.03333333, 0.08000000, 0.27333333,
    0.08666667, 0.03333333, 0.04000000, 0.02666667, 0.07333333, 0.07333333, 0.02000000, 0.02666667, 0.08000000, 0.07333333, 0.02666667, 0.06666667, 0.07333333,
    0.95333333, 0.05333333, 0.04000000, 0.11333333, 0.04000000, 0.07333333, 0.06666667, 0.05333333, 0.04000000, 0.04000000, 0.06000000, 0.12666667, 0.04666667,
    0.04000000, 0.21333333, 0.05333333, 0.97333333, 0.11333333, 0.02666667, 0.04000000, 0.03333333, 0.37333333, 0.25333333, 0.06000000, 0.06000000, 0.06000000,
    0.04666667, 0.26666667, 0.98000000, 0.02000000, 0.26000000, 0.06000000, 0.05333333, 0.28000000, 0.99333333, 0.04666667, 0.02666667, 0.04000000, 0.12666667,
    0.04666667, 0.18000000, 0.03333333) 
</code></pre>

<p>my response variable is not at all normally distributed (kolmogorov-smirnow and shapiro test + visual checking with qqplot()) nor is the residuals of a linear model (lm()). Moreover the common assumption of homoscedasticity is not respected neither.</p>

<p>I've always asked a similar question but not as much accurate <a href=""http://stats.stackexchange.com/questions/65388/which-model-should-i-use-logistic-regression"">here</a>.
Peter Flom has suggested that I use a ordinal logistic regression (polr()) but I might not have given him enough information (he did not know the number of levels for example). What do you think ? Which model would you suggest me ? Can I make a polr() with that much levels ? When I do it I actually get this:</p>

<p>Error message:
""Initial value ""vmin"" in not finite""</p>

<p>Notification message:
""glm.fit: fitted probabilities numerically 0 or 1 occurred ""</p>

<p>I'm struggling on this problem for quite a long time. All your contributions are more than welcome !</p>

<p>Thanks a lot !</p>
"
"0.131590338991954","0.0623176952849756"," 65656","<p>My design is as follows.</p>

<ul>
<li>$y$ is Bernoulli response </li>
<li>$x_1$ is a continuous variable </li>
<li>$x_2$ is a categorical (factor) variable with two levels</li>
</ul>

<p>The experiment is completely within subjects. That is, each subject receives each combination of $x_1$ and $x_2$.</p>

<p>This is a repeated measures logistic regression set-up. The experiment will give two ogives for $p(y=1)$ vs $x_1$, one for level1 and one for level2 of $x_2$. The effect of $x_2$ should be that for level2 compared to level1, the ogive should have a shallower slope and increased intercept.</p>

<p>I am struggling with finding the model using <code>lme4</code>. For example,</p>

<pre><code>glmer(y ~ x1*x2 + (1|subject), family=binomial)
</code></pre>

<p>So far as I understand it, the <code>1|subject</code> part says that <code>subject</code> is a random  effect. But I do not see how to specify that $x_1$ and $x_2$ are repeated measures variables. In the end, I want a model that includes a random effect for subjects, and gives estimated slopes and intercepts for level1 and level2.</p>
"
"0.197385508487931","0.0934765429274634"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.131590338991954","0.0623176952849756"," 65740","<p>I have binomial data infected/not-infected on individuals of a sample from different years for two different infections.
I have used <code>prop.trend.test</code> to test for trend for each infection.</p>

<p>I want to test if there is a trend in the ratio of infected/not-infected between the two infections. Since a ratio could vary between 0 and &infin;, what test is appropriate? </p>

<p>For testing trends in proportins, some have suggested <code>glm</code> with family <code>binomial</code>. Can i use <code>glm</code> with a different family?</p>

<p><a href=""http://stats.stackexchange.com/questions/29502/what-test-can-i-use-to-prove-there-is-an-upward-trend-in-time-series-of-ratios-a"">What test can I use to prove there is an upward trend in time series of ratios assesment in R?</a> suggests using the <code>Cochran-Armitage test</code> (which I think is the same as <code>prop.trend.test</code>) or logistic regression. Not sure if it is suitable in the case of ratios, though.</p>
"
"0.186096842079694","0.0881305298478463"," 65859","<p>Recently, I have read an article which name is â€œ<a href=""http://www.ncbi.nlm.nih.gov/pubmed/9618776"" rel=""nofollow""><em>Feed forward neural networks for the analysis of censored survival data: A partial logistic regression approach</em></a>â€.</p>

<p>Without a math background, I catch only a little about the ANN applying the analysis of censored survival data. </p>

<p>In the training group, each subject is replicated for all the intervals in which the subjects is observed and coupled with the event; why is this done? Especially, in the testing group, indicator each subjects are replicated into full number of time interval of observed with all event indicator as zero?</p>

<p>I know this process can take into account the censoring data.</p>

<p>In addition, are the output of neural networks are conditional probabilities of failure? My problem is how to produce the survival curve using the output data. Does anybody know the R code or MATLAB code to perform this whole process? Or give me some suggestion to find answers! The following R code is my try on this method but I can't go on it, for I don't know draw the survival curve depending on the output conditional failure probabilities!</p>

<pre><code>dat&lt;-read.csv(""traininglj.csv"",header=T)
tt &lt;- dat$time &lt;- as.numeric(cut(dat$TTR,c(0,6,12,18,24,30,36,42,48,54,200)))
dat2 &lt;- dat[rep(1:nrow(dat), tt), ]
time2 &lt;- NULL
for (i in 1:length(tt)) time2 &lt;- c(time2, 1:tt[i])
dat2$time &lt;- time2
    dat2$Recurrence &lt;- 0
dat2$Recurrence[cumsum(tt)] &lt;- dat$Recurrence
write.csv(dat2,file=""result.csv"")
mydat &lt;- apply(dat2[,13:23],2,function(x)(x-min(x,na.rm=TRUE))/
    (range(x,na.rm=TRUE)[2]-range(x,na.rm=TRUE)[1]))
training &lt;- cbind(dat2[,1:12],mydat,dat2[24])
library(nnet)
library(lattice)
attach(training)
dat.net &lt;- nnet(Recurrence ~ time+ALT+ALB+PLT+INR+age+MELD+logAFP+Diameter
        +sex+number+Gstage+HBsAg+AN+MVI, 
    data = training, 
    size = 12, 
    decay=0.025,
    maxit = 1000,
    entropy=TRUE,
    trace=TRUE)
</code></pre>
"
"0.0657951694959769","0.0311588476424878"," 66098","<p>I want to do a logistic regression with R.</p>

<p>I have 18 continuous covariates and a sample consisting of 100 observations.</p>

<p>When I enter all of the covariates into the <code>glm()</code> model, none of them are significant, but the model predicts the outcome perfectly on the test data!</p>

<p>My questions are:</p>

<ol>
<li>Is the sample size large enough for running <code>glm()</code> with this many covariates?</li>
<li>What might be other causes of the problem?</li>
<li>How can I run such a model properly?</li>
</ol>
"
"0.0657951694959769","0.0311588476424878"," 66279","<p>Has anyone written a package in <code>R</code> to calculate diagnostic plots after <code>clogit</code>, conditional logistic regression? e.g. leverage. Or a related question, how do you stratify using <code>glm</code> (perhaps I can stratify using <code>glm</code> and <code>family =binomial</code>, and then use diagnostic packages for <code>glm</code>?)</p>
"
"NaN","NaN"," 66946","<p>When you predict a fitted value from a logistic regression model, how are standard errors computed?  I mean for the <em>fitted values</em>, not for the coefficients (which involves Fishers information matrix).</p>

<p>I only found out how to get the numbers with <code>R</code> (e.g., <a href=""https://stat.ethz.ch/pipermail/r-help/2010-August/248241.html"">here</a> on r-help, or <a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">here</a> on Stack Overflow), but I cannot find the formula.</p>

<pre><code>pred &lt;- predict(y.glm, newdata= something, se.fit=TRUE)
</code></pre>

<p>If you could provide online source (preferably on a university website), that would be fantastic.</p>
"
"0.20806259464412","0.0985329278164293"," 67049","<p>Surprisingly, I was unable to find an answer to the following question using Google:</p>

<p>I have some biological data from several individuals that show a roughly sigmoid growth behaviour in time. Thus, I wish to model it using a standard logistic growth</p>

<pre><code>P(t) = k*p0*exp(r*t) / (k+p0*(exp(r*t)-1))
</code></pre>

<p>with p0 being the starting value at t=0, k being the asymptotic limit at t->infinity and r being the growth speed. As far as I can see, I can easily model this using nls (lack of understanding on my part: why can I not model something similar using standard logit regression by scaling time and data? EDIT: Thanks Nick, apparently people do it e.g. for proportions, but rarely <a href=""http://www.stata-journal.com/article.html?article=st0147"">http://www.stata-journal.com/article.html?article=st0147</a> . Next question on this tangent would be if the model can possibly handle outliers >1).</p>

<p>Now I wish to allow some fixed (mainly categorical) and some random (an individual ID and possibly also a study ID) effects on the three parameters k, p0 and r. Is nlme the best way of doing this? The SSlogis model seems sensible for what I am trying to do, is that correct? Is either of the following a sensible model to begin with? I cannot seem to get the starting values right and update() only seems to work for random effects, not fixed ones - any hints?</p>

<pre><code>nlme(y ~ k*p0*exp(r*t) / (k+p0*(exp(r*t)-1)), ## not working at all (bad numerical properties?)
            data = data,
            fixed = k + p0 + r ~ var1 + var2,
            random = k + p0 + r ~ 1|UID,
            start = c(p0=1, k=100, r=1))

nlme(y ~ SSlogis(t, Asym, xmid, scal), ## not working, as start= is inappropriate
            data = data,
            fixed = Asym + xmid + scal ~ var1 + var2, ## works fine with ~ 1
            random = Asym + xmid + scal ~ 1|UID,
            start = getInitial(y ~ SSlogis(Dauer, Asym, xmid, scal), data = data))
</code></pre>

<p>As I am new to non-linear mixed models in particular and non-linear models in general, I would appreciate some reading recommendations or links to tutorials / FAQs with newbie questions.</p>
"
"0.143619053022704","0.120914089414122"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.1176979772673","0.0696733014291618"," 67460","<p>I fitted the following multinomial regression:</p>

<pre><code>library(car)
p1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,2,2,3,4,3,3,4,3,4)

d1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,1,2,3,4,3,2,2,2,1)

d1&lt;-as.ordered(d1)

library(nnet)
test&lt;-multinom(p1~d1)
predi&lt;-expand.grid(d1=c(""1"",""2"",""3"",""4""))

pre&lt;-predict(test,predi,type=""probs"")
</code></pre>

<p>The output is a table of the predicted probabilities for every coefficient. I can also order the results for the confidence interval of the coefficents with:</p>

<pre><code>confint(test)
</code></pre>

<p>My question is: is it possible to get the results for the confidence interval for the predicted probabilities? It means for every amount in the ""pre"" output! 
PS: I found a similar question here in 
[""plotting confidence intervals""][1]<a href=""http://stats.stackexchange.com/questions/29044/plotting-confidence-intervals-for-the-predicted-probabilities-from-a-logistic-re"">Plotting confidence intervals for the predicted probabilities from a logistic regression</a></p>

<p>The main answer is perfect for my question, but I do not know how to combine with multinomial regression. 
I hope you understand my bad english :) Thank you for your help</p>
"
"0.307488845332535","0.201987040035874"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.1176979772673","0.0696733014291618"," 68129","<p>In addition to <code>proc varclus</code>, <code>randomForest</code>, and assessing multicollinearity among potential predictor variables, I am seeking other methods of variable selection in lieu of using stepwise methods for building more parsimonious binary logistic regression models from a wide array of potential predictor variables. I have done some research into other methods such as <a href=""http://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow"">Mutual Information</a> (MI), and I have two questions in regards to its use:</p>

<p>1) Has anyone used MI for binary logistic regression variable selection? If so, what are your thoughts on its application?</p>

<p>2) Does anyone know how to calculate MI using either Base SAS or R for potential predictor variables in reference to the outcome of interest? Any help or references in this area would be greatly appreciated!</p>

<p>Thanks!</p>
"
"0.134303827337563","0.0763232776972177"," 68388","<p>I have a great interest in learning new methods(at least to me) of variable selection in regards to binary logistic regression when I am working with over 500 potential predictor variables and have the duty of selecting 8 to 15 variables to build a parsimonious predictive model without using the notorious stepwise techniques. </p>

<p>With that being said, I was wondering if anyone has any experience using <code>proc factor</code> for binary logistic regression variable selection? I assume my factors will correlate, and thus I will use <code>promax</code> rotation, but with the results of the Exploratory Factor Analysis (EFA), I will simply retain the variable within each factor that has the highest loading on its own factor (latent variables models would confuse the hell out of the end-user of 99.999% of my models!) for further variable reduction through another technique such as <code>randomForest</code> until the number of variables is small enough to build a model that has fewer than 15 variables in it. </p>

<p>Does anyone have any thoughts in regards to this process? Any suggested readings or input would be greatly appreciated. Thanks!</p>
"
"0.149209419390598","0.0824385620013739"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.213834300861925","0.124635390569951"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.1176979772673","0.0696733014291618"," 69088","<p>For those of you familiar with <a href=""http://en.wikipedia.org/wiki/Exploratory_factor_analysis"" rel=""nofollow"">Exploratory Factor Analysis</a> (EFA) and <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> (RF), I have recently had an idea of combining these two methods to reduce the number of potential predictor variables for use in a parsimonious binary logistic regression model. For the purposes of this post, assume large <em>n</em> (200k or more) and 1000 potential predictor variables.</p>

<p>To employ this idea, the first step would be to perform an EFA with all potential predictor variables using <code>proc varclus</code>. Additionally, using <code>randomForest</code> to rank all potential predictor variables by <code>IncNodePurity</code> (<a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a>). </p>

<p>After these two methods are independently used, I propose retaining the variable with the largest <code>IncNodePurity</code> (from RF) within each factor (from EFA).</p>

<p>Does anyone have any thoughts/concerns with this methodology (or lack thereof) for feature selection? I am aware that this ""picking and choosing"" of methods may be complete garbage, but I had this random thought and wanted to share. Thanks!</p>
"
"0.0657951694959769","0.0467382714637317"," 69208","<p>After creating a <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> object using <code>randomForest</code> with around 500 candidate variables, I used <code>importance(object)</code> to display <em>IncNodePurity</em> for each of the candidate variables in relation to the binary outcome of interest (Payment/No Payment). </p>

<p>I am aware that <em>IncNodePurity</em> is the total decrease in node impurities, measured by the <a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a> from splitting on the variable, averaged over all trees. What I don't know is what should be the cutoff for candidate variables to be retained after making use of <code>randomForest</code> for feature selection in regards to binary logistic regression models. For example, the smallest <em>IncNodePurity</em> among my 498 variables is 0.03, whereas the largest <em>IncNodePurity</em> is 96.68.
In summary, I have one main question:</p>

<p>Is there a cutoff for <em>IncNodePurity</em>? If yes, what is it?</p>

<p>If no, how do you determine the cutoff? Do you simply take 10 candidate variables with the largest <em>IncNodePurity</em> if you want a model with only 10 predictor variables?</p>

<p>Any thoughts or references are greatly appreciated. Thanks!</p>
"
"0.223703576286321","0.124635390569951"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.0379868588198793","0.0539687072220866"," 69960","<p>I have a dataframe with 2 million rows and approximately 200 columns / features. Approximately 30-40% of the entries are blank. I am trying to find important features for a binary response variable.  The predictors may be categorical or continuous. </p>

<p>I started with applying logistic regression, but having so much missing entries I feel that this is not a good approach as glm discard all records which have any item blank. So I am now looking to apply tree based algorithms (<code>rpart</code> or <code>gbm</code>) which are capable to handle missing data in a better way. </p>

<p>Since my data is too big for <code>rpart</code> or <code>gbm</code>, I decided to randomly fetch 10,000 records from original data, apply <code>rpart</code> on that, and keep building a pool of important variables. However, even this 10,000 records seem to be too much for the <code>rpart</code> algorithm. </p>

<p>What can I do in this situation? Is there any switch that I can use to make it fast? Or it is impossible to apply <code>rpart</code> on my data. </p>

<p>I am using the following rpart command:</p>

<pre><code>varimp = rpart(fmla,  dat=tmpData, method = ""class"")$variable.importance
</code></pre>
"
"0.0986927542439653","0.0623176952849756"," 70174","<p>I have three variables, a factor (<code>c</code>) as the dependent variable and two ordinal independent variables (<code>a, b</code>). Each variable has five categories (<code>1,2,3,4,5</code>). Thus, I fitted a multinomial logistic regression (<code>testus</code>, see below) with the <code>car</code> package. Now I would like to get the predicted probabilities. </p>

<pre><code>&gt; a &lt;- sample(5,100,TRUE)
&gt; b &lt;- sample(5,100,TRUE)
&gt; c &lt;- sample(5,100,TRUE)
&gt; required(car)
&gt; a &lt;- as.ordered(a)
&gt; b &lt;- as.ordered(b)
&gt; c &lt;- as.factor(c)
&gt; testus        &lt;- multinom(c~a+b)
&gt; predictors    &lt;- expand.grid(b=c(""1"",""2"",""3"",""4"",""5""), a=c(""1"",""2"",""3"",""4"",""5""))
&gt; p.fit         &lt;- predict(testus, predictors, type='probs')
&gt; probabilities &lt;- data.frame(predictors,p.fit)
</code></pre>

<p>Now I got the predicted probabilities for a under b and c. </p>

<pre><code>&gt; head(probabilities)
  b a         X1         X2         X3         X4        X5
1 1 1 0.10609054 0.22599152 0.20107167 0.21953158 0.2473147
2 2 1 0.20886614 0.27207108 0.08613633 0.18276394 0.2501625
3 3 1 0.17041268 0.24995975 0.16234240 0.13111518 0.2861700
4 4 1 0.23704078 0.21179521 0.08493274 0.03135092 0.4348804
5 5 1 0.09494071 0.09659144 0.24162612 0.21812449 0.3487172
6 1 2 0.14059489 0.17793438 0.29272452 0.26104833 0.1276979`
</code></pre>

<p>The first two columns show the categories of the independent variables <code>a</code> and <code>b</code>. The next five columns show the conditional probabilities (e.g., $P(c=1|b=1~\&amp;~a=1) = 0.10609$. But now I would like to know only the predicted probabilities for <code>c</code> under <code>a</code> or the predicted probabilities for <code>c</code> under <code>b</code>. Is this possible?</p>
"
"0.134303827337563","0.0763232776972177"," 71292","<p>I need to compare the goodness of fit of several averaged logistic regression models by calculating the deviance explained. I'm using the <code>MuMIn</code> package in R to average many logistic regression models into a single averaged model. I ultimately want to compare the explanatory power of several averaged models, in part by using the deviance explained.</p>

<p>My questions are:</p>

<ol>
<li><p>Does deviance explained apply to averaged models as a strong measure of the goodness of fit?</p></li>
<li><p>How does one calculate the deviance explained (calculated as the null deviance less the residual deviance as a proportion of the null deviance) from the averaged model output from the <code>model.avg()</code> command in <code>MuMIn</code>? </p>

<p>Examining the structure of the averaged model object indicates that the null and residual deviances are calculated on each individual model that contributes to the averaged model, but I'm not sure how to extract them and then calculate the overall deviance explained by the averaged model.</p></li>
</ol>
"
"0.258069887046255","0.152768819375106"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.0994729462603988","0.0824385620013739"," 71727","<p>In addition to <a href=""http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_varclus_sect004.htm"">PROC VARCLUS</a>, <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"">randomForest</a>, <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a>, and assessing multicollinearity among potential predictor variables (without regards to the outcome of interest), I am seeking other methods of variable selection in lieu of using stepwise methods for building more parsimonious binary logistic regression models (containing 8 to 12 variables to predict outcomes such as loan payment/default or current/late payment history) from a wide array of potential predictor variables (500+ variables, 200k+ records). </p>

<p>Below I have included an R script using <a href=""http://cran.r-project.org/web/packages/FSelector/index.html"">FSelector</a> to select the 8 highest ""ranked"" variables:</p>

<pre><code>library(FSelector)
fit &lt;- information.gain(outcome ~ ., dataset)
fit2 &lt;- cutoff.k(fit,8)
reducedmodel &lt;- as.simple.formula(fit2,""outcome"")
print(reducedmodel)
</code></pre>

<p>I have two questions regarding this script and the <code>FSelector</code> algorithm in general:   </p>

<ol>
<li><p>Is the <code>information.gain</code> criteria in the above script synonymous with <a href=""http://en.wikipedia.org/wiki/Kullback-Leibler_divergence"">Kullback-Leibler divergence</a>?
If so, can someone explain this in more layman terms than Wikipedia as I am relatively new to this area of statistics and would like to start off with the right idea of this concept as I may likely use this approach a great deal in the future?</p></li>
<li><p>Is this a valid approach, if there is such a thing as a valid approach, to select a desired number of variables for a binary logistic regression model (e.g., selecting the 8 highest ""ranked"" variables for use in a parsimonious model)? If not, can you provide an alternative approach to do so?</p></li>
</ol>

<p>Any insight or references regarding this topic and/or these questions will be greatly appreciated!</p>
"
"0.0657951694959769","0.0311588476424878"," 72060","<p>In the following code I perform a logistic regression on grouped data using glm and ""by hand"" using mle2. Why does the logLik function in R give me a log likelihood logLik(fit.glm)=-2.336 that is different than the one logLik(fit.ml)=-5.514 I get by hand? </p>

<pre><code>library(bbmle)

#successes in first column, failures in second
Y &lt;- matrix(c(1,2,4,3,2,0),3,2)

#predictor
X &lt;- c(0,1,2)

#use glm
fit.glm &lt;- glm(Y ~ X,family=binomial (link=logit))
summary(fit.glm)

#use mle2
invlogit &lt;- function(x) { exp(x) / (1+exp(x))}
nloglike &lt;- function(a,b) {
  L &lt;- 0
  for (i in 1:n){
     L &lt;- L + sum(y[i,1]*log(invlogit(a+b*x[i])) + 
               y[i,2]*log(1-invlogit(a+b*x[i])))
  }
 return(-L) 
}  

fit.ml &lt;- mle2(nloglike,
           start=list(
             a=-1.5,
             b=2),
           data=list(
             x=X,
             y=Y,
             n=length(X)),
           method=""Nelder-Mead"",
           skip.hessian=FALSE)
summary(fit.ml)

#log likelihoods
logLik(fit.glm)
logLik(fit.ml)


y &lt;- Y
x &lt;- X
n &lt;- length(x)
nloglike(coef(fit.glm)[1],coef(fit.glm)[2])
nloglike(coef(fit.ml)[1],coef(fit.ml)[2])
</code></pre>
"
"0.0497364731301994","0.0706616245726062"," 72516","<p>I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. </p>

<p>My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But, there are multiple factors, other than English ivy, that affect soil moisture.</p>

<p><img src=""http://i.stack.imgur.com/k65Ag.jpg"" alt=""enter image description here""></p>

<p>My questions are:</p>

<ol>
<li><p>I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?</p></li>
<li><p>Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?</p>

<p><img src=""http://i.stack.imgur.com/ArgZm.jpg"" alt=""The relationship between soil moisture and English ivy cover on cover objects (&quot;the number of overstory trees&quot; for the left graph) for different levels of the surrounding overstory trees (&quot;English ivy cover on cover objects&quot; for the left graph""></p></li>
<li><p>Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small <strong>AND</strong> regressions do not reflect my hypothesized causal relationships accurately.</p></li>
</ol>

<p>I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though). </p>
"
"0.1359059177167","0.112632518139278"," 73165","<p>I have a logistic regression model (fit via glmnet in R with elastic net regularization), and I would like to maximize the difference between true positives and false positives.  In order to do this, the following procedure came to mind:</p>

<ol>
<li>Fit standard logistic regression model</li>
<li>Using prediction threshold as 0.5, identify all positive predictions</li>
<li>Assign weight 1 for positively predicted observations, 0 for all others</li>
<li>Fit weighted logistic regression model</li>
</ol>

<p>What would be the flaws with this approach?  What would be the correct way to proceed with this problem?</p>

<p>The reason for wanting to maximize the difference between the number of true positives and false negatives is due to the design of my application.  As part of a class project, I am building a autonomous participant in an online marketplace - if my model predicts it can buy something and sell it later at a higher price, it places a bid.  I would like to stick to logistic regression and output binary outcomes (win, lose) based on fixed costs and unit price increments (I gain or lose the same amount on every transaction).  A false positive hurts me because it means that I buy something and am unable to sell it for a higher price.  However, a false negative doesn't hurt me (only in terms of opportunity cost) because it just means if I didn't buy, but if I had, I would have made money.  Similarly, a true positive benefits me because I buy and then sell for a higher price, but a true negative doesn't benefit me because I didn't take any action.</p>

<p>I agree that the 0.5 cut-off is completely arbitrary, and when I optimized the model from step 1 on the prediction threshold which yields the highest difference between true/false positives, it turns out to be closer to 0.4.  I think this is due to the skewed nature of my data - the ratio between negatives and positives is about 1:3.</p>

<p>Right now, I am following the following steps:</p>

<ol>
<li>Split data intto training/test</li>
<li>Fit model on training, make predictions in test set and compute difference between true/false positives</li>
<li>Fit model on full, make predictions in test set and compute difference between true/false positives</li>
</ol>

<p>The difference between true/false positives is smaller in step #3 than in step #2, despite the training set being a subset of the full set.  Since I don't care whether the model in #3 has more true negatives and less false negatives, is there anything I can do without altering the likelihood function itself?</p>
"
"0.187256335179708","0.0886796350347864"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.149209419390598","0.0824385620013739"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0","0.0311588476424878"," 74304","<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
"
"0.113960576459638","0.0539687072220866"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0","0.0311588476424878"," 76155","<p>I'm dealing with some data where there are some infrequently occuring categorical variables related to a binary prediction target. </p>

<p>For example marketing partners... some send 1000s of leads but many others send less than 10 a minuscule number in a data set with 20K+ examples.</p>

<p>Typically, we deal with this by grouping all the infrequently occurring partners under the variable ""Else"" and using this as the reference category in logistic regression.</p>

<p>I'm wondering what the pros and cons are of handling the issue like this and if there is a better way to deal with it in pre-processing phase.</p>
"
"0.127738077005317","0.10370291340864"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.0930484210398471","0.0440652649239232"," 76850","<p>I am trying to do a multinomial logistic regression on some data that I generated. I am using R and the package mlogit. My data looks like the following:</p>

<pre><code>Class X1 X2  X3
V +0.0655197 +0.6418541 +1.8110291
V-0.6713268 -0.0262458 -0.3602958
V +0.2357610 -0.3602958 -0.6943458
M +0.3900129 +0.5583416 -1.7800082
M +0.5714871 -0.2767833 +0.5583416
M +1.0732807 -1.7800082 -0.3602958
S +0.9553640 -0.3602958 +0.6418541
S +0.1139899 +0.1356030 +0.3889280
S +0.4717283 -0.2852090 -1.1229880
</code></pre>

<p>My model is</p>

<pre><code>Class = B1*X1 + B2*X2 + B3*X3
</code></pre>

<p>So far, I have:</p>

<pre><code>library(mlogit);
allData &lt;- read.table(""Features/AllFeatures.dat"", header=TRUE);
allData$Class&lt;-as.factor(allData$Class);
mlData&lt;-mlogit.data(allData, choice=""Class"");
myData&lt;- mlogit(Class~1|X1 + X2 + x3, data = mlData);
print(summary(myData));
</code></pre>

<p>Which gives me:</p>

<pre><code>Coefficients :
                           Estimate Std. Error t-value  Pr(&gt;|t|)    
S:(intercept)                -0.6832392  0.0951834 -7.1781 7.068e-13 ***
V:(intercept)                -0.6696254  0.0943282 -7.0989 1.258e-12 ***
S:X1                         -0.1362492  0.1134039 -1.2015    0.2296    
V:X1                         -0.0052649  0.1128722 -0.0466    0.9628    
S:X2                         -0.0198451  0.0973608 -0.2038    0.8385    
V:X2                          0.0183261  0.0974789  0.1880    0.8509    
S:X3                          0.1728694  0.1110473  1.5567    0.1195    
V:X3                          0.0230260  0.1101147  0.2091    0.8344
</code></pre>

<p>However in the following: <a href=""http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf</a></p>

<p>The author gets:</p>

<pre><code>Coefficients :
Estimate Std. Error t-value Pr(&gt;|t|)
ic -0.00623187 0.00035277 -17.665 &lt; 2.2e-16 ***
oc -0.00458008 0.00032216 -14.217 &lt; 2.2e-16 ***
</code></pre>

<p>How can I change my function call to get the same? I want the actual coefficients for the model, not for the comparisons.</p>

<p>Also: What is the difference between 'wide' and 'long'? What form is my data in?</p>

<p>Also: Do you have some mathematical references for this type of multinomial logistic regression aimed at engineers?</p>

<p>Thanks!</p>
"
"0.0930484210398471","0.0881305298478463"," 76935","<p>I have data from a psychology experiment in which participants were assigned to one of 3 training conditions and then gave responses for 16 trials for each combination of two within-subjects factors, section and format.  Each response is classified as correct or incorrect.  I originally calculated a % correct for each participant for each combination of section and format, then used a mixed ANOVA to analyze these percentage scores with condition as a between-subjects factor and section &amp; format as within-subjects factors.  However, a reviewer of my manuscript commented that ANOVA is inappropriate for this accuracy data because it's not a genuine continuous variable and has various properties making it inappropriate to use ANOVA.  I think logistic regression WOULD be applicable to this data because it is, after all, binary choice data originally.  Assuming that's right (please tell me if not), how can I do this in R?</p>

<p>Specifically, I need to do logistic regression with correctness/incorrectness of responses as my DV, condition as a b-s factor, and section &amp; format as w-s factors.  (I do not have any continuous predictors.)  Here is what my data looks like:</p>

<pre><code>nsubj       = 150
nsection    = 3
nformat     = 3
ntrials     = 16
subjid      = rep( 1:nsubj, each=nformat*nsection )
condition   = c( replicate( nsubj, rep( sample( c( 'condition 1', 'condition 2', 'condition 3' ), 1 ), nformat*nsection ) ) )
section     = rep( rep( c( 'a', 'b', 'c' ), each=nformat ), nsubj )
format      = rep( rep( c( 'x', 'y', 'z' ), nsection ), nsubj )
nCorrect    = sample( 1:ntrials, nsubj*nsection*nformat, replace=TRUE )
nIncorrect  = ntrials - nCorrect
D           = data.frame( subjid=factor(subjid), condition=factor(condition), section=factor(section), format=factor(format), nCorrect=nCorrect, nIncorrect=nIncorrect )
D$accuracy  = D$nCorrect / (D$nCorrect+D$nIncorrect)
</code></pre>

<p>I tried this, but I know it is wrong because it does not account for the repeated measures, and may inflate significance in other ways (I keep getting significant results from randomly generated data, which seems wrong):    </p>

<pre><code>fit &lt;- glm( cbind( D$nCorrect, D$nIncorrect ) ~ section*format*condition, family=binomial(""logit""), data=D )
anova( fit, test=""Chisq"" )
</code></pre>
"
"0.116310526299809","0.0771142136168656"," 77245","<p>To better ask my question, I have provided some of the outputs from both a 16 variable model (<code>fit</code>) and a 17 variable model (<code>fit2</code>) below (all predictor variables in these models are continuous, where the only difference between these models is that <code>fit</code> does not contain variable 17 (var17)):</p>

<pre><code>fit                    Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
 Obs        102849    LR chi2   13602.84    R2       0.173    C       0.703    
  0          69833    d.f.            17    g        1.150    Dxy     0.407    
  1          33016    Pr(&gt; chi2) &lt;0.0001    gr       3.160    gamma   0.416    
 max |deriv| 3e-05                          gp       0.180    tau-a   0.177    
                                            Brier    0.190       


fit2                 Model Likelihood       Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
 Obs        102849    LR chi2   13639.70    R2       0.174    C       0.703    
  0          69833    d.f.            18    g        1.154    Dxy     0.407    
  1          33016    Pr(&gt; chi2) &lt;0.0001    gr       3.170    gamma   0.412    
 max |deriv| 3e-05                          gp       0.180    tau-a   0.177    
                                            Brier    0.190          
</code></pre>

<p>I used Frank Harrell's <code>rms</code> package to build these <code>lrm</code> models. As you can see, these models do not appear to vary much, if at all, across <em>Discrimination Indexes</em> and <em>Rank Discrim. Indexes</em>; however, using <code>lrtest(fit,fit2)</code>, I was provided with the following results:</p>

<pre><code> L.R. Chisq         d.f.            P 
3.685374e+01     1.000000e+00    1.273315e-09 
</code></pre>

<p>As such, we would reject the null hypothesis of this likelihood ratio test; however, I would assume this is likely due to the large sample size (<em>n</em> = 102849) as these models appear to perform in a similar fashion. Furthermore, I am interested in finding a better way of formally comparing nested binary logistic regression models when <em>n</em> is large.</p>

<p>I greatly appreciate any feedback, R scripts, or documentation that can steer me in the right direction in terms of comparing these types of nested models! Thanks!</p>
"
"0.0930484210398471","0.0440652649239232"," 77540","<p>I am try to evaluate a logistic regression with new data using R. I have created the logistic regression using the old data and evaluated that using chi^2 etc. i.e.</p>

<pre><code>model &lt;- glm(outcome ~ input + 0, data = training.data, family = binomial())
</code></pre>

<p>(note I do need the intercept = 0). And then the predicted probabilities:</p>

<pre><code>predicted.probabilities &lt;- predict(model, test.data, type = ""response"")
</code></pre>

<p>How can I now perform the same sort of tests with these new probabilities? and what are the best tests to do here?</p>
"
"0","0.0311588476424878"," 78360","<p>I need your help with a Statistical Learning homework in R.
I have to perform classification over this dataset: <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/"" rel=""nofollow"">mammographic masses</a> predicting Severity (0=""not severe"",1 = ""severe) using these predictors:</p>

<ul>
<li>Age (quantitative)</li>
<li>Margin (qualitative)</li>
<li>Shape (qualitative)</li>
</ul>

<p>Everything is fine and understandable when I use logistic regression, but I don't know if it's possible to run QDA (or linear discriminant analysis either), since two of the variables are qualitative.</p>
"
"0","0.0311588476424878"," 80463","<p>I have the following set of model-averaged fixed effects from a set of binomial GLMMs: </p>

<p><img src=""http://i.stack.imgur.com/yN4wR.png"" alt=""model parameters image""></p>

<p>I would like to plot the predicted effect of ""NBT"", along with confidence bands, while holding all the other variables at their baseline levels. My attempt to do this in ggplot:</p>

<pre><code>Xvars &lt;- seq(from=0, to=100, by=0.1)  #NBT range is 0-100
  binomIntercept &lt;- 1.317
  binomSlope &lt;- -0.0076     
  binomSE &lt;- 0.009    
Means &lt;- logistic(binomIntercept + binomSlope*Xvars)              
loCI &lt;- logistic(binomIntercept + (binomSlope - 1.96*binomSE)*Xvars)
upCI &lt;- logistic(binomIntercept + (binomSlope + 1.96*binomSE)*Xvars)
df &lt;- data.frame(Xvars,Means,loCI,upCI)
p &lt;- ggplot(data=df, aes(x = Xvars, y = Means)) + 
geom_line() +          
geom_line(data=df, aes(x = Xvars, y = upCI),col='grey') +
geom_line(data=df, aes(x = Xvars, y = loCI), col='grey')
p                                            
</code></pre>

<p><img src=""http://i.imgur.com/eMJBxQQ.png"" alt=""graph image""></p>

<p>I'm assuming that the confidence bands are cone shaped because I'm not accounting for uncertainty in the estimate for the intercept. Maybe this is okay (?), but it does look different from every regression line I've ever seen with confidence intervals plotted.</p>

<p>Can someone please tell me how I should be writing my equations to get the correct confidence intervals, given the intercept, slope, and standard errors from my model output?</p>

<p>(I know I can use the predict function to do this in R, but would like to know how to do it by hand.)  </p>
"
"0.0930484210398471","0.0440652649239232"," 80783","<p>I have 3 covariates $(x_1,x_2,x_3)$, each covariate takes only 2 values $\{0,1\}$ but each covariates have unequal sample size. </p>

<p>$n(x_1)=79,\;n(x_2)=80,\; n(x_3)=77$</p>

<p>$x_1$:(60 zeros and 19 ones)    </p>

<p>$x_2$: (56 zeros and 24 ones)    </p>

<p>$x_3$: (34 zeros and 43 ones)</p>

<p>My goal is to estimates: $$p = \frac{1}{1+e^{-(b_0+b_1x_1+b_2x_2+b_3x_3)}}$$  and fit a logistic regression model using all these three $x_i$'s , how can I do this?</p>
"
"0.1176979772673","0.0696733014291618"," 80880","<p>I have seen several articles and CrossValidated questions on bootstrapping ( <a href=""http://stats.stackexchange.com/questions/41625/can-i-use-boostrapping-why-or-why-not"">this</a>, <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">this</a> or <a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients"">this</a> for example); there are a lot of theoretical and statistical explanations, however since they are so theory based, I am afraid I might be understanding the use wrongly. Hence my questions:</p>

<p>1) When I make a non-parametric bootstrapping (changing the sample for every run) with logistic regression on my data, I basically will end up with several different coefficients for each predictor for each run. Eventually I'll have the confidence interval for each predictor as well. I understand until that point. My question is; assuming that the distribution is normal, when I want to come up with a final model on practice, can I just take the mean of the confidence intervals for each predictor and consider this as my final model coefficient?</p>

<p>2) If the answer to question #1 is yes, is this the only way of choosing coefficients while bootstrapping? If not, what else? I encountered in a few more articles a method called ""bagging"". This seems to be my main purpose. </p>

<p>3) This one is more of a curiosity question: Can above methodology be applied to the categorical predictors when they are assigned with Weight Of Evidences? I know we can split the  categorical predictors into dummy variables; but how would I treat each coefficient if I want to use WOE methodology?</p>
"
"0.0912414835752265","0.10370291340864"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0882734829504749","0.0557386411433294"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.147122471584125","0.0696733014291618"," 83260","<p>I used the clogit function (from the survival package) to run a conditional logistic regression in R with a big dataset of 1:M matched pairs with n=300368964 and number of events= 39995.</p>

<pre><code>model &lt;- clogit(Alliance ~ OVB + CVC + BVB + strata(Strata), method=""exact"")    
</code></pre>

<p>I received following results:</p>

<pre><code>                 coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
OVB        -0.0498174  0.9514031  0.0166275  -2.996  0.00273 ** 
BVB         0.0277405  1.0281289  0.0304956   0.910  0.36300    
CVC         1.1709851  3.2251683  0.1089709  10.746  &lt; 2e-16 ***
EarlyStage -1.3215824  0.2667129  0.0205851 -64.201  &lt; 2e-16 ***
AvgVCSize   0.0087976  1.0088364  0.0002035  43.224  &lt; 2e-16 ***
NumberVC    0.0643579  1.0664740  0.0034502  18.653  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Rsquare= 0   (max possible= 0.001 )
Likelihood ratio test= 6511  on 6 df,   p=0
Wald test            = 6471  on 6 df,   p=0
Score (logrank) test = 6801  on 6 df,   p=0
</code></pre>

<p>Since Rsquare equals 0 and the test ratios seems very high, I tried to plot the results to check whether the model fits. But I wasn't able to plot it properly.</p>

<p>I would online many papers which use the ratio Prob > chi2 = 0 from Stata as test ratio to proof the model fit. </p>

<p>How could I calculate this ratio in R? Are there any other ways I could check the model fit of my clogit results?</p>

<p>I would appreciate any help.</p>

<p>Thanks you very much in advance.</p>
"
"0.0657951694959769","0.0311588476424878"," 83364","<p>I have been reading a number of papers where researchers have created risk scores based on logistic regression models. Often they refer to ""<a href=""http://www.ncbi.nlm.nih.gov/pubmed/15122742"" rel=""nofollow"">Sullivan's method</a>"" but I have no access to this paper and the explanations provided are far from clear. I noticed that Dr. Harrell's excellent RMS package provides a nomogram function which is in a way similar to creating a risk score (albeit with a very pretty graphical output).</p>

<p>It seems after tinkering around with it that the way it works is by dividing the beta coefficients by the smallest beta coefficient and then multiplying a constant to create points for categorical variables. However I cannot for the life of me figure out what is going on with continuous variables. I've spent hours searching google without much luck, and I would appreciate if someone could shed some light on this for me. Thanks!</p>
"
"0.0657951694959769","0.0311588476424878"," 83852","<p>I have a dataset for vehicles and trying to predict what will fail. Here is my data set</p>

<pre><code>vId, MileageSincePartLastReplaced,  AgeOfPart, TypeOfVehicle, Failure

x,100000,200days,Truck,Alt Belt

y,200000,600days,PCar,Transmission Belt

z,140000,230days,Van,Fan Belt
</code></pre>

<p>Failure is outcome variable with 20 different types of failure categories. What I am looking for is given mileage driven and age of the part which one is likely to fail?</p>

<p>I am unable to figure out which model/models I should be looking for? I was looking into multinomial regression &amp; ordered logistic regression  but was not sure. Any help around how to go about this and which package I could use?  </p>

<p>Note: I asked same question in Stack Overflow and was suggested to move to this forum.</p>
"
"0.0746047096952991","0.0588846871438385"," 83908","<p>I am working in program R. I am modeling the incidence of flight in a seabird in relation to distance to the nearest ship (potential disturbance, range = 0 to 74 km from the bird). 1= flight during observation, 0 = no flight. The bird does fly with some unknown probability when no ships are present or really ""far"" away. I am trying to find this really far distance and associated probability of flight using binary logistic regression.</p>

<p>Model = Flight ~ ship distance. Other variables were explored but fell out with stepwise selection.</p>

<p>During exploratory analysis I truncated the data down only looking at smaller distances from the ship (20, 15, 10 km). These models are highly significant and predict that as the ship gets closer the probability of flight increases. However when I include all the data (out to 74 km) the intercept is significant (and predicts the true % of observed flight events) but the slope term is non-significant. </p>

<p>Can I use a weighting scheme to give more weight to observations when the ship was closer?</p>

<p>Thanks.</p>

<p>Edit: I am working through the suggestions made by @Scortchi and @Underminer. Here is a plot of a loess smooth on the observed data to better help visualize the pattern. </p>

<p><img src=""http://i.stack.imgur.com/ZabQh.jpg"" alt=""Loess smooth of probability of flight as a function of distance to nearest ship""></p>

<p>The distance to the ship data does not discriminate between approaching ships and departing ships it is just a straight line measure to the nearest ship. The dip in the probability of flight at 8.5 I believe can be attributed to ""unaffected"" birds that did not fly as the ship passed by them. So as the ship departs and gets further from the observation site we were more like to be observing birds that for whatever reason did not fly when the ship passed and are less likely to fly for ""naturally occurring"" reason. As additional birds fill back into the observation area the ""baseline"" flushing rate is resumed and birds start to fly at ""normal"" probabilities. </p>
"
"0.0465242105199235","0.0440652649239232"," 83945","<p>How do you solve the following problem?</p>

<blockquote>
  <p>A Simulation Study (Probit Regression).</p>
  
  <p>Assume $y|x\sim {\rm Binary}(p)$, where $p= E(y|x)$, and $Î¦^{-1}(\pi)=-1+5.1x_{1i}-0.3x_{2i}$
  Generate data with $x_{1i}\sim{\rm Unif}(0,1)$, $x_{2i}=1$ for $i$ odd and $x_{2i}=0$ for $i$ even, and sample size $n=500$. Try generalized linear model (GLM) with logistic and probit links.</p>
</blockquote>

<p>Here is what I did, I know there is a problem, but I don't know what:</p>

<pre><code>n         &lt;- 500
beta0     &lt;- -1
beta1     &lt;- 5.1
beta2     &lt;- -0.3
x1        &lt;- runif(n=n, min=0, max=1)
x2        &lt;- (1:n)%%2
y         &lt;- pnorm(beta0 + beta1*x1 + beta2*x2)
prob.glm  &lt;- glm(y~x1+x2, family=binomial(link=probit))
logit.glm &lt;- glm(y~x1+x2, family=binomial)
</code></pre>

<p>I know <code>y</code> is a probability here, but how do you simulate a binary variable from the probability? </p>
"
"0.0657951694959769","0.0467382714637317"," 84166","<p>I recently read a paper made a logistic regression and used a table like this to summarise the model:</p>

<pre><code>data.frame(predictors = c(""drat"", ""mpg""), ""chi squared statistic"" = c(""x"", ""x""), ""p-value"" = c(""x"", ""x""))

  predictors chi.squared.statistic p.value
1       drat                     x       x
2        mpg                     x       x
</code></pre>

<p>The data.frame presented the chi-squared and p value (the x's) for each predictor in the logistic regression model, and thus allowed to see at a glance which predictors were most important.</p>

<p>I have made this logistic regression model:</p>

<pre><code>mtcars_log_reg &lt;- glm(vs ~ drat + mpg, mtcars, family = ""binomial"")
</code></pre>

<p>How can I fill in the x's for the above table using R code?</p>
"
"0.080582296402538","0.0763232776972177"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.0657951694959769","0.0311588476424878"," 85890","<p>I have a logistic regression model with several variables. I want to test the hypothesis of, say $a \beta_i = b \beta_j$ for constants $a,b$. I know that this can be theoretically done with an $F$-test; but how do I technically do it in R?</p>

<p>What about in the case where I have a factor variables, to the different variables are in fact the dummy variables corresponding to the different levels?</p>
"
"0.172292196901579","0.142787777889297"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.170940864689457","0.107937414444173"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.149209419390598","0.0824385620013739"," 87872","<p>When I do a (logistic) regression in R, I run something like this:</p>

<pre><code>mydata &lt;- read.csv(""data.csv"")
mylogit &lt;- glm(a ~ c+d, data = mydata, family=""binomial"")
summary(mylogit)
</code></pre>

<p>As of a few months ago, the output for the coefficients might look like this:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6476     0.1898  -8.680  &lt; 2e-16 ***
c             2.4558     0.3414   7.194 6.29e-13 ***
d             2.3783     0.4466   5.326 1.01e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Trying it today (with a newer version of R), the output looks like the following:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6709     0.1924  -8.683  &lt; 2e-16 ***
c1            2.4961     0.3476   7.181 6.94e-13 ***
cc           18.2370   979.6100   0.019    0.985    
d1            2.4524     0.4630   5.296 1.18e-07 ***
dd                NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the ""c1"", ""cc"", etc fields mean?  I can't seem to find this any documentation, but perhaps I am looking in the wrong places?</p>
"
"0.0986927542439653","0.0623176952849756"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.113960576459638","0.0539687072220866"," 88036","<p>I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).</p>

<ol>
<li><p>I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  </p>

<p><code>output &lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)</code><br>
RE is a factor with several levels.</p>

<p>This works. But I'm wondering why it's necessary to use the <code>(1 | RE)</code> syntax instead of just <code>DV~IV1+IV2 | RE</code>.</p></li>
<li><p>I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using <code>ranef()</code>. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?</p></li>
</ol>
"
"0.0657951694959769","0.0311588476424878"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"0.109658615826628","0.0830902603799674"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.0657951694959769","0.0311588476424878"," 89130","<p>Consider this logistic regression:</p>

<pre><code>mtcars$vs &lt;- as.factor(mtcars$vs)
log_reg_mtcars &lt;- glm(am ~ vs*wt +vs*mpg, family = ""binomial"", mtcars)
</code></pre>

<p>I tried using the effects package to extract some coefficients from the model:</p>

<pre><code>as.data.frame(effect(""vs"", log_reg_mtcars)) 

NOTE: vs is not a high-order term in the model
  vs       fit       se       lower     upper
1  0 0.3957869 2.916877 0.002150238 0.9950031
2  1 0.0336822 2.181009 0.000484832 0.7146704
</code></pre>

<p>Could anyone explain why the coefficients/standard error for <code>vs</code> given by this code are different. Or in other words, what is the effects package doing?</p>

<pre><code>as.data.frame(summary(log_reg_mtcars)$coef)[2,1:2]

     Estimate Std. Error
vs1 -40.70047   40.53197
</code></pre>
"
"0.0746047096952991","0.0824385620013739"," 89474","<p>I ran this ordinal logistic regression in R:</p>

<pre><code>mtcars_ordinal &lt;- polr(as.factor(carb) ~ mpg, mtcars)
</code></pre>

<p>I got this summary of the model:</p>

<pre><code>summary(mtcars_ordinal)

Re-fitting to get Hessian

Call:
polr(formula = as.factor(carb) ~ mpg, data = mtcars)

Coefficients:
      Value Std. Error t value
mpg -0.2335    0.06855  -3.406

Intercepts:
    Value   Std. Error t value
1|2 -6.4706  1.6443    -3.9352
2|3 -4.4158  1.3634    -3.2388
3|4 -3.8508  1.3087    -2.9425
4|6 -1.2829  1.3254    -0.9679
6|8 -0.5544  1.5018    -0.3692

Residual Deviance: 81.36633 
AIC: 93.36633 
</code></pre>

<p>I can get the log odds of the coefficient for <code>mpg</code> like this:</p>

<pre><code>exp(coef(mtcars_ordinal))
 mpg 
0.7917679 
</code></pre>

<p>And the the log odds of the thresholds like:</p>

<pre><code>exp(mtcars_ordinal$zeta)

       1|2         2|3         3|4         4|6         6|8 
0.001548286 0.012084834 0.021262900 0.277242397 0.574406353 
</code></pre>

<p>Could someone tell me if my interpretation of this model is correct:</p>

<blockquote>
  <p>As <code>mpg</code> increases by one unit, the odds of moving from category 1 of <code>carb</code> into any of the other 5 categories, decreases by -0.23. If the log odds crosses the threshold of 0.0015, then the predicted value for a car will be category 2 of <code>carb</code>.  If the log odds crosses the threshold of 0.0121, then the predicted value for a car will be category 3 of <code>carb</code>, and so on.</p>
</blockquote>
"
"0.029424494316825","0.0557386411433294"," 89582","<p>I have made these three ordinal logistic regression models:</p>

<pre><code>model1 &lt;-  polr(as.factor(carb) ~ mpg,  Hess = T, mtcars)
model2 &lt;-  polr(as.factor(carb) ~ hp,   Hess = T, mtcars)
model3 &lt;-  polr(as.factor(carb) ~ drat, Hess = T, mtcars)
</code></pre>

<p>To figure out if the models are a good fit to the data, I calculated the proportion of variation explained like this:</p>

<pre><code>model_null &lt;-  polr(as.factor(carb) ~ 1, Hess = T, mtcars)

1-(model1$deviance/model_null$deviance)
0.1512784
1-(model2$deviance/model_null$deviance)
0.2520109
1-(model3$deviance/model_null$deviance)
0.003453936
</code></pre>

<p>Questions: </p>

<ol>
<li><p>Why doesn't summary give null deviance? </p></li>
<li><p>Have I calculated the proportion of variation explained correctly?</p></li>
<li><p>Am I right in saying <code>model1</code> and <code>model3</code> explain little variation in <code>carb</code>, but <code>model2</code> explains 25% of the variation in <code>carb</code>?</p></li>
</ol>
"
"0.166450075715296","0.0985329278164293"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.0465242105199235","0.0440652649239232"," 90000","<p>I want to find the sample size for logistic regression where I have a covariate with 15 levels and the covariates interacts with time, which means that the effect of the covariates is different for different periods of times. Can anyone help me find the sample size and effect size? I wish I could do this using simulations. Is there a code in SAS or R?</p>
"
"0.0465242105199235","0.0440652649239232"," 90536","<p>We have two huge correlation matrices at different experimental conditions. If we want to identify the significant differences between these matrices , what would be the ideal method. I have implemented PCA for dimentionality reduction for the two matrices, and compared both row by row. Will it give some information regarding the differences. More over how about LDA and logistic regression. Will it give any information.   </p>
"
"0.175844987113332","0.0999306278098956"," 90906","<p>I'm new to R and logistic regression and have to admit that I don't really know how to interpret the result. I'm trying to compute a pretty simple model with 2 predictors (A and B). When I first try to compute models with the predictors one by one they are both significant. When I put them together and add an interaction term they lose their significance (but the interaction term is weakly significant). I interpret this as A and B are overlapping and no longer significant when the oter parameter is hold constant. Right?</p>

<p>But now to the part I don't know how to interpret. I make predictions from my models (see code below) and then run t-tests for the predictions vs. the depending variable. I think this should give a hint on how good the model is (is there a better way?). When I do it this way I get a much lower p-value for the model with both A and B. I think this is contradictory. The first part tells me that A doesn't provide any significant information to the model when combined with B, but on the other hand I get much better predictions. I guess something is really wrong, but I can't figure out what. Can you help me?</p>

<pre><code>model1=glm(f~A, , family=binomial(link=""logit""))
model2=glm(f~B,   family=binomial(link=""logit""))
model3=glm(f~A*B, family=binomial(link=""logit""))
summary(model1)
summary(model2)
summary(model3)
p1=predict(model1, newdata=data, type=""response"", na.rm=TRUE)
p2=predict(model2, newdata=data, type=""response"", na.rm=TRUE)
p3=predict(model3, newdata=data, type=""response"", na.rm=TRUE)
t.test(p1~f)
t.test(p2~f)
t.test(p3~f)
</code></pre>

<p>Part of the output:  </p>

<pre><code>&gt; summary(model1)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.9756     0.3499  -5.647 1.64e-08 ***
A            -0.5898     0.2119  -2.784  0.00537 ** 

&gt; summary(model2)
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  8.354e-01  1.309e+00   0.638   0.5234  
B           -1.028e-04  5.122e-05  -2.007   0.0447 *

&gt; summary(model3)
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  1.254e+00  1.705e+00   0.735    0.462  
A            1.589e+00  9.743e-01   1.631    0.103  
B           -1.324e-04  7.333e-05  -1.805    0.071 .
A:B         -9.418e-05  4.632e-05  -2.033    0.042 *

&gt; t.test(p1~f)
t = -2.614, df = 11.83, p-value = 0.02286

&gt; t.test(p2~f)
t = -1.8702, df = 15.679, p-value = 0.08024

&gt; t.test(p3~f)
t = -4.9777, df = 17.344, p-value = 0.0001084
</code></pre>
"
"0.0930484210398471","0.0440652649239232"," 90926","<p>I would like to determine what variables from this sample data would be best predictors for CallHandleTimeSeconds. </p>

<p>Im thinking it would be a combination of CreditRating, EligibleForAssistance, TypeOfCall, AmtInArrears but unsure about how to do this. I understand the process when all the variables are numeric but categorical variables make my head spin! Please help, because I learn best from examples then I can basically plug and play other categorical variables in the future.</p>

<p>Like if CreditRating = Good;, EligibleForAssistance = T, TypeOfCall = 2, and AmtInArrears = 21 then CallHandleTimeSeconds = 432?????</p>

<pre><code> CreditRating = c(""Poor"", ""Poor"", ""Good"", ""Good"", ""Average"", ""Poor"", ""Average"") 
 EligibleForAssistance = c(""T"", ""F"", ""T"", ""F"", ""T"", ""T"",""T"") 
 Season = c(1,2,1,3,2,3,4)
 TypeOfCall = c(1,1,2,3,3,1,2)
 NumberOfDaysAccountOpen = c(111,2321,33,322,2321,343,785)
 AmtInArrears = c(0,0,0,22,232,2,0)
 CallHandleTimeSeconds= c(123,232,543,239,230,400,210)

 SampleData = data.frame(CreditRating,      EligibleForAssistance,Season,TypeOfCall,NumberOfDaysAccountOpen,AmtInArrears,CallHandleTimeSeconds) 
</code></pre>

<p>What test would I run? Logistic Regression? Please help.</p>
"
"0.131590338991954","0.0467382714637317"," 90953","<p><em>R in Action</em> (Kabacoff, 2011) suggests the following routine to test for overdispersion in a logistic regression:</p>

<p>Fit logistic regression using binomial distribution:</p>

<pre><code>model_binom &lt;- glm(Species==""versicolor"" ~ Sepal.Width,
                   family=binomial(), data=iris)
</code></pre>

<p>Fit logistic regression using quasibinomial distribution:</p>

<pre><code>model_overdispersed &lt;- glm(Species==""versicolor"" ~ Sepal.Width, 
                           family=quasibinomial(), data=iris)
</code></pre>

<p>Use chi-squared to test for overdispersion:</p>

<pre><code>pchisq(summary(model_overdispersed)$dispersion * model_binom$df.residual, 
       model_binom$df.residual, lower = F)
# [1] 0.7949171
</code></pre>

<p>Could somebody explain how and why the chi-squared distribution is being used to test for overdispersion here? The p-value is 0.79 - how does this show that overdispersion is not a problem in the binomial distribution model?</p>
"
"0.0657951694959769","0.0311588476424878"," 91701","<p><img src=""http://i.stack.imgur.com/OQpth.png"" alt=""enter image description here""></p>

<p>I have these data plotted above. The explanatory variable represents intensity levels of ground shaking at different locations in an earthquake, and the response variable represents amounts of compensation awarded to damaged buildings. I want to fit a model to these data so that how much compensation is likely to be awarded can be estimated given an intensity level. Can I use logistic regression here? How should I go about building the model for these data? Thanks in advance. </p>
"
"0.0465242105199235","0.0440652649239232"," 91747","<p>I have a logistic regression with data that are kind of like this:</p>

<pre><code>y &lt;- rep(c(""A"", ""A"", ""B""), each = 30)
x &lt;- c(     rep(1, 12), rep(2, 18), rep(3, 16), rep(4, 12), rep(5, 2),
            rep(1, 3), rep(2, 5), rep(3, 8), rep(4, 10), rep(5, 4)  )

da &lt;- data.frame(y = y, x = x)
table(da)
   x
y    1  2  3  4  5
  A 12 18 16 12  2
  B  3  5  8 10  4
</code></pre>

<p>I'd like to show that there are more A's than B's in <code>y</code> (instead of a Bernoulli with <code>p</code> = 0.5), even after controlling for <code>x</code>, so I fitted two logistic regression models and used an ANOVA to compare them.</p>

<pre><code>mlogis.x              &lt;- glm(y ~ x,     family = binomial, da)
mlogis.x.no_intercept &lt;- glm(y ~ x + 0, family = binomial, da)

summary(mlogis.x.no_intercept)
summary(mlogis.x)

anova(mlogis.x.no_intercept, mlogis.x, test = ""Chisq"")
</code></pre>

<p>I have a few questions:</p>

<ul>
<li>Does what I did make sense overall?</li>
<li>Is it okay to not have an intercept in the more basic model and then add it to the full(er) model?</li>
<li>The coefficient for <code>x</code> changes sign between the two models, how should I interpret this?</li>
</ul>
"
"NaN","NaN"," 91847","<p>Suppose I have a logistic regression model such like this:</p>

<pre><code>set.seed(123)
df&lt;-data.frame(
y=rbinom(100,1,0.5),
x1=rnorm(100,10,2),
x2=rbinom(100,20,0.6))

fit&lt;-glm(y~x1*x2,data=df,family=""binomial"")
coef(summary(fit))
               Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)  5.08314564 6.43692399  0.7896855 0.4297115
x1          -0.66691041 0.64071095 -1.0408912 0.2979260
x2          -0.28338654 0.51254819 -0.5528974 0.5803337
x1:x2        0.04037126 0.05100223  0.7915588 0.4286180
</code></pre>

<p>Does somebody know how to get the prediction matrix in a format like this:</p>

<pre><code>    intercept x1       x2  x1:x2
1   1        10.506637 10  105.06637
2   1        9.942906  17  169.02941
3   1        9.914259  10  99.14259
4   1        12.737205 11  140.10925
</code></pre>
"
"0.162834736819732","0.0771142136168656"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.131590338991954","0.0623176952849756"," 92575","<p>I'm pretty new to stats, so this may be dumb.
I've been running a bunch of models on randomly generated data to try and develop my understanding of type 1 error.</p>

<p>I've noticed that using <code>glm(family=binomial)</code> I get more type 1 errors than I should when giving a binomial input (a two-column matrix of success and failure). In the code below, the first loop generates a thousand logistic regression models for random data, but I get type 1 errors (p &lt; .05) about 15% of the time!</p>

<p>The second loop runs the a similar thing as a Bernoulli test (just single vector of zeros and ones in the y). Here I get what I want to see, about 50 type 1 errors per 1000 models.</p>

<p>Can anyone explain this to me? I see that if I change the possible values for the random numbers in my success/failure y-matrix (the variable I call range here) I can lower the type 1 errors, but I don't understand why. </p>

<p>This gives me about triple the number of type 1 errors that I expect.</p>

<pre><code>#Binomial glm
fit.p=c()
for(i in 1:1000){
    range=0:10
    y=matrix(sample(range,2000,replace=T),ncol=2,nrow=1000)
    x=rnorm(1000,100,50)      
    fit=glm(y~x,family=binomial(link='logit'))               
    fit.p[i]=anova(fit,test='Chisq')[2,5]                            
}
print(length(which(fit.p&lt;.05)))
</code></pre>

<p>This works fine. About 50 errors per 1000</p>

<pre><code>#Bernoulli glm
fit2.p=c()
for(i in 1:1000){
    y=sample(0:1,1000,replace=T)
    x=rnorm(1000,100,50)      
    fit2=glm(y~x,family=binomial(link='logit'))               
    fit2.p[i]=anova(fit2,test='Chisq')[2,5]                            
}
print(length(which(fit2.p&lt;.05)))
</code></pre>
"
"0.21711298242631","0.13219579477177"," 92737","<p>In my data, I have two treatment conditions with repeated measures for each subject. I would like to run a mixed logistic regression separately for each of my two conditions where my binary outcome DV (dependent variable) is regressed on my IV (independent variable) and also have a random slope and intercept fitted for each subject.</p>

<p>So, I run the following:</p>

<pre><code>modelT0 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D0, family = binomial)
modelT1 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D1, family = binomial)
</code></pre>

<p>In the above, D0 and D1 are data sets restricted to treatment conditions 0 and 1, respectively. What I would like to do is compare the estimated fixed effects coefficient on IV across conditions to see if it significantly changes. To do this, I pool D0 and D1 into a single data set, D, and create a treatment indicator that takes value 0 in D0 and 1 in D1. I then run:</p>

<pre><code>model &lt;- glmer(DV ~ IV + treatment + treatment:IV + (1 + treatment|subject:treatment) 
               + (0 + IV + treatment:IV|subject:treatment), data = D, family = binomial)
</code></pre>

<p>I should be able to look at the fixed effects coefficient on treatment:IV to get my answer, but the issue is that for whatever combination of random effects I seem to specify, the coefficients from the pooled regression are slightly different from the regressions specified by treatment. So for instance, the fixed effect coefficient on treatment:IV plus the one on IV in model is not equal to the coefficient on IV in model1.</p>

<p>Any idea about what I might be doing wrong or how to answer the question I have? Thanks!</p>

<p>EDIT:</p>

<p>As per Henrik's suggestion, I'm copying the random effects output of the models below:</p>

<p>summary(modelT0):</p>

<pre><code>    Random effects:
    Groups    Name        Variance  Std.Dev. 
    subject   (Intercept) 1.412e-07 0.0003758
    subject.1 IV          1.650e+00 1.2844341
</code></pre>

<p>summary(modelT1):</p>

<pre><code>    Random effects:
    Groups    Name        Variance Std.Dev.
    subject   (Intercept) 0.00378  0.06148 
    subject.1 IV          0.26398  0.51379 
</code></pre>

<p>summary(model):</p>

<pre><code>    Random effects:
    Groups              Name         Variance  Std.Dev. Corr 
    subject.treatment   (Intercept)  0.0005554 0.02357       
                        treatment    0.0066042 0.08127  -0.88
    subject.treatment.1 IV           1.6500112 1.28453       
                        IV:treatment 1.0278663 1.01384  -0.93
</code></pre>
"
"0.0930484210398471","0.0440652649239232"," 92839","<p>I'm currently testing a (binary) logistic regression model, which seems to have at least some issues with multicollinearity. Now I don't really trust the data anymore and would like to also test it on heteroscedasticity. I found some information on Breusch-Pagan Test on the internet, but I could not find an answer to the question if this test also applys on Maximum-Likelihood-Methods, as it is usually mentioned in the context of OLS. So, can I apply the Breusch-Pagan Test on my model?</p>

<p>Related to this question: How could I plot for heteroscedasticity-detection? I found <a href=""http://stats.stackexchange.com/questions/33028/measures-of-residuals-heteroscedasticity"">this thread</a>, but due to the binary nature of my dependent variable, the plot does not really work and unfortunately I'm pretty novice on plotting with R.</p>

<p>Thanks in advance!</p>
"
"0.0657951694959769","0.0311588476424878"," 93202","<p>I  am using a  decision tree and random forest for a classification problem.
The output is  binary {0,1} and some of the input variables are categorical while the others are continuous. </p>

<p>I would like to know if it is possible to extract odds ratio from one of these models. Odds ratio similar to the ones that are commonly obtained using logistic regression.</p>

<p>I am using R with the package <code>rpart</code> and <code>randomforest</code> so the best answer would be a theoretical explanation and an R code. </p>

<p>Thanks!</p>
"
"0.0759737176397586","0.0539687072220866"," 93390","<p>This question is about understanding the logistic regression output using R.  Here is my sample data frame:</p>

<pre><code>    Drugpairs             AdverseEvent  Y    N
1   Rebetol + Pegintron       Nausea   29 1006
2   Rebetol + Pegintron      Anaemia   21 1014
3   Rebetol + Pegintron     Vomiting   14 1021
4   Ribavirin + Pegasys       Nausea    5  238
5   Ribavirin + Pegasys      Anaemia   12  231
6   Ribavirin + Pegasys     Vomiting    1  242
7 Ribavirin + Pegintron       Nausea   15  479
8 Ribavirin + Pegintron      Anaemia    7  487
9 Ribavirin + Pegintron     Vomiting    9  485
</code></pre>

<p>This basically describes the number of times a particular drug pair has caused a medically adverse event. (<code>Y=yes, N=no</code>). I ran a logistic regression on this dataset in R using the following commands:</p>

<pre><code>mod.form    = ""cbind(Y,N) ~ Drugpairs * AdverseEvent""
glmhepa.out = glm(mod.form, family=binomial(logit), data=hepatitis.df)
</code></pre>

<p>The summary output was as follows (only showing the coefficients table):  </p>

<pre><code>                                                      Estimate Std. Error z value
(Intercept)                                          -3.8771     0.2205 -17.586
DrugpairsRibavirin + Pegasys                          0.9196     0.3691   2.491
DrugpairsRibavirin + Pegintron                       -0.3652     0.4399  -0.830
AdverseEventNausea                                    0.3307     0.2900   1.140
AdverseEventVomiting                                 -0.4123     0.3479  -1.185
DrugpairsRibavirin + Pegasys:AdverseEventNausea      -1.2360     0.6131  -2.016
DrugpairsRibavirin + Pegintron:AdverseEventNausea     0.4480     0.5457   0.821
DrugpairsRibavirin + Pegasys:AdverseEventVomiting    -2.1191     1.1013  -1.924
DrugpairsRibavirin + Pegintron:AdverseEventVomiting   0.6678     0.6157   1.085
</code></pre>

<p>I understand that the coefficients give probabilistic odds. I am curious however, as to why there are no coefficients for the <code>AdverseEventAnaemea</code> and also why is there no coefficient for any combination of the drugs and the adverse event anaemea? (The last 4 rows are the combination effects of drugs and adverse events)</p>
"
"0.178541910193085","0.103342206529982"," 93454","<p><strong>Base Data</strong>: I have ~1,000 people marked with assessments: '1,' [good] '2,' [middle] or '3' [bad] -- these are the values I'm trying to predict for people in the future. In addition to that, I have some demographic information: gender (categorical: M / F), age (numerical: 17-80), and race (categorical: black / caucasian / latino).</p>

<p><strong>I mainly have four questions:</strong></p>

<ol>
<li><p>I was initially trying to run the dataset described above as a multiple regression analysis. But I recently learned that since my dependent is an ordered factor and not a continuous variable, I should use ordinal logistic regression for something like this. I was initially using something like <code>mod &lt;- lm(assessment ~ age + gender + race, data = dataset)</code>, can anybody point me in the right direction?</p></li>
<li><p>From there, assuming I get coefficients I feel comfortable with, I understand how to plug solely numerical values in for x1, x2, etc. -- but how would I deal with race, for example, where there are multiple responses: black / caucasian / latino? So if it tells me the caucasian coefficient is 0.289 and somebody I'm trying to predict is caucasian, how do I plug that back in since the value's not numerical?</p></li>
<li><p>I also have random values that are missing -- some for race, some for gender, etc. Do I have to do anything additional to make sure this isn't skewing anything? (I noticed when my dataset gets loaded into R-Studio, when the missing data gets loaded as <code>NA</code>, R says something like <code>(162 observations deleted due to missingness)</code> -- but if they get loaded as blanks, it does nothing.)</p></li>
<li><p>Assuming all of this works out and I have new data with gender, age, and race that I want to predict on -- is there an easier way in R to run all of that through whatever my formula with new coefficients turns out to be, rather than doing it manually? (If this question isn't appropriate here, I can take it back to the R forum.)</p></li>
</ol>
"
"0.0657951694959769","0.0311588476424878"," 93708","<p>Suppose I have a data frame such that:</p>

<pre><code>set.seed(2014)
df&lt;-data.frame(y=rbinom(100,1,0.3),futime=as.integer(rnorm(100,100,10)),
     age=rnorm(100,50,5),sex=rbinom(100,1,0.5))
df[df&gt;100]&lt;-100
</code></pre>

<p>where <code>y</code> is the outcomes and <code>futime&lt;100</code> are right censored of follow up time. I would fit a logistic regression model to predict the outcomes as if they are not censored. I saw there are some options such as <code>tobit</code> model, <code>survival</code> model and <code>censReg</code> package, but seems not very similar situation. Would somebody know the best way for this using <code>age</code> and <code>sex</code> as predictors? </p>
"
"0.187256335179708","0.0985329278164293"," 94089","<p>I started to use the function <code>multinom</code> of <code>R</code> package <code>nnet</code> in order to fit several conditional probability distributions with the multinomial logistic model. I need the parameters of the fittings in order to pass them to a Java program, which will compute the probabilities and use them.</p>

<p>My problem is that the probabilities computed with the parameters returned by <code>multinom</code>, following the usual <a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_set_of_independent_binary_regressions"" rel=""nofollow"">definition</a> of multinomial logistic model, are not the same as those directly computed in <code>R</code>, which are the correct ones. On Stack Overflow I have already asked a <a href=""http://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"">question</a> about this issue, but I do not still know how the <code>R</code> function <code>multinom</code> computes these probabilities; my guess is that it relies on neural networks, since this function belongs to <code>R</code> package <code>nnet</code>, but I do not have any idea about the details, and an inspection of the code led to nowhere.</p>

<p>Do you know an <code>R</code> package which fits conditional probabilities and returns the corresponding parameters of the model, so that we may easily compute the probabilities in another program? E.g., using the MARS model (<code>R</code> package <code>earth</code>) or Projection Pursuit Regression (<code>R</code> package <code>ppr</code>) is not feasible, since computing the probabilities from the parameters of these models would be a mess. Besides, the function <code>mlogit</code> from the <code>R</code> package with the same name is not applicable as well, since the dataset should be in a certain format (we would also need the predictors corresponding to the alternative, ""non-chosen"" response variable).</p>
"
"0.166038918045633","0.135818268070785"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.145986373720362","0.11234482285936"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.0465242105199235","0.0440652649239232"," 94619","<p>Stata allows for fixed effects and random effects specification of the logistic regression through the <code>xtlogit fe</code> and <code>xtlogit re</code> commands accordingly. I was wondering what are the equivalent commands for these specifications in R.</p>

<p>The only similar specification I am aware of is the mixed effects logistic regression </p>

<pre><code>&gt; mymixedlogit &lt;- glmer(y ~ x1 + x2 +  x3 + (1 | x4), data = d, family = binomial)
</code></pre>

<p>but I am not sure whether this maps to any of the aforementioned commands.</p>
"
"NaN","NaN"," 95203","<p>I need to perform stepwise binary logistic regression (The horror! The horror!) on 1.5 million observations.  This takes far too long in SAS, so I'm wondering if I can use R to process it in a multicore environment.  Apparently package gmulti (<a href=""http://www.jstatsoft.org/v34/i12/paper"" rel=""nofollow"">http://www.jstatsoft.org/v34/i12/paper</a>) will do the trick, but it's not clear to me if it will do that outside of its genetic algorithm.  That still might work for me, but I don't have a large number of variables (about 30) so it's not necessary.  As long as the results of the brute force and ga approach could be assured to be similar, then I might try it.  However, I see others have had problems getting the parallel feature to run: <a href=""https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html</a>.  Any other suggestions 
on how to parallelize logistic regression in R?  A web search turned up a couple of papers, but not much that seemed specific to R.  And please spare me a lecture about stepwise regression-I'm very well aware of the pitfalls.  I'm replicating someone else's analysis.  I'm using a Windows 64 bit system.</p>
"
"0.0657951694959769","0.0311588476424878"," 95273","<p>I have a model produced by a logistic regression which tragically failed Breusch-Pagan test.</p>

<pre><code>&gt; bptest(lm.fit)

    studentized Breusch-Pagan test

data:  lm.fit
BP = 22837.5, df = 2, p-value &lt; 2.2e-16
</code></pre>

<p>I am hardly looking for alternatives (the response is zero-inflated). Does the fact that the model is clearly heteroscedastic imply that the model is useless or can I still adopt it with some caution? </p>
"
"0.0657951694959769","0.0311588476424878"," 95360","<p>I am working on a logistic regression on some fundraising data where ""gave"" is a rare event (approx 3.5%).  My current model has 64% accuracy on test data and an AUC of .604.</p>

<p>When I run the standard plot(model) I am seeing some outliers on each plot that are segmented completely away from the rest of the data set. What is your first thoughts when you see segregation in the plot like this? Is this indicative of a problem or just the nature of the nature of such a rare event?</p>

<p><img src=""http://i.stack.imgur.com/209Ri.png"" alt=""Residuals vs fitted plot""></p>
"
"0.21711298242631","0.13219579477177"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.164234670435408","0.11234482285936"," 95386","<p>What I have is a medical data set with several variables, all 0-1 variables. I want to make inference about them with logistic regression. I have a few problems:</p>

<ol>
<li><p>I have location variables for the disease. I was advised by my statistic advisor to put them in bins as follows: If it was solely in the right part of the organ then I would mark 1 in the column for right and similarily for left. However if it were in both places I marked in neither of the left and right column but marked one in column both. Using this approach I get error in R, numeric 0 1 error when I use glm in R and I think it is due to how these variables are constructed. Shouldn't I rather have just left and right variables and when we have the disease in both sides I should mark in left and right column and skip the both column and maybe introduce interaction term between left and right (that I would at least do in a linear model).</p></li>
<li><p>Using glm (family binomial for logistic regression) in R I was thinking how to find the best model describing some variable. I started with one usual approach with finding univarietly which variables had p-value less than $0.1$ in Fischer exact test. Then I included those variables in my model and started to delete them after which had the highest p-value. In most medical reasearches I have read when applying multivariate regression I see the usage of p-value $0.05$ but I have a feeling that it might be because of lack of understanding of the subject. When I ranked the model according to AIC and explored the best model I usually got variable with p-value around $0.1$. Which approach is preferably, is it justifyable to just cut of at p-value $0.05$ or should use AIC as an estimator of the best multivariate model? AIC does punish for extra variables and so it shouldnt give one too many variables.</p></li>
</ol>
"
"0.0657951694959769","0.0311588476424878"," 95425","<p>I am trying to calculate ePCP and Brier scores in R for a mixed-effect binary logistic regression. It cannot seem to find any packages that work for mixed models. I have tried the packages OOmisc and ModelGood, and spent much time searching the web with no success.</p>

<p>I am using the lme4 package to fit the regression.</p>

<p>I would be very grateful for any tips.</p>
"
"0.170940864689457","0.107937414444173"," 95451","<p>I've been having an argument with a friend of mine, and it's very possible I'm wrong here.</p>

<p>We are performing binary logistic regression on a dataset with 10000 observations, classifying action as ""good"" or bad"".  There are two independent variables (x1, x2), and class variable (y, with values ""good"" or ""bad"").  In this dataset, we  have 7,500 observations classified as ""bad"" and 2,500 classified as ""good"".  This is because there are several different ways for a user to perform a ""bad"" action, but only one way for them to perform a ""good"" action.</p>

<p>We are doing our analysis in R using the <code>glm()</code> function.</p>

<p>We create training data by randomly sampling 7,500 observations from the dataset, and create test data from the other 2,500 observations.  we then build a model using binary logistic regression on the training data, then test it on the test data.  The accuracy of our model is 75%.</p>

<p><strong>Can we say our model is better than guessing?</strong></p>

<p>He says that this model is no better than guessing.  Even though the error rate is better than 50%, because the original data had a prevalence of ""bad"" classifiers, we would need our model to predict better than 75% in order to say it performs better than random guessing.</p>

<p>I disagree...but I can't defend my point with anything other than ""that doesn't seem right"".  Can someone shed some light on the correct interpretation, and the reason for it?</p>
"
"0.0657951694959769","0.0311588476424878"," 95795","<p>from what I have studied in the data mining course (please correct me if I'm wrong) - in logistic regression, when the response variable is binary, then from the ROC curve we can determine the threshold.</p>

<p>Now I'm trying to apply the logistic regression for an ordinal categorical response variable with  more than two categories (4).
I used the function <code>polr</code> in r:</p>

<pre><code>&gt; polr1&lt;-polr(Category~Division+ST.Density,data=Versions.data)
&gt; summary(polr1)

Re-fitting to get Hessian

Call:
polr(formula = Category ~ Division + ST.Density, data = Versions.data)

Coefficients:
               Value Std. Error t value
DivisionAP   -0.8237     0.5195  -1.586
DivisionAT   -0.8989     0.5060  -1.776
DivisionBC   -1.5395     0.5712  -2.695
DivisionCA   -1.8102     0.5240  -3.455
DivisionEM   -0.5580     0.4607  -1.211
DivisionNA   -1.7568     0.4704  -3.734
ST.Density    0.3444     0.0750   4.592

Intercepts:
    Value   Std. Error t value
1|2 -1.3581  0.4387    -3.0957
2|3 -0.5624  0.4328    -1.2994
3|4  1.2661  0.4390     2.8839

Residual Deviance: 707.8457 
AIC: 727.8457  
</code></pre>

<p>How should I interpret the Intercepts?
and how can I determine the threshold for each group?</p>

<p>Thanks</p>
"
"0.161164592805076","0.0763232776972177"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.166450075715296","0.0985329278164293"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"0.0930484210398471","0.0771142136168656"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.0882734829504749","0.0696733014291618"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0986927542439653","0.0623176952849756"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"0.05884898863365","0.0557386411433294"," 97834","<p>I ran a mixed model using lme4::glmer for a logistic regression and consistently got these warning messages. I noticed there are still regular results even so, but are they accurate estimates?</p>

<pre><code>    &gt; glmm.ms1&lt;-glmer(as.formula(paste(paste(y[1], x, sep=""~""), mix[1], sep=""+"")),
    +             data=rtf2,control=glmerControl(optimizer=""bobyqa"",
    +             optCtrl=list(maxfun=100000),family=binomial)
Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.8766 (tol = 0.001)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; coef(summary(glmm.ms1))
                       Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)           1.810e+00  6.558e-01   2.760464 5.772e-03
lepidays             -3.340e+00  2.770e-01 -12.059620 1.726e-33
cldaysbirth          -1.555e+00  5.224e-01  -2.975934 2.921e-03
rotaarm              -2.057e-01  3.209e-01  -0.641102 5.215e-01
cldaysbirth2         -3.072e-01  2.955e-01  -1.039510 2.986e-01
bfh2                 -1.043e+01  1.160e+03  -0.008996 9.928e-01
bfh3                  4.653e-01  4.806e-01   0.968103 3.330e-01
bfh4                  2.547e-01  4.994e-01   0.509966 6.101e-01
bfh5                  3.744e-01  9.926e-01   0.377213 7.060e-01
ruuska               -1.020e-01  5.928e-02  -1.720396 8.536e-02
genderMale           -4.008e-01  2.645e-01  -1.515453 1.297e-01
epiexlbf              6.078e-04  2.796e-03   0.217391 8.279e-01
haz.epi              -7.211e-02  1.373e-01  -0.525039 5.996e-01
cldaysbirth:rotaarm   6.928e-01  4.771e-01   1.452148 1.465e-01
rotaarm:cldaysbirth2  5.181e-01  3.352e-01   1.545527 1.222e-01
Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
</code></pre>

<p>Due to data's sensitivity, I can't post the whole process for generating same messages, but I would like to know how to handle these warnings. I don't think it's suitable to keep my eye blind here.</p>
"
"NaN","NaN"," 99736","<p>I would like to plot the relationship between a binary categorical response variable and a continuous predictor to study its shape. The goal is to prep a logistic regression.
This image may clarify:
<img src=""http://i.stack.imgur.com/pUGv5.png"" alt=""Example Plot""></p>

<p>I have access to Minitab and R and would greatly appreciate any insight on how to recreate this histogram or alternatives that may do just as well.</p>
"
"0.0759737176397586","0.0539687072220866","101003","<p>I have a data set of around 5000 features. For that data I first used Chi Square test for feature selection; after that, I got around 1500 variables which showed significance relationship with the response variable. </p>

<p>Now I need to fit logistic regression on that. I am using glmulti package for R (glmulti package provides efficient subset selection for vlm) but it can use only 30 features at a time, else its performance goes down as the number of rows in my dataset is around 20000.</p>

<p>Is there any other approach or techniques to solve the above problems? If I go by the above method it will take too much time to fit the model.</p>
"
"0.0986927542439653","0.0623176952849756","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.161164592805076","0.0763232776972177","102695","<p>I'm using R to run some logistic regression. My variables were continuous, but I used cut to bucket the data. Some particular buckets for these variables always result in dependent variable being equal to 1. As expcted, the coefficient estimate for this bucket is very high, but the p-value is also high. There are about ~90 observations in either these buckets, and around 800 total observations, so I don't think it's a problem of sample size. Also, this variable should not be related to other variables, which would naturally reduce their p-values.</p>

<p>Are there any other plausible explanations for the high p-value?</p>

<p>Example:</p>

<pre><code>myData &lt;- read.csv(""application.csv"", header = TRUE)
myData$FICO &lt;- cut(myData$FICO, c(0, 660, 680, 700, 720, 740, 780, Inf), right = FALSE)
myData$CLTV &lt;- cut(myData$CLTV, c(0, 70, 80, 90, 95, 100, 125, Inf), right = FALSE)
fit &lt;- glm(Denied ~ CLTV + FICO, data = myData, family=binomial())
</code></pre>

<p>Results are something like this:</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.53831  -0.77944  -0.62487   0.00027   2.09771  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -1.33630    0.23250  -5.747 9.06e-09 ***
CLTV(70,80]     -0.54961    0.34864  -1.576 0.114930    
CLTV(80,90]     -0.51413    0.31230  -1.646 0.099715 .  
CLTV(90,95]     -0.74648    0.37221  -2.006 0.044904 *  
CLTV(95,100]     0.38370    0.37709   1.018 0.308906    
CLTV(100,125]   -0.01554    0.25187  -0.062 0.950792    
CLTV(125,Inf]   18.49557  443.55550   0.042 0.966739    
FICO[0,660)     19.64884 3956.18034   0.005 0.996037    
FICO[660,680)    1.77008    0.47653   3.715 0.000204 ***
FICO[680,700)    0.98575    0.30859   3.194 0.001402 ** 
FICO[700,720)    1.31767    0.27166   4.850 1.23e-06 ***
FICO[720,740)    0.62720    0.29819   2.103 0.035434 *  
FICO[740,780)    0.31605    0.23369   1.352 0.176236    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1037.43  on 810  degrees of freedom
Residual deviance:  803.88  on 798  degrees of freedom
AIC: 829.88

Number of Fisher Scoring iterations: 16
</code></pre>

<p>FICO in the range [0, 660) and CLTV in the range (125, Inf] indeed always results in Denial = 1, so their coefficients are very large, but why are they also ""insignificant""?</p>
"
"0.138865930150177","0.0845527144336216","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.200731263865498","0.10370291340864","102907","<p>I want to test the presence of an interaction term in a logistic regression with glm().</p>

<p>The formula is:</p>

<pre><code>Gest.hypertension ~ ART.conc + Parity + Age + Smoke + Nulliparity + Syst.disease.type
</code></pre>

<p>With Age the only continuous variable and the others categorical. </p>

<p>The interaction term is ART.conc (2 leves) x Parity (3 levels), which are also the variables of interest. I found two ways of representing the interaction in glm();</p>

<ul>
<li>By explicitly defining ART.conc * Parity, which create both simple and interaction effect.</li>
<li>By creating an artificial new variable with interaction(ART.conc, Parity) and adding the simple terms.</li>
</ul>

<p>This are the results, omitting the other controls variables (OR and p value presented):</p>

<pre><code>model1: Gest.hypertension ~ ART.conc * Parity + ...

ART.conc-&gt;yes                       3.35     0.073 .
Parity-&gt;2                           7.15     0.001 **
Parity-&gt;3                             38     &lt;0.001 ***
ART.conc-&gt;yes:Parity-&gt;2            0.262     0.054 .
ART.conc-&gt;yes:Parity-&gt;3           0.0532     0.009 **
</code></pre>

<hr>

<pre><code>model2: Gest.hypertension ~ interaction(ART.conc, Parity) + ART.conc + Parity + ...

interaction(ART.conc, Parity)-&gt;yes.1  3.35    0.072590 .  
interaction(ART.conc, Parity)-&gt;no.2   7.15    0.001206 ** 
interaction(ART.conc, Parity)-&gt;yes.2  6.26    0.002698 ** 
interaction(ART.conc, Parity)-&gt;no.3   38.05    0.000202 ***
interaction(ART.conc, Parity)-&gt;yes.3  6.78    0.011071 *  
ART.concyes                              NA         NA    
Parity2                                  NA         NA    
Parity3                                  NA         NA
</code></pre>

<p>so the two models give more or less the same results one one of the two terms is zero and that's ok. I have troubles understanding what happened when the two terms are different from zero. For example in the second model that keeping parity == 2 and passing ART.conc from no to yes, we pass from an OR of 7.15 to 6.26. Instead, applying what I understood of logistic regressions and interactions, the coeff for the case (ART.conc, Parity)->yes.2 should be
$$OR_{parity.2}*OR_{art.yes:parity.2} = 1.87$$
very different from 6.26...</p>

<p>Therefore, I'd like to know what I'm missing and which of the model should I use.</p>

<p>Thanks!</p>

<p>UPDATE!!!</p>

<p>Thanks to @gavin simpson answer I thought I solved the problem, and ditched the interaction() trick. But then I noticed a pretty silly error by me.</p>

<p>We said that to interpretate the interaction you do
$$OR_{simple.eff}*OR_{interaction.eff}$$
Taking in example the case ART.yes/Parity.2. I said that to compute the effect of interaction of ART on Parity with the classical method you should do 
$$OR_{art.yes}*OR_{art.yes:parity.2} â‰ˆ 3.35*0.262 â‰ˆ 0.87$$
But if you perform
$$OR_{parity.2}*OR_{art.yes:parity.2} â‰ˆ 7.15*0.262 â‰ˆ 1.87$$
I was troubled by this disparity. I need a number that could describe the overall risk of the pathology in the above mentioned case, and instead i got two numbers that i didn't know how to describe.
Trying to solve this I noticed my error. To compute the overall effect, is not enough to multiply one simple effect for the interaction effect, but you should add the other simple effect too!
$$OR_{simple.eff.1}*OR_{simple.eff.2}*OR_{interaction.eff}$$
or
$$OR_{art.yes}*OR_{parity.2}*OR_{art.yes:parity.2} â‰ˆ 3.35*7.15*0.262 â‰ˆ 6.26$$</p>

<p>6.26 is the same OR for the <code>interaction(ART.conc, Parity)-&gt;yes.2</code> that is produced using the interaction trick.</p>

<p>Therefore, using interaction(), with <code>drop=T</code> option and not putting at all the simple effects by themselves (to avoid the NAs), you have that model 1 and model 2 give the same results, with model two including the already computed effect of the interaction.</p>
"
"0.109658615826628","0.0934765429274634","102973","<p>I am using logistic regression to benchmark the performance of some students in different years. I created a scenario as below:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
benchmark.data &lt;- mydata[1:300,] # students form year 1990-1995 as benchmark
compare.data &lt;- mydata[301:400,] # students from year 1996

# logistic regression model created using benchmark student result
temp.glm &lt;- glm(admit~gre+gpa+rank,data=benchmark.data,family=""binomial"")

# using the regression model to predict how students in 1996 perform
compare.data[,""predict""] &lt;- predict(temp.glm,newdata=compare.data,type=""response"")

# making a threshold that if the predicted chance of admit &gt; 0.5, then it is asssumed that the student will get admitted
compare.data[,""predict_admit""] &lt;- ifelse(compare.data[,""predict""]&gt;0.5,1,0)
table(compare.data[,c(""admit"",""predict_admit"")])

#      predict_admit
# admit  0  1
#     0 59  6
#     1 26  9
</code></pre>

<p>From the table, it is seen that 15 students predicted to get admitted and actual number of students get admitted is 35, so the observed/expected ratio is <code>35/15=2.33</code>, as it is larger than <code>1</code>, so I will say that students in year 1996 is performing better than benchmark.</p>

<p>Can I draw my conclusion using the method mentioned above?</p>

<p>Besides, how should I set the threshold? Or should I <code>sum(compare.data[,""predict""])</code> and treat it as expected value?</p>

<h3>Update 1</h3>

<p>I tried and used ROC curve to determine the threshold:</p>

<pre><code>library(ROCR)
benchmark.data[,""predict""] &lt;- predict(temp.glm,newdata=benchmark.data,type=""response"")
preds &lt;- prediction(benchmark.data[,""predict""],as.numeric(benchmark.data[,""admit""]))
plot(performance(preds,""tpr"",""fpr""),print.cutoffs.at=seq(0,1,by=0.05))
</code></pre>

<p>And the charts suggests that threshold at 0.35 seems to give maximized sensitivity and specificity.</p>
"
"0.124837556786472","0.0985329278164293","102998","<p>I have roughly 15 variables / attributes characterizing 6k customers in my data set. As they are categorical I have transformed them into 1 attribute for each possible value (1-out-of-K coding). An example could be Region with values ""A"", ""B"" and ""C"", which is transformed into 3 variables: <code>Region_A</code>, <code>Region_B</code> and <code>Region_C</code>. The same goes for other variables such as the <code>Sales Channel</code>. After this transformation I now have around 70 attributes. </p>

<p>I would like to examine if there are any significant 2-way interactions between the different variables with regards to a response variable (concerning <code>customer quality</code>) using logistic regression. For instance, it is interesting to see if there is an interaction between <code>Region_A</code> and <code>Sales Channel 1</code>. However, there are very many possible interactions and therefore I would like to start by removing some variables, which have very few observations connected to them. An example could be that only 3 customers come from <code>Region_A</code>.</p>

<p>More specifically, I would start by removing all attributes that have 5 observations or less connected to them (out of 6k observations). However, I cannot find out how to do that. Thus I have the following questions:</p>

<ol>
<li><p>Does my thinking make sense? Or should I approach the issue in another way?</p></li>
<li><p>How do I remove all attributes in a dataset which has fewer than 5 observations connected to them? The values of the variables are always 0 or 1 as the customer is either from <code>Region A</code> (=1) or not from <code>Region A</code> (=0).</p></li>
<li><p>After removing these variables there should be fewer interactions. However, it would still be quite a large amount. I would therefore also like to only examine interactions with 5 observations or more. I am thinking this could be done using a formula in the logistic regression, but can you help me how I would find the right variables for the formula?</p></li>
</ol>
"
"0.131590338991954","0.0623176952849756","103884","<p>I would like to find if there is a significant difference between two ROC-curves. I've found the roc.test in the pROC package. However, I cannot seem to find any information on how this test is actually performed. Can anyone explain it to me or refer to a webpage that has information on this?</p>

<p>From the <a href=""http://www.inside-r.org/packages/cran/pROC/docs/roc.test"" rel=""nofollow"">documentation</a> for <code>roc.test()</code>, I've found that the used test is DeLong's test (based on the article 'Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach'). However, I don't quite understand the article. </p>

<p>My ROC-curves have been found by comparing two logistic regressions, where one has a subset of attributes of the other seeing as I want to examine whether these attributes are useful. The probabilities are found using 5-fold cross-validation.</p>
"
"0.10403129732206","0.0886796350347864","104191","<p>Apologies if the question is too broad, then please close the thread. Or maybe this belongs in StackOverflow?</p>

<p>I observed something strange and wonder if someone more educated can shed some light on the issue. I don't think it's necessary to go into a ton of detail (i.e., providing code and data). </p>

<p>I'm running a logistic regression in R using glm. I have some dataset where about 90% of my dependendent variables = 1. There are also 15 independent variables. About 10 of the variables are factors with 3 levels: ""Yes"", ""No"", ""No Data"" (not NA, to prevent exclusion). Overwhelmingly most of the data (80%+) for these factor variables is ""Yes"", but the default baseline for those independent variables was ""No Data."" With this configuration, the regression converges. Also, if I use ""No"" as the baseline, the regression converges. If I change the baseline for those variables to ""Yes,"" the regression no longer converges. </p>

<p>Any particular reason for this? Is this some property of logistic regression or a result of R code? Is it because my dependent variable has low variance or because the independent variables?</p>

<p>I'm curious because ideally we would like faster convergance, and if I knew some properties of logistic regression that prevent convergance, then I could build better regressions and be more confident about the results. For example, I had a similar dataset, and made the same changes (releveling). This regression happened to converge, but it took much more iterations, and some of the results were actually different than before I made the change. So, it seems like important knowledge.</p>

<p>Thanks!</p>
"
"0.119027940128723","0.103342206529982","104194","<p>My situation:</p>

<ul>
<li>small sample size: 116 </li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. There seems to be various methods for handling missing data, so I would like to know:</p>

<ul>
<li>Does LASSO impose any restriction in terms of the method of imputation that I can use?</li>
<li>What would be the best bet for imputation method? Ideally, I need a method that I could run on SPSS (preferably) or R.</li>
</ul>

<p>UPDATE1: It became clear from some of the answers below that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values and the usage of group lasso:</p>

<ul>
<li>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias.</li>
<li>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</li>
</ul>

<p>This is a follow-up on a previous question of mine (<a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">How to select a subset of variables from my original long list in order to perform logistic regression analysis?</a>).</p>

<p>OBS: I am not a statistician.</p>
"
"0.131590338991954","0.0623176952849756","104485","<p>I have more of a programming background, and I am fairly new to statistics. I am currently trying to solve some sample exercises to get more familiar with data science / modelling. </p>

<h3>Problem Background</h3>

<p>A user posts a request on a forum. Considering number of responses / number of up votes / users' reputation / etc., can you predict if the request will be fulfilled? There are about 4000 rows of test data available.</p>

<h3>Current Attempts</h3>

<p>I have created basic graphs to investigate correlation, and applied a few transformations to the data. 
I have decided to go with logistic regression, and based on the graphs, have chosen 2 factors for the initial model. </p>

<h3>Problem</h3>

<p>I have created the basic model using R, and I can see some summary stats, but I am a bit lost when I try to understand (a) How well the model fits the data and (b) whether the model is accurate. I tried Googling around, but most of the articles were too technical for me. Is there a cheat sheet/quick test of some sort that I can apply? Or can somebody suggest the simplest ""complicated"" article?</p>
"
"0.151947435279517","0.0899478453701443","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.0657951694959769","0","104883","<p>I have data of certain proportion $p$ in a population. I need to approximate the behaviour of this population through a Logistic curve of the form:</p>

<p>$p=\frac{1-e^{-\gamma t}}{1+ \alpha e^{-\beta t}}$</p>

<p>I have consulted several webpages and I have found out that the more simple process is to estimate the logistic curve as </p>

<p>$p=\frac{1}{1+ \alpha e^{-\beta t}}$</p>

<p>And then estimate the value for $\gamma$, but I still don't know how to do it because when I use a logistic regression the range of the dependent variable is one and zero, not all the range between them.</p>

<p>I have found a document about it, but it's implemented in MatLAB, how can I do it in R?
<a href=""http://home2.fvcc.edu/~dhicketh/diffeqns/activities/logistic.pdf"" rel=""nofollow"">http://home2.fvcc.edu/~dhicketh/diffeqns/activities/logistic.pdf</a></p>
"
"0.0657951694959769","0.0311588476424878","105301","<p>Given the outcome variable in a dataframe is a factored/categorical variable, when regressing the dependent variable (DV) onto a set of independent variables (IVs), what is the model predicting? The probability that the DV is the first level of the factor? Or the second?</p>

<p>A related question: I know that given a numerical column of $1$s and $0$s, a logistic regression would model the probability of the higher order variable (i.e., value=$1$), so I have been attempting to recode the factor ""character"" variable into a numerical one. I am coming from a SAS background, so I am entirely to used to <code>if var = ""yes"" then var_num = 1; else var_num=0;</code></p>

<p>That's clearly wrong. What's the most efficient way you have found to recode such variables?</p>
"
"0.116310526299809","0.0881305298478463","105346","<p>I am interested in estimating an adjusted risk ratio, analogous to how one estimates an adjusted odds ratio using logistic regression. Some literature (e.g., <a href=""http://aje.oxfordjournals.org/content/159/7/702.abstract"">this</a>) indicates that using Poisson regression with Huber-White standard errors is a model-based way to do this</p>

<p>I have not found literature on how adjusting for continuous covariates affects this. The following simple simulation demonstrates that this issue is not so straightforward: </p>

<pre><code>arr &lt;- function(BLR,RR,p,n,nr,ce)
{
   B = rep(0,nr)
   for(i in 1:nr){
   b &lt;- runif(n)&lt;p 
   x &lt;- rnorm(n)
   pr &lt;- exp( log(BLR) + log(RR)*b + ce*x)
   y &lt;- runif(n)&lt;pr
   model &lt;- glm(y ~ b + x, family=poisson)
   B[i] &lt;- coef(model)[2]
   }
   return( mean( exp(B), na.rm=TRUE )  )
}

set.seed(1234)
arr(.3, 2, .5, 200, 100, 0)
[1] 1.992103
arr(.3, 2, .5, 200, 100, .1)
[1] 1.980366
arr(.3, 2, .5, 200, 100, 1)
[1] 1.566326 
</code></pre>

<p>In this case, the true risk ratio is 2, which is recovered reliably when the covariate effect is small. But, when the covariate effect is large, this gets distorted. I assume this arises because the covariate effect can push up against the upper bound (1) and this contaminates the estimation.</p>

<p>I have looked but have not found any literature on adjusting for continuous covariates in adjusted risk ratio estimation. I am aware of the following posts on this site: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes"">Poisson regression to estimate relative risk for binary outcomes</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38004/poisson-regression-for-binary-data"">Poisson regression for binary data</a></li>
</ul>

<p>but they do not answer my question. Are there any papers on this? Are there any known cautions that should be exercised? </p>
"
"0.0328975847479884","0.0623176952849756","105480","<p>I have a survey question where the respondent can check one choice or two choice maximum. Question looks like this:</p>

<blockquote>
  <p>What is the more important characteristic when you buy chocolate?</p>
  
  <ul>
  <li>Sweetness (characteristic 1) </li>
  <li>Packaging (characteristic 2) </li>
  <li>Price (characteristic 3) </li>
  <li>... (characteristic 4)  </li>
  <li>... (characteristic 5) </li>
  <li>... (characteristic 6)</li>
  </ul>
</blockquote>

<p>In my data, I have 6 column (1 for each characteristic) with the value 1 if the respondent checked the ad-hoc column and 0 otherwise.</p>

<p>I would like to perform a regression where the outcome is the question and the predictors the socio demographic criteria (some of them are numerical such as the age and other categorical such as the gender).</p>

<p>Now I wonder how I should do. Should I do a simple logistic regression 6 times with same predictors but each time different outcome?</p>

<blockquote>
  <ul>
  <li>glm(caracteristic1 ~ age+gender)</li>
  <li>glm(caracteristic2 ~ age+gender)</li>
  <li>glm(caracteristic3 ~ age+gender)</li>
  <li>glm(caracteristic4 ~ age+gender)</li>
  <li>glm(caracteristic5 ~ age+gender)</li>
  <li>glm(caracteristic6 ~ age+gender)</li>
  </ul>
</blockquote>

<p>Or is it possible to use another method such as the  multinomial logistic regression. If yes which R packages do you recommend me to use?</p>
"
"0.131590338991954","0.0623176952849756","105633","<p>There are times when one might want to estimate a prevalence ratio or relative risk, in preference to an odds ratio, for data with binary outcomes - say, if the outcome in question isn't rare, so the RR ~ OR relationship doesn't hold.</p>

<p>I've implemented a model in R to do that, as follows:</p>

<pre><code>uni.out &lt;- glm(Death ~ onset, family= binomial(link=log), data=data)
</code></pre>

<p>But I'm continually getting convergence issues, even when providing starting values (such as the coefficient estimates pulled from a logistic regression), or turning up the number of allowed iterations. I've also tried <code>glm2</code> without any success.</p>

<p>The two ideas I have from here are to either fit a poisson model to the same data using a sandwich estimator for the variance, or fitting the model using MCMC and taking the standard error of the posterior (this is being used alongside multiple imputation, so I can't just report the posterior). The problem is, I have no idea how to implement either one of these in <code>R</code>, nor if they're the best solution.</p>

<p>Additionally, while using a model like:</p>

<pre><code>glm(Death ~ age, family= binomial(link=log),start=c(-3.15,0.03),data=data)
</code></pre>

<p>I'm regularly get an error message ""Error: cannot find valid starting values: please specify some"", but not always. What is generating this message?</p>
"
"0.0930484210398471","0.0440652649239232","105688","<p>I have a binary variable, $X=\{0,1\}$ and I want to use this variable to predict a continuous outcome variable $y$</p>

<p>What regression model ought I use to estimate the predictive ability of $X$ of $y$?</p>

<p>I know that switching them the other way around, would allow me to use a logistic regression model but that doesn't seem applicable in this scenario.</p>

<p>What statistic would I use to assess the significance of this relationship?</p>

<p>How can I do this in R?</p>
"
"0.196227812235748","0.128669938172323","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.191259213059362","0.132379272715407","106347","<p>I am somewhat familiar with various ways of testing mediation for factors in different types of regression analysis.  (I'm using R and currently working with a multilevel binary logistic regression.)  But now I have a situation in which I'd like to test whether one interaction between factors mediates another, and I'm not sure how this could be done properly.  </p>

<p>To give a simplified example of what I am interested in doing:</p>

<p>I have a multilevel binary model using student characteristics to predict pass/fail for students who are in a control versus experimental group.  </p>

<p>Let's say that the ""intervention"" appears to affect women more strongly than men, because the interaction gender*intervention is significant.  But then adding in a number of co-variates (and their interactions with the intervention),  results in a decrease in the magnitude of the coefficient and the significance of the fenale*intervention interaction, suggesting that once we control for these co-variates and their interactions with the intervention, differences between how the intervention ""affects"" men and women are no longer significantly different.  </p>

<p>I would like to be able to say something about mediation, and I understand how to test the individual factors for mediation of the gender*intervention interaction, but what if there is another interaction, such as (hours spent on childcare)*intervention, which I think may mediate the gender*intervention interaction?  Is there a way to test whether the first interaction mediates the second one?</p>

<p>EDIT:</p>

<p>As requested, here is a simple example equation which I think explains what I want to do.  For the purposes of simplicity, I am specifying this as a simple binary logistic regression model instead of a multilevel binary logistic regression model.  </p>

<p>Let's suppose there are three IV being used as factors:
intervention = whether student was in control or experimental group
gender
GPA</p>

<p>And the DV is whether the student passed or failed.  </p>

<p>And let's suppose we consider the following models (I'm just listing the factors here, without coefficients or error terms, for ease of readability):</p>

<p>M1: OUTCOME = INTERVENTION + GENDER + INTERVENTION*GENDER</p>

<p>M2: OUTCOME = INTERVENTION + GENDER + GPA + INTERVENTION*GENDER + INTERVENTION*GPA</p>

<p>Suppose in M1 that INTERVENTION*GENDER was significant, so that women benefited significantly more than men from the intervention.  </p>

<p>Then supposed that INTERVENTION*GENDER was not significant (and the female*experimental coefficient had a smaller magnitude) in M2, and we suspect that this is because INTERVENTION*GPA mediates INTERVENTION*GENDER.   </p>

<p>What I would like to know is if there is a way to test whether or not INTERVENTION*GPA mediates INTERVENTION*GENDER for these two models....</p>
"
"0.166450075715296","0.0985329278164293","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"0.0379868588198793","0.0539687072220866","108284","<p>Here is the R code that produced the output below:</p>

<pre><code>library(caret)
set.seed(934)
fitControl &lt;- trainControl(method= ""repeatedcv"", number=10, repeats=10)
logitfit &lt;- train(z~ a+b+c+d+e, data=train, method=""glm"", trControl = fitControl)
logitfit
</code></pre>

<p>Consider the following output from R:</p>

<pre><code>Generalized Linear Model 

900 samples
 20 predictors
  2 classes: '0', '1' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 

Summary of sample sizes: 846, 847, 846, 845, 847, 846, ... 

Resampling results

  Accuracy  Kappa  Accuracy SD  Kappa SD
  0.80     0.58  0.076       0.18    
</code></pre>

<p>This is a logistic regression model using a 10-fold cross validation repeated 10 times. Does the output above indicate that the average accuracy is 0.80? </p>
"
"0.21711298242631","0.13219579477177","108315","<p>I am running multinomial logistic regression analysis on my data.  The response variable is the number of calves produced each year (0,1, or 2).  I am trying to evaluate the influence of the <em>X</em> variables on the odds of producing a calf.  My <em>X</em> variables are predation risk (WR; continuous), age of mother (age; categorical or continuous), time (wolf; categorical).</p>

<p>First, I have 4 different age classification schemes (i only show 2) - I want to know which one of the age of mother would be ""best"" to use. I could use it as continuous variable - or as categories based on biological reasoning for senescence in older moose (old ladies don't invest in reproduction as much).  So, I thought I would use a likelihood ratio test.</p>

<pre><code>library(mlogit)

modata.model1 &lt;- mlogit(no.C ~ 1 | 1, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model2 &lt;- mlogit(no.C ~ 1 | age, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model3 &lt;- mlogit(no.C ~ 1 | age2, data=modata, reflevel=""1"", na.action = na.omit)  
</code></pre>

<hr>

<pre><code> lrtest(modata.model2,modata.model3)

Likelihood ratio test

Model 1: no.C ~ 1 | age
Model 2: no.C ~ 1 | age2
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   4 -213.22                         
2   4 -207.57  0 11.309  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>QUESTION: to interpret this output - there was a significant difference in the loglikelihood when we comparing the continuous age to a categorical age with 2 classes.  The loglik is smaller for model 1 and therefore it would be better to use? Or do I have that backwards? </p>

<p>Next I was going to use the Walds test to evaluate nested models.  To see if the addition of a variable was worth it.</p>

<pre><code>modata.model7 &lt;- mlogit(no.C ~ 1 | age+WR, data=modata, reflevel=""1"", na.action = na.omit) 

modata.model8 &lt;- mlogit(no.C ~ 1 | age+WR+wolf, data=modata, reflevel=""1"", na.action = na.omit) 
</code></pre>

<hr>

<pre><code>Wald test

Model 1: no.C ~ 1 | age + WR
Model 2: no.C ~ 1 | age + WR + wolf
  Res.Df Df  Chisq Pr(&gt;Chisq)
1    244                     
2    242  2 0.5828     0.7472
</code></pre>

<p>QUESTION: this tells me that there is no significant improvement when there is an additional variable of wolf added??  So, then I can use the smaller model or do I use the one with the smaller Res.DF?</p>

<p>In addition to confirming my interpretations of the results I have 2 side questions...  </p>

<p>1)to get the null model for <code>mlogit</code> library - is my <code>modata.model1</code> correct?  I want the intercept only model to compare against.</p>

<p>2) Hosmer and Lemshow suggest by getting Wald values to get significance levels for each coefficient - in mlogit, thats the same as using <code>summary(model)</code> and there they provide the t-values with p instead of needing to do an additional Walds test? (NOTE in the below model i use 2 category age class instead of continuous)</p>

<pre><code>summary(modata.model8)

Call:
mlogit(formula = no.C ~ 1 | age2 + WR + wolf, data = modata, 
    na.action = na.omit, reflevel = ""1"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
    1     0     2 
0.652 0.244 0.104 

nr method
6 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.4E-06 
successive function values within tolerance limits 

Coefficients :
              Estimate Std. Error t-value Pr(&gt;|t|)   
0:(intercept)  0.38730    0.40144  0.9648 0.334657   
2:(intercept) -2.40317    1.04546 -2.2987 0.021523 * 
0:age21       -1.39607    0.43989 -3.1737 0.001505 **
2:age21        0.53952    1.07275  0.5029 0.615012   
0:WR          -1.46584    0.64797 -2.2622 0.023686 * 
2:WR          -0.19214    0.60856 -0.3157 0.752206   
0:wolf1        0.56055    0.63292  0.8857 0.375797   
2:wolf1        0.42642    0.72375  0.5892 0.555744   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -203.11
McFadden R^2:  0.053606 
Likelihood ratio test : chisq = 23.009 (p.value = 0.00079359)
</code></pre>
"
"0.116310526299809","0.0881305298478463","108745","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. It became clear from some of the answers to my <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">previous question</a> that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values in continuous variables and the usage of group lasso:</p>

<ul>
<li><p>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias. If that is not advisable, what would be in the case of categorical variables?</p></li>
<li><p>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</p></li>
</ul>

<p>OBS: I am not a statistician.</p>
"
"0.109658615826628","0.0934765429274634","108750","<p>I'm working with a data set like the following:</p>

<p><em>X</em> =</p>

<pre><code>c1 c2 c3 c4 c5 y
a  c  f  h  j  0
a  d  f  i  k  0
a  c  g  h  j  1
a  c  f  h  k  0
b  d  g  h  k  0
b  e  f  h  j  0
</code></pre>

<p>I'm trying to create a logistic regression model that is able to predict <code>y</code> (0 or 1) based on the features of <em>X</em>. </p>

<p>Info on <em>X</em>:<br>
The values of <code>c1</code>â€“<code>c5</code> are all factors.<br>
The features have a varying number of levels (e.g. <code>c1</code>: 2 levels, <code>c2</code>: 4 levels, <code>c3</code>: 3 levels, etc.)<br>
Approx 10% of <code>y</code> is 1.  </p>

<p>I've tried using SVM and GLM without any good result.</p>

<pre><code>model &lt;- svm(y ~ ., data = X)  
pred &lt;- predict(model, X)   
table(pred,X$y)

        y
pred    FALSE TRUE
FALSE   1332   113
TRUE       0     0
</code></pre>

<p>and</p>

<pre><code>model &lt;- glm(y ~ ., family=binomial(""logit""), data=X)  
pred &lt;- predict(model, X, type=""response"")  
table(pred,X$y)  

pred                   FALSE TRUE
4.2260288377431e-08        2    0
4.24333100181876e-08       1    0
...
0.706714407238236          1    1
0.736650629322038          0    1
</code></pre>

<p>I'm used to working with features with continuous values when creating a predictive model, but I don't really know how to tackle a problem with factors.<br>
What would you guys recommend for this type of problem? Changing from logistic regression to something else or using other functions besides GLM &amp; SVM?</p>
"
"0.0657951694959769","0.0311588476424878","108869","<p>So I have performed a logistic regression on a data set with multiple predictors. I want to graphically represent the relationship between the outcome and only one of the predictors. What would be the best way of doing this?</p>

<p>I am having a hard time figuring out how to graphically show the effect that predictor X has on Y independent of the other predictor terms.  I read something about partial residual plots but I do not know how to implement this in R or how it would apply to data with a binary outcome. Any input is appreciated.</p>

<p>Thanks,</p>
"
"0.119027940128723","0.0939474604818017","109222","<p>I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.</p>

<p>D = 1 if disease B present, 0 otherwise</p>

<p>E = 1 if disease A present, 0 otherwise</p>

<p>I am also including in the model a measure of healthcare utilization. </p>

<p>F is a positive integer proportional to an individual's utilization of healthcare.</p>

<p>I am running the logistic regression model as such in R:</p>

<pre><code>glm(D ~ E + F, family = ""binomial"") 
</code></pre>

<p>Now, this works fine. </p>

<p>However, when I try to run conditional logistic regression, it gives me an error:</p>

<pre><code>library(survival)
clogit(D ~ E + F, strata(matched.pairs))
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge
</code></pre>

<p>I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)</p>

<p>This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. </p>

<p>I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). </p>

<p>I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?</p>

<p>Thank you all in advance.</p>
"
"0.080582296402538","0.0763232776972177","109796","<p>I am currently working on a project using a sales system and trying to come up with a way to use the current pipeline of potential sales to predict the amount of product that will be sold in the future. Iâ€™m looking for advice on how to approach this problem and hopefully some resources to teach me what approach to use and why.</p>

<p>The sales system Iâ€™m using has historical data for opportunities (potential sales). Around 50,000 of the opportunities are â€œclosedâ€ meaning that they are either won or lost. I have around 1,000 â€œopenâ€ opportunities that have not yet been won or lost. Some variables that I have on each sale include the product (which is generally homogenous except for the amount), the amount, the salesman, the date, the time it was input into the system, the customer, and other data about the customer.</p>

<p>I understand that if I want to predict a dichotomous variable like win / lose then I should look at a logistic regression. However, Iâ€™m looking for general advice on how to </p>

<ol>
<li>Predict the probability of each individual opportunity closing as won using the data I have (and how to tell if I've done it correctly).</li>
<li>Estimate the total amount of won opportunities for a period.</li>
</ol>

<p>I found a similar question here <a href=""http://stats.stackexchange.com/questions/66276/using-a-logistic-model-on-the-estimates-of-several-other-classification-models"">Using a logistic model on the estimates of several other classification models</a> but Iâ€™m hoping for a response that gives me a better idea of where to start. Iâ€™m comfortable using R or any other statistical software, but ideally I'd like some kind of book or other reference material that is as low-level as possible.</p>
"
"0.0994729462603988","0.0706616245726062","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.166450075715296","0.0985329278164293","110136","<p>I'm working on a prediction model for a continuous variable (amount of medicine injected) .I use R for modeling.My project flow is to multiply the prediction of a glm (logistic regression) model that is used to predict 0/1 if a medicine was injected at all with an lm (linear regression) model that is used predict amount of medicine injected - this model works rather good In R .My problem is that when I move this model to MSSQL I get different values for the prediction (i.e. for a random row the value in the R is 400 and in SQL the value for the same row is 640.The model in SQL is made by attaching the models coefficiants from the glm model to produce the glm prediction values and then multiplying it with the lm model prediction values. I don't understand why there is a difference if I use the same coefficients?</p>

<p>Here is the code for the lm and glm models in r:</p>

<pre><code>d7_lm&lt;-lm(Ttl_Inject~UserSource+IsNewIndividual+IsCross,data=train)
d7_glm&lt;-glm(Is_Injected~UserSource+IsNewIndividual+IsCross,data=train)
</code></pre>

<p>Here is a part of the r code for the prediction:</p>

<pre><code>demo$d7_lm_pred&lt;-predict(d7_lm,newdata=demo,type='response')
    demo$d7_glm_pred_response&lt;-ifelse(predict(d7_glm,newdata=demo,type='response')&gt;0.5,1,0)
demo$glm01_lm_response&lt;-demo$d7_lm_pred*demo$d7_glm_pred_response # this is used for a container of the prediction model's values.
</code></pre>

<p>Here is a part of the SQL code : </p>

<pre><code>select TOP 1000*, InjectionAmount_pred= (-2.213e -1.180e+00*(case when User='IAF' then 1 else 0 end)-1.665e+00*(case when UserSource='Viral' then 1 else 0 end)
+IsNewIndividual  *  1.167e+00+IsCross )

* IIF((1 / (1 + EXP(-(-1.346e-03+1.140e-02*(case when UserSource='IAF' then 1 else 0 end) -2.975e-03*(case when UserSource='Viral' then 1 else 0 end)
-IsNewIndividual  * 1.503e-04 +IsCross ))))&gt;0.5,1,0) 
</code></pre>
"
"0","0.0311588476424878","110148","<p>I have run a <code>multinomial logistic regression</code> test for the interaction between species of deer, days a camera trap was in the field and type of reaction. </p>

<p>The model with the best AIC value was: </p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following </p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone knows why this is considered the best model when neither days nor deer react (r) or strongly react (sr) higher than chance?</p>
"
"0","0.0440652649239232","110155","<p>I have run a multinomial logistic regression test for the interaction between species of deer, days a camera trap was in the field and type of reaction.</p>

<p>The model with the best AIC value was:</p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following</p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone would know what the interaction between species and days is in relation to r (reaction) and sr (strong reaction)? I know I can't reject the null hypothesis that there is no effect of species or days on r or sr but beyond that I'm lost!</p>
"
"0","0.0311588476424878","110236","<p>I have a survey data which has one dependent variable (""Overall experience"") and several independent variables(Quality of food, creativity of menu etc.). The response scorecard for both dependent and independent variables are as follows:</p>

<p>Excellent   5
Very Good   4
Good        3
Fair        2
Poor        1 </p>

<p>As per my understanding and from what I have read, I cannot run a simple linear regression and thus I have opted for logistic regression.
Can anyone guide me in the right direction regarding logistic regression using R. </p>
"
"NaN","NaN","110570","<p>For my survey data analysis, I ran an Ordinal Logistic regression using the 'polr' function.
The summary of the regression is as follows:</p>

<p><img src=""http://i.stack.imgur.com/csKGq.png"" alt=""enter image description here""></p>

<p>My question is:</p>

<ol>
<li>Do I need to standardize my  beta values?</li>
<li>If so, is lm.beta the right approach (as per my understanding, it only works for linear models)? And if not, could you please provide a method to do so.</li>
</ol>

<p>Thanks everyone!</p>
"
"0.0657951694959769","0.0467382714637317","110969","<p>I've obtained a logistic regression model (via <code>train</code>) for a binary response, and I've obtained the logistic confusion matrix via <code>confusionMatrix</code> in <code>caret</code>. It gives me the logistic model confusion matrix, though I'm not sure what threshold is being used to obtain it. How do I obtain the confusion matrix for specific threshold values using <code>confusionMatrix</code> in <code>caret</code>?</p>
"
"0","0.0311588476424878","111308","<p>I'm conducting a power analysis to derive the required sample size for a study - basically compared exposed / non-exposed with 30-day mortality as outcome. I'll check for crude mortality rates with chi-square, but also use logistic regression with probable confounders.</p>

<p>When I run a power analysis - power 0.8, significance level 0.05, effect size 0.15 and estimated 10 confounders I get that I'd need only n=117 which seem quite small.
comparing with chi-square - it suggest that I'd need 350.</p>

<p>I'm using R and <code>pwr</code>:</p>

<pre><code>pwr.f2.test(u=10, v=NULL, f2=0.15, sig.level=0.05, power=0.8)
pwr.chisq.test(w=0.15, N=NULL, df=1 , sig.level=0.05, power=0.8 )
</code></pre>

<p>Is this predictable or am I misusing this?</p>
"
"NaN","NaN","111339","<p>I am looking for the equation for the se.fit values when using logistic regression in R.
I have seen this answer - <a href=""https://stats.stackexchange.com/questions/66946/how-are-the-standard-errors-computed-for-the-fitted-values-from-a-logistic-regre/66947#66947"">How are the standard errors computed for the fitted values from a logistic regression?</a></p>

<p>but in my case, I'm calling ""predict"" function with 'type' parameter set to be ""response"".
In this case, the equation given in the linked I attached doesn't hold.</p>

<p>Here is an example of the predict function I'm calling:</p>

<pre><code>predicted.resutls &lt;- predict(glm.model, train.data, type = ""response"", se.fit=TRUE)
</code></pre>
"
"0.164487923739942","0.116845678659329","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.05884898863365","0.0696733014291618","111457","<p>I ran two logistic regression models, one with a dataset including outliers and one without outliers, with multiple predictors.</p>

<p>I checked each model's fit with the le Cessie â€“ van Houwelingen â€“ Copas â€“ Hosmer unweighted sum of squares test for global goodness of fit from the rms package in R (following advice <a href=""http://www.r-bloggers.com/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/"" rel=""nofollow"">here</a>).</p>

<pre><code>model1 &lt;- lrm(y ~ a + b + c + d, data1, method = ""lrm.fit"", model = TRUE, x = TRUE, y = TRUE, linear.predictors = TRUE, se.fit = FALSE)
residuals(model1, type = ""gof"")
</code></pre>

<p>For the model with outliers the p value was close to 0, indicating a lack of fit. For the model without outliers p value was 0.52, indicating that my model was not incorrect.</p>

<p>I then ran 10-fold cross validation for both models with DAAG package and was surprised to get identical (poor) accuracy results for both = 0.56</p>

<pre><code>cv10&lt;-CVbinary(model1,nfolds=10)
</code></pre>

<p>I thought that the model created using the dataset without outliers, having a much better fit, will give me higher accuracy. Am I missing something here? I will be grateful for your help.</p>
"
"0.0657951694959769","0.0311588476424878","111841","<p>I ran an ordinal logistic regression in R using the polr function on a survey analysis dataset. The responses of the dependent variable range from Poor to Excellent.
The responses to the independent variables range from 1 to 5 (1 being Poor and 5 being Excellent). I obtained the following result:</p>

<p><img src=""http://i.stack.imgur.com/1ZBij.png"" alt=""enter image description here""></p>

<p>I want to measure the individual percentage contribution of my independents (R1, R2,...,R17) to the dependent variable. </p>

<p>Is there a way to do this.</p>

<p>Thanks for any help.</p>
"
"0.253245725465862","0.155909598641583","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.149209419390598","0.0824385620013739","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.05884898863365","0.0696733014291618","112348","<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven't been able to figure out a way to extract any from the <code>zelig()</code> object. </p>

<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>

<p>Thanks in advance for your help!</p>
"
"0.182482967150453","0.11234482285936","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.161834718742537","0.125411942572491","112670","<p>I'm working on a dataset with the following variables:</p>

<pre><code>Y:  a boolean telling whether the subject has experienced a seizure
ID:  the id of the subject
Sess:  the subject's session number (a subject has been observed multiple times)
X3:  numeric measurements of the subject's behavior during the session 
X4:  ""
X6:  ""
</code></pre>

<p>The idea is to see if the behavior measurements(X3,X4,X6) can be used predict the Seizure status (Y) in the population.  If there were just one session per subject I'd model the data with a logistic regression, but since each subject has multiple sessions the observations cannot be assumed independent.</p>

<p>It seems a GEE logistic model makes the most sense for this dataset, but I'm having trouble understanding the results when compared to a regular glm.  When introducing correlation structure to the GEE equation the signs of the coefficients are reversed, and the predicted values are opposite of what they would be with an independent correlation structure.</p>

<pre><code>library(""geepack"")
#regular logistic model
mod0 = glm(Y~X3+X4+X6, family=binomial(""logit""), data=mice)
#gee logistic model, independent correlation structure
mod1 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""indep"", scale.fix=T, waves=Sess, data=mice)
#gee logistic model, exchangeable correlation structure
mod2 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""exchangeable"", scale.fix=T, waves=Sess, data=mice)
</code></pre>

<p>As expected, the parameter estimates of the glm model(mod0) and the independent correlation gee model(mod1) are the same.  But when an exchangeable correlation structure(mod2) is introduced, the estimates are completely different and change sign.</p>

<pre><code>&gt; 
&gt; summary(mod1)                    

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""indep"", scale.fix = T)

 Coefficients:
            Estimate Std.err  Wald Pr(&gt;|W|)    
(Intercept)   -1.494   0.459 10.59  0.00114 ** 
X3            -2.377   0.626 14.42  0.00015 ***
X4             1.090   0.398  7.50  0.00619 ** 
X6             1.233   0.403  9.36  0.00222 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = independenceNumber of clusters:   28   Maximum cluster size: 4 
&gt; summary(mod2)

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""exchangeable"", 
    scale.fix = T)

 Coefficients:
            Estimate Std.err Wald Pr(&gt;|W|)   
(Intercept)  -0.8928  0.4255 4.40   0.0359 * 
X3            0.1059  0.0383 7.64   0.0057 **
X4           -0.0427  0.0299 2.04   0.1535   
X6           -0.0528  0.0213 6.16   0.0131 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
Number of clusters:   28   Maximum cluster size: 4 
</code></pre>

<p>My biggest concern is that the predicted Y's of the two GEE models show opposite trends, i.e. behaviors that would predict a positive seizure status in mod1, predict a negative seizure status in mod2.    </p>

<pre><code>plot(mod1$fitted, mod2$fitted)
</code></pre>

<p><img src=""http://i.stack.imgur.com/nAogH.png"" alt=""scatter plot of fitted values from mod1, mod2""></p>

<p>Also, what's with the estimated correlation parameter of alpha = 1.09 in mod2?  Unless I'm interpreting this wrong, shouldn't this always fall between -1 and 1?</p>

<pre><code>Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
</code></pre>

<p>It seems odd to me that the results are completely flipped depending on whether or not the observations are assumed to have dependence structure.  Can anyone else offer insight on this behavior?</p>

<p>Here is the data:</p>

<pre><code>&gt; dput(mice)
structure(list(Y = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), ID = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 
5L, 5L, 6L, 6L, 6L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 
11L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 
19L, 19L, 19L, 20L, 20L, 20L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 
23L, 23L, 23L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L, 26L, 
26L, 26L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L), Sess = c(2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 
4L, 3L, 4L, 3L, 4L, 3L, 4L, 3L, 4L, 1L, 3L, 4L, 1L, 3L, 4L, 1L, 
2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 1L, 2L, 3L, 1L, 2L, 
3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 
4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L), X3 = c(0.511015060794264, 
0.898356533693696, 0.798280430157052, 1.31144372617517, 0.829923189452201, 
0.289089506643144, -0.763028944257538, -0.944459588217789, -1.16474609928919, 
-0.182524267014845, -0.338967193889711, -0.896037509887988, 0.00426073081308205, 
0.0576592603165749, -1.4984737260339, 1.34752684212464, 0.106461438449047, 
-0.108424579472268, -2.85991432039569, -0.230115838261355, -1.54479536845993, 
-1.23693649938367, -1.53704616612456, -1.04825100254239, 0.142768659484482, 
0.28358135516745, -0.236302896321009, 0.708743856986942, -0.507503006972081, 
0.401550711842527, -0.16928449007327, 0.867816722958898, 0.487459858572122, 
1.35172112260613, 0.14742652989871, 0.742155288774287, 0.348552056119878, 
-0.82489952485408, 0.0366834636917457, -0.731010479377091, 0.979544093857171, 
1.4161996712129, 0.661035838980077, 0.600235250313596, -1.10872641912335, 
-0.212101744145196, -0.919575135240643, -0.813993077336991, -0.547068540188791, 
-0.0260198210967738, -0.0962240349391501, -0.251025721625606, 
0.894913664382802, -0.21993004239326, 0.0628839847717805, 1.77763503559622, 
0.718459471596243, 0.984412886705251, 0.77603470471174, 0.486187732642953, 
1.78012655684609, 1.31622243756713, 1.29635178661133, 0.427995111986702, 
0.993748401511881, 0.387623239882247, 0.42006794384777, -0.815889182132972, 
-0.897540332229183, -1.041943103505, 0.379425827374942, -1.00707718576756, 
-0.889182530787803, 0.148432805676879, -0.287928359114935, -0.747152636892815, 
-1.41003790431546, -0.611571256991109, -1.02569548477235, -1.02700056733181, 
-1.45808867127733, -1.47973458605138, 2.23643966561508, 2.69397876103083, 
0.81841473415516, 2.12167589051282, 0.267133799544379, -0.326215175076418, 
-1.08788244901967, -1.18733017947214), X4 = c(0.050598970482242, 
-0.0279694583060402, 0.999225143631274, 0.199872317584803, 0.779316284168575, 
-0.3552692229881, -0.232161792808608, -0.333479851296274, -0.748169603107953, 
-0.57785843363913, -0.480747933235349, -0.740466500603612, -0.618559437949564, 
-0.591541699294345, -0.538855647639331, 0.431376763414175, -0.327931008191724, 
-0.469416282917978, -0.659224551441466, -0.55285236403596, -0.637082867133913, 
-0.780321541069982, -0.40539035027884, -0.54024676972473, -0.185562290173831, 
0.054439450703482, 0.624097793456316, 0.24018937319873, -0.264194638773171, 
-0.389590537012038, -0.42771343162755, -0.738790918078674, -0.122411831542788, 
0.600119921164627, 0.0442597161778152, -0.0955011351192086, -0.521259643827527, 
-0.550050365103255, -0.504566887441653, -0.506571005423286, 0.523650149759566, 
0.341920916685254, -0.396343801985993, -0.366532239883921, -0.739276449002057, 
-0.56054127343218, -0.587601788901296, -0.56798329186843, -0.454937006653748, 
-0.672730639942183, -0.564864467446687, -0.678853515419629, 0.573072971483937, 
0.596973680548765, 0.0403978228634349, 1.93617633381248, 2.54301964691615, 
0.363075891004736, 0.0205658396444095, 0.560923287570261, 1.24212005971229, 
2.32518793880728, 2.69979166871713, 0.626868716830008, 0.219463581391793, 
0.236477261174534, -0.115429539698909, -0.49754151674106, -0.40827433350644, 
-0.0433283703798658, -0.578451015506926, -0.714208713291922, 
-0.802387726290423, -0.836794085697031, -0.471800405613954, -0.668030208971065, 
-0.610945789491312, -0.780838257914176, -0.411360572155088, -0.494388869332376, 
-0.63231547268951, -0.743853022088574, 4.90627675753856, 3.38455016460328, 
0.859445571488139, 2.42212262705776, -0.324759764820016, -0.541581784452693, 
-0.485324968098865, -0.770539730529603), X6 = c(0.0150287583709043, 
-0.151984283645294, -0.347950002037732, 0.379891135882966, 0.129107019894704, 
-0.314047917638528, -0.516381047940779, -0.751192211830495, -0.884460389494645, 
-0.462363867892961, -0.397583161539858, -0.559528880497725, -0.842987555132397, 
-0.922797893301111, -1.01175722882932, 0.32346425626624, -0.610909601293237, 
-0.605155952259822, -1.29840867980623, 0.0793710626694382, -0.806959976634144, 
-0.674523251142452, -0.960113466801064, -0.783836535852452, -0.0665321645536412, 
0.482235339656537, -0.319499220427413, -0.115345965733089, -0.30806448545927, 
0.251747727063608, -0.305013811851957, -0.931916656036151, 0.415032839884745, 
0.337184728843034, 0.0584335852357015, -0.0712185313438638, 0.78632612201797, 
0.490831043388539, 0.8902425262631, 0.160088439571744, 0.90343086944952, 
0.928495373121098, -0.389259569427933, -0.304578433259833, -0.593364448723133, 
-0.411333868741105, -0.882691663964141, -0.91208274239495, -0.708633954450382, 
-0.339396626779965, -0.420927315080057, -0.421383857909298, 0.407474183771483, 
0.629710767351175, -0.438726438495567, 2.40977730548689, 2.47250810430208, 
0.783562677342961, 0.781304150896319, 0.563221804716475, 1.85514126067038, 
1.30723846671955, 1.94869625911545, 0.876751836832149, 0.626629859119409, 
0.067113945916172, 3.54280776301513, 0.0082773667305384, -0.311414481848668, 
-0.732779325538588, -0.594477082903005, -1.0385239418576, -1.04141739541776, 
-0.99472304141247, -0.599659297534257, -0.804801224448196, -1.13096932958525, 
-0.641957537144073, -0.722959119516237, -0.671146043591047, -0.714432955420477, 
-0.766750949574034, 1.20993739830475, 2.79011376379402, 2.64532317075082, 
2.54251033029822, -0.539516582572252, -0.6419726544563, -0.663768795224503, 
-0.644829826467113)), .Names = c(""Y"", ""ID"", ""Sess"", ""X3"", ""X4"", 
""X6""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>
"
"0.220847116289638","0.112632518139278","112760","<p>I have used <code>mlogit</code> package and I am trying to summarize the results I have from my model.  I have a question regarding the reference value and will get to that in a moment.</p>

<pre><code>redata.full &lt;- mlogit(no.C~ 1| WR+age+age2+BP+noC.1yr, data=redata, reflevel=""0"", na.action=na.fail)

no.C = number of offspring    
WR = risk
age+age2 = the non-linear relationship that as an individual ages their production decreases
BP = browsing pressure
noC.1yr = number of offspring produced the year before
</code></pre>

<p>I recognize that my data is ordinal in nature, but Im following other people's methods who have done this and used the reference based approach rather than ordinal logistic regression.  However, I am still shakey on justification other than citing the other person and saying ""he did it too!""  If anyone has a suggestion I would appreciate it.</p>

<p>My results for this model are: </p>

<pre><code>Call:
mlogit(formula = no.C ~ 1 | WR + age + age2 + BP + noC.1yr, data = redata, 
    na.action = na.fail, reflevel = ""0"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
       0        1        2 
0.233766 0.675325 0.090909 

nr method
5 iterations, 0h:0m:0s 
g'(-H)^-1g = 2.16E-07 
gradient close to zero 

Coefficients :
               Estimate Std. Error t-value Pr(&gt;|t|)  
1:(intercept) -0.281226   1.225763 -0.2294  0.81854  
2:(intercept) -0.605312   1.997179 -0.3031  0.76183  
1:WR           0.847273   0.518854  1.6330  0.10248  
2:WR           1.347976   0.689916  1.9538  0.05072 .
1:age          0.314075   0.275486  1.1401  0.25425  
2:age         -0.422368   0.395240 -1.0686  0.28523  
1:age2        -0.018998   0.014446 -1.3151  0.18847  
2:age2         0.022572   0.018949  1.1912  0.23359  
1:BP          -0.143720   0.173585 -0.8280  0.40770  
2:BP          -0.074553   0.331108 -0.2252  0.82185  
1:noC.1yr      0.574304   0.377821  1.5200  0.12850  
2:noC.1yr      1.251673   0.626033  1.9994  0.04557 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -116.6
McFadden R^2:  0.079844 
Likelihood ratio test : chisq = 20.236 (p.value = 0.0271)

exp(cbind(OddsRatio = coef(redata.full), ci))
              OddsRatio      2.5 %    97.5 %
1:(intercept) 0.7548580 0.06831155  8.341351
2:(intercept) 0.5459038 0.01089217 27.360107
1:WR          2.3332750 0.84394900  6.450831
2:WR          3.8496270 0.99577472 14.882511
1:age         1.3689929 0.79782462  2.349065
2:age         0.6554925 0.30209181  1.422317
1:age2        0.9811815 0.95379086  1.009359
2:age2        1.0228284 0.98553735  1.061530
1:BP          0.8661299 0.61634947  1.217136
2:BP          0.9281585 0.48504538  1.776078
1:noC.1yr     1.7758933 0.84686698  3.724076
2:noC.1yr     3.4961862 1.02497823 11.925441
</code></pre>

<p>I would like confirmation of my interpretations:
The model is better than a null - obtained from the likelihood ratio test.</p>

<p>Question: How do I test how well the model is actually working (i.e., goodness of fit)?  Hosmer-Lemshow test? Ive read warnings about using the McFaddin's Pseudo R where they really aren't applicable to multinomial regressions.  Ive found a HL test with <code>ResourceSelection</code> library and it says my model is NOT doing well at all.  Now what?</p>

<p>Interpretation:
WR and noC.1yr are the only variables that are coming out as slightly significant.  But this is only between the reference value of 0 and production of 2 calves.  It is not significantly different between 0 or 1 for these variables.  </p>

<p>Question: Ive been trying to find somewhere in the vignette what the t-value is - it is just a t-test?  How would I refer to the estimate as being significant?  ""The estimated odds for 2-offspring being produced versus 0 were 3.85 (95% CI = 1.0-14.88) which was significant (t= 1.99, P=0.05)""</p>

<p>Referring to my statement regarding setting the reference value.  When I run this exact same model using my other options of 0 or 1 offspring - I get completely different results of which variables are significant.  If I use 2 as the reference value then Age+WR+noC.yr are significant.  If I use 1, then Age only is sig.  So, which one to use?  I have read you want to pick one that is most relevant to your hypothesis, but in this case I could motivate any of the 3 levels.  </p>
"
"0.153522062157279","0.0830902603799674","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.134303827337563","0.0763232776972177","113252","<p>I am so sorry, I am beginner in statistic analysis, I have project using R to analyze the correlation between dependent variables and independents variables. </p>

<p>In this case I have two dependent variables (1. Extrovert, 2. Introvert). 
And the independent variables i have the data from (Call Log-> how long they call everyday, how many they call everyday, SMS log-> how length text in SMS body every day, how many they sent/received sms for each day).</p>

<p>I am so confused how I can do it, please anyone can give me some good references about it. 
I also have some questions such as : </p>

<ol>
<li>I use the different type of variables, independent variables (data type : numeric) but dependent variable (data type is categorical), so it is possible to apply logistic regression and Pearson? </li>
<li>Or any someone will give me some advice the better solution such as another methods for solving this problem. </li>
</ol>

<p>The example of data from dput()</p>

<pre><code>structure(list(sumcallin = c(462L, 998L, 335L, 179L, 34L, 0L, 
0L, 0L, 0L, 0L), caountcallin = c(7L, 5L, 8L, 5L, 1L, 1L, 0L, 
1L, 1L, 1L), sumcallout = c(1068L, 81L, 519L, 393L, 342L, 0L, 
583L, 1902L, 358L, 1017L), countcallout = c(15L, 3L, 10L, 5L, 
6L, 0L, 3L, 3L, 3L, 3L), sumreceived = c(322L, 75L, 20L, 35L, 
8L, 35L, 135L, 103L, 471L, 173L), countreceived = c(15L, 4L, 
2L, 3L, 1L, 2L, 7L, 3L, 18L, 5L), sumsent = c(171L, 31L, 25L, 
23L, 8L, 55L, 87L, 9L, 400L, 258L), countsent = c(10L, 4L, 1L, 
3L, 1L, 3L, 4L, 1L, 13L, 8L), personality = structure(c(2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""extro"", ""intro""), class = ""factor"")), .Names = c(""sumcallin"", 
""caountcallin"", ""sumcallout"", ""countcallout"", ""sumreceived"", 
""countreceived"", ""sumsent"", ""countsent"", ""personality""), row.names = c(1L, 
2L, 3L, 4L, 5L, 37L, 38L, 39L, 40L, 41L), class = ""data.frame"")
</code></pre>

<p>Thank you for your help.</p>
"
"NaN","NaN","114034","<p>I fit the logistic regression model for gender and drink for the data ihd using the following command </p>

<pre><code>model&lt;-glm(ihd~as.factor(gender)*as.factor(drink),Family='binomial',data=ihd)
</code></pre>

<p>My question is how can I get the estimated log odd-ratio for:  </p>

<ol>
<li>gender in non-drinker stratum, and</li>
<li>gender in drinker stratum</li>
</ol>
"
"0.124341182825498","0.0824385620013739","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.0379868588198793","0.0539687072220866","114195","<p>I have a dataset with a lot of missing values and mix of continues and categorical variables.  I want to use something like group lasso to do features selection. Probably the output is binary 0,1 and so grouped lasso logistic regression seems to be the more sensible choice. </p>

<p>My problem is the very large number of missing values. Deleting non complete rows is not an option.</p>

<p>Is there any R implementation that can be used similarly to the lasso and that can handle missing values and categorical variables at the same time?   </p>

<p>A possible solution has been proposed <a href=""http://stats.stackexchange.com/questions/110461/lasso-or-other-regularized-regression-with-censored-missing-data"">here</a> but it does not refer to any R package.</p>
"
"0.0465242105199235","0.0440652649239232","114218","<p>After running a gradient boosted model with <code>n</code> data points using multinomial regression where the response variable (a factor, as required by the gbm function) has <code>k</code> levels with R package gbm, I see that the predictions are output as as a vector of length <code>n*k</code>. Predicted responses are from:</p>

<pre><code>probs.var.multinom &lt;- predict.gbm(gbm.model.multinom, test.data, best.iter.gbm, 
                                  type=""response"")
</code></pre>

<p>Note that this is different from the output of a logistic (distribution = ""bernoulli"") model, where the results are a vector the same length as the number of cases.</p>

<p>How should this be interpreted? Specifically, how can I link the response vector back to the input data set to evaluate the classification?</p>
"
"0.0657951694959769","0.0623176952849756","114399","<p>I have a theoretical growth function that can be perturbed by events, and I'd like to estimate the growth parameters as well as the perturbation, and the rate of falloff after that perturbation.</p>

<p>I'm thinking of using a logistic function to model the effect of the event and the falloff of that effect (if any).</p>

<p>To ground this, $x$ is time, and $t$ is the time the event occurs. Before time $t$, or if the event never occurs, we have a simple linear regression. After the event occurs, I model the contribution of the event with magnitude controlled by $\beta_2$ and rate of falloff by $\beta_3$.</p>

<p>$y_i=\left\{x_{i}&lt;t:\beta_{0}+\beta_1x_i+\epsilon_i,x_i&gt;t:\beta_0+\beta_1x_i+2\beta_2\frac{1}{\left(1+e^{\beta_3\left(x_i-t\right)}\right)}+\epsilon_i\right\}$</p>

<p>(<em>edited to add the error term</em>)</p>

<p>Here's a <a href=""https://www.desmos.com/calculator/nzmusqqosq"" rel=""nofollow"">Desmos graph</a> if it helps.</p>

<p>I'm really not sure how to estimate parameters for this model in any of the stats packages I'm familiar with in R. Do I need to turn to Bayesian methods?</p>
"
"0.0657951694959769","0.0311588476424878","114583","<p>I want to simulate a binary response variable which depends on two normally distributed continuous variables, and I want to have more 1s than 0s in the response variable. I wonder how this can be done such that a logistic regression will not identify a significant interaction term.</p>

<p>My current approach in R looks like this:</p>

<pre><code>n = 1e5
x1 = rnorm(n)
x2 = rnorm(n)
y = x1+x2+rnorm(n)
y = ifelse(y &gt; 2, 1, 0)
df=data.frame(x1=x1, x2=x2, y=y)
summary(glm(y ~ x1*x2, df, family=binomial(logit)))$coefficients
</code></pre>

<p>This usually results in a highly significant interaction term, even though the y is just the sum of x1 and x2.
So how can I simulate a y which depends on both x1 and x2, but not on their interaction?</p>
"
"0.175844987113332","0.116585732444878","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.178541910193085","0.103342206529982","115188","<p>I am trying to look at whether 2 variables (one dichotomous categorical and one continuous) predict the occurrence of a dichotomous categorical dependent variable.</p>

<pre><code>dependent variable is LENIpos - 0 = no event, 1 = event
predictor variables are Hip.Prox.Femur - 0 = no hip fracture, 1 = hip fracture
                and     age (continuous)
</code></pre>

<p>Both predictor variables have significant p values in separate chi square test and Mann Whitney U test respectively.</p>

<p>When I run a logistic regression <code>glm(LENIpos ~ age + Hip.Prox.Femur, family = ""binomial)</code>, the variables come out as not significant. (1)</p>

<p>However, when I run the logistic regression with interactions <code>glm(LENIpos ~ age * Hip.Prox.Femur...)</code> (2), they are no both significant.  How is this to be interpreted?</p>

<p>Example R outputs:</p>

<p>(1)</p>

<pre><code>Call: glm(formula = LENIpos ~ age + Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -0.9346  -0.7826  -0.4952  -0.3374   2.1897  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)              -3.46888    1.00693  -3.445 0.000571 ***
age                       0.02122    0.01519   1.397 0.162535  
Hip.Prox.Femhip fracture  0.72410    0.57790   1.253 0.210212    

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 135.48  on 149  degrees of freedom
AIC: 141.48

Number of Fisher Scoring iterations: 5
</code></pre>

<p>(2)</p>

<pre><code>glm(formula = LENIpos ~ age * Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
        Min       1Q   Median       3Q      Max  
    -1.0364  -0.7815  -0.5373  -0.1761   2.3443  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)                  -5.89984    1.98289  -2.975  0.00293 **
age                           0.05851    0.02818   2.076  0.03788 * 
Hip.Prox.Femhip fracture      5.04990    2.46269   2.051  0.04031 * 
age:Hip.Prox.Femhip fracture -0.06058    0.03339  -1.814  0.06965 . 


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 131.82  on 148  degrees of freedom
AIC: 139.82

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.178541910193085","0.0939474604818017","115219","<p><strong><em>Imagine the situation:</em></strong> Mythical Seafolk use holes in the seabed as their burrows. Each hole has two parameters - diameter and depth. <strong>Majority of holes are unoccupied</strong> due to their surplus (n = 235). Occupied holes (n = 15) are (generally) expected to be much deeper and with larger diameter than random.</p>

<pre><code># generate data
set.seed(1234)
x &lt;- runif(250, min=0, max=10)
y &lt;- runif(250, min=0, max=10)
rbPal   &lt;- colorRampPalette(c(""deepskyblue"",""darkblue""))
my.data &lt;- data.frame(x_coor = x, y_coor = y,
                      diameter = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      depth = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      usage = rep(c(1,0), times = c(15, 235)))
my.data$col &lt;- rbPal(10)[as.numeric(cut(my.data$depth,breaks = 10))]

# look at the situation
# occupied holes are marked by red circles
plot(my.data$x, my.data$y, cex = my.data$diameter, col = my.data$col, pch = 20)
grid(5, 5, lwd = 0.75, lty = 2, col = ""grey"") 
points(my.data$x[1:25], my.data$y[1:25], pch = 1, cex = 2,
       col = ""red"", lwd = 1.75)
</code></pre>

<p><img src=""http://i.stack.imgur.com/8O1Xg.png"" alt=""enter image description here""></p>

<p>Although this analysis seem to be rather straightforward, <strong>I have some doubts about comparing unequal samples (15 vs 235)</strong>. Such problem do not occur in other <em>Seafolk burrow selection studies</em> because <strong>mapping of seabed is very expensive and time demanding</strong> and when researchers find 10 used holes, they continue to map the seabed only till they get find additonal 10 nonused holes (which are most probably located in close vicinity of used holes). Due to this
data collection approach the sample sizes of compared groups are usually almost equal.
<strong>However, we were able to find ALL holes in seabed - which can be now considered as a ""handicap"".</strong></p>

<p><strong><em>We do not want to do random sampling of 15 holes from 235 to obtain equal sizes (15 vs 15). Why to throw away such large amount of data!? Is there any solution/approach to this situation which is able to fully embrace such unique dataset?</em></strong></p>

<p><em>I have in mind two possible ways (please comment or add others):</em></p>

<p><strong>1.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, each of these reduced groups will be compared with sample of occupied holes by binomial logistic regression. By this I will obtain 50 logistic curves which will
be ""averaged"" into just one curve - final model.</p>

<p><strong>2.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, from each of these reduced groups (and from sample of used holes also) we gain (mean, sd, sample size). Thus, we will have six numbers for each comparison and we calculate effect size - standardized mean difference (SMD). Effect sizes will be inserted into meta-analytic model and tested if there is a heterogeneity present among effect sizes (<em>to see if the ""randomnes"" of creating unused holes sample has the significant impact on test result</em>).</p>
"
"0.189934294099397","0.0989426299071587","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.152894157431288","0.112632518139278","115647","<p>I have the following problem: I have a set of English words which I want to translate to Dutch. Of each words I mined a set of possible translations. For example, for the word ""Eighteen"" I obtained only one possible Dutch translation: ""Achttien"", which is correct. However, for other words I obtained multiple translations. For the word ""Good""  I have the translations ""Goed"", ""Braaf"" and ""Eerlijk"", which are technically correct translations but by far the best and most commonly used translation is only the word ""Goed"".</p>

<p>For a training set of English words I manually defined the optimal (correct) translation. Using this set I want to train some model to optimally pick for each English word the optimal Dutch word using some predictors. For example, I assume words that are more frequently used are probably better translations than others, and I assume that words that are noted first in a list of translations are probably better translations than others (e.g., in a dictionary, the first translation is usually the best).</p>

<p>So, my data looks something like this:</p>

<pre><code>English     Dutch       Frequency   Order   Correct
---------------------------------------------------
Eighteen    Achttien    800         1       TRUE
Good        Goed        900         1       TRUE
Good        Braaf       500         2       FALSE
Good        Eerlijk     600         3       FALSE
old         bejaard     300         1       FALSE
old         oud         900         2       TRUE
</code></pre>

<p>I want to predict the classification in the column <code>Correct</code>. At first I thought a logistic regression could do this, but that does not take into account that each row is not independent. e.g., for each unique value of the column <code>English</code> only one is correct and all others are false. Thus, some other classification method is required.</p>

<p>I was hoping you could point me in the right direction as to what method (or even better, an <code>R</code> package) would be suitable to tackle this problem. I guess this problem occurs more often in Machine Learning but I have no experience in that field.</p>
"
"0.0994729462603988","0.0706616245726062","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.0465242105199235","0.0881305298478463","116401","<p>I have been reading the <a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">odds tutorial on UCLA's stats page</a>. And I am trying to figure out if my interpretation of the results below is correct.  Based upon looking at the data the results seem to hold true. </p>

<pre><code>Variables to Predict Submit or Cancel (1,0)
DummyServiceA: Binary
DummyServiceB: Binary
DummyServiceC: Binary
AT_START: Continous
ID_SEQ: Continous
TOT: Continous

Logistic Regression Results 
exp(cbind(OR = coef(mymodel), confint(mymodel)))
                                OR      2.5     97.5      
DummyServiceA               0.3994   0.3215   0.4982   
DummyServiceB               6.5028   5.1442   8.2549   
DummyServiceC               0.2928   0.239    0.3604   
AT_START                    0.9986   0.9984   0.9987   
ID_SEQ                      0.949    0.94     0.9579   
TOT                         0.9992   0.9984   0.9998   
</code></pre>

<ul>
<li>Odds of Submit are 60% lower if <code>ServiceA</code> is selected instead of <code>ServiceB</code> or <code>ServiceC</code></li>
<li>Odds of Submit are 550% higher if <code>ServiceB</code> is selected instead of <code>ServiceA</code> or <code>ServiceC</code></li>
<li>Odds of Submit are 71% lower if <code>ServiceC</code> is selected instead of <code>ServiceA</code> or <code>ServiceB</code></li>
<li>For every unit increase in <code>AT_START</code> there is 0% change in odds for Submit</li>
<li>5% decrease in odds of Submit for every unit increase in <code>ID_SEQ</code></li>
<li>For every unit increase in <code>TOT</code> there is 0% change in odds for Submit</li>
</ul>
"
"0.131590338991954","0.0623176952849756","116850","<p>I have a dataset consisting of repeated measures data of graded toxicity scores (0-4) in a large number of patients being treated with a anti-cancer drug. We would like to identify predictors for developing dose limiting toxicity (e.g. score 3 or 4).</p>

<p>Initially the dataset was analyzed using a cumulative link mixed model to identify such predictors (clmm in R package ordinal).</p>

<p>However, there is also informative dropout occuring in the dataset (MNAR), e.g. the risk of dropping out is related to the severity of toxicity. As this therefore may bias parameter estimates, it seems a joint model that includes a survival model for dropout would be best. </p>

<p>My questions are:</p>

<ol>
<li><p>Is a joint model for dropout and outcome the only suitable option to account for dropout?  Or would there be other ways as well ? The only ugly alternative would be to analyze only the first part of the dataset, but I would rather like to use ALL the available data.</p></li>
<li><p>Is there an R package available that would readily allow such an analysis. E.g. longitudinal mixed-effect modelling of ordered categorical data, together with a dropout survival model. Or, alternatively, a mixed effect model logistic regression model with dropout (e.g. in this case we would reduce the dataset to having a dose limiting toxicity yes/no) ? The JM and joinR packages appear to be based on continuous data for the repeated measures.</p></li>
</ol>
"
"0.131590338991954","0.0623176952849756","117192","<p>I would like to get a covariance matrix of fitted probabilities for a logistic regression model in R. I would like to do this because I want to find the variance of the difference between the two fitted probabilities ($\hat{p}_1 - \hat{p}_2$).</p>

<p>Here is my attempt:</p>

<pre><code>x&lt;-rnorm(10,10,10)
y&lt;-x+rnorm(10,0,15)
z&lt;-round(runif(10,0,1))

m1&lt;-glm(z~x+y,family=binomial(link = ""logit""))

predict(m1,newdata=data.frame(x=c(1,0),y=c(1,1)),se.fit=TRUE,vcov=TRUE,type=""response"")
</code></pre>

<p>This gives me the standard errors of the fitted probabilities but not the covariance. </p>

<p>I am aware of the delta method to find the distribution of a function of a normal distribution but I would really like to avoid using the delta method if possible because my actual code needs to be extremely flexible. It'll be difficult to implement the delta method properly in my actual situation.</p>
"
"0.0986927542439653","0.0623176952849756","117340","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 50</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Following a suggestion to a previous <a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">question</a> of mine, I have run LASSO (using R's glmnet package) in order to select the subset of exaplanatory variables that best explain variations in my binary outcome variable.</p>

<p>I have calculated lambda.min through cross-validation (cv.glmnet command) and got the correspondent coefficients for my explanatory variables. For 6 of my total 50 explanatory variables, the coefficients were non-zero. Are those coefficients comparable, i.e. can I say that the variables with the highest ones are the most important? If they are not comparable, can I run logistic regression (using glm) with those 6 variables and then compare them in terms of coefficients and p-values?</p>
"
"0.116310526299809","0.0660978973858848","117450","<p>I have a confusing situation where I have strongly conflicting results from two ways of analyzing my simple data. I measure two binary variables from each participant, AestheticOnly and ChoiceVA. I want to know if AestheticOnly depends on ChoiceVA and whether this relation is different in two different experiments. Here is my participant count data:</p>

<pre><code>Experiment 1
                 AestheticOnly
                 0   1  All
ChoiceVA A      35   6   41
         V      20  13   33
         All    55  19   74

Experiment 2
                 AestheticOnly
                 0   1  All
ChoiceVA A      12  10   22
         V      31  11   42
         All    43  21   64
</code></pre>

<p>I run a logistic regression where AestheticOnly is modelled by ChoiceVA, Experiment, and the interaction:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ ChoiceVA*Experiment, data = d, family=binomial)
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ ChoiceVA * Experiment, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1010  -0.7793  -0.5625   1.2557   1.9605  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -3.3449     0.9820  -3.406 0.000659 ***
ChoiceVAV              3.5194     1.2630   2.787 0.005327 ** 
Experiment             1.5813     0.6153   2.570 0.010170 *  
ChoiceVAV:Experiment  -2.1866     0.7929  -2.758 0.005820 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apparently all factors are significant. But, this just doesn't make sense to me. For example, looking at the main effect of experiment should be equivalent to performing a Fisher's Exact test comparing 55 and 19 with 43 and 21 (bottom lines of each table). This is obviously not significant (p=.452). So why does the regression model give such a different result? Any help much appreciated.</p>
"
"0.170940864689457","0.0989426299071587","117593","<p>This is a really simple problem I am having, yet for the life of me I can't find a solution searching around. In theory I can simply recode the data, but that is an extreme solution I would rather not use if I don't have to. </p>

<p>I am simply trying to do a logistic regression with an ordered factor as my predictor. For a toy data set, consider:</p>

<pre><code>  radiation leukemia other total
1         0       13   378   391
2       1-9        5   200   205
3     10-49        5   151   156
4     50-99        3    47    50
5   100-199        4    31    35
6       200       18    33    51
</code></pre>

<p>I want to execute the following:</p>

<pre><code>glm(cbind(leukemia,other)~radiation,data=leuk,family=binomial(""logit""))
</code></pre>

<p>That is, leukemia are the ""successes"" and other are the ""failures"". Basically, trying to predict dose-response relationship between radiation and the proportional mortality rates for leukemia. However, this model is oversaturated:</p>

<pre><code>Call:  glm(formula = cbind(leukemia, other) ~ radiation, family = binomial(""logit""), 
    data = leuk)

Coefficients:
     (Intercept)      radiation1-9    radiation10-49  radiation100-199  
         -3.3699           -0.3189           -0.0379            1.3223  
    radiation200    radiation50-99  
          2.7638            0.6184  

Degrees of Freedom: 5 Total (i.e. Null);  0 Residual
Null Deviance:      54.35 
Residual Deviance: -3.331e-15   AIC: 33.67
</code></pre>

<p>I don't want each level of radiation as a factor to be its own predictor variable; that makes no sense, especially when you only have a small number of data points (note, this isn't actually the real data I am using, this is just a toy example that is similar). In any case, how do I force R to simply consider the factor radiation as a single variable with multiple levels? For example, if I do the following:</p>

<pre><code>x&lt;-c(0,1,2,3,4,5)
glm(cbind(leukemia,other)~x,data=leuk,family=binomial(""logit""))

Call:  glm(formula = cbind(leukemia, other) ~ x, family = binomial(""logit""), 
    data = leuk)

Coefficients:
(Intercept)            x  
    -3.9116       0.5731  

Degrees of Freedom: 5 Total (i.e. Null);  4 Residual
Null Deviance:      54.35 
Residual Deviance: 10.18        AIC: 35.84
</code></pre>

<p>This is more in line with what I want. But I am nervous about using that x variable in the regression for fear of changing the interpretation of the results. Similarly, I'd prefer to avoid an irritating system of dummy variables. </p>

<p>How do I go about doing this? Or is there a better workaround altogether for studying this type of relationship that I am not considering?</p>
"
"0.145986373720362","0.09506100395792","117631","<p>I measure two binary responses from each participant (ChoiceVA = V or A, AestheticOnly = 0 or 1). There are two experiments (between-participant). I want to test the following hypotheses:</p>

<p>AestheticOnly depends on Experiment (main effect)
AestheticOnly depends on ChoiceVA (main effect)
The way AestheticOnly depends on Experiment depends on ChoiceVA (interaction)</p>

<p>Here is my data. The first number in each cell is the proportion of participants scoring 1 for AestheticOnly, and the second number is the n for participants in that cell.</p>

<pre><code>                         ChoiceVA               
                        A       V     All

Experiment  1      0.1463  0.3939  0.2568
                       41      33      74

            2      0.4545  0.2619  0.3281
                       22      42      64

            All    0.2540  0.3200  0.2899
                       63      75     138
</code></pre>

<p>Just from looking at the data it is pretty obvious that neither main effect is significant (e.g. for ChoiceVA, bottom row, .25 of 63 participants is not significantly different from .32 of 75 participants). In my naivity I thought perhaps I could test these hypotheses with a straightforward binary logistic regression:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ Experiment+ChoiceVA+Experiment*ChoiceVA, data = d, family=binomial )
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ Experiment + ChoiceVA + Experiment *
    ChoiceVA, family = binomial, data = d)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.1010  -0.7793  -0.5625   1.2557   1.9605 

Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)            -1.7636     0.4419  -3.991 6.57e-05 ***
Experiment2             1.5813     0.6153   2.570  0.01017 * 
ChoiceVAV               1.3328     0.5676   2.348  0.01887 * 
Experiment2:ChoiceVAV  -2.1866     0.7929  -2.758  0.00582 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Clearly, the main effects are not being tested here in the way I hoped. I believe that this model, in testing main effects, rather than testing e.g. ChoiceVA=A against ChoiceVA=V across both levels of Experiment, is confining itself to that comparison only when Experiment=1. Can a model be constructed that instead tests the main effects in the way I would like?</p>

<p>This is related to a previous question (<a href=""http://stats.stackexchange.com/questions/117450/logistic-regression-gives-very-different-result-to-fishers-exact-test-why"">Logistic regression gives very different result to Fisher&#39;s exact test - why?</a>), but when I asked it I understand this even worse than I do now and consequently the question was so unclear that I need to start again.</p>
"
"0.139572631559771","0.0771142136168656","117783","<p>Despite having only a single binary outcome for each ID, there are multiple correlated measurements for the same test for each ID at different timepoints. The individual IDÂ´s are obviously independent, but the measurements of the same test at different time-points are not independent. The aim is to see whether the test can discern if the patients is cured or not. </p>

<p>-I cannot decide whether IÂ´d need to fit a mixed-level logistic regression model or if a regular logistic regression model would suffice. Is it possible somehow to fit a logistic regression model with time varying covariates like in cox models?</p>

<p>-Is it possible to use a clustering or CART based model (allowing for the longitudinal independent variable) instead that would be easier to comprehend in a clinical setting?</p>

<p>-I could turn the question around and do repeated measure Anova with <code>summary(aov(val~Long_term*time+Error(id),data=stat))</code>- however, 1)only time and the interaction term are significant, leaving me uncertain how to treat the other main effect of Long_term in face of only a significant interaction and 2)it feels contraintuitive to set an independent variable as a dependent variable when using anova.</p>

<pre><code>&gt; dput(stat)
structure(list(Long_term = structure(c(2L, 2L, 2L, 2L, 2L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,  2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L,  2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L,  2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,  2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,  1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L,  2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L,  2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,  2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L), .Label = c(""No"",  ""Yes""), class = ""factor""), id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L,  8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L,  2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,  16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,  11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L,  5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,  19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,  14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,  9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L,  3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,  17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,  12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L,  7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,  1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,  15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,  10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L,  4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,  18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,  13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,  8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L,  2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,  16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,  11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L,  5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,  19L), time = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  12, 12, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,  18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 24, 24, 24, 24,  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 30, 30, 30, 30,  30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 36,  36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,  36, 36, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,  42, 42, 42, 42, 42, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,  48, 48, 48, 48, 48, 48, 48, 48, 54, 54, 54, 54, 54, 54, 54, 54,  54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 60, 60, 60, 60, 60,  60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 66, 66,  66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66,  66, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,  72, 72, 72, 72, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,  78, 78, 78, 78, 78, 78, 78, 84, 84, 84, 84, 84, 84, 84, 84, 84,  84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 90, 90, 90, 90, 90, 90,  90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 96, 96, 96,  96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,  102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102,  102, 102, 102, 102, 102, 102), val = c(273, 194, 618, 755, 802,  395, 2438, 482, 502, 692, 607, 618, 579, 864, 579, 453, 673,  572, 707, 57, 373, 1197, 1026, NA, 697, 712, NA, NA, 616, NA,  NA, NA, NA, NA, NA, NA, 76, 1128, 560, 76, 819, 982, 303, 1294,  267, 1117, 346, 996, 652, 95, 951, 3250, 1584, 948, 981, 465,  411, 57, 197, 535, 498, 87, 1382, 210, 1649, 96, 450, 252, 42,  1086, 2137, 1395, 464, 1388, 532, 67, 25, 230, 566, 545, 38,  691, 216, 1412, 33, 151, 113, 29, 663, 806, 528, 240, 1508, 421,  50, 39, NA, 182, 412, 32, 414, 232, 868, 791, 201, 86, 33, 250,  345, 224, 381, 1069, 536, NA, NA, NA, 500, 312, NA, 287, 97,  227, 653, 69, 69, NA, NA, 225, 308, 256, 963, 420, NA, NA, NA,  368, 605, NA, 399, 69, 77, 20, 39, 70, NA, 122, 306, 103, 175,  807, 530, NA, NA, NA, 246, 443, NA, 363, 87, 39, NA, 25, 63,  NA, 163, 289, 172, 128, 1019, 582, NA, NA, NA, 231, 820, NA,  284, NA, 40, NA, NA, NA, NA, 238, 288, 217, NA, 903, 471, NA,  NA, NA, 236, 577, NA, 461, NA, 691, NA, NA, NA, NA, 158, 170,  168, NA, 681, 434, NA, NA, NA, 399, 634, NA, 85, NA, 83, NA,  NA, NA, NA, 72, 419, NA, NA, 912, NA, NA, NA, NA, NA, 635, NA,  295, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,  NA, NA, 138, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, 251, NA, NA, NA, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 37, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 132,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), row.names = c(NA,  -323L), .Names = c(""Long_term"", ""id"", ""time"", ""val""), class = ""data.frame"") 
</code></pre>
"
"0.0759737176397586","0.0359791381480577","118034","<p>I would like to build a logistic regression model in which I will be looking for predictor variables having a significant effect on the breeding success of a raptor bird. </p>

<p>The predictors in the dataset are highly correlated, which led me to consider logistic ridge regression. Furthermore, I investigated different breeding grounds in which one or multiple birds have been breeding. Since this makes the data clustered, I would need to add the breeding ground as a random effect in the model.</p>

<p>Thus, I would need a 'mixed logistic ridge regression' approach if I am getting things right here. This paper suggests this approach too for another problem:</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/22049265"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/22049265</a></p>

<p>Are there any people aware of the availability of an R package or something related having implemented a mixed logistic ridge regression approach as the paper and myself just described? I did not succeed in finding one.</p>

<p>Thank you.</p>
"
"0.265908011739155","0.206880044351332","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.0657951694959769","0.0311588476424878","121020","<p>I am trying to do Logistic Regression in R.</p>

<p>My data set contains more than 50 variables. Some of them are factor (qualitative variable) and others are independent variable(quantitative). I would like to get the significance of the variables from their p-value.</p>

<p>So far I came to know, I can do ANCOVA test to calculate p value of the factors. It (ANCOVA) combines features of both ANOVA and regression. It augments the ANOVA model with one or more additional quantitative variables, called covariates, which are related to the response variable.</p>

<p>How can I calculate P-value of quantitative variables? Or if I am wrong about ANCOVA, what other possibilities are available?</p>

<p>Any suggestion or help will be appreciated.</p>
"
"0.193552415284691","0.146658066600101","121120","<p>I am trying to create a logistic regression model with mgcv::gam
with what I think is a simple decision boundary, but the model
I build performs very poorly.  A local regression model built
using locfit::locfit on the same data finds the boundary very easily.
I want to add additional parametric regressors to my real-life model, so
I do not want to switch to a purely local regression.</p>

<p>I want to understand why GAM is having trouble fitting the data,
and whether there was ways of specifying the smooths that
can perform better.</p>

<p>Here's a simplified, reproducible example:</p>

<p>Ground truth is 1 = point lies within the unit circle, 0 if outside</p>

<p>e.g. z = 1 if sqrt(x^2 + y^2) &lt;= 1, 0 otherwise</p>

<p>The observed data is noisy, with both false positives and false negatives</p>

<p>Construct a logistic regression to predict whether a point
is inside the circle or not, based on the point's Cartesian
coordinates.</p>

<p>Local regression can find the boundary well (50% probability contour
is very close to the unit circle), but a logistic GAM consistently 
overestimates the size of the circle for the same probability band.</p>

<pre><code>library(ggplot2)
library(locfit)
library(mgcv)
library(plotrix)

set.seed(0)
radius &lt;- 1 # actual boundary
n &lt;- 10000 # data points
jit &lt;- 0.5 # noise factor

# Simulate random data, add polar coordinates
df &lt;- data.frame(x=runif(n,-3,3), y=runif(n,-3,3))
df$r &lt;- with(df, sqrt(x^2+y^2))
    df$theta &lt;- with(df, atan(y/x))

# Noisy indicator for inside the boundary
df$inside &lt;- with(df, ifelse(r &lt; radius + runif(nrow(df),-jit,jit),1,0))

# Plot data, shows ragged edge
(ggplot(df, aes(x=x, y=y, color=inside)) + geom_point() + coord_fixed() + xlim(-4,4) + ylim(-4,4))
</code></pre>

<p><img src=""http://i.stack.imgur.com/BfzkT.png"" alt=""enter image description here"">    </p>

<pre><code>### Model boundary condition using x,y coordinates

### local regression finds the boundary pretty accurately
m.locfit &lt;- locfit(inside ~ lp(x,y, nn=0.3), data=df, family=""binomial"")
plot(m.locfit, asp=1, xlim=c(-2,-2,2,2))
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/fy6z3.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM fits very poorly, also tried with fx=TRUE but didn't help
m.gam &lt;- gam(inside ~ s(x,y), data=df, family=binomial)
plot(m.gam, trans=plogis, se=FALSE, rug=FALSE)
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/ObeIm.png"" alt=""enter image description here"">  </p>

<pre><code>### gam.check doesn't indicate a problem with the model itself
gam.check(m.gam)

Method: UBRE   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [5.41668e-10,5.41668e-10]
(score -0.815746 &amp; scale 1).
Hessian positive definite, eigenvalue range [0.0002169789,0.0002169789].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

           k'    edf k-index p-value
s(x,y) 29.000 13.795   0.973    0.08

#### Try using polar coordinates

### Again, locfit works well
m.locfit2 &lt;- locfit(inside ~ lp(r, nn=0.3), data=df, family=""binomial"")
plot(m.locfit2)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9zr73.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM misses again
m.gam2 &lt;- gam(inside ~ s(r, k=50), data=df, family=binomial)
plot(m.gam2, se=FALSE, rug=FALSE, trans=plogis)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/55XiQ.png"" alt=""enter image description here"">    </p>

<pre><code>### Can also plot gam on link scale for alternate view
plot(m.gam2, se=FALSE, rug=FALSE)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/voMP4.png"" alt=""enter image description here"">    </p>

<pre><code>gam.check(m.gam2)

Method: UBRE   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-3.29203e-08,-3.29203e-08]
(score -0.8240065 &amp; scale 1).
Hessian positive definite, eigenvalue range [7.290233e-05,7.290233e-05].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

         k'    edf k-index p-value
s(r) 49.000 10.537   0.979    0.06
</code></pre>
"
"0.0930484210398471","0.0440652649239232","121823","<p>I am quite new in the R universe, so please excuse me if the question is too simple..</p>

<p>I would like to perform a logistic regression on a marketing data set (only categorical variables), of the form [outcome, X1,X2,X3,X4,X5,X6]</p>

<p>I split the data set into a training set and a validation set.</p>

<p>My problem: Predictor X1 has originally 3 levels. The model using glm retains only 2 of these 3 levels.</p>

<p>When I try to run the model on the validation set (where X1 still has 3 levels) I get an error message stating that the factor X1 has now a new level. </p>

<p>How can I prevent the glm function from excluding factor levels? I don't mind if their coefficients are set to zero. </p>

<p>Thanks for any help on this. Tried all sites, but to no avail. </p>
"
"0.145643816250884","0.0985329278164293","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"0.186096842079694","0.0954747406685002","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.113960576459638","0.0539687072220866","122580","<p>I'm running a logistic regression with backard selection method. I get coefficients with p-values>.10. Here's an example:</p>

<pre><code>    DF   Estimate   Error   Chi-Square  Pr &gt; ChiSq  Estimate    Exp(Est)
Intercept   1   -30,32       11,48       6,97        0,01            -     
v1  1    0,001       0,00        9,70        0,00        0,10        1,00   
v2  1   -0,001       0,00        2,84        0,09       -0,07        1,00   
v3  1    0,000       0,00        0,12        0,73        0,01        1,00   
v4  1   -0,000       0,00        0,11        0,74       -0,01        1,00   
v5  1   -0,000       0,00        0,74        0,39       -0,03        1,00   
v6  1    0,000       0,00        0,58        0,45        0,02        1,00   
v7  1   -0,005       0,00        3,98        0,05       -0,07        1,00   
v8  1    0,002       0,01        0,04        0,84        0,01        1,00   
v9  1   -0,016       0,05        0,09        0,76       -0,02        0,98   
v10 1    0,014       0,03        0,29        0,59        0,03        1,01   
v11 1    0,102       0,03        14,77       0,00        0,09        1,11   
v12 1    0,009       0,01        1,27        0,26        0,05        1,01   
v13 1   -0,017       0,01        2,39        0,12       -0,05        0,98   
v14 1   -0,005       0,01        0,48        0,49       -0,03        1,00   
</code></pre>

<p>My question is, if the algorithm selects best variables, how is it be possible that keeps the variables that have p-values greater than 0.1? I know that the effect is reflected in the value of the coefficient but the pvalue shows the probability that having that value in that coefficient is only a coincidence, and the coefficient is 0 (considering all the other variables). So why is still keeping those?</p>
"
"0.1176979772673","0.0696733014291618","122593","<p>I wanted to check whether the level of satisfaction relates to the level of support to the value of democracy.
Dependent variable (support) is binary variable (Good/Bad) and independent variable is ordinary variable (level of satisfaction).
After doing binary logistic regression, I got this â€œstrangeâ€ figure. Is this result correct? Or how can I fix it?</p>

<pre><code>satisfaction &lt;- data_american$V23c
    support &lt;- data_american$V130b
dat=as.data.frame(cbind(satisfaction,suport)) 
library(ggplot2)
ggplot(dat, aes(x=satisfaction, y=support)) + geom_point() + 
  stat_smooth(method=""glm"", family=""binomial"", se=FALSE)
</code></pre>

<p>Here is a part of data:</p>

<pre><code>   satisfaction support
1             7       1
2             8       1
3             8       1
4             8       1
5            10       1
6             6       1
7             7       1
8             7       1
9             7       1
10            8       0
11            8       1
12            7       1
13            7       0
14            1       1
15            7       1
16            8       1
17            6       1
18            7       1
19            8       1
20            8       1
</code></pre>

<p><img src=""http://i.stack.imgur.com/CgjzY.png"" alt=""results I got""></p>

<p>[Added 1]
Based on the first answer, I got this results:</p>

<pre><code>&gt; satisfaction_j &lt;- jitter(satisfaction)
&gt; chisq.test(table(satisfaction_j,support))

    Pearson's Chi-squared test

data:  table(satisfaction_j, support)
X-squared = 2158, df = 2157, p-value = 0.4899

&gt; t.test(satisfaction_j~support)

    Welch Two Sample t-test

data:  satisfaction_j by support
t = -2.7775, df = 459.931, p-value = 0.005703
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.57390716 -0.09829989
sample estimates:
mean in group 0 mean in group 1 
       7.164214        7.500317 
</code></pre>
"
"0.113960576459638","0.0539687072220866","123172","<p>I am making a table from results of an analysis using generalised linear model which involves detecting association of a categorical predictor variable over multiple outcome variables. Of those multiple outcome variables, few are binary where I display the odds ratio for each category of the predictor (as we do in logistic regression); while few are continuous outcome variables, in which case I can display the beta estimate for each category of the predictor. My question is will it be ok if exponentiate the beta value  and express it as odds ratios. Can I do that?</p>
"
"0.0877268926613025","0.0830902603799674","123210","<p>I am doing a logistic regression in R, where I am modeling how potholes and weather correlate to accidents. When I run a logistic regression, I get the message ""Algorithm does not converge""</p>

<p>The problem I think I have is that I have 24,000 accidents with only 350 potholes related to these accidents. Is this to small of a sample size?</p>

<p>The other possible issue I thought of, is that when I look at sample logistic regressions, the outcome is either zero or one, but the only outcome I have is the outcome of one, or accident in my case. I do not have any non accident data in my set, could this be what is causing the problem? </p>

<p>I will attach my current code and its output.</p>

<pre><code>require(ggplot2)
require(sandwich)
require(msm)
mydata &lt;- read.csv(""C:\\Users\\myname\\downloads\\logreg1.csv"")
## view the first few rows of the data
head(mydata)
summary(mydata)
mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")

&gt; head(mydata)
      Date Pothole Rain Snow Accident
1 1/1/2012       0    0    0        1
2 1/1/2012       0    0    0        1
3 1/1/2012       0    0    0        1
4 1/1/2012       0    0    0        1
5 1/1/2012       0    0    0        1
6 1/1/2012       0    0    0        1
&gt; summary(mydata)
         Date          Pothole            Rain              Snow            Accident
 1/8/2014  :   87   Min.   :0.0000   Min.   :0.00000   Min.   : 0.0000   Min.   :1  
 1/30/2013 :   82   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:1  
 3/21/2013 :   77   Median :0.0000   Median :0.00000   Median : 0.0000   Median :1  
 12/21/2012:   76   Mean   :0.0173   Mean   :0.08077   Mean   : 0.1129   Mean   :1  
 3/10/2013 :   66   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:1  
 12/13/2013:   59   Max.   :8.0000   Max.   :3.32000   Max.   :11.1000   Max.   :1  
 (Other)   :23606                                                                   
&gt; mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")
Warning message:
glm.fit: algorithm did not converge
</code></pre>
"
"0.182482967150453","0.11234482285936","123498","<p>I would like to generate a confidence interval for predicted vs actual rates.</p>

<p>I am auditing my group of anaesthetists (aka anesthesiologists) to see how we compare on a number of potentially preventable complications (eg post-operative nausea, severe pain, hypothermia).</p>

<p>I have 20000 surgical operation records and I can make a GLM to make a ""case-mix adjusted risk"" (using age, gender, type of surgery, duration of surgery as risk factors) and thus <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">generate a risk</a> for each patient.</p>

<p>I can then aggregate the risk and actual per clinician I can generate an actual and predicted rate for each complication. I can make a confidence interval for my actual - but it seems a bit simplistic to just test to see if the confidence interval on the actual rate includes the rate generated from summing the glm-predicted risks.</p>

<p><a href=""http://stats.stackexchange.com/questions/7344/how-to-graphically-compare-predicted-and-actual-values-from-multivariate-regress"">This question</a> has some pointers to a package but I am hoping for some more specific suggestions.</p>

<p>To clarify what I have already (using ""requirement for pain protocol"" as an example):</p>

<pre><code># make model (dependent variable has values 1/0)
model.pp = glm(
pain_protocol1 ~
age + log_age + age2 + inv_age
+ op_time + log_op_time + op_time2
+ gender
+ category
+ thimble,
family = ""binomial"",
data=d4)
# calculate predicted PACU time and then difference between predicted and actual:
d4$pred_pp = predict(model.pp, newdata=d4, type=""response"", na.action=""na.pass"")

d4$extra_pp = d4$pain_protocol1 - d4$pred_pp
# aggregate deviation from predicted rate
ppr_pa &lt;- aggregate(extra_pp ~ adult_anaesthetist, data=d4, FUN=mean)
barplot(ppr_pa$extra_pp, name=ppr_pa$adult_anaesthetist,las=2) 
</code></pre>

<p>So I can make this plot for my colleagues, showing the variation we have in how much pain our patients experience in the ""post anaesthesia care unit"". These variations are great enough so that they definitely represent a material difference in patient experience, and most of the difference will also be unlikely to be variation due to chance (ie ""statistically significant""). However, as I examine smaller subgroups and other complications that are less frequent it would be good to be able to calculate confidence intervals.</p>

<p><img src=""http://i.stack.imgur.com/FFe8W.png"" alt=""bar plot illustrating difference between predicted and actual rates of &quot;needing pain protocol&quot;""></p>

<p>Note that each clinician has a different number of cases, and each clinician is given a bird code-name for anonymity.</p>
"
"0.166450075715296","0.0886796350347864","124512","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 1-75
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>There are two additional variables where the data is only available from 2004:</p>

<pre><code>Pupil reactivity (1 is brisk, 2 is sluggish and 3 is unreactive)
Pupil size (continuous variable in mm - 1-10)
</code></pre>

<p>Based on literature these are significant predictors of outcome. However, out of a series of 2140, I am missing 936. Secondly, the measure is not missing at random, having only been collected in recent years.</p>

<p>My questions are the following in order to address the year range 1994-2013:</p>

<p>1) My data is heavily skewed to later years; how can I adapt the logistic regression to reduce the effect of this when assessing the effect of the year of procedure on outcome?</p>

<p>2) Can I exclude pupil reactivity since it was not collected before 2004 in performing this analysis even if it is a strong predictor?</p>

<p>3) If I should include pupil reactivity, can a multivariate regression be built with the variables above with which to perform imputation to create data for 1994-2003 given 43% of the data is missing?</p>

<p>4) If not possible, could imputation be performed based on data since 2009 where ~15% is missing?</p>

<p>I perform all statistical analyses exclusively with R and would be grateful if you could add known packages/formulae to execute your suggestions.</p>
"
"0.149209419390598","0.0824385620013739","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.187256335179708","0.0985329278164293","125130","<p>I realize that a similar question to this has been asked, but it was not ultimately resolved. I have tried the suggestions posted to that question <a href=""https://stackoverflow.com/questions/23347467/is-there-any-way-to-fit-a-glm-so-that-all-levels-are-included-i-e-no-refer"">here</a>, but have had no success. I am using the following code:       </p>

<pre><code>allinfa4.exp = glm(survive ~ year + julianvisit + class + sitedist + roaddist
+ ngwdist, family = binomial(logexp(alldata$expos)), data=alldata)
summary(allinfa4.exp)

 Call:
glm(formula = survive ~ year + julianvisit + class + sitedist + 
roaddist + ngwdist, family = binomial(logexp(alldata$expos)), 
data = alldata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.6435   0.3477   0.4164   0.4960   0.9488  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  4.458e+00  7.117e-01   6.265 3.74e-10 ***
year2013     3.680e-01  1.862e-01   1.976  0.04819 *  
year2014     2.136e-02  1.802e-01   0.119  0.90564    
julianvisit -5.714e-03  3.890e-03  -1.469  0.14192    
classb       2.863e-02  2.194e-01   0.131  0.89615    
classc      -2.394e-01  2.277e-01  -1.051  0.29304    
classd      -1.868e-01  2.479e-01  -0.754  0.45109    
classe      -4.500e-01  2.076e-01  -2.167  0.03021 *  
classf      -5.728e-01  2.005e-01  -2.858  0.00427 ** 
classg      -8.495e-01  3.554e-01  -2.390  0.01684 *  
classh      -1.858e-01  2.224e-01  -0.835  0.40351    
classi      -3.196e-01  4.417e-01  -0.724  0.46932    
sitedist    -2.607e-04  5.043e-04  -0.517  0.60520    
roaddist     6.768e-05  4.311e-04   0.157  0.87525    
ngwdist     -5.751e-05  9.456e-05  -0.608  0.54306
</code></pre>

<p>The main thing to note here is that I have two categorical variables, <code>year</code> and <code>class</code>, and R has combined the first level of each (2012 and class a) into a reference level intercept term. Not only do I need to know the intercept term for these levels individually, but I also need to know the base intercept terms itself (beta0), just as SAS produces. </p>

<p>I have tried changing the contrasts and deviation coding to accomplish this, but although doing so allows me to extract different levels, it changes the way they are calculated and still does not produce beta0. I've also tried adding +0 and -1, but this also does not provide what I need. Is what I'm trying to do simply impossible in R? It may seem like a strange request, but beta0 is necessary to convert the results of logistic exposure (special kind of logistic regression for nest-survival data) to daily survival rates. Any help would be hugely appreciated. Thanks!</p>

<p>Here is an example of SAS output I want to emulate (taken from a similar analysis done by my lab mate) :</p>

<pre><code>Parameter Estimates
Parameter   Estimate    Standard Error  DF  t Value Pr &gt; |t|              
beta0       7.8404      2.8479          19  2.75    0.0127  
NT         -3.8786      1.8831          19  -2.06   0.0534  
bgdensity  -0.1127      0.1614          19  -0.70   0.4935  
nwh         1.3466      1.4625          19  0.92    0.3687      
NRD        -2.6981      1.9496          19  -1.38   0.1824      
NAGW       -0.4898      2.2518          19  -0.22   0.8301      
</code></pre>
"
"0","0.0311588476424878","125211","<p>I am new to machine learning/statistical modelling.</p>

<p>I am trying to run a classification on a highly sparse dataset with 100 features, most of which are categorical (TRUE/FALSE) with the remaining values missing. To handle missing values, I filled the missing spots with the text 'Nothing', thereby creating a new level.</p>

<p>Next, I am trying to run a logistic regression using a penalty (glmnet package). When I check the coefficients, I see dummy variables corresponding to 'Nothing' having the higher coefficients.</p>

<p>How should I remove these coefficients? What would be a better approach to this?</p>

<p>Or should I just use trees? Please suggest the best way forward.</p>

<p>Thanks!</p>
"
"0.248682365650997","0.158988655288364","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0882734829504749","0.0696733014291618","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.113960576459638","0.0539687072220866","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"0.131590338991954","0.0623176952849756","125764","<p>I'm trying to understand what factors contribute to the a certain outcome which is a ordered factor variable. In order to just understand which factor is statistically more significant than the others, I would like to build a model and given that my output variable is an ordered factorial variable - I thought I should go for Ordinal Logistic Regression. Given the tradeoff between interpretability and flexibility of models, in my case since I'm only making inferences and not predictions, should I rather go for a easier model to handle like Generalized Linear Models? It kind of boils down to me choosing the <code>polr</code> package in R versus the <code>glm</code> one.</p>
"
"0.0657951694959769","0.0311588476424878","126150","<p>I need some help answering a homework question, </p>

<p><a href=""https://www.dropbox.com/s/xs1gflfl063m4lo/IMG_20141201_123018~2.jpg?dl=0"" rel=""nofollow"">Question</a></p>

<p>I have entered the data into R using assignments    </p>

<pre><code>data &lt;- data.frame(category=c(1:8),
                   obese =c(597,380,665,524,1014,365,942,552),
                   number = c(2346,1659,2576,1732,1499,639,1491,769), 
                   male=c(1,0,1,0,1,0,1,0),
                   white =c(1,1,0,0,1,1,0,0),
                   younger =c(1,1,1,1,0,0,0,0))
</code></pre>

<p>I then used  </p>

<pre><code>output = glm(obese ~ male + white + younger, family = binomial)
</code></pre>

<p>to model the data. But this doesn't seem to be working. I've not really used much R before for GLMs or logistic regression and I don't understand how to get GLM from categorial, binary predictor variables.
If someone could explain the theory behind because searching online I have only found answers where the response variable is binary and this seems straight forward. </p>
"
"NaN","NaN","126650","<p>I would like to specify a logistic regression model where I have the following relationship:</p>

<p>$E[Y_i|X_i] = f(\beta x_{i1} + \beta^2x_{i2})$ where $f$ is the inverse logit function. </p>

<p>Is there a ""quick"" way to do this with pre-existing R functions or is there a name for a model like this? I realize I can modify the Newton-Raphson algorithm used for logistic regression but this is a lot of theoretic and coding work and I'm looking for a short cut.</p>

<p>EDIT: getting point estimates for $\beta$ is pretty easy using optim() or some other optimizer in R to maximize the likelihood. But I need standard errors on these guys. </p>
"
"0.131590338991954","0.0830902603799674","126715","<p>I am doing a logistic regression. The predictor variables are a mixture of categorical and continuous. I ran glm and out of the 80 predictors about 36 came out to be significant based on the p value. The accuracy of the model was also very good.</p>

<ol>
<li><p>I am still stuck with 36 variables but I want to narrow it down further to identify which of the predictors have the greatest impact. I understand that all the 36 predictors are statistically significant but all of these variables do not impact the DV equally. Is their a way to rank these variables based on their influence on the DV? Please feel free to suggest any methods/algorithms you know that does this efficiently.</p></li>
<li><p>Once I narrow down the predictors of my interest, I want to come up with rules based on the variables, much like a decision tree gives. I have tried running <code>rpart</code> and <code>ctree</code> on the 80 variable dataset but the output tree is very small meaning only a few variables appear in the tree, and thus there are very few rules which I can make based on that. I wonder if their is a way to increase the size of my tree to include more variables. Suppose I narrow down to 10-12 predictors, what all modeling techniques can I use to makes rules.</p>

<p>For example, I want something like: when x1 in range (a, b), x2 in range (c, d), ... and so on then the probability of $y(dv) &gt; 0.5$ or the event occurs i.e., $y = 1$ so that the range of values of the predictors can act as rules for determining when the event occurs.</p></li>
</ol>
"
"0.158703920171631","0.103342206529982","126768","<hr>

<h2>Original</h2>

<p>I have fitted an ordered logistic regression in R using the <code>polr</code> function, but I am having some trouble bringing the model coefficients into Excel and getting the probabilities there. </p>

<p>For explanatory variables <code>FlowMonth2, Orders_Apt, GeoUnits, HomeOwner, Platform, CreditScore</code>, my coefficients for the model are as follows: </p>

<pre><code>                                Value Std. Error  t value
FlowMonth2Aug                 0.12321    0.03852   3.1990
FlowMonth2Dec                 0.31092    0.03854   8.0672
FlowMonth2Feb                 0.02497    0.03873   0.6447
FlowMonth2Jan                -0.01874    0.03940  -0.4757
FlowMonth2Jul                 0.02924    0.03886   0.7525
FlowMonth2Jun                -0.02618    0.04054  -0.6456
FlowMonth2Mar                 0.09369    0.03739   2.5054
FlowMonth2May                -0.08169    0.03581  -2.2811
FlowMonth2Nov                 0.32610    0.03889   8.3841
FlowMonth2Oct                 0.45240    0.03708  12.2009
FlowMonth2Sep                 0.22771    0.04015   5.6711
Orders_Apty                   0.03786    0.02206   1.7160
GeoUnits1                    -0.04070    0.03260  -1.2487
GeoUnits2                     0.11923    0.03735   3.1920
GeoUnitsOther                 0.30464    0.20803   1.4644
GeoUnits5                    -0.19669    0.01892 -10.3942
HomeOwnery                    0.16577    0.02828   5.8624
PlatformMobile               -0.32933    0.01631 -20.1882
CreditScore525 - 600          1.01909    0.02937  34.7036
CreditScore600 - 700          1.12578    0.02953  38.1284
CreditScore700 - 800          1.29098    0.03091  41.7694
CreditScore800 - 900          1.43500    0.03085  46.5179
CreditScore900+               1.33816    0.02851  46.9414
CreditScoreHit with No Score  0.33832    0.03424   9.8812
CreditScoreNo Hit             0.37199    0.06443   5.7737
</code></pre>

<p>The intercepts are </p>

<pre><code>Intercepts:
                Value    Std. Error t value 
0|1              -1.2788   0.0377   -33.9349
1|2              -0.6609   0.0371   -17.8175
2|3              -0.1683   0.0369    -4.5571
3|4               0.1520   0.0369     4.1159
4|5               0.3813   0.0370    10.3163
5|6               0.5615   0.0370    15.1714
6|7               0.7314   0.0371    19.7357
7|8               0.8551   0.0371    23.0486
8|9               0.9608   0.0371    25.8740
9|10              1.0510   0.0372    28.2760
10|11             1.1342   0.0372    30.4826
11|12             1.2607   0.0373    33.8295
12|13             1.4770   0.0374    39.5140
13|14             1.5414   0.0374    41.1957
14|15             1.5827   0.0374    42.2710
15|16             1.6127   0.0375    43.0505
16|Still Active   1.6358   0.0375    43.6499
</code></pre>

<hr>

<p>Now when I bring this into Excel, I bring in the coefficients, select certain values to add together, say <code>FlowMonth2 = ""Aug"", Orders_Apt = ""n"", GeoUnits = ""5"", HomeOwner = ""y"", Platform = ""Desktop"", CreditScore = ""800 - 900""</code>. </p>

<p>I add these values together to get my logit statistic, $T = \mathbf{x}\mathbf{\beta}$, and then I add this $T$ to each different intercept to get $\beta_{0, i} - T$ for $1 \leq i \leq 17$ where the $17$th stage is transition from 16 to Still Active. </p>

<p>I then take $\mathrm{logit}(\beta_{0, i} - T)$ or ${1 \over 1 + \exp(-[\beta_{0, i} - T])} = \Pr(\text{being in the $i$th stage})$</p>

<p>But when I try to do this in Excel, and compare it to the output of <code>predict</code> in R, then I can't get these values to match up? What am I doing wrong in Excel? </p>

<hr>

<h2>Edit</h2>

<p>To compare the values from R and Excel, it's by more than a rounding error that they differ: </p>

<p>R: </p>

<pre><code>0                                                0.048650293
1                                                0.037989009
2                                                0.047738406
3                                                0.041799312
4                                                0.035787006
5                                                0.031644868
6                                                0.032650167
7                                                0.025400235
8                                                0.022740618
9                                                0.020074660
10                                               0.019006405
11                                               0.029758088
12                                               0.052613086
13                                               0.015949280
14                                               0.010274309
15                                               0.007485204
16                                               0.005777627
Still Active                                     0.514661427
</code></pre>

<p>Excel: </p>

<pre><code>0|1 0.048648622
1|2 0.086640673
2|3 0.134381676
3|4 0.176177948
4|5 0.211958542
5|6 0.243615257
6|7 0.27626595
7|8 0.301669593
8|9 0.32439208
9|10    0.344464821
10|11   0.363487303
11|12   0.393228837
12|13   0.44584823
13|14   0.461809529
14|15   0.472089045
15|16   0.479571379
16|Still Active 0.485339204
</code></pre>

<hr>

<h2>Edit 2</h2>

<p>Why are there only 17 intercepts in Excel, but 18 predicted points in R? </p>
"
"0.131590338991954","0.0623176952849756","127068","<p>The data is about:
a binary variable as response; age; some binary categorical variables (0,1); some categorical variables which have more than 2 outcomes </p>

<p>The goal is to find which factors affect the response binary variable.</p>

<p>I intend to use logistic regression to fit this model, but I think more analyses are supposed to be applied.
I'm unfamiliar with handling categorical variables when the response variable is also a 0/1 variable. Could u give me some advice about this? </p>

<p>Thanks!  :)</p>
"
"0.139572631559771","0.115671320425298","127134","<p><strong>Updated</strong></p>

<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure (expressed as decimal of year) = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 16-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
Mechanism - Mechanism of injury = Fall &lt;2m, Fall &gt;2m, Shooting/stabbing, RTC (Road Traffic Collision), Other
neuroFirst - Location of first admission (Neurosurgical Unit) = NSU vs. Non-NSU
rcteye - Pupil reactivity = NA / Both unreactive = O, 1 reactive = 1, both reactive = 2
rcteyeYN - dummy = 0 or 1 for presence or absence of data
GCS - Glasgow Coma Scale = 3-15
GCSYN - dummy = 0 or 1 for presence or absence of data
</code></pre>

<p>Dummy variables were included to enable a larger sample size where the majority of cases were excluding  <code>GCS</code> and <code>rcteye</code> variables (missing not at random).</p>

<p>In order to test for interactions, initially I ran the following:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN + GCS + GCSYN + rcs(Yeardecimal))^2, data = ASDH_Paper1.1)
</code></pre>

<p>but when I did I got the following error:</p>

<pre><code>singular information matrix in lrm.fit (rank= 151 ).  Offending variable(s):
GCSYN * Yeardecimal''' GCSYN * Yeardecimal' GCSYN * Yeardecimal GCS * Yeardecimal''' GCS * Yeardecimal GCS * GCSYN rcteyeYN * Yeardecimal''' rcteyeYN * Yeardecimal'' rcteyeYN * Yeardecimal rcteyeYN * GCSYN rcteye * Yeardecimal''' rcteye * Yeardecimal rcteye * rcteyeYN Mechanism=RTC * Yeardecimal''' Mechanism=Other * Yeardecimal''' Mechanism=Fall &gt; 2m * Yeardecimal''' Mechanism=Shooting / Stabbing * Yeardecimal Mechanism=RTC * Yeardecimal Mechanism=Other * Yeardecimal Mechanism=Fall &gt; 2m * Yeardecimal neuroFirst * Yeardecimal ISS'' * Yeardecimal''' ISS * Yeardecimal''' ISS'' * Yeardecimal'' ISS'' * Yeardecimal ISS' * Yeardecimal ISS * Yeardecimal ISS'' * GCSYN ISS'' * rcteyeYN ISS'' * Mechanism=RTC Age'' * Yeardecimal''' Age'' * Yeardecimal'' Age''' * Yeardecimal' Age''' * Yeardecimal Age'' * Yeardecimal Age' * Yeardecimal Age * Yeardecimal Age'' * GCSYN Age''' * rcteyeYN 
Error in lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + neuroFirst + Mechanism +  : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>The only way I could run the model is with an adjustment. <code>Yeardecimal</code> is excluded from any interaction as is the interaction of <code>GCS:GCSYN</code> and <code>rcteye:rcteyeYN</code> which produced the same error as written above. It made sense to exclude the interactions between a variable and its missing dummy but I am not sure what to do about <code>Yeardecimal</code>:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN) * (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + GCS + GCSYN) + rcs(Yeardecimal), data = ASDH_Paper1.1)
</code></pre>

<p>From this model the following interactions were identified with an <code>anova</code> output:</p>

<pre><code>&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor                                                Chi-Square d.f. P     
 Age  (Factor+Higher Order Factors)                    130.42      52  &lt;.0001
  All Interactions                                      78.68      48  0.0034
  Nonlinear (Factor+Higher Order Factors)               46.53      39  0.1901
 ISS  (Factor+Higher Order Factors)                    181.65      42  &lt;.0001
  All Interactions                                      52.43      39  0.0738
  Nonlinear (Factor+Higher Order Factors)               55.01      28  0.0017
 neuroFirst  (Factor+Higher Order Factors)              37.68      16  0.0017
  All Interactions                                      11.54      15  0.7136
 Mechanism  (Factor+Higher Order Factors)               63.72      52  0.1277
  All Interactions                                      58.35      48  0.1455
 rcteye  (Factor+Higher Order Factors)                 242.07      15  &lt;.0001
  All Interactions                                      19.39      14  0.1507
 rcteyeYN  (Factor+Higher Order Factors)               204.58      15  &lt;.0001
  All Interactions                                      29.88      14  0.0079
 GCS  (Factor+Higher Order Factors)                    162.81      15  &lt;.0001
  All Interactions                                      11.62      14  0.6365
 GCSYN  (Factor+Higher Order Factors)                   94.50      15  &lt;.0001
  All Interactions                                      41.74      14  0.0001
 Yeardecimal                                            51.96       4  &lt;.0001
  Nonlinear                                             10.27       3  0.0164
 Age * ISS  (Factor+Higher Order Factors)               11.90      12  0.4534
  Nonlinear                                              9.40      11  0.5851
  Nonlinear Interaction : f(A,B) vs. AB                  9.40      11  0.5851
  f(A,B) vs. Af(B) + Bg(A)                               7.96       6  0.2411
  Nonlinear Interaction in Age vs. Af(B)                 8.75       9  0.4605
  Nonlinear Interaction in ISS vs. Bg(A)                 8.58       8  0.3790
 Age * neuroFirst  (Factor+Higher Order Factors)         2.66       4  0.6166
  Nonlinear                                              2.05       3  0.5624
  Nonlinear Interaction : f(A,B) vs. AB                  2.05       3  0.5624
 Age * Mechanism  (Factor+Higher Order Factors)         17.58      16  0.3493
  Nonlinear                                             13.82      12  0.3127
  Nonlinear Interaction : f(A,B) vs. AB                 13.82      12  0.3127
 Age * GCS  (Factor+Higher Order Factors)                6.24       4  0.1819
  Nonlinear                                              3.89       3  0.2741
  Nonlinear Interaction : f(A,B) vs. AB                  3.89       3  0.2741
 Age * GCSYN  (Factor+Higher Order Factors)             20.11       4  0.0005
  Nonlinear                                              8.86       3  0.0312
  Nonlinear Interaction : f(A,B) vs. AB                  8.86       3  0.0312
 ISS * neuroFirst  (Factor+Higher Order Factors)         3.23       3  0.3571
  Nonlinear                                              0.87       2  0.6480
  Nonlinear Interaction : f(A,B) vs. AB                  0.87       2  0.6480
 ISS * Mechanism  (Factor+Higher Order Factors)         23.95      12  0.0206
  Nonlinear                                             20.66       8  0.0081
  Nonlinear Interaction : f(A,B) vs. AB                 20.66       8  0.0081
 ISS * GCS  (Factor+Higher Order Factors)                0.77       3  0.8570
  Nonlinear                                              0.42       2  0.8102
  Nonlinear Interaction : f(A,B) vs. AB                  0.42       2  0.8102
 ISS * GCSYN  (Factor+Higher Order Factors)              6.53       3  0.0886
  Nonlinear                                              2.35       2  0.3085
  Nonlinear Interaction : f(A,B) vs. AB                  2.35       2  0.3085
 neuroFirst * Mechanism  (Factor+Higher Order Factors)   2.45       4  0.6533
 neuroFirst * GCS  (Factor+Higher Order Factors)         0.00       1  0.9726
 neuroFirst * GCSYN  (Factor+Higher Order Factors)       1.39       1  0.2382
 Mechanism * GCS  (Factor+Higher Order Factors)          0.10       4  0.9987
 Mechanism * GCSYN  (Factor+Higher Order Factors)        1.74       4  0.7828
 Age * rcteye  (Factor+Higher Order Factors)             8.66       4  0.0702
  Nonlinear                                              7.29       3  0.0633
  Nonlinear Interaction : f(A,B) vs. AB                  7.29       3  0.0633
 ISS * rcteye  (Factor+Higher Order Factors)             4.18       3  0.2424
  Nonlinear                                              1.49       2  0.4744
  Nonlinear Interaction : f(A,B) vs. AB                  1.49       2  0.4744
 neuroFirst * rcteye  (Factor+Higher Order Factors)      0.10       1  0.7460
 Mechanism * rcteye  (Factor+Higher Order Factors)       3.44       4  0.4867
 rcteye * GCS  (Factor+Higher Order Factors)             2.30       1  0.1297
 rcteye * GCSYN  (Factor+Higher Order Factors)           2.57       1  0.1090
 Age * rcteyeYN  (Factor+Higher Order Factors)           7.23       4  0.1242
  Nonlinear                                              7.23       3  0.0649
  Nonlinear Interaction : f(A,B) vs. AB                  7.23       3  0.0649
 ISS * rcteyeYN  (Factor+Higher Order Factors)           2.47       3  0.4814
  Nonlinear                                              0.11       2  0.9462
  Nonlinear Interaction : f(A,B) vs. AB                  0.11       2  0.9462
 neuroFirst * rcteyeYN  (Factor+Higher Order Factors)    0.12       1  0.7280
 Mechanism * rcteyeYN  (Factor+Higher Order Factors)     1.81       4  0.7701
 rcteyeYN * GCS  (Factor+Higher Order Factors)           3.70       1  0.0543
 rcteyeYN * GCSYN  (Factor+Higher Order Factors)         8.74       1  0.0031
 TOTAL NONLINEAR                                       102.74      64  0.0015
 TOTAL INTERACTION                                     178.52     103  &lt;.0001
 TOTAL NONLINEAR + INTERACTION                         241.87     111  &lt;.0001
 TOTAL                                                 889.91     123  &lt;.0001
</code></pre>

<p>The <code>summary</code> function revealed the following results:</p>

<pre><code>             Effects              Response : Survive 

 Factor                                    Low    High   Diff. Effect       S.E.   Lower 0.95 Upper 0.95    
 Age                                         37.6   72.0 34.40         0.15   0.38   -0.58      8.900000e-01
  Odds Ratio                                 37.6   72.0 34.40         1.16     NA    0.56      2.430000e+00
 ISS                                         20.0   26.0  6.00        -1.34   0.31   -1.95     -7.400000e-01
  Odds Ratio                                 20.0   26.0  6.00         0.26     NA    0.14      4.800000e-01
 neuroFirst                                   0.0    1.0  1.00        -0.23   0.37   -0.95      5.000000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.80     NA    0.39      1.650000e+00
 rcteye                                       0.0    2.0  2.00         3.20   0.50    2.22      4.170000e+00
  Odds Ratio                                  0.0    2.0  2.00        24.41     NA    9.24      6.452000e+01
 rcteyeYN                                     0.0    1.0  1.00        -3.34   0.44   -4.21     -2.480000e+00
  Odds Ratio                                  0.0    1.0  1.00         0.04     NA    0.01      8.000000e-02
 GCS                                          0.0   12.0 12.00         1.94   0.49    0.98      2.890000e+00
  Odds Ratio                                  0.0   12.0 12.00         6.94     NA    2.67      1.799000e+01
 GCSYN                                        0.0    1.0  1.00        -1.32   0.45   -2.20     -4.400000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.27     NA    0.11      6.400000e-01
 Yeardecimal                               2005.5 2012.4  6.85         0.20   0.12   -0.03      4.400000e-01
  Odds Ratio                               2005.5 2012.4  6.85         1.22     NA    0.97      1.550000e+00
 Mechanism - Fall &gt; 2m:Fall &lt; 2m              1.0    2.0    NA        -0.89   0.35   -1.58     -2.000000e-01
  Odds Ratio                                  1.0    2.0    NA         0.41     NA    0.21      8.200000e-01
 Mechanism - Other:Fall &lt; 2m                  1.0    3.0    NA         0.25   0.42   -0.58      1.080000e+00
  Odds Ratio                                  1.0    3.0    NA         1.28     NA    0.56      2.930000e+00
 Mechanism - RTC:Fall &lt; 2m                    1.0    4.0    NA        -0.68   0.43   -1.52      1.700000e-01
  Odds Ratio                                  1.0    4.0    NA         0.51     NA    0.22      1.190000e+00
 Mechanism - Shooting / Stabbing:Fall &lt; 2m    1.0    5.0    NA        18.97 116.63 -209.63      2.475600e+02
  Odds Ratio                                  1.0    5.0    NA 172906690.96     NA    0.00     3.272814e+107

Adjusted to: Age=54.2 ISS=25 neuroFirst=0 Mechanism=Fall &lt; 2m rcteye=1 rcteyeYN=0 GCS=3 GCSYN=0 
</code></pre>

<p>Remaining questions are:</p>

<p><strong>1</strong> - Is my dummy variable treatment for variables missing not at random appropriate, including the exclusion of interactions with the main term?</p>

<p><strong>2</strong> - Can I resolve the issues with assessing interaction of the Yeardecimal term?</p>

<p><strong>3</strong> - Should I exclude non-significant interaction terms? I read that exclusion only of a ""chunk"" is advised - <a href=""http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model"">Including the interaction but not the main effects in a model</a></p>

<p><strong>4</strong> - Is the odds ratio for each variable the ""Effect"" column? If so, is this the OR between the lowest and highest value of each variable?</p>
"
"NaN","NaN","127226","<p>I'm trying to simulate a logistic regression. My goal is showing that if <code>Y=1</code> is rare, than the intercept is biased. In my R script I define the logistic regression model through the latent variable's approach (see for example pp. 140 <a href=""http://gking.harvard.edu/files/abs/0s-abs.shtml"" rel=""nofollow"">http://gking.harvard.edu/files/abs/0s-abs.shtml</a>):</p>

<pre><code>x   &lt;- rnorm(10000)

b0h &lt;- numeric(1000)
b1h &lt;- numeric(1000)

for(i in 1:1000){
  eps &lt;- rlogis(10000)
  eta &lt;- 1+2*x+eps
  y   &lt;-numeric(10000)
  y   &lt;- ifelse (eta&gt;0,1,0)

  m      &lt;- glm(y~x,family=binomial)
  b0h[i] &lt;- coef(m)[1]
  b1h[i] &lt;- coef(m)[2]
}

mean(b0h)
mean(b1h)
hist(b0h)
hist(b1h)
</code></pre>

<p>The problem here is that I don't know how to force the observations y to be balanced before (50:50), then unbalanced (90:10). As we can see with the function table(), in my script the proportion of ones is random.</p>

<pre><code>table(y)
</code></pre>

<p>How to solve this problem?</p>
"
"0.0657951694959769","0.0311588476424878","127385","<p>I'm running a logistic regression in which I'm predicted a binary response from a continuous predictor... I'm interested in determining the exact point in which the predicted probability <code>(exponentiated y-hat / (1 + exponentiated y-hat)</code> becomes significantly different than the average y-hat (or average predicted probability of the discrete event...</p>

<p>I was thinking of some sort of Johnson Neyman technique, but am unsure how to begin this exercise...</p>

<p>Would like to do this in <code>R</code> at the end of the day, but not required for an answer.</p>
"
"NaN","NaN","127415","<p>I'm building a logistic regression model and one of the variables I have is postcode, I might be over thinking this but is it fine for me to leave postcode as is and regress it as:</p>

<pre><code>fitlogit &lt;- glm(target~postcode+..., family=""binomial"", data = dat)
</code></pre>

<p>I was thinking it might be better to combine postcodes into 4 distinct areas and model it as dummy variables once I figure out rough ranges for the groups. </p>
"
"NaN","NaN","128667","<p>I want to fit a logistic function of the form $$f(t) = \frac{C}{1+ab^{-t}}$$to some data that I have, using <code>R</code>.</p>

<p>There is some uncertainty to $f(t)$, and its magnitude is assumed to be constant. There is no uncertainty in $t$.</p>

<p>Answers to the question <a href=""http://stats.stackexchange.com/questions/8436/logistic-regression-for-bounds-different-from-0-and-1"">Logistic regression for bounds different from 0 and 1</a> mention that it's possible, but don't specify how. I am interested in estimating $C$, $a$ and $b$ and also $R^2$.</p>
"
"0.131590338991954","0.0623176952849756","129260","<p>I'm currently estimating a survival model (accelerated failure time model) with a log-logistic distribution in R using the survival package and the survreg function. I want to simulate expected survival times in line with King et al (2001), but I am unsure of the link function needed to calculate the expected survival time for the log-logistic distribution from the survreg regression output. I have added a minimal working example below.</p>

<pre><code>library(survival)
data(kidney)

survreg(formula = Surv(time, status) ~ 
                  age + 
                  cluster(id), 
                  data = kidney, 
                  dist = ""loglogistic"", 
                  robust = TRUE)

               Value Std. Err (Naive SE)     z        p
(Intercept)  4.38127   0.6783     0.5338  6.46 1.05e-10
age         -0.00298   0.0135     0.0114 -0.22 8.26e-01
Log(scale)  -0.23009   0.0732     0.1038 -3.14 1.67e-03

Scale= 0.794 

Log logistic distribution
Loglik(model)= -342   Loglik(intercept only)= -342
    Chisq= 0.07 on 1 degrees of freedom, p= 0.79 
(Loglikelihood assumes independent observations)
Number of Newton-Raphson Iterations: 3 
n= 76 
</code></pre>

<p>I simply want to know how I can calculate expected survival time from the estimated parameters from the survreg output.</p>

<p>King, G., Tomz, M., &amp; Wittenberg, J. (2000). Making the most of statistical analyses: Improving interpretation and presentation. American journal of political science, 347-361.</p>
"
"0.149209419390598","0.0824385620013739","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.134303827337563","0.0763232776972177","129657","<p>What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? </p>

<p>Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or ""with shrinkage"".</p>

<p>I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge.</p>

<p>There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential.</p>

<p>This could be done via <code>lme4</code> in <code>R</code>. However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model?</p>

<p>EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc.</p>

<p>EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts.</p>

<p>EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).</p>
"
"0.140675989690666","0.083275523174913","129739","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as <code>Outcome30</code> measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure = 1994.0-2013.99
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
ISS - Injury Severity Score = 1-75
Age - Age of patient = 16.0-101.5
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both)
neuroFirst2 - Location of admission (Neurosurgical unit or not)
Other - other traums (0 - No, 1 - Yes)
othopYN - Other operation required
LOS - Length of stay in days
LOSCC - Length of stay in critical care in days 
</code></pre>

<p>When I conduct univariate analysis of the variables, I have conducted a logistic regression for each continuous variable. I am unable to model Yeardecimal however, with the following result:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimal, data = ASDH_Paper1.1)
singular information matrix in lrm.fit (rank= 1 ).  Offending variable(s):
Yeardecimal 
Error in lrm(formula = Survive ~ Yeardecimal, data = ASDH_Paper1.1) : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>However, the restricted cubic spline works:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ rcs(Yeardecimal), data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ rcs(Yeardecimal), data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2     106.61    R2       0.027    C       0.578    
 0           1281    d.f.             4    g        0.319    Dxy     0.155    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.376    gamma   0.160    
max |deriv| 2e-08                          gp       0.057    tau-a   0.052    
                                           Brier    0.165                     

               Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept      -68.3035 45.8473 -1.49  0.1363  
Yeardecimal      0.0345  0.0229  1.51  0.1321  
Yeardecimal'     0.1071  0.0482  2.22  0.0262  
Yeardecimal''   -2.0008  0.6340 -3.16  0.0016  
Yeardecimal'''  11.3582  4.0002  2.84  0.0045  
</code></pre>

<p>Could anyone explain why this is? I am nervous about using a mode complicated model if I am unable to model with a simpler approach.</p>

<p>I am currently using restricted cubic splines to model Age, ISS and Yeardecimal. Would anyone recommend any alternative approach?</p>
"
"0.150388958847947","0.142440446365658","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.0657951694959769","0.0311588476424878","129864","<p>I have to model the 4 seasons on basis of temperature and precipitation.
I have no idea which model to use. I'm a new R user and I've used only linear model and logistic regression till now. Is there another model that could fit well with this kind of dataset ?</p>
"
"NaN","NaN","130151","<p>I am finding that trying to do a stepwise logistic regression is far too slow on my data set (6 hours). Is anyone aware of any faster solutions out there? Perhaps one that takes advantage of the multiple processors on my machine?</p>

<pre><code>model &lt;- glm(y ~ .) # 30 or so independent variables
# start timer
step(model, trace=1, direction=""both"", k=log(179188))
# end timer -- 6 hours
</code></pre>
"
"0","0.0440652649239232","130414","<p>How to perform a logistic regression and/or SVM on sparse data in R?  I have $ 10^6 $ observations, $ 10^4 $ TRUE/FALSE features, and the data is sparse, i.e.</p>

<pre><code>1,0,0,0,0,0,1,0,0,...
0,0,0,1,0,0,0,0,0,...
0,0,0,0,0,0,1,0,1,...
...
</code></pre>

<p>The sparseness is about 99%.</p>

<p>The package should be able to estimate feature significances and/or perform feature selection of some kind.</p>
"
"NaN","NaN","130813","<p>used the glm function in R to model a logistic regression of a binary repsonse and 3 categorical predictors. My problem is that according to the summary non of the levels is significant. </p>
"
"0.162834736819732","0.0881305298478463","131331","<p>This is the first time I am posting a question, so please excuse any etiquette violations and poorly worded questions!</p>

<p>I am working on the analysis for a chapter of my thesis. I am examining the behavioural response of an animal to a visual stimulus, and trying to determine which of eight explanatory variables (and their two-way interactions) affect this response. I recorded the response on an ordinal scale of 0 (no response), 1 (attention to but no avoidance of stimulus) or 2 (escape response to stimulus). I am leaning towards collapsing categories and using logistic regression where a 1 is an escape response, and 0 is anything else because logistic regression seems much easier to interpret. </p>

<p>I have 794 observations. I am including observer and location (because field sites differed) as random effects, although I am unsure this is a good approach. </p>

<p>I am having trouble with model selection. I ran all possible subsets using the dredge function in packing 'MuMIn'. I thought I was avoiding data dredging by </p>

<ul>
<li>including main effects which were selected because I thought they would have an effect (rather than all conceivable variables)</li>
<li>including only the two-way interactions of interest (R will not run if the global model includes all possible two-way interactions because of the huge number of terms/models)</li>
</ul>

<p>I've come to realise that the second point may be problematic because it leads to an unbalanced model set as in Burnham and Anderson (2002). </p>

<blockquote>
  <p>Page 169: When assessing the relative importance of variables using sums of the AIC    weights, it is important to achieve a balance in the number of models that contain each variable j.</p>
</blockquote>

<p>My questions are</p>

<ol>
<li><p>Is it possible to have a balanced model set without it being considered data dredging? If so, how? </p></li>
<li><p>Is my approach at all reasonable? If not, are there other avenues I should explore? I started with Hosmer&amp;Lemeshow purposeful forward selection, as advocated by my supervisor, but I had some issues with this which I can elaborate on if necessary. </p></li>
</ol>
"
"0.0930484210398471","0.0440652649239232","131349","<p>Following Dormann et al 2007 Ecography, I have employed a GLMM approach in R to account for spatial autocorrelation in a binomial regression model (logistic regression) that does not have random terms. Using data from mtcars (just so we all have the same numbers), my code looks as follows:</p>

<pre><code>library(MASS)
data &lt;- mtcars
data$group &lt;- factor(rep(""A"", nrow(data)))
mod1 &lt;- glmmPQL(vs ~ mpg, random=~1|group, correlation=corExp(form=~disp+qsec),
        data=data, family=binomial)
</code></pre>

<p>Question 1: Is this a reasonable way to account for spatial autocorrelation in a binomial model, as Dormann et al suggest?</p>

<p>Question 2: Should/can I demonstrate that there is no longer spatial autocorrelation (SAC) in the error for this model? Can a vario/correlagram be used? How? I have had trouble finding information about how to look for SAC in GLMMs, and the variogram I've managed to create looks very funky -- hard to interpret.</p>
"
"0.246731885609913","0.124635390569951","131456","<p>I'm exploring the effects of removing the intercept in a logistic regression model.</p>

<p>Assume a model:</p>

<p>$$logit(Y = 1) = \beta_1 x + \beta_2z + 0$$</p>

<p>with $x$ and $z$ being categorical variables with 2 levels each and no intercept.</p>

<p>I understood that having no intercept with categorical predictors produce coefficients that compare the $P(Y = 1)$ in each level of the two predictor against a null case where $P(Y=1) = 0.5$ or $logit(Y=1) = 0$.</p>

<p>I noticed a phenomenon that can understand. Using glm() function in R if you change the order of the variable in the right hand part of the formula, the coefficients change too. But even more oddly, the coefficient of the first variable is always the same.</p>

<p>Here's an <code>R</code> demo:</p>

<pre><code>y &lt;- as.factor(sample(rep(1:2), 30, T))
x &lt;- as.factor(sample(rep(1:2), 30, T))
z &lt;- as.factor(sample(rep(1:2), 30, T))

coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ z + x - 1, binomial))
#        z1         z2         x2 
#-0.1764783 -0.3099976  0.5025523 
</code></pre>

<p>As you can see the first predictors have the same coefficient while the other are different in the two models.</p>

<p>Here is what I expected and instead behave differently than what I though:</p>

<ol>
<li>Since every level of the two predictors is compared to the same null case, I expected to have the same coefficients in the two models, independently from the order in which I use them.</li>
<li>I expected to see the coefficients of every level of every predictor, instead the coefficient for the 1 level of the second predictor is not shown.</li>
<li>I therefore assume that only the first variable is compared against the null case, while the second is compared against a reference level; but what is this level? Is it $P(Y = 1 | X = 1 \cap Z = 1)$? Reproducing one of the models WITH the intercept we get:</li>
</ol>

<p>`(for some reason stackexchange don't understand the following is code without the tick)   </p>

<pre><code>coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ x + z, binomial))
#(Intercept)         x2          z2 
#-0.1764783   0.5025523  -0.1335192
</code></pre>

<p>As expected x1 become the intercept, and x2 is likely relative to x1. z1 is missing also in this case and z2 is the same as in the model without intercept.</p>

<p>Thus should I assume that the comparison against the null case $P(Y = 1) = 0.5$ is made only for the first variable in a formula, while the other are compared against the usual intercept?
Is this behavior normal?
What about the fact that the first coefficient has the same value whichever the order of the predictors in the formula?
What if I want to compare all level of each predictor against the null case and have a coefficient for all levels?
Or it's theoretically impossible for some reason I don't get?</p>
"
"0.149209419390598","0.0824385620013739","133248","<p>Iâ€™m analyzing student performance data. In my dataset, each row corresponds to a student and each column contains several performance metrics (continuous) and the student type (categorical, 4 types). The student type was computed in another analysis, using Expectation-Maximization, based on how students were graded over time. My sample is small, with 50 students.</p>

<p>I want to understand what characterizes each student type, regarding the performance features I have. I want to understand things like â€œthe more grade they have the more likely is to belong to a particular clusterâ€ and so on, if they are present at all in my data.</p>

<p>My first question is:
1) I believe that what I need is Multinomial Logistic Regression. Am I right or is there a better way to achieve this?</p>

<p>If yes, Iâ€™ve been exploring Multinomial Logistic Regression in R, using the multinom of the nnet package, but I need help with the following:</p>

<p>2) Understanding if the model is any good. So far I have the percentage of correctly classified instances, but I know this is not a very good goodness of fit measure. </p>

<p>3) How to assess how good each individual predictor is. I know how to look for the exponentiated B, but I donâ€™t know how to assess its significance. I read that using the t-distribution to compute the p-value here is usually a mistake. I found a <a href=""http://stats.stackexchange.com/questions/63222/getting-p-values-for-multinorm-in-r-nnet-package"">similar post here</a>, but a clear answer was not provided.</p>

<p>Thank you in advance for any answer, suggestion or comment.</p>
"
"0.0657951694959769","0.0311588476424878","133281","<p>I've been using the glm.fit function in R to fit parameters to a logistic regression model.  By default, glm.fit uses iteratively reweighted least squares to fit the parameters.  What are some reasons this algorithm would fail to converge, when used for logistic regression?</p>
"
"0.0657951694959769","0.0311588476424878","133320","<p>I am using logistic regression to solve the classification problem.</p>

<pre><code>g = glm(target ~ ., data=trainData, family = binomial(""logit""))
</code></pre>

<p>There are two classes (target): 0 and 1 </p>

<p>When I run the prediction function, it returns probabilities.</p>

<pre><code>p = predict(g, testData, type = ""response"")
</code></pre>

<p>However, it is not clear to me how to understand which class has been assigned?</p>

<pre><code>Real  p 

1   0.17568578
1   0.41698474
1   0.19151927
1   0.25587242
1   0.25604452
0   0.39976069
0   0.39910282
0   0.16879320
</code></pre>

<p>I appreciate if someone can explain me how this works based on the above example. Thanks</p>
"
"0.18113336514069","0.135818268070785","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"NaN","NaN","134139","<p>Could some one suggest a method for gaining high accuracy with weak predictors in logistic regression?</p>

<p>Thanks in advance </p>
"
"0.0882734829504749","0.0696733014291618","134837","<p>My data has a binary outcome (attack or not attack), day (20 day in repeated measured design) and some covariates (nestlingâ€™s movement).
The objectives of my experiment are testing the effect of time and other factors and selecting useful variables affecting outcomes.</p>

<p>My data look like below</p>

<pre><code>subject outcome Day nestling.move
   1        A    1      N 
   2        A    1      Y 
   3        A    1      Y 
   4        N    1      Y 
   5        N    1      Y 
   6        N    1      Y 
   7        N    1      Y 
   8        N    1      N 
   9        N    1      N 
   .        .    .      . 
   .        .    .      . 
   1        A    20     N 
   2        A    20     N   
</code></pre>

<p>First of all, I simply transformed outcomes to ratios(attack rate for each day) and test if there is a correlation between attack rates and days by using Spearmanâ€™s rank correlation. But I think it is not a good way to test the effect of time on outcome.</p>

<p>I checked other <a href=""http://stats.stackexchange.com/questions/81246/unable-to-fit-repeated-measures-in-r"">post</a>. and I think I should used an AR1 model with logistic regression since it could be a time-varying processes. However, I don't know how to do this with R or SPSS. </p>

<p>Is this the correct syntax to use in R?</p>

<pre><code>model&lt;- glmmPQL(outcome ~  nestling.move + Day, data=mydata, family=binomial,  random = ~ 1 | subject, correlation = corAR1(form=~Day|subject)) 
</code></pre>
"
"NaN","NaN","134885","<p>Since a feedforward NN with a logistic function as activation function is not linear, does it make sense to reduce variables first with principal components or discriminant analysis?</p>

<p>Because shouldn't be done this before training the NN as with logistic regression?</p>
"
"0.0949671470496983","0.0989426299071587","135792","<p>I'm interested in building a set of candidate models in R for an analysis using logistic regression. Once I build the set of candidate models and evaluate their fit to the data using AICc (<code>aicc = dredge(results, eval=TRUE, rank=""AICc"")</code>), I would like to use k-fold cross fold validation to evaluate the predictability of the final model chosen from the analysis. I have a few questions associated to k-fold cross validation: </p>

<ol>
<li><p>I assume you use your entire data set for initially building your candidate set of models. For example, say I have 20,000 data values, wouldn't I first build my candidate set of models based on the entire 20,000 data values? Then do use AIC to rank the models and select the most parsimonious model?</p></li>
<li><p>After you select the final model (or model averaged model), would you then conduct a k-fold cross validation to evaluate the predictability of the model? </p></li>
<li><p>What is the easiest way to code a k-fold cross-validation in R? </p></li>
<li><p>Does the k-fold cross validation code break up your entire data set (e.g., 20,000 data values) into training and validation sets automatically? Or do you have to subset the data manually? </p></li>
</ol>
"
"0.138865930150177","0.0939474604818017","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.147122471584125","0.0696733014291618","136040","<p>I'm implementing a logistic regression model in R and I have 80 variables to chose from. I need to automatize the process of variable selection of the model so I'm using the step function.</p>

<p>I've no problem using the function or finding the model, but when I look at the final model I find that some of the variables chosen by the step function are not significant (I look at this using the summary function and looking at the fourth column in $coef, this is the Wald Test). This is a problem because I need all the variables included in the model to be significant.</p>

<p>Is there any function or any way to get the best model based on AIC or BIC methods but that also consider that all the coefficients must be significant?
Thanks</p>
"
"0.0930484210398471","0.0440652649239232","136085","<p>I'm building a logistic regression in R using LASSO method with the functions <code>cv.glmnet</code> for selecting the <code>lambda</code> and <code>glmnet</code> for the final model. </p>

<p>I already know all the disadvantages regarding the automatic model selection but I need to do it anyway.</p>

<p>My problem is that I need to include factor (categorical) variables in the model, is there any way to do it without creating a lot of dummy variables? This variables are almost all strings and not numbers.</p>

<p>Thanks in advance.</p>
"
"0.0746047096952991","0.0824385620013739","136094","<p>I was trying to get an intuition for the interpretation of the coefficients in a logistic regression that was intended to reproduce to some extent that presented in a youtube video (<a href=""http://youtu.be/vq-_4kWmzTo"" rel=""nofollow"">http://youtu.be/vq-_4kWmzTo</a>). So I created a fictitious data set reflecting the chances of getting accepted (Accepted: int: 0 0 ... 1 0) into a college as related to SAT scores (int 1136 1347 1504) and family/ethnic background (categories = ""red"" vs ""blue""). </p>

<pre><code>fit &lt;- glm(Accepted ~ Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded:</p>

<pre><code>                      OR     2.5 %    97.5 %
Backgroundblue 0.7088608 0.5553459 0.9017961
Backgroundred  1.7352941 1.3632702 2.2206569
</code></pre>

<p>The interpretation seemed easy: Red applicants have 1.7 times more chances of getting in over the rest; blue applicants were at a disadvantage, and had  7 over 10 odds of getting in.</p>

<p>However, the more complete model,</p>

<pre><code>fit &lt;- glm(Accepted ~ SAT.scores + Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded coefficients for background that are difficult to reconcile or interpret:</p>

<pre><code>                         OR        2.5 %       97.5 %
SAT.scores     1.008558e+00 1.006940e+00 1.010297e+00
Backgroundblue 8.730056e-06 8.459031e-07 7.634723e-05
Backgroundred  2.329513e-05 2.426748e-06 1.929259e-04
</code></pre>

<p>Can you help point out what I am missing? Thank you.</p>

<p>Thanks to the enlightening answer from Maarten below, I was able to make some progress, and obtain the correct Odds Ratios without and with the SAT confounder:</p>

<p>Here is just regressing to Background (""Red"" versus ""Blue""):</p>

<pre><code>fit &lt;- glm(Accepted ~ Background, data = dat, family = ""binomial"")
exp(cbind(Odds_Ratio_RedvBlue = coef(fit), confint(fit)))

                        Odds_Ratio_RedvBlue             2.5 %       97.5 %
(Intercept)             0.7088608                     0.5553459   0.9017961
Backgroundred           2.4480042                     1.7397640   3.4595454
</code></pre>

<p>Which brought up a couple of additional questions (probably very basic): 1. Why is the Odds Ratio of the (Intercept) - 0.7088608 - the same for this model as for the <em>Odds</em> - as opposed to <em>Odds Ratio</em> - of the Background Blue in the model without the intercept above? And 2. Shouldn't the OddsRatio of Blue and Red be the reciprocal of each other $OddsRatioBlue = 1 / OddsRatioRed$?</p>
"
"0.134303827337563","0.0763232776972177","137424","<p>I am fitting a binomial logistic regression in R using glm. By chance, I have found out that if I change the order of my predictor variables, glm fails to estimate the model. The message I get is  <em>unexpected result from lpSolveAPI for primal test</em>. </p>

<p>I am using the safeBinaryRegression package, so I am confident there are no separation issues between my outcome and predictor variables. However, I am not so confident that there are no quasi-separation issues among my predictor variables. Am I correct that if this is the case, then I might be running into multicolinearity, and this is the source of glm not being able to fit the model? </p>

<p>If so, my question is for advice on how to approach the issue. Should I look for the predictor variables highly correlated and omit one of them? Is there any convenient way of doing so for 11 categorical predictors? </p>

<p>What I see right now: </p>

<pre><code>lModel &lt;- glm(mob_change ~ education + gender + start_age + income + dist_change + lu_change + dou_change + marriage + student2work + wh_change,
              data = regression_data, 
              family = binomial())
# Fine, and I can inspect the model. No predictor has std. error &gt; 1.05

# Now if I move the last variable (or any of the last three, for what I've tested) to
# be the first in predictor... 
lModel.3 &lt;- glm(mob_change ~ wh_change + gender + education + start_age + income + dist_change + lu_change + dou_change + marriage + student2work,
            data = regression_data, 
            family = binomial())

Error in separator(X, Y, purpose = ""find"") : 
  unexpected result from lpSolveAPI for primal test
</code></pre>
"
"0.0930484210398471","0.0440652649239232","137901","<p>I am using the package MatchIt in R to perform propensity score matching. I have chosen to use nearest neighbor matching with a caliper of 0.2 and since in my case i have more cases than controls i have to use the replacement=TRUE option, so that a control can be used more than once.</p>

<p>The graphical histogram check is satisfying and the stand.mean differences are all small with a max of 0.03 (btw any other suggestions for testing the matching?)
I want to use the matched dataset to check the treatment effect after all the matching(perform logistic regression with mortality as outcome and treatment as explanatory variable now) and i am wondering if i should take into consideration the weights that were resulted from the matching. Since i used the replacement option not all observations have a weight of 1 anymore. Shall i use this somehow or can i just perform an unweighted final logistic regression on the matched data to estimate the effect of treatment.</p>
"
"0.0465242105199235","0.0440652649239232","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"0.154303349962092","0.139504861402068","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.0759737176397586","0.0539687072220866","138279","<p>I'm doing Text Classification in R, and my initial features are just word frequency inside a Document. For example:</p>

<pre><code>docID, label, word1, word2, word3, ...wordN
doc123, 1, 10, 2, 5, ..., 12
doc456, 1, 8, 1, 3, ..., 10
doc789, 0, 2, 10, 4, ..., 4
</code></pre>

<p>How should i approach scaling and normalization in this case? For example, if i normalize the frequency across rows, should i drop the 'wordN' feature? (since the sum at each row is 1).</p>

<p>I'm getting better results using this row normalization idea, but the Logistic regression output is complaining about the last column.</p>

<p>Thanks for any insights on this! </p>
"
"NaN","NaN","138332","<p>Iâ€™m trying to do logistic regression,</p>

<p>I utilize the following command:</p>

<pre><code>mylogit &lt;- glm(Var0 ~Var1, data = mydata, family = ""binomial"")
</code></pre>

<p>And I obtain a p-value of <code>0.003</code></p>

<p>After that I want to know the effect of <code>Var2</code> and <code>Var3</code> and I use the following command:</p>

<pre><code>mylogit &lt;- glm(Var0 ~Var1+Var2+Var3, data = mydata, family = ""binomial"")
</code></pre>

<p>obtaining a p-value of <code>0.993</code></p>

<p>My problem is that <code>Var1</code> and <code>Var2</code> are dependent and for that reason I obtain such p value.
Is there any method to indicate that <code>Var1</code> and <code>Var2</code> are dependent or I have to remove <code>Var2</code>?</p>
"
"0.237835356004225","0.120677698006369","138424","<p>My data is binary with two linear independent variables.  For both predictors, as they get bigger, there are more positive responses.  I have plotted the data in a heatplot showing density of positive responses along the two variables.  There are the most positive responses in the top right corner and negative responses in the bottom left, with a gradient change visible along both axes.</p>

<p>I would like to plot a line on the heatplot showing where a logistic regression model predicts that positive and negative responses are equally likely.  (My model is of the form <code>response~predictor1*predictor2+(1|participant)</code>.)</p>

<p>My question: How can I figure out the line based on this model at which the positive response rate is 0.5?</p>

<p>I tried using predict(), but that works the opposite way; I have to give it values for the factor rather than giving the response rate I want.  I also tried using a function that I used before when I had only one predictor (<code>function(x) ((log(x/(1-x)))-fixef(fit)[1])/fixef(fit)[2]</code>), but I can only get single values out of that, not a line, and I can only get values for one predictor at a time.</p>

<p>I am using R.</p>

<p>Edit: I have added a contour plot over the heat plot (using geom_contour in ggplot2), which produces this:</p>

<p><img src=""http://i.stack.imgur.com/qObZc.png"" alt=""Each cell represents the frequency of positive responses for a single stimulus.  I added the numbers for clarity.""></p>

<p>I'd like to have a line that actually predicts the cutoff point in a fine-grained way; right now for the independent variables I have stimuli at points 40, 45, 50, etc. but I would like to see a line that predicts, e.g., that when x=32 and y=36 that's the threshold for 50% positive responses.  It could be a curve or it could even be a straight line (whose slope might help visualise the relative contributions of the two factors), but I'm not looking for a pure description of the cells which are >50 vs &lt;50, which is what I think this is doing, I'm looking for a way to plot the regression's predictions.</p>
"
"0.0930484210398471","0.0440652649239232","138671","<p>I was doing logistic regression in R on 'Smarket' data set available in the ISLR library.
Since correlation between variables were less, I used all variables in my model and I was getting the following result</p>

<p><img src=""http://i.stack.imgur.com/esOCA.png"" alt=""Logistic Regression Output""></p>

<p>Here p -values are greater then 0.05 for all the variables.</p>

<p>Then I checked the misclassification error and the error was almost zero.</p>

<p><img src=""http://i.stack.imgur.com/tF8iq.png"" alt=""Misclassification Output""></p>

<p>I have tried removing some variables. But still the p values are greater than 0.05 and the misclassification error rates are high for those models.</p>

<p>Can I use this model ??
Is p-value insignificant in logistic regression?</p>
"
"0.178541910193085","0.103342206529982","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.0379868588198793","0.0539687072220866","138923","<p>I'm working on a dataset where I have dates as the main unit of analysis. I'm trying to see if two events are related; that is, if the first event happens, will the second event happen within a month of the first event? </p>

<p>I have the dates for both, but I'm kind of at a loss for how to proceed. I thought I should be using logistic regression because the variables (outcomes) are binary: either the second event happened within a month of the first event, or it didn't. Should I? How do I make this month-long envelope after the first event? Am I totally on the wrong track?  </p>
"
"0.1176979772673","0.0696733014291618","139411","<p>In SPSS, when performing binary logistic regression using multiple categorical predictors, a significance level is detailed for the variable overall in addition to each category. This strikes me as useful as the model is built up as the addition of a predictor may negate the effect of previously added variables. What is this overall significance a measure of?</p>

<p>In R, the summary of my GLMs do not include this information. As I am building models I can anova(model0, model1) to test the impact of a new addition. However, how would one then detect if a previous predictor had become insignificant overall? Is the significance of a single category sufficient to warrant inclusion?</p>

<p>I have read the following article which was helpful:</p>

<p><a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a></p>
"
"0.186870636860463","0.112632518139278","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.113960576459638","0.0539687072220866","139653","<p>Original post on stackoverflow:
<a href=""http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved"">http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved</a></p>

<p>I am trying to do a logistic regression analysis in R with two continuous explanatory variables and six other explanatory categorical variables, and find a good regression model to do predictions. When I do step-wise model selections, there are always some levels of certain categorical variables identified as insignificant. I am just wondering how should I deal with this situation. Should I simply drop these levels, or I should force the program to keep all levels of the categorical variables and try to drop the relatively insignificant variables?</p>

<p>Thanks a lot!</p>
"
"NaN","NaN","139756","<p>I'm working on a classification problem with continous and categorical predictors with Random Forests (RF). I'm particularly interested on RF as we avoid the specification of the functional form.</p>

<p>However when it comes to the partial dependance for categorical variables, I'm not sure how to interpret that. For instance, the partial dependence (with the command <code>partialPlot</code> in the <code>R</code> package <code>randomForest</code>) for a binary predictor would give two values, one for each category. My question is: how exactely do you interpret those values? The documentation of <code>partialPlot</code> is quite cryptic in this respect.</p>

<p>My confusion arises, I guess, because I'm used with usual logistic regression where with a dummy coding system you in general obtain the log-odds of the variable of interest against the baseline category. But with RF things are different... Any help is appreciable!</p>
"
"0.174077655955698","0.0824385620013739","139988","<p>For example, if you have a logistic regression on certain dataset:</p>

<pre><code>fit &lt;- glm(y ~ x, data = test, family = ""binomial"")
</code></pre>

<p>If you do <code>predict(fit, newdata, type = ""link"", se = TRUE)</code>, you will get a column named <code>se.fit</code>, which is the standard error for each predicted y value.</p>

<p>My questions are:  </p>

<ol>
<li><p>How is the MSE value for the link function is computed here?  </p>

<p>The variance of the fitting coefficients are basically the MSE times the variance-covariance matrix, there should be a way to compute the MSE value first. But for response variables that have 0 and 1 values, the link function corresponds to 0 and infinity. In this case, how does the model compute this value? Is there any way I can get the MSE value for the <code>glm</code> fitting in R?</p></li>
<li><p>Is <code>se.fit</code> the standard error for the link function value of the fitted line at point <code>x0</code>, or the standard error for the predicted link function value of <code>y</code> at point <code>x0</code>?</p></li>
</ol>
"
"0.0657951694959769","0.0311588476424878","140509","<p>I used logistic regression and found that my model fits well: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6434  -1.4623   0.8704   0.9013   1.0066  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.41595    0.02115   19.67   &lt;2e-16 ***
init_att_cnt  0.02115    0.00146   14.48   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 154956  on 122239  degrees of freedom
Residual deviance: 154746  on 122238  degrees of freedom
AIC: 154750
</code></pre>

<p>The chi-squared test is hightly statisticaly significant: <code>p = 9.642755e-48</code>. I decided to check the Nagelkerke $R^2$ statistic, </p>

<pre><code>R2 &lt;- R2/(1-exp((-mylogit$null.deviance)/n))
</code></pre>

<p>but it was $R^2 = 0.001350927$. This is unbelievable, why is $R^2$ so small, if my model fits well?</p>
"
"0.252496390847059","0.132861772763874","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.0930484210398471","0.0440652649239232","140621","<p>I wanted to know if there is any method in state of art that deals with document classification methods with very few training samples in R.</p>

<p>I have just 20 documents and need to classify them into 3 different classes. </p>

<p>I know it would be great to have a large corpus as part of the training data for every class, but the reality is that I have just 20. So I am assuming that approaches like topic modelling will have very little value add.</p>

<p>I am thinking of a simple approach like taking the term document matrix and running a logistic regression.</p>

<p>Wondering what others would like to suggest.</p>
"
"0.169882397145875","0.112632518139278","140761","<p>I have a question regarding the use of propensity score in a survival analysis with use of mutliple imputation to handle missing data. The question is of theoretical nature and may well apply to other situations.</p>

<p>I have a data set of <em>n</em> individuals. The aim is to estimate the effect of a treatment on a binary outcome (death). The analysis is based on propensity score; the propensity score is derived by means of logistic regression, which includes 30 predictors variables. Effect estimation is carried out by means of Cox regression (which uses the propensity score in various ways [stratification, covariate adjustments etc]). There are a large number of patients, and on average 2â€“7% missing for each variable (of which there are 30 included in the prop. score).</p>

<p>Thus, I have a large data set with a substantial amount of missing data (at least in terms of complete cases) which is why I use multiple imputation - 5 complete data sets are imputed. Now the question is what to do with the muliply imputed data sets; which one of the strategies below should I prefer?</p>

<p><strong>1.</strong> Calculate one average propensity score for each individual using the 5 separate data sets. That way, each individual will have one propensity score, which is the average from the n complete data sets. Then do the Cox regression..</p>

<p><strong>2.</strong> Analyze each separate multiply imputed data set (with Cox regression), and then pool the 5 hazard ratio estimates to one hazard ratio.</p>

<p>The second method appears to be used more often, but is it better/worse?</p>

<p>Any thoughts about this?</p>
"
"0.124341182825498","0.0588846871438385","140929","<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
"0.17654696590095","0.132379272715407","140972","<p>Iâ€™m using a maximal logistic regression model to analyze some data. I would like to keep using this technique if possible, just include more data in the model. The main data Iâ€™m looking at is counts of a particular behavior over items in a sequence, and I would like my analysis to also include data from a post-experiment questionnaire (8 items, 1-9 Likert scored). Hereâ€™s some info about my data:</p>

<pre><code>'data.frame':
Pair          : Factor w/ 36 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
SpeakerID     : Factor w/ 72 levels ""10A"",""10B"",""11A"",..: 21 22 21 22 22 21 22 21 21 22 ...
Speaker       : Factor w/ 2 levels ""A"",""B"": 1 2 1 2 2 1 2 1 1 2 ...
Condition1     : Factor w/ 4 levels ""ANTI"",""CONTROL"",..: 1 1 1 1 1 1 1 1 1 1 ...
..- attr(*, ""contrasts"")= num [1:4, 1:3] -0.333 1 -0.333 -0.333 0.25 ...
.. ..- attr(*, ""dimnames"")=List of 2
.. .. .. : chr  ""ANTI"" ""CONTROL"" ""IN"" ""OUT""
.. .. .. : NULL
Condition2         : Factor w/ 3 levels ""0"",""90"",""180"": 2 3 1 1 2 1 1 2 2 3 ...
..- attr(*, ""contrasts"")= num [1:3, 1:2] 0 -0.5 0.5 -0.5 0.25 0.25
.. ..- attr(*, ""dimnames"")=List of 2
.. .. ..$ : chr  ""0"" ""90"" ""180""
    .. .. ..$ : NULL
Item         : Factor w/ 16 levels ""MAP1"",""MAP10"",..: 1 9 10 11 12 13 14 15 16 2 ...
Foo       : num  0.847 1.099 1.946 -1.099 -0.452 ...
wtsFoo          : num  0.952 0.889 2.286 0.889 0.468 ...
Close      : num  -1.798 0.202 -1.798 0.202 0.202 ...
Similar    : num  0.505 0.505 0.505 0.505 0.505 ...
Like       : num  -0.833 0.167 -0.833 0.167 0.167 ...
Task1Hard   : num  -0.89 4.11 -0.89 4.11 4.11 ...
Task2Hard: num  -1.02 2.98 -1.02 2.98 2.98 ...
</code></pre>

<p>My analysis is based on this guide to empirical logit analyses:
<a href=""http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html</a>
So far, so good. In my regression model, Iâ€™m testing the fixed effects of Condition1 (4 levels) and Condition2 (3 levels) on Foo (the behavior, expressed as a proportion converted into empirical logit form, see link for how and why). Pair, Pair:Subject (Subject nested within Pair) and Item are included as random effects. Condition1 is between-subjects/pairs and Condition2 is within-subjects. Hereâ€™s the model Iâ€™m using in R:</p>

<pre><code>model &lt;- lmer(Foo ~ Condition1*Condition2 + (1+Condition1 | Pair) 
+ (1+Condition1 | Pair:Subject) + (1+Condition2 | Item), weights=1/wtsFoo, data)
</code></pre>

<p>This all works fine, but hereâ€™s where it gets fun. Where should the questionnaire data go? </p>

<p>Bad idea #1: Each participant has one score for each questionnaire item, so each questionnaire item type should be included as a fixed effect, so that Foo can be predicted by any of the variables discovered in the post-experiment questionnaire (things like social closeness and task difficulty). This is a terrible idea because the questionnaire items are NOT independent variables from Condition1 and Condition2, and if I include them as fixed effects it will introduce a mess of multicollinearity and will just be flat-out wrong.</p>

<p>Bad idea #2: Analyzing the questionnaire data separately. Not such a bad idea, just one that my committee doesnâ€™t like. </p>

<p>Less bad ideas: please suggest a model that allows me to observe the effects of Condition1 and Condition2 on questionnaire items (Close, Similar, Like, Task1Hard, Task2Hard) AND allows me to observe the effects of questionnaire items on Foo. Failing that, explain to me why the only good thing to do is analyze the questionnaire separately from the observation data.</p>

<p>I've read around on Stackexchange and I haven't seen this particular problem covered, although some answers come close to looking useful, I don't yet have the R or stats chops to make them work for me. If I've missed something obvious, please clue me.</p>
"
"0.166450075715296","0.0985329278164293","141379","<p>I'm working with a dataset that has multiple categorical classes that cannot be ranked into any relative or priority order.  To approach this, I'm looking at glmnet's multinomial option. </p>

<p>As I understand it (and please correct me if I'm wrong), for this multinomial model, one class is chosen as the ""pivot"" class, and all other classes are modeled relative to that class.  For example, if I have 4 classes, one class is chosen as the pivot, 3 different logistic regression models are made, one for each of the other 3 classes relative to the pivot.  Probabilities are then scaled relatively in order ensure class probabilities sum to one.</p>

<p>The main question that I have is, which class is chosen as the pivot class?</p>

<p>The next question is why this ""pivot"" process is done rather than a 1-versus-all approach? i.e., 4 models, class 1 versus all others, class 2 versus all others, etc.</p>

<p>Further, in reading this page (<a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression"" rel=""nofollow"">http://en.wikipedia.org/wiki/Multinomial_logistic_regression</a>) under the section ""As a set of independent binary regressions"" it discusses the scaling of probabilities to ensure the sum of all probabilities is 1.  But when I use their formulas on the data below, it seems to give an odd result.</p>

<pre><code>Model Prob      Scaled prob
0.72            0.327272727
0.33            0.15
0.15            0.068181818
0.61            0.454545455
</code></pre>

<p>In this case, I chose the last class as the pivot and used it for the scaling formulas.  Notice that the scaled probability for this class is higher than for the first class, despite having a higher Model Probability.</p>

<p>If I use the simpler formula of Scaled Prob = (Model Prob / sum(all Model Probs)), the answer is more intuitive, but perhaps wrong.  Any guidance/explanation there is appreciated.</p>

<p>Finally, any suggestions of other multiclass classification tools is appreciated.</p>
"
"0.139572631559771","0.0881305298478463","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"0.223407415813095","0.120914089414122","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.0759737176397586","0.0539687072220866","143297","<p>Laplacian logistic regression. I have a training set of data and an evaluation set. The response is binary. I have to verify the models by calculating posterior predictive on the evaluation set. Last step compare the two models' predictive distribution variance.</p>

<p>First I trained the model using MCMCprobit() function from R.
How do I verify the correctness on the evaluation set? How do I calculate posteriors for each observation from the evaluation set?</p>
"
"NaN","NaN","143328","<p>I am developing a logistic regression model where perfect variable separation occurs. I want to calculate a cutoff from this data. Interestingly, the length of the slot <code>cutoffs</code> of <code>pred.obj</code> is only 5, as well as the slots <code>fp</code>, <code>tp</code>, <code>tn</code>, <code>fn</code>, <code>n.pos.pred</code> and <code>n.neg.pred</code>. I expect it to have the same length as the observations. </p>

<p>Has anybody an explanation for this? (And knows how to solve it?) </p>

<p>MWE:</p>

<pre><code> library(ROCR) # package for prediction/performance functions
 y &lt;- c(0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0)
 x &lt;- c(-5, 5, 3, -2, 4, 3, -8, 2, 5, 3, -5, -3, -2)
 model &lt;- glm(as.factor(y) ~ x, family = ""binomial"")
 preds &lt;- predict(model, type = ""response"")
 (pred.obj &lt;- prediction(preds, y))
 perf &lt;- performance(pred.obj, ""acc"")
 (cutoff &lt;- perf@x.values[[1]][which.max(perf@y.values[[1]])])
</code></pre>
"
"0.116310526299809","0.0881305298478463","143615","<p>I estimate a multilevel ordinal logistic regression models in Stata and R, and receive different estimators for the variance and the covariance of the latent variable of the higher level.
Among other data, I use this Stata datafile (from Stata help for meologit):
<a href=""http://www.stata-press.com/data/r13/tvsfpors.dta"" rel=""nofollow"">http://www.stata-press.com/data/r13/tvsfpors.dta</a>
I estimate (among others) this model:</p>

<pre><code># R formula syntax
library(ordinal)
library(readstata13)
mydata &lt;- read.dta13(file = ""http://www.stata-press.com/data/r13/tvsfpors.dta"",  
                     convert.factors = FALSE)
res &lt;- clmm(as.factor(thk) ~ prethk + cc + tv + (1 | school), mydata, nAGQ  =7)
summary(res)

* Stata equation
webuse tvsfpors
meologit thk prethk cc tv || school:
</code></pre>

<p>The columns in the variance-covariance-matrix for the variance of the latent higher-level variable seem to differ systematically. The covariances differ by an identical constant, and the variances (of the variance) themselves by the squared term of the constant. The constant differ dependent on the data and model structure.</p>

<p>Does anyone know the source of this difference?</p>

<p>This is the variance-covariance-matrix for the estimation described above for R:
Here is the var-cov-matrix for R</p>

<pre><code>&gt; vcov(m1)
                1|2          2|3          3|4        prethk            cc            tv           ST1
1|2     0.023567239  0.022089030  0.022112907  0.0030638974  0.0105995218  0.0111725311 -0.0043760134
2|3     0.022089030  0.023866162  0.023611332  0.0033154896  0.0111666464  0.0111977570 -0.0031374135
3|4     0.022112907  0.023611332  0.026259339  0.0036017005  0.0116806803  0.0112381854 -0.0018698361
prethk  0.003063897  0.003315490  0.003601701  0.0015113830  0.0003659838  0.0001657595 -0.0009479396
cc      0.010599522  0.011166646  0.011680680  0.0003659838  0.0216625428 -0.0002137013  0.0004195043
tv      0.011172531  0.011197757  0.011238185  0.0001657595 -0.0002137013  0.0214436765 -0.0012672853
ST1    -0.004376013 -0.003137414 -0.001869836 -0.0009479396  0.0004195043 -0.0012672853  0.0642318747
</code></pre>

<p>This is the analogue in Stata</p>

<pre><code>. matrix list e(V), nohalf

symmetric e(V)[7,7]
                                   thk:          thk:          thk:         cut1:         cut2:         cut3:  var(_cons~):
                                prethk            cc            tv         _cons         _cons         _cons         _cons
              thk:prethk     .00151138     .00036598     .00016576      .0030639     .00331549      .0036017    -.00015693
                  thk:cc     .00036598     .02166253    -.00021371     .01059951     .01116663     .01168067     .00006945
                  thk:tv     .00016576    -.00021371     .02144367     .01117254     .01119776     .01123818    -.00020983
              cut1:_cons      .0030639     .01059951     .01117254     .02356724     .02208902      .0221129    -.00072444
              cut2:_cons     .00331549     .01116663     .01119776     .02208902     .02386615     .02361132     -.0005194
              cut3:_cons      .0036017     .01168067     .01123818      .0221129     .02361132     .02625932    -.00030956
var(_cons[school]):_cons    -.00015693     .00006945    -.00020983    -.00072444     -.0005194    -.00030956     .00176027
</code></pre>

<p>Note that the column and row order is different. My question points at the diffference between the rows/columns of the variable var(_cons[school]) in the Stata matrix and ST1 in the R matrix.</p>
"
"0.131590338991954","0.0623176952849756","143943","<p>I have a need to do realtime predictions for individual rows of data based on a previously computed randomForest algorithm.  How can I run the ""predict"" command without recomputing ""fit"" on the entire training data set each time?  </p>

<p>I am using R and here's the line of code that computes ""fit"" by applying the randomForest algorithm on the training set.</p>

<pre><code>fit &lt;- randomForest(formula2, data=training, importance=TRUE, ntree=2000, na.action = na.omit)
</code></pre>

<p>And here's the predict command - I want to be able to run this without having to recompute fit every time.  Is this possible?</p>

<pre><code>outp_rf &lt;- predict(fit, testing)
</code></pre>

<p>For LogisticRegression, I know the coefficients so I can rerun the logistic function to compute the outcome.  However not sure how I can do it for RandomForest.</p>
"
"0","0.0311588476424878","144152","<p>While I am reasonably comfortable with performing and interpreting the output from logistic regression using glm in R, I had a question about the mechanics of the calculation to better understand what is going on.</p>

<p>I am trying to fit a logistic model,</p>

<p>$\log \large( \frac{p}{1-p} \large) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$</p>

<p>where the input data consists of single rows for each observation with success/failure coded as 0/1 and the dependent numerical values $x_1,x_2$.</p>

<p>How are the individual 0/1 values converted into the LHS of equation above and used for the fitting of the coefficients?</p>
"
"NaN","NaN","144520","<p>I would like to conduct nominal logistic regression analysis using <code>molt</code>in R, but don't know how to enter the data below into R. Any advice?</p>

<p><img src=""http://i.stack.imgur.com/sAnc4.png"" alt=""enter image description here""></p>
"
"0.0465242105199235","0.0220326324619616","144603","<p>I have built a logistic regression where the outcome variable is being cured after receiving treatment (<code>Cure</code> vs. <code>No Cure</code>). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. </p>

<p>In R my logistic regression output looks as follows: </p>

<pre><code>Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.2735     0.1306   9.749   &lt;2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75
</code></pre>

<p>However, the confidence interval for the odds ratio <strong>includes 1</strong>:</p>

<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167
</code></pre>

<p>When I do a chi-squared test on these data I get the following:</p>

<pre><code>data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365
</code></pre>

<p>If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:</p>

<pre><code>Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)
</code></pre>

<p>My question is: Why don't the p-values and the confidence interval including 1 agree? </p>
"
"NaN","NaN","145226","<p>Upon performing <em>binary logistic regression</em>, I have found <code>VIF</code>, using <code>R</code> programming, as follows:</p>

<pre><code>             GVIF Df  GVIF^(1/(2*Df))
agem     2.213242  3        1.141576
eduM     2.842857  3        1.190216
eduF     2.576725  3        1.170877
ageC     1.315301  1        1.146866
diarrhea 1.031031  1        1.015397
uweight  1.129919  1        1.062977
fever    1.033433  1        1.016579
res      1.341470  1        1.158218
dis      1.440215  6        1.030866
WI       2.610752  4        1.127446
nlc      2.407934  3        1.157730
</code></pre>

<p>Based on those results, should I remove <code>agem</code>, <code>eduM</code>, <code>eduF</code>, <code>WI</code> and <code>nlc</code> for multi-collinearity? Or do I need to apply another approach? Could anybody help me?</p>
"
"0.0882734829504749","0.0418039808574971","145315","<p>I have age as a covariate in my material. A continuous variable. The age varies between 18-70 years.</p>

<p>I'm into a logistic regression and do not really know how to treat the variable. As a linear effect or as a polynomial?</p>

<pre><code>   gender       passinggrade age    prog
1    man          FALSE      69     FRIST
2    man             NA      70     FRIST
3 woman             NA       65     FRIST
4 woman           TRUE       68      FRIST
5 woman             NA       65     NMFIK
6    man          FALSE      70     FRIST
</code></pre>

<p>my model;</p>

<pre><code>mod.fit&lt;-glm(passinggrade ~prog+gender+age,family=binomial,data=both)
</code></pre>

<p>summary(mod.fit)</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.42653    0.28096   8.636  &lt; 2e-16 ***
progLARAA    0.44931    0.25643   1.752 0.079746 .  
progNASTK   -0.15524    0.26472  -0.586 0.557597    
progNBFFK    0.12091    0.65460   0.185 0.853462    
progNBIBK   -0.18850    0.37656  -0.501 0.616659    
progNDATK   -2.84617    0.73077  -3.895 9.83e-05 ***
progNFYSK    0.64391    0.19634   3.280 0.001040 ** 
progNMATK    0.18424    0.16451   1.120 0.262733    
progNMETK    0.22433    0.29086   0.771 0.440554    
progNMFIK    0.38877    0.42152   0.922 0.356373    
progNSFYY    0.97205    0.29320   3.315 0.000915 ***
progSMEKK   -0.58043    0.18185  -3.192 0.001414 ** 
genderman   -0.05623    0.10477  -0.537 0.591496        
age         -0.11780    0.01028 -11.462  &lt; 2e-16 ***
</code></pre>

<p>how would you treat the variable age?
and how should I interpret the results for age?</p>
"
"NaN","NaN","145402","<p>I have been trying logistic regression to fit the data and get an estimation of the response rate, but the power of the model is quite limited. The area under the ROC curve is always around 0.6. </p>

<p>I am just wondering whether there are other models to predict binary response variables that I can try? Thanks!</p>
"
"0.0624187783932359","0.0788263422531435","145978","<p>I am not an expert in regression, but I have a problem that I believe should be solved by logistic regression. The problem is rather specific, so I try to describe it using a more tangible example.</p>

<p>Say there are five airline carriers, A,..., E, that offer flights to several locations. Let's also assume the following quality of service for the carriers: Q(A) > Q(B) > ... > Q(E), that is, carrier A offers the ""best"" service during the flight. However, this comes at the cost of a higher price, i.e., P(A) > P(B) > ... > P(E). Therefore, there is a clear trade-off between quality of service and price.</p>

<p>Also, to get the general preference of a user, we ask her/him several questions in the form of the following example: ""Which carrier would you select to get a flight to New York given the following prices: p(A) = \$1000, P(B) = \$900,..., p(E) = \$650?"". Before asking the questions, we let the user know about the quality of service provided by each carrier, so that she/he considers it in their decision making.</p>

<p>Once we have enough sample points (say 30), we use them to build a model that can predict user's choice for scenarios that are not in the sample points. To put it concretely, I am looking for a model that can take as input the list of prices, and provide as output the user's preferred carrier.</p>

<p>Following are few points that I have figured about the model (which may not be correct):</p>

<ul>
<li>it seems to be a multinomial logistic regression model, where the outcome is one of the five carriers.</li>
<li>explanatory variables need not include characteristics of the user, instead they should include attributes of the alternatives (i.e., the price of each carrier).</li>
<li>the decision is made by comparing all the alternatives in a scenario, that is, an undesirable alternative in one scenario might be considered desirable in another scenario.</li>
</ul>

<p>I would really appreciate any suggestions.</p>
"
"0.0930484210398471","0.0440652649239232","147693","<p>I have a dataset of 1931 observations and I intend to predict a binary outcome out of that. There is a list of 128 predictors (both binary and continuous). First I ran logistic regression modeling using all predictors and got a highly significant model (AUC = 0.84). Assuming that the high value of AUC was due to overfitting the model by using too many predictors, I did stepwise modeling to find the effective predictors: </p>

<pre><code>mylogit &lt;- glm(outcome ~ . , data = temp,family=""binomial"")
step &lt;- step(mylogit, direction=""both"")
</code></pre>

<p>Now, I am not sure whether should I have done cross validation before or after stepwise modeling. </p>
"
"NaN","NaN","147793","<p>I would like to know the reason why we ignore those variables in logistic regression whose information value is more than 0.5 though it might carry high information about the prediction.</p>
"
"NaN","NaN","147904","<p>This is covariate age in my logistic regression. How should I treat it? Gets a little insecure. Have tried to read, but still insecure. 
Right now I treat it as if it were linear. A polynomial is not appropriate.
Some tips? I have a binary response variable</p>

<p><img src=""http://i.stack.imgur.com/dmg3i.png"" alt=""enter image description here""></p>

<p>x axis is age, the y axis is the percentage of approved students first semester</p>
"
"0.0994729462603988","0.0824385620013739","147923","<p>I have a data set with continuous variable and a binary target variable (0 and 1). </p>

<p>I need to discretize the continuous variables (for logistic regression) with respect to the target variable and with the constrained that the frequency of observation in each interval should be balanced. I tried machine learning algorithms like Chi Merge, decision trees. Chi merge gave me intervals with very unbalanced numbers in each interval (an interval with 3 observations and another one with 1000). The decision trees were hard to interpret.</p>

<p>I came to the conclusion that an optimal discretization should maximise the $\chi^2$ statistic between the discretized variable and the target variable and should have intervals containing roughly the same amount of observations. </p>

<p>Is there an algorithm for solving this?</p>

<p>This how it could look like in R (def is the target variable and x the variable to be discretized). I calculated Tschuprow's $T$ to evaluate the ""correlation"" between the transformed and the target variable because $\chi^2$ statistics tends to increase with the number of intervals. I'm not certain if this is the right way.</p>

<p>Is there another way of evaluating if my discretization is optimal other than Tschuprow's $T$ (increases when number of classes decreases)? </p>

<pre><code>chitest &lt;- function(x){
  interv &lt;- cut(x, c(0, 1.6,1.9, 2.3, 2.9, max(x)), include.lowest = TRUE)
  X2 &lt;- chisq.test(df.train$def,as.numeric(interv))$statistic
  #Tschuprow
  Tschup &lt;- sqrt((X2)/(nrow(df.train)*sqrt((6-1)*(2-1))))
  print(list(Chi2=X2,freq=table(interv),def=sum.def,Tschuprow=Tschup))
}
</code></pre>
"
"0.0657951694959769","0.0311588476424878","148859","<p>I have a variable I do not know how I should handle my logistic regression.
The variable is the number of registered students each semester.
If I plot it against my binary outcome, I get the following plot:
<img src=""http://i.stack.imgur.com/cdeEd.png"" alt=""enter image description here""></p>

<p><strong>what kind of explanatory variables should I use? Linear, polynomial, categorical? I feel myself confused when it looks like this and would therefore need some tips.</strong></p>
"
"0.145643816250884","0.0985329278164293","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.0759737176397586","0.0539687072220866","149140","<p>Currently I am working on a large data set with well over 200 variables (238 to be exact) and 290 observations for each variable (in theory). This data set is missing quite a lot of values, with variables ranging from 0-100% 'missingness'. I will eventually be performing logistical regression on this data, so of my 238 columns I will at most only be using ten or so.</p>

<p>However as almost all of my columns are missing some data, I am turning to multiple imputation to fill in the blanks (using the MICE package).</p>

<p>My question is; given that I have a large amount of variation in the missing data, at what percentage missing should I start to exclude variables from the mice() function? </p>

<p>Can mice function well with variables that are missing 50% of their values? What about 60%, 70%, 80%, 90%?</p>
"
"0.175453785322605","0.0934765429274634","151568","<p>I'm working with a large 3-way contingency table (roughly $180 \times 40 \times 2$) &mdash; both independent variables are categorical and the response is binary.  One independent variable (X) represents a classification of experimental test objects into types, and the other (Y) a set of test conditions.  The experiment is only balanced for Y - that is, each test object was tested in all Y conditions, but each test object is only one X type, and the number of objects assigned to each type varies by three orders of magnitude (40Â­&ndash;69600).  Response to the test is either ""normal"" or ""anomalous"", and, critically, I know <em>a priori</em> that only interactions between X and Y are interesting.  I know it's necessary to model the main effects of X and Y as well, but (in the context of the larger experiment) they are noise factors.  So the thing that I care about is identifying all pairs of levels of X and Y such that the interaction is significant (given some threshold).</p>

<p>In R-ese, the natural model is</p>

<pre><code>glm(cbind(anom, total-anom) ~ X * Y, family=""binomial"", data=apt.d)
</code></pre>

<p>where <code>apt.d</code> (a $10\times10\times2$ downsample) is at the end of this question.  Now, I have two problems:</p>

<ol>
<li><p><code>summary(fit object)</code> tells me that this model leaves me with zero residual degrees of freedom and a residual deviance of $4 \times 10^{-10}$, which makes me worry about overfitting, but I don't see how I could structure the model any other way.</p></li>
<li><p>Actually running the fit on the full data frame is painfully slow -- nearly an hour just to generate the fit object.  And then it says that the fit didn't converge and ""fitted probabilities numerically 0 or 1 occurred"".  Is there an alternative implementation of logistic regression in R that would be more appropriate for this size of data set and/or this sort of unbalanced data with widely varying levels of anomalous response?</p></li>
</ol>

<p>Downsampled data:</p>

<pre><code>apt.d &lt;- structure(list(X = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L), .Label = c(""A"", 
""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""I"", ""J""), class = ""factor""), 
    Y = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
    1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
    9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 
    3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 
    7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
    1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 8L, 9L, 10L), .Label = c(""A"", ""B"", ""C"", ""D"", 
    ""E"", ""F"", ""G"", ""H"", ""I"", ""J""), class = ""factor""), total = c(765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L), anom = c(148L, 
    235L, 150L, 3L, 0L, 12L, 113L, 148L, 74L, 1L, 121L, 290L, 
    116L, 1L, 3L, 15L, 276L, 220L, 44L, 2L, 47L, 66L, 23L, 1L, 
    0L, 3L, 50L, 28L, 20L, 4L, 51L, 42L, 64L, 13L, 0L, 3L, 33L, 
    67L, 42L, 0L, 91L, 148L, 128L, 0L, 0L, 10L, 69L, 106L, 53L, 
    4L, 242L, 546L, 307L, 13L, 14L, 25L, 320L, 431L, 106L, 22L, 
    45L, 37L, 61L, 11L, 0L, 3L, 20L, 28L, 28L, 0L, 143L, 251L, 
    150L, 16L, 12L, 26L, 118L, 193L, 61L, 3L, 34L, 58L, 29L, 
    7L, 0L, 3L, 40L, 36L, 33L, 0L, 43L, 32L, 73L, 1L, 1L, 3L, 
    24L, 30L, 24L, 0L), p.anom = c(0.193464052287582, 0.15359477124183, 
    0.167785234899329, 0.0612244897959184, 0, 0.123711340206186, 
    0.145057766367137, 0.133213321332133, 0.166292134831461, 
    0.0222222222222222, 0.158169934640523, 0.189542483660131, 
    0.129753914988814, 0.0204081632653061, 0.038961038961039, 
    0.154639175257732, 0.354300385109114, 0.198019801980198, 
    0.098876404494382, 0.0444444444444444, 0.061437908496732, 
    0.0431372549019608, 0.0257270693512304, 0.0204081632653061, 
    0, 0.0309278350515464, 0.0641848523748395, 0.0252025202520252, 
    0.0449438202247191, 0.0888888888888889, 0.0666666666666667, 
    0.0274509803921569, 0.0715883668903803, 0.26530612244898, 
    0, 0.0309278350515464, 0.0423620025673941, 0.0603060306030603, 
    0.0943820224719101, 0, 0.118954248366013, 0.0967320261437909, 
    0.143176733780761, 0, 0, 0.103092783505155, 0.0885750962772786, 
    0.0954095409540954, 0.119101123595506, 0.0888888888888889, 
    0.316339869281046, 0.356862745098039, 0.343400447427293, 
    0.26530612244898, 0.181818181818182, 0.257731958762887, 0.410783055198973, 
    0.387938793879388, 0.238202247191011, 0.488888888888889, 
    0.0588235294117647, 0.0241830065359477, 0.0682326621923937, 
    0.224489795918367, 0, 0.0309278350515464, 0.0256739409499358, 
    0.0252025202520252, 0.0629213483146067, 0, 0.186928104575163, 
    0.164052287581699, 0.167785234899329, 0.326530612244898, 
    0.155844155844156, 0.268041237113402, 0.151476251604621, 
    0.173717371737174, 0.137078651685393, 0.0666666666666667, 
    0.0444444444444444, 0.0379084967320261, 0.0324384787472036, 
    0.142857142857143, 0, 0.0309278350515464, 0.0513478818998716, 
    0.0324032403240324, 0.0741573033707865, 0, 0.0562091503267974, 
    0.0209150326797386, 0.08165548098434, 0.0204081632653061, 
    0.012987012987013, 0.0309278350515464, 0.030808729139923, 
    0.027002700270027, 0.0539325842696629, 0)), .Names = c(""X"", 
""Y"", ""total"", ""anom"", ""p.anom""), row.names = c(NA, -100L), class = ""data.frame"")
</code></pre>
"
"0.0657951694959769","0.0311588476424878","151600","<p>Has anyone written a package for R that can do a logistic regression over categorical variables (like <code>glm</code>) but with the constraint, and I do realize this is weird, that <em>all the residuals must be nonnegative?</em>  (In response space, not link space.  In other words, the predicted probability in each cell must come out less than or equal to the observed probability in that cell.) Alternatively, is there a straightforward way to transform a <code>glm</code> problem so that it will come out with nonnegative residuals?</p>

<p>I know I can probably persuade <code>optim</code> to do what I want but if a shortcut exists that sure would be nice.</p>
"
"0.134303827337563","0.0763232776972177","151915","<p>I've performed a logistic regression with L-BFGS on R and noticed that if I changed the initialization, the model retuned was different.</p>

<p>Here is my dataset (390 obs. of 14 variables, Y is the target variable) :</p>

<pre><code>GEST    DILATE    EFFACE    CONSIS    CONTR    MEMBRAN    AGE    STRAT    GRAVID    PARIT    DIAB    TRANSF    GEMEL    Y
31           3       100         3        1         2     26         3         1        0       2         2       1     1
28           8         0         3        1         2     25         3         1        0       2         1       2     1
31           3       100         3        2         2     28         3         2        0       2         1       1     1
...
</code></pre>

<p>This dataset is found here: <a href=""http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html"" rel=""nofollow"">http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html</a> in ""DonnÃ©es : prematures.xls"". Y is a column I created with the column ""PREMATURE"", Y=IF(PREMATURE=""positif"";1;0)</p>

<p>I've used the optimx package like here <a href=""http://stats.stackexchange.com/questions/17436/logistic-regression-with-lbfgs-solver"">Logistic regression with LBFGS solver</a>, here is the code: </p>

<pre><code>install.packages(""optimx"")
  library(optimx)

vY = as.matrix(premature['PREMATURE'])
# Recoding the response variable
vY = ifelse(vY == ""positif"", 1, 0)

mX = as.matrix(premature[c('GEST', 'DILATE', 'EFFACE', 'CONSIS', 'CONTR', 
                           'MEMBRAN', 'AGE', 'STRAT', 'GRAVID', 'PARIT', 
                           'DIAB', 'TRANSF', 'GEMEL')])

#add an intercept to the predictor variables
mX = cbind(rep(1, nrow(mX)), mX)

#the number of variables and observations
iK = ncol(mX)
iN = nrow(mX)

#define the logistic transformation
logit = function(mX, vBeta) {
  return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )
}

# stable parametrisation of the log-likelihood function
logLikelihoodLogitStable = function(vBeta, mX, vY) {
  return(-sum(
    vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))
    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))
  )  # sum
  )  # return 
}

# score function
likelihoodScore = function(vBeta, mX, vY) {
  return(t(mX) %*% (logit(mX, vBeta) - vY) )
}

# initial set of parameters (arbitrary starting parameters)
vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)

optimLogitLBFGS = optimx(vBeta0, logLikelihoodLogitStable,
                         method = 'L-BFGS-B', gr = likelihoodScore, 
                         mX = mX, vY = vY, hessian=TRUE)
</code></pre>

<p>I get this :</p>

<pre><code> optimLogitLBFGS
                p1         p2       p3         p4         p5         p6
L-BFGS-B 9.720242 -0.1652943 0.525449 0.01681583 0.02781123 -0.3921004
                 p7          p8         p9       p10        p11        p12
L-BFGS-B -1.694412 -0.03461208 0.02759248 0.1993573 -0.6718275 0.02537887
                 p13      p14   value fevals gevals niter convcode  kkt1  kkt2
L-BFGS-B -0.8374338 0.625044 187.581    121    121    NA        1 FALSE FALSE
          xtimes
L-BFGS-B  0.044
</code></pre>

<p>But if I change </p>

<pre><code>vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)
</code></pre>

<p>in</p>

<pre><code>vBeta0 = rep(0.1, iK)
</code></pre>

<p>I get a different result :</p>

<pre><code>optimLogitLBFGS
                 p1             p2             p3              p4               p5
L-BFGS-B 0.372672689046 0.206785276091 0.398104550108 0.0175008380158 -0.0460042719084
                 p6             p7               p8            p9            p10
L-BFGS-B 0.139760396213 -1.43192069477 -0.0207666651106 -1.1396642657 0.212186387416
                 p11             p12             p13            p14         value
L-BFGS-B -0.583698421298 0.0576485672766 -0.802789658686 0.993103617257 185.472518798
         fevals gevals niter convcode  kkt1  kkt2 xtimes
L-BFGS-B    121    121    NA        1 FALSE FALSE   0.05
</code></pre>

<p>How can I choose the initial parameters to get the best model?</p>
"
"0.145986373720362","0.11234482285936","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.123091490979333","0.108258180127387","152033","<p>I'm having some difficulty interpreting how to correctly create a matrix input for regression from a long form data source. I have table containing marketing data where each row represents a view of an advert, with person ID, time of day and channel also included, and purchase decision for that person (success). </p>

<p>This R code creates a rough sample of the long form data:</p>

<p><code>aa &lt;- data.frame(ID=rep(letters[1:4]), success=c(1,0,0,1,1,0,0,1,1,0,0,1), 
+            viewTime=rep(c(""night"",""day"")), channel=rep(c(""tv"",""web"",""email""), 
+                                                        c(5,3,4)))
aa
   ID success viewTime channel
1   a       1    night      tv
2   b       0      day      tv
3   c       0    night      tv
4   d       1      day      tv
5   a       1    night      tv
6   b       0      day     web
7   c       0    night     web
8   d       1      day     web
9   a       1    night   email
10  b       0      day   email
11  c       0    night   email
12  d       1      day   email</code></p>

<p>To model this I'm interested in summarizing the data at the person ID level and then fitting a logistic regression based on the purchase decision (success). I'm curious whether time of day, channel, and the interactions between time and channel influence the decision.</p>

<p>The problem I'm having is how to summarize this. I can summarize across both variables of interest like this:</p>

<p><code>library(reshape2)</code></p>

<p><code>&gt; bb &lt;- dcast(aa, ID + success ~ channel + viewTime)</code></p>

<p><code>&gt; bb
  ID success email_day email_night tv_day tv_night web_day web_night
1  a       1         0           1      0        2       0         0
2  b       0         1           0      1        0       1         0
3  c       0         0           1      0        1       0         1
4  d       1         1           0      1        0       1         0</code></p>

<p>Which returns the count of each combination of channel/time across each person, but I worry that this would model the interaction without modeling the main effect, which I know to be incorrect. Another option would be to summarize the count of ads by each variable of interest independently, like this:</p>

<p><code>library(dplyr)</code></p>

<pre><code>cc &lt;- left_join(dcast(aa, ID + success ~ viewTime), dcast(aa, ID + success ~ channel))
</code></pre>

<p><code>&gt; cc
  ID success day night email tv web
1  a       1   0     3     1  2   0
2  b       0   3     0     1  1   1
3  c       0   0     3     1  1   1
4  d       1   3     0     1  1   1</code></p>

<p>But now it seems rather odd to assign separate count data to each view, e.g. the count of views per person is double-counted. A third option is to join the two previous tables together.</p>

<pre><code>dd &lt;- left_join(cc,bb)
</code></pre>

<p><code>&gt; dd
  ID success day night email tv web email_day email_night tv_day tv_night web_day web_night
1  a       1   0     3     1  2   0         0           1      0        2       0         0
2  b       0   3     0     1  1   1         1           0      1        0       1         0
3  c       0   0     3     1  1   1         0           1      0        1       0         1
4  d       1   3     0     1  1   1         1           0      1        0       1         0</code></p>

<p>Which returns individual counts for <code>Time</code> and <code>Channel</code>, as well as the count across each possible interaction. My question is which of these three approaches would be most correct and what is the reasoning behind that. </p>
"
"0","0.0311588476424878","152091","<p>I am fairly new with logistic regression. I have a binary response. And did this plot. The binary response is:</p>

<p>Y = 0: The student fails</p>

<p>Y = 1: The student succeed</p>

<pre><code>library(ggplot2)
ggplot(data = both, aes(x = age, y = succeed)) + 
  stat_smooth(method = 'glm', family = 'binomial') +
  theme_bw()+xlab(""X"")+ylab(""The student succeed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xM9wS.png"" alt=""enter image description here""></p>

<p><strong>What is the y-axis? estimated probability that the student succeed? Or estimated log odds of a student succeed?
Feeling a little confused. Could someone explain what they are I have on the y-axis?</strong></p>
"
"0.0657951694959769","0.0311588476424878","152251","<p>I ask this because all resources regarding logistic regression in R involve binary outcomes, so they try to model questions like when will increase in temp cause a switch to fail (0, 1)â€”involving probabilities.</p>

<p>But if we are talking about population growth in any ecology, is it just scaled? I'm completely unaware of logistics in general, but the Richard's Curve seems to model anything with a leveling out population.</p>

<p>What is population growth modeling called in statistics, specifically in R? </p>
"
"0.0930484210398471","0.0440652649239232","152268","<p>Can someone explain to me how you would know which variables to remove? And how do you know if something is accurate or not? Because when I plot an ROC curve the specificity/sensitivity curve, it looks like a square for a random forest.</p>

<p>If I use 2 different models, I assume I would have to use the same variables if I did stepAIC(logistic regression model) and then use those exact same variables for the random forest? </p>

<p>I have 3 unbalanced datasets that come from the same sample and I have to make something of it and I am not too sure where to start. Thanks!</p>
"
"0.127738077005317","0.0864190945072","152841","<p>I have a dependent variable distinguishing between patients that should go to treatment A or treatment B. </p>

<p>I want to develop a questionnaire containing binary variables that should decide if the patients have to go to treatment A or treatment B. We have found six variables which we think of will predict that a patient goes to treatment A or treatment B (we have used a methodology to come up with these variables). </p>

<p>I have draw a ROC curve using a logistic regression in R using the following syntax.</p>

<pre><code> mod &lt;- glm(df$treatmentAorB~df$varA+df$varB+df$varC+df$varD+df$varE+df$varF, 
family=""binomial"")

df$predpr &lt;- predict(mod,newdata=data.frame(df$treatmentAorB,
                                            df$varA,df$varB,df$varC,
                                            df$varD,df$varE,df$varF),
                     type=c(""response""))
</code></pre>

<p>So the roc curve can be drawn:</p>

<pre><code>roccurve &lt;- roc(df$treatmentAorB ~ df$predpr)
plot(roccurve)
</code></pre>

<p>I calculated the cut-off point (or threshold) (specificity and sensitivity are equal important) by using the Youden index. </p>

<p>As I need the cut-off score based on the total score of the questionnaire I used the following (-4.9048 is the B0 of the logistic regression).</p>

<pre><code>rawcutoffscore = log(treshold[1]/(1-treshold[1]))- -4.9048 
</code></pre>

<p>I got a cutoff score of 4.5 and in the questionnaire I tell the researchers to use the weights associated with the variables I put in the logistic regression. If the total score is above 4.5 they have to sent the patient to treatment A and if it is below to treatment B.</p>

<p>So the weights of the logistic regression are the weights I use in the questionnaire. </p>

<ul>
<li><p>My first question is if this is a sound methodology. And if not, what
is a better way to address this issue?</p></li>
<li><p>My second question is, if I can use the ROC curve with my independent
(binary variables)?</p></li>
</ul>
"
"0.1176979772673","0.0696733014291618","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"NaN","NaN","152948","<p>What can I do about rank deficiency? Can I just ignore it? I have an unbalanced dataset and using logistic regression (caret glm). I get 50 errors saying that my data is rank deficient and the results may be misleading. When I try to remove the correlated variables, it just deletes my binary classification variable Class. Any input would be great. Thanks!</p>
"
"0.134303827337563","0.0763232776972177","153510","<p>I am trying to fit a regularized logistic regression to my data using glmnet. Using $\alpha=1$ I get a LASSO-regression, which is what I want. My problem is though that I don't know how the intercept is fitted. In glmnet one has the option to put <code>Intercept=TRUE</code> or <code>Intercept=FALSE</code>. As far as I understand <code>FALSE</code> sets my intercept to 0. When <code>TRUE</code>, I understood that the intercept was fitted as the mean of the $y$-values. Since my data is balanced binary data with values 0 and 1, $\bar{y}=0.5$, but my analysis gives me the value -2.6. </p>

<p>I read <a href=""http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet"">How is the intercept computed in GLMnet? </a> but I don't understand it, so I hope someone will give some details. Also, in the link's article there is a likelihood function (13) and (14) on page 8 and I don't understand why it has $1/N$ in front.  </p>
"
"0.0657951694959769","0.0467382714637317","153527","<p>I am looking at the <code>wine</code> dataset from the R <code>FactoMineR</code>package to find out whether the categorical variable soil type affects the feature set. Basically there are four distinct soil types and 29 different features and there are 20 observations</p>

<pre>
obs&nbsp;soil&nbsp;f1&nbsp;f2&nbsp;...&nbsp;f29
  1&nbsp;   1&nbsp;11&nbsp;87&nbsp;...&nbsp;20
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
  7&nbsp;   2&nbsp;13&nbsp;10&nbsp;...&nbsp;10
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 15&nbsp;   3&nbsp;77&nbsp;53&nbsp;...&nbsp;54
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 20&nbsp;   4&nbsp;88&nbsp;81&nbsp;...&nbsp;21
</pre> 

<p>Given that I cannot use ANOVA or logistic regressions what other ways are there to figure out if the soil type affects the features? I am considering using box plots for visual inspection, but 29 box plots seems like a bit much. Are there any succinct plot types that could give me this kind of information?</p>
"
"0.159576725580782","0.128471220002505","154112","<p>I have a dataset with more than 20 predictors and a single binary response variable. With only $n=181$ observations (64 deaths, 117 survivors), I decided to apply penalized logistic regression to modeling, with all predictors involved (so that I avoid problems associated with model selection). Nevertheless, I have to produce a ''simpler'' model too (i.e. one that is simple enough to be suitable for a nomogram-style hand calculation in clinical setting). For that end, I intend to use <code>rms</code>'s <code>fastbw</code>.</p>

<p>To exemplify my questions, I'll use the <code>support</code> dataset from <code>Hmisc</code>:</p>

<pre><code>library( rms )
getHdata( support )
fit &lt;- lrm( hospdead ~ rcs( age ) + sex + rcs( meanbp ) + rcs( crea ) + rcs( ph ) + rcs( sod ), data = support, x = TRUE, y = TRUE )
fit
</code></pre>

<p>First, I apply penalization:</p>

<pre><code>p &lt;- pentrace( fit, seq( 0, 10, by = 0.01 ) )
plot( p )
fitPen &lt;- update( fit, penalty = p$penalty )
fitPen
</code></pre>

<p>I hope I'm correct up to this point.</p>

<p>Next, I validate the model and calculate its calibration curve. If I understand it correctly, I shouldn't validate/calibrate the simpler model, rather, I have to run the necessary functions on the <em>original</em> model, but with <code>bw=T</code>. That is:</p>

<pre><code>validate( fitPen, B = 1000, bw = TRUE )
plot( calibrate( fitPen, B = 1000, bw = TRUE ) )
</code></pre>

<p><strong>Question #1</strong>: Am I correct in this? I.e. is it true that to get the simpler model's validation/calibration I have to run these not on the simpler model, but on the original one (with <code>bw=T</code>)? And the results will be those pertaining to the simpler model, despite the fact that I haven't run validation/calibration on the simpler model itself?</p>

<p>Next, I try to come up with the simpler model <em>explicitly</em>. Interestingly, <a href=""http://www.aliquote.org/cours/2011_health_measures/harrell98.pdf"" rel=""nofollow"">(Harrell, 1998)</a> uses a method which is based on calculating the logits for the observations, then modeling them with OLS, then narrowing this model with <code>fastbw</code>. Although it is surely my statistical shortcoming, I simply can't understand why this is necessary.</p>

<p><strong>Question #2</strong>: Why can't we <em>directly</em> use <code>fastbw</code> on the logistic regression model? Such as:</p>

<pre><code> fastbw( fitPen )
 fitApprox &lt;- lrm( as.formula( paste( ""hospdead ~"", paste( fastbw( fitPen )$names.kept, collapse = ""+"" ) ) ), data = support, x = TRUE, y = TRUE )
</code></pre>

<p>And finally, I am not completely sure on where should I apply penalizing in the whole process.</p>

<p><strong>Question #3</strong>: Should I penalize the original model, then run <code>fastbw</code> (see above), and then re-penalize the obtained model? I.e.</p>

<pre><code>p &lt;- pentrace( fitApprox, seq( 0, 10, by = 0.01 ) )
plot( p )
fitApproxPen &lt;- update( fitApprox, penalty = p$penalty )
fitApproxPen
</code></pre>

<p>Or I don't have to re-penalize the narrowed model? Or I don't have to penalize the original model and it is sufficient to penalize the simpler one? (I suspect that the very first option is the correct, but I'm not entirely sure.)</p>
"
"0.0465242105199235","0.0440652649239232","154448","<p>The logistic regression model is:</p>

<p>$$\log\bigg(\frac{p}{1-p}\bigg) = \ldots$$</p>

<p>The most interesting case (for me) is the case that we have $p=1$ and $p=0$. But in this case, the ratio $p/(1-p)$ doesn't exist</p>

<p>For example: In my model, $p$ is the probability that the customer will come back after the first purchase. We observed that 100% of the clients with the income $&gt; 5000$ euros comes back after the first purchase ($p=1$), and 100% of the clients with the income $&lt; 1000$ don't ($p=0$). </p>

<p>When I treat the income as a continuous explanatory variable, there is no problem (income is a significant variable). But it isn't significant when I segment income into intervals $(0,1000)$, $(1000,3000)$, $(3000,5000)$, $(&gt;5000)$. All the categories become non-significant. I think it's because of the $p=1$ that makes $1-p=0$, then the ratio $p/(1-p)$ is degenerate. What should I do in this case?</p>
"
"0.0930484210398471","0.0440652649239232","154578","<p>Link functions are typically sigmoid. The idea being that the underlying data is fitted to the curve with an increase in the predictor summation gives an increase in the response. However, could one use a link function that is Gaussian shaped? Such that there is a ""sweet spot"" with a trailing off on each side? </p>

<p>Doing more reading, I am asking if a glm can be used with a radial basis function similar to a radial basis function (RBF) network. You may ask why I wish for this? I need the model to be very interpretable. White box only, no black boxes allowed.</p>

<p><img src=""http://i.stack.imgur.com/nWmc1.png"" alt=""enter image description here""></p>

<p>I could address this situation by using logit logistic regression with polynomial expansion of the predictors or by introducing addition variables with a knot pivot.</p>

<p>I am just wondering if there was a more direct path.</p>
"
"0.232621052599618","0.124851583951116","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.0930484210398471","0.0440652649239232","154909","<p>I want to simulate data from multilevel logistic regression .</p>

<p>I focus on the following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , $u_{0j}\sim N(0,\sigma_0^2)$ , $u_{1j}\sim N(0,\sigma_1^2)$ , $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$</p>

<p>I prefer <code>R</code> and started to write codes for the simulation as :</p>

<pre><code> set.seed(36)

 x &lt;- rnorm(1000)  ### individual level variable , x_ij

 z &lt;- rnorm(1000)  ### group level variable , z_j
</code></pre>

<p>If I have initial value for $\gamma_{00}=-1 , \gamma_{01}=0.3,\gamma_{10}=0.3,\gamma_{11}=0.3$ , how can I generate $\pi_{0j},\pi_{1j}$ since there is $u_{0j},u_{1j}$ in equation (2) and (3) ?</p>
"
"0.124341182825498","0.0706616245726062","154917","<p>I have written <code>R</code> codes for simulating data from Multilevel logistic regression model . </p>

<p>I focus on the following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , $u_{0j}\sim N(0,\sigma_0^2)$ , $u_{1j}\sim N(0,\sigma_1^2)$ , $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$</p>

<p>In this <a href=""http://www.biomedcentral.com/1471-2288/7/34#sec2"" rel=""nofollow"">paper</a> in equation (2) , they assumed $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$ , that is not independent . But also they mentioned in the methodology section that :</p>

<blockquote>
  <p>The group random components $u_{0j}$ and $u_{1j}$
  are ""independent"" normal variables with mean zero and
  standard deviations $Ïƒ_0$ and $Ïƒ_1$. </p>
</blockquote>

<p>So I assumed $\text{cov}(u_{0j},u_{1j})=0$ . </p>

<p>R code :</p>

<pre><code>## Simulating data from multilevel logistic regression 

set.seed(1234)
x &lt;- rnorm(1000) ### individual level variable
z &lt;- rnorm(1000) ### group level variable

##fixed effect parameter
g_00 &lt;- -1
g_01 &lt;- 0.3
g_10 &lt;- 0.3
g_11 &lt;- 0.3

g &lt;- matrix(c(g_00,g_01,g_10,g_11),ncol=1)

require(mvtnorm)

##need variance values as input 
s2_0 &lt;- 0.36
s2_1 &lt;- 1
s01 &lt;- 0

##generate bi-variate normal rv for u0, u1

avg &lt;- c(0,0) ##mean
sigma &lt;- matrix(c(s2_0,s01,s01,s2_1),ncol=2)

u &lt;- rmvnorm(1000,mean=avg,sigma=sigma,method=""chol"")

pi_0j &lt;- g_00 +g_01*z + as.vector(u[,1])
pi_1j &lt;- g_10 +g_11*z + as.vector(u[,2])
p &lt;- exp(pi_0j+pi_1j*x)/(1+exp(pi_0j+pi_1j*x))

y &lt;- rbinom(1000,1,p)
</code></pre>

<p>But i am not understanding where is to consider the group ? If i select number of groups to be $100$ $(j=1,2,\ldots, 100)$, then will I assign the groups randomly against each $y_{i,j}$ ?</p>

<ul>
<li>Have i correctly simulated data from <code>Multilevel Logistic Distribution</code> ?</li>
</ul>
"
"0.147122471584125","0.0696733014291618","154986","<p>We want to do the logistic regression analysis to consider the effect of Age, CD4 on drug resistance mutations. The code that we wrote is:</p>

<pre><code>logist.summary(glm(DRM ~ Age, data = Database, family = binomial),""wald"")
</code></pre>

<p>The results are: </p>

<pre><code>            log.OR OR lower.CI upper.CI p.value
(Intercept)  -0.31 0.74     0.05     9.95  0.8169
Age          -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>However, we want to do the test like, we will consider whether, 20 years old differences between the subjects, what the results is? Is it relative to DRMs? We wrote:</p>

<pre><code>logist.summary(glm(DRM ~ I(Age+20), data = Database, family = binomial),""wald"")
</code></pre>

<p>Results:</p>

<pre><code>            log.OR   OR lower.CI upper.CI p.value
(Intercept)   1.17 3.22     0.05   190.62  0.5742
I(Age + 20)  -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>I want to ask:</p>

<ul>
<li>Is the code we wrote correct?</li>
<li>Can you help me explain what is meaning of these table?</li>
<li>Why it is the same results for the Age and Age+20? But differences in the Intercept? What does intercept meaning in this case?</li>
</ul>
"
"0.0657951694959769","0.0311588476424878","155354","<p>I'm running a logistic regression in order to descriptively analyze the relationship between my independent and dependent variables. As I understand it, it's not mathematically necessary to normalize variables in order for the logit model to converge. However, if I want to compare my odds ratios at the end and see which independent variables have the greatest effect, it seems like the variables should all be on the same scale, so I should normalize them. I'm thinking that feature scaling so everything is between the ranges [0,1] would be adequate. Does that make sense?</p>
"
"NaN","NaN","155459","<p>I'm trying to predict the outcome ""Decision"" in the function of Age, Gender, Occupation, .... </p>

<p>The independent variable ""Occupation"" is known to be significant. But when I do the logistic model, each sub-group (modality) of it is not.</p>

<p>Should I regroup the levels having the same value of estimated coefficient? (which I guess doesn't make many sense because the levels are not statistically significant)</p>

<p>The variable Occupation has 74 different sub-groups.</p>

<p>And another problem is that when checking the multicollinearity, the function VIF in R doest work, it produces the NaN value, may be its due to the large number of sub-groups of Occupation.</p>

<p><img src=""http://i.stack.imgur.com/ruScu.png"" alt=""Summary(Logistic Regression)""></p>
"
"0.131590338991954","0.0830902603799674","155668","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has: 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>doing this below doesn't specify the individual age group, or can this be worked out from the summary stats? </p>

<pre><code>Model&lt;-glm(Maturity~Lgnth+age, family=binomial(logit)) 
</code></pre>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this. Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.139572631559771","0.0771142136168656","155762","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has:
 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this.
Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.131590338991954","0.0623176952849756","155977","<p>I just built a credit risk score model (using logistic regression).</p>

<p>Now that I have all estimates and resulting score per observation I would like to create risk groups, e.g.: 10 risk groups where 1 is very low or now risk and 10 is very high risk.
I am looking for an algorithm in R (or SAS) that helps me split the resulting model score into 10 (or similar number) of heterogeneous (based on regression model target odds).</p>

<p>A simple example is to split population into groups where the average odds are doubled by the next group :</p>

<pre><code>Risk 
Group               Odds    %Population
1(score 1-80)       0.8     10%
2(score 80-120)     1.6      5%
3(score 12-220)     3.2     20%
4(score 220-400)    6.4      8%
....
</code></pre>

<p>All ideas are much appreciated!</p>
"
"0.0465242105199235","0.0440652649239232","156564","<p>I just developed a logistic regression model predicting customer churn (i.e how likely is a customer to leave us in the future?)</p>

<p>To understand the impact of my independent variables I calculated Odds ratio using the following function. </p>

<p><code>exp(trainingmodel$coefficients)</code></p>

<p>Where trainingmodel is the name of my model.</p>

<p>And I get the following results:</p>

<pre><code>AIRTIME 
9.789127e-01

Site.Report.By.Vehicle1
1.241823e+00
</code></pre>

<p>Both AIRTIME and Site Report are a feature of product we offer. In my dataset, AIRTIME is a continuous variable whereas Site.Report.By.Vehicle1 is a categorical variable with just two levels, ie. someone using the Site Report or not?</p>

<p>Can someone please help me to understand how to interpret the above number for AIRTIME and Site Report?</p>
"
"0.0465242105199235","0.0440652649239232","156610","<p>Whenever I run a logistic regression, I need to set the threshold so that it groups probabilities higher than the threshold to my positive group:</p>

<pre><code>table(test$Noshow, logpred&gt;0.5)
</code></pre>

<p>What I sometimes do is to optimize my confusion matrix so that I get a good Sensitivity and Specificity values. Is this a good way to do it?</p>

<p>On the other hand, when I run a randomForest model using the ""randomForest"" package, I can set the cutoffs such as this:</p>

<pre><code>k &lt;- 0.5
rfcutoff &lt;- c(k, 1-k)
rftree &lt;- randomForest(rffrmla_all, data = train1, mtry = 2, keep.forest = TRUE, importance = TRUE, ntree = 500, cutoff = rfcutoff)
</code></pre>

<p>Again I optimize my confusion matrix to get a good cutoff or base the cutoff on a naive baseline proportion. What is a good way to do this? </p>
"
"0.0379868588198793","0.0359791381480577","156766","<p>I have been having real issues trying to calculate the length at which certain probabilities of maturing are reached. This is NOT the same as the proportion of individuals that are mature as <code>dose.p</code> would calculate.</p>

<p>I know I have to run a logistic regression with binomial errors, something similar to the below code</p>

<pre><code>mylogit &lt;- glm(Maturity ~ Lngth, data = data, family = ""binomial"")
</code></pre>

<p>But from this how can I work out the length at which the probability of maturing equals 50%.
My data frame consists of Maturity (0,1) data and length data. (I have other data but believe this is all I need at this stage (I could be wrong!)
Any help would be greatly appreciated! </p>
"
"0.0465242105199235","0.0440652649239232","156780","<p>I am running a logistic regression with 5 continuous independent variables (IV). The problem is that IV4 when taken alone has a positive correlation with outcome (coeff > 0), and when taken with the other variables has a negative correlation (coeff &lt; 0). I evaluated correlation between IV4 and the other variables, and the results are: 
IV4 vs. IV1 (-0.51), IV4 vs. IV2 (-0.48), IV4 vs. IV3 (0.61) and IV4 vs. IV5 (0.73).</p>

<p>I ran other logistic regressions <em>eliminating one at a time all the other variables</em> to look if one of them was responsible for the sign change, and I noticed that when eliminating IV1, the sign of V4 coefficient became positive.</p>

<p>Thus, it seems that IV1 changes the sign of the coefficient of IV4. 
Is there someone who knows what might be the cause and (possibly) the solution?</p>

<p>Practically, do I have to eliminate the IV4 (or IV1) from the model and explain why?</p>

<p>Thanks a lot for answering</p>

<p>Leonardo Frazzoni, MD</p>
"
"0.0465242105199235","0.0440652649239232","156804","<p>I have estimated a mixed-effects logistic regression with glmer
and want to draw a bootstrapped confidence-region for the mean predicted probability for two subgroups of the sample.</p>

<p>I have a $1000 \times 2$ Matrix $X$ containing the bootstrapped mean predicted probabilities for the two groups.
One could now compute the empirical covariance matrix $S$ and draw a
circle around the means using the metric induced by $S^{-1}$,
i.e. drawing a confidence ellipsoid based on normality-assumption.</p>

<p>Are there any widely used alternatives to this approach that do not imply distributional assumptions? </p>

<p>skeletor</p>
"
"0.0657951694959769","0.0623176952849756","157909","<p>I have a binary variable Y that is a dichotomization of an unknown latent variable, generated by a regression model with normal error. Therefore it makes sense to fit a probit model to Y.
R enables me to do so using the ""glm"" function.
I would also like to fit a probit model with LASSO penalty.
Using the function ""cv.glmnet"" I can fit a logistic regularized model to this problem, however I couldn't find a way to fit a probit regularized model in R.</p>

<p>My questions:</p>

<ol>
<li>Is there a simple way to fit a probit regularized model in R?</li>
<li>If not, can I fit a logistic one instead (as the probit and logit are quite similar)?</li>
</ol>

<p>Thanks!</p>

<p>Amit</p>
"
"0.1176979772673","0.0696733014291618","158926","<p>I want to generate outliers in binary logistic model</p>

<p>What I want is:<br>
to select 3 elements  of 10 elements  that are randomly generated, and:<br>
if the selected value is 1  convert it to zero
&amp;
if the selected value is 0  convert it to 1
and display the new y with 10 elements including the changed and the unchanged values</p>

<p>I searched through r and I  was able to write the following code to generate y-outliers:  </p>

<pre><code>Y &lt;- as.numeric(runif(10)&gt;0.5)
Y[toreplace] &lt;- sample(y = seq_along(Y), size = 3)
Y[sample(Y, 3)] &lt;- 1
</code></pre>

<p>but this code displays only the 3 changed elements,  besides it change 0  to 1 only; I want to change both: from 0 to 1 and from 1 to 0.</p>

<p>I also want correct code to change x-values in logistic regression to to obtain good leverage points and bad leverage points</p>

<p>any help, please</p>
"
"0.0465242105199235","0.0440652649239232","159301","<p>I have a classification problem I am attempting to model using logistic regression (via the <code>glm</code> package in R): </p>

<pre><code>cols &lt;- c(""x"", ""z"", ""a"", ""b"", ""c"") 
formula = paste0(""x ~ "", paste(cols, collapse = ""+""))
formula = as.formula(formula)
</code></pre>

<p>I have a bunch of explanatory variables at the moment. How advisable is it to model this relationship using <code>gbm</code>, see the relative inference strength of each variable, and then remove seemingly meaningless variables before <code>glm</code> regression?</p>

<p>I ask just because I have done this in the past, with <code>glm</code> and <code>gbm</code> giving seemingly contradictory signals on different explanatory variables. </p>
"
"0.0986927542439653","0.0467382714637317","159316","<p>I have been running logistic regression in R, and have been having an issue where as I include more predictors the z-scores and respective p-values approach 0 and 1 respectively.  For example if have few predictors:</p>

<pre><code>&gt; model1
b17 ~ i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -6.9461     1.8953  -3.665 0.000247 ***
i74           0.6842     0.9543   0.717 0.473384    
i73           1.7691     4.8008   0.368 0.712502    
i72           0.5134     2.0142   0.255 0.798812    
i71          -0.6753     4.9173  -0.137 0.890771    
</code></pre>

<p>The results appear to be fairly reasonable; however, if I have more predictors:</p>

<pre><code> &gt; model1
b17 ~ i90 + i89 + i88 + i87 + i86 + i85 + i84 + i83 + i82 + i81 + 
i80 + i79 + i78 + i77 + i76 + i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.887e+02  3.503e+05  -0.001    0.999
i90          1.431e-01  1.009e+04   0.000    1.000
i89          8.062e+01  1.027e+05   0.001    0.999
i88          9.738e+01  7.398e+04   0.001    0.999
i87         -1.980e+01  9.469e+03  -0.002    0.998
i86          9.829e+00  1.098e+05   0.000    1.000
i85          5.917e+01  3.074e+04   0.002    0.998
i84         -2.373e+01  1.378e+05   0.000    1.000
i83          7.257e+00  2.173e+05   0.000    1.000
i82         -1.397e+01  1.894e+05   0.000    1.000
i81          6.503e+01  1.373e+05   0.000    1.000
i80          3.728e+01  4.904e+04   0.001    0.999
i79          1.010e+02  5.556e+04   0.002    0.999
i78         -2.628e+01  1.546e+05   0.000    1.000
i77          4.725e+01  3.027e+05   0.000    1.000
i76         -6.517e+01  1.509e+05   0.000    1.000
i74          1.267e+01  1.175e+05   0.000    1.000
i73          2.796e+02  5.280e+05   0.001    1.000
i72         -2.533e+02  4.412e+05  -0.001    1.000
i71         -1.240e+02  4.387e+05   0.000    1.000
</code></pre>

<p>I know it is hard to say exactly what is going on without seeing the data, but the predictors are all 5-point Likert Scale items.  However, are there any thoughts to what is occurring here?  I don't have much experience with logistic regression, so I apologize if the question seems naive, but is there a certain threshold of predictors where logistic regression falls apart due to having such a large amount of predictors what is ultimately a very small amount of variance?  Is the potentially a multi-co-linearity issue?  Finally, when I run OLS regression on the data I get results that make more sense (or at least appear to), is it okay/what are the consequences of running OLS regression on a binary outcome?  Thank you!</p>
"
"0.0930484210398471","0.0881305298478463","159469","<p>I am relatively new to R. The short version of the data looks ike this:   </p>

<pre><code>sNumber  blockNo running TrialNo    wordTar   wordTar1   Freq Len code code2
1        1       1       5           spouse    violent   5011   6    1     2
1        1       1       5          violent     spouse  17873   7    2     1
1        1       1       5           spouse    aviator   5011   6    1     1
1        1       1       5          aviator       wife    515   7    1     1
1        1       1       5             wife    aviator  87205   4    1     1
1        1       1       5          aviator     spouse    515   7    1     1
1        1       1       9        stability    usually  12642   9    1     3
1        1       1       9          usually   requires  60074   7    3     4
1        1       1       9         requires     client  25949   8    4     1
1        1       1       9           client   requires  16964   6    1     4
2        2       1       5            grimy      cloth    757   5    2     1
2        2       1       5            cloth       eats   8693   5    1     4
2        2       1       5             eats    whitens   3494   4    4     4
2        2       1       5          whitens      woman     18   7    4     1
2        2       1       5            woman    penguin 162541   5    1     1
2        2       1       9              pie   customer   8909   3    1     1
2        2       1       9         customer  sometimes  13399   8    1     3
2        2       1       9        sometimes reimburses  96341   9    3     4
2        2       1       9       reimburses  sometimes     65  10    4     3
2        2       1       9        sometimes   gangster  96341   9    3     1
</code></pre>

<p>I have a code for ordinal regression analysis for one participant for one trial (eye-tracking data - eyeData) that looks like this:</p>

<pre><code>#------------set the path and import the library-----------------
setwd(""/AscTask-3/Data"")
library(ordinal)

#-------------read the data----------------
read.delim(file.choose(), header=TRUE) -&gt; eyeData

#-------------extract 1 trial from one participant---------------
ss &lt;- subset(eyeData, sNumber == 1 &amp; runningTrialNo == 5) # extract the 5th trial from the 1st participant

#-------------delete duplicates = refixations-----------------
ss.s &lt;- ss[!duplicated(ss$wordTar), ] 

#-------------change the raw frequencies to log freq--------------
ss.s$lFreq &lt;- log(ss.s$Freq)

#-------------add a new column with sequential numbers as a factor ------------------
ss.s$rankF &lt;- as.factor(seq(nrow(ss.s))) 

#------------ estimate an ordered logistic regression model - fit ordered    logit model----------
m &lt;- clm(rankF~lFreq*Len, data=ss.s, link='probit')
summary(m)

#---------------get confidence intervals (CI)------------------
(ci &lt;- confint(m)) 

#----------odd ratios (OR)--------------
exp(coef(m))
</code></pre>

<p>The eyeData file is a huge massive of data consisting of 91832 observations with 11 variables. In total there are 41 participants with 78 trials each. In my code I extract data from one trial from each participant to run the anaysis. However, it takes a long time to run the analysis manually for all trials for all participants. Could you, please, help me to create a loop that will read in all 78 trials from all 41 participants and save the output of statistics (I want to save <strong>summary(m), ci, and coef(m)</strong>) in one file.</p>
"
"0.113960576459638","0.0539687072220866","159735","<p>I have two factors that are fully crossed, the levels of the factor are each coded 0 and 1. I am running a regression testing for one main effect and one interaction. The following is my logistic regression formula:</p>

<pre><code>m1=glmer(y~1+A+A:B+(1|Participants)+(1|Word),data=data, family = ""binomial"")
</code></pre>

<p>I am wondering if this is acceptable (only testing for one main effect and an interaction), and also why I am getting two interaction terms in my output:</p>

<pre><code>Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.18740    0.21600  -0.868  0.38561   
A1           0.74546    0.28399   2.625  0.00867 **
A0:B1        0.01537    0.28244   0.054  0.95662   
A1:B1        0.15884    0.28650   0.554  0.57929   
</code></pre>
"
"0.175453785322605","0.0934765429274634","160109","<p>I'm trying to fit a (logistic) regression model to predict the successful funding of crowdfunding ventures (0/1) based on a series of IV with different level of measurement. One of these IVs is a categorial variable that indicates the nature of the venture (like technology, media, etc.). From visually inspecting the IVs I can see that the other IVs take different values relative to the venture category.</p>

<p><strong>My question is, how do I properly incorporate the categorial variable into a regression model (using R)?</strong></p>

<p>I do realize I could simply add the categorial variable with all the other IVs like this:</p>

<pre><code>glm(success ~ IV1 + IV2 + IV3 + ... + ventureCategory +, data=data, family=""binomial"")
</code></pre>

<p>From what I understand this would only return the overall influence of the venture categories on successful funding, but not the potential interaction between venture category and IV1 to IVn.</p>

<p>If I was to include interactions, how would I know which interactions to add?
And if I assume the venture category to be influential on all the IVs do I include separate interactions for every IV with the ventureCategory like this</p>

<pre><code>glm(success ~ IV1 + Iv1:ventureCategory + IV2 + Iv2:ventureCategory + ... + ventureCategory, data=data, family=""binomial"")
</code></pre>

<p>It seems that model quickly becomes quiet messy and I got something mixed up here.</p>

<p>Finally, I read two studies, who include this exact variable differently.
Here is a link to one of them: <a href=""https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44"" rel=""nofollow"">https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44</a></p>

<p>Their model simply states they controled for categories (see table p.12) without providing any coefficients or p values. I read this in other papers too, but do not understand how the control variable actually contributes to the model in this case.</p>

<p>I also read through some of the threads here, but couldn't find anything that really helped me understand the underlying rationale.</p>

<p>Update: I have 5 IV in total, excluding the categorial one.</p>

<p>Best</p>
"
"0.162834736819732","0.0881305298478463","160425","<p>I am somewhat new to R and trying to polish my logistic regression. I am testing if my risk factors(cruise, age, sex, and year) have a significant effect on my dependent variable, MPS infection (named MPS_BINARY). I have a total of four cruises (5, 7, 9, 11), three years, thirteen ages, and two sexes (1 or 0). </p>

<p>I am able to run the full model with the following command:    </p>

<pre><code>&gt;mylogit&lt;-glm(MPS_BINARY~ AGE * SEX * CRUISE * YEAR, data=mps,family=""binomial"")
</code></pre>

<p>From this I have my p-values, I can now identify significant risk factors and interactions. My data shows a significant p-value for the interaction between cruise 7 and year. Following backwards selection, I now need to run the model again with my original risk factors and significant terms. I am having increasing difficulty isolating cruise 7 from my cruise data to run as an interaction with year. I have tried using the command:</p>

<pre><code>&gt;mylogit&lt;-glm(MPS_BINARY~AGE+SEX+YEAR+CRUISE+mps$CRUISE7*YEAR,data=mps,family=""binomial"")
</code></pre>

<p>But this, of course, does not recognize cruise 7 and I receive the error message: Error in model.frame.default(formula = MPS_BINARY ~ AGE + SEX + CRUISE +  : 
  invalid type (NULL) for variable 'mps$CRUISE7'.</p>

<p>My question is how can I run my logistic regression with all of my risk factors, and the significant interaction between year and cruise 7? I cannot figure out how to isolate only cruise 7 for the interaction with year. Please let me know if you need more information, thank you! </p>
"
"0.0657951694959769","0.0311588476424878","160495","<p>I working with R on a classification problem. My outcome variable is binary with two levels 1 and 2. 
First of all I tried the logistic regression, which of all methods has the best performance, altough still poor. </p>

<p>I tried nnet package, random forest, the fuzzy package frbs and decision trees. </p>

<p>The nnet function gives me only one class - in this case 2.</p>

<p>I had some hope with frbs package. See my code below:</p>

<pre><code>obj &lt;- frbs.learn(train,method.type=""FRBCS.CHI"",control=list(num.labels=3,type.mf=""GAUSSIAN""))
summary(obj)
#test set without def 
pred&lt;-predict(obj,newdata=test[,1:8])
</code></pre>

<p>But the predictions are wrong, the class 1 is completely missclassified</p>

<pre><code>#percentage error
tdef&lt;-test$def
err = 100*sum(pred!=tdef)/ nrow(pred)
print(err)
[1] 16.93038
</code></pre>

<p>I'm wondering what I could improve to classify the output variable. Is something wrong with my data? 
Are the parameters not right? </p>

<p>Can someone please verifiy?  I'm at the end of my knowledge...</p>

<p>You can find the (normalized) data here:
<a href=""https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg"" rel=""nofollow"">https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg</a></p>
"
"0.0882734829504749","0.0696733014291618","160545","<p>I recently ran two tests in R - one using glm() and one using lm() with the goal being to test the relationship between a binary response and binary predictor.  I ran glm() first and got an estimate of -0.68 for the predictor coefficient which I thought was pretty good.  P&lt;.05 and AIC of 653.  </p>

<p>When I ran lm() however I got an estimate of -.14, a multiple r-squared of .008, P&lt;.05.  </p>

<p>My understanding is that linear regression is usually a poor choice for a categorical response compared with logistic regression, but when is this not the case? I noticed in this post <a href=""http://statisticalhorizons.com/linear-vs-logistic"" rel=""nofollow"">http://statisticalhorizons.com/linear-vs-logistic</a> that the author states there's middle ground where it does make sense to use linear regression.  Are there any common rules (or rules of thumb you personally use) that determine when to try out linear regression on a categorical response?  Do any of these differ from the author's cases?</p>
"
"0.113960576459638","0.0539687072220866","160598","<p>I have an imbalanced dataset with 4995:5 ratio as well as other datasets with less imbalanced ratios. I split this 4995:5 ratio into training and testing for about 2/3 training and 1/3 testing. I also decided to downSample using caret for the 4995:5 ratio dataset - this dataset now becomes 5:5.</p>

<p>Repeated cross validation works fine for the other datasets since there are more of the minority class, but for the training set of the 4995:5 ratio, I get the binomial class has less than 8 observations for either random forest or logistic regression.</p>

<p>Would I have to resort to bootstrap or LOOCV? This dataset seems to be problematic because of the terrible ratio.</p>
"
"0.127661380464626","0.120914089414122","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.175453785322605","0.0934765429274634","160709","<p>I am doing binary logistic regression in R and I need to calculate the Count R squared for various model specifications. Count R2 is the number of correctly predicted observations using the model divided by the total number of observations. It measures how well the model predicts the correct value of the dependent variable, using known values. I'm planning to use the model for prediction, so the percent of observations that are predicted correctly would be really useful for me. Creating a classification table is difficult because I have missing data, so the fitted table and the original table have different numbers of records. I'm pretty inexperienced with R, so I don't know if there's a straightforward way to get around that.</p>

<p>My specific question is:</p>

<p>Is there a command in R to get the Count R2, (and better yet, the adjusted count R2)?</p>

<p>If not, is there an easy way to get R to put the predicted probabilities and the original dependent variable in a table together, when there are different numbers of records? This would allow me to calculate the Count R2 myself.</p>
"
"0.175453785322605","0.0934765429274634","161113","<p>I am working on example 7.3.1 from the Second Edition of the book <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=an+introduction+to+generalized+linear+models+second+edition+pdf"" rel=""nofollow"">An Introduction to Generalized Linear Models</a> in section <em>7.3 Dose response models</em>. This example fits a simple logistic regression model on the following data: </p>

<p><img src=""http://i.stack.imgur.com/YkHCG.png"" alt=""enter image description here""></p>

<p>This seems easy enough. However, I am having an issue with the Deviance Statistic calculated for this example. The following is my R code that will reproduce a Deviance Statistic $D=11.23$ just like this example in the book has. </p>

<pre><code>#original data
#copied in by row
( df &lt;-  data.frame( 
  Trial = 1:8,
  Dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839),
  Yes = c(6, 13, 18, 28, 52, 53, 61, 60),
  No = c(59, 60, 62, 56, 63, 59, 62, 60)- c(6, 13, 18, 28, 52, 53, 61, 60),
  Total = c(59, 60, 62, 56, 63, 59, 62, 60)
) )

#Logistic Regression Model
mle_beet &lt;- glm(cbind(Yes, No)~Dose, family=binomial(logit), data=df)
mle_beet$deviance
##
</code></pre>

<p>Section 5.6.1 of this same book derives the <em>Deviance Statistic</em> for the Binomial Model to be: </p>

<p>$D = 2\sum^{N}_{i=1}y_{i}[ log_{e}(\frac{y_i}{\hat{y_i}})+(n_i - y_i)log_{e}(\frac{n_i - y_i}{n_i - \hat{y_i}}) ]$</p>

<p>However, looking closely at the given data, it can be seen that for the last row, the number of beetles killed is the same as the total number of beetles ( $n_{8}=y_{8}$ ). This means that the very last part in the sum for <code>D</code> is: </p>

<p>$ y_{8}log_{e}(\frac{y_8}{\hat{y_8}})+(n_8 - y_8)log_{e}(\frac{n_8 - y_8}{n_8 - \hat{y_8}}) = 60log_{e}(\frac{60}{\hat{y_8}})+(0)log_{e}(\frac{0}{n_8 - \hat{y_8}})$</p>

<p>In particular, this value contains: </p>

<p>$0log_{e}(0)=0(-\infty)=$ <strong><em>undefined</em></strong></p>

<p>Here is the R code that agrees with this: </p>

<pre><code>sum( 2*(df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) + (df$Total-df$Yes)*
log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) )
</code></pre>

<p>My question is: What is the mathematical reasoning for computing the Deviance Statistic when $n_i=y_i$? What do the book and R do in the background to obtain $D=11.23$?</p>

<p>(Note that the book likely didn't use R to get this value, but the two agree)</p>

<p>Thank you!</p>

<p>EDIT: See the accepted answer and its comments for a great explanation.</p>

<p>If you happen to be computing the Deviance through the formula in R (you likely shouldn't since <code>mle_beet$deviance</code> shows this for you), you can replace <code>-Inf</code> or <code>Nan</code> in each vector that results from an individual operation. The following works for this example: </p>

<pre><code>x &lt;- df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) 
x[is.na(x) | x==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 
y &lt;- (df$Total-df$Yes)*
    log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) 
    y[is.na(y) | y==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 

sum(x+y)*2 #the deviance
</code></pre>
"
"0.131590338991954","0.0467382714637317","161338","<p>I have data that can be fit, more or less, by logistic growth functions. Hence I used <a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html"" rel=""nofollow"">this tutorial</a> to do this.</p>

<p>Now I want to get an x value for a specific y value from the model. Maybe this is too trivial, but I could not find anything on the forums...or perhaps I was looking in the wrong way. For the below example, I would want to get the age at which Menarche is 0.5. In Excel I'd get the formula of the fit, solve it for x and put in y=0.5 ... but in R with logistic fit?</p>

<pre><code>library(""MASS"")
data(menarche)
str(menarche)

summary(menarche)

plot(Menarche/Total ~ Age, data=menarche)

glm.out = glm(cbind(Menarche, Total-Menarche)~Age, family=binomial(logit), data=menarche)

plot(Menarche/Total ~ Age, data=menarche)
lines(menarche$Age, glm.out$fitted, type=""l"", col=""red"")
title(main=""Menarche Data with Fitted Logistic Regression Line"")
</code></pre>
"
"0.0379868588198793","0.0539687072220866","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.175844987113332","0.108258180127387","162426","<p>I am trying to create a logistic regression model and a random forest model on the same data to predict probability of default. For the logistic regression model, I have created some dummy variables from categorical variables. Finally, for the input of logistic regression, I have 9 dummy variables and 2 numeric variables (age and level, age takes values from 18 to 60, level from 4 to 10). I want to use same input dataset for the random forest model. When I did so, using ""randomForest"" Package, I get following Variable Importance Plot.</p>

<p><img src=""http://i.stack.imgur.com/qscyb.png"" alt=""enter image description here""></p>

<p>Level seems to be a very good variable both by MSE and Node Purity. Also, level is a very important variable in logistic regression (p value ~ 10^-5). 
However, Age is very important by Node purity, but not by MSE. Also, in logistic regression, age is not a very good variable with p value of 0.026. So I want to understand, Does being numeric increases the node purity importance of a variable by overfitting? Is it not suitable to use numeric and dummy variables together in random forest model? Or is there something I am missing.</p>

<p>I had similar doubts about using numeric and dummy variables in logistic regression, but in logistic regression it did not create any problem. Please help.</p>
"
"0.145986373720362","0.10370291340864","162463","<p>I am doing some data analysis on a fairly large health data set of patients with diagnoses and the respective procedures received for each event. I was asked to run a multinomial logistic regression on my data.</p>

<p>The dataset has around 4,000 columns of attributes, of which around 3,000 are unique diagnoses. The diagnosis variables take on the value of 1 if the patient had that diagnosis and 0 if he or she did not.  The remaining approximately 1,000 variables pertain to unique procedures, which also take on the value of 1 if the patient has received it, and 0 if he or she did not.</p>

<p>The dataset contains information on approximately 30,000 patients. I, admittedly naively, ran a the multinom function in the multinom package in R on all 4,000 variables, with the dependent variable being the very last procedure the patient has received (marked as ""Final procedure"" in the matrix), but R isn't able to complete the computation. </p>

<p>I would like some overall advice in perhaps a different package I could use for running regressions on large data sets (cannot use bigmemory however because this is on windows) or even perhaps reformatting my data. </p>

<p>Initially, my data set had around 50 columns, because the maximum number of diagnoses and procedures a patient had was 25 diagnoses and 25 procedures, so each column was marked as ""Diagnosis X"" and ""Procedure x,"" with the corresponding element being the actual diagnosis/procedure identifier. For all the patients who did I have all 25 diagnoses/procedures (so most of them), the values in the data frame would just be NA. Now I am wondering if I could perhaps resort to using this data frame instead and have a nicer, smaller matrix to work with? The only real reason I reformatted my data set into the much larger matrix was because my grad student asked me to do so, but maybe this isn't the way to go.</p>
"
"0.0657951694959769","0.0623176952849756","162599","<p>This question is in response to an answer given by @gung in regards to this <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">question</a></p>

<p>I am also wanting to use simulation to conduct a power analysis on a multiple logistic regression. To keep it simple <strong>I want to do a post-hoc power analysis to determine the power associated with my regression</strong>. Lets take an <code>alpha=0.05</code></p>

<p>Lets say we have 1000 samples of data. Our dataset can be assumed as:</p>

<pre><code>set.seed(123)
N &lt;- 1000
var1 &lt;- runif(N, min=0, max=0.5)
var2 &lt;- runif(N, min=0.3, max=0.7)
var3 &lt;- rbinom(n=N, size=1, prob = 0.15)
output &lt;- rbinom(n=N, size=1, prob = 0.05)

df &lt;- data.frame(var1, var2, var3, output)
</code></pre>

<p>And a simple logistic model we are using is</p>

<pre><code>model &lt;- glm(output~var1+var2+var3, 
             data=df,
             family = binomial()
</code></pre>

<p>Now where this question differs from the example, we have our binary output (<code>output</code>) and not a continuous rate. </p>

<p>From what I have read, when unsure of the effect size, select a medium rate.</p>
"
"0.0657951694959769","0.0623176952849756","162867","<p>I'm currently building zero-inflated Poisson &amp; negative binomial predictive models using the zeroinfl() function from the pscl package in R.</p>

<p>Incorporating penalized regressions into my model to account for shrinkage and variable selection is a priority. In addition I'd like to use penalization to avoid convergence issues due to perfect/quasi separation in my data (better than manually removing variables).</p>

<p><strong>Question</strong>: Realizing that zero-inflated models $\neq$ hurdle models, for purposes of variable selection will my models be <strong>seriously biased</strong> if I first run separate run lasso (or elastic net) Poisson and logistic regressions with glmnet to select variables for the zeroinfl()?</p>
"
"0.193429485824666","0.108258180127387","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.0882734829504749","0.0557386411433294","163819","<p>I am running a multinomial logistic regression model (with 3 possible outcomes) in R. I am trying to find the best way to assess the predictive power/accuracy of the model, and the best thing I've come up with is using a ROC curve.</p>

<p>For multi-class ROC analysis, I know that there is the one vs. one comparison or the one vs. all comparison. For the one vs. one comparison, would I need three separate ROC curves for each possible combination of outcome comparisons? If so, do I need to make a third model for comparing the two outcomes that were initially being compared to the baseline outcome?</p>

<p>For one vs. all comparison is the threshold for a r ""random model"" now 33% instead of 50%?</p>

<p>And finally, is there a better way to go about doing this/visualizing it?</p>

<p>EDIT: I know the pROC package has a multi class.roc function, but I don't totally get what it does.</p>
"
"0.0657951694959769","0.0311588476424878","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.0657951694959769","0.0623176952849756","164110","<p>I am running a logistic regression on a data set containing Continuous, Ordinal, Categorical and Dichotomic variables.</p>

<p>I would like to know how to calculate the correlation for all possible combinations (see matrix below - cases marked with an X do not occur in my data set) in order to check for colinearity. I can do this either with SAS or R.</p>

<hr>

<pre><code>             Continuous    Ordinal    Categorical     Dichotomic
Continuous        X           1             2              3
Ordinal                       5             6              7
Categorical                                 8              9
Dichotomic                                                 X
</code></pre>

<p>Case 8 I use <code>proc freq data=data chisq ;</code> to return Cramer's V.</p>

<p>As for the rest I am unsure - is it possible for cases 3, 7 and 9 to consider a dichotomic variable as categorical in two classes in order to compute Cramer's V?</p>
"
"0.145643816250884","0.0886796350347864","164120","<p>I am attempting to conduct a logistic regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. My response variable (service points) is binary in the sense that it can have only two outcomes for each observation - a success (service point win) or a failure (service point loss). </p>

<p>I have an issue with my data: For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>250 matches, each with about 70 dependent variable observations (service points). So for each match I currently have the two variables: Total_Service_Points_Played <strong>and</strong> Total_Service_Points_Won. </p>

<p>Eg - Match 1: Total_Service_Points_Played: 70 ; Total_Service_Points_Won: 47</p>

<p>So my data isn't in 1's and 0's. Is there a way I can implement a logistic regression with my dependent variable observations in their current form? Is there any simple transformation that comes to mind?</p>

<p>What springs to mind for me is to flesh out my match data into 1's and 0's. So following on from Match 1 above I would have: 47 1's followed by 26 0's . My data doesn't provide information as to what sequence these 1's and 0's arrived in, but since the depdendent variable observations are i.i.d this won't cause an issue? Correct me if I'm wrong please. Another issue posed by this technique would be the massive increase in my data - from 250 observations as a ratio (service point wins/service points played) to 250*70=17500 observations or more.</p>

<p>As a side note, the last thing I'm wondering is about the dispersion of my dependent variable data. Specifically, in the ratio of serve wins to total serve points as above, there exists no values &lt; 0.2 or 20% .... In addition, there exists no value > 0.9 ..... Does this fit the bill for the (link=logit) argument? I know this relates to an S shape curve which is undefined at 0 and 1, but approaches both values.... I might be going off track here but is this something to be concerned about? </p>
"
"0.113960576459638","0.0539687072220866","164158","<p>In R and in binomial logistic regression to be specific, the modelling is based on which class amongst 0 and 1? And if it builds model based on 1 by default, is there a parameter or something in which we can pass 0 so that the model is built basing on class 0?</p>

<p>At least this is what I percieve, please correct me if I'm wrong.</p>

<p>I've heard that in SAS, modelling is done based on class 1 rather than class 0(Is it true?). So I was curious to know how it works in R.</p>
"
"0.1176979772673","0.0696733014291618","164186","<p>I have continuous variable with missing values. Missing values are of different types (indicated by special values such as 991, 992). How do I best encode my data for logistic regression? I can create separate variables for 991 and 992, but what I should use for the column with the rest of data? If I use NA, then R fails (using <code>na.pass</code>). If I use <code>na.exclude</code> then 991, 992 variables do not make any sense.</p>

<p>In this case, 991 represents missing value (not collected), and 992 represent not provided value (value was attempted to be collected, but response was not provided). I do not want to exclude rows with 991, 992 as these are valid inputs and I need to model response even for these rows. Also, in real-life scenario I have many such columns and removing all rows with special values would exclude vast majority of the rows.</p>

<pre><code>df &lt;- read.table(header= TRUE, text = '
x y
1 0
1 0
1 1
2 1
2 1
2 0
3 1
3 1
3 0
991 1
992 0
')

glm(y ~ x, df, family = binomial(), na.action = na.pass)
</code></pre>
"
"0.124341182825498","0.0824385620013739","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.131590338991954","0.0623176952849756","164648","<p>I have created a Logistic Regression using the following code:</p>

<pre><code>full.model.f = lm(Ft_45 ~ ., LOG_D)
base.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg)
step(base.model.f, scope=list(upper=full.model.f, lower=~1),
     direction=""forward"", trace=FALSE)
</code></pre>

<p>I have then used the output to create a final model:</p>

<pre><code>final.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg + IP_util_E2_m02_flg + 
                           AE_NumVisit1_flg + OP_NumVisit1_m01_flg + IP_TotLoS_m02 + 
                           Ft1_45 + IP_util_E1_m05_flg + IP_TotPrNonElecLoS_m02 + 
                           IP_util_E2pl_m03_flg + LTC_coding + OP_NumVisit0105_m03_flg +
                           OP_NumVisit11pl_m03_flg + AE_ArrAmb_m02_flg)
</code></pre>

<p>Then I have predicted the outcomes for a different set of data using the predict function:</p>

<pre><code>log.pred.f.v &lt;- predict(final.model.f, newdata=LOG_V)
</code></pre>

<p>I have been able to use establish a pleasing ROC curve and created a table to establish the sensitivity and specificity which gives me responses I would expect. </p>

<p>However What I am trying to do is establish for each row of data what the probability is of Ft_45 being 1. If I look at the output of log.pred.f.v I get, for example,:</p>

<pre><code>1 -0.171739593    
2 -0.049905948    
3 0.141146419    
4 0.11615669    
5 0.07342591    
6 0.093054334    
7 0.957164383    
8 0.098415639    
.
.
.
104 0.196368229    
105 1.045208447    
106 1.05499112
</code></pre>

<p>As I only have a tentative grasp on what I am doing I am struggling to understand how to interpret the negative and higher that 1 values as I would expect a probability to be between 0 and 1.</p>

<p>So my question is am I just missing a step where I need to transform the output or have I gone completely wrong.
Thank you in advance for any help you are able to offer.</p>
"
"0.0986927542439653","0.0623176952849756","164912","<p>I am modelling invertebrate.biomass ~ habitat.type * calendar.day + habitat.type * calendar.day ^ 2, with a random intercept of transect.id (50 transects were repeated 5 times)</p>

<p>My response is zero-heavy - about 25% are 0s - and the non-zeroes are strongly right-skewed. </p>

<p>I understand a possible way of dealing with this is to construct 2 models - one modelling a binary response in a logistic regression and the other modelling the non-zero response in a (e.g.) Gamma regression. I'm working in R and following the ideas in <a href=""http://seananderson.ca/2014/05/18/gamma-hurdle.html"" rel=""nofollow"">this post</a>.</p>

<p>I want to check the method of combining the results of these 2 models, in order to generate quantitative predictions (ultimately with CI). Am I correct in multiplying the predicted probabilities from the logistic regression with the predicted (non-zero) biomass from the Gamma regression? Thus, the predicted (non-zero) biomass gets down-weighted according to the probability of there actually being an invertebrate present at all. This makes sense in my head, but feels too easy to be true. </p>

<p>See plots below which demonstrate my method in it's current form.
<a href=""http://i.stack.imgur.com/MVmJc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MVmJc.png"" alt=""Gamma hurdle model""></a></p>

<p>Assuming I'm right so far, how would I then go about generating a SE / CI for the predictions combining two models? </p>
"
"0.124837556786472","0.0886796350347864","165214","<p>I am attempting to conduct a dynamic or time series regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. 
For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>Numerous matches, for each I have:</p>

<ul>
<li>No. of service points played </li>
<li>No. of service points Nadal won </li>
<li>(Thus) Nadal's Service point win %</li>
<li>The court surface the match was played on (independent variable)</li>
<li><p>Nadal, and his opponent's world ranking points at time of match </p>

<p>I will be using a model like this:</p></li>
</ul>

<p><em>Nadal serve win % = surface + (Nadal rank points - opponent rank points)</em> </p>

<p>However I would like to include an independent variable that accounts for ""form"" or ""hot hands"" in tennis. So I want to include an independent variable : </p>

<ul>
<li>Xi = Avg. Serve % of Last 5 matches </li>
</ul>

<p>i.e. a moving average of sorts </p>

<p>Is this a good idea? Does anyone have any suggestions how this could be implemented in R specifically? </p>

<p>Lastly, since my dependent variable data is binary, and can be used in a logistic regression in R.... Can I run a dynamic logistic regression/logistic time series for the model I discuss above? </p>

<p>Any advice on how I can account for this form/trend would be massively appreciated</p>
"
"0.127661380464626","0.105799828237357","166584","<p>I am conducting a regression in order to predict a tennis player's service point win % i.e. the percentage of points he wins when he is the server.
Model 1 If my DV data lies in the range 0.3-0.9, does it make sense to use a logistic regression? If using logistic I would endeavor to build a model with serve win % as my DV and my IV's as:</p>

<p>+average serve win % of last n matches (maybe n=5 or 10) to account for form </p>

<p>+surface </p>

<p>+player ranking </p>

<p>+opposition ranking</p>

<p>..... Would this be a good model to use? Preliminary logistic regressions just involving serve win % regressed on surface + player ranking + opponent ranking ... are showing some strange results so im losing faith in logistic for this data.</p>

<p>An alternative I'm considering is to use raw variables in a linear regression type model with interactions.... Along the lines of Aiken &amp; West 1991
My dependent variable will be number of service points won in match, and my independent variables will be:</p>

<p>+no. service points played in match +the surface the match played on </p>

<p>+the player's ranking points +the opponents ranking points</p>

<p>+an interaction between player and opponent ranking points </p>

<p>+an interaction between surface and no. points played </p>

<p>+average service points won in last n matches</p>

<p>+average % of service points won in last m matches</p>

<p>Do either of these models stand out as smart or appropriate ways to model this data? For context, for each player I have between 100-350 matches worth of data. I would love to hear what you guys think, or if you have any other suggestions on how to predict serve win % using the stated variables I would really appreciate it. I'm conducting this analysis in R so any code/package suggestions would also be great </p>
"
"0.119027940128723","0.0939474604818017","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.1359059177167","0.104587338272187","166913","<p>I am conducting a logistic regression in order to predict the service point win percentage of a tennis player.</p>

<p>In terms of data - I have (for each player A) approx 300 matches - for each match I have the total number of player A service points (points where he is the server), total number of player A service point wins and total number of player A service point losses. </p>

<p>To do so, I have service point win percentage as the DV, and my independent variables are:</p>

<p>+Average service win percentage of last 3 matches<br>
+ln(player's ranking points)<br>
+ln(opposition's ranking points)<br>
+surface the match was played on  </p>

<p>My dependent variable data, service win percentage, lies usually in the range of 0.4-0.8, there are pretty much no values greater that 0.8 (about 2.8% of values and this drops to &lt; 1% at around 0.84) and there exists no values less than 0.22. In addition my data is much more concentrated above 0.5 than it is below 0.5. </p>

<p>Thus, I worry that since my data doesn't have points close to zero or 1, and is not symmetrical around 0.5 (like the sigmoidal curve of logistic regression) that I am wasting my time with this model type. The results it is giving for my preliminary model outlined above are, although not shocking,  pretty volatile.</p>

<p>I am conducting this in R and using the <code>weights</code> command to allow me use a proportion in the DV, giving the total number of trials as the weights. I use ln(points) because ranking points are exponential in nature. </p>

<p>The goal is to predict / forecast the service point win percentage of the player based on the IV's. Considering my data distribution, and my goal, does logistic regression make sense? If not is there any other type of model that makes more sense? </p>
"
"0.153522062157279","0.0830902603799674","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.0882734829504749","0.0557386411433294","167324","<p>I'm trying to obtain the variance-covariance matrix of a logistic regression:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
mylogit &lt;- glm(admit ~ gre + gpa, data = mydata, family = ""binomial"")
</code></pre>

<p>through matrix computation. I have been following the example published <a href=""http://www.ats.ucla.edu/stat/r/library/matrix_alg.htm"" rel=""nofollow"">here</a> for the basic linear regression</p>

<pre><code>X &lt;- as.matrix(cbind(1, mydata[,c('gre','gpa')]))
beta.hat &lt;- as.matrix(coef(mylogit))
Y &lt;- as.matrix(mydata$admit)
y.hat &lt;- X %*% beta.hat

n &lt;- nrow(X)
p &lt;- ncol(X)

sigma2 &lt;- sum((Y - y.hat)^2)/(n - p)        
v &lt;- solve(t(X) %*% X) * sigma2
</code></pre>

<p>But then my var/cov matrix doesn't not equals the matrix computed with <code>vcov()</code></p>

<pre><code>v == vcov(mylogit)

1   gre   gpa
1   FALSE FALSE FALSE
gre FALSE FALSE FALSE
gpa FALSE FALSE FALSE
</code></pre>

<p>Did I miss some log transformation?</p>
"
"0.0986927542439653","0.0623176952849756","167389","<p>I have a database with more than 500 samples with 22 quantitative features each and I would like to predict a categorical variable (0 or 1).
I am trying to fit a logistic regression model and a neural network in R using glm and the neural net package.</p>

<p>By selecting different features I noted that I get different results in terms of ability of my two models to predict the test set (another 500+ samples database) a +-10% in accuracy depending on how many and what features are used (using the same test set).
Essentially the models seems to get worse using more than 12 features on average but of course I cannot try all the combinations and I'd rather not use random features as a blind guess. Also the neural network seems a little bit picky on features and does not always converge (however this is a minor issue).</p>

<p>What tests could I run in R to select the most ""explicative"" features?</p>

<p>(By the way, is ""explicative"" the right terminology?)</p>
"
"0.1176979772673","0.0696733014291618","167440","<p>I am trying to get the bootstrapped confidence intervals of the coefficients for an ordinal logistic regression. 
Here below, my R code on fake data (reproducible example here below). This one does not work properly.</p>

<p>I suppose I need to enter a list of data with one line for each of the 20 subjects (this is the most simple way to proceed). Then the bootstrap with randomly select 20 rows (using sampling with replacement) to generate a new data set with 20 rows. That data set is converted into a new table of counts and a coefficient value is computed from that new â€œbootstrappedâ€ table.  This is repeated for each bootstrap sample. I can't get it! Thanks for your help.</p>

<pre><code>####################
library(rms)
x=c(1,2,3,2,3,1,2,3,3,3,2,2,1,2,1,2,3,2,1,2)
y=c(""math"",""eco"",""eco"",""lit"",""lit"",""eco"",""eco"",""math"",""math"",""lit"",""lit"",""math"",""eco"",""eco"",""math"",""lit"",""lit"",""math"",""eco"",""math"")
Dataset&lt;-data.frame(x,y)
h &lt;- orm(x ~ y)
h

# calculate coefficients using bootstrap
library(boot)
logit.bootstrap &lt;- function(data, indices) {
d&lt;-data[indices,]
fit&lt;-orm(x ~ y, data=data[indices,])
return(coefficients(fit))
}

# bootstrapping with 1000 replications
logit.boot &lt;- boot(data=Dataset, statistic=logit.bootstrap,R=1000)

# view results
logit.boot
plot(logit.boot)

# get 95% confidence interval
boot.ci(logit.boot, type=""all"")
############################
</code></pre>
"
"0.113960576459638","0.0359791381480577","168335","<p>I'm trying to play around with classification models and started off with logistic regression in R. When I have all the numeric variables in the data set the model works correctly and I was able to interprer the results.</p>

<p>The question is, with another data set I have more than 5 variables which are categorical but they are important though. </p>

<p>1) How do I deal with the variables with categorical values? the total unique categorical values are more than 100 for almost 5 predictors.</p>

<p>2) My dependent variable is having 3 classes, is it okay to using logistic regression for this purpose? (I would still need to explore other classification techniques but right now I'm exploring logistic regression.</p>
"
"0.0986927542439653","0.0623176952849756","168474","<p>I am doing forward stepwise logistic regression. I have heard that its common for a previously statistically significant variable to become not statistically significant when 1 or more variables is introduced into the model. However, I have never heard of the opposite. This is my case right now. The first variable selected was <code>x1</code> (by lowest p-value), the second variable selected was <code>x2</code>. What's interesting is that <code>x2</code> is not statistically significant when it is modeled with <code>Y</code> alone. I am using R to do this though, I think this more of statistics question.</p>
"
"0.241511153520921","0.128669938172323","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.05884898863365","0.0696733014291618","169291","<p>I have a logistic regression model below, predicting a dichotomous variable <em>type</em> from a single continuous predictor <em>fatigue</em>. Using the coefficients below I can obtain the increase in the odds of a positive <em>type</em> from a 1 unit increase in fatigue.</p>

<p>Also I believe by forming the model expression</p>

<pre><code>logit(type) = 0.3134 - 91.1171 * fatigue 
</code></pre>

<p>I can obtain the odds of a positive <em>type</em> for a given value of <em>fatigue</em> by plugging it in, say for a value <em>fatigue</em> = 1.</p>

<p><strong>However</strong>, what I want to do is to obtain the odds of a positive <em>type</em> for a range of <em>fatigue</em> values, i.e. <strong>&lt;= 0</strong>. Is this possible?</p>

<pre><code>## Call:
## glm(formula = type ~ fatigue, family = binomial(), data = myData)

## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6703 -1.3104 0.8369 1.0049 1.4695
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.3134 0.1496 2.095 0.0362 *
## fatigue -91.1171 36.3785 -2.505 0.0123 *
## ---
## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 282.84 on 210 degrees of freedom
## Residual deviance: 276.03 on 209 degrees of freedom
## AIC: 280.03
##
## Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.153522062157279","0.0830902603799674","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.193429485824666","0.116585732444878","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.113960576459638","0.0539687072220866","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.180936716113936","0.116845678659329","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.0930484210398471","0.0440652649239232","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.162834736819732","0.0881305298478463","173076","<p>I was following the procedure in a statistics textbook to run a multinomial logistic regresion using <code>mlogit</code>. However, the Odds Ratios calculated seemed too high for some of the variables (>1000). Can someone take a look at this and check wether I am doing everything correctly? The data can be downloaded from <a href=""https://dl.dropboxusercontent.com/u/14303378/LogisticRegressionSample.csv"" rel=""nofollow"">here</a>. I prepared the data with the following commands:</p>

<pre><code>#read in the data
test&lt;-read.csv(file=""LogisticRegressionSample.csv"",sep="","")
#trasnform data into the correct form for mlogit
mlogitData&lt;-mlogit.data(test,choice=""Outcome"",shape=""wide"")
#build model
MLogitFit&lt;-mlogit(Outcome~1|V1+V2+V3+V4+V5+V6+V7+V8,reflevel=3,data=mlogitData)
#summary of the model
summary(MLogitFit)
#OddsRatios
data.frame(exp(MLogitFit$coefficients))
# confidence Interval of the odds Ratios
exp(confint(MLogitFit))
</code></pre>

<p>The summary of mlogit gives me:</p>

<pre><code>    Call:
mlogit(formula = Outcome ~ 1 | V1 + V2 + V3 + V4 + V5 + V6 + 
    V7 + V8, data = mlogitData, reflevel = 3, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
      Z       A       B 
0.43333 0.25556 0.31111 

nr method
7 iterations, 0h:0m:0s 
g'(-H)^-1g = 1.56E-06 
successive function values within tolerance limits 

Coefficients :
               Estimate Std. Error t-value  Pr(&gt;|t|)    
A:(intercept)  -6.74640    5.97451 -1.1292 0.2588147    
B:(intercept)  -7.12401    4.50350 -1.5819 0.1136759    
A:V1            3.65979    3.90808  0.9365 0.3490331    
B:V1            4.24363    3.25687  1.3030 0.1925822    
A:V2          -15.11554    6.92901 -2.1815 0.0291475 *  
B:V2           -4.88778    3.65249 -1.3382 0.1808302    
A:V3            1.71465    6.57907  0.2606 0.7943839    
B:V3            2.94335    3.96557  0.7422 0.4579497    
A:V4           -1.70660    1.58849 -1.0744 0.2826633    
B:V4           -1.67210    1.17575 -1.4222 0.1549820    
A:V5            1.18494    1.60760  0.7371 0.4610682    
B:V5            1.03084    1.25573  0.8209 0.4116971    
A:V6            8.28902    2.51631  3.2941 0.0009873 ***
B:V6            3.44578    1.91844  1.7961 0.0724727 .  
A:V7           -1.34395    2.67943 -0.5016 0.6159612    
B:V7            1.04803    1.95147  0.5370 0.5912343    
A:V8           -7.46263    4.12978 -1.8070 0.0707577 .  
B:V8            0.21861    2.13596  0.1023 0.9184810    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -64.636
McFadden R^2:  0.33149 
Likelihood ratio test : chisq = 64.1 (p.value = 1.0515e-07)
</code></pre>

<p>Running <code>data.frame(exp(MLogitFit$coefficients))</code> to calculate the odds ratios gives:</p>

<pre><code>              exp.MLogitFit.coefficients.
A:(intercept)                1.175103e-03
B:(intercept)                8.055280e-04
A:V1                         3.885310e+01
B:V1                         6.966040e+01
A:V2                         2.725226e-07
B:V2                         7.538147e-03
A:V3                         5.554743e+00
B:V3                         1.897938e+01
A:V4                         1.814819e-01
B:V4                         1.878524e-01
A:V5                         3.270504e+00
B:V5                         2.803423e+00
A:V6                         3.979917e+03
B:V6                         3.136764e+01
A:V7                         2.608125e-01
B:V7                         2.852036e+00
A:V8                         5.741439e-04
B:V8                         1.244345e+00
</code></pre>

<p>I obtained the confidence interavls with: <code>exp(confint(MLogitFit))</code>:</p>

<pre><code>                     2.5 %       97.5 %
A:(intercept) 9.650816e-09 1.430830e+02
B:(intercept) 1.182216e-07 5.488637e+00
A:V1          1.831725e-02 8.241213e+04
B:V1          1.176881e-01 4.123248e+04
A:V2          3.446800e-13 2.154711e-01
B:V2          5.864847e-06 9.688857e+00
A:V3          1.394913e-05 2.211978e+06
B:V3          7.994348e-03 4.505896e+04
A:V4          8.066986e-03 4.082774e+00
B:V4          1.875058e-02 1.881996e+00
A:V5          1.400307e-01 7.638467e+01
B:V5          2.392271e-01 3.285238e+01
A:V6          2.870699e+01 5.517731e+05
B:V6          7.303065e-01 1.347282e+03
A:V7          1.366460e-03 4.978060e+01
B:V7          6.223884e-02 1.306918e+02
A:V8          1.752860e-07 1.880591e+00
B:V8          1.891518e-02 8.185990e+01
</code></pre>

<p>The predicted Probabilities are as following:</p>

<pre><code>fitted(MLogitFit, outcome=FALSE)
                 Z            A          B
 [1,] 0.2790108926 3.880184e-01 0.33297074
 [2,] 0.5191458618 2.900625e-01 0.19079169
 [3,] 0.7263001933 1.633014e-02 0.25736966
 [4,] 0.8386056883 3.700203e-03 0.15769411
 [5,] 0.8050365007 7.487290e-03 0.18747621
 [6,] 0.7855655154 3.860347e-02 0.17583101
 [7,] 0.7878404896 7.992930e-03 0.20416658
 [8,] 0.8386056883 3.700203e-03 0.15769411
 [9,] 0.7878404896 7.992930e-03 0.20416658
[10,] 0.4363708036 2.827104e-01 0.28091885
[11,] 0.6126060746 3.320075e-02 0.35419317
[12,] 0.0274357267 8.418204e-01 0.13074390
[13,] 0.1438998597 5.869087e-01 0.26919146
[14,] 0.1850027820 2.105586e-01 0.60443858
[15,] 0.8427092407 5.933393e-03 0.15135737
[16,] 0.1537160539 4.929905e-01 0.35329341
[17,] 0.0434283140 6.358897e-01 0.32068201
[18,] 0.1868202029 1.141679e-01 0.69901186
[19,] 0.3064594418 1.156597e-01 0.57788084
[20,] 0.5737141160 6.734724e-02 0.35893865
[21,] 0.5841338911 1.374758e-01 0.27839031
[22,] 0.0866451414 4.019366e-01 0.51141821
[23,] 0.2794060013 9.964607e-02 0.62094793
[24,] 0.0252343516 7.343045e-01 0.24046118
[25,] 0.1314775919 4.602643e-01 0.40825811
[26,] 0.0274357267 8.418204e-01 0.13074390
[27,] 0.1303195991 6.649645e-01 0.20471586
[28,] 0.2818251202 4.896734e-01 0.22850146
[29,] 0.0063990341 8.874618e-01 0.10613917
[30,] 0.0002408527 9.742025e-01 0.02555668
[31,] 0.0523052465 7.073015e-01 0.24039322
[32,] 0.3287956423 2.756959e-01 0.39550841
[33,] 0.0419093705 7.521689e-01 0.20592173
[34,] 0.0523052465 7.073015e-01 0.24039322
[35,] 0.3287956423 2.756959e-01 0.39550841
[36,] 0.0100998700 7.475180e-01 0.24238212
[37,] 0.1609808596 2.268570e-01 0.61216212
[38,] 0.0119603037 8.065964e-01 0.18144331
[39,] 0.0697132279 4.549378e-01 0.47534896
[40,] 0.5756435353 6.315652e-02 0.36119994
[41,] 0.4689676672 6.796615e-02 0.46306619
[42,] 0.2652679745 6.358962e-02 0.67114240
[43,] 0.7870195702 2.038999e-03 0.21094143
[44,] 0.6438437943 9.222002e-03 0.34693420
[45,] 0.7462282258 5.881047e-04 0.25318367
[46,] 0.3532662528 2.193975e-01 0.42733620
[47,] 0.9563852795 4.133754e-05 0.04357338
[48,] 0.9079031419 2.786314e-03 0.08931054
[49,] 0.0220230156 8.017508e-01 0.17622619
[50,] 0.2268852285 1.745210e-01 0.59859376
[51,] 0.2268852285 1.745210e-01 0.59859376
[52,] 0.0751929214 6.261548e-01 0.29865225
[53,] 0.9426667411 4.520877e-06 0.05732874
[54,] 0.0212631471 6.729961e-01 0.30574075
[55,] 0.0212631471 6.729961e-01 0.30574075
[56,] 0.9218535421 1.166953e-02 0.06647693
[57,] 0.6374868816 3.856300e-02 0.32395012
[58,] 0.2920703240 2.410709e-01 0.46685876
[59,] 0.7047942848 1.728601e-02 0.27791970
[60,] 0.1850395244 5.297673e-01 0.28519316
[61,] 0.4402296785 8.870861e-03 0.55089946
[62,] 0.6781988218 3.852569e-04 0.32141592
[63,] 0.9889453179 4.036588e-05 0.01101432
[64,] 0.1618635354 8.011851e-02 0.75801796
[65,] 0.3008372801 9.835522e-02 0.60080750
[66,] 0.0740319347 4.284039e-01 0.49756417
[67,] 0.5529727485 1.768537e-01 0.27017351
[68,] 0.7824740564 5.001713e-03 0.21252423
[69,] 0.5343045050 5.865850e-02 0.40703700
[70,] 0.4564647083 1.733995e-01 0.37013579
[71,] 0.4711837972 8.449081e-03 0.52036712
[72,] 0.9154349308 2.364316e-02 0.06092191
[73,] 0.1858643216 2.217595e-01 0.59237621
[74,] 0.3770813535 9.943397e-02 0.52348468
[75,] 0.8124141650 3.243679e-04 0.18726147
[76,] 0.3195206223 2.932236e-01 0.38725578
[77,] 0.8615871019 5.063299e-04 0.13790657
[78,] 0.8615871019 5.063299e-04 0.13790657
[79,] 0.8254986241 2.059378e-03 0.17244200
[80,] 0.1208591778 4.615235e-01 0.41761730
[81,] 0.0035765650 9.093754e-01 0.08704806
[82,] 0.7583239965 3.544345e-02 0.20623255
[83,] 0.8141948591 5.016280e-03 0.18078886
[84,] 0.1204323818 2.545405e-01 0.62502710
[85,] 0.9594950290 3.694056e-05 0.04046803
[86,] 0.6858228916 1.691396e-01 0.14503752
[87,] 0.8254986241 2.059378e-03 0.17244200
[88,] 0.8254986241 2.059378e-03 0.17244200
[89,] 0.2463233530 2.793410e-01 0.47433568
[90,] 0.5674338104 1.448538e-02 0.41808081
</code></pre>

<p>To assess multicolinearity I calculated the VIF statistic but using the a glm model of the same dataset.</p>

<pre><code>fullmod&lt;-glm(as.factor(Outcome)~.,data=test,family=binomial())
vif(fullmod)
      V1       V2       V3       V4       V5       V6       V7       V8 
1.789116 1.822252 2.216444 1.320244 1.821820 1.439183 1.512865 1.121805 
</code></pre>
"
"0.0657951694959769","0.0311588476424878","173568","<p>In helping us understand how to fit a logistic regression in <code>R</code>, we are told to first replace 0 and 1 in the response variable by 0.05 and 0.95, respectively and second to take the logit transform of the resulting response variable. Last we fit these data using iterative re-weighted least squares method. </p>

<p>Then we are asked to use 0.005 and 0.995 instead of 0.05 and 0.95. Then the resulting coefficients are quite <strong>different</strong>.</p>

<p>My question is in <code>glm</code> function, how are 0 and 1 dealt with? Are they replaced by some numbers as above? What numbers are used by default and why are they used? How sensitive is the choice of these numbers?</p>
"
"0","0.0539687072220866","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.0759737176397586","0.0539687072220866","174396","<p>Hi I'm doing some logistic regression, currently using <code>glmnet</code> package in R.</p>

<p><code>glmnet</code> provides a few measure metrics for cross validation. For classification, we can use type.measure = 'auc' (area under ROC curve) or type.measure = 'class' (misclassification rate).</p>

<p>I'm working with some insurance data with a low rate of positive examples (y = 0 for 95% data, and = 1 for 5% of them. OR 95% people did't buy insurance). </p>

<pre><code>library(ISLR)
data(Caravan)
y = dta$Purchase
x = as.matrix( dta[ , -which(colnames(dta)=='Purchase') ] )
</code></pre>

<p>And I want to predict who are likely to buy insurance.</p>

<p>I think I need to use F1 score or some custom metrics. For example, if I can make 200 by selling an insurance, and the cost to contact one person is 20, then I want to maximize </p>

<p>metric = 200 * N(true positive) - 20 * N(predicted positive)</p>

<p>Is there a way I can do this with glmnet or something else more suitable? Or is <code>AUC</code> a similar metric as F1 score? Any help or discussion's appreciated. Thanks</p>
"
"0.0537215309350254","0.0763232776972177","174518","<p>consider the following data set:</p>

<pre><code>a &lt;- c(1, 2, 3, 1, 4, 1968, 2, 1)
b &lt;- c(2, 1, 2, 4, 3, 1984, 2, 0)
c &lt;- c(3, 3, 4, 2, 1, 1945, 1, 0)
d &lt;- c(4, 1, 4, 3, 2, 1975, 3, 1)
df &lt;- data.frame(rbind(a,b,c,d))
names(df) &lt;- c(""ID"", ""OptionW"", ""OptionX"", ""OptionY"", ""OptionZ"", ""yearofBirth"", ""education"", ""sex"")


ID OptionW OptionX OptionY OptionZ yearofBirth education sex
1       2       3       1       4        1968         2   1
2       1       2       4       3        1984         2   0
3       3       4       2       1        1945         1   0
4       1       4       3       2        1975         3   1
</code></pre>

<p>Two hundred people where asked to rank Options W to Z from 1 to 4 in their effectiveness to lower crime rates in their community. Their age, highest academic degree and sex are annotated as well.
I want to find out:</p>

<ul>
<li>which options are preferred by the majority of citizens?</li>
<li>are there significant differences in what men or women, old or young, well or less well educated citizens believe?</li>
<li>how likely is the ranking order going to change if the person is older/younger, has had more or less formal education and is male or female? </li>
</ul>

<p>I read that a multinomial logistic regression might be the way to go, but I find it hard to adapt the examples I find to my data set. Often they allow for only one option to be chosen, making each choice (W, X Y Z) a level of one variable (Options). But in my case I have several variables (OptionW, OptionX, OptionY, OptionZ) where the ranking placement appears to be the level (1,2,3,..10). Or am I looking at it the wrong way?</p>

<p>Which function from what package would be suitable? And are there other methods available apart from mlr?</p>

<p>I use R mostly for spatial analysis and am not very fluent in statistics. Hopefully you can help me here.</p>
"
"0.0657951694959769","0.0311588476424878","174989","<p>Please bear with me, I am very new to R.</p>

<p>My question is regarding the use of the <code>improveProb</code> function in the <code>Hmisc</code> package. I have two logistic models, the only difference being that the second model contains my novel marker of interest. I am trying to calculate NRI and IDI.</p>

<p>I have the PredRisks for both models - PredRisk1 and PredRisk2, and my outcome is disease 0/1. How do I define this in R in order to run</p>

<p><code>improveProb(x1, x2, y)</code>?</p>

<hr>

<p>The data are the same for both models. We are looking at ways to validate our findings. We have performed k-fold cross-validation (MSE=0.08) and bootstrapping with optimism (AUC original = 0.826 After correction =0.791) to check for overfitting. Is this appropriate? The LRT was significant for both logistic regression models, but I need to check this. Also, the AIC for model 2 is lower than model 1. Thanks again for your expert knowledge :)</p>
"
"0.131590338991954","0.0623176952849756","175203","<p>I'm reading <a href=""https://onlinecourses.science.psu.edu/stat504/node/177"" rel=""nofollow"">this tutorial</a> to understand how to interpret the coefficients of an ordinal logistic regression which assumes the proportional odds. </p>

<p>They use a dataset about a cheese tasting experiment. Subjects were randomly assigned to taste one of four different cheeses (A,B,C,D). Response categories are 1 = strong dislike to 9 = excellent taste.</p>

<pre><code>m1=polr(response~cheese,weights=N,data=dati)
summary(m1)

Re-fitting to get Hessian

Call:
polr(formula = response ~ cheese, data = dati, weights = N)

Coefficients:
         Value Std. Error t value
cheeseB -3.352     0.4287  -7.819
cheeseC -1.710     0.3715  -4.603
cheeseD  1.613     0.3805   4.238

Intercepts:
    Value    Std. Error t value 
1|2  -5.4674   0.5236   -10.4413
2|3  -4.4122   0.4278   -10.3148
3|4  -3.3126   0.3700    -8.9522
4|5  -2.2440   0.3267    -6.8680
5|6  -0.9078   0.2833    -3.2037
6|7   0.0443   0.2646     0.1673
7|8   1.5459   0.3017     5.1244
8|9   3.1058   0.4057     7.6547

Residual Deviance: 711.3479 
AIC: 733.3479 
</code></pre>

<p>The tutorial's author writes:</p>

<blockquote>
  <p>we see that the implied ordering of cheeses in terms of quality is D >
  A > C > B. Furthermore, D is significantly better preferred than A,
  but A is not significantly better than C.</p>
</blockquote>

<p>Is this correct? 
I do agree that cheese B and C are significantly worse than A, and that D is significantly better than A, but I don't understand why cheese A should not be significantly better than C, as the author claims.</p>

<p>This are instead my conclusions:<br>
Since $\beta_B \neq 0$ and $\beta_B &lt; 0$, then $B&lt;A$.<br>
Since $\beta_C \neq 0$ and $\beta_C &lt; 0$, then $C&lt;A$.<br>
Since $\beta_D \neq 0$ and $\beta_D &gt; 0$, then $D&gt;A$.<br>
So, $D&gt;A&gt;B$ and $D&gt;A&gt;B$.
But since $\beta_B &lt; \beta_C$, then $D&gt;A&gt;B&gt;C$.<br>
So, I would say instead that cheese A is significantly better than C.</p>
"
"NaN","NaN","175216","<p>I have a binary dependent variable (<code>R</code>) and four numeric independent variables (<code>Q, M, S, T</code>) and want to examine coefficients for them.</p>

<p>Here is my glm code in R:</p>

<pre><code>fit = glm(R ~ Q + M + S + T, data=data, family=binomial())
</code></pre>

<p>When I run <code>predict(fit)</code>, I get a lot of predicted values greater than one (but none below 0 so far as I can tell). I have tried bayesglm and glmnet per suggestions to similar questions but both are a little over my head and the output I did get didn't seem to fix my problem.</p>

<p>I want to know:
A) Is this typical of logistic regression?
B) If not, how do I fix it?</p>
"
"0.0657951694959769","0.0311588476424878","175312","<p>I'm using the model ReLogit from the package Zelig in R. ReLogit is a logistic regression for rare events data.
After having estimated the model on the training set, I want to calculate the AUC (Area under the ROC Curve) of this model in the test set. 
How can I do this with this package?</p>
"
"0.0930484210398471","0.0440652649239232","175325","<p>I am trying to determine which of several independent variables {A,B,C} best predicts a subject's response R, which can be one of 3 choices, {high, medium, low}.</p>

<p>As the dependent variable is multinomial i.e. not binomial, a binomial logistic regression is not suitable. However, using the lme4 package to fit a glm with <code>family = 'binomial'</code> to the data runs without error or warning, and correctly identifies that independent variable B is the best predictor.</p>

<p>Is this a lucky coincidence, or is the call to <code>glm()</code> automatically fitting multiple binomial regressions to the data (one for each binary combination), or is it something else entirely? Any comments are welcome.</p>
"
"0.161164592805076","0.0636027314143481","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.124341182825498","0.0824385620013739","175682","<p>I'm new to using R. I'm attempting to create a microsimulation of individuals health through time.  To do this, I have two survey datasets with the same variables.  First, a large base file, second a smaller but more detailed health transition dataset.  The outcome variable is self-reported health with three states (1 - good, 2 - fair, 3 - poor), the predictors - Age (continuous), health at time t-1, marital status, highest educational qualification, housing tenure and socio-economic social group.</p>

<p>I have conducted a multinomial logistic regression (test) on the second dataset and now wish to use the predict function to apply this to the larger, fist base dataset. In an ideal world, this will be in the form of predicted category probabilities, that I can then generate random numbers (0,1), and assign new health states. </p>

<p>Currently the best I can come up with is:</p>

<pre><code>test &lt;- multinom(health5 ~ ContAge1 + health4 + marstat1 + highqual1 + tenure1 + socstat1, data = EW5FDR)

newpred &lt;- predict(test, newdata = base, type = ""c"")
</code></pre>

<p>This appears to give me predicted outcome category for the new dataset, my question is: how would I change this to give me predicted category probabilities?</p>

<p>And indeed, is this the correct function to be using in the first place?</p>
"
"0.158260488401999","0.116585732444878","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0379868588198793","0.0359791381480577","175782","<p>I want to predict the impact of oil price over a Colombian oil company's stock price. I plan to use a logistic regression for this with a categorical variable (Up or Down given the direction of the stock price). Here is part of my dataset:</p>

<pre><code>Minute  ecopet  profit  sum_profit   direccion  cl1_chg   sum_cl1    direccion_cl1
571     2160     0       10           Up         -0.03     0.00      Down
572     2160     0        0           Neutral     0.07    -0.03      Down
573     2160     0       -5           Down       -0.08     0.04      Up
574     2160     0       -5           Down       -0.07    -0.04      Down
575     2160     5       -5           Down       -0.08    -0.11      Down
576     2165     0       -25          Down        0.00    -0.19      Down
577     2165     0       -25          Down       -0.05    -0.19      Down
578     2165     0       -15          Down       -0.17    -0.24      Down
579     2165     5       -15          Down       -0.06    -0.41      Down
580     2170     0       -20          Down        0.03    -0.47      Down
581     2170    -10       0           Neutral     0.04    -0.44      Down
</code></pre>

<p>My dependent variable is 'direccion'. But as you can see it has 3 response classes. I know that to implement a binary logistic regression in R the code is:</p>

<pre><code>glm.fit=glm(direccion~direccion_cl1, data=datos, family=binomial)
</code></pre>

<p>I am working with intraday information and plan to predict what happens when the oil moves up/ down (in the previous 10 minutes) and how it impacts the stock price in the next 10 minutes.</p>

<p>Could anyone tell me how could I perform this? I don't really know how to perform the logistic regression with 3 response classes. Thanks!</p>
"
"0.0759737176397586","0.0539687072220866","175956","<p>For a (fictional) <strong>multiple logistic regression</strong>, let's consinder a DV 'hired' (0,1) and <strong>three dichotomous IVs</strong> 'college_degree' (0,1), 'affluent' (0,1) and 'recommendated' (0,1) for <em>N</em> = 1,000 participants.</p>

<p>Running a logistic regression and generating predicted probabilities of being hired using the <code>predict</code> function for a <code>glm</code> object works well. For every respondent I have a probability value ranging continuously from 0 to 1.</p>

<p>Since I do have a base distribution of all three IVs and the DV, I want a kind of simulator that predicts the <strong>percentage/proportion</strong> of the DV using each indivduals predicted probability.</p>

<p>Let's say in the sample 20% are hired, 50% have a college degree, 10% are affluent and 35% are recommendated. I want to use the predicted values to see how much would the <strong>proportion of 'hired' goes</strong> up, when I, e.g., <strong>change the proportion of recommendations to 50%</strong>. I guess, I could also use the equation with the coefficients of the logit model, but would need to run it for every individual.</p>

<p>Is there any way to implement this in R (well or Excel, if that is easier)?</p>
"
"0.201604912253002","0.124851583951116","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0328975847479884","0.0623176952849756","176671","<p>During the first half of 2015 I did the <a href=""https://www.coursera.org/learn/machine-learning"">coursera course of Machine Learning</a> (by Andrew Ng, GREAT course). And learned the basics of machine learning (linear regression, logistic regression, SVM, Neuronal Networks...)</p>

<p>Also I have been a developer for 10 years, so learning a new programming language would not be a problem.</p>

<p>Lately, I have started learning R in order to implement machine learning algorithms.</p>

<p>However I have realized that if I want to keep learning I will need a more formal knowledge of statistics, currently I have a non-formal knowledge of it, but so limited that, for example, I could not properly determine which of several linear models would be better (normally I tend to use R-square for it, but apparently that is not a very good idea). </p>

<p>So to me it seems pretty obvious that I need to learn the basics of statistics (I studied that in uni but forgot most of it), where should I learn, please note that I don't really need a fully comprehensive course, just something that within a month allows me to know enough so I can get eager and learn more :).</p>

<p>So far I have read about ""<a href=""https://books.google.co.in/books/about/Statistics_Without_Tears.html?id=WyB1QgAACAAJ&amp;redir_esc=y"">Statistics without tears</a>"", any other suggestion?</p>
"
"0.0465242105199235","0.0440652649239232","177219","<p>When I run a <code>glm</code> with binomial-family (logistic regression), R output gives me the logit-estimates, which can be transformed into probabilities using <code>plogis(logit)</code>. So using something like <code>plogis(predict(glm_fit, type = ""terms""))</code> would give me the adjusted probabilities of success for each predictor.</p>

<p>But what would be the equivalent for Poisson regression? How can I ""predict"" the adjusted incidents rates for each predictor?</p>

<p>Given this example:</p>

<pre><code>set.seed(123)
dat &lt;- data.frame(y = rpois(100, 1.5),
                  x1 = round(runif(n = 100, 30, 70)),
                  x2 = rbinom(100, size = 1, prob = .8),
                  x3 = round(abs(rnorm(n = 100, 10, 5))))

fit &lt;- glm(y ~ x1 + x2 + x3, family = poisson(), data = dat)
</code></pre>

<p>and using <code>predict.glm(fit, type = ""terms"")</code></p>

<p>I get:</p>

<pre><code>         x1          x2          x3
1 -0.023487964  0.04701003  0.02563723
2  0.052058119 -0.20041119  0.02563723
3  0.003983339  0.04701003  0.01255701
4 -0.119637524  0.04701003 -0.03322376
5  0.010851165  0.04701003 -0.00706332
6 -0.105901873 -0.20041119 -0.00706332
...
attr(,""constant"")
[1] 0.3786072
</code></pre>

<p>So, how many ""incidents"" (y-value) would I expect for each value of <code>x1</code>, holding <code>x2</code> and <code>x3</code> constant (what <code>predict</code> does, afaik)?</p>

<p><em>I'm not sure whether this question fits better into Stackoverflow or Cross Validated - please excuse if posting here was wrong!</em></p>
"
"0.131590338991954","0.0623176952849756","177650","<p>I have a binary response variable and a categorical predictor variable. If I test for associations between the 2 variables using chi-square test , it turns out to be significant. However, if I do a logistic regression with the same set of variables, the predictor is not significant. Why does this happen?</p>

<pre><code>  table(Data1$pred,Data1$target)

                            0    1
  Level1                    1    0
  Level2                    4    0
  Level3                   98    1
  Level4                 2056   22
  Level5                    1    0
  Level6                    2    0
  Level7                  311    0
  Level8                    6    1
  Level9                  131    7
  Level10                  49    2

  chisq.test(table(Data1$pred,Data1$target))

  Pearson's Chi-squared test

  data:  tabletable(Data1$pred,Data1$target)
  X-squared = 34.2614, df = 9, p-value = 8.037e-05
</code></pre>

<p>Logistic Regression on the same</p>

<pre><code>  logit.glm &lt;- glm(as.factor(target) ~ pred,                  
               data=Data1, family=binomial(link=""logit"")
  summary(logit.glm)
  Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5553  -0.1459  -0.1459  -0.1459   3.0315  

  Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)
  (Intercept)   -2.057e+01  1.773e+04  -0.001    0.999
  Data1Level2   -6.313e-06  1.982e+04   0.000    1.000
  Data1Level3    1.598e+01  1.773e+04   0.001    0.999
  Data1Level4    1.603e+01  1.773e+04   0.001    0.999
  Data1Level5   -6.312e-06  2.507e+04   0.000    1.000
  Data1Level6   -6.312e-06  2.172e+04   0.000    1.000
  Data1Level7   -6.312e-06  1.776e+04   0.000    1.000
  Data1Level8    1.877e+01  1.773e+04   0.001    0.999
  Data1Level9    1.764e+01  1.773e+04   0.001    0.999
  Data1Level10   1.737e+01  1.773e+04   0.001    0.999

  (Dispersion parameter for binomial family taken to be 1)

   Null deviance: 356.09  on 2691  degrees of freedom
   Residual deviance: 333.06  on 2682  degrees of freedom
   AIC: 353.06

   Number of Fisher Scoring iterations: 19
</code></pre>
"
"0.178541910193085","0.0939474604818017","177654","<p>When running an ordered logistic regression using the <code>polr</code> function of the <code>MASS</code>package (DV is low, medium, high) and have a look at the summary I get Î²s for every IV and the intercepts for low|medium and medium|high.</p>

<p>The <code>predict</code>function for assessing the probabilities (<code>type='p'</code>) or the classes (<code>type='class'</code>) also works just fine.</p>

<p>However I want to calculate the probabilities myself in order to use them with different data sets.</p>

<p>If I use the following code for a <em>logistic model with a binary (!) dependent variable</em>, I can exactly replicate the <code>predict</code> - outcome:</p>

<p><code>log_pred &lt;- (logit_model$coefficients[1] + logit_model$coefficients[2]*IV_1 + logit_model$coefficients[3]*IV_2)</code></p>

<ul>
<li><code>logit_model</code> is my <code>glm</code>-object</li>
<li><code>logit_model$zeta[1]</code> is the first intercept</li>
<li><code>logit_model$zeta[2]</code> is the second intercept</li>
<li><code>logit_model$coefficients[1]</code> is the Î² of IV_1</li>
<li><code>logit_model$coefficients[2]</code> is the Î² of IV_2</li>
</ul>

<p>the only thing I have to do now, to get the predicted probabilities is:</p>

<p><code>log_pred_probs &lt;- exp(log_pred)/(1+exp(log_pred))</code></p>

<p>If I understand all the posts on ordered logistic regression I read correctly, the only thing I have to change with a <code>polr</code> object with the 3 ""groups"" of low, medium, and high would be to:</p>

<ul>
<li>run the <code>log-pred</code>part for each group using their own intercepts, let's call them <code>log_pred1</code> and <code>log_pred2</code></li>
<li>and to, then, run the following code (similar to the logistic model above):
<code>log_pred_probs1 &lt;- exp(log_pred1)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""low""
<code>log_pred_probs2 &lt;- exp(log_pred2)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""medium""
<code>log_pred_probs3 &lt;- 1/(1+exp(log_pred1)+exp(log_pred2))</code> for ""high""</li>
</ul>

<p>I think there are at least two problems ('cause this doesn't work at all):</p>

<ol>
<li>I need the Î²-coefficients for every level of the dependent variable, and <code>summary(polr-object)</code>does only show the Î²s for the first group (so does <code>$coefficients</code>)</li>
<li>and I am not sure about the computation of the predicted probabilities for group 3, ""high"".</li>
</ol>

<p>So these are the questions in short: <strong>How do I assess the Î²-coefficients for every level of the DV in a <code>polr</code>object?</strong></p>

<p>And</p>

<p><strong>How do I compute the predicted probabilities for every level of the DV myself?</strong></p>
"
"0.119027940128723","0.103342206529982","177805","<p>R and statistics beginner here, trying to do a quantile regression on a non-linear dataset. </p>

<p>I want to identify datapoints that have a higher y axis value that expected given their value on the x axis. 
I should highlight that the y-data are means of discrete values (0.1-1, in steps of 0.1) taken in dependence on the x-data. x values are number of SNPs in a gene. Each SNP has a discrete value and the y value is a mean of these SNP values for each gene.</p>

<p>After initially investigating  funnel plots it seems that a quantile regression might be most appropriate for this dataset, though thoughts on this are welcome.  I'd appreciate any guidance in fitting a quantile regression to identify that don't fall within 95 percent of the data.</p>

<p>Sample of data (I actually have ~20,000 datapoints):</p>

<pre><code>GENE    mean  total
X1  0.1 3
X2  0.1466666667    30
X3  0.1375  8
X4  0.24    5
X5  0.2625  8
X6  0.2 1
X7  0.1466666667    15
X8  0.2 1
X9  0.1666666667    9
X10 0.1 1
X11 0.1928571429    14
X12 0.1 2
X13 0.1545454545    11
X14 0.1333333333    3
X15 0.1666666667    3
X16 0.2117647059    34
X17 0.1452380952    42
X18 0.16    5
X19 0.2 1
X20 0.25    2
X21 0.125   4
X22 0.2 13
X23 0.1714285714    7
X24 0.15    6
X25 0.2 3
X26 0.2894736842    19
X27 0.2352941176    17
X28 0.1333333333    6
X29 0.12    5
X30 0.2 3
X31 0.1 1
X32 0.1571428571    7
X33 0.2125  8
X34 0.18125 16
X35 0.26    10
X36 0.1368421053    19
X37 0.1333333333    6
X38 0.15    2
X39 0.14    5
X40 0.18    15
X41 0.14    5
X42 0.3 1
X43 0.1 2
X44 0.1 6
X45 0.1 4
X46 0.1 1
X47 0.1333333333    3
X48 0.1166666667    6
X49 0.225   4
X50 0.2 15
X51 0.125   12
X52 0.1 3
X53 0.1714285714    14
X54 0.175   4
X55 0.3404761905    42
X56 0.1 1
X57 0.25    2
X58 0.15    4
X59 0.1 1
X60 0.1666666667    3
X61 0.3 2
X62 0.225   4
X63 0.3076923077    13
X64 0.1 1
X65 0.1666666667    3
X66 0.1666666667    6
X67 0.1 3
X68 0.1 3
X69 0.1166666667    6
X70 0.125   8
X71 0.2 1
X72 0.2 2
X73 0.1333333333    42
X74 0.1 1
X75 0.2 8
X76 0.1444444444    9
X77 0.1666666667    15
X78 0.1 2
X79 0.176744186 43
X80 0.1275  40
X81 0.1666666667    3
X82 0.125   4
X83 0.2545454545    11
X84 0.1304347826    46
X85 0.21    10
X86 0.1571428571    7
X87 0.3 9
X88 0.275   16
X89 0.11    10
X90 0.1333333333    6
X91 0.2333333333    3
X92 0.2 2
X93 0.2866666667    15
X94 0.25    2
X95 0.1125  8
X96 0.4 11
X97 0.1 1
X98 0.2 2
X99 0.15    2
X100    0.1625  8
X101    0.24    5
X102    0.175   4
X103    0.15    4
X104    0.1333333333    3
X105    0.4 2
X106    0.2 3
X107    0.25    2
X108    0.32    5
X109    0.2333333333    3
X110    0.1714285714    7
X111    0.2 1
X112    0.225   4
X113    0.2 1
X114    0.1714285714    7
X115    0.15    2
X116    0.1166666667    6
X117    0.16875 16
X118    0.1555555556    9
X119    0.15    6
X120    0.12    5
X121    0.1 1
X122    0.1333333333    6
X123    0.2333333333    3
X124    0.1 1
X125    0.2333333333    3
X126    0.1333333333    3
X127    0.1 1
X128    0.1827586207    29
X129    0.25    8
X130    0.2 7
X131    0.25    6
X132    0.1 1
X133    0.125   4
X134    0.2 1
X135    0.1666666667    3
X136    0.1 3
X137    0.12    5
X138    0.1 1
X139    0.175   4
X140    0.1 1
X141    0.1666666667    3
X142    0.1666666667    3
X143    0.1 1
X144    0.1375  8
X145    0.1 9
X146    0.1 2
X147    0.125   4
X148    0.1333333333    3
X149    0.1769230769    13
X150    0.15    2
X151    0.1214285714    14
X152    0.1 1
X153    0.2555555556    18
X154    0.2 1
X155    0.1 1
X156    0.1 1
X157    0.1 1
X158    0.4 1
X159    0.14    5
X160    0.1 2
X161    0.1333333333    3
X162    0.375   8
X163    0.2263157895    19
X164    0.1636363636    11
X165    0.3 1
X166    0.1 3
X167    0.2 1
X168    0.3 1
X169    0.1428571429    7
X170    0.1 2
X171    0.1222222222    9
X172    0.1 8
X173    0.1 5
X174    0.1 8
X175    0.1666666667    3
X176    0.2 5
X177    0.1 4
X178    0.1166666667    6
X179    0.15    2
X180    0.3666666667    3
X181    0.25    4
X182    0.1 1
X183    0.1 2
X184    0.1 1
X185    0.1 1
X186    0.1 1
X187    0.184   25
X188    0.2333333333    3
X189    0.2333333333    3
X190    0.1 2
X191    0.32    5
X192    0.1 2
X193    0.12    5
X194    0.1 5
X195    0.2 1
X196    0.1 6
X197    0.1 2
X198    0.4 1
X199    0.2 2
X200    0.1 2
X201    0.2 1
X202    0.2333333333    6
X203    0.35    2
X204    0.1 1
X205    0.12    5
X206    0.14    5
X207    0.125   4
X208    0.3333333333    3
X209    0.1 2
X210    0.1 3
X211    0.1 1
X212    0.2 4
X213    0.15    8
X214    0.125   4
X215    0.1548387097    31
X216    0.2 7
X217    0.225   4
X218    0.125   4
X219    0.15    2
X220    0.4 1
X221    0.275   4
X222    0.325   4
X223    0.2 3
X224    0.175   4
X225    0.3 1
X226    0.1 1
X227    0.19    10
X228    0.25    4
X229    0.2666666667    9
X230    0.1 1
X231    0.2 1
X232    0.3 1
X233    0.2166666667    6
X234    0.26    5
X235    0.225   4
X236    0.1 1
X237    0.1857142857    7
X238    0.58    5
X239    0.25    10
X240    0.6066666667    15
X241    0.3 1
X242    0.5 2
X243    0.2333333333    3
X244    0.25    2
X245    0.1 4
X246    0.1 1
X247    0.1714285714    7
X248    0.16875 16
X249    0.2 1
X250    0.4 3
X251    0.1 1
X252    0.1666666667    6
X253    0.2 6
X254    0.3166666667    12
X255    0.1 1
X256    0.1 2
X257    0.4 1
X258    0.1333333333    3
X259    0.225   4
X260    0.2571428571    7
X261    0.4 5
X262    0.15    10
X263    0.1571428571    7
X264    0.2 11
X265    0.2285714286    7
X266    0.15    4
X267    0.3 1
X268    0.1384615385    13
X269    0.1 4
X270    0.1 1
X271    0.16    5
X272    0.1285714286    7
X273    0.1 1
X274    0.2222222222    9
X275    0.2083333333    12
X276    0.2153846154    13
X277    0.1888888889    9
X278    0.1 1
X279    0.1 2
X280    0.3 2
X281    0.17    10
X282    0.1 5
X283    0.2833333333    6
X284    0.1333333333    6
X285    0.1833333333    6
X286    0.1833333333    12
X287    0.1953488372    43
X288    0.2526315789    19
X289    0.1 1
X290    0.125   4
X291    0.26    5
X292    0.1 2
X293    0.2578947368    19
X294    0.2545454545    11
X295    0.1 1
X296    0.3666666667    3
X297    0.1714285714    7
X298    0.1833333333    6
X299    0.16    5
X300    0.2733333333    15
X301    0.275   4
X302    0.1 1
X303    0.2 7
X304    0.1583333333    12
X305    0.1666666667    3
X306    0.1 1
X307    0.1 6
X308    0.1642857143    14
X309    0.1 1
X310    0.1606060606    33
X311    0.1428571429    7
X312    0.1888888889    9
X313    0.2 2
X314    0.1388888889    18
X315    0.35    2
X316    0.3 2
X317    0.1 4
X318    0.15    16
X319    0.1166666667    12
X320    0.1888888889    9
X321    0.16    5
X322    0.2333333333    3
X323    0.1857142857    14
X324    0.31    20
X325    0.2 1
X326    0.1 1
X327    0.1952380952    21
X328    0.215625    32
X329    0.1 1
X330    0.1 1
X331    0.1307692308    13
X332    0.1 4
X333    0.1666666667    3
X334    0.2 14
X335    0.1583333333    12
X336    0.1961538462    26
X337    0.2222222222    9
X338    0.1 3
X339    0.1 2
X340    0.1285714286    14
X341    0.175   4
X342    0.125   4
X343    0.1 4
X344    0.1428571429    7
X345    0.1 4
X346    0.1 2
X347    0.15    2
X348    0.25    4
X349    0.22    5
X350    0.1 2
X351    0.1 3
X352    0.14    10
X353    0.1666666667    18
X354    0.1333333333    3
X355    0.2 3
X356    0.16    5
X357    0.3 1
X358    0.175   4
X359    0.5 1
X360    0.1111111111    9
X361    0.2333333333    6
X362    0.175   4
X363    0.227027027 37
X364    0.3857142857    7
X365    0.1 2
X366    0.2 3
X367    0.1916666667    12
X368    0.1428571429    14
X369    0.2666666667    3
X370    0.2 9
X371    0.25    2
X372    0.2 1
X373    0.1 2
X374    0.225   4
X375    0.1 1
X376    0.1 3
X377    0.3 2
X378    0.1 1
X379    0.1545454545    11
X380    0.1730769231    52
X381    0.1 3
X382    0.1333333333    3
X383    0.1814814815    27
X384    0.108   25
X385    0.2666666667    6
X386    0.1666666667    3
X387    0.25    8
X388    0.225   4
X389    0.24    25
X390    0.2666666667    6
X391    0.1 2
X392    0.15    4
X393    0.1666666667    6
X394    0.1 1
X395    0.2375  8
X396    0.125   4
X397    0.1 7
X398    0.1 7
X399    0.1 4
X400    0.1 2
X401    0.1625  8
X402    0.3 1
X403    0.3 2
X404    0.25    4
X405    0.2 1
X406    0.1285714286    7
X407    0.15    8
X408    0.5 1
X409    0.1 1
X410    0.1285714286    7
X411    0.1 1
X412    0.2166666667    30
X413    0.22    5
X414    0.2714285714    14
X415    0.1214285714    14
X416    0.2 8
X417    0.28    5
X418    0.24    35
X419    0.15    4
X420    0.1333333333    12
X421    0.125   4
X422    0.1 1
X423    0.1666666667    3
X424    0.2111111111    9
X425    0.3 4
X426    0.2 2
X427    0.2 3
X428    0.1 1
X429    0.1 1
X430    0.1617021277    47
X431    0.15    8
X432    0.1142857143    14
X433    0.15    4
X434    0.1384615385    13
X435    0.1 2
X436    0.1166666667    12
X437    0.1714285714    14
X438    0.2416666667    12
X439    0.1 1
X440    0.1428571429    7
X441    0.1 1
X442    0.1416666667    12
X443    0.3333333333    6
X444    0.2 1
X445    0.14    5
X446    0.2 3
X447    0.225   28
X448    0.1571428571    14
X449    0.1 1
X450    0.1583333333    12
X451    0.1518518519    27
X452    0.1363636364    11
X453    0.2 1
X454    0.1666666667    6
X455    0.1 1
X456    0.1333333333    3
X457    0.2368421053    19
X458    0.1222222222    9
X459    0.15    2
X460    0.2 1
X461    0.1625  24
X462    0.2 6
X463    0.1666666667    3
X464    0.1 3
X465    0.3 8
X466    0.1523809524    21
X467    0.1 3
X468    0.1 3
X469    0.15    4
X470    0.1 1
X471    0.1642857143    28
X472    0.1 5
X473    0.1 2
X474    0.12    15
X475    0.1 3
X476    0.1090909091    11
X477    0.1346153846    26
X478    0.125   4
X479    0.1444444444    9
X480    0.2 1
X481    0.1 1
X482    0.1 3
X483    0.2 3
X484    0.1375  8
X485    0.1 4
X486    0.12    5
X487    0.1739130435    23
X488    0.25    2
X489    0.1333333333    6
X490    0.3 1
X491    0.225   20
X492    0.175   4
X493    0.1 3
X494    0.1222222222    9
X495    0.1 1
X496    0.175   4
X497    0.2333333333    6
X498    0.1615384615    13
X499    0.15    8
X500    0.1666666667    6
X501    0.2 2
X502    0.1777777778    9
X503    0.15    4
X504    0.2666666667    3
X505    0.1 4
X506    0.1222222222    9
X507    0.15    2
X508    0.2 3
X509    0.1333333333    15
X510    0.14    5
X511    0.1 1
X512    0.4 1
X513    0.2125  8
X514    0.36    5
X515    0.34    5
X516    0.4 1
X517    0.1428571429    7
X518    0.3333333333    3
X519    0.1 3
X520    0.2277777778    18
X521    0.1916666667    12
X522    0.2 4
X523    0.1857142857    7
X524    0.1 2
X525    0.1 5
X526    0.2222222222    9
X527    0.1818181818    11
X528    0.2151515152    33
X529    0.1 3
X530    0.1214285714    14
X531    0.2 1
X532    0.1 2
X533    0.1 3
X534    0.1166666667    12
X535    0.1 2
X536    0.1 2
X537    0.1 1
X538    0.2379310345    29
X539    0.175   4
X540    0.1363636364    11
X541    0.1 1
X542    0.1479166667    48
X543    0.1928571429    28
X544    0.4 1
X545    0.1951219512    41
X546    0.1333333333    3
X547    0.15    4
X548    0.2833333333    6
X549    0.1547619048    42
X550    0.1555555556    9
X551    0.2363636364    11
X552    0.2142857143    7
X553    0.5 1
X554    0.15    4
X555    0.1709677419    31
X556    0.17    10
X557    0.1 2
X558    0.2866666667    15
X559    0.4 2
X560    0.15    2
X561    0.1424242424    66
X562    0.25    2
X563    0.1 3
X564    0.1285714286    7
X565    0.12    5
X566    0.25    4
X567    0.2263157895    19
X568    0.1 12
X569    0.1666666667    6
X570    0.5 1
X571    0.147826087 23
X572    0.1 1
X573    0.1818181818    11
X574    0.2 2
X575    0.15    2
X576    0.2 3
X577    0.16    15
X578    0.1621621622    37
X579    0.1333333333    3
X580    0.1333333333    12
X581    0.18    5
X582    0.1534482759    58
X583    0.1538461538    26
X584    0.1 9
X585    0.2142857143    7
X586    0.1 1
X587    0.1222222222    9
X588    0.1 1
X589    0.1 3
X590    0.1 6
X591    0.15    2
X592    0.1 2
X593    0.3 1
X594    0.1285714286    21
X595    0.2 2
X596    0.12    5
X597    0.1 1
X598    0.1 1
X599    0.1 2
X600    0.1153846154    13
X601    0.1 15
X602    0.1 1
X603    0.1 1
X604    0.1 4
X605    0.15    10
X606    0.15    4
X607    0.15    4
X608    0.2 1
X609    0.14    5
X610    0.2 1
X611    0.1 2
X612    0.1 3
X613    0.125   4
X614    0.172   25
X615    0.2 4
X616    0.1727272727    11
X617    0.2090909091    22
X618    0.1333333333    3
X619    0.1 7
X620    0.15    4
X621    0.1181818182    11
X622    0.1375  8
X623    0.1666666667    3
X624    0.1 3
X625    0.1090909091    11
X626    0.125   8
X627    0.1 2
X628    0.12    5
X629    0.1 8
X630    0.13    40
X631    0.1666666667    3
X632    0.34    5
X633    0.1714285714    7
X634    0.1636363636    11
X635    0.1 1
X636    0.1 1
X637    0.18125 16
X638    0.2 4
X639    0.2 8
X640    0.1 2
X641    0.1 1
X642    0.1166666667    6
X643    0.2 1
X644    0.6 1
X645    0.2666666667    9
X646    0.2666666667    3
X647    0.2 2
X648    0.1 2
X649    0.1 1
X650    0.1 2
X651    0.1 1
X652    0.125   4
X653    0.15    2
X654    0.1 1
X655    0.1 1
X656    0.35    4
X657    0.2666666667    3
X658    0.1 2
X659    0.1 1
X660    0.2 1
X661    0.1 2
X662    0.1 2
X663    0.1333333333    3
X664    0.1 2
X665    0.1 1
X666    0.225   4
X667    0.1666666667    6
X668    0.1 2
X669    0.1 3
X670    0.175   4
X671    0.1 3
X672    0.15    4
X673    0.1666666667    3
X674    0.1 3
X675    0.175   4
X676    0.25    8
X677    0.25    4
X678    0.2571428571    7
X679    0.1 1
X680    0.2571428571    7
X681    0.208   25
X682    0.325   12
X683    0.1 1
X684    0.25    2
X685    0.1 2
X686    0.3047619048    21
X687    0.24    5
X688    0.15    6
X689    0.1333333333    6
X690    0.3 1
X691    0.1 1
X692    0.15    2
X693    0.23    20
X694    0.2 2
X695    0.1666666667    6
X696    0.1342857143    35
X697    0.25    6
X698    0.2 8
X699    0.2 5
X700    0.5 1
X701    0.1333333333    6
X702    0.3 1
X703    0.15    2
X704    0.15    2
X705    0.1833333333    6
X706    0.15    6
X707    0.1493506494    77
X708    0.36    5
X709    0.3 2
X710    0.15    2
X711    0.38    5
X712    0.2666666667    3
X713    0.25    4
X714    0.225   4
X715    0.5 1
X716    0.1 2
X717    0.16    5
X718    0.3 2
X719    0.3538461538    13
X720    0.1 2
X721    0.175   4
X722    0.22    5
X723    0.175   4
X724    0.2333333333    6
X725    0.34    5
X726    0.2 7
X727    0.1 1
X728    0.3 3
X729    0.1 1
X730    0.1 3
X731    0.3 5
X732    0.35    6
X733    0.2875  8
X734    0.1 1
X735    0.1 2
X736    0.2 5
X737    0.1714285714    7
X738    0.375   4
X739    0.1 4
X740    0.3 1
X741    0.1 1
X742    0.1142857143    7
X743    0.1 1
X744    0.2285714286    7
X745    0.14    5
X746    0.15    6
X747    0.1 1
X748    0.125   4
X749    0.1666666667    6
X750    0.125   8
X751    0.1 1
X752    0.15    2
X753    0.2 1
X754    0.225   4
X755    0.3 1
X756    0.3 5
X757    0.175   4
X758    0.1 3
X759    0.1333333333    18
X760    0.1230769231    13
X761    0.2 1
X762    0.11    10
X763    0.1666666667    6
X764    0.1 1
X765    0.2090909091    11
X766    0.145   20
X767    0.14    5
X768    0.2375  8
X769    0.1571428571    7
X770    0.1 1
X771    0.1 2
X772    0.2 2
X773    0.16    5
X774    0.2 1
X775    0.1777777778    9
X776    0.1210526316    19
X777    0.2 1
X778    0.225   12
X779    0.1666666667    3
X780    0.1 6
X781    0.2333333333    6
X782    0.1692307692    13
X783    0.19    10
X784    0.2 3
X785    0.1489361702    47
X786    0.2 5
X787    0.45    2
X788    0.1666666667    6
X789    0.18    5
X790    0.3 1
X791    0.2 2
X792    0.11    10
X793    0.3333333333    3
X794    0.25    2
X795    0.2 1
X796    0.25    2
X797    0.2 2
X798    0.2 1
X799    0.1 3
X800    0.1333333333    18
X801    0.1473684211    19
X802    0.2 5
X803    0.14    5
X804    0.125   4
X805    0.1583333333    12
X806    0.1857142857    7
X807    0.1 1
X808    0.2 1
X809    0.1769230769    26
X810    0.1 1
X811    0.1 2
X812    0.1833333333    6
X813    0.1409090909    22
X814    0.1416666667    24
X815    0.1307692308    13
X816    0.1235294118    17
X817    0.1 1
X818    0.1 1
X819    0.18    30
X820    0.2514285714    35
X821    0.18    5
X822    0.2 4
X823    0.1 1
X824    0.2333333333    9
X825    0.1222222222    9
X826    0.15    2
X827    0.14    5
X828    0.1588235294    51
X829    0.15    2
X830    0.2 4
X831    0.1 2
X832    0.1391304348    23
X833    0.18    20
X834    0.15    2
X835    0.3 1
X836    0.1 8
X837    0.1666666667    9
X838    0.1954545455    22
X839    0.225   16
X840    0.1222222222    9
X841    0.1210526316    19
X842    0.1 2
X843    0.1 2
X844    0.125   4
X845    0.1 4
X846    0.1 1
X847    0.2 2
X848    0.275   4
X849    0.1 3
X850    0.2833333333    6
X851    0.175   4
X852    0.32    5
X853    0.1 1
X854    0.1428571429    7
X855    0.2277777778    18
X856    0.15    8
X857    0.12    5
X858    0.1 2
X859    0.175   4
X860    0.18    5
X861    0.16    5
X862    0.2333333333    6
X863    0.1 1
X864    0.3333333333    3
X865    0.1 2
X866    0.15    12
X867    0.1636363636    11
X868    0.4 1
X869    0.4 1
X870    0.1 3
X871    0.1555555556    9
X872    0.2 1
X873    0.3 1
X874    0.2 2
X875    0.15    12
X876    0.1 1
X877    0.1181818182    11
X878    0.1428571429    7
X879    0.1461538462    13
X880    0.3076923077    13
X881    0.2 2
X882    0.3 1
X883    0.205   20
X884    0.2 5
X885    0.1333333333    3
X886    0.15    2
X887    0.25    2
X888    0.15    4
X889    0.3 1
X890    0.125   4
X891    0.1875  8
X892    0.1428571429    7
X893    0.2333333333    3
X894    0.1 2
X895    0.1 1
X896    0.35    6
X897    0.1444444444    9
X898    0.2 2
X899    0.3 1
X900    0.1 2
X901    0.1 1
X902    0.25    2
X903    0.1 1
X904    0.1 1
X905    0.7 1
X906    0.2 1
X907    0.45    4
X908    0.25    2
X909    0.15    4
X910    0.1 2
X911    0.4 13
X912    0.1 2
X913    0.1842105263    19
X914    0.1 1
X915    0.1333333333    3
X916    0.2 2
X917    0.1 7
X918    0.1 1
X919    0.225   4
X920    0.2 1
X921    0.2 3
X922    0.18    5
X923    0.1 1
X924    0.1875  8
X925    0.2833333333    6
X926    0.5 3
X927    0.2 1
X928    0.1 1
X929    0.1 2
X930    0.2 3
X931    0.4 1
X932    0.2875  16
X933    0.1857142857    7
X934    0.1 1
X935    0.2 2
X936    0.1 1
X937    0.2 13
X938    0.2444444444    9
X939    0.1 1
X940    0.1714285714    7
X941    0.3 1
X942    0.1 1
X943    0.2857142857    7
X944    0.15    2
X945    0.1 1
X946    0.15625 16
X947    0.1666666667    3
X948    0.3 1
X949    0.2 2
X950    0.1 8
X951    0.1 1
X952    0.1 3
X953    0.3 1
X954    0.3 1
X955    0.1 3
X956    0.1125  8
X957    0.18    5
X958    0.2666666667    3
X959    0.2 1
X960    0.125   4
X961    0.1333333333    3
X962    0.2444444444    9
X963    0.25    10
X964    0.25    4
X965    0.2 1
X966    0.225   4
X967    0.1625  8
X968    0.1333333333    3
X969    0.1333333333    3
X970    0.1 1
X971    0.2 7
X972    0.3 10
X973    0.1 1
X974    0.3 2
X975    0.225   4
X976    0.1 1
X977    0.1 2
X978    0.4 1
X979    0.1333333333    3
X980    0.1333333333    9
X981    0.13125 16
X982    0.1 1
X983    0.2 1
X984    0.1782608696    23
X985    0.2225806452    31
X986    0.15    4
X987    0.1 3
X988    0.1 3
X989    0.15    4
X990    0.2285714286    14
X991    0.2384615385    26
X992    0.4 1
X993    0.4 2
X994    0.1 1
X995    0.1 1
X996    0.1666666667    3
X997    0.1 6
X998    0.13    20
X999    0.2666666667    3
</code></pre>

<p>Code I am using:</p>

<pre><code>Asianpig &lt;- NULL; Asianpig$x &lt;- (Asianpig_data$total)
Asianpig$y &lt;- (Asianpig_data$mean)
plot(Asianpig)

#increase maxiterations for nls
nlc &lt;- nls.control(maxiter = 21811)

# fit first a nonlinear least-square regression
Dat.nls &lt;- nls(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, control = nlc); Dat.nls
lines(1:8000, predict(Dat.nls, newdata=list(x=1:8000)), col=1)

# and finally ""external envelopes"" holding 95 percent of the data
Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.025, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)

Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.975, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)
</code></pre>

<p>How this looks: </p>

<p><a href=""http://i.stack.imgur.com/tF8Vu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tF8Vu.png"" alt=""enter image description here""></a></p>

<p>I was expecting the quantile regression line to more dynamically follow the slope of the datapoints. 
I adapted the code from an example that was using <code>SSlogis()</code> for the input data:</p>

<pre><code># build artificial data with multiplicative error
Dat &lt;- NULL; Dat$x &lt;- rep(1:25, 20)
    set.seed(1)
    Dat$y &lt;- SSlogis(Dat$x, 10, 12, 2)*rnorm(500, 1, 0.1)
plot(Dat)
</code></pre>

<p>I have a feeling I should not be using <code>SSlogis()</code> in my code, but instead should be modelling an exponential distribution. SSlogis is a selfStart model evaluates the logistic function and its gradient. It has an initial attribute that creates initial estimates of the parameters Asym, xmid, and scale.</p>

<p>But I am still trying to understand how to fit a quantile regression for this non-linear data.</p>

<p>Here is a hexbin plot that gives a feeling for how the data is clustered:<a href=""http://i.stack.imgur.com/NCrLX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NCrLX.png"" alt=""enter image description here""></a></p>
"
"0.0465242105199235","0.0440652649239232","177903","<p>I fitted an ordinal logistic regression but I'm unable to interpret the coefficients. Can anyone assist in this regard? Here is the output generated: </p>

<pre><code>Call:
polr(formula = factor(grade) ~ factor(Month) + Day, data = myData, 
    Hess = TRUE)

Coefficients:
                  Value Std. Error t value
factor(Month)4 1.405114    0.51547  2.7259
Day            0.007672    0.01944  0.3947

Intercepts:
    Value   Std. Error t value
1|2 -0.6785  0.7019    -0.9667
2|3  1.6767  0.7162     2.3412

Residual Deviance: 333.602 
AIC: 341.602 
</code></pre>

<p>The grade is factored: </p>

<p>1 = good<br>
2 = very good<br>
3 = excellent</p>

<p>Month is factored:</p>

<p>3 = March<br>
4 = April</p>

<p>The grade is the response while month and day are my explanatory variables.</p>
"
"0.113960576459638","0.0539687072220866","178150","<p>I split my dataset into 2 parts: 75% of it is the training set, 25% of it is the test set. Then I estimated the logistic regression parameters in the training set and I compute the Area Under the ROC Curve (AUC) (of the model estimated in the training set) from the test set.
Since the test set is formed by 2500 observations on 20 variables, I was expected to get 2500 AUC, one each row. Why I just get one AUC?</p>
"
"0.029424494316825","0.0557386411433294","178153","<p>I am running into difficulties when using <code>randomForest</code> (in R) for a classification problem. My R code, an image, and data are here: <a href=""http://www.psy.plymouth.ac.uk/research/Wsimpson/data.zip"" rel=""nofollow"">http://www.psy.plymouth.ac.uk/research/Wsimpson/data.zip</a> The observer is presented with either a faint image (contrast=<code>con</code>) buried in noise or just noise on each trial. He rates his confidence (<code>rating</code>) that the face is present. I have categorised rating to be a yes/no judgement (<code>y</code>). The face is either inverted (<code>invert=1</code>) or not in each block of 100 trials (one file). I use the contrast (1st column of predictor matrix <code>x</code>) and the pixels (the rest of the columns) to predict <code>y</code>.</p>

<p>It is critical to my application that I have an ""importance image"" at the end which shows how much each pixel contributes to the decision <code>y</code>. I have 1000 trials (length of <code>y</code>) and 4248 pixels+contrast=4249 predictors (ncols of <code>x</code>). Using <code>glmnet</code> (logistic ridge regression) on this problem works fine</p>

<p><code>fit&lt;-cv.glmnet(x,y,family=""binomial"",alpha=0)</code></p>

<p>However <code>randomForest</code> does not work at all,</p>

<p><code>fit &lt;- randomForest(x=x, y=y, ntree=100)</code></p>

<p>and it gets worse as the number of trees increases. For <code>invert=1</code>, the classification error for <code>randomForest</code> is 34.3%, and for <code>glmnet</code> it is 8.9%. I realise that these are not directly comparable, because <code>randomForest</code> is giving OOB error rate.</p>

<p>Please let me know what I am doing wrong with <code>randomForest</code>, and how to fix it.</p>
"
"0.131590338991954","0.0623176952849756","178420","<p>I have data where number of observation <code>n</code> is smaller than number of variables <code>p</code>. The answer variable is binary. For example:</p>

<pre><code>n &lt;- 10
p &lt;- 100
x &lt;- matrix(rnorm(n*p), ncol = p)
y &lt;- rbinom(n, size = 1, prob = 0.5)
</code></pre>

<p>I would like to fit logistic model for this data. So </p>

<pre><code>model &lt;- glmnet(x, y, family = ""binomial"", intercept = FALSE)
</code></pre>

<p>The function returns 100 model for different lambda values (panalization parameter in lasso regression). I choose the biggest model which has <code>n - 1</code> parameters or less (so less than number of observations). Let's say chosen model is for <code>lambda_opt</code>. </p>

<pre><code>model_one &lt;- glmnet(x, y, family = ""binomial"", intercept = FALSE, lambda = lambda_opt)
</code></pre>

<p>Now I would like to do the second step - use <code>step</code> function to my model to choose the submodel which will be the best in term of BIC. Unfortunately <code>step</code> function doesn't work.</p>

<pre><code>step(model_one, direction = ""backward"", k = log(n))
</code></pre>

<p>How can I make it work? Is there some other function for this specific kind of model to do what I want?</p>
"
"0.0268607654675127","0.0508821851314784","179015","<p>Let's say that I've got a 100 advertising campaigns and for each I'm trying to predict that a user will click on the advertisement or not using logistic regression. So basically I'm creating 100 models. In each campaign (model) I'm using the same features to predict the outcome.</p>

<p>Is there a way to cluster these 100 campaigns (datasets)? I'm looking for a method that will put in the same cluster campaigns in which similar users click on the advertisement. In other words I want to find similar campaigns and create models on clusters.</p>

<p>Simple example:</p>

<pre><code>campaign1&lt;-data.frame(click=c(1,1,1,1,1,1,0,0,0,0),
              f1=c(33,22,17,23,41,26,110,121,99,103),
              f2=c(1,1,1,1,0,1,0,0,0,0),
              f3=c(0.3,0.1,0.2,0.23,0.12,0.25,1.1,0.98,0.993,1.13))
campaign2&lt;-data.frame(click=c(0,0,0,0,0,0,1,1,1,1),
              f1=c(31.2,26,15,23.4,32,27,201,190,166,193),
              f2=c(1,0,1,1,0,1,0,0,0,0),
              f3=c(0.03,0.23,0.12,0.18,0.09,0.21,2.1,2.7,1.99,2.34))
campaign3&lt;-data.frame(click=c(0,0,0,0,0,0,1,1,1,1),
              f1=c(33,12,17,23,22,36,177,181,189,173),
              f2=c(1,1,0,1,0,1,0,0,0,0),
              f3=c(0.14,0.3,0.2,0.12,0.12,0.25,1.7,2.98,1.993,1.97))
campaign4&lt;-data.frame(click=c(1,1,1,1,1,1,0,0,0,0),
              f1=c(24,25.5,19,21,38,50,102,97,100,111),
              f2=c(1,1,1,0,0,1,0,0,0,1),
              f3=c(0.2,0.4,0.22,0.43,0.15,0.25,1.21,01.09,0.89,1.22))
</code></pre>

<p>In this example campaign1 and campaign4 could be in the same cluster because in both campaigns users who clicked on advertisement have similar values of f1, f2 and f3. The same for campaign2 and campaign3. But for example users from campaign1 and campaign2 are very different.</p>
"
"0.182482967150453","0.10370291340864","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"NaN","NaN","179329","<p>If your logistic regression fit has coefficients with the following attributes, do you look at the values of <code>Pr(Z&gt;|z|)</code> are smaller than 0.95 to determine whether that variable is needed at a 5% level of significance? </p>

<p>ie. If <code>Pr(&gt;|z|)</code> is 0.964, this variable is not needed at 5% significance.</p>

<p><a href=""http://i.stack.imgur.com/6UTBa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6UTBa.png"" alt=""enter image description here""></a></p>
"
"0.0465242105199235","0.0440652649239232","179435","<p>I have a dataset of accelerometer readings and I'm using <code>fft</code> to transform my data into frequency domain. Then, I would like to apply <code>glm</code> to find a model.</p>

<p>The problem is that <code>glm</code> does not allow the use of complex variables, and I can't just give up of the imaginary parts.</p>

<p>I'm trying to use Logistic Regression (that's why I'm using <code>glm</code>). Is there a way to do it with the complex variables?</p>
"
"0.134303827337563","0.0763232776972177","179751","<p>In generating a logistic regression model for a data set of c.8000 observations over c.40 variables (not including interactions) I am trying to figure the relations between the variables. Most variables are binary. 
The purpose is to be able to combine different variables which explains the same variance in the data. 
I have done some web-research of different possibilities but have found their contribution to the process somehow limited. Most of the 2x2 tests points out their is significantly no correlation between any of the pairs. The reason is, as I understand it, that the numbers are relatively big, for instance:</p>

<p><a href=""http://i.stack.imgur.com/leP3C.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/leP3C.png"" alt=""enter image description here""></a></p>

<p>It seems like their is some kind of correlation between both sets though due to the big data sets differences are significant. </p>

<p>Is there any suggestion of how to capture even low value correlations between binary variables? or other ways to understand these relations? </p>

<p>Many thanks.</p>
"
"0.0657951694959769","0.0311588476424878","179898","<p>I have a clinical dataset (1400 cases) and I applied 4 data mining techniques (ANN, Decision Tree, SVM, Logistic Regression) to predict the binary outcome (Yes, No).</p>

<p>Now, I want to improve prediction accuracy through ensemble methods.<br>
What are the criteria to choose which model can be combined with another model?
And how can that be done in R? Can I use the ""caret"" package?</p>
"
"0.107443061870051","0.0763232776972177","180191","<p>We can apply the Hosmer-Lemeshow goodness of fit to logistic regression modelling and to test if an underlying assumption is not applicable.</p>

<p>This <a href=""https://www.youtube.com/watch?v=MYW8gA1EQCQ"" rel=""nofollow"">link</a> shows a video of the application to a standard <code>glm()</code> model</p>

<p>This <a href=""http://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best-r"">detailed question</a>, outlines various simulation-based tests one can run to assess underlying distributions.</p>

<p><strong>But I want to apply the Hosmer-Lemeshow goodness of fit to survival analysis with assumed underlying data distributions</strong>.</p>

<p>Much literature points one towards a cox proportional hazards model, but from what I understand, a cox ph model does not assume an underlying distribution of data.
Therefore lets take some random data from the <code>survreg()</code> function of the <code>survival</code> package</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

s &lt;- Surv(ovarian$futime, ovarian$fustat)
sWei &lt;- survreg(s ~ age,dist='weibull',data=ovarian)
</code></pre>

<p>How can we applying a H+L G.O.F statistic test?
I had hoped to follow this <a href=""http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/"" rel=""nofollow"">link</a>, however the <code>survreg()</code> does not allow a <code>fitted()</code> function. Thus this does not work</p>

<pre><code>library(ResourceSelection)
hl &lt;- hoslem.test(sWei$y, fitted(sWei), g=10))
</code></pre>
"
"0.138865930150177","0.103342206529982","180302","<p>IÂ´m fairly new to the tools of statistics and I need some help.
IÂ´m working in R.
I have a list of students, their age, sex, test results(continuous variable) and education (a categorical variable. 0 if student has no education, 1 if student has education). This is stored in the data frame df.</p>

<p>I have assigned each student an age group depending on their age. The groups are 5-10,11-15, 16-20, 21-25. So now I have a new categorical variable with 4 levels.</p>

<p>I have done logistic regression for each age group. I did the test in R like this:</p>

<pre><code>model1 = glm(education ~ test results + sex + age, data = df, family = binomial())
</code></pre>

<p>From the logistic regression result I compute the odds ratio (exp(coefficient)) for the test results variable. For each age group I got an odds ratio and confidence interval. There are some differences in the odds ratio between age groups. The odd ratio gets closer to 1 with higher age group. I interpret this in the following way; as the age of students is higher there is less effect from education on test result.
My question is how can I test if the difference in odds ratios is significant, i.e. what test can I use in R ?</p>

<p>Here is a subset of the data:</p>

<pre><code>student sex age testScore education ageGroup

1        1   10   0.12       1        1

2        1    8   0.08       1        1

3        2   16   0.85       0        3

4        2   20   0.12       0        3

5        1   22   1.02       0        4

6        2   18   0.98       1        3

7        1   19   0.46       1        3

8        1   16   0.83       0        3

9        2   12   0.26       1        2

10       2   14   0.46       0        2
</code></pre>

<p>I have been searching books and the web without success, canÂ´t seem to find an example that I can relate to.
Any suggestions would be appreciated.</p>
"
"NaN","NaN","180337","<p>I always report odds ratios when using logistic regression for predictions. 
I wanted know is it meaningful to report odds ratios when modeling with gradient boosting approach? 
I am using gbm package in R to make the predictions.</p>

<p>Thanks!</p>
"
"0.0465242105199235","0.0440652649239232","180382","<p>I have binary count data as a response variable in my logistic regression. The independent variables include, among others, two variables of inclination and orientation measurements, annotated in degrees of arc. For 'orientation' (or aspect), it ranges from 0Â° to 360Â°, and for 'inclination' from 0Â° to 90Â°. In cases where 'inclination' is 0, the orientation is annotated as '-1', because horizontal surfaces do not face any direction.</p>

<p>For a logistic regression, my workflow would include to use R's scale-function to standardize all continuous variables, among them 'inclination' and 'orientation'. And that is what I did. But does that make sense here? Keep in mind, that an orientation of 0 (north) is the same as 360 (also north), and that 1Â° and 359Â° are only two degrees apart. </p>

<p>How can I standardize those measurements? How would you recode an orientation of '-1', which isn't either north nor east, south or west? At this point, both variables appear to be highly influential on my model fit, but can i trust that conclusion?</p>
"
"0.1359059177167","0.120677698006369","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"NaN","NaN","180580","<p>To understand my logistic regression fit and identify non linear effects, I plan to estimate the conditional density and then calculate the log odds comparing to log odds from logistic regression. To me  this is the equivalent of scatter plot of single  independent variable vs dependent and prediction. </p>

<p>A) Does this seem like the right approach? </p>

<p>B) I am using R, and I am surprised that there is no package already doing this? </p>
"
"0.113960576459638","0.0539687072220866","181463","<p>I made a logistic regression in R statistics, but I don't know how to interpret it with 2 categorical variables (the examples I found on the internet and / or stackoverflow were just with one and I have difficulties to imagine it with two). </p>

<p>So imagine I want to see which factors infuence the fact of having a special desease (1: yes, 0: no) and I have:</p>

<pre><code>City: Manhattan, New York
hospital: St. Mary, Avante, Copperfield
bloodshugar: 1, 28, 7 ... , 66 (numeric)
timetoreact: 113, 423, 334, ... (numeric),
</code></pre>

<p>I give it all in a glm-model glm (desease dependent on: <code>City</code>, <code>hospital</code>, <code>City:hospital</code>, ...)</p>

<p>In the output I have the problem that it's all comprised with the factor level of the first letter of the alphabet, so i.e. ""Manhattan"" and ""Avante"" doesn't appear anymore. </p>

<p>There is just written: </p>

<pre><code>NewYork:Bloodshugar: Coeff.: 0.034 
</code></pre>

<p>and I don't know now what it is... Manhattan:Bloodshugar doesn't appear. Is it the difference of the incline from the probability on bloodshugar in Newyork in comparison to Manhattan? Where can I see if the probability to get the desease sinks or inclines with more bloodshugar in New York? When there's written bloodshugar: Coef.: 0.021, is it the bloodshugar ""mean"" of Manhattan and New York or is it just from Manhattan?  </p>

<p>What is the intercept now? Is it the probability to show the desease when cured in the Avantehospital and raised in Manhattan (because it's always the first letter)?</p>

<p>I hope I explained it well, I still can add some more explanations if you'd like to. </p>
"
"0","0.0311588476424878","181695","<p>In linear regression, if I have a model,</p>

<pre><code>b0 + b1x1 + b2x2 + b3x3 + b4x4 = y
</code></pre>

<p>and I want to fix some of the coefficients ,say b1 = 1 and b3 = 2, I could just do the following</p>

<pre><code>b0 + b2x2 + b4x4 = y - x1 - 2x3
</code></pre>

<p>and just fit a linear regression on the other three parameters on the new y. Is there a way to do this for logistic regression? The sigmoid function seems to complicate things. Im looking to do this in r, so if theres an easy way to do it in r, that would be very appreciated.</p>
"
"0.0986927542439653","0.0623176952849756","182162","<p>I have the following data,</p>

<pre><code>Cond.1 &lt;- c(2.9, 3.0, 3.1, 3.1, 3.1, 3.3, 3.3, 3.4, 3.4, 3.4, 3.5, 3.5, 3.6, 3.7, 3.7,
        3.8, 3.8, 3.8, 3.8, 3.9, 4.0, 4.0, 4.1, 4.1, 4.2, 4.4, 4.5, 4.5, 4.5, 4.6,
        4.6, 4.6, 4.7, 4.8, 4.9, 4.9, 5.5, 5.5, 5.7)
Cond.2 &lt;- c(2.3, 2.4, 2.6, 3.1, 3.7, 3.7, 3.8, 4.0, 4.2, 4.8, 4.9, 5.5, 5.5, 5.5, 5.7,
        5.8, 5.9, 5.9, 6.0, 6.0, 6.1, 6.1, 6.3, 6.5, 6.7, 6.8, 6.9, 7.1, 7.1, 7.1,
        7.2, 7.2, 7.4, 7.5, 7.6, 7.6, 10, 10.1, 12.5)
</code></pre>

<p>and I want to apply Logistical Regression,  </p>

<pre><code>library(caret)
dat  = stack(list(cond1=Cond.1, cond2=Cond.2))
lr.model = glm(ind~values+I(values^2), dat, family=""binomial"")
lr.preds = predict(lr.model, type=""response"")
</code></pre>

<p>After using this code, I get value of lr.preds to be</p>

<pre><code>0.4195376 0.3559950 0.3022795 0.3022795 0.3022795 0.2226140 0.2226140 0.1945281 0.1945281 0.1945281 0.1726781 0.1726781 0.1560078 0.1436538 0.1436538 0.1349528 0.1349528 0.1349528 0.1349528 0.1294300 0.1267819 0.1267819 0.1268630 0.1268630 .....
</code></pre>

<p>but if change variable names from cond1 to cond1 and cond2 to con2 and run the same code i get different result of lr.preds.</p>

<pre><code>dat  = stack(list(uc=Cond.1, c=Cond.2))
lr.model = glm(ind~values+I(values^2), dat, family=""binomial"")
lr.preds = predict(lr.model, type=""response"")
</code></pre>

<p>In this case values of lr.preds are</p>

<pre><code>5.804624e-01 6.440050e-01 6.977205e-01 6.977205e-01 6.977205e-01 7.773860e-01 7.773860e-01 8.054719e-01 8.054719e-01 8.054719e-01 8.273219e-01 8.273219e-01 8.439922e-01 8.563462e-01 8.563462e-01 8.650472e-01 8.650472e-01 8.650472e-01 
</code></pre>

<p>Why are the values changing just by changing the variable?</p>
"
"0.153522062157279","0.0830902603799674","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"NaN","NaN","182467","<p>I'm running a logistic regression (presence/absence response) in R, using glmer (lme4 package). Ben Bolker'sÂ <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">overdisp_fun</a> (see link) tells me my model is overdispersed, so I decided to include an individual-level random effect. This is not solving my problem, as I get convergence issues and overdispersion is not reduced. Could anyone recommend an alternative?</p>

<p>Thanks!! </p>
"
"0.0930484210398471","0.0440652649239232","182509","<p>I am using logistic regression (with R) for detecting fraudulent transactions. So far I am achieving a relatively good ratio of success (f-score).</p>

<p>However I have noticed something, once I have my model built, the threshold that gives me the best f-score is to consider something as fraudulent if the logit function is bigger than 0.059 (I started with 0.5). </p>

<p>For the record, I am using 7099 observations as training examples and 3042 as testing data, in total I am using 6 features/columns for prediction (planning to add a couple more)</p>

<p>Now my questions are the following</p>

<ol>
<li>Am I doing something terribly wrong so that I have to use such a low limit to start labeling transactions as fraudulent?</li>
<li>That said, the vast majority of transaction are legitimate, does it justify the low threshold for the labeling (again, 0.059) ?</li>
<li>Would it be worth to explore other algs such random forest or neuronal networks?</li>
</ol>
"
"0.1176979772673","0.0557386411433294","182656","<p>I have data in longitudinal or clustered format (please see the example below). My response variable is dichotomous. I want to examine which factors explains why a subject in the dataset gets Y=1. In the example below I show only one predictor â€“ X.</p>

<p>Since I have a dichotomous response variable, I am thinking of logistic regression. However, the longitudinal format violates the distributional assumption of ML-theory. <strong>So my question is which logistic model would be appropriate here?</strong> And if possible, which R-package would be relevant (if not covered by the standard stats)? </p>

<p>All subjects are countries and observed (let's say) from 1990-1994. A country can get more than 1 Y per year, from different Z's.  I have been thinking of logistic panel models. Although I am not sure which specific model would be appropriate (assuming that panel models are more appropriate group of models). Perhaps random effects as each observation is not of the same nature (a country can get Y=1 from different groups, see variable Z). The Z variable is not a part of analysis though. Grateful for all suggestions!</p>

<pre><code>COUNTRY      YEAR  Y           X        Z

    A        1990  0           0        K
    A        1991  1           0        K
    A        1992  0           0        K
    A        1993  1           0        L
    B        1994  0           1        L
    B        1990  0           1        L
    B        1991  0           1        L
    B        1992  1           1        L
    C        1990  1           0        K
    C        1991  1           0        K
    C        1992  0           0        L
    C        1993  0           1        K
    C        1994  0           1        L
    D        1990  0           1        L
    D        1991  0           0        K
    D        1992  0           0        K
    D        1993  0           1        K
    D        1994  0           1        K
</code></pre>
"
"NaN","NaN","183150","<p>Can I use binary variables in R's glm function with a binomial outcome (logistic regression)?</p>
"
"0.196227812235748","0.135818268070785","183320","<p>I have the following dataframe on which I did logistic regression with response as outcome. There are some good predictors in these variables so I expected significant variables.</p>

<pre><code>structure(list(response = c(0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 
    0L, 0L, 1L, 0L, 1L, 0L), HIST1H3F_rna = c(1.09861228866811, 0.693147180559945, 
    2.07944154167984, 1.09861228866811, 1.79175946922805, 0, 0, 0, 
    2.39789527279837, 1.38629436111989, 1.6094379124341, 1.6094379124341, 
    0.693147180559945, 1.79175946922805, 0), NCF1_rna = c(2.77258872223978, 
    3.09104245335832, 2.63905732961526, 2.19722457733622, 2.30258509299405, 
    2.56494935746154, 3.09104245335832, 3.98898404656427, 2.56494935746154, 
    4.06044301054642, 3.87120101090789, 2.07944154167984, 3.49650756146648, 
    3.17805383034795, 3.95124371858143), WDR66_rna = c(5.06890420222023, 
    4.49980967033027, 5.11799381241676, 3.40119738166216, 3.25809653802148, 
    4.02535169073515, 5.8348107370626, 5.89440283426485, 3.87120101090789, 
    5.67675380226828, 5.35185813347607, 4.15888308335967, 6.23441072571837, 
    5.91889385427315, 3.68887945411394), PTH2R_rna = c(0.693147180559945, 
    5.08759633523238, 0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 
    6.56526497003536, 5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 
    1.09861228866811, 3.49650756146648, 1.38629436111989, 5.93753620508243
    ), HAVCR2_rna = c(4.48863636973214, 3.40119738166216, 3.09104245335832, 
    2.94443897916644, 3.2188758248682, 3.76120011569356, 3.95124371858143, 
    2.83321334405622, 2.07944154167984, 4.36944785246702, 3.58351893845611, 
    1.94591014905531, 4.23410650459726, 3.43398720448515, 2.56494935746154
    ), CD200R1_rna = c(2.484906649788, 2.94443897916644, 0.693147180559945, 
    1.94591014905531, 0.693147180559945, 2.89037175789616, 2.56494935746154, 
    1.6094379124341, 1.6094379124341, 1.94591014905531, 2.19722457733622, 
    0.693147180559945, 4.26267987704132, 1.6094379124341, 0.693147180559945
    )), .Names = c(""response"", ""HIST1H3F_rna"", ""NCF1_rna"", ""WDR66_rna"", 
    ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna""), row.names = c(NA, 
    -15L), class = ""data.frame"")
</code></pre>

<p>However, running the following lines and getting a summary of the model I find that all variables have a p-value of 1 and the standard errors seem so high. What's going on here?</p>

<pre><code>fullmod &lt;- glm(response ~ ., data=final_model,family='binomial')
summary(fullmod)
Call:
glm(formula = response ~ ., family = ""binomial"", data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-6.515e-06  -2.404e-06  -2.110e-08   2.110e-08   7.470e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   1.460e+02  5.598e+05       0        1
HIST1H3F_rna  2.135e+01  5.145e+05       0        1
NCF1_rna     -4.133e+01  3.388e+05       0        1
WDR66_rna     1.296e+01  6.739e+05       0        1
PTH2R_rna     1.975e+00  3.775e+05       0        1
HAVCR2_rna   -2.477e+01  1.191e+06       0        1
CD200R1_rna  -1.420e+01  1.315e+06       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0190e+01  on 14  degrees of freedom
Residual deviance: 2.2042e-10  on  8  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25
</code></pre>

<hr>

<p>In response to your comments I'll show the feature selection step (and the complete dataframe I'm working with below that).  </p>

<pre><code># forward  feature selection 
library('boot')
z = c()
nullmod &lt;- glm(response ~ 1, data=final_model, family='binomial') ## â€˜emptyâ€™ 
fullmod &lt;- glm(response ~ ., data=final_model, family='binomial') ## Full model
first = T
for(x in 1:ncol(final_model)){
  stepmod &lt;- step(nullmod, scope=list(lower=formula(nullmod), upper=formula(fullmod)),
                  direction=""forward"", data=final_model, steps=x, trace=F)
  cv.err  &lt;- cv.glm(data=final_model, glmfit=stepmod, K=nrow(final_model))$delta[1]
  if (first == T){
    first=F
    final_features &lt;- stepmod
  }else{
    if (cv.err &lt; min(z)){ final_features &lt;- stepmod }
  }
  z[x] &lt;- cv.err
  print(paste(x,cv.err))
  print(colnames(final_features$model))
}

plot(z, main='Forward Feature Selection GLM Final Model', 
     xlab='Number of Steps', ylab='LOOCV-error', col='red', type='l')
points(z)
colnames(final_features$model)
summary(final_features)

structure(list(response = c(0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 0L, 1L, 1L, 1L), HIST1H3F_rna = c(1.09861228866811, 2.07944154167984, 
1.09861228866811, 1.79175946922805, 0, 0, 0, 2.39789527279837, 
1.38629436111989, 1.6094379124341, 1.6094379124341, 0.693147180559945, 
2.19722457733622, 2.39789527279837, 2.89037175789616), NCF1_rna = c(2.77258872223978, 
2.63905732961526, 2.19722457733622, 2.30258509299405, 2.56494935746154, 
3.09104245335832, 3.98898404656427, 2.56494935746154, 4.06044301054642, 
3.87120101090789, 2.07944154167984, 3.49650756146648, 2.07944154167984, 
2.07944154167984, 1.09861228866811), WDR66_rna = c(5.06890420222023, 
5.11799381241676, 3.40119738166216, 3.25809653802148, 4.02535169073515, 
5.8348107370626, 5.89440283426485, 3.87120101090789, 5.67675380226828, 
5.35185813347607, 4.15888308335967, 6.23441072571837, 4.0943445622221, 
4.21950770517611, 3.95124371858143), PTH2R_rna = c(0.693147180559945, 
0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 6.56526497003536, 
5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 1.09861228866811, 
3.49650756146648, 0, 0.693147180559945, 1.38629436111989), 
HAVCR2_rna = c(4.48863636973214, 
3.09104245335832, 2.94443897916644, 3.2188758248682, 3.76120011569356, 
3.95124371858143, 2.83321334405622, 2.07944154167984, 4.36944785246702, 
3.58351893845611, 1.94591014905531, 4.23410650459726, 1.38629436111989, 
1.09861228866811, 1.38629436111989), CD200R1_rna = c(2.484906649788, 
0.693147180559945, 1.94591014905531, 0.693147180559945, 2.89037175789616, 
2.56494935746154, 1.6094379124341, 1.6094379124341, 1.94591014905531, 
2.19722457733622, 0.693147180559945, 4.26267987704132, 1.94591014905531, 
0, 0.693147180559945), GDF7 = c(0.2232, -0.7281, 0.0655, -0.7919, 
0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 0.7371, 
-0.4978, -0.5096, -0.0827), HS1BP3 = c(0.2232, -0.7281, 0.0655, 
-0.7919, 0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 
0.7371, -0.4978, -0.5096, -0.0827), NKAIN3 = c(0.4072, 0.3216, 
-0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 0.6331, 
-0.135, 1.3532, -0.503, -0.1241, 0.2061), UG0898H09 = c(0.4072, 
0.3216, -0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 
0.6331, -0.135, 1.3532, -0.503, -0.1241, 0.2061), C15orf41 = c(0.122, 
-0.7519, -1.1267, -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, 
-0.5944, 0.0714, -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), 
    FAM98B = c(-0.1871, -0.7519, -1.1267, -0.7882, -0.1117, -0.5105, 
    -0.3905, -0.6834, -0.5944, 0.0714, -0.8134, -0.0115, -1.1112, 
    -1.1488, -0.4878), SPRED1 = c(-0.1871, -0.7519, -1.1267, 
    -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, -0.5944, 0.0714, 
    -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), MPDZ_ex = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0), TPR_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BUB1B_ex = c(0, 
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), APC_ex = c(0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ATM_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0), DYNC1LI1_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), TTK_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), PSMG2_ex = c(1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), NegRegMitosis = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0), brca1ness = c(0.037719, 
    0.900878, 0.013261, 0.900878, 0.659963, 0.005629, 9.8e-05, 
    0.996336, 0.910072, 0.850776, 0.000613, 0.104428, 0.978114, 
    0.938767, 0.041696), Methylation = c(0L, 0L, 0L, 1L, 1L, 
    1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L), LinoleicAcid_Metab = structure(c(2L, 
    2L, 2L, 2L, 1L, 3L, 2L, 2L, 1L, 5L, 2L, 5L, 1L, 2L, 2L), .Label = c(""CYP2E1_high"", 
    ""CYP2E1_med"", ""high"", ""low"", ""PLA2G2A_high""), class = ""factor""), 
    Neuro_lr = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 3L, 1L, 3L, 
    1L, 1L, 3L, 3L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor""), 
    NOX_signalling = structure(c(2L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L), .Label = c(""high"", ""low""
    ), class = ""factor"")), .Names = c(""response"", ""HIST1H3F_rna"", 
""NCF1_rna"", ""WDR66_rna"", ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna"", 
""GDF7"", ""HS1BP3"", ""NKAIN3"", ""UG0898H09"", ""C15orf41"", ""FAM98B"", 
""SPRED1"", ""MPDZ_ex"", ""TPR_ex"", ""BUB1B_ex"", ""APC_ex"", ""ATM_ex"", 
""DYNC1LI1_ex"", ""TTK_ex"", ""PSMG2_ex"", ""NegRegMitosis"", ""brca1ness"", 
""Methylation"", ""LinoleicAcid_Metab"", ""Neuro_lr"", ""NOX_signalling""
), row.names = c(NA, -15L), class = ""data.frame"")
</code></pre>

<p>Summary now gives the following:</p>

<pre><code>Call:
glm(formula = response ~ NegRegMitosis, family = ""binomial"", 
    data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-3.971e-06  -3.971e-06   3.971e-06   3.971e-06   3.971e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       25.57   76367.61       0        1
NegRegMitosis    -51.13  111790.71       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0728e+01  on 14  degrees of freedom
Residual deviance: 2.3655e-10  on 13  degrees of freedom
AIC: 4

Number of Fisher Scoring iterations: 24
</code></pre>

<p>Again even in a single predictor model, my p-value is 1. The predictor in this case is equal to the response, so it should predict perfectly. Then why is my pvalue 1?</p>
"
"0.116310526299809","0.0881305298478463","183528","<p>I have three questions about the assumptions of the logistic regression:</p>

<ol>
<li><p>I read that the percentages of zeros and ones should be equal. If there's a data set where one of them is abundand, i.e. there are 80% zeros and 20% ones can I somehow put different weights in my glm? There's also the weight-function, but I don't understand what it's exactly for... </p></li>
<li><p>I didn't really get what the pseudo-coefficients of determination tell me - Do this, i.e. the Nagelkerke's index tell me something about the assumptions of my model, how much they are fullfilled or just how much my predicted model differ from the data points I've observed. </p></li>
<li><p>I also red for the assumptions that there should be at least 25 data points / group, what exactly is my group? When I i.e. have the mtcars-dataset in R </p>

<p>data(""mtcars"") </p></li>
</ol>

<p>and I want to look</p>

<pre><code>glm(vs ~ carp + disp, family = binomial) 
</code></pre>

<p>what are my groups? (maybe this is also a false point of view, but I'm really irritated...)</p>

<p>Best wishes </p>

<p>Marry</p>
"
"0.180936716113936","0.101266254838085","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.0657951694959769","0.0311588476424878","183845","<p>I'm going crazy, because I can't find a simple description how the coefficients are calculated in R statistics in the multivariable logistic regression (and I'm not a mathematician). 
Are they standardised? So i.e. when I have x ~ y1 + y2 and the coefficient for y1 = 0.2, is this the coefficient in the model when the parameter y2 is 0, the mean of y2 or somehow all the parameters of y2? </p>

<p>Sorry, I'm stuck on this simple question... </p>

<p>p.s.: I also have an interaction y1:y2 if this changes anything...</p>
"
"0.0759737176397586","0.0539687072220866","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"0.138865930150177","0.103342206529982","183908","<p>I have a binary logistic regression with just one binary fixed factor predictor. The reason I don't do it as a Chi square or Fisher's exact test is that I also have a number of random factors (there are multiple data points per individual and individuals are in groups, although I don't care about coefficients or significances for those random variables). I do this with R glmer.</p>

<p>I would like to be able to express the coefficient and associated confidence interval for the predictor as a risk ratio rather than an odds ratio. This is because (maybe not for you but for my audience) risk ratio is much easier to understand. Risk ratio here is the relative increase in chance of the outcome being 1 rather than 0 if the predictor is 1 rather than 0.</p>

<p>The odds ratio is trivial to get from the coefficient and associated CI using exp(). To convert an odds ratio to a risk ratio, you can use ""RR = OR / (1 â€“ p + (p x OR)), where p is the risk in the control group"" (source: <a href=""http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/"" rel=""nofollow"">http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/</a>). But, you need the risk in the control group, which in my case means the chance that the outcome is 1 if the predictor is 0. I believe the intercept coefficient from the model is in fact the odds for this chance, so I can use prob=odds/(odds+1) to get that. I'm pretty much so-far-so-good on this as far as the central estimate for the risk ratio goes. But what worries me is the associated confidence interval, because the intercept coefficient also has its own associated CI. Should I use the central estimate of the intercept, or to be conservative should I use whatever limits of the intercept CI make my relative risk CI widest? Or am I barking up the wrong tree entirely?</p>
"
"0.0930484210398471","0.0881305298478463","183961","<p>Will the results of the feature selection be biased if I perform the feature selection before imputing missing data? </p>

<p>I have a large data set of 20000 samples and 130 variables. The data sets consists of binary, numeric, and ordinal variables. The outcome variable is binary. </p>

<p>I want to do two things:</p>

<p>1) Feature selection to determine the most important variables
2) Build a predictive model with SVM, Random Forest, and Logistic Regression.</p>

<p>The complete case data set contains 70% of the original data (i.e if I keep only samples with no missing variable values, then I'm left with 70% of the samples)</p>

<p>I am using MICE in R to impute the missing data. 
Following some guidelines I found in <a href=""http://onlinelibrary.wiley.com/doi/10.1002/sim.4067/abstract"" rel=""nofollow"">this paper</a>, I plan to impute 30 datasets. (I estimate the Fraction of Missing Information using the percentage of incomplete cases, which is 30%. This is where the 30 comes from)</p>

<p>This is computationally intensive and will take too long. If I take only the top 10 predictors and impute this smaller data set, I will be able to impute my 30 data sets as desired in a reasonable amount of time. </p>

<p>I cannot assume the data are Missing Completely at Random (MCAR). Most variables are Missing at Random (MAR) where the missing values can be modeled from existing data. </p>

<p>Will the results of the feature selection be biased because of missing-ness in the data? </p>
"
"0.0930484210398471","0.0440652649239232","183976","<p>I created a SEM model in R (<code>lavaan</code> package), but one of my dependent variables is continuous, while the other is binary.</p>

<p>The model is as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1 + a2 + a3
b =~ b1 + b2 + b3
c =~ c1 + c2 + c3
x ~ a + b + c + z + w

y ~ a + b + c + z + w
'
sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p><code>z</code> and <code>w</code> are covariates. <code>x</code> is a scale (0-12), however <code>y</code> is a binary variable (0;1). </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that.</p>
"
"0.0882734829504749","0.0696733014291618","184137","<p>I would like to get the predicted values (with confidence intervals) for a multinomial logistic regression. I know this could be done with predict but in my case I have clustered standard errors in the following way:</p>

<pre><code>multinom &lt;- mlogit(Y ~0| X1+ X2 , data)
cl.mlogit   &lt;- function(fm, cluster){
  M &lt;- length(unique(cluster))
  N &lt;- length(cluster)
  K &lt;- length(coefficients(fm))
  dfc &lt;- (M/(M-1))
  uj  &lt;- apply(estfun(fm),2, function(x) tapply(x, cluster, sum));
  vcovCL &lt;- dfc*sandwich(fm, meat.=crossprod(uj)/N)
 coeftest(fm, vcovCL) 
}
cl.mlogit(multinom, data$group)
</code></pre>

<p>How I could use these results to get the predicted probabilities (with confidence intervals) for X1=1 and X2=0 for example and compare it with predicted probalities for X1=2 and X2=0. </p>

<p>Also, how could I get a confidence interval for that difference? In Stata prvalue do this last thing by using the delta method to get the confidence interval <a href=""http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf"" rel=""nofollow"">http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf</a>. Is there an easy way to do it in R?</p>
"
"0.0657951694959769","0.0311588476424878","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"0.166450075715296","0.0985329278164293","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.0930484210398471","0.0440652649239232","184951","<p>How do I calculate the R-squared value for my ordered logistic regression machine learning model? My outcome variable is an ordered value between 0 to 5. The levels 1 to 5 are equally spaced, but that's not the case with 0 to 1. I also have the actual values which fall in that ordered range, for my test data.</p>

<p>How do I represent the comparison of these two?</p>
"
"0.0465242105199235","0.0440652649239232","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.0882734829504749","0.0696733014291618","185495","<p>I am developing some stochastic simulations in which I have four explanatory variables that I named <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. They are all continuous data. I have several responses. One of these responses is a categorical value that can take 5 categories that I just labelled 1 to 5. I called this response an outcome.</p>

<p>I am having lots of trouble figuring out an analysis that I could use to understand the influence of my explanatory variables (all continuous data) on my response (categorical data). For example, I am interested in understanding how the outcome is influenced by <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. </p>

<p>I thought about a multinomial logistic regression, but I am not quite sure whether I can actually apply it to my data.</p>

<hr>

<p><em>Edit</em>: I have followed <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0ahUKEwj30O-9u8zJAhXIqg4KHWlwDjQQFggsMAI&amp;url=https%3A%2F%2Fstatsthewayilikeit.files.wordpress.com%2F2015%2F05%2Fmultinomial-logistic-regression.docx&amp;usg=AFQjCNHj4IdMOAdjfavqcd-Q8HG66vYoag&amp;sig2=ScEoSVJGu1dDvMyIiAyljQ&amp;cad=rja"" rel=""nofollow"">this Word document</a> to do a multinomial logistic regression. An example of one of my plots after performing the regression is reproduced below (the x-axis is in log10 for the <code>Birth Rate</code>):  </p>

<p><a href=""http://i.stack.imgur.com/jjITQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jjITQ.png"" alt=""enter image description here""></a></p>

<p>My code in R:</p>

<pre><code>all.m        &lt;- multinom(code_Final_Info ~ Model+Birth_Rate+Inact_Rate+Del_Rate1+Time, 
                         data=all.mod)
PredProb     &lt;- cbind(preds, predict(all.m, newdata=preds, type='probs', se=TRUE))
PredProbMelt &lt;- melt(PredProb, value.name=""Probability"", 
                     id.vars=c(""Model"",""Birth_Rate"",""Inact_Rate"",""Del_Rate1"",""Time""))

(p &lt;- ggplot(PredProbMelt, aes(x=Del_Rate1, y=Probability, colour=Time)) + 
                           geom_line() +
                           facet_grid(variable ~., scales=""free"") + 
                           theme_bw()) 
</code></pre>

<p>My data.frame looks like:  </p>

<pre><code>&gt; str(all.mod)
'data.frame':   900000 obs. of  6 variables:
 $ Model          : num  0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 ...
 $ Birth_Rate     : num  -4.05 -4.05 -4.05 -4.05 -4.05 ...
 $ Inact_Rate     : num  -3.14 -3.14 -3.14 -3.14 -3.14 ...
 $ Del_Rate1      : num  -4.26 -4.26 -4.26 -4.26 -4.26 ...
 $ code_Final_Info: Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Time           : Factor w/ 3 levels ""0T250"",""1T500"",..: 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>
"
"0.139572631559771","0.0881305298478463","185757","<p>I'm doing a meta-analysis which involves some multiple logistic regression.  As it's a meta-analysis, I'm compiling data from various studies, which therefore differ slightly in their methodology.  One particular aspect of methodology is likely to affect the response variable, and I therefore decided to include it as a random effect in my models. </p>

<p>However, now I'm being asked to provide more information about the likely effect of this predictor.  Is it significant?  How strong is the effect?  The only way I can think to do so is to treat it as a fixed effect and see if adding/removing it improves the model or not, using F tests to compare models (they are quasi-binomial models).  However, I had a feeling that treating variables as both random and fixed variables was wrong - it should be one of the other.  I've never quite got a grip on the difference between the two, and would appreciate any advice.
Thanks</p>

<p>edit: Here's a bit more info about my study.  In my meta-analysis I'm comparing the results of other studies to my own study.  The response in the model is the similarity in the data.  Regarding this particular aspect of methodology, I used ""method A"". If all the other studies used ""method A"" it would be fine, but some used ""method A"", some ""method B"", some ""method C"" and some ""method D"". Each different method will introduce bias, but may do so to a different degree.  Hope that makes sense.</p>
"
"0.0882734829504749","0.0696733014291618","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.158260488401999","0.116585732444878","185926","<p>I'm struggling a little bit with comparing these two classification methods. Although I know it is often a bad idea to use stepwise logistic-regression, I still want to perform it and analyse the difference. I had different approaches in mind. My data set contains about 2500 observations and 40 feature variables.</p>

<ol>
<li><p>Split data randomly into training testing set. For example 80%/20 and run a classification tree and stepwise logistic regression (using different information criteria) on the training set and then evaluate it on the test set</p></li>
<li><p>However, since the size of the trees and the number of feature variables selected by the stepwise regression vary, I thought, it would be a good idea to run cross-validation. However, this is kind of tricky to me. Let's say I try to run a 5-fold CV on my 80% training data. I can evaluate my models within the cross-validation and get for example averaged accuracy and other performance measures for the different models (classification tree and logistic regression). But, how can I use that since I still want to evaluate the test model?</p></li>
<li><p>Use all my data to run cross-validation and then take average performance measures as final results to interpret.</p></li>
</ol>

<p>Are these legitimate approaches? Or at least some of them? What would you recommend? Thank you in advance for your help!</p>
"
"0.0379868588198793","0.0539687072220866","185937","<p>Suppose I have a (labeled) date, where features have many categories. For example, one can take kaggle's Wallmart dataset <a href=""https://www.kaggle.com/c/walmart-recruiting-trip-type-classification/data"" rel=""nofollow"">https://www.kaggle.com/c/walmart-recruiting-trip-type-classification/data</a>. Usually I make one-hot-encoding and then PCA in order to reduce space dimensionality. However, when number of categories is too high, this seems  infeasible. Can anyone suggest please how to deal with such data?</p>

<p>To be more precise, suppose we are given a labeled (0/1) dataset of ~250k observations with 5-10 categorical features each of which has ~10k categories. If I use one hot encoding for all features, the design matrix will have ~2.5 billions of entries, which cannot be allocated in RAM. Although, I can use sparse representation, I do not know whether it is possible to process it in such form.</p>

<p>For example, for kNN approach I can provide a simple metric $d(x,y)=\sum \{x \neq y\}$, which solves the issue. However, how can I apply logistic regression, svm or some other approach here?</p>
"
"0.0657951694959769","0.0311588476424878","186219","<p>in the programs I know (i.e. R statistics) the default numbers of groups to use in Hosmer and Lemeshow test (goodness-of-fit for models, especially logistic regression) is set to 10. I wonder why or if I should define another number for my data. Because I also found that the number of groups is really important for the p-value which results from this. </p>

<p>Does it depend on the amount of data, the degrees of freedom, or something else?  </p>
"
"0.0657951694959769","0.0311588476424878","186259","<p>My situation is the following; I'm running a classification tree (with the function rpart in R) and a logistic regression on a data set using 10 Fold cross-validation. Since I'm estimating the model for each combination of folds, my idea was to show the most important variables for each combination. For the classification tree, I automatically get the important variables. However, since I am running a logistic model using 39 variables, this is a little bit tricky. If someone has any ideas, I would appreciate it a lot!  Thank you</p>
"
"0.113960576459638","0.0539687072220866","186464","<p>I am working on a data set (n= 230) with a categorical dependent variable (outcome: 0/1) and six categorical independent variables (mostly, with only two levels). </p>

<p>There is a certain degree of multicollinearity between two variables (X1 and X6. Anova model comparison shows that a model with X1 performs slightly better than one containing X6) and <strong>a quasi-complete separation issue regarding X4</strong> (due to an empty cell).</p>

<p>I first ran a Random Forest model (all variables were included. Ntree = 5000, mtry = 3). The result was that X1, X2 and X3 are by far the most significant predictors. X4, X5 and X6 seem to have almost no discriminative power (especially X4 whose value  in vimp() is 0.00).The model seems to be reliable (C = 0.73).  </p>

<p><strong>Question 1</strong>: does it make sense at this point to fit Binary Logistic Regression only on the most important predictors obtained through the Random Forest model (X1, X2, X3) without even considering the other three?</p>

<p><strong>Question 2:</strong> In order to avoid the separation problem with Binary Logistic Regression would it make sense to get rid of X4? 
I am quite sure that the empty cell is a bias of my data set. Moreover, this category as a whole represents only 3% of the data (The contingency table is a: 140 <strong>b:0</strong> c:86 <strong>d:6</strong>).</p>
"
"NaN","NaN","186560","<p>I am fitting a multinomial logistic regression using the glmnet package in R:</p>

<pre><code>library(glmnet)
data(MultinomialExample)
cvfit=cv.glmnet(x, y, family=""multinomial"", type.multinomial = ""grouped"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/gF135.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gF135.png"" alt=""rplot""></a></p>

<p>What is ""Multinomial Deviance"" and how does it relate to ""<a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"" rel=""nofollow"">Multinomial Logloss</a>""?</p>
"
"0.0657951694959769","0.0311588476424878","186800","<p>For Binary Classification / Logistic Regression Models, Is there a specific preference or standard of what metric to be used for comparison of 2 models, especially when the model types are different - e.g logistic regression vs svd vs gbm vs neural networks? </p>

<p>As I understand AUC is the one used most frequently. Is there some documentation that compares these from a practical standpoint?  </p>

<p>Thanks,
Manish </p>
"
"0.227921152919276","0.107937414444173","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"NaN","NaN","187443","<p>I wanna run sample selection on my data. My response is binary and also, I have some continuous and some binary variables. I was wondering how can I use ""sampleSelection"" package? 
It seems that it does not support logistic regression as well as factor variables.
Any thought?</p>

<p>PS. My goal is modeling my data using binomial logistic regression.</p>
"
"0.131590338991954","0.0623176952849756","187468","<p>I have a question about analyzing a dataset that I'm currently working with. Each row of the dataset represents an individual songbird, and its reproductive success over the course of a breeding season. Reproductive success was recorded as a score or rank that was based on breeding activities that we observed for each bird. Scores were recorded as follows:</p>

<pre><code>1 - unpaired
2 - paired
3 - successfully raised 1 brood of fledglings
4 - successfully raised 2 broods
5 - successfully raised 3 broods
</code></pre>

<p>In my analysis, these scores will be the response variables, and several environmental covariates will be used as predictor variables. Typically I see that ranked data is analyzed using ordinal logistic regression, but would it also be reasonable to model this data using a poisson glm/glmm? I have experience with poisson glm's, and this distribution is often used in my field (wildlife ecology) for count data or to model age structure. This reproductive index is not commonly used, so there are not many examples I've come across that attempted to do a similar analysis.</p>

<p>Thanks!
Jay</p>
"
"0.0986927542439653","0.0623176952849756","187658","<p>I ran a logistic regression in R using driving data from about 10,000 people. The model included age, years of driving experience, as well as 4 driving test results. The dependent variable was whether or not they had been involved in a crash recently (yes or no, a categorical variable). </p>

<p>The coefficients of the model are given below:</p>

<pre><code>                    Estimate     Std.Err       z value     Pr(&gt;|z|)    
(Intercept)        -1.450041     0.207144      -7.000      2.56e-12 ***
riding experience  -0.014115     0.003697      -3.818      0.000134 ***
age                -0.034544     0.003608      -9.575       &lt; 2e-16 ***
test 1              0.261485     0.088645       2.950      0.003180 ** 
test 2              0.090102     0.051328       1.755      0.079184 .  
test 3              0.228918     0.073666       3.108      0.001887 ** 
test 4              0.070106     0.063652       1.101      0.270729    
</code></pre>

<p>Firstly, with 10,000 people am I right in thinking that p-values aren't going to be that useful?</p>

<p>I calculated the probabilities of being involved in a crash with a 1 unit increase in each variable by doing <code>exp(variable)</code> to get the odds and then, <code>probability = odds/(1+odds)</code>. It gave me:</p>

<pre><code>(Intercept)        0.1899952          
ridingexp          0.4964712
age                0.4913648
test 1             0.5650012
test 2             0.5225104
test 3             0.5569810   
test 4             0.5175193
</code></pre>

<p>These seem awfully high! It is like saying that an increase in age of 1 year makes you 49% less likely to be involved in a crash? Surely that can't be right.</p>
"
"0.0930484210398471","0.0440652649239232","187679","<p>I'd like to test two classifiers at the same time, that is logistic regression and classification trees. To find a classification threshold, which for example maximises F1-score, I split my data set into train, validation and test set. Because this is all pretty new to me, I wrote my own loop to understand the procedure behind it. I was wondering, how would you find manually (without just using cross-validation from the caret package) the optimal cp-value? Is the optimal cp the average of all the cp picked for each fold? But if so, what do I need the validation set for? In logistic regression, this is more clear to me since I need it to find the threshold which maximises my F1 score. I appreciate your help!</p>
"
"0.05884898863365","0.0557386411433294","187796","<p>I've been trying to fit exactly the same logistic regression model (same data) in SAS and R. As far as the coefficients are concerned I didn't notice any differences. 
However, when I tried to perform some of the Goodness of fit tests (Pearson residuals and Deviance residuals GOF tests ) I noticed there is huge difference on how they are computed.
It's hard to bring in some reproducible data here but that's my output:</p>

<ol>
<li>R

<blockquote>
  <p>1 - pchisq(deviance(modelx),df.residual(modelx))</p>
</blockquote></li>
</ol>

<p>[1] 0.0003661318</p>

<blockquote>
  <p>1 - pchisq(sum(residuals(modelx, type = ""pearson"")^2),df.residual(modelx))</p>
</blockquote>

<p>[1] 0.4574779</p>

<blockquote>
  <p>deviance(modelx)</p>
</blockquote>

<p>[1] 3284.208</p>

<blockquote>
  <p>df.residual(modelx)</p>
</blockquote>

<p>[1] 3015</p>

<blockquote>
  <p>sum(residuals(modelx, type = ""pearson"")^2)</p>
</blockquote>

<p>[1] 3022.632</p>

<p>While in SAS its:</p>

<p>Criterion | Value | DF | Value/DF | Pr. > chi-sq.</p>

<p>Deviance | 2347.8792 | 2116 | 1.1096 | 0.0003 </p>

<p>Pearson | 2126.1138 | 2116 | 1.0048 | 0.4343 </p>

<p>the probabilities are similar but values and the degrees of freedom are completely different. </p>

<p>I've read that both the statistic and DF in SAS are calculated using ""profiles"" (<a href=""http://support.sas.com/resources/papers/proceedings14/1485-2014.pdf"" rel=""nofollow"">http://support.sas.com/resources/papers/proceedings14/1485-2014.pdf</a>, page 3) but I still don't understand how those profiles are calculated - I have 7 predictors in my data, each with 3,4,5,5,5,6,6 categories - or why one would use profiles at all.</p>

<p>Any ideas?</p>
"
"0.149209419390598","0.0824385620013739","187963","<p>I have a fundamental question about cross-validation in logistic regression. I would really appreciate some help since something is still unclear to me. My situation is the following: I split my data set into training, validation and test set. When using for example rpart for classification trees on a training set, it automatically splits it into k-folds (basically creating a validation set) and suggests an optimal complexity parameter. I can then run the suggested tree on my test set. However, if I run a logistic regression. Let's say I run a stepwise regression model (although I know stepwise regression model have to be used with caution). How can I use cross-validation to improve my model? Since, by using different folds, I will get models with different numbers of features. How can I choose one to eventually run it on my test set? Thank you very much in advance!</p>
"
"0.153522062157279","0.0830902603799674","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"NaN","NaN","188399","<p>I would like to train a model that has a probability (a success rate between 0 and 1) as outcome.</p>

<p>So the data looks like this:</p>

<pre><code>feature1  feature2   success_rate
0.1       0.3        0.55
0.3       0.6        0.45
</code></pre>

<p>I started using <em>xgboost</em> (gradient boosting machine) with:</p>

<pre><code>""objective"" = ""reg:logistic""
""eval_metric"" = ""auc""
</code></pre>

<p>which means I doing a logistic regression using the Area Under the Curve (AUC) as evaluation function to measure the improvement of the model.</p>

<p>But I understand a logistic regression is usually trained with a categorical target (success or failure), not a probability.
Does this matter? and is this the right approach?</p>
"
"0.139572631559771","0.0881305298478463","188661","<p>I have cross-sectional data with ~800 individuals nested within 6 countries, across 3 time points. Each individual is sampled only once, so time is cross-sectional here too. The number of individuals within each country ranges between ~75 to ~200. </p>

<p>I would like to use fixed effects logistic regression to model the data (the outcome variable consists of the number of successes out of the total number of trials). I know I can use a conditional likelihood estimator to avoid incidental parameter bias, but this does not allow me to report the results as unconditional predicted probabilities (the best format for my audience). </p>

<p>I'm therefore considering including dummy variables for countries in the model. With the small number of countries and large number of observations within each, is this a reasonable approach? </p>

<p>The model (in <code>R</code> syntax) would look like this:</p>

<pre><code>glm(cbind(success, total - success) ~ var1 * factor(time) + factor(country),
    family = bimonial(link = ""logit""),
    data = dat)
</code></pre>

<p>where <code>factor(country)</code> produces 5 dummy variables for the 6 countries and <code>factor(time)</code> produces 2 dummy variables for the 3 time periods. I'm interested in how the slope of <code>var1</code> changes across the 3 time periods, while controling for the clustering by country.</p>

<p>(Note: I get very similar results to that obtained from the conditional likelihood approach, but I can report unconditional predicted probabilities.)</p>
"
"0.139572631559771","0.0881305298478463","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.189934294099397","0.0989426299071587","189903","<p>I am conducting logistic regression analysis: The data includes 107 observations, dependent variable is a binary one, there is about 5 covariates which are both continuous, binary and multi-categorical variables. I want to use some cut_off points to predict the outcome.</p>

<p>So basicly, I select one cut_off point (based on the requirement that the sensitivity >70% and specificity > 70%). Then I devide my data into train set (85% data points) and test set (15% data points).</p>

<p>I fit the model with 5 covariates on the train set, and use the model to predict the outcome on the test set. I use the glm() function to fit the model, and glm.predict() function to predict on test sets. </p>

<p>Since there is missing data, I create 40 imputed data sets using MICE package in R. The procedure above is repeated over 40 imputed data sets. For each data set, I obtain the mis-classification errors.</p>

<p>So, to get the overall mis-classification errors, I averaged over 40 mis-classification error rates.</p>

<p>My question is: </p>

<p>How to assess the variability of this overall mis-classification errors?</p>

<p>As I am thinking that we can not use the usual formula to calculate the variance for this number, as the mis-classification errors over different imputed data sets might be correlated to each other.</p>

<p>Does any one have a suggestion or reference to do this?
(I am using R).
Thank you for any inputs.</p>
"
"0.147122471584125","0.0696733014291618","190389","<p>I built a conditional logistic regression with the function clogit (package survival) in R and in which I included one categorical independent variable (habitat type) with 15 levels. I noted that the sign of parameter estimates changed between models that were built for each level of the categorical independent variable and a model that included the categorical variable (thus, all levels). Contrary to the model including the categorical variable, the results of models for each level of the categorical variable made sense from a biological standpoint. Does sign changes signify a multicollinearity issue? However, in my case, the values of VIFs for each level of the categorical variable were &lt; 3. Should I group some levels of my categorical variable because I noted the levels that were significant, were often those with few observations ?</p>
"
"0.0657951694959769","0.0623176952849756","190403","<p>I am working on a dataset that has 300+ predictors and the dependent variables is very imbalanced (99:1). I need to have a prediction accuracy to show to my client.Here is my analytical process. </p>

<ol>
<li>clean data: remove incomplete columns and rows, then I have 80% of rows remaining and 100+ predictors. </li>
<li>use LASSO: use LASSO with logistic regression to generate the model (by setting up train and testing sets).
Then I have problem finding the best cut points. Below is the accuracy stats for the prediction in testing set if I set cut point as 50%:</li>
</ol>

<p><code>
pred   0   1
    0 825  36
    1  23  43
</code></p>

<p>The prediction accuracy is too low and I am wondering if it could be improved by choosing different cut points.</p>

<p>Appreciate any helps and suggestions.
Thanks.</p>
"
"0.0832250378576479","0.0886796350347864","191063","<p>need help.
I use <code>rms</code> and can't understand different between <code>orm</code> and <code>lrm</code> when i used <code>contrasts</code>. For example:</p>

<pre><code>x &lt;- factor(rbinom(100,2,0.6), labels = c(""a"",""b"",""c""), ordered = TRUE)
y &lt;- factor(rbinom(100,1,0.5), labels=c(""no"",""yes""))
l&lt;-lrm(x~y);l
Logistic Regression Model

lrm(formula = x ~ y)
                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       
Obs           100    LR chi2      0.51    R2       0.006    C       0.529    
 a             24    d.f.            1    g        0.133    Dxy     0.059    
 b             40    Pr(&gt; chi2) 0.4764    gr       1.143    gamma   0.117    
 c             36                         gp       0.024    tau-a   0.039    
max |deriv| 1e-10                         Brier    0.181                     

      Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=b   1.0188 0.2988  3.41  0.0007  
y&gt;=c  -0.7162 0.2884 -2.48  0.0130  
y=yes  0.2642 0.3715  0.71  0.4769

o&lt;-orm(x~y);l;o
Logistic (Proportional Odds) Ordinal Regression Model

orm(formula = x ~ y)
                     Model Likelihood          Discrimination          Rank Discrim.    
                        Ratio Test                 Indexes                Indexes       
Obs           100    LR chi2      0.51    R2                  0.006    rho     0.071    
 a             24    d.f.            1    g                   0.133                     
 b             40    Pr(&gt; chi2) 0.4764    gr                  1.143                     
 c             36    Score chi2   0.51    |Pr(Y&gt;=median)-0.5| 0.259                     
Unique Y        3    Pr(&gt; chi2) 0.4766                                                  
Median Y        2                                                                       
max |deriv| 7e-05                                                                       

      Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=b   1.0188 0.2988  3.41  0.0007  
y&gt;=c  -0.7162 0.2884 -2.48  0.0130  
y=yes  0.2642 0.3715  0.71  0.4769  
</code></pre>

<p>We can see, that results <code>orm</code> and <code>lrm</code> are equal. When we use <code>contrasts</code> results are different:</p>

<pre><code>contrast(l,list(y=""no""),list(y=""yes""))
     Contrast      S.E.      Lower     Upper     Z Pr(&gt;|z|)
11 -0.2642454 0.3714673 -0.9923081 0.4638172 -0.71   0.4769
Confidence intervals are 0.95 individual intervals
</code></pre>

<p>and </p>

<pre><code>contrast(o,list(y=""no""),list(y=""yes""))

Contrast      S.E.      Lower   Upper    Z Pr(&gt;|z|)
11 0.7545878 0.3714672 0.02652544 1.48265 2.03   0.0422

Confidence intervals are 0.95 individual intervals
</code></pre>

<p>Why <code>orm</code> contrast aren't equal beta regression coefficient as <code>lrm</code> contrast? p.s. sorry for bad English</p>
"
"0","0.0311588476424878","191434","<p>I want to do a logistic regression simulation using R </p>

<p>I use this code</p>

<pre><code>set.seed(666)
age = rnorm(60)         
blood_pressure = rnorm(60)
race = sample(c(rep(1,30),rep(0,30)))
inactivity = sample(c(rep(1,30),rep(0,30)))
weight = rnorm(60)

z=1+1*age+blood_pressure*2+3*weight+3*inactivity+0*race
pr=exp(z)/(1+exp(z))
y=rbinom(60,1,pr)

df = data.frame(y=y,age,blood_pressure,inactivity,weight,race)
glm(y~age+blood_pressure+inactivity+weight+race,data=df,family=binomial(link='logit'),control = list(maxit = 50))
</code></pre>

<p>I got very strange result from it.</p>

<pre><code>Coefficients:
   (Intercept)             age  blood_pressure      inactivity          weight            race  
        -39.75           46.64          106.65          143.52          229.75          100.87  
</code></pre>

<p>And it says the model doesn't converge.</p>

<p>Does someone know what's wrong and how to fix it?</p>
"
"0.1176979772673","0.0696733014291618","191506","<p>My dependent variable has 4 categories, but when I run the multinomial logistic regression using the package <code>nnet</code> with function <code>multinom</code> the results only show 3 categories. </p>

<p>I've tried changing the category numbers from 0,2,3,4 to 1,2,3,4, and also tried using names instead of numbers for the categories but it still wont show all 4 categories in the results. </p>

<p>Also, when I changed the categories to names instead of numbers, the resulting p values for each category drastically changed. Why is this? 
The p values were acquired using these commands</p>

<pre><code>z &lt;- summary(siglm)$coefficients/summary(siglm)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
p
</code></pre>
"
"0.0930484210398471","0.0440652649239232","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.299803051256798","0.219052967867616","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.175844987113332","0.108258180127387","191891","<p>I'm hoping someone can help clarify a few things for me.</p>

<p>I ran some relatively simple logistic regressions in r and am having trouble with interpretation.  I'm interested in the effects of elevation and a species diversity index on the presence/absence of a disease in individual animals.</p>

<p>I ran a simple model of: <code>Result~Elevation+Diversity</code> which gave this result</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation + Simpsons_Diversity, family = binomial, 
    data = XXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8141  -0.6984  -0.5317  -0.4143   2.3337  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.118e+00  1.594e-01 -13.289  &lt; 2e-16 
Elevation           1.316e-04  2.247e-05   5.855 4.76e-09 
Simpsons_Diversity -9.907e-01  2.725e-01  -3.635 0.000278 

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2923.6  on 3297  degrees of freedom
AIC: 2929.6
</code></pre>

<p>I have a strong suspicion that diversity decreases with increasing elevation which I have confirmed although the relationship isn't quite as strong as I thought. When I run a model with an interaction term <code>elevation*diversity</code> I get:</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation_1000 + Simpsons_Diversity_100 + 
    Elevation_1000 * Simpsons_Diversity_100, family = binomial, 
    data = XXXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7908  -0.6959  -0.5437  -0.3963   2.4215  

Coefficients:
                                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                           -2.014422   0.179507 -11.222  &lt; 2e-16 
Elevation_1000                         0.112466   0.027433   4.100 4.14e-05 
Simpsons_Diversity_100                -0.015851   0.005780  -2.743   0.0061  
Elevation_1000:Simpsons_Diversity_100  0.001408   0.001200   1.173   0.2406   

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2922.2  on 3296  degrees of freedom
AIC: 2930.2

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Showing that adding the interaction term doesn't really help the fit of the model (AIC = 2930) and the interaction term itself is not significant (p-value=0.24).</p>

<p>Am I on the right track so far?</p>

<p>If I am, I understand how to convert coefficients to odds ratios and interpret those.  My main question is can I plot the predicted probabilities for a combination of elevation and diversity where each variable is allowed to vary? Or is this essentially plotting the interaction?  </p>

<p>I was able to create a dataframe where I varied elevation and diversity and I used my simple non-interaction model to obtain predicted probabilities using the PREDICT fuction) for those combinations, but I want to make sure that I am doing things correctly.  I've attached the plot of predicted probs for different levels of diversity. </p>

<p><a href=""http://i.stack.imgur.com/NINBE.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NINBE.gif"" alt=""Elevation vs. Predicted Probabilities for various levels of diversity)""></a></p>
"
"0.169882397145875","0.120677698006369","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.0657951694959769","0.0311588476424878","193166","<p>I have limited statistic knowledge but I am trying to conduct logistic regression by using a data with 300+ predictors. So I decided to use glmnet and LASSO. Below please see my code:</p>

<pre><code>fit.lasso = glmnet(x, y,family=""binomial"",alpha = 1)
    plot(fit.lasso, xvar = ""lambda"", label = TRUE)
    cv.lasso = cv.glmnet(x,y,family=""binomial"",alpha = 1)
    plot(cv.lasso)
    coef(cv.lasso)
    cv.lasso$lambda.min
        bestlam = cv.lasso$lambda.min
    lasso.pred=predict(fit.lasso,s=bestlam,newx = x,type = ""response"")
</code></pre>

<p>I have two questions and appreciate any helps.</p>

<ol>
<li>(removed since it was more related to purely programming question)</li>
<li>I have used CV to select lambda but I didn't partition the data into training and testing. Is it necessary since I have already used CV? I will need the <code>lasso.pred</code> to compare with actual to calculate the prediction accuracy.</li>
</ol>

<p>Thank you in advance!</p>
"
"NaN","NaN","193419","<p>What is the implication if I don't fix a logistic regression that has complete or quasi separation? can I still read the marginal effects or are they not going to be valid? </p>

<p>My exercise is actually to just find out which independent variables are most predictive of Y. </p>

<p>I read some responses to complete/quasi-separation and I tried using logistf package for R but got this error message ""NA/NaN/Inf in foreign function call logistf"". why does this arises? </p>
"
"0.0657951694959769","0.0623176952849756","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"0.113960576459638","0.0539687072220866","194582","<p>I'm attempting <strong>one-to-one exact</strong> matching in R. That is, I have 60,000 treatment observations and 200,000 control observations but I only want 60,000 control observations and I want those to be matched exactly to the treatment set on a set of variables; normally this would reduce N greatly but it will not in this case. The <strong>one-to-one</strong> aspect of this can be done with nearest-neighbor matching in R's MatchIt package but that method isn't <strong>exact</strong>; it seems to rely on logistic regression propensity scores. MatchIt can also do <strong>exact</strong> matching but then it's not <strong>one-to-one</strong>. Does MatchIt, or another package, implement <strong>exact one-to-one</strong> matching? I realize I can probably create the solution myself with a lot of code but this seems like something that might already exist.</p>
"
"0.161164592805076","0.0763232776972177","194867","<p>At several time points, I sample different individuals from a population (say 60 ind/time point). I assign to each of them one category (either low &lt; middle &lt; high). I then want to model this ordinal dependent variables using several covariates including time (to evaluates temporal changes in proportions/probabilities). </p>

<p>If I had only one time point, I would use ordered logistic regression in R with polyR, vglm... But these time series, I fear to have residual autocorrelation. Can I easily check residual autocorrelation with these models? Should I include the lagged dependent variable as a covariate to remove it? What are the different alternatives to analyse this kind of data in R?</p>

<p>For the moment, I analysed separately each of the categorical variable level with binomial regression and the lagged dependent variable as one of the covariates but hope to find a better solution.</p>
"
"0.123091490979333","0.116585732444878","195293","<p>I thought I understood this issue, but now I'm not as sure and I'd like to check with others before I proceed.</p>

<p>I have two variables, <code>X</code> and <code>Y</code>. <code>Y</code> is a ratio, and it is not bounded by 0 and 1 and is generally normally distributed. <code>X</code> is a proportion, and it is bounded by 0 and 1 (it runs from 0.0 to 0.6). When I run a linear regression of <code>Y ~ X</code> and I find out that <code>X</code> and <code>Y</code> are significantly linearly related. So far, so good.</p>

<p>But then I investigate further and I start to think that maybe <code>X</code> and <code>Y</code>'s relationship might be more curvilinear than linear. To me, it looks like the relationship of <code>X</code> and <code>Y</code> might be closer to <code>Y ~ log(X)</code>, <code>Y ~ sqrt(X)</code>, or <code>Y ~ X + X^2</code>, or something like that. I have empirical reasons to assume the relationship might be curvilinear, but not reasons to assume that any one non-linear relationship might be better than any other. </p>

<p>I have a couple of related questions from here. First, my <code>X</code> variable takes four values: 0, 0.2, 0.4, and 0.6. When I log- or square-root-transform these data, the spacing between these values distorts so that the 0 values are much further away from all the others. For lack of a better way of asking, is this what I want? I assume it isn't, because I get very different results depending on the level of distortion I accept. If this isn't what I want, how should I avoid it?</p>

<p>Second, to log-transform these data, I have to add some amount to each <code>X</code> value because you can't take the log of 0. When I add a very small amount, say 0.001, I get very substantial distortion. When I add a larger amount, say 1, I get very little distortion. Is there a ""correct"" amount to add to an <code>X</code> variable? Or is it inappropriate to add <em>anything</em> to an <code>X</code> variable in lieu of choosing an alternative transformation (e.g. cube-root) or model (e.g. logistic regression)? </p>

<p>What little I've been able to find out there on this issue leaves me feeling like I should tread carefully. For fellow R users, this code would create some data with a sort of similar structure as mine.</p>

<pre><code>X = rep(c(0, 0.2,0.4,0.6), each = 20)
Y1 = runif(20, 6, 10)
Y2 = runif(20, 6, 9.5)
Y3 = runif(20, 6, 9)
Y4 = runif(20, 6, 8.5)
Y = c(Y4, Y3, Y2, Y1)
plot(Y~X)
</code></pre>
"
"0.0986927542439653","0.0623176952849756","196586","<p>I would like to do a gene x environment interaction analysis in a matched (1-1) case control samples. I referred all related previous publications and in most of the papers authors used either STATA or SAS. I got few references for performing conditional logisitic regression in R, for example using survival (clogit) package. But I couldn't find any reference for adding interaction terms in conditional logistic models in R. Can someone help me with references for interaction analysis using conditional logistic regression in R?</p>
"
"0.113960576459638","0.0539687072220866","196628","<p>I am struggling with answering a question on how i should handle the vast number of NAs in my data. It is a behavioural study of the impact of traffic on certain mammals and i have approximately 500 rows (one for each type of vehicle) across 3 years. However, there is variable stating if there was a 'behavioural response' or not (Y/N). This I had planned to use in a 'binary logistic regression' as the response variable. However, many of the rows indicate no response and therefore the continuous variables in the dataset (distance to vehicle etc..) have not been filled in, leaving many blanks!</p>

<p>Should I be performing the regression based on a small subset of the data e.g. fewer rows? and if so does this have to only be the complete rows. Any help on how to proceed with this problem would be appreciated</p>
"
"NaN","NaN","196734","<p>I have a panel data set with binary dependent variable of 20,000 observations and 11 independent variables.  I ran a logistic regression with fixed effects and the model returns maximum log likelihood value of <code>-7417.845</code> and AIC equals <code>Inf</code>. I am not sure why here the AIC value goes to infinity?</p>

<p>I am using R package ""<a href=""https://cran.r-project.org/web/packages/glmmML/glmmML.pdf"" rel=""nofollow"">glmmML</a>"".</p>
"
"0.124341182825498","0.0824385620013739","196829","<p>I have a dataset with 15 binary covariates and a continuous response variable bounded between 0 and 1. The binary variables represent correct or incorrect answers on a short test and the response variable is a measure of the same test takers performance on a related but more advanced and reliable test. I would like to select the best variables and weights to predict the score on the more advanced test. What would be the best way of doing this?</p>

<p>PS. I'm not a statistician but a computer scientist with only basic statistics and machine learning in my portfolio.</p>

<p>(Side note: One idea I had was to use some kind of logistic L1 or L2 regularized regression, however, glmnet does not seem to accept non-binary response variables when fitting a logistic model, which I guess is reasonable for normal use. The built-in glm function does accept a (0,1)-bounded response but does not perform regularization. If this approach seems reasonable, any tips on suitable packages or would I have to implement it myself? Other ideas I had was using ""normal"" regularized regression, or perhaps Principal Component Regression, however, I have tried both these and they give very different results and neither perform very well.)</p>
"
"0.0465242105199235","0.0440652649239232","198219","<p>I ran the same Logistic regression with R and STATA. </p>

<p>The regressors include many dummy variables.</p>

<p>In R, the code I used is</p>

<pre><code>fit &lt;- glm(formula = y ~ ., family = ""binomial"", data = df)
</code></pre>

<p>which reports the warning message:</p>

<pre><code>glm.fit: algorithm did not converge 
</code></pre>

<p>In STATA, I simply ran</p>

<pre><code>Logit y x1 - x20
</code></pre>

<p>and the reported table looks OK and the estimation seems to be reasonable.</p>

<p>Actually if I have only a few regressors, say only $x_1, x_2, x_3$, they report the same result.</p>

<p>I'm wondering why there could be such difference? In R, how to fix the problem?</p>

<p>Thanks a lot!</p>
"
"0.175453785322605","0.0934765429274634","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.113960576459638","0.0539687072220866","198374","<p>This is a question for those out there working in data scientist roles within your organizations. How many variables in acceptable to use within models that are going to be deployed in production for marketing or other purposes?</p>

<p>The reason I ask is this, we have four analysts, two of whom have been with the company for 10 years, and two of us who are recently graduated with our Masters in Predictive Analytics. Our senior analysts primarily build with linear / logistic regression models, and think that using the least amount of variables (regardless of technique) is always best, usually trying to use around 10-15 variables.</p>

<p>Us newer analysts work primarily with random forest and xgboost, and are comfortable using 100-800 variables in our models. I havnt encountered anything to say that using this many variables in random forest or xgboost should cause any concern, but we cannot come to an agreement. Even if holdout results are better using 100+ variables, we are still encouraged to use less.</p>

<p>Can anyone provide any information regarding this topic that might help shape our decision making progress?</p>

<p>Thank you,</p>

<p>Nate</p>
"
"0.208927723509336","0.107937414444173","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0746047096952991","0.0588846871438385","198925","<p>Although there has been some detailed discussions about power analysis on this website (for example <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">here</a> and <a href=""http://stackoverflow.com/questions/27234696/how-do-you-conduct-a-power-analysis-for-logistic-regression-in-r"">here</a>), the answer provided to this question has  outlines the steps to simulating a power analysis, <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>.</p>

<p>Say we take some data (data was linked to a <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">bootstrapping question</a>)</p>

<p>We create a regression that will predict <code>admit</code> based on the two continous variables <code>gpa</code> and <code>gre</code></p>

<ul>
<li>Now we have a <code>n=400</code>. </li>
<li>We can then elect our power level, <code>alpha = 0.5</code></li>
<li>The effect size you would like to detect, e.g., odds ratios  (we obtain this from our regression)</li>
</ul>

<p>So in following the detailed method provided by @gung <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>, I want to run the simulation. Here is the code I have adjusted, but my output is not correct. Can someone outline what I have not understood</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
head(mydata)

set.seed(1234)

my.mod &lt;- glm(admit ~ gre + gpa , data = mydata, family = ""binomial"")


repetitions &lt;- length(mydata$admit)

gre &lt;- mydata$gre
    gpa &lt;- mydata$gpa


significant = matrix(nrow=repetitions, ncol=4)

for(i in 1:repetitions){
  responses          = mydata$admit
      #responses          = rbinom(n=N, size=1, prob=mydata$admit)      # we can interchange this comment
  model              = glm(responses ~ gre + gpa, family = binomial(link=""logit""))
  significant[i,1:2] = (summary(model)$coefficients[2:3,4]&lt;.05)
      significant[i,3]   = sum(significant[i,1:2])
      modelDev           = model$null.deviance-model$deviance
  significant[i,4]   = (1-pchisq(modelDev, 2))&lt;.05
}



sum(significant[,1])/repetitions      # pre-specified effect power for gre

sum(significant[,2])/repetitions      # pre-specified effect power for gpa

sum(significant[,4])/repetitions  # power for likelihood ratio test of model

sum(significant[,3]==2)/repetitions   # all effects power

sum(significant[,3]&gt;0)/repetitions    # any effect power
</code></pre>
"
"0.0657951694959769","0.0311588476424878","199284","<p>I have a large dataset with 4000 variables and 20000 observations. Most of the variables are a variety of moving averages since I am trying to create a predictive model. I'd like to use logistic regression with the best of these 4000 variables. I know this is an absurd amount and I am looking to trim it down to less than ten or so but am unsure of the best way to do it. </p>

<p>I'm aware that using stepAIC or the like will take forever and I need something that can select variables based on the p value or any other criteria.</p>

<p>Any help is appreciated.</p>
"
"0.0882734829504749","0.0557386411433294","199336","<p>I have a dataset with hundreds of individual trees. These individuals were from seven sites that demonstrated different rates of land cover change over time (% change in area over 30 years). I'm looking to test how the rate of land cover change influences the growth trend of trees (outcomes for each individual could be positive, negative, or neutral - these are each columns in my dataset). </p>

<p>A reviewer suggested that logistic regression would be most appropriate for this data, but I have not been able to find a method of performing a multiple logistic regression of some sort that works for multiple binomial predictors. I could run individual logistic regressions for each outcome, however this strikes me as potentially inflating error. Does anyone have suggestions as to a test that could be used for this sort of data, or would it be best to use something like a bonferroni correction?</p>

<p>I'm working in R, if you have any specific suggestions for resources in the R programming environment!</p>

<p>Thanks in advance.</p>
"
"0.0465242105199235","0.0440652649239232","199504","<p>I would like to compare an average probability to a fixed probability value in order to determine if there is a significant difference between the two.</p>

<p>My participants had to detect and point a target that appeared among three distractors. The possibility they answered hazardously therefore is 1 out of 4 (i.e., .25). My dependent variable being binary (Correct answer: 1 ; Incorrect answer: 0) I am using logistic regression for my analyses:</p>

<pre><code>model1 &lt;- glm(Accuracy ~ Task * Masking,
              data = DF,
              family = binomial(link = ""logit""))
</code></pre>

<p>âŸ¶ <a href=""http://i.stack.imgur.com/AyPzG.png"" rel=""nofollow"">Plot of results</a></p>

<p>More precisely, I would like to know if the probability of report in the Reach-Masked condition is significantly different from 0.25.</p>

<p>How do I test this possibility in R?</p>

<p>Thank you for your time. Have a very nice day.</p>
"
"0.0657951694959769","0.0311588476424878","199787","<p>Hi I have a dataset where I have P number of predictors.  One of the predictors is ""Color"".  Color is always either ""Green"" or ""Red"". In the results of the glm call - which was btw - glm(target ~ color + age, data = d, family = binomial(logit)) - 
only ""colorGreen"" is there and it is not statistically significant.  Why does ""colorRed"" not have a row in the results?</p>

<p>Edit:  I am not looking for general logistic regression help.  This question is not a duplicate for another question that is a general question. This is a specific question. </p>
"
"0.1176979772673","0.0696733014291618","199912","<p>I have a large dataset with 4000 variables and 15000 observations. I am looking to build a predictive model using logistic regression. I believe that the glmnet package (using elastic net) is the best tool to use with such a large set of variables. Every variable of the 4000 is a moving average. I have split the dataset into two - training and testing.</p>

<p>When I run the code with glmnet I find something unusual happening. As I increase the number of variables for glmnet to select the model probabilities get more and more extreme which causes the misclassification rate to converge to 0%. I realise something is wrong but I cannot figure what it is.</p>

<p>Here is the code I have used:</p>

<pre><code>x &lt;- as.matrix(training[1:4000])
newx &lt;- as.matrix(testing[1:4000])

model &lt;- cv.glmnet(x, y, alpha = 0.5, family = 'binomial')

predict(model, type = ""coefficients"",s = model$lambda.min)
predict(model, newx, type= ""response"",s = model$lambda.min)
</code></pre>

<p>Is this overfitting?
I also read that categorical variables need to worked around with glmnet - none of the 4000 are categorical but they are grouped by external categorical vars.</p>

<p>I'm desperate for some help!</p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.109658615826628","0.0934765429274634","199978","<p>I've been building a logistic regression model (using the ""glm"" method in caret). The training dataset is extremely imbalanced (99% of the observations in the majority class), so I've been trying to optimize the probability threshold during the resampling process using the train function from the caret package as described in this example of a svm model: <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">Illustrative Example 5: Optimizing probability thresholds for class imbalances.</a></p>

<p>The idea is to get the classification parameters for different values of the probability thershold, like this:</p>

<pre><code>threshold   ROC    Sens   Spec   Dist   ROC SD  Sens SD  Spec SD  Dist SD
 0.0100     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.0616     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1132     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1647     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
  ...        ...                  ...                      ...      ...
</code></pre>

<p>I noticed that the 'glm' method in caret uses 0.5 as the probability cutoff value as can be seen in the predict function of the model:</p>

<pre><code>code_glm &lt;- getModelInfo(""glm"", regex = FALSE)[[1]]
code_glm$predict
    function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata)
                    if(modelFit$problemType == ""Classification"") {
                      probs &lt;-  predict(modelFit, newdata, type = ""response"")
                      out &lt;- ifelse(probs &lt; .5,
                                    modelFit$obsLevel[1],
                                    modelFit$obsLevel[2])
                } else {
                  out &lt;- predict(modelFit, newdata, type = ""response"")
                }
                out
              }
</code></pre>

<p>Any ideas about how to pass a grid of probability cutoff values to the predict function shown above to get the optime cutoff value?</p>

<p>I've been trying to adapt the code from the <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">example shown in the caret website</a>, but I haven't been able to make it work. I think I'm finding difficult to understand how caret uses the model's interfaces... </p>

<p>Any help to make this work would be much appreciated... Thanks in advance.</p>
"
"0.0994729462603988","0.0824385620013739","200031","<p>I have very easy question that I'm hoping someone can assist me with:</p>

<p>I ran an example logistic regression using this R code:</p>

<pre><code>     hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5)
        pass &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
        data &lt;- data.frame(hours, pass)
        mylogit &lt;- glm(pass ~ hours, data = data, family = ""binomial"") #Activates the logistic regression model
        summary(mylogit) #Summary of the model

    Call:
    glm(formula = pass ~ hours, family = ""binomial"", data = data)

    Deviance Residuals: 
         Min        1Q    Median        3Q       Max  
    -1.70557  -0.57357  -0.04654   0.45470   1.82008  

    Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
    hours         1.5046     0.6287   2.393   0.0167 *
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 27.726  on 19  degrees of freedom
    Residual deviance: 16.060  on 18  degrees of freedom
    AIC: 20.06

    Number of Fisher Scoring iterations: 5

    round(exp(cbind(OR = coef(mylogit), confint(mylogit))),3)

               OR 2.5 % 97.5 %
   (Intercept) 0.017 0.000  0.281
    hours       4.503 1.698 23.223
</code></pre>

<p>I know that by taking the exponent of the log-odds/coefficients for hours the odds of passing increase by a factor of 4.503 for a one-unit change in hours.  However, given that the explanatory variable (hours) is continuous, what is considered a 'one-unit change' i.e. going from 1 to 2 hours as one unit? or from 1.75 to 1.76 hours as one unit?  Also, is this interpretation of one-unit the same for regular OLS regression as well? I'm seeking to better understand the rules R applies to creating its regression coefficients.  </p>
"
"0.0657951694959769","0.0311588476424878","200324","<p>I want to run logistic regression to predict binary outcome , however I have 300+ independent variable.</p>

<p>I am new in analytics and statistics , in my opinion first I need to dimension reduciton.</p>

<p>I ran PCA in R and I am getting below error  ""Error in princomp.default(input, scores = TRUE, COR = TRUE) :  covariance matrix is not non-negative definite""</p>

<p>I am not able to resolve above error also in terms of approach if anyone can provide guidance that would be good .</p>

<p>1) what should I do to reduce number of variables , idenitfying powerful predictors..</p>

<p>Thanks in advance</p>
"
"0.0986927542439653","0.0623176952849756","200477","<p>I am conducting multiple imputation by chained equations in R using the MICE package, followed by a logistic regression on the imputed dataset.</p>

<p>I need to compute a 95% confidence interval about the predictions for use in creating a plotâ€”that is, the grey shading in the image at this link.</p>

<p><a href=""http://imgur.com/guLEyTQ"" rel=""nofollow"">http://imgur.com/guLEyTQ</a></p>

<hr>

<p>I followed the approach described in the answer to this question...</p>

<p><a href=""http://stats.stackexchange.com/questions/66946/how-are-the-standard-errors-computed-for-the-fitted-values-from-a-logistic-regre"">How are the standard errors computed for the fitted values from a logistic regression?</a></p>

<p>...which uses the following lines of code to yield the std.er of prediction for any specific value of the predictor:</p>

<pre><code>o &lt;- glm(y ~ x, data = dat)
C &lt;- c(1, 1.5)
std.er &lt;- sqrt(t(C) %*% vcov(o) %*% C)
</code></pre>

<p>But of course <strong>I need to adapt this code to the fact that I am using a model resulting from multiple imputation</strong>.  In that context, I am not sure <strong><em>which</em></strong> variance-covariance matrix (corresponding to â€œvcov(o)â€ in the above example) I should be using in my equation to produce the ""std.er"".</p>

<hr>

<p>Based on the documentation for MICE I see three candidate matrices:</p>

<ul>
<li><p>ubar - The average of the variance-covariance matrix of the complete data estimates.</p></li>
<li><p>b - The between imputation variance-covariance matrix.</p></li>
<li><p>t - The total variance-covariance matrix.</p></li>
</ul>

<p><a href=""http://www.inside-r.org/packages/cran/mice/docs/is.mipo"" rel=""nofollow"">http://www.inside-r.org/packages/cran/mice/docs/is.mipo</a></p>

<p>Based on trying all three, the b matrix seems patently wrong, but both the t and the ubar matrices seem plausible.  Can anybody confirm which one is appropriate?</p>

<p>Thank you.</p>
"
"0.140675989690666","0.0999306278098956","200703","<p>I'm using matched pairs logistic regression (1-1 matched case-control; Hosmer and Lemeshow 2000) to model differences between vegetation selected at nest sites vs. paired random sites. To do this, I created a data frame that contained the difference in vegetation measurements between nest and random sites (so nest minus random) and used R to fit a logistic regression model, using a vector of all 1's as the 'Response' and a no-intercept model.</p>

<p>Here's the data frame (I only include 1 of the covariates, grass density, for the example):</p>

<pre><code>nest&lt;-structure(list(VerGR = c(1.380952381, 1.952380953, 2.666666667, 
-3.809523809, 2.428571428, 2.142857143, 0.142857143, 2.095238095, 
1.952380952, 3.333333334, 3.190476191, -2.857142857, 2.857142858, 
-1.666666667, 0.523809524, 4.761904762, 0.571428571, 2.238095238, 
-2.809523809, 0.857142857, 1.523809524, -2.476190476, -0.428571428, 
-5.190476191, 4.142857143, 2.857142858, -2.476190476, 4.095238096, 
1.428571428, 1.714285714, -2.80952381, 3.142857143, 2.809523809, 
7.238095238, 2.523809523, 2.333333333, -0.095238096, -0.095238096, 
-0.142857143, 4.047619048, 4.761904759, -1.285714285, -1.190476191, 
2.523809524, -2.095238095, -2, 4.761904761, 8.952380952, 1.095238096, 
5.666666666, -0.714285714, 0, 2.809523809, -0.238095239, 3.666666667, 
0.904761905, -4.952380952, -3.666666667, 2, -0.619047619, 4.523809524, 
1.523809524, 4.619047619, 6.142857143, 3.19047619, -2.190476191, 
-1.666666667, 2.714285714, -1.285714286, 2.857142857, 2.761904762, 
2.809523809, -7.142857139, -5.952380949, -1.19047619, 1.523809524, 
-0.38095238, 5.571428571, 5.238095239, 2.047619048, 7.857142857, 
0.61904761, 2.523809524, -1.190476191), Response = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L)), .Names = c(""VerGR"", ""Response""), class = ""data.frame"", row.names = c(NA, 
-84L))
</code></pre>

<p>And the no-intercept logistic regression models I am running:</p>

<pre><code>grass.mod &lt;- glm(Response ~ VerGR - 1, data=nest, family=""binomial"")
grass2.mod &lt;- glm(Response ~ VerGR + I(VerGR^2) - 1, data=nest, family=""binomial"")
</code></pre>

<p>For the most part the models run fine, and give the same parameter estimates as models implemented using the 'clogit' function from the survival R package. The data set for the clogit models is slightly different, with Responses = 1 (nest) or = 0 (random point), and includes a column called 'PairID' to indicate nest-random pairs. Here's what the clogit models look like:</p>

<pre><code>library(survival)
grass.mod.clog &lt;- clogit(Response ~ VerGR + strata(PairID), data=full)
grass2.mod.clog &lt;- clogit(Response ~ VerGR + I(VerGR^2) + strata(PairID), data=full)
</code></pre>

<p>But when I run the glm's, I get these 2 warnings if using a quadratic term:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>I'm able to satisfy the first warning if I use more iterations in the glm formula, but I'm not sure what is happening with the second warning. I would be glad to use the 'clogit' function (which works with quadratic terms), but I'm unsure how to create prediction plots to visually display the data when going that route. Any suggestions?</p>

<p>Thanks,
Jay</p>
"
"0.0657951694959769","0.0311588476424878","200794","<p>I have a problem with outputting the terms for a logistic regression model in R. For a given list of independent values, say list l of terms {w,y,z} to determine dependent variable {x}, I want to find out what the biggest regressor is when we pair two terms together. I want to be able to group multiple independent variables together and say ""when a record has this combination of values, then they have a very strong chance of predicting X"". I tried to just add the interactions when calling the glm function like glm(x~y + w + z + w:z + y:z + y:w, data = l). But the results come out very hard to explain, because of how they are measured between themselves and not just measured against the mean. Does anyone know a way to do this?</p>
"
"0.113960576459638","0.0539687072220866","201015","<p>I am currently working on a project where I need to predict a outcome which on average occurs less number of times (for example, let's say the outcome is that a batter reaches the base in a baseball game. Now on average this event occurs with a probability of roughly 0.25-0.39 and seldom ever goes higher than that or lower than that)</p>

<p>Now the questions that I have for this situation:</p>

<ol>
<li><p>Working towards a logistic regression model to predict this, the output of that model on a test set is some kind of a probability which would roughly be in the same range (0.25-0.39). The question is how to come up with a decision threshold probability for such a low probability such that I can predict the class. Is there a specific algorithm, steps, or method that would be give me this?</p></li>
<li><p>Is having a different decision threshold for each observation a possibility?</p></li>
<li><p>Any other algorithm I can use to tackle this problem?</p></li>
</ol>

<p>Apologies if the question is vague or not well formed. Please let me know if I need to provide any additional information. Also, I am using R.</p>
"
"0.0657951694959769","0.0311588476424878","201019","<p>I'm a newbie in R and statistics and I don't know how to test if the participants of my experiment are more accurate in one condition or in the other.</p>

<p>This is the problem: Let's suppose I test participants in two experimental conditions, A and B. I simply need to verify if they are more correct in the A or B condition.</p>

<pre><code>subject &lt;- c(1,1,2,2,3,3)
condition &lt;- c(""A"",""B"",""A"",""B"",""A"",""B"")
error &lt;- c(1,0,1,0,0,1)
data &lt;- cbind(subject, condition, error)
</code></pre>

<p>Do I have to use logistic regression? And How I can implement that in R?</p>

<p>Thanks</p>
"
"0.0986927542439653","0.0467382714637317","201462","<p>I'm fitting a logistic regression model with <code>patient_group</code> (0,1) as response variable and the explanatory variable being an interaction between two SNPs. When running summary for the model, the alert 'Coefficients: (1 not defined because of singularities)' is shown, and I guess it is due to the fact that the combination AACT has 0 observations. </p>

<p>My question is whether the statistics are still valid, or is there a better way to analyse this kind of data? (The SNPs are located close to each other and are most likely strongly linked.)</p>

<pre><code>&gt; table(data$SNP1, data$SNP2)    
     CC CT
  TT 27  9
  AT 83 14
  AA 47  0
&gt; model &lt;- glm(patient_group ~ SNP1 * SNP2, data=data, family=""binomial"")
&gt; summary(model)
Call:
glm(formula = patient_group ~ SNP1 * SNP2, family = ""binomial"", 
data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2735  -0.9072  -0.7679   1.4742   1.8365  

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    -1.4816     0.4954  -2.991  0.00279 **
SNP1AT          0.8065     0.5471   1.474  0.14048   
SNP1AA          0.4112     0.5978   0.688  0.49158   
SNP2CT          1.7047     0.8339   2.044  0.04093 * 
SNP1AT:SNP2CT  -2.3289     1.0833  -2.150  0.03157 * 
SNP1AA:SNP2CT       NA         NA      NA       NA   

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 218.19  on 179  degrees of freedom
Residual deviance: 212.31  on 175  degrees of freedom
(26 observations deleted due to missingness)
AIC: 222.31

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.1176979772673","0.0696733014291618","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0","0.0311588476424878","202211","<p>I have a logistic regression in R whose goal is to predict the probability of default on some test data. </p>

<p><code>glm(default ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>What I'd like to do is 'bin' this data so that bins 1 to n each have a certain rate of default. How can I bin the logistic regression results in this way? For example, the bins on a sample set of 1000 might look like:</p>

<pre><code>Bin# P(Default) Count
1         4%     400
2         2%     300
3         1%     300
</code></pre>

<p>That is, I set in advance the probabilities I want each bin to have (.04,.02,.01) and then bins are created based around those settings.</p>
"
"0.131590338991954","0.0623176952849756","202447","<p>I often come across a classification problem - where we have 0/1 binary outcome and several features. And the main goal is build a classifier on training set.</p>

<p>Now given several choices of algorithms - Random forests, logistic regression, SVM, etc., is there a scientific approach one can apply to choose one among the above algorithms just based on the data attributes. By attributes I mean number of features in dataset, no. of categorical variables, how many levels in categorical variables, etc.</p>

<p>In other words, you have dataset and based on it you take a call which method suits best.</p>

<p>The reason I ask is that I currently apply different methods and choose one with the best accuracy on cross validation set. But I think there is a way to narrow down on methods just based on dataset features.</p>

<p>Would appreciate any thoughts on this.</p>

<p>Thanks in advance!</p>
"
"0.197385508487931","0.124635390569951","202973","<p>Suppose I have a data set of <code>N</code> observations <code>(n = 1...N)</code> for out-of-sample estimation and values of ($y_n$). I have also <code>I</code> statistical models <code>(i= 1...I)</code> which every model has its own estimate on each data point ($\hat{y}^i_n$).</p>

<p>In addition I have a model selection method $\phi$ which would pick a model's estimate among the model set as its own according to its assessment on previous performance of the models ($\hat{y}^\phi_n = \min_i\{\hat{f_i}(y_n), i \in I\} $).</p>

<p>My claim should be ""model selection's performance is better than all models it picks estimates from"". I am trying to find a proper method to describe the statistical power of the model selection method, compared to individual models in the model set.</p>

<p>All individual models follow different assumptions, distributions and dependence structure. Some are iid, some have heteroskedasticity. Actually, there is no restriction on models except it should yield an estimate.</p>

<p>Some The models are employed on time series but what they do is asset pricing on different assets and contracts. But for a broaded audience I will make the following analogy.</p>

<p>Suppose you have a machine that predicts the scores on basketball matches. It does not only predict the final score, it also predicts a distribution of the scores throughout the time. It also predicts which player will score when.</p>

<p>Suppose you have many machines of this sort and all have different predictions. All of them had been right on some occasion (That is what statistics is after all right? No model is perfect.). </p>

<p>I am trying to figure out which machine is better at predicting what and when, using the previous performance of the machines. I can say stuff like 'oh machine A was good at predicting scores occured in the last 10 mins, but for the last 2 months model B became better'. </p>

<p>It turns out my estimates using the machines are better than any machine could do it alone in the long run. I checked for several error terms starting with MAPE and MSE. But I want to show that it is not a coincidence but a statistically significant fact. I have a fair sample size (~100k) over a good enough time period (5 years).</p>

<p>I fiddled with some thoughts about proportion of $\phi$ selecting the model with the lowest error and some logistic regression on that according to the criteria it uses to pick the models. But I lack the comprehensive knowledge on this domain of statistics.</p>

<p>ps. R package suggestions are also appreciated.</p>
"
"NaN","NaN","203298","<p>I am implementing some machine learning algorithms on a  large data set (90K rows) with 274 different variables. I have to carry out Logistic Regression and Random Forest for this data set. meanwhile, I want to carry out feature selection to reduce the number of those variables drastically.</p>

<p>What would be an effective feature selection algorithm (in R) for classification use case?
Thanks,
Aman</p>
"
"0.113960576459638","0.0359791381480577","203544","<p>I am running a logistic regression in R on data that is approximated with the following code: </p>

<pre><code>z &lt;- c(0,0,0,0,0,0,0,0,0,0)
y &lt;- round(runif(10, min = 0, max = 100))
y1 &lt;- c(z, y)
y2 &lt;- rep(100, 20)

x1 &lt;- runif(20)
x2 &lt;- runif(20)
response &lt;- as.data.frame(cbind(y1, y2, x1, x2))
response
</code></pre>

<p>Fitting the logistic model:</p>

<pre><code>lrfit &lt;- glm(cbind(y1, y2) ~ x1 + x2, data = response, family = binomial(logit))
</code></pre>

<p>I have fit the model using a two-column matrix with successes and failures (non-negative), but there are many 0s in the data (close to half). So, two questions. 1) How does R handle zeroes in the response variable in a logistic regression? And 2) How might this be problematic when interpreting our modeling results?</p>

<p>For context, this data shows the number of extinctions (y1) over 100 replicates (y2) in simulated landscapes with varying degrees of habitat availability (x1) and fragmentation (x2). Each row represents a different species. So, zeroes in y1 represent no extinctions across 100 replicates.</p>

<p>Thank you in advance.</p>
"
"0.152894157431288","0.104587338272187","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.131590338991954","0.0623176952849756","203929","<p>I have a dataset with the same dependent and independent variables as those for a logistic regression model whose equation has been published in the literature. How do I go about testing whether that equation fits well with my data, since their model was obviously fitted with a different dataset?</p>

<p>In other words I want to know if their model can be generalisable to a different sample/population.</p>

<p>I want to do this in R and all the searches I have done seem to only discuss how to fit a model with my data using the glm() function. I can fit a new model with my data and will therefore get different coefficients to those published, how do I then compare and contrast the two?</p>
"
"0.166450075715296","0.0886796350347864","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.107443061870051","0.0763232776972177","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.217615908702282","0.168145833956435","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"0.0877268926613025","0.0934765429274634","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.1176979772673","0.0696733014291618","206702","<p>I have some data to fit a logistic regression, although the data seems quite good, the resulted fit does not look as expected.</p>

<blockquote>
<pre><code>  paramValue      normality
1  3.69             0
2  1.16             0
3  6.12             1
4  2.78             1
5  1.45             1
6  3.56             0
</code></pre>
</blockquote>

<pre><code>mylogit &lt;- glm(normality ~paramValue,  family = binomial(link=""logit""))
summary(mylogit)
</code></pre>

<blockquote>
<pre><code>Call:
glm(formula = normality ~ paramValue, family = binomial(link = ""logit""))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.44994  -0.73312   0.08151   0.63377   1.41140  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.9945     0.9531  -2.093   0.0364 *
paramValue    1.2582     0.5655   2.225   0.0261 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 29.065  on 20  degrees of freedom
Residual deviance: 19.746  on 19  degrees of freedom
AIC: 23.746

Number of Fisher Scoring iterations: 5
</code></pre>
</blockquote>

<pre><code>    plot(paramValue,normality)

    x &lt;- seq(-1, 6, 0.1)

curve(predict(mylogit,data.frame(paramValue=x),type=""response""),add=TRUE, col=""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1f0DY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1f0DY.png"" alt=""enter image description here""></a></p>

<p>Did I do something wrong? Is there any way to force the regression to cross the origin?</p>
"
"0.0657951694959769","0.0311588476424878","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.134303827337563","0.0763232776972177","206782","<p>I am trying to build a logistic regression model using Revolution R 64 bit. My data is 300 million rows by 12 columns. The data is stored on AWS Redshift, but takes about 6 hrs to import and save as an xdf file on the server.</p>

<pre><code>Var 1: probe_name
   17 factor levels:
Var 2: network_name
   87 factor levels: 
Var 3: cm_mac_address
   55858 factor levels: 
Var 4: time, Type: character
Var 5: status
   3 factor levels:
Var 6: duration, Type: integer, Low/High: (0, 9008993)
Var 7: latency, Type: numeric, Low/High: (0.0000, 995.0000)
Var 8: down_speed, Type: numeric, Low/High: (0.0000, 4294967285.0000)
Var 9: up_speed, Type: numeric, Low/High: (0.0000, 4294967174.0000)
Var 10: down_power, Type: numeric, Low/High: (-1802.0000, 9780.0000)
Var 11: down_snr, Type: numeric, Low/High: (-13.0000, 2147483647.0000)
Var 12: up_power, Type: numeric, Low/High: (0.0000, 652.0000)
Var 13: error_between, Type: numeric, Storage: int64, Low/High: (0.0000, 300.0000)
</code></pre>

<p>I currently have a windows 2012 server with 8 cpus, 32 GB of ram, and 500 GB of memory. When I try and run my model it chugs along for about an hour before it returns with the error.</p>

<pre><code> myLM2 &lt;- rxLogit( error_between ~ probe_name + network_name +  cm_mac_address + status + duration + latency + down_speed + up_speed + down_power + down_snr + up_power,
              data = mydata)

Failed to allocate 25062470448 bytes.
Error in doTryCatch(return(expr), name, parentenv, handler) : 
  bad allocation
</code></pre>

<p>I have set <code>rxOptions(numCoresToUse=-1)</code> to take advantage of all cores available. I think the fact my factors have so many levels is a big strain. I do notice though that my average cpu usage is never very high, while my physical memory usage is very high.</p>

<p>I was wondering what configurations people have used successfully on projects this large?</p>
"
"0.134303827337563","0.0763232776972177","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.0986927542439653","0.0623176952849756","207177","<p>I am fitting a logistic regression model for the likelihood of patients suffering morbidity after surgery. The most commonly used prediction tool at the moment is POSSUM (Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity), which I would like to compare my model against.</p>

<p>In terms of discrimination, I have the Area Under the ROC curves calculated for both and would like to compare the two. </p>

<p>It seems in Stata that the command to use is <code>roccomp</code>. This produces a chi2 statistic and a p-value.</p>

<p>The R equivalent seems to require the <code>pROC</code> package and the function to use is <code>roc.test()</code>. However this function returns a z-statistic and p-value.</p>

<p>Looking at the documentation, both seem to be implementations of DeLong et al's methods of comparing AUROCs[1], but I cannot for the life of me understand why one gives a chi2 and the other a z-statistic. Are the tests equivalent?</p>

<p><em>Reference</em>:
1. Elisabeth R. DeLong, David M. DeLong and Daniel L. Clarke-Pearson (1988) â€œComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approachâ€. Biometrics 44, 837--845.</p>

<p><strong>EDIT</strong>: Does this have anything to do with the explanation: <a href=""http://stats.stackexchange.com/questions/173415/at-what-level-is-a-chi2-test-mathematically-identical-to-a-z-test-of-propo/173483#173483"">At What Level is a $\chi^2$ test Mathematically Identical to a $z$-test of Proportions?</a> ?</p>
"
"0.113960576459638","0.0539687072220866","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.198379900214539","0.103342206529982","208090","<p>I have a dataset with multiple dependent variables, which are counts of about 53 different categories of debris found on beaches. I also have a variety of independent variables, some of which I am interested in as fixed effects, but others of which are probably best off as random effects. They are a mix of categorical and continuous variables, e.g. State, county, distance from north to south, number of people present, etc. </p>

<p>A very tiny sample data set is as follows, though there are additional factors such as distance to road, date of survey, number of people on beach, and multiple transects per site. </p>

<pre><code>Counts&lt;- as.data.frame(matrix (rpois(100,1), ncol=5))
colnames(Counts)&lt;-c(""Glass"", ""HardPlastic"", ""SoftPlastic"", ""PlasticBag"", ""Fragments"")
State&lt;-rep(c(""CA"",""OR"",""WA""), each=6)
Counts$State&lt;-c(State,""CA"",""OR"")
    County&lt;-rep((1:9), each=2)
    Counts$County&lt;-c(County, 1,4)
Counts$Distance&lt;-c(10, 15, 13, 19, 18, 23, 38, 40, 49, 44, 47, 45, 52, 53, 55, 59, 51, 53, 14, 33)
    Year&lt;-rep(c(""2010"",""2011"",""2012""), times=7)
    Counts$Year&lt;-Year[1:20]
</code></pre>

<p>I would like to know whether the data vary by state, whether they change over time, and ultimately, whether the variability is higher within a site or between sites. </p>

<p>I think the best way to look at the data would be through a multinomial logistic regression model. I have been working in R, so I have looked at nnet (multinom) and VGAM (vglm), but it appears that neither of these support random effects. It could also be useful to have a smooth on some of the geographic data, so I've had a look at mgcv, but I can't find whether that package would support multiple dependent variables. </p>

<p>I have read that MCMCglmm will handle random effects, but I must admit I am a bit daunted by the complexity of how to set up the model structure, especially with respect to the priors. </p>

<p>My specific questions therefore are:</p>

<ol>
<li>can mgcv handle multiple DVs? </li>
<li>Is there another package I have overlooked?</li>
</ol>

<p>UPDATE:</p>

<p>I have found the following site: <a href=""http://search.r-project.org/library/mgcv/html/mvn.html"" rel=""nofollow"">http://search.r-project.org/library/mgcv/html/mvn.html</a>, which describes using the mvn family in mgcv as a way to run multivariate normal additive models. The model structure then for my sample data set would appear to look something like this:</p>

<pre><code>b&lt;-gam(list(Glass~s(State)+s(County),SoftPlastic~s(State)+s(County),
  PlasticBag~s(State)+s(County),family=mvn(d=3),data=Counts))
</code></pre>

<p>If this is correct, however, the model will get enormously long and complex as additional categories are added, and furthermore, I don't believe my data are normally distributed. </p>

<p>In other words, I'm still very much looking for an answer to this question!</p>
"
"0.0657951694959769","0.0311588476424878","208895","<p>I have a logistic regression model where Pstatus (a binary variable is coded as 1,2 (1 being apart, 2 being together in R see code below) has a coefficient of 0.8. I am wondering how I should interprete this coefficient of 0.8. And the dependent variable is alcohol consumption. </p>

<p>Is this positive coefficient telling me that being together increases the likelihood/log(odds ratio) of alcohol consumption? </p>

<pre><code>   logit.model&lt;-glm(alc~sex+age+famsize+Pstatus+Medu+Fedu+studytime+
   activities+romantic+famrel+freetime+goout+health+absences+grades,
   data=h,family=""binomial"")

   **Result** 
   PstatusT       0.800220   0.281027   2.847 0.004407 ** 

   Pstatus   : Factor w/ 2 levels ""A"",""T"": 1 2 2 2 2 2 2 1 1 2 ...
</code></pre>
"
"0.0657951694959769","0.0311588476424878","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.0759737176397586","0.0359791381480577","209747","<p>I am building CTR(<a href=""https://en.wikipedia.org/wiki/Click-through_rate"" rel=""nofollow"">https://en.wikipedia.org/wiki/Click-through_rate</a>)
Click prediction model with different (61) variables.Dependent variable is weather 0/1( click).I have build logistic regression model and getting probabilities of click for different combination of independent variable.</p>

<p>I am confused about model validation-</p>

<p>1)  What are the parameters should I use for model validation?</p>

<p>2) I am not classifying anything but using classification model for click through rate prediction so using Pseudo R square/ likelihood ration would work?</p>

<p>3) Is there any strategy that I can use for model validation?</p>
"
"0.147122471584125","0.0696733014291618","209773","<p>I am estimating a model of the type (logistic regression with random slopes and random intercepts clustered by the variable ID):</p>

<pre><code>formula = result ~ year + (1 | ID) + (year | ID)
</code></pre>

<p>using</p>

<pre><code>glmer(formula, data = data1, family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
</code></pre>

<p>However, I am getting results for three random effects:</p>

<pre><code>Random effects:
 Groups Name        Variance  Std.Dev.  Corr 
 ID    (Intercept) 3.645e-01 0.6037203      
 ID.1  (Intercept) 1.228e+00 1.1082860      
        year       3.043e-07 0.0005516 -1.00
</code></pre>

<p>Is there something I am specifying incorrectly?</p>

<p>On the other hand, the fixed effects are correct:</p>

<pre><code>Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 55.328196   8.052619   6.871 6.38e-12 ***
year       -0.027933   0.004008  -6.969 3.20e-12 ***
</code></pre>
"
"0.0657951694959769","0.0311588476424878","210285","<p>I'm new to R (used to work with SPSS), and looking for a function that will output the Cox &amp; Snell and Nagelkerke R-Square measures of logistic regression. In SPSS they are displayed as part of the regular output, but in R I'm not sure what manipulation should I employ on the <code>glm</code> summary to output those measures.</p>
"
"0.134303827337563","0.0636027314143481","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.0832250378576479","0.0985329278164293","210900","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and he second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<ul>
<li>X1= friends and X2= location</li>
<li>X1= friends and X2= time</li>
<li>X1= public and X2= location </li>
<li>X1= public and X2= time</li>
</ul>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way.
If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks.</p>
"
"0","0.0623176952849756","210914","<p>I have a logistic regression model obtained in R comparing association between two index diagnoses (0 or 1) with <code>Age</code> (continuous) + <code>Sex</code> (Factor) + <code>Renal.Fn</code> (continuous). My variable of interest is <code>Renal.Fn</code>. </p>

<pre><code> model &lt;- glm(diagnosis ~ Age + Sex + Renal.Fn, data = data, family = ""binomial"")
</code></pre>

<p>Currently to obtain Odds Ratio:</p>

<pre><code>exp(coef(model))[4]       # Renal.Fn 0.9884664
exp(confint(model))[[4]]  # Renal.Fn 0.9848022 0.9920815
</code></pre>

<p>My interpretation: Per unit increase in renal function the odds of diagnosis of interest reduces.</p>

<p>I wish to demonstrate the opposite.  For example: Per unit decrease in renal function, the odds of diagnosis increases.</p>

<p>Question:</p>

<ol>
<li>Is it correct to take <code>1 / exp(coef)</code> to derive the odds per unit decrease?</li>
<li>Is it subsequently correct to take <code>1/exp(coef) ^10</code> to derive odds per ten unit decrease?</li>
</ol>
"
"0.0759737176397586","0.0539687072220866","211094","<p>I am following the advice to ""keep it maximal"", and am analyzing the results from several psycholinguistic experiments. My main interest is in the fixed effects, with the random effect terms in there to provide a better test of these fixed effects.</p>

<p>That being said, I would like to keep things simple and, as much as possible, have the same analyses in each experiment.</p>

<p>In several of the experiments my random subject intercepts and slopes are perfectly, negatively correlated.</p>

<p>Given everything I said, would I be committing a cardinal sin by keeping both of them in the model?</p>

<p>Some details: I am the glmer function in R. I have only one fixed effect. I am running a logistic regression.</p>
"
"0.131590338991954","0.0623176952849756","211174","<p>We are oversampling the data to use in logistic regression. Aim  is to predict CTR(click probability) which is rare event scenario.
I have predicted the probabilities of click but CTR results are inflated as we over sampled positive class.</p>

<p>model2&lt;-SMOTE(V61 ~ ., z2, perc.over = 600,perc.under=100, learner = 'glm',family=binomial())</p>

<p>Is there any way to undo oversampling results so that I can get exact probabilities ? Based on research so far, one easiest way to divide the output probability by the multiplier we used in over sampling. I dont feel it would be the exact way as I have used synthetic minority over sampling technique(SMOTE) in R.</p>
"
"0.0882734829504749","0.0696733014291618","211936","<p>I am trying to use H2O in R to run a random forest.  <a href=""http://docs.h2o.ai/h2oclassic/Ruser/rtutorial.html"" rel=""nofollow"">http://docs.h2o.ai/h2oclassic/Ruser/rtutorial.html</a></p>

<p>In the documentation, I saw that there is an option for an offset parameter but I cannot find much information about how it is leveraged.  </p>

<p>In logistic regression, I have used an offset in two ways:
1.  To adjust for oversampling a binary event (<a href=""http://support.sas.com/kb/22/601.html"" rel=""nofollow"">http://support.sas.com/kb/22/601.html</a>)
2.  To do a two stage model where the first stage logit is calculated and then I used the logit score as an offset in the second stage so in effect the residual is modeled in stage two.</p>

<p>I would like to replicate both of these through random forest, if possible, but did not think it was possible until I saw the offset parameter in the H2O implementation of Random Forest.  Does anyone know if the H2O offset parameter functions the same as the offset option in SAS proc logistic? </p>

<p>Thank you!
nsl</p>
"
"0.0759737176397586","0.0539687072220866","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.134303827337563","0.0763232776972177","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.0930484210398471","0.0440652649239232","212375","<p>I work most of my time with categorical data (predictors and outcome), I usually do a trees in SPSS to make groups and rank which groups are more predominant to buy / not buy.</p>

<p>But now I'm into R, and I'm finding limitations trying to use other modelling techniques as they require numerical data (SVM, NN, Logistic Regression)... I'm using the package ""caret"" to do the models but I usually get no results.</p>

<p>How do you treat categorical variables to fit in those models?</p>

<p>Most of the data is multinomial... should I create n-1 predictors for a n-level predictor? Does R have a function to do this automatically?</p>
"
"0.0657951694959769","0.0311588476424878","212392","<p>I have just started learning classification techniques in R. I am using a Yelp dataset to predict whether a user is an elite user or not. I tried to build a trained model using logistic regression that I can use to predict on my test dataset. However, when I run glm() on my independent and dependent variables, I get following warning:</p>

<pre><code>logreg.model = glm(eliteStatus~nmonths+review_count+fans+total_compliments+
                total_votes+nfriends+AverageLeniencyScore,data=train_vardata,family=binomial(),maxit=100)
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<p>The other links suggest there might be a problem of perfect separation but I am failing to understand how my variables are accounting for it. All my IVs are continuous. I would really want to get rid of this warning while running glm().</p>
"
"0.147122471584125","0.0696733014291618","212430","<p>When building models with the <code>glm</code> function in R, one needs to specify the family. A family specifies an error distribution (or variance) function and a link function. For example, when I perform a logistic regression, I use the <code>binomial(link = ""logit"")</code> family.</p>

<p>What are (or represent) the error distribution (or variance) and link function in R ?</p>

<p>I assume that the link function is the type of model built (hence why using the <code>logit</code> function for the logistic regression. But I am not too sure about the error distribution function.</p>

<p>I had a look at R's documentation but could not find detailed information other than how to use them and what parameters can be specified.</p>
"
"0.113960576459638","0.0989426299071587","212611","<p>I have ran these two Logistic Regression models (below) on some small data and I am able to interpret the output - significance and direction - of the regressors, but I do not know for sure how to interpret all the data which is supposed to tell me everything related to <strong>effect (size) etc</strong>. I did select my predictors properly by adding one each time and checking whether the model was still significant (which yielded the same result as an automatic stepAIC from the MASS package) and I also did some diagnostic checks (outliertest, VIF-score).</p>

<p>What (I think) I got from the models is:</p>

<ul>
<li><strong>R2</strong>: model1 only explains 4.8% of all variation and model2 6.6%, so no predictive power?</li>
<li><strong>C</strong>: model1 does not have acceptable discrimination, neither does model2 (&lt;0.7)</li>
</ul>

<p>Is there other <strong>important information that I am ignorant of</strong>? It seems that these models do <strong>not have much 'power'</strong> (according to <strong>R2</strong> and <strong>C</strong>), but how are they then <strong>still significant</strong> (there is also very significant behaviour (***) for regressors)?</p>

<p>*PS: Sorry if am missing obvious things - I do not have that strong of a statistical background. I am also finding it a hard time searching for all the parameters and metrics since they are often denoted by a one letter name (e.g. C, g) - which is not easy to search for if you do not know what you are looking for... So that's why I came to CrossValidated!</p>

<p>I have found <a href=""http://stats.stackexchange.com/questions/104485/logistic-regression-evaluation-metrics"">this question</a>, but it does not really have an answer since it's maybe way too vague? If someone else has a reading suggestion for my problem, that's also welcomed!*</p>

<h2>First model: Agentivity ~ Period + Genre</h2>

<pre><code>(from lrm)      
                     Model Likelihood      Discrimination    Rank Discrim.    
                       Ratio Test            Indexes           Indexes       
Obs          700    LR chi2      25.55    R2       0.048    C       0.602    
 strong      403    d.f.             4    g        0.440    Dxy     0.204    
 weak        297    Pr(&gt; chi2) &lt;0.0001    gr       1.553    gamma   0.240    
max |deriv| 3e-14                         gp       0.105    tau-a   0.100    
                                          Brier    0.236                     

(from glm)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3171  -0.9825  -0.8094   1.1882   1.6090  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 954.29  on 699  degrees of freedom
Residual deviance: 928.74  on 695  degrees of freedom
AIC: 938.74

Number of Fisher Scoring iterations: 4
</code></pre>

<h2>Second model: Type ~ Period</h2>

<pre><code>(from lrm)
                      Model Likelihood      Discrimination    Rank Discrim.    
                         Ratio Test           Indexes           Indexes       
Obs           872    LR chi2      36.70    R2       0.066    C       0.637    
 mediopassive 701    d.f.             2    g        0.552    Dxy     0.275    
 passive      171    Pr(&gt; chi2) &lt;0.0001    gr       1.736    gamma   0.401    
max |deriv| 9e-10                          gp       0.087    tau-a   0.087    
                                           Brier    0.151                     

(for glm)
Deviance Residuals: 
 Min       1Q   Median       3Q      Max  
-0.8645  -0.6109  -0.4960  -0.4960   2.0767  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 863.19  on 871  degrees of freedom
Residual deviance: 826.49  on 869  degrees of freedom
AIC: 832.49

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.0930484210398471","0.0440652649239232","212803","<p>I am trying to test out the effects of second order methods on logistic regression. I have a function that looks like</p>

<pre><code>log_reg &lt;- function(x, y, test, maxit=100) {
  trainErr &lt;- numeric(maxit)
  testErr &lt;- numeric(maxit)

  w &lt;- runif(ncol(x), -0.01, 0.01)
  for(epoch in 1:maxit) {
    o &lt;- optim(par=w, fn=function(w) cross_entropy(x, y, w), 
               gr=function(w) logistic_gradient(x, y, w), method=""BFGS"")
    w &lt;- o$par

    trainErr[epoch] &lt;- o$value
    testErr[epoch] &lt;- cross_entropy(test[,-1], test[,1], w)
  }

  return(list(w, trainErr, testErr))
}
</code></pre>

<p>with helper functions</p>

<pre><code>logistic_gradient &lt;- function(x, y, w) {
  delta &lt;- sigmoid(x %*% w) - y
  dw &lt;- as.vector(t(delta) %*% x)
  return(dw / nrow(x))
}

cross_entropy &lt;- function(x, y, w) {
  sigma &lt;- sigmoid(x %*% w)
  error &lt;- -colSums(y*log(sigma) + (1-y)*log(1 - sigma))
  return(error / nrow(x))
}

sigmoid &lt;- function(x) return(1/(1+exp(-x)))
</code></pre>

<p>But when I plot train and test errors, it turns out that there is massive overfitting (the test error is even non-decreasing!). Because I cannot find a reason why second order methods would overfit that badly, I have this feeling I understood something wrong. Either in my implementation or conceptually...</p>

<p>Is there anyone who could enlighten me?</p>
"
"0.178541910193085","0.103342206529982","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.0465242105199235","0.0440652649239232","213697","<p>Say I have the following in R</p>

<pre><code>rm(list=ls())
set.seed(1000)
n&lt;-20
x&lt;-rnorm(n, 0.5,1)
y&lt;-rnorm(n, 0.5,1)
type&lt;-rep(2,n)
df1&lt;-data.frame(x,y,type)
x&lt;-rnorm(n, -0.5,1)
y&lt;-rnorm(n, -0.5,1)
type&lt;-rep(5,n)
df0&lt;-data.frame(x,y,type)
df&lt;-merge(x=df0,y=df1,all=T)
plot(df$x,df$y,col=df$type)
</code></pre>

<p><a href=""http://i.stack.imgur.com/eRFp0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eRFp0.png"" alt=""enter image description here""></a></p>

<p>Now as you can see I have classes that overlap -- say the cost of classifying the red class incorrectly is 10 times the cost of classifying  the blue class incorrectly.</p>

<p>Say I want to use a logistic regression -- how would I do incorporate the cost into my logistic regression model? </p>
"
"0.0657951694959769","0.0311588476424878","213789","<p>I have a data set and its response variable consists of only two results that are success and failure.That's why I used logistic regression method to construct a model. However, I don't know how I determine the outliers or leverage points etc. I used plot() function, but the given outputs are hard to interpret. In addition, I used outliersTest() but it also did not work. How can I detect outliers ? </p>
"
"0.0657951694959769","0.0311588476424878","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"0.0930484210398471","0.0440652649239232","213933","<p>I am searching for a machine learning algorithm that I can use to predict customer retention/churn rate. My response variable is a proportion in the range 0 to 1 (0 and 1 inclusive). I am using R. The catch here is that I have already played around with GLM models and I am currently using a logistic regression model. I was wondering if there are models, other than regression, that I can use in such a setup.</p>
"
"0.196227812235748","0.135818268070785","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"0.0930484210398471","0.0440652649239232","214022","<p>I was trying out the Subselect R package to see how it worked and if it would be useful for a logistic regression problem I'm working on.  <a href=""https://cran.r-project.org/web/packages/subselect/vignettes/subselect.pdf"" rel=""nofollow"">Link</a> to the package.</p>

<p>I decided I would follow Example 4 on page 28 to see if I could perform the Anneal function on the glm I previously fit to my data.  I used the helper function, glmHmat, to extract the required matrices in the same way sa done in example 4.</p>

<pre><code>Fullmodel&lt;-glm(G,family=binomial,data)
Hmat &lt;- glmHmat(Fullmodel)
</code></pre>

<p>I then tried the anneal function and got the following error.</p>

<pre><code>test&lt;-anneal(Hmat$mat,1,10,H=Hmat$H,r=1,nsol=10,criterion = ""Wald"")
</code></pre>

<p>Yet, I got this error.</p>

<pre><code>Error in validmat(mat, p, tolval, tolsym, allowsingular = FALSE, algorithm) : 

 The covariance/total matrix supplied is not symmetric.
  Symmetric entries differ by up to 1.02445483207703e-07.
</code></pre>

<p>So, I thought I would test if this were true:</p>

<pre><code>isSymmetric(Hmat$mat,tol=1e-09)
[1] TRUE
isSymmetric(Hmat$H,tol=1e-09)
[1] TRUE
</code></pre>

<p>So I can't make heads or tails of this error message.  Any ideas?</p>
"
"0","0.0623176952849756","214052","<p>We are currently collecting data for a study whose purpose is to show whether scientists are focusing more or less on a specific subject with time. To keep some privacy let's say the subject is <em>jelly beans</em>: we reviewed a thousand random studies and we checked whether they were about jelly beans or not. The dataset has only two columns and it looks like:</p>

<pre><code>| JellyBeans | Year |
|------------|------|
|    YES     | 2010 |
|    NO      | 2001 |
|    NO      | 2010 |
|    NO      | 2015 |
|    YES     | 2009 |
|    NO      | 2016 |
|    ...     | .... |
|    YES     | 1999 |
</code></pre>

<p>We thought of using logistic regression for the purpose as the DV is categorical. In R, this would look something like:</p>

<pre><code>logreg_jelly_year = glm(JellyBeans ~ Year, family = ""binomial"", data = dataset)
</code></pre>

<p>We have, however, some doubts about the validity of the procedure, in particular:</p>

<ol>
<li>Is there any specific assumption we have to check that could jeopardise the scientific value of the procedure?</li>
<li>Is the fact that <code>Year</code> is not truly continuous a problem?</li>
<li>Is there any other test or procedure that we should run on top or instead of logistic regression?</li>
</ol>
"
"0.0930484210398471","0.0440652649239232","214175","<p>I'm new to xgboost package and here is the <a href=""https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"" rel=""nofollow"">doc</a> on the parameters of this library for your reference.</p>

<p>My question is, logistic regression <strong>doesn't do</strong> binary splitting and build a tree unlike decision trees. If so, why max.depth and eta (learning rate) has been used in the <a href=""http://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees"">example</a> where the objective parameter is binary:logistic. (and the answer is accepted) </p>

<p>Isn't it wrong combination? or am I missing anything?</p>

<pre><code># xgboost fitting with arbitrary parameters
xgb_params_1 = list(
  objective = ""binary:logistic"",                                               # binary classification
  eta = 0.01,                                                                  # learning rate
  max.depth = 3,                                                               # max tree depth
  eval_metric = ""auc""                                                          # evaluation/loss metric
)
</code></pre>
"
"NaN","NaN","214512","<p>I'm trying to run a logistic regression with a L2-Penalty on a dataset I have. For the regression coefficient I also want to have the p-values of the signifance or at least the standard errors.
My plan was to do it with Python but unfortunately none of the package fulfilled my needs. </p>

<p>As I need a solution for this problem really quick, I thought to use R in this case. Unfortunately I don't have the time to read all about the package R offers, so I just liked to ask if there is any way to do the above mentioned in R?</p>
"
"0.0657951694959769","0.0311588476424878","214790","<p>My model is logistic regression. Is there a way to tune the parameter lambda of lasso or ridge based on cross-validated log-loss and brier(eg. proper scores?) in any R packages? </p>

<p>I'm using glmnet right now and the only measure available seems to be deviance, mean absolute error, misclassification error(is this based on 0.5 cut off?) and auc which are not proper scores and are therefore less desirable. </p>

<p>On a related note, is there a score like squared loss but penalize the deviation from one outcome more severely?</p>
"
"0.107443061870051","0.0763232776972177","214882","<p>I am currently performing a retrospective study that is comparing a surgical procedure vs a modified version of the same procedure. There is obvious selection bias because of the selection criteria necessary to perform the modified procedure. I was wondering how I would control for these 3 variables (all are simple T/F requirements)? Should I just perform a logistic regression for each dependent variable we are investigating and hope none of them reach significance? Or is there a statistical test that automatically adjusts for these 3 selected covariates?</p>

<p>Initially I did not perform any statistical tests between these groups for this reason, but if we were to prove that these variables are not confounding then I could simply perform the appropriate two sample test, correct?</p>

<p>My Data:</p>

<ul>
<li>Independent Variable (2 groups) = Procedure 1, Procedure 2</li>
<li>We also have multiple dependent variables we want to compare between the two groups: Numerical and Logical (e.g. Length of Stay,
or Re-operation within 30 days, etc.)</li>
<li>But I have 3 Logical variables that I am afraid are confounding.</li>
</ul>

<p>PS. I'm using R to carry out this analysis and any reference to R functions would be a plus. </p>
"
"0.113960576459638","0.0539687072220866","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.0930484210398471","0.0220326324619616","215105","<p>I want to simulate a data set for logistic regression in which my $Y_i \sim Bin(n_i, p_i)$ and $n_i &gt;1 ~ \forall i$. I want something like:</p>

<p><a href=""http://i.stack.imgur.com/Nx0yU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Nx0yU.png"" alt=""enter image description here""></a></p>

<p>In another <a href=""http://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression"">question</a>, data has been generated for a logistic in which $n_i = 1$. I am confused as to whether it would be correct to follow this method and then bin the $x$ variables and call that a population. I'm not quite sure how to do this without creating some sort of bias in the data that I won't account for in the logistic regression. I'm looking for an explicit description of how to account for $n_i&gt;1$, if possible using R.</p>

<p><strong>EDIT</strong>: Using the code in the question which I've tweaked, here is what I have:</p>

<pre><code>set.seed(1)
x1 &lt;- rnorm(6)           # some continuous variables 
n &lt;- round(runif(6, min = 1, max = 20))
z = 1 + 2*x1                
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y &lt;- matrix(0,6,1)
for( i in 1:6 ) { y[i] &lt;- sum(rbinom(n[i], 1, pr[i]) == 1)}

Y &lt;- y/n
</code></pre>

<p>Are there any reasons this is not a reasonable way of doing things?</p>
"
"0.0832250378576479","0.0985329278164293","215447","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and the second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<pre><code>X1= friends and X2= location
X1= friends and X2= time
X1= public and X2= location
X1= public and X2= time
</code></pre>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. </p>

<p>The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way. If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks in advance.</p>
"
"0.175453785322605","0.0934765429274634","215532","<p>I am a clinicians (limited statistical knowledge) who is trying to use mlogit pkg in R to analyze a clinical dataset, running logistic regression on it. I am trying to ascertain if there is any correlation seen in patients with many vars and heart block (var = block (0s and 1s))</p>

<p>I have a dataset with these variables</p>

<pre><code> [1] ""Age""                    ""Sex""                    ""Race""                       ""Obesity""               
 [5] ""CAD""                    ""HTN""                    ""DM""                     ""HLD""                   
 [9] ""CHF""                    ""COPD""                   ""Asthma""                 ""Thyroid.disorder""      
[13] ""Smoking""                ""Illicit.drug.use""       ""Alcohol""                ""INR""                   
[17] ""TB""                     ""AST""                    ""ALT""                    ""Cirrhosis""             
[21] ""Adenosine""              ""Amiodarone""             ""Beta.blocker""           ""CCB""                   
[25] ""Digoxin""                ""TCA""                    ""SSRI""                   ""Antipsychotic""         
[29] ""AV.block""               ""Bundle.branch.block""    ""PAC.PVC""                ""Afib""                  
[33] ""Other.Arrythmia""        ""Nonspecific.ST.wave""    ""Anterioseptal..ST.wave"" ""Anteriolateral.ST.wave""
[37] ""Inferior.ST.wave""       ""Posterior.ST.wave""      ""Axis.deviation""         ""Low.voltage""           
[41] ""Qt.prolongation""        ""Hypertrophy""            ""block""
</code></pre>

<p>Now I have used the mlogit pkg in R</p>

<p>My Code is</p>

<pre><code># Reshaping data
mydata &lt;- mlogit.data(data=mydata, shape=""wide"", choice=""block"")

# Creating Model with all Vars
model &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking+Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy+INR+TB+AST+ALT)
</code></pre>

<p>Generates this error :</p>

<pre><code> Error in solve.default(H, g[!fixed]) : 
 Lapack routine dgesv: system is exactly singular: U[43,43] = 0
</code></pre>

<p>Now if I break the variables into different variables like..</p>

<pre><code>model1 &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking)
model2 &lt;- mlogit(data=mydata, formula=block~0|Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy)
model3 &lt;- mlogit(data=mydata, formula=block~0|INR+TB+AST+ALT)
</code></pre>

<p>IT WORKS without any errors.</p>

<p>But here I would like to know,
how breaking into different models would change my Coefficients?
What should I do to avoid the error and try to incorporate all variables in one model?
How should I interpret my results if break into 3 different models as opposed to one?</p>

<p>Any help is highly appreciated.</p>
"
"0.0930484210398471","0.0440652649239232","215560","<p>I have three data sets that, when joined, have O(320) independent variables for a classification problem.  </p>

<p>Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous.</p>

<p>I'm at a loss as to how to proceed.  </p>

<p>How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable?  How do you decide between variables?  What calculations can you go on to supplement your gut and experience?  How do you avoid throwing away significant variables?</p>

<p>A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant.  How to accomplish that?</p>

<p>Should I just toss all of it into a logistic regression and see what happens, without any forethought?</p>

<p>More detail in response to comments:</p>

<ol>
<li>Classification. </li>
<li>Many more observations than columns. </li>
<li>Yes, big oh notation meaning approximately. </li>
<li>Linear model at first. Also interested in boosted models in addition to logistic regression. </li>
</ol>
"
"NaN","NaN","215940","<p>I am a statistician. I'm pretty good with the concepts of topics like Linear &amp; Logistic Regression &amp; Time Series.
But in order to run data I need to learn the R language. Since, having no programming background makes it difficult for me to understand it.</p>

<p>How can I easily learn and construct commands in R software? What could help me with achieving that?</p>
"
"0.0657951694959769","0.0311588476424878","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.21711298242631","0.124851583951116","216122","<p>As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. </p>

<p>Normally, we can use <code>glm</code> to obtain the logistic models by <code>glm(y ~ x1+x2, data = dat, family = binomial(logit))</code>. </p>

<p>For frm, we change <code>family = binomial(logit)</code> to <code>family = quasibinomial(logit)</code>.  </p>

<p>I noticed we can also use <code>family = binomial(logit)</code> to obtain frm's parameter since it gives the same estimated values. See the following example</p>

<pre><code>library(foreign)
mydata &lt;- read.dta(""k401.dta"")


glm.bin &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = binomial('logit'))
summary(glm.bin)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = binomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.074e+00  8.869e-02  12.110  &lt; 2e-16 ***
mrate        5.734e-01  9.011e-02   6.364 1.97e-10 ***
age          3.089e-02  5.832e-03   5.297 1.17e-07 ***
sole         3.636e-01  9.491e-02   3.831 0.000128 ***
totemp      -5.780e-06  2.207e-06  -2.619 0.008814 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: 1997.6

Number of Fisher Scoring iterations: 6
</code></pre>

<p>And for <code>family = quasibinomial('logit')</code>,</p>

<pre><code>glm.quasi &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = quasibinomial('logit'))
summary(glm.quasi)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = quasibinomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.074e+00  4.788e-02  22.435  &lt; 2e-16 ***
mrate        5.734e-01  4.864e-02  11.789  &lt; 2e-16 ***
age          3.089e-02  3.148e-03   9.814  &lt; 2e-16 ***
sole         3.636e-01  5.123e-02   7.097 1.46e-12 ***
totemp      -5.780e-06  1.191e-06  -4.852 1.26e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2913876)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>The estimated Beta from both <code>family</code> are the same, but the difference is the SE values.  However, to obtain the correct SE, we have to use <code>library(sandwich)</code> as in this <a href=""http://stackoverflow.com/questions/37584715/fractional-response-regression-in-r"">post</a>.</p>

<p>Now, my questions:</p>

<ol>
<li>What is the difference between these two codes?</li>
<li>Is frm about to obtain robust SE?</li>
</ol>

<p>If my understanding is not correct, please give some suggestions.</p>
"
"NaN","NaN","217417","<p>I have a logistic regression model. I'm looking for a non-graphical way to find the optimal cut-off where sensitivity is above a threshold(say 0.95) and maximizes sensitivity+specificity. I don't have a fitted model. Only two vectors of observations and predicted probabilities.</p>
"
"0.131590338991954","0.0623176952849756","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.124341182825498","0.0706616245726062","217974","<p>I am using the GLM Summary in R to determine the significance of the variables in the logistic regression model.
I am trying to figure out if there is a way to flip the null and alternative hypotheses such that the null is that a certain variable is not insignificant in the logistic regression model and the alternative is that a certain variable is insignificant in the logistic regression model. This way, if I reject the null, I can accept the alternative with some level of significance. 
Is there a way to do this? Or is there some other test or model I can use?</p>
"
"0.113960576459638","0.0539687072220866","218276","<p>Imagine a data set with approximately 100 variables and 5000 cases. The outcome is a two-level factor. All variables are factors, most of them three levels (yes, no, or indifferent).</p>

<p>After building a simple logistic regression model I'm in doubt about how to reduce the amount of variables used in the final model (and find a proper validation method).</p>

<p>Before building the model I've used chi-squared to examine individual relations between some predictors and the outcome, but this becomes kind of stupid.</p>

<p>Any advice about how to tackle this issue, preferably in an automated way? I've some understanding of basic probability but would use Lasso or Ridge Regression more as a black box now. </p>

<p>As tools I use R and Python.
Thanks in advance!</p>
"
"0.147122471584125","0.0696733014291618","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.0465242105199235","0.0440652649239232","218714","<p>I would like to accept the hypothesis that a variable is insignificant in determining the dependent variable of a logistic regression, but in R the glm summary which contains the p-values for each variable is set so that the null hypothesis is that the variable is insignificant and the alternative is that the variable is significant. </p>

<p>Can I customize the hypothesis test for logit regression? How do I solve this problem?</p>
"
"0.218217890235992","0.103342206529982","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.0657951694959769","0.0311588476424878","218842","<p>I'm using a random forest in R (randomForest) to predict a binary output (1,0) for a dataset that is heavily unbalanced. In this example let's assume the population has 1% 1's and 99% 0's.</p>

<p>Building the random forest on such unbalanced data is difficult and I get much better results when building it on a 50:50 sample.  When predicting a validation set, I obtain the % of trees that predicted that data point to be a 1.  For example, customer A has a 75% probability of being a 1 (based on the # of trees that predicted 1)</p>

<p>If I want to re-scale these predictions back to the original population ratio of 1:99, is there a good way to do this?</p>

<p>In the past I've used logistic regression, and I can adjust the intercept accordingly to down-scale the predicted probability.</p>

<p>Is there a good way to think about this from the RF point of view?  Can I simply just down-weight the predictions from the 50:50 sample by 50 (50% down to 1%)?</p>

<p>Thanks in advance for any thoughts and help</p>
"
"0.0930484210398471","0.0440652649239232","219169","<p>I am developing a logistic regression model on a large dataset consisting of 15 variables and 200k observations.
In initial model fitting, I find variables - ""Purchase Frequency"" and ""Average Payment Amount"" are highly correlated (GVIF values around 20) and both are significant in terms of p value.
When I remove one of these two variable, the other variable becomes insignificant and also few other variables (low VIF value) becomes insignificant from significant in previous case.</p>

<p>How should I proceed with this?</p>
"
"0.119027940128723","0.103342206529982","219304","<p>I am examining social interaction data in individuals within two groups. Each social encounter has been coded to one of 4 categories, and these encounters are nested within individual, whom are nested within groups. The number of social encounters per individual is variable and my groups are unequal sample sizes. </p>

<p>I want to examine whether the proportion of social encounters in different categories significantly differ as a function of group. I previously examined a different DV in this data that was continuous, not categorical, and used a multilevel model in R (nlme package) to do so (data nested within individuals within groups). I have done some looking online and as far as I can tell, R should be able to run a multilevel model with categorical dependent variable as well. (i.e., <a href=""http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf"" rel=""nofollow"">http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf</a>). However, I am not sure how to implement this and I think that the naming online is inconsistent (some sources referring to this analysis as MLM with categorical variable, others calling it a multinomial logistic regression). </p>

<p>Is it possible to modify my current R script for continuous DV so that it analyzes for a categorical DV instead? Or do I need a different script? Thank you in advance for any help.</p>
"
"0.0986927542439653","0.0623176952849756","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.113960576459638","0.0539687072220866","219453","<p>The Hosmer-Lemeshow test is a statistical test for goodness of fit for logistic regression models. According to <code>?hoslem.test</code>, it deals only with binary logistic regression. However, I wonder if this test can be used to a ordered logit model which has more than 2 levels for the dependent variable.</p>

<p>The ordered logit model is also known as the proportional odds model, or a cumulative logit model. And I use the ""Ordinal"" package. Thanks a lot.</p>
"
"0.080582296402538","0.0763232776972177","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.174077655955698","0.0824385620013739","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.248129122772926","0.13219579477177","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.113960576459638","0.0539687072220866","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"NaN","NaN","220462","<p>Can you please explain in simple way.</p>

<p>Is it so important in logistic regression?</p>
"
"0.1359059177167","0.0965421584050956","220813","<p>I have troubles finding methodology to test a model in R. My question may be long and contain many details, but I will be glad if somebody can help me and give some hints how to tackle it.  </p>

<p>I have multilevel (nested) panel data, where one observation is a frequency of flights (or ratio of use of bio fuel from 0 to 1) of airline <code>i</code> on route <code>j</code> for time period <code>t</code> corresponding to one month.
 I suspect that I need to introduce airline fixed effect since frequency of flights may depend on which airline it is (Lufthansa or Singapore Airlines for example). 
Then I also need to do route fixed effect, since frequency of flights between Frankfurt-New York and Frankfurt-Abidjan will be also specific to each route. And I also need to do time fixed effect, since frequency of flight in the following month most likely to depend on the previous month, since airlines when they schedule flights in advance they choose particular aircraft and competitors behavior. </p>

<p>Below I provided the illustration of data set that I have</p>

<pre><code>A &lt;- data.frame(route=as.factor(rep('Paris-London', 138)), 
                 airline = as.factor(c(rep('Luft', 12), rep('DL', 8), rep('Sin', 72), rep('Turkish', 46))),
                 Date = c(seq(as.Date('2010-01-01'), as.Date('2011-01-01'), by = 'month'), 
                          seq(as.Date('2010-01-01'), as.Date('2010-07-01'), by = 'month'),
                          seq(as.Date('2010-01-01'), as.Date('2015-12-01'), by = 'month'),
                          seq(as.Date('2012-03-01'), as.Date('2015-12-01'), by = 'month')), 
                 ratio_use_bio_fuel= runif(138),
                distance_km=c(rep(600, 138)),
                revenue=sample(10000:25000, 138),
                frequency=sample(100:250, 138, replace=TRUE)
)


B&lt;- data.frame(route=as.factor(rep('Amsterdam-Brussels', 108)), 
               airline = as.factor(c(rep('EEE', 36), rep('FFF', 36), rep('GGG', 36))),
               Date = c(seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month'), 
                        seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month'),
                        seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month')),
               ratio_use_bio_fuel= runif(108),
               distance_km=c(rep(1000, 108)),            
                revenue = sample(10000:25000, 108),
               frequency=sample(50:100, 108, replace=TRUE)



)


C &lt;- data.frame(route=as.factor(rep('Tokyo-Bangkok', 131)), 
                airline = as.factor(c(rep('Chin', 46), rep('Thai', 13), rep('Jap', 72))),
                Date = c(seq(as.Date('2012-03-01'), as.Date('2015-12-01'), by = 'month'),
                         seq(as.Date('2010-01-01'), as.Date('2011-01-01'), by = 'month'), 
                         seq(as.Date('2010-01-01'), as.Date('2015-12-01'), by = 'month')), 
                ratio_use_bio_fuel= runif(131),
                distance_km=c(rep(2500, 131)),
                revenue=sample(10000:25000, 131),
                frequency=sample(32:78, 131, replace=TRUE)

)

d&lt;-rbind(A,B,C)
</code></pre>

<p>Some of the airlines do not have observations for particular time period. In some cases they exited the route and do not provide flights any more or there was a merger. Therefore, I have unbalanced panel data. Does it cause any issues if I run the regression? </p>

<p>I know there are different packages in R such as plm and lme. </p>

<p>But do not know how to do logistic fixed effect regression where my dependent variable is ratio of usage of bio fuel (continuous of 0 to 1) conditional on airline and route. </p>
"
"0.158703920171631","0.103342206529982","221046","<p>I have a small data set of cases from an outbreak. For each case I have collected data for a few variables (age, HIV status, hospitalisation, country of infection etc. ). There are only 28 cases and so I imagine that it won't be possible to do any meaningful inferential statistical analysis. I would however like to present the data in as scientific manner as possible. </p>

<p>I would like to, for example, tabulate the data showing the difference between the cases that are HIV positive and HIV negative. I've included some R script and the results of what I've managed to do so far: </p>

<pre><code>data &lt;- read.csv(""Shigella.csv"")
library(dplyr)
library(epiR)
data$Hospitalised &lt;- factor(data$Hospitalised, levels = c(""Yes"", ""No""), labels = c(""Hospitalised"", ""Not Hospitalised""))
data$HIV &lt;- factor(data$HIV, levels = c(""Positive"", ""Negative""))
ttab &lt;- table(data$Hospitalised, data$HIV)
ttab


                   Positive Negative
  Hospitalised            2        1
  Not Hospitalised       13       11


Odds_Ratio &lt;- epi.2by2(ttab, method = ""case.control"", conf.level = 0.95)
Odds_Ratio

&gt; Odds_Ratio
             Outcome +    Outcome -      Total        Prevalence *        Odds
Exposed +            2            1          3                66.7        2.00
Exposed -           13           11         24                54.2        1.18
Total               15           12         27                55.6        1.25
Point estimates and 95 % CIs:
-------------------------------------------------------------------
Odds ratio (W)                               1.69 (0.13, 21.27)
Attrib prevalence *                          12.50 (-44.45, 69.45)
Attrib prevalence in population *            1.39 (-25.97, 28.75)
Attrib fraction (est) in exposed  (%)        39.79 (-1206.69, 99.08)
Attrib fraction (est) in population (%)      5.45 (-22.83, 27.23)
-------------------------------------------------------------------
 X2 test statistic: 0.169 p-value: 0.681
 Wald confidence limits
 * Outcomes per 100 population units
</code></pre>

<p>From the above, my interpreting is that the odds of a hospitalised case being HIV postive are 1.69 that of a non-hospitalised case being HIV positive. The 95% CI is (0.13, 21.27). </p>

<p>I am not sure that the p value is however. I see a p value of 0.681 but that seems to be for the X2 value. </p>

<p>Here is what I'm stuck with: 
1) am I using the right statistical test (or should I be doing logistic regression or something else)? 
2) how do I get a p value for the odds ratio (or is the p value provided above actually for the odds ratio)? </p>

<p>With appreciation! </p>

<p>Greg</p>
"
"NaN","NaN","221231","<p>I want to check multicollinearity to avoid any redundancy in my database before doing the multinomial logistic regression with categorical dependent variable using R, knowing that the majority of my variables expressed as dichotomous and ordinal. Not the VIF method! Is there any other method that I can use before the regression?</p>
"
"0.124341182825498","0.0824385620013739","221427","<p>I want to understand the interpretation of logistic regression coefficients in terms of an increase in probability of dependent variable being 1. </p>

<p>I tested a logistic regression model in R and got the following coefficients (all statistically significant):</p>

<pre><code>&gt; mud$coefficients
  (Intercept)          var1          var2          var3          var4
-3.557573e+00  1.051031e-01  4.937244e-07 -1.308386e-06  3.937646e-01
</code></pre>

<p>Raising these numbers to the power of e resulted in numbres below. I would interpret them so that a 1 unit increase in var1 would increase the probability of dependent variable being 1 by 11% and 1 000 000 unit decrease in var3 would increase that probability by 1%. </p>

<pre><code>&gt; exp(mud$coefficients)
(Intercept)       var1        var2        var3        var4
0.02850792  1.11082516  1.00000049  0.99999869  1.48255150
</code></pre>

<p>As suggested in a previous a question (<a href=""http://stats.stackexchange.com/a/24422/121763"">http://stats.stackexchange.com/a/24422/121763</a>), below is what I should actually do to find the probabilities.</p>

<pre><code>&gt; exp(mud$coefficients)/(1+exp(mud$coefficients))
(Intercept)       var1        var2        var3        var4 
0.02771774  0.52625162  0.50000012  0.49999967  0.59718862
</code></pre>

<p>So which numbers should I use if I'd like to express the effect of independent variables on the probability of and event occurring? E.g. in case of var1 is it 11% or 53% or something else? </p>
"
"0.155080701733078","0.117507373130462","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.124341182825498","0.0824385620013739","221525","<p>I am using the <code>svyglm</code> function in the <code>survey</code> package in <code>R</code> to fit logistic regression models to a stratified, cluster survey. I want to calculate confidence intervals for my regression coefficients. The default method for <code>confint.svyglm</code> says that it creates Wald confidence intervals by adding and subtracting a multiple of the standard error. But the confidence interval this produces is not consistent with the p-value from the model - confidence intervals that do not overlap 0 still have p-values greater than .05.</p>

<p>I tried to replicate the p-value and confidence interval calculations by hand. It appears the p-value is calculated using a t-test, with the df of the t distribution taken from the residual degrees of freedom from the model. So far so good. But the confidence interval provided by <code>confint.svyglm</code> is just coefficient +/- 1.96*standard.error. This seems wrong - for a 95% confidence interval, I think the multiplier for the standard error should be the .975 quantile of a t-distribution with the appropriate degrees of freedom (in my case 10), which can be somewhat different from 1.96 (the .975 quantile of a z-distribution). True? Has anyone else had this problem? I am relatively new to working with survey data. Is there a reason to always use the z-quantile instead of the t-quantile for complex surveys specifically, or is this just a bug in the package?</p>
"
"0.1176979772673","0.0696733014291618","222426","<p>I would like to analyze a Randomized Response variable as the final response variable in a Structural Equation Model (SEM), with <code>R</code>. However, I found no example about this. To the best of my knowledge <code>R</code> packages enable users to fit multivariate logistic regressions only. 
However, I have seen that Randomized Response can be analyzed with SEM in Mplus (Hox, J., &amp; Lensvelt-Mulders, G. (2004). <a href=""http://www.tandfonline.com/doi/abs/10.1207/s15328007sem1104_6?journalCode=hsem20"" rel=""nofollow"">Randomized response analysis in Mplus</a>. Structural equation modeling, 11(4), 615-620.). </p>

<p>Given that <code>R</code> has good packages for latent variable analysis, like <code>lavaan</code>, could you tell me how could I model a randomized response with SEM in <code>R</code>?</p>
"
"0.189934294099397","0.107937414444173","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.161164592805076","0.0763232776972177","222544","<p>I used the package for random forest. It is not clear to me how to use the results. 
In  logistic regression you can have an equation as an output, in standard tree some rules. If you receive a new dataset you can apply the equation on the new data and predict an outcome (like default/no default). Or saying the customers with characteristics a and characteristics b will have a default, so you can predict the outcome before it happens. That is the scoring tecnique.</p>

<p>Is it possible to use random forest in a similar situation, or how would you use the results of a RF? </p>

<p>my python code:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

#creating a test and train dataset

from sklearn.cross_validation import train_test_split

train, test = train_test_split(df, test_size = 0.3)

clf = RandomForestClassifier(max_depth = 30, min_samples_split=2, n_estimators = 200, random_state = 1)

#training the model
clf.fit(train[columns], train[""churn""])

#testing the model
predictions = clf.predict(test[columns])

print(predictions)

print(roc_auc_score(predictions, test[""churn""]))
</code></pre>
"
"NaN","NaN","223236","<p>I am using logistic regression to predict binary outcomes with 5 features. When putting 20x weight on the 0.001% outliers the peformance gets a lot better.</p>

<p>It seems that some really high/low values in the features are predictive.</p>

<p>What are some suggested methods here to further improve? (e.g. transformation, finding the best weight, filtering the data with robust PCA to remove noise)</p>
"
"0.107443061870051","0.0763232776972177","223441","<p>I am running a multivariate binary logistic regression trying to predict political party affiliation using the variables: <strong>Age, Sex, Race, Level of Education, Church Attendance</strong></p>

<p>I have ran the regression using glm() in R and have got some pretty decent results. All of them being pretty significant with p-values less than .05.</p>

<p>I know that doesn't really speak to the validity of the regression but I am now trying to graph it and can't figure out if it's possible given how many ""values"" are within each category.</p>

<p>Although I am only using 5 variables I end up with about 15 values and I know that means a lot of potential graphs.</p>

<p>Is this possible and would anybody know how to set it up using R?</p>

<p>I have attempted to graph just the first few values thus far but it doesn't look correct and I'm thinking it would be impossible to have all the variable values in a single graph. </p>
"
"0.109489780290272","0.09506100395792","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.0930484210398471","0.0440652649239232","223785","<p>I am trying to model a process with a binary response (pass/fail).  The process fails if the predictor is large or small, but in the middle it passes.  The result looks something like this:</p>

<p><a href=""http://i.stack.imgur.com/B4um2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B4um2.png"" alt=""enter image description here""></a></p>

<p>I want to fit a model to this data.  I have tried logistic regression, which doesn't yield good results:</p>

<p><a href=""http://i.stack.imgur.com/d8LY0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d8LY0.png"" alt=""enter image description here""></a></p>

<p>I'm thinking there must be an obvious way to model this type of data but I haven't come across anything yet.  Looking for advice on what modelling approach should be taken with data like this.  And is it able to be generalized for multiple predictors.</p>
"
"NaN","NaN","224265","<p>I'm fitting  a Bayesian logistic regression to model the effect of two covariates on a Randomized Response, with the 'rr' package.
I would like to compare two nested models by using the Bayes factor. Unfortunately, the 'rr'package does not estimate it by default. Is there any way to obtain it from the output of the 'rrreg.bayes' function?</p>

<p>For anybody who would like to help me, here is an example of application. If you were able to explain me how to compute the Bayes factor from the output of the example provided, I will be able to do the same on my data.</p>

<p><a href=""http://www.inside-r.org/node/333540"" rel=""nofollow"">http://www.inside-r.org/node/333540</a></p>

<p>Kind regards,</p>

<p>Jacopo Cerri </p>
"
"0.0379868588198793","0.0539687072220866","224438","<p>I've created a multivariate binary logistic regression web app using shiny and R for one of my final projects of the semester. I would like a little critique on it and would love to learn more of just exactly what I'm dealing with. </p>

<p>My model attempts to predict Political Party Affiliation by using the variables of <strong>Age, Sex, Race, Degree of Education, Church Attendance.</strong></p>

<p>I used data collected from the General Social Survey (GSS) and have about 1300 observations in my final data set after everything was cleaned and garbage values taken out. </p>

<p>One problem that I can already see is the distribution of <strong>Race</strong> in my dataset, it's pretty skewed. </p>

<p><a href=""https://rtutorials-portfolio.shinyapps.io/PartyID/"" rel=""nofollow"">My web app can be found here</a></p>

<ul>
<li><p>The probabilities tab shows the expected probability of someone being republican.</p></li>
<li><p>The presentation tab shows a quick presentation on how I went about creating the logistic regression.</p></li>
<li><p>The graphics tab holds all of my graphics of the dataset.</p></li>
<li><p>The model info tab shows the <code>summary()</code> of the glm() and the results of the Hosmer-Lemeshow Test. </p></li>
</ul>

<p>I would love any critique or things you think I could do better with (web design, statistics, coding, etc.)</p>
"
"0.147122471584125","0.0696733014291618","224539","<p>I have a dataset with about 30 potential predictors and 115 observations. I'm looking into building a prediction model with the data using logistic regression.</p>

<p>From what I have read - the typical rule of thumb to split the dataset is an 80/20 split, where 80 percent of the dataset will be used for training the model and the remaining 20 percent will be used for validation/testing. </p>

<p>Using the Confusion Matrix from the Caret package in R, the accuracy of the model is 91%, No information Rate of .55, and a significant p value comparing the Accuracy to NIR (P&lt;.0001).</p>

<p>I'm wary to trust these results, since only 92 observations were used for training and 23 observations for testing.</p>

<p>Is the sample size enough to create a generalizable prediction model? If not, how do I determine the required sample size for model training and testing?</p>
"
"0.139572631559771","0.0881305298478463","224743","<p>This post <a href=""http://stats.stackexchange.com/questions/185926/how-to-compare-the-performance-of-two-classification-methods-logistic-regressi"">How to compare the performance of two classification methods? (logistic regression and classification trees)</a> represents a very similar problem I am just trying to solve - I have a dataset based on survey (arguably with less then desirable design), with about 3000 observations, and 100+ categorical variables. The variable of interest is binary. 
Despite identifying over 30 variables as significant, logit regression does not provide very interesting increase in the correctly predicted cases (approx. in 80% y=1, 20% y=0), the overall increase in the number of correctly predicted cases is roughly from 80% (naive model) to 81.5%.</p>

<p>Which brings me to classification trees. While very appealing as a method, they inherently do not provide very stable results (and the choice of parameters influences the output). Using several classification methods you might obtain somewhat similar (i.e. several variables can be in all trees, but what about the variables that show up only in one tree?) </p>

<p>Which brings me to a question, if there are somewhat similar, but distinct outputs, how do you interpret the results?</p>

<p>I used rpart, evtree, ctree and chaid (because chaid can only work with categorical variables, all variables in the example are categorical to allow direct comparison), but I am not much wiser in terms of what the data say.</p>

<p>I would attach a real dataset, but for the sake of discussion, let's create a random one. The variable of interest is binary variable y</p>

<pre><code>Df1 &lt;- data.frame(
  y = as.factor(sample(0:1,300,r=T)),
  x1 = as.factor(sample(1:8,300,r=T)),
  x2 = as.factor(sample(1:3,300,r=T)),
  x3 = as.factor(sample(1:2,300,r=T)),
  x4 = as.factor(sample(1:8,300,r=T)),
  x5 = as.factor(sample(1:3,300,r=T)),
  x6 = as.factor(sample(1:2,300,r=T)),
  x7 = as.factor(sample(1:8,300,r=T)),
  x8 = as.factor(sample(1:3,300,r=T)),
  x9 = ordered(sample(1:5,300,r=T))
 )
 library(""partykit"")
 library(""rpart"")
 library(""evtree"")
 library(""CHAID"")

 ct &lt;- ctree(y~ . , data = Df1, minbucket=50)
 plot(ct)
 rp &lt;- rpart(y ~ .,data=Df1, minbucket=30) 
 plot(as.party(rp))
 ev &lt;- evtree(y ~ ., data = Df1, maxdepth = 5)
 plot(ev)
 ctrl &lt;- chaid_control(minsplit=90, minbucket=30, minprob=0.05,alpha2=0.01, alpha3=-1, alpha4=0.01)
 chaid1 &lt;- chaid( y ~ ., data= Df1, control=ctrl)
 plot(chaid1,cex=0.6)
</code></pre>
"
"0.0877268926613025","0.0934765429274634","224947","<p>What are some of the best practices and steps to building models for prediction and or inferences? </p>

<p>What have been taught to me during my classes was the steps outlined in Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates"". The method to screen a large dataset with many potential predictors is to use a algorithmic approach such as Stepwise, best subset regression, etc. Then verify the model after the fact for potential collinearity, confounders, etc.  However, I have read much criticism on this site in regards to those said steps and methods.</p>

<p>For example - if I was provided a dataset with ~100 potential predictors, what would be the best practice to selecting those said predictors for inclusion or exclusion of the model for prediction/inference ? </p>

<p>According to Hosmer et al., the steps would be to perform univariate analysis to screen for all of those potential predictors (p &lt; .25), then move to inclusion of those said predictors to a multivariate model. Take a stepwise approach to removing insignificant predictors, then add back and verify the significance of each non significant predictor. </p>

<p>However - the more I've read on this site the more confused I've gotten about what is considered best practices, and I've come to question more and more of what was taught during my classes.</p>

<p>Once again just to reiterate - </p>

<ol>
<li><p>What would be the best practices for building a model for obtaining unbiased measure of association for each individual predictors?</p></li>
<li><p>What would be the best practices for building a model strictly for prediction?</p></li>
</ol>

<p>I'm still learning much about the world of data science and appreciate any help that is provided!</p>

<p>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</p>
"
"0.20385887657505","0.120677698006369","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225645","<p>I am trying to perform a logistic regression with the following code </p>

<blockquote>
  <p><code>Y ~ x1+x2+x3,data=data, family=binomial(link=""logit"")</code>. </p>
</blockquote>

<p>However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? </p>
"
"0.131590338991954","0.0934765429274634","225697","<p>Let me give a simple example,</p>

<pre><code>set.seed(100)
disease = sample(c(0,1),100,replace = TRUE)
snp1 = sample(c(""AA"",""AB"",""BB""),100,replace = TRUE)
snp2 = sample(c(""XX"",""XY"",""YY""),100,replace = TRUE)

summary(glm(disease~snp1*snp2, family = binomial))
</code></pre>

<p>output1</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  
snp1AB         5.232e-01  9.718e-01   0.538   0.5903  
snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .
snp2XY         4.074e-16  8.498e-01   0.000   1.0000  
snp2YY         1.504e+00  9.280e-01   1.621   0.1051  
snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  
snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  
snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  
snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71
</code></pre>

<p>Output2</p>

<pre><code>snp12 = interaction(snp1,snp2)
summary(glm(disease~snp12, family = binomial))


Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  
snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  
snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .
snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  
snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  
snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  
snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,</p>

<ol>
<li>Are both output1 and output2 same ? </li>
<li>If same, which one is more appropriate?</li>
<li>How to interpret the coefficients (and odds ratios) in each case?</li>
</ol>
"
"0.0657951694959769","0.0311588476424878","225713","<p>Let's say that I have a full and a restricted model that looks like this:</p>

<pre><code>Full&lt;- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method=""logistic"")

Restricted&lt;- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method=""logistic"")
</code></pre>

<p>I want to conduct F-tests to determine whether the information from the <code>X4</code>variable statistically improves our understanding of Y. </p>

<p>What command is convenient for carrying out this test for logistic regression? Is it <code>aov()</code>? 
For example: </p>

<pre><code>summary(aov(Y ~ X1+X2+X3+X4)) #Full model
summary(aov(Y ~ X1+X2+X3)) #Restricted model
</code></pre>

<p>In linear regression case this would be the way to do it, I am not sure for ordered logistic regression...</p>
"
"NaN","NaN","226703","<p>I have a language learning study with a few different dependent variables. For the majority of the tasks, responses to each item are either right (1) or wrong (0), and thus the data has been analysed with a mixed effects logistic regression.</p>

<p>However, I have one task that has allowed for partial scoring - so participants can score 0, 0.5, or 1. Is there any way to analyse this in a consistent way to the other tasks? I wondered about multinomial regressions but (a) these seem complicated (no lme4 option?), and (b) I'm not sure whether it's appropriate given that the categories are related? There are also very few 0.5 scores.</p>

<p>Any pointers would be much appreciated, many thanks in advance. </p>
"
"0.116310526299809","0.0881305298478463","227013","<p>I'm new to propensity score matching (PSM). So, my questions can be bit trivial.</p>

<p>1) Suppose I've 3 treatment levels and want to check the effectiveness of the treatment levels. Treatment levels are taking drug on time, not taking drug on time and not taking drug at regular interval. For this I need to do multinomial logistic regression. </p>

<p>But in PSM we do case-control study. So, how we are going to define which will be the case and which will be the control? Will it be the case that we will use one group as control for each time and other 2 groups as case?</p>

<p>2) Can anyone please tell me which package to use for multilevel group in R. I checked <a href=""http://stats.stackexchange.com/questions/81434/compare-regressions-among-more-than-2-groups"">this</a> link. But this link is old. I also checked <a href=""https://cran.r-project.org/web/packages/twang/"" rel=""nofollow"">this</a> package which seems to do multilevel. But is there any other option for packages?</p>
"
"0.228598483247332","0.116585732444878","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"NaN","NaN","228445","<p>I have an ordinal logistic regression model that I fit using <code>vglm</code>. The dependent variable is ordinal with multiple levels. I would like to fix the intercepts of the model to known values. Is there a way I could do this?</p>

<pre><code>vglm(y ~ x , data = subset(data), family = propodds)
</code></pre>

<p>Thanks a lot!</p>
"
"0.174077655955698","0.0824385620013739","228493","<p>The literature on Survival Analysis is mainly from the Medical science where tipically the researcher want to evaluate the effect of a treatment to that of another one. So far, all the example I read and studied thus contain one or more categorical variable (with at least 2 levels) and possibly some continuous variable as a covariate. Anyway the main interest is on a categorical variable (e.g. treatment).</p>

<p>Is it possible and correct to run a non parametric Cox model (or alternatively a parametric one) using only one or more continuous variables? In particular without categorizing the continuous var into 2 or more groups? </p>

<p>Something like a logistic regression. </p>

<p>To give you a more practical example, I'm trying to model the survival of say bush in a field depending on the number of cows in the same field. </p>

<p>I'm pretty sure it can be done but the lack of examples leave me in the doubt.</p>

<p>If possible how can one use the predict function for example to predict the survival when the predictor has a specified value? like survival of my plant when 10 cows are in the field...</p>

<p>any help is welcome!</p>
"
"0","0.0311588476424878","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.174077655955698","0.0824385620013739","229336","<p>I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.</p>

<p>The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.</p>

<p>I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)</p>

<p>Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:</p>

<p>I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciated</p>

<pre><code>Here is the R output and sample of dataset:





 &gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+ 
                                  D2011+D2012,family=binomial, data=orf)
      summary(mod)

    Call:
    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) + 
        D2011 + D2012, family = binomial, data = orf)

    Deviance Residuals: 
       Min      1Q  Median      3Q     Max  
    -1.862  -1.293   1.023   1.065   1.318  

    Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)        0.3917290  0.1626769   2.408    0.016 *
    F2011              0.0003269  0.0002782   1.175    0.240  
    F2012             -0.0003596  0.0002786  -1.291    0.197  
    as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708  
    D2011             -0.0311978  0.0272068  -1.147    0.252  
    D2012              0.0226963  0.0274981   0.825    0.409  
    Small sample data set:

    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012
    155     150     1       0               0               0
    740     760     2       0               1               1
    1000    850     1       0               0               0
    1630    1520    1       1               1               1
    0       460     1       0               0               0
    1300    1335    1       0               1               1
    450     450     1       0               0               0
    390     730     1       1               0               1
    390     380     2       0               0               0
    600     600     2       0               0               0
</code></pre>
"
"0.201604912253002","0.13219579477177","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"0.0759737176397586","0.0539687072220866","229702","<p>In a follow up to one of the business problem i have discussed here,<br>
for one of the logistic regression model, i have 19 predictor variables of     which around 8 are categorical with multiple categories.   </p>

<p>Instead of including all in the model where one could correlate to another,<br>
i am thinking of using a chi square test of independence at a given p value.<br>
With this i would come to realization if there is any correlation among the     categorical predictors.  </p>

<p>If so i would include the correlated variables one by one in the model<br>
and would used the significant one.<br>
I have researched on this on SE where they have suggested to use chi-<br>
square but unsure if my approach to drop these correlated var is justified.<br>
Please advice. </p>
"
"0.119027940128723","0.103342206529982","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0930484210398471","0.0881305298478463","230167","<p>I started tinkering with the random forest (RF) model in R recently. I made a long list of coding mistakes on my way to getting a final solution. The output from the mistake that surprised me was when I used the sample operation incorrectly and used the same records for training and validation. The RF algorithm accurately classified all but about 5 of 2000 responders. This shocked me. I later learned how to correctly sample.</p>

<p>The idea that RF could memorize specific records is the focus of my concern. Even though each tree votes on whether the record is responder or non-responder, for example, I think probability has been short-circuited for validation or non-training scoring files by a type of specification bias that ignores maximum likelihood if the validation or scoring record matches a record combination (of fields) that was used in training. This circumstance is how a Relation Database System (RDB) dictionary helps the Oracle or DB2 optimizer (it memorizes performance info to optimize the next exact job/query). The very nature of variability on earth (including statistics) implies that there will be some degree of inconsistencies in observed phenomenon. RF ignores at least some potential variability and the very nature of maximum likelihood probabilities if it correctly classifies almost 100% of records if the validation records are the training records. What happened to the accumulation of (or count of) observed inconsistencies in the data? I think this is a type of specification bias that ignores inherent variability for the validation or non-training scoring data file.</p>

<p>The idea that RF has outperformed Logistic Regression doesn't preempt the issue.</p>

<p>Could we agree that RF is more ""Rule Induction"" than probability?</p>
"
"0.0465242105199235","0.0220326324619616","230760","<p>I need to do a multinomial logistic regression with a nominal variable, and I've heard about the Gmulti package in r ,and how it provides automatic selection methods models, However all the examples that i found are only applied on binaire logistic regression, so I wonder if it's even working on a multinomial logistic regression and in case is true, is the Gmulti take in consideration the problem of multicolinearity between the independents variables. Please help me thank you </p>
"
"0.0657951694959769","0.0311588476424878","230776","<p>I'am trying to do a multinomial logistic regression to explain the political orientations in Tunisia but I'am really confused about using the vglm() function from package VGAM or the multinom() function from package nnet or the mlogit function... can someone tell me please the difference and wich one is the best ?
Thank you very much</p>
"
"0.0379868588198793","0.0539687072220866","230902","<p>I collected data from 70 participants (see end of question for the data, and for <code>R</code> code for reproducing purposes). I have the following binary variables:</p>

<ol>
<li><code>Condition</code> (experiment, control) [IV1]</li>
<li><code>Gender</code> (male, female) [IV2]</li>
<li><code>Willingness to volunteer</code> (yes, no) [DV]</li>
</ol>

<p>I want to analyze my data using a two-way logistic regression following Van Vugt et al., 2007 (pss.sagepub.com/content/18/1/19.abstract), as they had the same design I have (all binary variables, 2 IVs, 1 DV). On page 20, paragraph 3, they say:</p>

<p><a href=""http://i.stack.imgur.com/9Bmc2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9Bmc2.png"" alt=""enter image description here""></a></p>

<p><strong>First question:</strong> How can I analyze my data to get the statistics they report? I am not interested in $p_{rep}$, though, because as I understand, it is no longer reported in scientific papers. But I would like to report regular <em>p</em> values instead.</p>

<p>They next report the following:</p>

<p><a href=""http://i.stack.imgur.com/z2Sfp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/z2Sfp.png"" alt=""enter image description here""></a></p>

<p><strong>Second question:</strong> How can I produce a similar graph and get the statistics they report here?</p>

<p>I'm using either R, SPSS or SAS.</p>

<p><code>R</code> code to reproduce the data:</p>

<pre><code>condition &lt;- c(rep(0,40),rep(1,30))
gender &lt;- c(rep(0,18),rep(1,22),rep(0,12),rep(1,18))
volunteer &lt;- c(rep(0,12),rep(1,6),rep(0,7),rep(1,15),rep(0,5),rep(1,7),rep(0,14),rep(1,4))

data &lt;- as.data.frame(matrix(c(condition, gender, volunteer),
                             nrow = 70,
                             ncol = 3,
                             dimnames = list(c(),c(""condition"", ""gender"", ""volunteer""))))

print(data)

   condition gender volunteer
1          0      0         0
2          0      0         0
3          0      0         0
4          0      0         0
5          0      0         0
6          0      0         0
7          0      0         0
8          0      0         0
9          0      0         0
10         0      0         0
11         0      0         0
12         0      0         0
13         0      0         1
14         0      0         1
15         0      0         1
16         0      0         1
17         0      0         1
18         0      0         1
19         0      1         0
20         0      1         0
21         0      1         0
22         0      1         0
23         0      1         0
24         0      1         0
25         0      1         0
26         0      1         1
27         0      1         1
28         0      1         1
29         0      1         1
30         0      1         1
31         0      1         1
32         0      1         1
33         0      1         1
34         0      1         1
35         0      1         1
36         0      1         1
37         0      1         1
38         0      1         1
39         0      1         1
40         0      1         1
41         1      0         0
42         1      0         0
43         1      0         0
44         1      0         0
45         1      0         0
46         1      0         1
47         1      0         1
48         1      0         1
49         1      0         1
50         1      0         1
51         1      0         1
52         1      0         1
53         1      1         0
54         1      1         0
55         1      1         0
56         1      1         0
57         1      1         0
58         1      1         0
59         1      1         0
60         1      1         0
61         1      1         0
62         1      1         0
63         1      1         0
64         1      1         0
65         1      1         0
66         1      1         0
67         1      1         1
68         1      1         1
69         1      1         1
70         1      1         1
</code></pre>
"
"0.186649879976711","0.142787777889297","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.134303827337563","0.0763232776972177","231777","<p>I have a data frame with 1200 observations and 30 variables and I'am trying to do a multinomial logistic regression to explain the intentions of vote of Tunisian citizens using multinom(). my dependent variable has 10 levels.
When I executed the command multinom () i got this warning 
Warning messages: 1: In sqrt(diag(vc)) : NaNs produced </p>

<p>so i reduced the number of the predictor variables to 13 , the levels of my dependent variable to only 3 and the warning message no longer appears , but once I calculate the p.value the mojority of my predictor variables are non significant.</p>

<pre><code>      &gt; str(k)
    'data.frame':   1081 obs. of  19 variables:
     $ URBRUR    : Factor w/ 2 levels ""Rural"",""Urban"": 2 2 2 2 2 2 2 2 2 2  ...
     $ REGION    : Factor w/ 24 levels ""Ariana"",""Beja"",..: 23 23 23 23 23 23 23 23 23 23 ...
     $ classe_age: Factor w/ 5 levels ""60 ans et plus"",..: 3 5 1 1 3 1 5 4 1 2 ...
       $ Q3A       : Factor w/ 5 levels ""Fairly bad"",""Fairly good"",..: 2 1 1 4 4 4 2 4 1 3 ...
       $ Q3B       : Factor w/ 5 levels ""Fairly bad"",""Fairly good"",..: 2 1 1 3 1 4 2 4 1 3 ...
       $ Q7        : Factor w/ 2 levels ""Going in the right direction"",..: 1 2 2 2 2 2 2 2 2 1 ...
       $ Q14       : Factor w/ 4 levels ""Not at all interested"",..: 4 3 3 2 3 3 3 3 3 4 ...
       $ Q27       : Factor w/ 9 levels ""Did not vote for some other reason"",..: 6 6 6 6 6 3 6 6 6 1 ...
       $ Q46A      : num  9 5 8 0 3 3 4 5 0 3 ...
       $ Q63PT1    : Factor w/ 8 levels "" Services gouvernementaux"",..: 5 5 4 4 4 4 5 4 4 5 ...
       $ Q89A      : Factor w/ 9 levels ""Non"",""Oui, autre"",..: 7 1 1 8 5 1 1 1 1 1 ...
       $ Q96       : Factor w/ 3 levels ""No (looking)"",..: 3 2 2 2 1 2 2 3 2 1 ...
       $ Q96_ARB   : Factor w/ 9 levels ""Agriculteur exploitant"",..: 2 6 4 4 1 6 7 4 6 6 ...
       $ Q97       : Factor w/ 4 levels ""Aucune Ã©ducation formelle "",..: 1 3 1 4 4 3 4 3 1 4 ...
       $ Q98B      : Factor w/ 4 levels ""Not at all important"",..: 4 4 4 4 3 4 4 4 4 4 ...
     #the logistic regression
      library(nnet)
      k$out=relevel(k$Q99,ref = ""Nahdha"")
     fit=multinom(out ~ URBRUR+ REGION +    classe_age+ Q3A +Q3B+ Q7 +  Q14+    Q27+ Q46A+  Q63PT1+ Q96+ Q96_ARB+ Q97   + Q98B,data=k,maxit=3000)

     summary(fit)
     #calculate the p.value
     z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
     p &lt;- (1 - pnorm(abs(z), 0, 1))*2
     p
</code></pre>

<p>this is a part from the output R  </p>

<pre><code>                        (Intercept) URBRUR[T.Urban] REGION[T.Beja] REGION[T.Ben        Arous]
          CPR            0.0000000       0.8006384     0.50724591           0.3490626
          Nahdha         0.6480962       0.9298628     0.09299337           0.2426325
          Nidaa Tounes   0.1547996       0.1210917     0.01340229           0.5486973
                           REGION[T.Bizerte] REGION[T.Gabes] REGION[T.Gafsa]
           CPR                  0.6667980      0.86525482      0.01971166
          Nahdha               0.2933951      0.03008731      0.05240173
          Nidaa Tounes         0.5154798      0.51222561      0.03301253
                         REGION[T.Jendouba] REGION[T.Kairouan] REGION[T.Kasserine]
          CPR                  0.21477728          0.4552543          0.53160327
         Nahdha               0.01548534          0.9322695          0.22102722
         Nidaa Tounes         0.06993081          0.7833111          0.09259959
                       REGION[T.Kebili] REGION[T.Le Kef] REGION[T.Mahdia]
           CPR                0.49607138        0.0000000        0.3084810
           Nahdha             0.09437504        0.6338189        0.1629434
           Nidaa Tounes       0.17968658        0.1360486        0.1955159
</code></pre>

<p>I'm sorry if I am asking a complicated question but I would like an explication for this issue</p>

<pre><code>   &gt; table(k$out)

    Ne pas voter       Nahdha Nidaa Tounes 
     307          292          266 
</code></pre>
"
"0.256161261580242","0.187021404178367","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.0465242105199235","0.0440652649239232","232346","<p>As important as I have found external model validation to be, there is certainly a lack of material out there. The closest thing I have found is a paper that is focused on external validation for a logistic regression model. </p>

<p>External validation in this context refers to applying a model to a different population than the model was originally built on. </p>

<p>I am trying to build up a defense for a piece of intuition I have about this topic. I have a SVM regression model that is build on ~ 15K observations. The modeling process was a difficult one. My intuition tells me that the dataset for this external validation should be as large as the dataset the model was built on (15K). Does this intuition line up with any work that has been done?</p>
"
"0.149209419390598","0.0824385620013739","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.0930484210398471","0.0440652649239232","232548","<p>I'm doing research into understand the influential factors within a logistic regression model I've built in R using the glm() function.</p>

<p>From my research, it seems that using the summary() function to summarize the model is a popular method to identify which variables are significant. What I can't seem to find however is a description of what the summary function is doing to determine the significance codes (eg. the *) for each variable. <a href=""http://stats.stackexchange.com/questions/72258/how-to-interpret-the-significance-code"">This answer</a> states that the significance codes are simply categorizations of the p-value, but I don't really understand that. </p>

<p>Is there anyone out there that could maybe help me understand how R computes this?</p>
"
"0.161164592805076","0.127205462828696","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0930484210398471","0.0440652649239232","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.1176979772673","0.0696733014291618","233178","<p>I have a database with 1200 observations and 14 variables and I'am trying to do a classification tree for my dependent nominal variable who hase 4 modality</p>

<pre><code>    &gt; table(testarbre2$Q99)

  Autres       Nahdha Ne pas voter Nidaa Tounes 
     248          351          303          298 
</code></pre>

<p>at firt i tried to do a multinom logistic regression but i got the mojority of my predictor variables non significant. it seems that Even with 1200 people I was trying to fit a model for which I don't have sufficient data. 
so i tried to do a classification tree using the package rpart from R 
but the problem is that the error is so high about 65% and more, and the missclassification is about 70% 
this is the code R that i used </p>

<pre><code>   #preparation of the data
   set.seed(26)
   train=sample(1:nrow(testarbre2),nrow(testarbre2)*7/10)
   test=-train
   training_data=testarbre2[train,]
   testing_data=testarbre2[test,]
   testing_vote=vote[test]

   #fitting the model
   library(rpart)
   library(rpart.plot)
   Tree &lt;- rpart(Q99~.,data=training_data)
   rpart.plot(Tree)
   printcp(Tree)
   plotcp(Tree)

    #Construction of the complete tree
  Tree &lt;-rpart(Q99~.,data=training_data,control=rpart.control(minsplit=50,cp=0))

     #Prune the tree
    treeOptimal &lt;- prune(Tree,cp=Tree$cptable[which.min(Tree$cptable[,4]),1])
    rpart.plot(treeOptimal)

   #Prediction
   a=predict(ptitanicOptimal,testing_data2,type = ""class"")
   mc=table(a,testing_vote2)
</code></pre>

<p>I don't know if i missed a step or i used a wrong approach in the construction of my classification tree or the database is causing the problem</p>

<p>Please someone help me to understand what's wrong with my model</p>
"
"0.161164592805076","0.0763232776972177","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.0465242105199235","0.0440652649239232","233435","<p>I want to model a logistic regression test with the Data I have. I have weather data as predictors. Some predictors have granularity of every minutes, 15 minutes, 5 minutes and such while one has granularity of one hour only. Would it be possible to perform Logistic Regression with predictors having different granularity?</p>
"
"0.0930484210398471","0.0440652649239232","233802","<p>I've been given a data set containing 155 training examples and 108 features.I removed the features with more than 79 NA values and brought then them down to 99. I trained using the first 140 examples and used the rest for testing the prediction.The target variable is a discrete value i.e 0,1,2,3,4 though not restricted to those. I tried using multinomial logistic regression and random forest and got poor accuracy. Is there an algorithm that performs well on small data sets? Decision trees and regression don't seem to be working well for this. I am using R for the data analysis</p>
"
"0.149209419390598","0.0824385620013739","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.158703920171631","0.103342206529982","234541","<p>I am currently doing an analysis for my Master Thesis and encountered some results I cannot explain.</p>

<p>In my paper, I am trying to explore factors that decide whether people joined a local energy initiative or not. Since I have a lot of different variables, my instructor suggested a model building approach. Concretely, I am adding sets of predictors to my logistic regression and only keep those that are significant in the model, before adding the next set. To assess model fit, I was told to use classification tables.</p>

<p>My problem now is the following:</p>

<p>I start with a set of dummies to control for participants coming from different neighbourhoods. This basic model classifies 56% of cases correctly. Now I add the second set of predictors and some of them are significant, so I keep those in the model. If I now use the classification table again, my classification got worse. Even worse than chance! (48%).</p>

<p>How can I find significant predictors but my model gets worse than chance?</p>

<p>EDIT FOR ADDITIONAL INFO:</p>

<p>My Dataset consits of 636 cases. 318 are partakers of the initiative, 318 are not partakers. The sets of variables I use are structured as follows:</p>

<p>1) ""Control"": People come from 30 different neighbourhoods, so I added 29 dummy variables to control for differences due to neighbourhood membership (not the best approach, I know, but IÂ´m just following orders on this one)</p>

<p>2) Individual predictors: 15 demographic and psychological variables</p>

<p>3) Assessment of group predictors: 8 variables that measure how individuals perceive the group of potential partakers</p>

<p>I used the classification tables on the same data that I used for building the model, unfortunately I only have this one dataset and IÂ´m trying to figure out which predictors are most promising for future (causational) research.</p>
"
"0.0657951694959769","0.0311588476424878","234672","<p>It's well known that there are <a href=""http://rstatistics.net/variable-importance-of-predictors-that-contribute-most-significantly-to-a-response-variable-in-r/"" rel=""nofollow"">a lot of feature selection libraries out there</a> for R. What are the most commonly used (and statistically sound) packages to use when selecting features for use in a logistic regression?</p>
"
"0.0882734829504749","0.0696733014291618","234763","<p>I have a fairly large dataset ($\approx 3 \bar{M}$ observations for a dozen candidate predictors) and I would like to perform a logistic regression on that dataset.
I have a problem of separation in that dataset so usual model can't converge. That's why I am using Firth penalization (logistf package for R) to have my model to adjust.</p>

<p>I would like to select the best subset of variables for my final model but I can't find the proper way to do that. I know that stepwise selection is out of question and I usually would use L1 or L2 penalized regression so that some coefficients are reduced to 0.</p>

<p>My problem is : the function I am using to adjust my model doesn't handle extra penalization so no Elasticnet-Firth regression.</p>

<p>Is there, apart from stepwise selection, another way to select my variables ?</p>

<p>Thanks in advance !</p>
"
"0.0537215309350254","0.0636027314143481","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.113960576459638","0.0359791381480577","234998","<p>For testing purposes I made up some correlated data in R like this:</p>

<pre><code>mydata = data.frame(
  outcome   = c(1, 0, 1, 0, 0, 1, 1, 0, 1, 1),
  predictor = c(0.1, -0.2, 0, 0.1, -0.3, 0.3, 0.2, -0.1, 0.1, 0.1)
)
</code></pre>

<p>Then I did this in order to create a logistic model that modeled this data:</p>

<pre><code>model1 = glm(family = binomial, formula = outcome ~ predictor, data = mydata)
</code></pre>

<p>Running <code>plot(model1)</code> yields the following plots:</p>

<p><a href=""http://i.stack.imgur.com/tbdpD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tbdpD.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/4O3jW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4O3jW.png"" alt=""enter image description here""></a></p>

<p>I need answers to some questions in order to understand how to perform diagnostics on such a logistic model. As someone with only an introductory course in statistics I'm having trouble gathering knowledge on how to interpret the plots.</p>

<ol>
<li>What do the ""Predicted Values"" in the first plot represent?</li>
<li>What does residual mean in the context of logistic regression?</li>
<li>Which of these plots can in any way be useful for model diagnostics based on real data? How?</li>
</ol>
"
"0.162834736819732","0.0881305298478463","235272","<p>Given a multinomial logistic regression model with 4 independent variables, 4 relevant interactions and a dependent variable with 3 categorical outcomes, I wanted to test for linearity of the logit.</p>

<p>R told me, it is always a good idea to scale the independent variables to the range [0,1], so I did.</p>

<p>So when I wanted to test for linearity of the logit by including the interactions between each predictor and its natural log in the model, I found that two of them were significant, so I had to reject the hypothesis of linearity of the logit.</p>

<p>However, when I ran the same model without scaling my predictors to [0,1] (the original range is [0,1500]) p-values for the log interactions were > 0.8 suggesting that I don't have to reject the linearity of the logit assumption.</p>

<p>When I looked at the transforms in the different ranges, it made sense why the outcome would be different:</p>

<p><a href=""http://i.stack.imgur.com/WFEbW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WFEbW.png"" alt=""log(x)*x[0,1]""></a></p>

<p><a href=""http://i.stack.imgur.com/PiPOE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PiPOE.png"" alt=""log(x)*x[0,1000]""></a></p>

<p>So my question is, does the Box Tidwell test for linearity of the logit require predictors to be in the range [0,1]? If so, why is it so hard to find any mention of this on the internet? If not, what is a valid range for the test? Because the test-results obviously depend on the range.</p>

<p>Thank you very much for your help.</p>
"
"0.1176979772673","0.0696733014291618","235402","<p>I am a newbie at R. I am trying to do some logistic regressions. My predictors are categorical, and most have more than two levels.</p>

<p>A couple questions:
1. It looks like R already creates the relevant contrasts for the categorical predictors (I am used to SAS where I need to specify all the contrasts). Is this correct for R? As in, I do <strong>not</strong> need to manually create the contrasts myself?</p>

<ol start=""2"">
<li><p>the ""family=binomial"" step in the glm syntax, will I always need to write this regardless of the number of predictors, and even if the categorical predictors have more than two levels (DV is always binary)?</p></li>
<li><p>For interpreting the results, if I use a categorical predictor (e.g., lettergroup) with different levels (a, b, c, d) as my predictor, with the binary DV as 1=yes and 0=no, if R created the lettergroupb contrast, and it is a significant, positive coefficient, this means b is more likely to say yes compared to a, c, and d? thank you!</p></li>
</ol>
"
