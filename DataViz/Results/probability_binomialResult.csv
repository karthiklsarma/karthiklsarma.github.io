"V1","V2","V3","V4"
"0.210850961768776","0.195467509243381","  4175","<p>I am trying to understand how I can use resampling techniques to compliment my pre-planned analyses. This is not homework. I have a 5 sided die. 30 subjects call a number (1-5) and then roll the die. If it matches it's a hit, if not it's a miss. Each subject does this 25 times. </p>

<p>N is the the number of trials (=25) and p is the probability of getting it correct (=.2) then the population value (mu) of the mean number correct is n*p=5. The population standard deviation, sigma, is square-root(n*p*[1-p]), which is 2.</p>

<p>The experimental hypothesis (H1) is that subjects in this study will score above chance (above mu). The null hypothesis (H0) assumes a binomial distribution for each subject (they will score at mu).</p>

<p>[<em>Please don't get too worried about why I am doing this. If it helps you to understand the problem then you can think of it as an ESP test (and therefore I am testing the ability of subjects to score above mu). Also if it helps, imagine that the task is a virtual reality die throwing task, where the virtual 5-sided die performs according to chance. There can be no bias from an imperfect die because the die is virtual.</em>]</p>

<p>Okay. So before I conducted the ""experiment"" I had planned to compare the 30 subjects score with a one-sample t-test (comparing it to the null that mu=5). Then I discovered that the one-sample z-test was a more powerful test given what we know about the null hypothesis. Okay.</p>

<p>Here is a simulation of my data in R:</p>

<pre><code>binom.samp1 &lt;- as.data.frame(matrix(rbinom(30*1, size=25, prob=0.2), ncol=1))
</code></pre>

<p>Now R has a binom.test function, which gives the p-value regarding the number of successes over the number of trials. For my collected data (not the simulated data given):</p>

<pre><code>&gt;binom.test(174, 750, 1/5, alternative=""g"")
number of successes = 174, number of trials = 750, p-value = 0.01722
</code></pre>

<p>Now the one-sample t-test that I had originally planned to use (mainly because I'd never heard of the alternatives - should've paid more attention in higher statistics):</p>

<pre><code>&gt;t.test(binom.samp1-5, alternative=""g"")
t = 1.7647, df = 29, p-value = 0.04407
</code></pre>

<p>and for completedness sake: the one-sample z-test (<a href=""http://rgm2.lab.nig.ac.jp/RGM2/R_man-2.9.0/library/BSDA/man/z.test.html"" rel=""nofollow"">BSDA package</a>):</p>

<pre><code>&gt;z.test(binom.samp1, mu=5, sigma.x=2, alternative=""g"")
z = 2.1909, p-value = 0.01423
</code></pre>

<p>So. My first question is, <strong>am I right in concluding that the <a href=""http://sekhon.berkeley.edu/stats/html/binom.test.html"" rel=""nofollow"">binom.test</a> is the correct test given the data and hypothesis?</strong> In other words, does <em>t</em> approximate to <em>z</em> which approximates to the exact binom.test (<a href=""http://en.wikipedia.org/wiki/Bernoulli_trial"" rel=""nofollow"">Bernoulli trial</a>)?</p>

<p>Now my second question relates to the resampling methods. I have several books by Philip Good and I've read plenty on permutation and bootstrapping. I was just going to use the one-sample permutation test given in the <a href=""http://pbil.univ-lyon1.fr/library/DAAG/html/onet.permutation.html"" rel=""nofollow"">DAAG</a> package:</p>

<pre><code>&gt;onet.permutation(binom.samp1-5)
0.114
</code></pre>

<p>And the perm.test function in the <a href=""http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/exactRankTests/html/perm.test.html"" rel=""nofollow"">exactRankTests</a> package gives this:</p>

<pre><code>&gt;perm.test(binom.samp1, mu=5, alternative=""g"", exact=TRUE)
T = 42, p-value = 0.05113
</code></pre>

<p><strong>I have the feeling that what I want to do is conduct a one-sample permutation binom.test.</strong> The only way I can see it working is if I take a subset of the 30 subjects and calculate the binom.test and then repeat it for a large number of N. <strong>Does this sound like a reasonable idea?</strong></p>

<p>Finally, I did repeat this experiment with the same equipment (the 5 sided die) but a larger sample size (50 people) and I got exactly what I expected. My understanding is that the two studies are like a <a href=""http://en.wikipedia.org/wiki/Bean_machine"" rel=""nofollow"">Galton box</a> that hasn't filled up yet. The 30 n experiment has a bit of a skew, but had it been run for longer it would have filled up to the binomial. <strong>Is this all gibberish?</strong></p>

<pre><code>&gt;binom.test(231, 1250, 1/5, alternative=""g"")
number of successes = 231, number of trials = 1250, p-value = 0.917

&gt;t.test(binom.samp2-5)
t = -1.2249, df = 49, p-value = 0.2265

&gt;z.test(binom.samp2, mu=5, sigma.x=2)
z = -1.3435, p-value = 0.1791

&gt;onet.permutation(binom.samp2-5)
0.237

&gt;perm.test(binom.samp2, mu=5, alternative=""g"", exact=TRUE)
T = 35, p-value = 0.8991
</code></pre>
"
"0.141247084629227","0.138216401287639","  4312","<p>This is related to another <a href=""http://stats.stackexchange.com/questions/4175/resampling-binomial-z-and-t-test-help-with-real-data"">question</a> I asked recently. To recap:</p>

<p>[<em>I had 30 people call a number and then roll a 5 sided die. If the call matches the subsequent face then the trial is a hit, else it is a miss. Each subject completes 25 trials (rolls) and thus, each participant has a score out of 25. Since the die is a virtual one, it cannot be biased. Before the experiment was conducted I was going to compare the subjects score with a one-sample t-test (compared to mu of 5). However I was pointed towards the more powerful z-test, which is appropriate because we know the population parameters for the null hypothesis: that everyone should score at chance. Since NPQ means the binomial approximates to the normal or Gaussian distribution, we can use a parametric test. So I could just forget about it all and go back to the t-test I planned to use, but it seems to me that although the z-test is not often used in real research it is appropriate here. That was the conclusion from my previous question. Now I am trying to understand how to use resampling methods (either permutation or bootstrap) to compliment my parametric analysis.</em>]</p>

<p>Okay. I am trying to program a one-sample permutation <a href=""http://statistic-on-air.blogspot.com/2009/07/one-sample-z-test.html"" rel=""nofollow"">z-test</a>, using the <a href=""http://pbil.univ-lyon1.fr/library/DAAG/DESCRIPTION"" rel=""nofollow"">DAAG</a> package <a href=""http://pbil.univ-lyon1.fr/library/DAAG/html/onet.permutation.html"" rel=""nofollow"">onet.permutation</a> as inspiration. This is as far as I've got:</p>

<pre><code>perm.z.test = function(x, mu, var, n, prob, nsim){
  nx &lt;- length(x)
  mx &lt;- mean(x)
  z &lt;- array(, nsim)
  for (i in 1:nsim) {
    mn &lt;- rbinom(nx*1, size=n, prob=prob)
    zeta = (mean(mn) - mu) / (sqrt(var/nx))
    z[i] &lt;- zeta
  }
  pval &lt;- (sum(z &gt;= abs(mx)) + sum(z &lt;= -abs(mx)))/nsim
  print(signif(pval, 3))
}
</code></pre>

<p>Where: <code>x</code> is the variable to test, <code>n</code> is the the number of trials (=25) and <code>prob</code> is the probability of getting it correct (=.2). The population value (<code>mu</code>) of the mean number correct is n*p. The population standard deviation, <code>var</code>, is square-root(n*p*[1-p]).</p>

<p>Now I guess this compares x to an array composed of randomly generated binomial sample. If I centre x at 0 (variable-mu) I get a p-value. <strong>Can somebody confirm that it is doing what I think it is doing?</strong></p>

<p>My testing gives this:</p>

<pre><code>&gt; binom.samp1 &lt;- as.data.frame(matrix(rbinom(30*1, size=25, prob=0.2), 
                                     ncol=1))
&gt; z.test(binom.samp1$V1, mu=5, sigma.x=2)
data:  binom.samp1$V1 
z = 0.7303, p-value = 0.4652

&gt; perm.z.test(binom.samp1$V1-5, 5, 2, 25, .2, 2000)
[1] 0.892

&gt; binom.samp1 &lt;- as.data.frame(matrix(rbinom(1000*1, size=25, prob=0.2),
                                     ncol=1))
&gt; perm.z.test(binom.samp1$V1-5, 5, 2, 25, .2, 2000)
[1] 0.937
</code></pre>

<p><strong>Does this look right?</strong></p>

<p>UPDATE:</p>

<p>Since this obviously doesn't do what I want, I do have another angle. This <a href=""http://www.stat.umn.edu/geyer/old/5601/examp/perm.html#one"" rel=""nofollow"">website</a> offers this advice:</p>

<blockquote>
  <p>There is no reason whatsoever why a
  permutation test has to use any
  particular test statistic. Any test
  statistic will do! ... For one-sample
  or paired two-sample tests, in
  particular, for Wilcoxon signed rank
  tests, the permutations are really
  subsets. The permutation distribution
  choses an arbitrary subset to mark +
  and the complementary subset is marked
  -. Either subset can be empty.</p>
</blockquote>

<p>What about an arbitrary subset with a one-sample z-test?</p>
"
"0.110974190404619","0.108593060690767","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.0744437500478198","0.0728464396767948","  5543","<p>I have found some distributions for which BUGS and R have different parameterizations: Normal, log-Normal, and Weibull.</p>

<p>For each of these, I gather that the second parameter used by R needs to be inverse transformed (1/parameter) before being used in BUGS (or JAGS in my case). </p>

<p>Does anyone know of a comprehensive list of these transformations that currently exists?</p>

<p>The closest I can find would be comparing the distributions in table 7 of the <a href=""http://sourceforge.net/projects/mcmc-jags/files/Manuals/2.x/jags_user_manual.pdf"">JAGS 2.2.0 user manual</a> with the results of <code>?rnorm</code> etc. and perhaps a few probability texts. This approach appears to require that the transformations will need to be deduced from the pdfs separately. </p>

<p>I would prefer to avoid this task (and possible errors) if it has already been done, or else start the list here.</p>

<p><strong>Update</strong></p>

<p>Based on Ben's suggestions, I have written the following function to transform a dataframe of parameters from R to BUGS parameterizations.</p>

<pre><code>##' convert R parameterizations to BUGS paramaterizations
##' 
##' R and BUGS have different parameterizations for some distributions. 
##' This function transforms the distributions from R defaults to BUGS 
##' defaults. BUGS is an implementation of the BUGS language, and these 
##' transformations are expected to work for bugs.
##' @param priors data.frame with colnames c('distn', 'parama', 'paramb')
##' @return priors with jags parameterizations
##' @author David LeBauer

r2bugs.distributions &lt;- function(priors) {

  norm   &lt;- priors$distn %in% 'norm'
  lnorm  &lt;- priors$distn %in% 'lnorm'
  weib   &lt;- priors$distn %in% 'weibull'
  bin    &lt;- priors$distn %in% 'binom'

  ## Convert sd to precision for norm &amp; lnorm
  priors$paramb[norm | lnorm] &lt;-  1/priors$paramb[norm | lnorm]^2
  ## Convert R parameter b to JAGS parameter lambda by l = (1/b)^a
  priors$paramb[weib] &lt;-   1 / priors$paramb[weib]^priors$parama[weib]
  ## Reverse parameter order for binomial
  priors[bin, c('parama', 'paramb')] &lt;-  priors[bin, c('parama', 'paramb')]

  ## Translate distribution names
  priors$distn &lt;- gsub('weibull', 'weib',
                       gsub('binom', 'bin',
                            gsub('chisq', 'chisqr',
                                 gsub('nbinom', 'negbin',
                                      as.vector(priors$distn)))))
  return(priors)
}

##' @examples
##' priors &lt;- data.frame(distn = c('weibull', 'lnorm', 'norm', 'gamma'),
##'                     parama = c(1, 1, 1, 1),
##'                     paramb = c(2, 2, 2, 2))
##' r2bugs.distributions(priors)
</code></pre>
"
"0.141247084629227","0.153573779208488","  6728","<p>I was trying to fit my data into various models and figured out that the <code>fitdistr</code> function from library <code>MASS</code> of <code>R</code> gives me <code>Negative Binomial</code> as the best-fit. Now from the <a href=""http://en.wikipedia.org/wiki/Negative_binomial_distribution"">wiki</a> page, the definition is given as:</p>

<blockquote>
  <p>NegBin(r,p) distribution describes the probability of k failures and r
  successes in k+r Bernoulli(p) trials
  with success on the last trial.</p>
</blockquote>

<p>Using <code>R</code> to perform model fitting gives me two parameters <code>mean</code> and <code>dispersion parameter</code>. I am not understanding how to interpret these because I cannot see these parameters on the wiki page. All I can see is the following formula:</p>

<p><img src=""http://i.stack.imgur.com/Tpnyi.png"" alt=""Negative Binomial Distribution Formula""></p>

<p>where <code>k</code> is the number of observations and <code>r=0...n</code>. Now how do I relate these with the parameters given by <code>R</code>? The help file does not provide much information either. </p>

<p>Also, just to say a few words about my experiment: In a social experiment that I was conducting, I was trying to count the number of people each user contacted in a period of 10 days. The population size was 100 for the experiment. </p>

<p>Now, if the model fits the Negative Binomial, I can blindly say that it follows that distribution but I really want to understand the intuitive meaning behind this. What does it mean to say that the number of people contacted by my test subjects follows a negative binomial distribution? Can someone please help clarify this?</p>
"
"0.248145833492733","0.223395748342171","  7036","<p>This question may have been asked before, but I couldn't find it.   So, here goes.</p>

<p>From about 3000 data points that can be characterized as ""wins"" or ""losses"" (binomial), it turns out that there are 52.8% dumb luck wins.   This is my dependent variable.</p>

<p>I also have some additional data that may help in predicting the above wins that could be considered an independent variable.</p>

<p>The question I'm trying to answer is:</p>

<p>If my independent variable can be used to predict 55% wins, how many trials are required (giving me 55% wins) for me to be 99% sure that this wasn't dumb luck?</p>

<p>The following R code is purposely hacky so I can see everything that is happening.</p>

<pre><code>#Run through a set of trial sizes
numtri &lt;- seq(2720, 2840, 1)

#For a 52.8% probability of dumb luck wins, and the current trial size,
#calculate the number of wins at the 99% limit
numwin &lt;- qbinom(p=0.99, size=numtri, prob=0.528)

#Divide the number of wins at the 99% limit by the trial size to
#get the percent wins.
perwin &lt;- numwin/numtri

#Plot the percent wins versus the trial size, looking for the
#""predicted"" 55% value. See the plot below.
plot(numtri, perwin, type=""b"", main=""Looking for 0.55"")
grid()

#Draw a red line at the 55% level to show which trial sizes are correct
abline(h=0.55, lwd=2, col=""red"")

#More than one answer? Why?........Integer issues
head(numtri)
head(numwin)
head(perwin)
</code></pre>

<p>From the graph, the answer is: 2740 &lt;= numtri &lt;= 2820 </p>

<p>As you can guess, I'm also looking for the required number of trials for 56% wins, 57% wins, 58% wins, etc.   So, I'll be automating the process.</p>

<p>Back to my question.   Does anyone see a problem with the code, and if not, does anyone have a way to cleanly sneak up on the right and left edges of the ""answer""?</p>

<p><img src=""http://i.stack.imgur.com/2CSHV.jpg"" alt=""enter image description here""></p>

<p>Edit (02/13/2011) ==================================================</p>

<p>Per whuber's answer below, I now realize that to compare my alternate 55% wins (which came from the data) with my 52.8% ""dumb luck"" null (which also came from the data), I have to deal with the fuzz from both measured values.  In other words, to be ""99% sure"" (whatever that means), the 1% tail of BOTH proportions needs to be compared.</p>

<p>For me to get comfortable with whuber's formula for N, I had to reproduce his algebra.  In addition, because I know that I'll use this framework for more complicated problems, I bootstrapped the calculations.</p>

<p>Below are some of the results.   The upper left graph was produced during the bootstrap process where N (the number of trials) was at 9000.  As you can see, the 1% tail for the null extends further into the alternate than the 1% for the alternate.   The conclusion?   9000 trials is not enough.   The lower left graph is also during the bootstrap process where N was at 13,000.   In this case, the 1% tail for the null falls into an area of the alternate that is less than the required 1% value.   The conclusion?  13,000 trials is more than enough (too many) for the 1% level.</p>

<p>The upper right graph is where N=11109 (whuber's calculated value) and the 1% tail of the null extends the right amount into the alternate, aligning the 1% tails.   The conclusion?  11109 trials are required for the 1% significance level (I am now comfortable with whuber's formula).</p>

<p>The lower right graph is where I used whuber's formula for N and varied the alternate q (at both the 1% and 5% significance levels) so I could have a reference of what might be an ""acceptable zone"" for alternates.    In addition, I overlaid some actual alternates versus their associated number of data points (the blue dots).    One point falls well below the 5% level, and even though it might provide 72% ""wins"", there simply aren't enough data points to differentiate it from 52.8% ""dumb luck"".   So, I'll probably drop that point and the two others below 5% (and the independent variables that were used to select those points).</p>

<p><img src=""http://i.stack.imgur.com/sEsZx.jpg"" alt=""enter image description here""></p>
"
"0.141247084629227","0.153573779208488","  7385","<p>this is my first post. I'm truly grateful for this community.</p>

<p>I am trying to analyze longitudinal count data that is zero-truncated (probability that response variable = 0 is 0), and the mean != variance, so a negative binomial distribution was chosen over a poisson.</p>

<p>Functions/commands I've ruled out:</p>

<p>R</p>

<ul>
<li>gee() function in R does not account for zero-truncation nor the negative binomial distribution (not even with the MASS package loaded)</li>
<li>glm.nb() in R doesn't allow for different correlation structures</li>
<li>vglm() from the VGAM package can make use of the posnegbinomial family, but it has the same problem as Stata's ztnb command (see below) in that I can't refit the models using a non-independent correlation structure.</li>
</ul>

<p>Stata</p>

<ul>
<li>If the data wasn't longitudinal, I could just use the Stata packages ztnb to run my analysis, BUT that command assumes that my observations are independent.</li>
</ul>

<p>I've also ruled out GLMM for various methodological/philosophical reasons.</p>

<p>For now, I've settled on Stata's xtgee command (yes, I know that xtnbreg also does the same thing) that takes into account both the nonindependent correlation structures and the neg binomial family, but not the zero-truncation. The added benefit of using xtgee is that I can also calculate qic values (using the qic command) to determine the best fitting correlation structures for my response variables.</p>

<p>If there is a package/command in R or Stata that can take 1) nbinomial family, 2) GEE and 3) zero-truncation into account, I'd be dying to know.</p>

<p>I'd greatly appreciate any ideas you may have. Thank you.</p>

<p>-Casey</p>
"
"0.140372481268719","0.137360563948689","  9237","<p>I have several dependent variables that are measures of racial disproportionality; I've calculated them as:</p>

<p>% of events caused by racial minority group / % of events caused by racial majority group</p>

<p>I have a dependent variable for each racial minority group in my sample. I am running longitudinal Generalized Estimating Equations (GEE) on these models, however I am somewhat stumped as to which family is appropriate for these dependent variables. The probability range for my ratios are truncated at 0, as it's not possible to have negative values in my DVs. This makes me question the validity of using a Gaussian family for my models.</p>

<p>The idea behind these variables is that a ratio greater than 1 indicates some level of greater burden of events that a given racial minority is bearing compared to the racial majority, and a ratio less than 1 indicates the opposite.</p>

<ul>
<li>What would be the most appropriate family to use for my GEE regressions?</li>
</ul>

<p>EDIT:</p>

<p>I misspoke about the racial disproportionality measure I was using. The correct formula is:</p>

<p>% events by minority / % of total enrollment that is minority OVER
% events by non-minority / % of total enrollment that is non-minority</p>

<p>Because they are ratios, the number of observations with value less than 1 is comparable to the number of observations greater than 1, with the lower bound being 0 and the upper bound being non-bounded. Looking at the histograms of my response variables, they definitely seem to fit a negative binomial distribution better than the normal. The QIC (GEE adjustment to AIC) confirms this suspicion. My questions now are:</p>

<ul>
<li>Can I trust this evidence to move forward with the negative binomial family?</li>
<li>If so, how do I possibly interpret the exponentiated coefficients from the resulting models? They don't see to be Incidence Rate Ratios, as one would interpret them to be from count variables...</li>
</ul>
"
"0.122825921110129","0.137360563948689"," 12223","<p>I am trying to figure out how to control the smoothing parameters in an mgcv:gam model.</p>

<p>I have a binomial variable I am trying to model as primarily a function of x and y coordinates on a fixed grid, plus some other variables with more minor influences.  In the past I have constructed a reasonably good local regression model using package locfit and just the (x,y) values.  </p>

<p>However, I want to try incorporating the other variables into the model, and it looked like generalized additive models (GAM) were a good possibility.  After looking at packages gam and mgcv, both of which have a GAM function, I opted for the latter since a number of comments in mailing list threads seem to recommend it.  One downside is that it doesn't seem to support a local regression smoother like loess or locfit.</p>

<p>To start, I just wanted to try to replicate approximately the locfit model, using just (x,y) coordinates.  I tried with both regular and tensor product smooths:</p>

<pre><code>my.gam.te &lt;- gam(z ~ te(x, y), family=binomial(logit), data=my.data, scale = -1)

my.gam.s  &lt;- gam(z ~  s(x, y), family=binomial(logit), data=my.data, scale = -1)
</code></pre>

<p>However, plotting the predictions from the model, they are much much more smoothed compared to the locfit model.  So I've been trying to tune the model to not oversmooth as much.  I've tried adjusting the parameters sp and k, but it's not clear to me how they affect the smoothing.  In locfit, the nn parameter controls the span of the neighborhood used, with smaller values allowing for less smoothing and more ""wiggling"", which helps to capture some areas on the grid where the probability of the binomial outcomes changes rapidly.  How would I go about setting up the gam model to enable it to behave similarly?</p>
"
"0.0859602382591879","0.0841158231138067"," 14832","<p>I am trying to learn how to fit a probability distribution to a vector of data, using the program R, but there are a lot of potential probability distributions to use!  So my question is, how do I find the best distribution for my data, and how do I prove that I have picked the right distribution?  Can I acquire AIC values for a whole set of different distributions?</p>

<p>The data are observational count data of bees visiting flowers.  Each species has a certain number of visits, hence the differing frequencies.  The goal is to find the best distribution to describe the bee visitation, show that I have selected the right one, and then use that distribution to sample from randomly for a set of simulations.</p>

<p>Here is what the data looks like, it is a vector of count observations.  It is zero inflated, with a long tailed distribution (maybe zero-inflated negative binomial?).</p>

<pre><code>i.vec=c(0,63,1,4,1,44,2,2,1,0,1,0,0,0,0,1,0,0,3,0,0,2,0,0,0,0,0,2,0,0,0,0,
0,0,0,0,0,0,0,0,6,1,11,1,1,0,0,0,2)
</code></pre>

<p>And here are some basic parameters that I have calculated.  I am using standard deviation for sigma, and phi is the proportion of zeroes in the data.</p>

<pre><code>m=mean(i.vec)
#[1] 3.040816
sig=sd(i.vec)
#[1] 10.86078
tab&lt;-table(i.vec)
zero.prop&lt;-as.numeric(tab[1])/sum(as.numeric(tab))
#[1] 0.6122449
</code></pre>

<p>As you can see, the standard deviation is much greater than the mean, and I have a very high proportion of zeroes.</p>
"
"0.0701862406343596","0.0686802819743445"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.112548371022619","0.110133464732153"," 19169","<p>I'm trying to implement the extended binomial density function with support on c( 0 : (floor(N) + 1)), but I'm running into (I think) precision issues, as running:</p>

<pre><code>########################
#---DENSITY FUNCTION---#
########################
debinom &lt;- function(k, n, p, sum) {
    if (k &lt;=  n) {
  return( choose(n, k) * p^k * (1-p)^(n-k) )
  } else {
    return (1.0 - sum)
    }
}#END: pebinom

##########################################
#---CUMULATIVE DISTRIBUTION FUNCTION 2---#
##########################################
pebinom &lt;- function(x, n, p) {

  # point mass at 0
  totalDensity = cumProb = debinom(0.0, n, p, 0.0)

  k = 0
  while (k &lt;= (x)) {
    density2 = debinom(k, n, p, totalDensity)
    totalDensity = totalDensity + density2
    cumProb = cumProb + density2
    k = k + 1
  }

  k = k + 1
  density = debinom(k, n, p, totalDensity)
  cumProb = cumProb + density * (x - k)

  return (cumProb) 
}#END: debinom

############
#---TEST---#
############
for (i in 0:10) {
x = i + runif(1)
cat(x, "" "", pebinom(x, 100, 0.1), ""\n"")
}
</code></pre>

<p>gives a negative probabilities for tail values. </p>

<h1>EDIT</h1>

<p>I have changed, and mostly simplified the routines along the comments and answers I've received:</p>

<pre><code>#########################################
#---PROBABILITY DISTRIBUTION FUNCTION---#
#########################################

debinom &lt;- function(k, n, p) {

if (k &lt;=floor(n)) {

  return( choose(n, k) * p^k * (1-p)^(n-k) )

  } else if(k == (floor(n)+1)) {

    cumProb = 0.0
    for(i in 0 : floor(n)) {
      cumProb = cumProb + debinom(i, n, p)  
    }

    return (1.0 - cumProb)

    } else {

  return(0.0)
    }

}#END: pebinom

########################################
#---CUMULATIVE DISTRIBUTION FUNCTION---#
########################################
pebinom &lt;- function(x, N, P) {

cumProb = 0
for(i in 0 : (floor(x)) ) {
 cumProb = cumProb + debinom(i, N, P)
}

return(cumProb)
}
</code></pre>
"
"0.121566134770966","0.118957737857722"," 19650","<p>I did an experiment with 12 participants. They were asked to rate 3 stimuli, with the following 3 possible answers: ""increased"", ""decreased"", ""no difference"".
I have as a result this table:</p>

<pre><code>          Increase Decreased No_difference 

StimulusA   10      1           1
StimulusB   12      0           0
StimulusC    8      2           2
</code></pre>

<p>Now,if I want to understand in one shot if the ""Increase"" answer is chosen significantly more than the other two answers in all the Stimuli, is it correct if I simply use the binomial distribution summing the successes along all the 3 stimuli?</p>

<p>I mean, is it correct doing this?</p>

<pre><code>&gt; prop.test(30,36)

    1-sample proportions test with continuity correction

data:  30 out of 36, null probability 0.5 
X-squared = 14.6944, df = 1, p-value = 0.0001264
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.6652978 0.9303666 
sample estimates:
    p 
0.8333333 
</code></pre>

<p>Or it is better to use the function in this way instead?</p>

<pre><code>&gt; prop.test(c(10,12,8),c(12,12,12),c(0.5,0.5,0.5))

    3-sample test for given proportions without continuity correction

data:  c(10, 12, 8) out of c(12, 12, 12), null probabilities c(0.5, 0.5, 0.5) 
X-squared = 18.6667, df = 3, p-value = 0.0003204
alternative hypothesis: two.sided 
null values:
prop 1 prop 2 prop 3 
   0.5    0.5    0.5 
sample estimates:
   prop 1    prop 2    prop 3 
0.8333333 1.0000000 0.6666667 
</code></pre>
"
"0.099258333397093","0.0971285862357264"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.099258333397093","0.0971285862357264"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.0744437500478198","0.0728464396767948"," 20835","<p>Let's say that I generate the probability of an outcome based on a certain factor and plot the curve of that outcome. Is there any way to extract the equation for that curve from R?</p>

<pre><code>&gt; mod = glm(winner~our_bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod)

Call:
glm(formula = winner ~ our_bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7443  -0.6083  -0.5329  -0.4702   2.3518  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.781e-01  2.836e-02  -34.49   &lt;2e-16 ***
our_bid     -2.050e-03  7.576e-05  -27.07   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 42850  on 49971  degrees of freedom
Residual deviance: 42094  on 49970  degrees of freedom
AIC: 42098

Number of Fisher Scoring iterations: 4

&gt; all.x &lt;- expand.grid(winner=unique(winner), our_bid=unique(our_bid))
&gt; all.x
&gt; won = subset(all.x, winner == 1)
&gt; y.hat.new &lt;- predict(mod, newdata=won, type=""response"")
&gt; options(max.print=5000000)
&gt; y.hat.new
&gt; plot(our_bid&lt;-000:1000, predict(mod, newdata=data.frame(our_bid&lt;-c(000:1000)),
       type=""response""))
</code></pre>

<p><img src=""http://i.stack.imgur.com/WsP47.png"" alt=""enter image description here""></p>

<p>How can I go from this probability curve to an equation in R? I was hoping for something like:</p>

<pre><code>Probability = -0.08*bid3 + 0.0364*bid2 - 0.0281*bid + 4E-14
</code></pre>
"
"0.0744437500478198","0.0971285862357264"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.131306432859723","0.128489042187512"," 22450","<p>We draw $n$ values, each equiprobably among $m$ distinct values. What are the odds $p(n,m,k)$ that at least one of the values is drawn at least $k$ times? e.g. for $n=3000$, $m=300$, $k=20$.</p>

<p>Note: I was passed a variant of this by a friend asking for ""a statistical package usable for similar problems"".</p>

<p>My attempt: The number of times a particular value is reached follows a binomial law with $n$ events, probability $1/m$. This is enough to get odds $q$ that a particular value is reached at least $k$ times [Excel gives $q\approx 0.00340$ with <code>=1-BINOMDIST(20-1,3000,1/300,TRUE)</code>]. Given that $n\gg k$, we can ignore the fact that odds of a value being reached depends on the outcome for other values, and get an <em>approximation</em> of $p$ as $1-(1-q)^m$ [Excel gives $p\approx 0.640$ with <code>=1-BINOMDIST(20-1,3000,1/300,TRUE)^300</code>].</p>

<p><em>update: the exponent was wrong in the above, that's now fixed</em></p>

<p>Is this correct? <em>(now solved, yes, but the approximation made leads to an error in the order of 1% with the example parameters)</em></p>

<p><strong>What methods can work for arbitrary parameters $(n,m,k)$?</strong> Is this function available in R or other package, or how could we construct it? <em>(now solved, both exactly for moderate parameters, and theoretically for huge parameters)</em></p>

<p>I see how to do a simulation in C, what would be an example of a similar simulation in R? <em>(now solved, a corrected simulation in R and another in Python gives $p\approx 0.647$)</em></p>
"
"0.099258333397093","0.0971285862357264"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.115801388963275","0.129504781647635"," 23270","<p>I am modeling some data where I think I have two crossed random effects.  But the data set is not balanced, and I'm not sure what needs to be done to account for it.</p>

<p>My data is a set of events.  An event occurs when a client meets with a provider to perform a task, which is either successful or not.  There are thousands of clients and providers, and each client &amp; provider participates in varying numbers of events (roughly 5 to 500).  Each client and provider has a level of skill, and the chance that the task is successful is a function of the skills of both participants.  There is no overlap between clients and providers.</p>

<p>I am interested in the respective variances of the population of clients and providers, so we can know which source has a bigger effect on the success rate. I also want to know the specific values of the skills among the client and providers we actually have data for, to identify best/worst clients or providers.  </p>

<p>Initially, I want to assume that the probability of success is driven solely by the combined skill levels of the client and provider, with no other fixed effects.  So, assuming that x is a factor for the client and y is a factor for provider, then in R (using package lme4) I have a model specified as:</p>

<pre><code>  glmer( success ~ (1 | x) + (1 | y), family=binomial(), data=events)
</code></pre>

<p>One problem is that clients are not evenly distributed across providers.  Higher skill clients are more likely to be matched up with higher skill providers.  My understanding is that a random effect has to be uncorrelated with any other predictors in the model, but I'm not sure how to account for it.</p>

<p>Also, some clients and providers have very few events (less than 10), while others have many (up to 500), so there's a wide spread in the amount of data we have on each participant.  Ideally this would be reflected in a ""confidence interval"" around each particpants skill estimate (although I think the term confidence interval isn't quite correct here).</p>

<p>Are crossed random effects going to be problematic because of the unbalanced data?  If so, what are some other approaches I should consider?</p>
"
"0.0859602382591879","0.0841158231138067"," 24181","<p>Say I have 5 binary variables and 2 normal variables. I want to get the probability of success, say one of the variable 1 or 0, 1 for success. How can I do that?</p>

<p>I tried <code>glm(A~B+C+D+E, binomial, data)</code> in R.</p>

<p>$A$ is a binary variable for which I want to know the probability of success. 
$B$, $C$, $D$, $E$ are all binary variables. Am I doing right?</p>
"
"0.0496291666985465","0.0485642931178632"," 24290","<p>If p is the probability of success of a binomial trial i would like to calculate the number of trials n required that if performed would give a probability x of at least one success. Is there a way to obtain this n in R? I have a large vector of probabilities and would like to extract this n for a given x.</p>

<p>Thank you</p>
"
"0.0701862406343596","0.0686802819743445"," 24975","<p>I am working on an assignment involving a logistic regression model, where I need to plot the pearson standardized residuals against one of the predictors. Here's the basic setup:</p>

<pre><code>model &lt;- glm(outcome ~ predictor1 + predictor2, family=binomial(logit))
res &lt;- residuals(model, ""pearson"")
</code></pre>

<p>When looking at the residuals' distribution, I see something totally different than my colleagues who use Stata (using predict and rstandard). Their residuals are more or less normal, whereas in mine there is a gap in the values (not a singe residual is between -0.05 and 1.15). That does make sense in the context of logistic regression, especially that the maximum predicted probability is not so high (38%). </p>

<p>I'd like to understand what's happening here... What is Stata doing that R isn't, with those residuals? </p>
"
"0.0887793523236951","0.0868744485526139"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.121566134770966","0.118957737857722"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.0701862406343596","0.0686802819743445"," 26994","<p>The Bradleyâ€“Terryâ€“Luce(BTL) model states that $p_{ji} = logit^{-1}(\delta_j - \delta_i)$, where $p_{ij}$ is the probability that object $j$ is judged to be ""better"", heavier, etc,  than object $i$, and $\delta_i$, and $\delta_j$ are parameters.</p>

<p>This seems to be a candidate for the glm function, with family = binomial. However, the formula would be something like ""Success ~ S1 + S2 + S3 + S4 +..."", where Sn is a dummy variable, that is 1 if object n is the first object in the comparison, -1 if it is the second, and 0 otherwise. Then the coefficient of Sn would be the corresponding $delta_n$.</p>

<p>This would be fairly easy to manage with only a few objects, but could lead to a very long formula, and the need to create a dummy variable for each object. I just wonder if there is a simpler method. Suppose that the name or number of the two objects being compared are variables (factors?) Object1, and Object2, and Success is 1 if object 1 is judged better, and 0 if object 2 is.</p>
"
"0.0496291666985465","0.0485642931178632"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.0859602382591879","0.0841158231138067"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.310727733132933","0.304060569934149"," 31747","<p>I've got a problem that I think should be simple but can't quite figure it out.  I'm looking at seed pollination, I have plants (n=36) that flower in clusters, I sample 3 flower clusters from each plant, and 6 seed pods from each cluster (18 seed pods in total from each plant).  A pod can have between 0 to at most 4 seeds pollinated.  So, the data is count, with an upper bound. I'm finding an average of ~10% of seeds are pollinated, but anywhere between 1 - 30% on a given plant, so over dispersed data, and of course, there are 4 missing cluster replicates on 3 plants, so not perfectly symmetrical.  </p>

<p>The question I'm asking is if this data supports the idea this plant requires pollinators for seed set.</p>

<p>I'm finding that the distribution for the number of seeds in a pod looks like there are more 0 pollinated seed pods (6-9 pods out of 16) and more 3 and 4 pollinated seed pods (2-4 for each)  than would be expected if seeds in the population were just randomly pollinated.  Basically, I think this is classic example for zero inflated data, first a insect either does or does not visit the flower at all (one zero generator) and if it does, then pollinates 0-4 of the seeds in another distribution.  The alternative hypothesis is the plant is partially selfing, and it would then be expected that every seed would have the same probability of being pollinated (this data suggests a roughly 0.1 chance, which means 0.01 chance for two seeds in the same pod, etc).</p>

<p>But I simply want to demonstrate the data best fits one or the other distribution, not actually DO an ZIP or ZINB on the data.  I think whatever method I use should take into account the actual number of pollinated seeds and the number of pods sampled on each plant.  The best thing I have come up with is to do some sort of boot strap thing where I just randomly assign the number of pollinated seeds for a given plant into the number of seed pods I sampled, do that 10,000 times and see how likely it is the experimental data for the given plant came out of that random distribution.</p>

<p>I just feel there is something about this that should be a lot easier than brute force bootstrapping, but after days of thinking and searching Iâ€™m giving up.  I canâ€™t just compare to a Poisson distribution because itâ€™s upper bound, itâ€™s not binomial because I need to generate the expected distribution somehow 1st.   Any thoughts?  And Iâ€™m using R so advice there (especially how to most elegantly generate 10,000 random distributions of n balls into 16 boxes that can each contain at most 4 balls) would be most welcome.</p>

<p>ADDED 9/07/2012
First, thanks to you all for all the interest and help.  Reading over the answers has made me think to reword my question a bit.  What Iâ€™m saying is that I have one hypothesis (which for now I am thinking of as the null) that seeds are pollinated randomly across pods, and my alternative hypothesis is that a seed pod with at least 1 pollinated seed is more likely to have multiple pollinated seeds than would be expected by a random process.   Iâ€™ve provided real data from three plants as examples to illustrate what Iâ€™m talking about.  First column is the # of pollinated seeds in a pod, second column is the frequency of pods with that seed count.</p>

<p>plant 1 (total 3 seeds: 4% pollination) </p>

<p>num.seeds ::pod.freq</p>

<p>0::16</p>

<p>1::1</p>

<p>2::1</p>

<p>3::0</p>

<p>4::0    </p>

<p>plant 2 (total 19 seeds: 26% pollination)</p>

<p>num.seeds::pod.freq </p>

<p>0::12</p>

<p>1::1</p>

<p>2::1</p>

<p>3::0</p>

<p>4::4</p>

<p>plant 3 (total 16 seeds: 22% pollination)</p>

<p>num.seeds::pod.freq </p>

<p>0::9</p>

<p>1::4</p>

<p>2::3</p>

<p>3::2</p>

<p>4::0</p>

<p>In plant #1, only 3 seeds were pollinated in 18 pods, one pod had one seed, and one pod had two seeds.  Thinking about a process of adding one seed to the pods at random, the first two seeds each go into their own pod, but for the 3rd seed, there are 6 spots available in pods that already have one seed but 64 spots in the 16 pods with no seeds, so the highest probability of a pod with 2 seeds here is 6/64= 0.094.  Thatâ€™s a bit low, but not really extreme, so Iâ€™d say that this plant fits the hypothesis of random pollination across all seeds with a ~4% chance of pollination occurring. But plant 2 looks much more extreme to me, with 4 pods completely pollinated, yet 12 pods with nothing.  Iâ€™m not quite sure how to calculate the odds of this distribution directly (hence my bootstrap idea) but Iâ€™d guess the odds of this distribution occurring at random if each seed has a ~25% chance of pollination are quite low.  Plant #3 I really have no idea, I think there are more 0â€™s and 3â€™s than one should expect for a random distribution but my gut feeling is that this distribution for this number of seeds is much more likely than the distribution for plant #2, and may not be that unlikely.  But obviously I want to know for sure, and across all plants.  </p>

<p>In the end Iâ€™m looking to write a statement like â€œThe distribution of pollinated seeds in seed pods fits (or does not fit) the hypothesis that plants are not simply partially self compatible, but require visitation of a pollinator to effect seed set. (results of statistical test).â€  This is really just part of my forward looking section, where Iâ€™m talking about what experiments to conduct next, so Iâ€™m not desperate for this to be one thing or the other, but I want to know for myself, if possible.  If I canâ€™t do what Iâ€™m trying to do with this data, Iâ€™d like to know that too!</p>

<p>I did ask a rather broad question at first, since Iâ€™m curious as to whether or not there are any good tests to show if data should go into a zero inflated model in the first place.  All of the examples Iâ€™ve seen seem to say â€“â€œlook, there are a lot of zeros here, and there is a reasonable explanation for that, so letâ€™s use a zero inflated modelâ€.  Thatâ€™s what Iâ€™m doing right now on this forum, but I had an experience on my last chapter where I used a Poisson glm for count data and my one of my supervisors said â€œNo, glms are too complex and unnecessary, this data should go into a contingency tableâ€ and then sent me a data dump of the massive contingency table generated by their expensive stats package that gave the same p values for all my factors + interactions to three significant digits!!  So, Iâ€™m trying to keep the stats clear and simple, and make sure I understand them well enough to robustly defend my choices, which I donâ€™t feel I can do for a zero inflated model right now.  Iâ€™ve used both a quasibinomial (for whole plants to get rid of pesudoreplicaiton) and a mixed model for the above data to compare treatments and answer my main experimental questions, either seems to do the same job, but Iâ€™m going to also play around with ZINBâ€™s tonight, to see how well that performs.  Iâ€™m thinking if I can explicitly demonstrate that this data is strongly clustered (or zero inflated) at first, then provide a good biological reason for that occurring, Iâ€™ll be much better set up to subsequently pull out a ZINB, than to just compare one to a quasibinomial/mixed model and argue since it gives better results, thatâ€™s what I should use.  </p>

<p>But I donâ€™t want to distract too much from my primary question, how can I determine if my data really is more zero inflated than expected from a random distribution?  In my case the answer to that is what is of real interest to me, with the possible benefit for model justification being a bonus.</p>

<p>Thanks again for all your time and help!</p>

<p>Cheers,
BWGIA</p>
"
"0.16517585103707","0.16163173753211"," 34080","<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the 'true model'. I have written a blog post with a more detailed analysis <a href=""http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/"" rel=""nofollow"">here</a>.</p>

<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>

<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression Ã  la Rivers and Vuong (1988). </p>

<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=""http://www.cemfi.es/~arellano/binary-endogeneity.pdf"" rel=""nofollow"">these lecture notes</a>) the ASF can be calculated as follows:</p>

<p>$P(y|\bar{\textbf{X}},v)=\widehat{ASF}=\frac{1}{N}\sum^{N}_{i} \Phi(\bar{\textbf{X}}\boldsymbol{\hat{\beta}}+\rho \hat{\textbf{v}_{i}}) $</p>

<p>where the $\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>

<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>

<p>A picture of my attempt is:</p>

<p><img src=""http://i.stack.imgur.com/OZBA8.jpg"" alt=""enter image description here""></p>

<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>

<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>

<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>

<pre><code>rm(list=ls())
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
y1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)
y2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)

# true
r1 &lt;- glm(y2~y1+x1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))
names(data) &lt;- c(""y1"",""x1"")
asf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))
plot(asf1,type=""l"",col=""blue"",xlab=""y1"",ylab=""P(y2)"")

# no endog correction
r2 &lt;- glm(y2~y1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2)))
names(data) &lt;- c(""y1"")
asf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))
lines(asf2,type=""l"",col=""green"")

# control function approach
v1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))
r3 &lt;- glm(y2~y1+v1,binomial(link=""probit""))
# proceedure to get asf
asf3 &lt;- cbind(seq(-4,6,0.2),NA)
for(i in 1:dim(asf3)[1]){
    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))
    names(dat2) &lt;- c(""y1"",""v1"")
    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))
}
lines(asf3,type=""l"",col=""red"")
</code></pre>
"
"0.121566134770966","0.118957737857722"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.140372481268719","0.137360563948689"," 34518","<p>I am trying to predict a binary outcome with a model that includes a random effect using survey data. I've included a description of the sampling design below, so feel free to comment on my survey weighting approach.  My primary question is how to include a random effect in the survey weighted model.  Here is the code up to this point:</p>

<pre><code># Libraries
library(survey)
# Make dataframe object where d is the working dataframe
dfobj &lt;- svydesign((id = ~cluster+household, 
   strata = ~interaction(region, urban),  
   weights = ~chweight, strata = ~strata, data = d)

# Run a logit model
formula1 &lt;- stunting ~ modern_toilet + diarrhoea + fever 
   + insurance + sex + age + region_code
model1 &lt;- svyglm(formula=formula1,design=dfobj,family = quasibinomial)
</code></pre>

<p>I would like the random effect to be on the region.  Thanks, </p>

<p>Sampling Description:</p>

<p>The MICS 2006 used a two-stage stratified sample design. At the first stage of sampling, 300
census enumeration areas (124 urban and 176 rural EAs) were selected. These are a subsample
of the 660 EAs (281 urban and 379 rural) selected for the GLSS 5. The clusters in each
region were selected using systematic sampling with probability proportional to their size.</p>
"
"0.216599441022544","0.222549549284631"," 40572","<p>I have a series of descriptors, some continuous, some discrete and an output variable which is dichotomous.</p>

<p>I have several parameters, but for the sake of simplicity let's say my data look like:</p>

<pre><code> Sex  |  Age  |  Genotype  | Dose 1  |  Dose  2  |  Outcome
------|-------|------------|---------|-----------|------------
  M   |  32   |    AABB    |   150   |     30    |    YES
  F   |  65   |    aaBb    |   110   |     30    |    YES
  M   |  42   |    AaBb    |   200   |     50    |    NO
...
</code></pre>

<p>I would like to make a predictive model to determine the optimal combination of <code>Dose 1</code> and <code>Dose 2</code> to have a desired outcome.</p>

<p>So my question is, if I have a new male subject of given genotype and age, what is the best combination of doses that will give a positive outcome with the highest probability? Or, to see things the other way around, given the other parameters, what are the odds of having a positive outcome with a given set of doses?</p>

<p>I thought I could use R to generate a linear model with <code>glm</code>, and then use <code>predict</code> to predict the outcome. However, I never really dealt with this type of problems before and I am a bit confused on how to use these functions and interpret their results. Also, I am not sure if this is the correct way to deal with the problem.</p>

<p>For instance, let's generate some random data:</p>

<pre><code>set.seed(12345)

num.obs &lt;- 50
sex &lt;- sample(c(""M"", ""F""), num.obs, replace=T)
age &lt;- sample(20:80, num.obs, replace=T)
genotype &lt;- sample(LETTERS[1:8], num.obs, replace=T)
dose.1 &lt;- sample(100:200, num.obs, replace=T)
dose.2 &lt;- sample(30:70, num.obs, replace=T)
outcome &lt;- sample(0:1, num.obs, replace=T)

data &lt;- data.frame(sex=sex, age=age, genotype=genotype,
              dose.1=dose.1, dose.2=dose.2, outcome=outcome)
</code></pre>

<p>Which gives 50 observation such as</p>

<pre><code>&gt; head(data)
  sex age genotype dose.1 dose.2 outcome
1   F  78        C    183     54       0
2   F  70        E    156     66       1
3   F  39        H    180     35       0
4   F  32        E    135     51       0
5   M  64        E    121     57       1
6   M  50        H    179     61       1
</code></pre>

<p>Now, I generate a model with</p>

<pre><code>model &lt;- glm(outcome ~ sex + age + genotype + dose.1 + dose.2, 
    data=data, family=""binomial"")
</code></pre>

<p>First question: without any <em>a priori</em> knowledge of the interactions between the descriptors, <strong>how do I choose the correct formula</strong>? Should I try various interactions and see which models gives the best fit e.g. looking at residual deviance or AIC? Are there functions to do this for me or should I try all of the combinations manually? </p>

<p>OK, let's say I found the model is good, now I use <code>predict</code></p>

<pre><code>new.data &lt;- list(sex=factor(""M"", levels=c(""M"", ""F"")), age=35, 
                 genotype=factor(""C"", levels=LETTERS[1:8]), 
                 dose.1=150, dose.2=30)
outcome &lt;- predict(model, new.data, se=T)
</code></pre>

<p>Which gives:</p>

<pre><code>$fit
        1 
-2.774538 

$se.fit
[1] 1.492594

$residual.scale
[1] 1
</code></pre>

<p>So... what do I do with this? <code>?predict.glm</code> says <code>$fit</code> is the prediction but obviously that is not a yes/no type of prediction... what I would ideally need is something on the lines of ""89% YES / 11% NO"".</p>

<p><strong>How do I interpret the result of <code>predict</code> and how would I go about having the type of result I want?</strong></p>

<p>Finally, <strong>are there functions to explore the parameter space so that I get a graph with the outcome in the dose1 vs dose2 space?</strong></p>

<p>EDIT: just to clarify: I do not have a specific reason to use a generalized linear model, it is just something that came to my mind as a possible solution. If anyone can think of other ways to solve this type of problem I would gladly welcome their suggestion!</p>
"
"0.0701862406343596","0.0686802819743445"," 41660","<p>I'm modeling a set of outcome data the depends on two parameters:</p>

<ol>
<li>time, T</li>
<li>-100 &lt; A &lt; 100</li>
</ol>

<p>I've done logistic regression using R with the command:</p>

<pre><code>model &lt;- glm(Outcome ~ A + T, family = ""binomial"", data = myData)
</code></pre>

<p>My expectation (the only thing that makes sense) is that when A &lt; 0, the fit probability should be an increasing function of time approaching 0.5, while when A > 0 it should be a decreasing function of time approaching 0.5.</p>

<p>However, the fit I get is that A &lt; 0, A > 0, and A = 0 all are increasing functions of time.  They in fact appear to be the same curve just shifted (ie same ""shape"").</p>

<p>What am I doing incorrectly?  Any suggestions?</p>
"
"0.0496291666985465","0.0485642931178632"," 41885","<p>I am trying to generate binomial probabilities (in R) as follows:</p>

<p>${N \choose{k}} p^{k} (1-p)^{(n-k)}$</p>

<p>My problem is given $p \approx 0.03$, and $N =400$, $k&gt;270$, I get the probability equal to $0$. I am actually evauating all the $k$'s up to $n$, so a high $n$ returns a probability matrix that has $0$'s for $k &gt;270$. </p>

<p>I was wondering if there is a function in R / or an approximation (binomial or normal) that you would recommend in this case? </p>

<h2>Edit</h2>

<p>I ve tried dbinom function in R as well - <code>dbinom(400,280,0.03) = 0</code>. So that does not help.. What I am after is a non-zero approximation..</p>
"
"0.0859602382591879","0.0560772154092044"," 41902","<p>After doing some reading about the binomial distribution I found this about lower and upper bound probabilities</p>

<p>We can get the <strong>lower tail probability</strong> of X as:
$$P(X \leq x)=P(X=0)+...+P(X=x)$$ </p>

<ul>
<li><strong>Stata:</strong> as <code>binomial(n,k,p)</code></li>
<li><strong>R:</strong> <code>pbinom(q, size, prob, lower.tail = TRUE)</code></li>
</ul>

<p>And the <strong>upper tail probability</strong> can be obtained as the complement of lower tail probability to find the area above the cutoff x-value
$$P(X&gt;x)= 1-P(X \leq x)$$ </p>

<ul>
<li><strong>Stata:</strong> <code>1- binomial(n, k, p)</code></li>
<li><strong>R:</strong> <code>pbinom(q, size, prob, lower.tail = FALSE)</code></li>
</ul>

<p><img src=""http://i.imgur.com/glxH4.png?1"" alt=""enter image description here""></p>

<p>Am I right for the R and Stata commands? </p>

<p>And also how can I get these probabilities? 
Are they complementary, as upper and lower bound probabilities?</p>

<p>$$P(X &lt; x)$$
$$P(X \geq x)$$</p>
"
"0.0744437500478198","0.0971285862357264"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.0701862406343596","0.0686802819743445"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"0.179398740368981","0.18808869846582"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.099258333397093","0.0971285862357264"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.099258333397093","0.0971285862357264"," 49408","<p>I'd like to plot the probability distribution for a set of binomial trials in R, the catch is that each trial has an independent probability of success (which I have in vector form).</p>

<p>So, in R if I were doing this with coin tosses: <code>plot(x,dbinom(x,10,.5))</code> would work, where <code>x</code> is <code>0:10</code>. This shows the probability distribution by plotting number of successful trials on the x-axis with percent of the time that specific results is achieved as the <code>y</code> (4 successes is 20.5%).</p>

<p>That said, how do I plot the same graph for 10 discrete trials with varying probabilities. For instance if <code>odds&lt;-c(.1,.8,.2,.2,.3,.7,.9,.99,.05,.5)</code>, was the probability of success for each independent toss, and I wanted to see a probability distribution?</p>
"
"0.099258333397093","0.0971285862357264"," 53154","<p>Using R, what function(s) would I use to obtain the following probabilities? </p>

<ul>
<li>Roll at least one 1 when rolling 2 six-sided dice (2d6) = 11/36</li>
<li>Roll at least one 1 when rolling 3 six-sided dice (3d6) = 91/216</li>
<li>Roll at least one 1 when rolling 1d4, 1d6, 1d8, and 1d8 = 801/1536</li>
</ul>

<p>First I hope my answers above are correct!  I did these pretty much manually.</p>

<p>I think I need to use binomial distributions and/or probability-generating functions, but not sure if I'm over-complicating things.  I've tried using R's *binom() functions but can't seem to arrive at the answers I need.</p>
"
"0.131306432859723","0.128489042187512"," 53403","<p>I have some parameter estimates and confidence intervals estimated from a set of model-averaged binomial GLMMs: two main effects and their interaction. I would like to plot [population level] fitted values for the main effects and their interaction, as well as 95% confidence bands. I can plot the population level fitted values, but I'm not sure how to combine the parameter estimates with their CI's to plot confidence bands.</p>

<p>Code for plotting the fitted values, and resulting graph, is below. I'm plotting rat caputre probability within forest patches (<code>Rat</code>) against distance from forest edge (<code>Dist</code>) and whether the forest patch was grazed or not by livestock (<code>Grazed</code>). Distance was square-root transformed in the model (<code>Dist_T</code>).</p>

<p>Could someone please tell me how to combine the parameter estimates with their CI's to plot confidence bands?</p>

<p>Thank you for your help!</p>

<p>Jay</p>

<p>Parameter estimates from the model were:</p>

<p>Intercept: -1.557; CI=-3.034, -0.08</p>

<p>Dist_T: 0.097; CI=-0.017, 0.212</p>

<p>GrazedY: -1.898; CI=-4.17, 0.375</p>

<p>Dist_T:GrazedY: 0.182; CI=-0.038, 0.401   </p>

<pre><code># estimated abund where grazing=N:
PlotData &lt;- data.frame(Dist_T=seq(from=min(Rat$Dist_T), to=max(Rat$Dist_T), by = 0.1))           
g &lt;- -1.557 + 0.097*PlotData$Dist_T                                               
Rat.fitted &lt;- exp(g)/(1+exp(g))                                                 
Dist.bt &lt;- PlotData$Dist_T^2
plot(Rat$Dist, jitter(Rat$Rat, amount=0.02), ylim=c(0,1), cex=0.7, cex.lab=0.7, cex.main=0.4, ylab=""Rat capture probability"", 
xlab=""Distance from forest edge (m)"", pch=ifelse(Rat$Grazed==""Y"", 1,2), col=ifelse(Rat$Grazed==""Y"", ""grey"",""black""))
lines(Dist.bt,Rat.fitted, lty=1, col=""black"")  

# estimated abund where grazing=Y:
par(new=T)
PlotData &lt;- data.frame(Dist_T=seq(from=min(Rat$Dist_T), to=max(Rat$Dist_T[Rat$Grazed==""Y""]), by = 0.1))#only plotting within range; where Grazed=Y
g &lt;- -1.557 + (0.097+0.182)*PlotData$Dist_T -1.898*1 #additional grazing term and interaction term                                              
Rat.fitted &lt;- exp(g)/(1+exp(g))                                                 
Dist.bt &lt;- PlotData$Dist_T^2
plot(Rat$Dist, jitter(Rat$Rat, amount=0.05), ylim=c(0,1), xlab="""", ylab="""", type=""n"") 
lines(Dist.bt,Rat.fitted, lty=1, col=""grey"")   
</code></pre>

<p><img src=""http://i.stack.imgur.com/os581.png"" alt=""enter image description here""></p>
"
"0.0859602382591879","0.0841158231138067"," 56509","<p>I have this example logit model where some of the variables are factors but I'm not too sure how to interpret the effects. If I understand logit models correctly the coefficients that we get from the fitted model are the change in log-odds per unit change in the explanatory variable holding everything else constant. </p>

<p>If take the <code>exp()</code> of the coefficients then I have the odds. I'm interested in the impact over the probability of some cathegorical variable to be ""<em>male</em>"" or ""<em>female</em>"" for instance. Could you please help me understand this? if <code>allEffects()</code> is not what I'm looking for, could you please let me know how could I get them?</p>

<pre><code>library(effects)

titanic &lt;- glm(survived ~ passengerClass + sex + age,data=Titanic,
               family=binomial)
titanic.all &lt;- allEffects(titanic, typical=median, 
                          given.values=c(passengerClass2nd=1/3,
                          passengerClass3rd=1/3, sexmale=0.5))
plot(titanic.all,
     ticks=list(at=c(.01, .05, seq(.1, .9, by=.2), .95, .99)),
     ask=FALSE)
</code></pre>

<p>EDIT: I don't think it is a duplicate. I'm interested in the output of the package <code>effects</code>, in particular in the output of the function <code>allEffects</code>.</p>

<p>I found one document with the following notes ""Notice that the print method for the object returned by allEffects reports tables of the effects, which, by default, are on the scale of the response variable, for a logit model, on the probability scale""</p>
"
"0.121566134770966","0.118957737857722"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0701862406343596","0.0686802819743445"," 59009","<p>For a binomial GLM, both the probability and the number of trials are important for each data entry. Using the <code>glm</code> fucntion in R, how do I specify them separately and explicitly?</p>

<p>Previously, when using <code>glm</code> for the binomial family, I used <code>glm(cbind(V4, V5) ~ V1 + V2 + V3, family=binomial)</code> where V4 is the number of success and V5 is the number of failures. Is there any other way to do it?</p>
"
"0.099258333397093","0.0971285862357264"," 59683","<p>I would like to do power analyses for hypothesis tests of (non-)equality of proportions in which the proportions are <em>very</em> small.  I would like to do so <strong>without</strong> using normal (or Poisson) approximations of the binomial distribution.  There are several general types of power questions I'd like to be able to address.</p>

<ol>
<li><i>Post-hoc</i>:  Given $\Pr_1$ (probability of a success in group 1) and $\Pr_2$ and $N_1$ (sample size group 1) and $N_2$ to calculate the power of the design given $\alpha$.</li>
<li><i>A priori</i> solve for $N$ given $\alpha$, the ratio $N_1\over{N_2}$, $1 - \beta$ (power), $\alpha$, $\Pr_1$, and an expected $\Pr_2$</li>
<li><i>A priori</i> solve for $1 - \beta$ given $\alpha, N_1, N_2, \Pr_1$, and $\Pr_2$.</li>
</ol>

<p>An ideal response would include R code and point out any other givens that I forgot to point out.  A simulation approach is not a suitable response due to the small proportions.  With your solution, please also mention what kind of statistical test it is applicable to.</p>
"
"0.121566134770966","0.099131448214768"," 61512","<p>Suppose that an urn contains a number of black and a number of white balls, and suppose that it is known that the ratio of the numbers is $\,3\mathbin{:}1\,$ but that it is not known whether the black or the white balls are more numerous. Find the estimator of the probability of drawing a black ball.</p>

<p>I have done this by the following way :</p>

<p>The probability of drawing a black ball is either 0.25 or 0.75.</p>

<p>If $n$ balls are drawn from the urn, the distribution of $X$, the number of black balls, is given by the <em>Binomial Distribution</em></p>

<p>$f(x;p)=\binom{n}{x}p^x(1-p)^{n-x}\quad ; x=0,1,2,...,n$ </p>

<p>where $p$ is the probability of drawing a black ball. Here $p=0.25$ or $0.75$</p>

<p>We shall draw a sample of three balls, that is, n=3, with replacement and attempt to estimate the unknown parameter $p$ of the distribution.</p>

<p>Let us anticipate the results of the drawing of the sample. The possible outcomes and their probabilities are given below :</p>

<p>\begin{array}{c|lcr}
Outcome:x &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\
\hline
f(x;\frac{3}{4}) &amp; \frac{1}{64} &amp; \frac{9}{64} &amp; \frac{27}{64} &amp; \frac{27}{64} \\
f(x;\frac{1}{4}) &amp; \frac{27}{64} &amp; \frac{27}{64} &amp; \frac{9}{64} &amp; \frac{1}{64} \\
\end{array}</p>

<p>In the present example if we found x=0 or 1 in a sample of 3, the estimate for $p$ would be preferred over 0.75.</p>

<p>The estimator may be defined as :</p>

<p>$\hat p$ =
  \begin{cases}
  0.25, &amp;\text{for $x=0,1$} \\
  0.75, &amp;\text{for $x=2,3$} \\
  \end{cases} </p>

<p>I want to estimate $p$ for the above situation in R. I tried to do that in the following procedure which seems totally incorrect to me because if several alternative values of $p$ were possible, I can't proceed in the same manner. But I have no better idea:</p>

<pre><code>  x=2
  ifelse(x==0 &amp; 1, ""hat_p=0.25"", ""hat_p=0.75"")

  x=0
  ifelse(x==0 &amp; 1, ""hat_p=0.25"", ""hat_p=0.75"")
</code></pre>

<p><em>How do I compute the above situation in R?</em></p>
"
"0.0859602382591879","0.0841158231138067"," 63943","<p>I would like to model the performance of a rainwater tank, which has a stochastic input (rainfall). The data are the empty volume in the tank at the end of each day. The values are skewed towards the extremes, and I am not sure how to model this or present it statistically. Reviewing various distributions in Wikipedia, I found that it seems like a Beta Distribution - but I am not sure whether it is one. I need to find a statistical method of representing the 'empty volume'.</p>

<p>One friend suggested that I use binomial distribution of getting probability of tank being 25% empty, 50% empty or 75% empty and find confidence intervals associated with those values.</p>

<p>Here is the distribution of my data:</p>

<p><img src=""http://i.stack.imgur.com/yBg7d.png"" alt=""DataDist""></p>

<p>EDIT - 11 July 7:28 GMT (following comments for clarification)</p>

<p>The inflow into the tank occurs randomly due to the rainfall. There is regular abstraction from the tank if there is stored volume. </p>

<p>I would like to estimate the probability of the empty volume in the tank on any random day in future based on the the historic data, and associated confidence of that probability. </p>

<p>I would then like to use that 'empty volume' figure to estimate how much of a large storm rainfall it can a large number of such tanks hold back and reduce the flash flooding volumes. Possibly may need to present combined probabilities with the storm probability.</p>
"
"0.0496291666985465","0.0485642931178632"," 64703","<p>I've done a literature review. The variable <code>treat</code> takes only TRUE and FALSE values. It says whether they applied a given treatment in a given article.
I want to know if the probability of applying the treatment changes depending on the <code>journal</code> and on the <code>article.type</code>. By the way, I should say that all journals do not have the same frequency of article type.</p>

<p>Here is a data to use as an example:</p>

<pre><code>df = data.frame ( treat=c(T,F,T,T,F,T,T,T,F,T,T,T,T,F,T,T,F,T,F,T,T,T,F,F),
journal=rep(c('a','b','c'),8),
article.type=c('a','b','c','a','b','a','a','b','c','a','a','b','c','a','c',
'a','a','a','b','a','c','a','a','c')
)
</code></pre>

<p><strong>First question:</strong></p>

<p>What statistics test should I use ?</p>

<p>I thought of using GLM (generalized linear model) with a binomial error distribution. Does it seem good to you ?</p>

<p><strong>Second question: Already Answered</strong></p>

<pre><code>summary(glm(treat~journal*article.type, data=df, family='binomial'))

Coefficients: (1 not defined because of singularities)
                         Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)             1.957e+01  4.390e+03   0.004    0.996
journalb               -1.997e+01  4.390e+03  -0.005    0.996
journalc                8.429e-09  8.781e+03   0.000    1.000
article.typeb          -3.913e+01  1.162e+04  -0.003    0.997
article.typec           4.134e-08  1.162e+04   0.000    1.000
journalb:article.typeb  3.884e+01  1.162e+04   0.003    0.997
journalc:article.typeb  3.913e+01  1.756e+04   0.002    0.998
journalb:article.typec         NA         NA      NA       NA
journalc:article.typec -1.916e+01  1.388e+04  -0.001    0.999
</code></pre>

<p>When running this, I get several p-values per variable. One p-value for ""article.typeb"" and one for ""article.typec"" for example. What does it mean ?</p>
"
"0.0701862406343596","0.0686802819743445"," 65095","<p>In documentation to <code>glm</code> I read: ""<em>For binomial and quasibinomial families the response can also be specified as a factor (when the first level denotes failure and all others success)</em>"" Does it mean that probability of failure or success is being modeled? </p>

<p>I'm trying to apply simple logistic model to ""german credit scoring"" dataset where there are levels ""good"" and ""bad"". To get correct results (higher probability means higher likelihood of being good) I have to assume that <code>Failure=Good</code> and <code>Success=Bad</code>. This works, but it is really counterintuitive. I interpret this as - this will model probability of Failure (failed to be bad).</p>

<pre><code>require(ggplot2)

german_data &lt;- read.csv(file=""http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"",
              sep="" "", header=FALSE)

names(german_data) &lt;- c('ca_status','mob','credit_history','purpose','credit_amount','savings',
'present_employment_since','status_sex','installment_rate_income','other_debtors',
'present_residence_since','property','age','other_installment','housing','existing_credits',
'job','liable_maintenance_people','telephone','foreign_worker','gb')

str(german_data)

german_data$gb &lt;- factor(german_data$gb,levels=c(2,1),labels=c(""bad"",""good""))

levels(german_data$gb)[1] 

table(german_data$gb)

model &lt;- glm(data=german_data,formula=gb~.,family=binomial(link=""logit""))

german_data$prob &lt;- predict(model,newdata=german_data, type=""response"")

ggplot(data=german_data) + geom_boxplot(aes(y=prob,x=gb))  + coord_flip()
</code></pre>

<p><img src=""http://i.stack.imgur.com/kcbao.png"" alt=""enter image description here""></p>
"
"0.0701862406343596","0.0686802819743445"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.14888750009564","0.14569287935359"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.110974190404619","0.108593060690767"," 65730","<p>I want to check the probability distribution for my data using R.In some site I found <code>fitdistr</code> function in package <code>MASS</code> can do that. To check that I have generated 105 random Poisson numbers &amp; run the <code>fitdistr</code> function to check whether it is able to identify or not. I used following code</p>

<pre><code>library(MASS)
zpois=rpois(105,0.1)

fitdistr(zpois, 't')$loglik
fitdistr(zpois, 'normal')$loglik
fitdistr(zpois, 'logistic')$loglik
fitdistr(zpois, 'weibull')$loglik
fitdistr(zpois, 'gamma')$loglik
fitdistr(zpois, 'lognormal')$loglik
fitdistr(zpois, 'exponential')$loglik
fitdistr(zpois, 'Poisson')$loglik
fitdistr(zpois, 'negative binomial')$loglik
</code></pre>

<p>I found that it is giving lowest value for normal distribution. I know that the large sample approximation of Poisson distribution is normal distribution but I don't want the large sample approximation. I want to see the exact distribution.</p>

<p>Can you help me to guide the suitable function in R using which I can get the exact distribution fit?</p>
"
"0.110974190404619","0.108593060690767"," 66192","<p>I have the following histogram of count data. And I would like to fit a discrete distribution to it. I am not sure how I should go about this. <img src=""http://i.stack.imgur.com/C3up0.png"" alt=""enter image description here""></p>

<p>Should I first superimpose a discrete distribution, say Negative Binomial distribution, on the histogram so that I would obtain the parameters of the discrete distribution and then run a Kolmogorovâ€“Smirnov test to check the p-values? </p>

<p>I am not sure if this method is correct or not. </p>

<p>Is there a general method to tackle a problem like this?</p>

<p>This is a frequency table of the count data. In my problem, I am only focusing on non-zero counts. </p>

<pre><code>  Counts:     1    2    3    4    5    6    7    9   10 
 Frequency: 3875 2454  921  192   37   11    1    1    2 
</code></pre>

<p><strong>UPDATE:</strong> I would like to ask: I used the fitdistr function in R to obtain the parameters for fitting the data.</p>

<pre><code>fitdistr(abc[abc != 0], ""Poisson"")
     lambda  
  1.68147852 
 (0.01497921)
</code></pre>

<p>I then plot the probability mass function of Poisson distribution on top of the histogram. <img src=""http://i.stack.imgur.com/djIBz.png"" alt=""enter image description here""></p>

<p>However, it seems like the Poisson distribution fails to model the count data. <strong>Is there anything I can do?</strong></p>
"
"0.0496291666985465","0.0485642931178632"," 67211","<p>I received this great answer, <a href=""http://stats.stackexchange.com/questions/67090/statistical-significance-of-conditional-probabilties/67115?noredirect=1#67115"">Statistical significance of conditional probabilties</a>, to my question about how to calculate the significance of conditional probability equations. They explained HDI was calculated by a binomial distribution cdf. I need an Unix-based program which will calculate the binomial distribution CDF for HDI. (I tried R but its <code>pbinom</code> function only gives p values. Which I suppose p-values can be equally useful in comparison.)</p>
"
"0.140372481268719","0.137360563948689"," 67241","<p>I've trawled the internet looking for R packages to compute the confidence intervals for the mean of a Poisson distribution using the different methods cited by Patil and Kulkarni in their papaer ""comparison of confidence intervals for the Poisson mean: some new aspects"", REVSTAT vol. 10, no. 2, June 2012, pp 211-227.</p>

<p>I have found in the R package, 'exactci', the routine poisson.exact, which gives a few methods. In the epitools R package there is pois.exact, pois.daly, pois.yar, and pois.approx.</p>

<p>In the 'stats' R package there is poisson.test which uses the Clopper Pearson method for the confience interval of the mean.</p>

<p>But in addition to these I was looking for those methods where the confidence interval is narrower, i.e. in a similar vein to the  methods outlined for interval estimation of binomial proportions, as laid out in the paper by Agresti and Coull, 'Approxiamte is better than ""Exact"" for Interval Estimation of binomial proportions' the Amer. Stat. May 1998, vol. 52, no.2 pp 119-126.</p>

<p>And on reading similar papers to the above on the Poisson distribution it was clear that for a good survey of confidence intervals, the score method and its variants should be included. It is these methods that do not appear to have been included in any R packages yet ? The closest I found was an R Package on Tolerance Intervals, ('Tolerance'). I don't claim to be totally clear on the distinction between CI and TI, but I think the latter guarantees something (e.g. measured length of a part) will lie in that interval to a given level of probability, but with the CI we are making a statement that the mean will lie in an interval to a given level of probablity.</p>

<p>Is there an R package that does the CI calculation for the Poisson Distribution using these approximate methods (but not the normal dist. approx.) ?</p>

<p>Thanks</p>
"
"0.173702083444913","0.182116099191987"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.196619535671478","0.212653457594913"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.112548371022619","0.128489042187512"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.131306432859723","0.128489042187512"," 77057","<p>I have a data set consisting of six independent environmental variables (all binomial: present / absent) and one dependent variable (binomial: disease present / absent). 
In order to determine the combination of factors that have the highest probability of leading to disease, I first need to conduct an Expert Opinion poll where I will have several experts rank all possible combinations of variables according to their probability of leading to the occurrence of disease. Then, I will obtain regression parameters for each variable using a conjoint analysis approach where each expert conforms a level (hierarchical design), the six environmental variables are independent variables, and the rankings are the dependent variable. There being six factor variables, there exist a total of 64 possible orthogonal combinations. 
I reduced this overwhelming number of possible combinations (while retaining orthogonality) using the <code>AlgDesign</code> package of R. Here is the code followed only by relevant pieces of output:</p>

<pre><code>levels.design = c(2,2,2,2,2,2)
full.design &lt;- gen.factorial(levels.design)

   X1 X2 X3 X4 X5 X6
1  -1 -1 -1 -1 -1 -1
2   1 -1 -1 -1 -1 -1
3  -1  1 -1 -1 -1 -1
   .................
63 -1  1  1  1  1  1
64  1  1  1  1  1  1

set.seed(69)
fractional &lt;- optFederov(~., data=full.design, approximate=FALSE, criterion=""D"")
fractional
</code></pre>

<p>The result is a subset of 12 combinations to be included in the conjoint analysis:</p>

<pre><code>$design
    X1 X2 X3 X4 X5 X6
4   1  1 -1 -1 -1 -1
5  -1 -1  1 -1 -1 -1
    ................
57 -1 -1 -1  1  1  1 
</code></pre>

<p>From what I understand, doing a regression analysis on all 64 combinations should lead to the same regression parameters as those obtained if I use only the reduced set (i.e. 12 combinations from the fractional factorial design).  </p>

<p>Questions: </p>

<ol>
<li>Do the code and the resulting output make sense?</li>
<li>Could anyone point me to a good and simple reference on how this fractional design works? I am afraid I might be doing things wrong by selecting a subset that produces different results from those obtained if a full factorial design was employed.</li>
</ol>
"
"0.110974190404619","0.0868744485526139"," 78358","<p>I have run a zero-inflated Poisson model in WinBUGS without problems, and now I am trying to run its equivalent negative binomial. However, I get an ""undefined real result"" trap message over and over.
Here is the data set <a href=""http://www.megafileupload.com/en/file/474809/data1.html"" rel=""nofollow"">http://www.megafileupload.com/en/file/474809/data1.html</a></p>

<pre><code>#Data handling
library(R2WinBUGS)
setwd('') # working directory
dget('data1')
data1 &lt;- data1[order(data1$C), ]
    n = nrow(data1)
    n0 = length(which(data1$C == 0))

sink(""ZINB.txt"")
cat(""
model {

# Pr(Y=0|x,z) # probability of zeros
K &lt;- 10000
for(i in 1:n0){
zeros0[i] &lt;- 0
zeros0[i] ~ dpois(mu0[i])
mu0[i] &lt;- -log(p00[i] + (1-p00[i])*pow(overd[i], r)) + K

overd[i] &lt;- r/(r + lambda0[i])
p00[i] &lt;- min( 0.999999, max(.000001,p0[i]) ) # avoid stackoverflow
logit(p0[i]) &lt;- a0 + b0.A*A[i] + b0.M*M[i] + b0.cat*cat[i] + b0.d*d[i]
log(lambda0[i])&lt;- a + logh[i] + b.A*A[i] + b.M*M[i] + b.cebo*cebo[i] + b.descart*descart[i] + b.d*d[i]
}

# Pr(Y&gt;0|x,z) # probability of positive counts
for(j in (n0+1):n){
zeros[j] &lt;- 0
zeros[j] ~ dpois(mu[j])
mu[j] &lt;- -log((1-p1[j])*fnb[j]) + K

lfnb[j] &lt;-  r*log(overd[j]) + C[j]*log(1 - overd[j]) + loggam(C[j] + r) - loggam(r) - loggam(C[j] + 1) # - log(1 - pow(overd[j], r))
fnb[j] &lt;- exp( lfnb[j] )
overd[j] &lt;- r/(r + exp(lambda[j]))
p1[j] &lt;- min( 0.999999, max(.000001,p[j]) )
logit(p[j]) &lt;- a0 + b0.A*A[j] + b0.M*M[j] + b0.cat*cat[j] + b0.d*d[j] + b0.descart*descart[j]
log(lambda[j])&lt;- a + logh[j] + b.A*A[j] + b.M*M[j] + b.cebo*cebo[j] + b.descart*descart[j] + b.d*d[j] + b.dcos*dcos[j]
pred[j] &lt;- (1 - p1[j])*lambda[j] # predicted values (see Zuur et al 2009)
var[j] &lt;- (1 - p1[j])*(lambda[j] + lambda[j]*lambda[j]/r) + lambda[j]*lambda[j]*(p1[j]*p1[j] + p1[j]) # variance (see Zuur et al 2009)
res[j] &lt;- (C[j] - lambda[j]*(1 - p[j]))/sqrt( var[j] ) # Pearson residuals
disp[j] &lt;- pred[j]/var[j] # dispersion
}

# Vague priors for model coefficients
r ~ dunif(0,10)
a0 ~ dnorm(0,0.001)
a ~ dnorm(0,0.001)
b.A ~ dnorm(0,0.001)
b0.A ~ dnorm(0,0.001)
b.M ~ dnorm(0,0.001)
b0.M ~ dnorm(0,0.001)
b.d ~ dnorm(0,0.001)
b0.d ~ dnorm(0,0.001)
b.descart ~ dnorm(.0,0.001)
b0.descart ~ dnorm(0,0.001)
b0.cat ~ dnorm(0,0.001)
b.dcos ~ dnorm(0,0.001)   
b.cebo ~ dnorm(0,0.001)
}
"",fill=TRUE)
sink()

# Data passed to WinBUGS
win.data &lt;- list( C = data1$C, A = as.numeric(data1$A), M = as.numeric(data1$M), d = as.numeric(scale(data1$dcol)), dcos = as.numeric(scale(data1$dcos)), cebo = as.numeric(data1$cebo), descart = as.numeric(data1$descart), cat = as.numeric(data1$cat), logh = log(data1$h), n = n, n0 = n0 )

# Initial values
inits &lt;- function(){ list( r = runif(1, 1, 5), a0 = rnorm(1), a = rnorm(1), b.A = rnorm(1), b0.A = rnorm(1), b.M = rnorm(1), b0.M = rnorm(1), b.d = rnorm(1), b0.d = rnorm(1), b.descart = rnorm(1), b0.descart = rnorm(1), b0.cat = rnorm(1), b.dcos = rnorm(1), b.cebo = rnorm(1) ) }

# Parameters
params &lt;- c( ""r"", ""mu"", ""lambda"", ""a"", ""a0"", ""b.A"", ""b0.A"", ""b.M"", ""b0.M"", ""b.d"", ""b0.d"", ""b.descart"", ""b0.descart"", ""b0.cat"", ""b.dcos"", ""b.cebo"", ""res"", ""pred"", ""var"" )

# MCMC settings
nc &lt;- 3
ni &lt;- 30000
nb &lt;- 3000
nt &lt;- 3

# WinBUGS execution
out &lt;- bugs( data = win.data, inits = inits, parameters.to.save = params, model.file = ""ZINB.txt"", n.thin = nt, n.chains = nc, n.burnin = nb, n.iter = ni, debug = TRUE )

print(out, dig = 2)
</code></pre>

<p>The trap arises from the Pr(Y > 0) part of the model since the Pr(Y = 0) runs without problems. I've tried different inits and priors to no avail.</p>
"
"0.140372481268719","0.120190493455103"," 81120","<p>I frequently use this model to test catch efficiency and size selection properties of a given trawl fishing gear:</p>

<p>\begin{equation}
\theta(l)=\frac{s\times r(l)}{(1-s)+s\times r(l)}
\end{equation}</p>

<p>where $\theta(l)$ denotes the expected catch rate in the test gear ($T$), which has been fishing in parallel with a non-selective gear (the control gear with blind meshes,$C$). The parameters affecting $\theta(l)$ are:</p>

<ul>
<li><p><strong>Split parameter</strong> $(s)$: It defines the probability of a fish to enter in $T$ ($s$) or in $C$ ($1-s$), $s\in\{0,1\}$</p></li>
<li><p><strong>Fish size selection</strong> ($r(l)$): It defines the likelihood of fish retention in $T$. This likelihood is conditioned to fish body length, therefore it describes the size selection in $T$. Fish size selection is used to be defined using the logit function:</p></li>
</ul>

<p>\begin{equation}
r(l)=\frac{exp(\beta_1+\beta_2\times l )}{1+exp(\beta_1+\beta_2\times l )} 
\end{equation}</p>

<p>Overall, there is a total of 3 parameters to be estimated ($s$, $\beta_1$, $\beta_2$). We use nonlinear regression techniques to estimate such parameters by maximizing the binomial log-likelihood function:</p>

<p>\begin{equation}
\sum_l(N_{l}^T\times \log\theta(l)+N_{l}^C\times \log(1-\theta(l))) 
\end{equation}</p>

<p>where $N_{l}^T$ is the number of fishes per length-class caught in $T$, and $N_{l}^C$ is the numbers caught in $C$.</p>

<p>During a normal experiment, we deploy the pair of gears ($T$ and $C$) several times to perform parallel fishing. To account for the between-haul variation, we use the bootstrap (using a resampling scheme based on resampling between hauls and fishes within hauls) to estimate the errors of $s$, $\beta_1$ and $\beta_2$.</p>

<p>IÂ´m wondering if itÂ´s possible to shift towards a nonlinear mixed modeling approach, where the hauls are considered as a random component. At the moment I only could find such approach by using least squares as minimization criteria. But I could not find a way to keep using the log-likelihood binomial mass function as target criteria.</p>

<p>Thank you beforehand for any comment or guidance.</p>
"
"0.0701862406343596","0.0686802819743445"," 83945","<p>How do you solve the following problem?</p>

<blockquote>
  <p>A Simulation Study (Probit Regression).</p>
  
  <p>Assume $y|x\sim {\rm Binary}(p)$, where $p= E(y|x)$, and $Î¦^{-1}(\pi)=-1+5.1x_{1i}-0.3x_{2i}$
  Generate data with $x_{1i}\sim{\rm Unif}(0,1)$, $x_{2i}=1$ for $i$ odd and $x_{2i}=0$ for $i$ even, and sample size $n=500$. Try generalized linear model (GLM) with logistic and probit links.</p>
</blockquote>

<p>Here is what I did, I know there is a problem, but I don't know what:</p>

<pre><code>n         &lt;- 500
beta0     &lt;- -1
beta1     &lt;- 5.1
beta2     &lt;- -0.3
x1        &lt;- runif(n=n, min=0, max=1)
x2        &lt;- (1:n)%%2
y         &lt;- pnorm(beta0 + beta1*x1 + beta2*x2)
prob.glm  &lt;- glm(y~x1+x2, family=binomial(link=probit))
logit.glm &lt;- glm(y~x1+x2, family=binomial)
</code></pre>

<p>I know <code>y</code> is a probability here, but how do you simulate a binary variable from the probability? </p>
"
"0.218368333473605","0.242821465589316"," 86605","<h3>Background</h3>

<p>In sensory science, ""replication"" means having a panelist in a taste panel do multiple rounds of the same test. You cannot just count those additional rounds as additional panelists, because panelists might show consistent performance. In that case, replications would just confirm the abilities of individual panelists rather than provide more information about the difference between the two stimulants. So, theory suggests using a corrected beta-binomial model in which the abilities of the panelists are assumed to be beta-distributed, not between 0 and 1, but between $C$ (the guessing probability of the taste test used) and 1:</p>

<p>$$
P(x|n,a,b,C)=\frac{(1-C)^n}{B(a,b)}\binom{n}{x}\sum_{i=0}^{x}\binom{x}{i}\left(\frac{C}{1-C}\right)^{x-i}B(a+i,n+b-x)
$$</p>

<p>$C$ is the guessing probability of the test method used (in my case $C=\frac{1}{3}$),<br>
$n$ is the number of replications (in my case $n=2$ for all panelists),<br>
$x$ is the number of successes.</p>

<p>Log likelihood:</p>

<p>$$
L=\sum_{i=1}^{k}log[Pr(x_i|n_i,\mu,\gamma)]
$$</p>

<p>$k$ is the number of panelists (in my case $k=54$),<br>
$n_i$ and $x_i$ are the number of replications and successes of the $i$th panelist,<br>
$\mu$ is the mean,<br>
$\gamma$ is the over-dispersion parameter (sometimes also referred to as $\rho$).</p>

<p>$a$ and $b$ can be calculated from $\mu$ and $\gamma$:</p>

<p>$$
a=\mu\left(\frac{1}{\gamma}-1\right)\text{ and }b=(1-\mu)\left(\frac{1}{\gamma}-1\right)
$$</p>

<p>(Bi, Sensory discrimination tests and measurements, 2006, as shown in <a href=""http://books.google.be/books?id=3gRjHnMcG0gC&amp;pg=PA533"" rel=""nofollow"" title=""Gacula et al., Statistical Methods in Food and Consumer Research, 2009 Â§ 10.3"">Gacula et al., Statistical Methods in Food and Consumer Research, 2009 Â§ 10.3</a>. See also Brockhoff, The statistical power of replications in difference tests, 2003 and NÃ¦s et al., Statistics for Sensory and Consumer Science, 2010 Â§ 7.7.)</p>

<h3>My test results</h3>

<p>I had 54 panelists perform 2 replications of a sensory test with a guessing probability of 33% (n=2, k=54, C=1/3), all comparing the same two pancake recipes. Depending on the taste difference between the pancake recipes, you would expect the observed proportion of successes $p_c$ to run up from 33% in case of no taste difference to 100% in case of obvious taste difference.</p>

<p>However, my test results show panelists correctly grouped the pancakes in only 29 of the 108 rounds (27%). Here is the R-code for my data:</p>

<pre><code>round1 &lt;- c(0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 
    1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0)
round2 &lt;- c(1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 
    0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
    0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0)
x &lt;- round1 + round2
n &lt;- c(rep(2,length(x)))
dat &lt;- data.frame(id = factor(1:length(x)), x, n)
</code></pre>

<p>I have no reason to assume the panelists gave wrong answers on purpose, nor that I mixed up samples, so I guess it was just chance that led to this 'low' performance.</p>

<h3>Maximum Likelihood Estimation</h3>

<p>As described in Gacula (see link above), I created an R-function of the above-mentioned log-likelihood of the corrected beta-binomial (CBB) model, called it llcbb, then ran nlminb to find MLE-estimates of $\mu$ and $\gamma$. The estimate for $\mu$ is close to 0 and the estimate for $\gamma$ will usually stay near its start value:</p>

<pre><code>fit&lt;-nlminb(start=c(.1, .9), function(x) -1*llcbb(dat,mu=x[1],gamma=x[2],1/3)
    ,control=list(trace=1))
fit$par
# [1] 5.986091e-13 8.974503e-01

fit&lt;-nlminb(start=c(.1, .1), function(x) -1*llcbb(dat,mu=x[1],gamma=x[2],1/3)
    ,control=list(trace=1))
fit$par
# [1] 3.040535e-12 9.976926e-02
</code></pre>

<p>The following picture shows my findings in black and some possible configurations of the CBB-model. The cyan line shows that when $\mu$ approaches 0, it does not really matter anymore what $\gamma$ is. I guess this explains why nlminb leaves $\gamma$ untouched.</p>

<p><img src=""http://i.stack.imgur.com/OPwa4.png"" alt=""observed results and CBB models""></p>

<h3>Moment estimation</h3>

<p>Using other formulas provided by Gacula (see link above) I calculated the following moment estimates:
$$
\hat{\mu}=-0.097222\text{, }\hat{\gamma}= 0.349006
$$</p>

<p>Wikipedia states the following about negative moment estimates:</p>

<blockquote>
  <p>Note that these estimates can be non-sensically negative which is evidence that the data is either undispersed or underdispersed relative to the binomial distribution. In this case, the binomial distribution and the hypergeometric distribution are alternative candidates respectively.</p>
</blockquote>

<p>(Beta-binomial distribution. (2014, February 7). In Wikipedia, The Free Encyclopedia. Retrieved 14:42, February 14, 2014, from <a href=""http://en.wikipedia.org/w/index.php?title=Beta-binomial_distribution&amp;oldid=594290522"" rel=""nofollow"">http://en.wikipedia.org/w/index.php?title=Beta-binomial_distribution&amp;oldid=594290522</a>)</p>

<p>I understand the suggestion to revert to the binomial distribution, but even then I still feel I need to be sure about the dispersion, because the upper limit of the confidence interval on $p_c$ varies quite a bit depending on whether I can use the replication data (in case of no dispersion) or not (in case of over-dispersion):</p>

<pre><code>binom.test(29,108,p=1/3,alternative=""less"")
# 95 percent confidence interval:
#  0.0000000 0.3475966

binom.test(15,54,p=1/3,alternative=""less"")
# 95 percent confidence interval:
#  0.0000000 0.3950352
</code></pre>

<h3>Question</h3>

<p>How to deal with possible dispersion ($\gamma$ or $\rho$) in beta-binomial distribution in case $\mu$ is estimated to approach 0, or to be below 0?</p>
"
"0.216328522291098","0.211686845965254"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.121566134770966","0.118957737857722"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.187163308358292","0.206040845923034"," 89991","<p>Which binnedplot of the glmer should I use to check the model? The residuals against the predicted values without random part(REform=NA) or residuals against the predicted values with random part(REform=NULL)?</p>

<p>I have one binary response variable (y.10) derived from one continuous variable with around 50 to 75% of zeros. I want to model the probability to exceed the limit of 10.
For this example I used only one predictor ""fragments"" which is transformed by  taking the logarithm to get a normal distribution an later a better fit  . All variables are measured in tree regions (region). Within this regions are different plots (plot) and a set of samples were taken from some objects (object).</p>

<p>To inspect the residuals I used binnedplot like discribed in the answer of the question:
<a href=""http://stats.stackexchange.com/questions/63566/unexpected-residuals-plot-of-mixed-linear-model-using-lmer-lme4-package-in-r"">Unexpected residuals plot of mixed linear model using lmer (lme4 package) in R</a>.
To save calculation time with very complex models I modeled at first with 
glm {stats} and based on this results the model with less variables with glmer{lme4}. Doing this I could observe a big difference in the binnedplot of residuals.</p>

<p>To examination the differences I created this example with only one variable. Like you can see in the picture bellow the models of glm and glmer without the random part show a very similar behavior. At the end the random part is not for interest. I need the random part only during model selection.</p>

<p>Which binnedplot of the glmer should I use to check the model? The residuals against the predicted values without random part(REform=NA) or residuals against the predicted values with random part(REform=NULL)?</p>

<p>The code and the resulting picture is given here:</p>

<pre><code>fit.glm=glm(y.10 ~ x.t , data=data, family=""binomial"")
fit.glmer=glmer(y.10 ~ x.t + (1|region) + (1|plot) + (1|object), 
          data=data, family=""binomial"")

y.glm=predict(fit.glm, type =""response"")
y.glmer=predict(fit.glmer,REform=NA,type =""response"") 
y.glmer.ran=predict(fit.glmer,REform=NULL,type =""response"")

par(mfrow=c(2,2))
plot(y.10~x, data=data, type=""n"", main=""Models"")
points(y.glmer.ran~data$x, col=""green"", pch=4, cex=0.5)
    lines(y.glm~data$x, col=""red"")
lines(y.glmer~data$x, col=""darkgreen"")

binnedplot(fitted(fit.glm),resid(fit.glm), main=""Binned residual plot glm"")
binnedplot(y.glmer.ran,resid(fit.glmer), main=""Binned residual plot glmer(REform=NULL)"")
binnedplot(y.glmer,resid(fit.glmer), main=""Binned residual plot glmer(REform=NA)"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HoW04.jpg"" alt=""Shows in the first first Plot the glm fitted model (red) and glmer fitted models with  (green crosses) and without (dark green line) random part. The other plots show the corresponding residuals.""></p>
"
"0.099258333397093","0.0971285862357264"," 91171","<p>When I simulate normal data in R, I make sure that the sample have the exact mean and sd of the sampling distribution: <code>x = scale(rnorm(n))*sd + mean</code>.</p>

<p>I want to do the same for binomial data, making the sample express the near-exact probability that they were generated from. Of course it can't be exact when the probability is continuous and the sample is discrete but something that gets pretty close would be nice <code>x = rbinom(n, 18, 0.5)</code> can potentially give samples where an MLE estimate would indicate a probability of p=0.2 or p=0.8 which is pretty far from p=0.5.</p>

<p>Purpose: I'm building a Bayesian model where I infer a binomial rate from a small sample. To test that the model works, I'd like to simulate well-specified data, in order to diagnose whether a strange inferential result is due to chance in the simulation or in the model.</p>
"
"0.14888750009564","0.129504781647635"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"NaN","NaN"," 92307","<p>I have a dataset with 1206 deputies from two different chambers (1998 and 2002, respectively). In addition, there are 18 parties, and some deputies are in both chambers (the ones who were reelected). I am interested in the relation between party discipline (in a scale from 0 to 100) and the probability of geting appointed as party leader (0 or 1), but I am also interested in separate estimates for each party.</p>

<p>A classmate told me that the following mixed effect model would be appropriate, but I don't know what to do with the correlation within subjects: </p>

<pre><code>model &lt;- glmer(leader ~ discipline + (1 + discipline|chamber:party), 
               data, family=binomial)
</code></pre>

<p>What is the structure of these data and which model would be appropriate in this case (using R)? </p>
"
"0.204626296409494","0.200235710158407"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.110974190404619","0.108593060690767"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0496291666985465","0.0485642931178632","105247","<p>Let's say <code>X</code> can take on 5 values</p>

<pre><code>X&lt;-1:5
</code></pre>

<p>each of the 5 values occur with some probability:</p>

<p><code>p&lt;-c(0.2,0.3,0.1,0.3,0.1)</code>, where <code>sum(p)=1</code></p>

<p>I would like to get the probability that number 5 occurs at least 2 times out of 3 trials.</p>

<p>For the binomial case I would use:</p>

<pre><code>sum(dbinom(2:3,3,0.1))
</code></pre>

<p>What is the equivalent of this in the multinomial case?</p>

<p>I know that the equivalent command is <code>dmultinom()</code>, but I don't understand what the equivalent arguments are. </p>

<pre><code>dmultinom(x,size,prob)
</code></pre>

<p>I don't understand what would correspond to 'at least 2 out of 3'</p>
"
"0.112548371022619","0.128489042187512","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.0701862406343596","0.0686802819743445","109822","<p>I have a two-dimensional predictor plane, 0-1-Observations and a priori knowledge of minimal probabilities per combination of the predictors. I would like to fit a model (e.g. GAM from the mgcv package) that yields predictions that lie above the minimal values.</p>

<p>The problem is how to specify the model. The following is a toy example where the minimal probability is 0.5.</p>

<pre><code>rm(list=ls())
require(mgcv)
set.seed(1)
l&lt;-1000
x&lt;-seq(0,1,l=l)
p&lt;-0.5
y&lt;-as.numeric(runif(length(x))&lt;1-x^4)

df&lt;-data.frame(x=x,y=y,min=p)
m1&lt;-gam(y~s(x)+min,family=binomial(link=""logit""),data=df)
predict(m1,newdata=data.frame(x=1,min=0.5),type=""response"")
</code></pre>

<p>I found that offset does not help.</p>

<p>Subtracting the minimal probability from the response does not help as this yields negative responses which do not make sense.</p>

<p>How can I adapt the model to accomodate for the given minimal value?</p>
"
"0.110974190404619","0.108593060690767","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.0859602382591879","0.0841158231138067","111269","<p>Although I may ask a question that is already solved (but I found none that would explicitly refer to this), I would like to know, if (and I am no statistician) I am right with programming the log-likelihood function of a beta-binomial distribution in R as follows:</p>

<pre><code>sum(lgamma(theta) - lgamma(Pi * theta) - lgamma((1 - Pi) * theta) + lgamma(n + 1) -
lgamma(Y + 1) - lgamma((n - Y) + 1) + lgamma(Y + Pi * theta) +
lgamma(n - Y + (1 - Pi) * theta) - lgamma(n + theta))
</code></pre>

<p>Here, theta is used as an overdispersion parameter. n is the number of trials, Y are the actual successes (from the real data), and Pi is the probability of success as derived from my model in JAGS.
I (naively) took the density function as stated in Bolker (2008) and tried to take the logarithm of it. After that, I used this to compute an AIC and it gave reasonable numbers, but I am not sure!</p>

<p>Thank you for your help!</p>

<p>P.S. I know there are caveats about using AIC, especially in a bayesian context, but I just want to know if I am right in the way to derive the answer to this, although trying it may be foolish from a philosophical point of view in the first place...</p>
"
"0.157593770141845","0.168231646227613","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.121566134770966","0.118957737857722","113980","<p>I have the following GLMM: </p>

<pre><code>success ~ age + gender + group/task + (1 + group/task|school/subject), family = binomial
</code></pre>

<p>I want to know whether participants' probability to succeed in certain problem-solving tasks can be predicted by the type of task. 
I have 6 tasks which can be categorized into 3 groups (A, B, C) with 2 tasks in each group. Each participant received 3 tasks (one from A, one from B, one from C; combination and order counterbalanced).
Cochran's Q- and post-hoc McNemar tests revealed that the three groups differ in their success rates: A is easier than B and C, and B and C are equally difficult.
I used crosstabs to analyze whether the tasks within each group differ in their success rates and found that they are different for A and B.</p>

<p>Now I would like to do a comparison of all individual tasks (not just those within one category). </p>

<p>My question is: Is the equation above correct in terms of the fixed effects or is there any reason to include group as an extra fixed effect (e.g. to see whether task has an effect on top of the group effects found in the McNemar tests)? Would that be unnecessary? </p>

<p>What does it mean to include both group/task and group?</p>
"
"0.140372481268719","0.103020422961517","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.0937903091855161","0.110133464732153","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.122825921110129","0.120190493455103","120589","<p>I've got what I think is a fairly basic problem. I'm not sure if it is a conceptual question or software question, but I'm fairly new to using R and these kinds of stats, so it could be either or both. </p>

<p>I'm working on a practice problem with the occurence of a certain bird in fields with various combinations of two vegetation types (hydric or mesic) and two perscribed burn treatments (burned or not burned). The data set oringally contained counts of birds in each field, but since detections of the birds are fairly rare (i.e., there are a lot of 0's in the counts), we are using presence/absence of the bird as our response and using a generalized linear model with a binomial distribution. The data (which includes columns of data for other problems) look something like:</p>

<pre><code>&gt; birddata
   vegtype d_veg       burn d_burn birdsum birdpa  offset     loff    lnoff  forb rcdom
1   hydric     0 not burned      1       0      0 25.1328 1.400241 3.224174  0.10  5.20
2   hydric     0     burned      0       7      1 25.1328 1.400241 3.224174  6.55  5.20
3   hydric     0     burned      0       0      0 25.1328 1.400241 3.224174  6.40  4.60
4   hydric     0     burned      0       3      1 25.1328 1.400241 3.224174 13.35  4.45
5   hydric     0 not burned      1       0      0 25.1328 1.400241 3.224174 11.70  4.20
6   hydric     0     burned      0       0      0 25.1328 1.400241 3.224174 19.10  0.80
7   hydric     0     burned      0       0      0 25.1328 1.400241 3.224174  1.90  5.10
8    mesic     1     burned      0       6      1 25.1328 1.400241 3.224174 12.95  0.05
...
49   mesic     1 not burned      1       2      1 25.1328 1.400241 3.224174 24.40  0.05
50   mesic     1 not burned      1      14      1 25.1328 1.400241 3.224174  4.10  1.05
</code></pre>

<p>and the model we're using is:</p>

<pre><code>&gt; glm.birdpa2 &lt;- glm(birdpa ~ burn + vegtype, family = ""binomial"", data = birddata)
</code></pre>

<p><strong>What I've been trying to figure out is how to use R to get the probability of detecting a bird in each of the vegetation types while holding the burn treatment at their mean value.</strong> I think what this problem is trying to get at is: how could you tell someone what the overall chance of seeing one of these birds is in a field of either particular vegetation type? I think in SAS I would be using an estimate statement to do ths, but I don't know how to do this in R. I've been trying to use the predict and fitted functions but everything I've tried so far has seemed to be a dead end. Does anyone have some tips for how I'd go about doing this?</p>
"
"0.218368333473605","0.242821465589316","121120","<p>I am trying to create a logistic regression model with mgcv::gam
with what I think is a simple decision boundary, but the model
I build performs very poorly.  A local regression model built
using locfit::locfit on the same data finds the boundary very easily.
I want to add additional parametric regressors to my real-life model, so
I do not want to switch to a purely local regression.</p>

<p>I want to understand why GAM is having trouble fitting the data,
and whether there was ways of specifying the smooths that
can perform better.</p>

<p>Here's a simplified, reproducible example:</p>

<p>Ground truth is 1 = point lies within the unit circle, 0 if outside</p>

<p>e.g. z = 1 if sqrt(x^2 + y^2) &lt;= 1, 0 otherwise</p>

<p>The observed data is noisy, with both false positives and false negatives</p>

<p>Construct a logistic regression to predict whether a point
is inside the circle or not, based on the point's Cartesian
coordinates.</p>

<p>Local regression can find the boundary well (50% probability contour
is very close to the unit circle), but a logistic GAM consistently 
overestimates the size of the circle for the same probability band.</p>

<pre><code>library(ggplot2)
library(locfit)
library(mgcv)
library(plotrix)

set.seed(0)
radius &lt;- 1 # actual boundary
n &lt;- 10000 # data points
jit &lt;- 0.5 # noise factor

# Simulate random data, add polar coordinates
df &lt;- data.frame(x=runif(n,-3,3), y=runif(n,-3,3))
df$r &lt;- with(df, sqrt(x^2+y^2))
    df$theta &lt;- with(df, atan(y/x))

# Noisy indicator for inside the boundary
df$inside &lt;- with(df, ifelse(r &lt; radius + runif(nrow(df),-jit,jit),1,0))

# Plot data, shows ragged edge
(ggplot(df, aes(x=x, y=y, color=inside)) + geom_point() + coord_fixed() + xlim(-4,4) + ylim(-4,4))
</code></pre>

<p><img src=""http://i.stack.imgur.com/BfzkT.png"" alt=""enter image description here"">    </p>

<pre><code>### Model boundary condition using x,y coordinates

### local regression finds the boundary pretty accurately
m.locfit &lt;- locfit(inside ~ lp(x,y, nn=0.3), data=df, family=""binomial"")
plot(m.locfit, asp=1, xlim=c(-2,-2,2,2))
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/fy6z3.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM fits very poorly, also tried with fx=TRUE but didn't help
m.gam &lt;- gam(inside ~ s(x,y), data=df, family=binomial)
plot(m.gam, trans=plogis, se=FALSE, rug=FALSE)
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/ObeIm.png"" alt=""enter image description here"">  </p>

<pre><code>### gam.check doesn't indicate a problem with the model itself
gam.check(m.gam)

Method: UBRE   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [5.41668e-10,5.41668e-10]
(score -0.815746 &amp; scale 1).
Hessian positive definite, eigenvalue range [0.0002169789,0.0002169789].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

           k'    edf k-index p-value
s(x,y) 29.000 13.795   0.973    0.08

#### Try using polar coordinates

### Again, locfit works well
m.locfit2 &lt;- locfit(inside ~ lp(r, nn=0.3), data=df, family=""binomial"")
plot(m.locfit2)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9zr73.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM misses again
m.gam2 &lt;- gam(inside ~ s(r, k=50), data=df, family=binomial)
plot(m.gam2, se=FALSE, rug=FALSE, trans=plogis)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/55XiQ.png"" alt=""enter image description here"">    </p>

<pre><code>### Can also plot gam on link scale for alternate view
plot(m.gam2, se=FALSE, rug=FALSE)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/voMP4.png"" alt=""enter image description here"">    </p>

<pre><code>gam.check(m.gam2)

Method: UBRE   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-3.29203e-08,-3.29203e-08]
(score -0.8240065 &amp; scale 1).
Hessian positive definite, eigenvalue range [7.290233e-05,7.290233e-05].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

         k'    edf k-index p-value
s(r) 49.000 10.537   0.979    0.06
</code></pre>
"
"0.0701862406343596","0.0686802819743445","121842","<p>I'm a little confused by the predict function with a cv.glmnet object. </p>

<p>I'm running these two lines:</p>

<pre><code>cvFit &lt;- cv.glmnet(x = as.matrix(imputedTrainingData[,2:33]), y = imputedTrainingData[,1], family = ""binomial"", type.measure = ""class"" )

response&lt;-predict(cvFit, as.matrix(imputedTestData[,2:33]), s= ""lambda.min"")
</code></pre>

<p>The y variable is a 2-level factor</p>

<p>Why is it that the predict statement gives a numeric vector and not the the class variable outcome predicted? 
I thought for a moment that perhaps it gives the probability or being in one class or another but the max value of results is just above .35 in my data and the min is -.42.</p>

<p>Thanks!</p>
"
"0.28522124947857","0.287310232695114","123123","<p>Say someone who is well practiced (appears to have reached a performance plateau) shoots 20 free throws on 15 different days and is successful the number of times shown in the upper histogram (<code>dat</code> in the code). </p>

<ol>
<li><p>My understanding is that the distribution of outcomes should be predicted by the binomial distribution. Is this correct?</p></li>
<li><p>The expected variance is $np(1-p)$, where $n = 20$ (the number of trials per session) and $p = {\rm mean}/n =.65$ or the average percent of successes.</p></li>
<li><p>I could not figure out how to theoretically calculate the distribution of <em>sample variances</em>, so ran a Monte Carlo simulation. These results are shown in the lower panel. the mean of these variances matches with the theoretical expected variance, but the variance of the data is much less.</p></li>
</ol>

<p><strong>R code:</strong></p>

<pre><code>dat  &lt;- c(12,12,13,12,13,12,12,14,13,13,14,13,14,13,14)
n    &lt;- 20
p    &lt;- mean(dat)/n
Nobs &lt;- length(dat)

sim.vars = matrix(nrow=10000)
for(s in 1:10000){
  sim.vars[s] &lt;- var(rbinom(Nobs, n, p))
}
par(mfrow=c(2,1))
hist(dat,      breaks=seq(0,20,by=1))
hist(sim.vars, breaks=20)


&gt; var(dat)       # Variance of Data
[1] 0.6380952
&gt; n*p*(1-p)      # Expected Variance given binomial model
[1] 4.569778
&gt; mean(sim.vars) # Mean of simulated sample variances
[1] 4.542159
</code></pre>

<p><img src=""http://i.stack.imgur.com/1arqJ.png"" alt=""enter image description here""></p>

<p>@Whuber, I hit enter when the cursor was outside the text box and it submitted before completing the question. I apologize. The first thing I wanted to know if I have made some error anywhere in my thinking (choice of binomial model, simulation, calculation), which your comment suggests I have not. </p>

<p>The second is what processes could possibly generate such data? I have >30 like this from multiple sources, so it is probably not data entry error or made up data. The actual task is not shooting free throws but you can take my word that it really is an equivalent situation. </p>

<p>This peculiarity of the data has not been noted previously. Others have interpreted such data as representing the max performance level achieved, and compared group averages under different conditions. Difference between individuals have been interpreted as differences in skill level, somehow related to neurological characteristics. As far as I can tell, this interpretation (as plateau/ asymptote/ max performance) implies sampling from a binomial distribution, which is really inconsistent with the underdispersion.</p>

<p>An analogous situation would be someone flipping a coin 20 times and always getting 9/10/11 heads. This is too consistent. The only mechanism I have thought of is introducing negative correlation between consecutive trials. Something like:</p>

<pre><code>if(dat[t-1]=success){ p=0 }else{ p=0.95 } # Arbitrary probs used for example
dat[t]=sample(c(miss,success),1,prob=c(1-p,p))
</code></pre>

<p>What other processes could result in this underdispersion? The literature on underdispersion appears to be very sparse. I found it consists mostly of simply finding distributions that can fit such data that lack any clear physical interpretation. That type of analysis is not of interest to me here. Perhaps I missed something due to using inappropriate terminology?</p>

<p><strong>Edit2:</strong>
@whuber In response to your second comment: It really is just like the free throws, almost any explanation that works for that will also apply. An exception is that a person may purposefully miss on the free throw task to maintain a certain score, while that is implausible here. </p>

<p>The task requires motor coordination to attain a goal. A success requires performing a sequence of movements in the correct order, each in the correct fashion (of course with some level of variation). There may also be multiple strategies that can yield success with different/same probability (ie underhand vs overhand shots). It is possible these are used in different trials by the same subject. Unfortunately, the only data available is number of successes per session (20 trials). </p>

<p>I do not think I am looking for ""ways to construct probability models of underdispersed phenomena"", at least not in general. I am not interested in only describing the data, rather for a process that can result in this type of data. The goal is to elucidate what may actually be being measured here if not max/asymptotic/plateau performance level.</p>

<p>To clarify what I mean by ""process"", I am thinking that a monte carlo simulation can be created using some combination of if/then statements and (possibly multiple per trial) samples of correct/incorrect actions, states, and/or events that occur with various probabilities. However, there may be other ways of modeling this.</p>

<p><strong>Edit3:</strong>
@gung I do not think we will be able to <em>identify</em> a process/mechanism from this data alone, but we can hypothesize a few consistent with the data. These will then make predictions regarding other/more detailed measurements (eg trial-to-trial scores) before running the study. This is useful because it suggests what it is important to look for and record when performing the experiments.</p>

<p>I thought of another possible mechanism. The model below simulates a situation where the subject is ""satisfied"" after a threshold # of successes (here thresh=12). The output shown had variance=0.495. If this model were accurate, rather than performance, these experiments appear to measure some kind of motivation threshold. This would be completely different than measuring a skill level, and really alter how these results are interpreted. However, this model predicts many more successes at the beginning of the session than the end. While I do not have actual data recorded regarding this, the prediction is inconsistent with my memory/impression of what unfolded. If anything, I suspect the opposite would be true.</p>

<p>I am looking for further ideas on what the explanation may be as I could not find any hints in the literature.</p>

<pre><code>p.motivated=.9; p.unmotivated=.1; n=20; thresh=12; sessions=15
results&lt;-matrix(nrow=sessions)
for(s in 1:sessions){
  session.dat&lt;-matrix(nrow=n,0)
  for(t in 1:n){
    if(sum(session.dat)&lt;thresh){
      session.dat[t]&lt;-sample(c(0,1),1,prob=c(1-p.motivated,p.motivated))
    }else{
      session.dat[t]&lt;-sample(c(0,1),1,prob=c(1-p.unmotivated,p.unmotivated))
    }
  }
  results[s]&lt;-sum(session.dat)
}

hist(results,breaks=seq(0,20,by=1))
var(results)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QbkrR.png"" alt=""enter image description here""></p>
"
"0.198861015130686","0.206040845923034","123609","<p>I am trying to solve the following question:</p>

<blockquote>
  <p>Player A won 17 out of 25 games while player B won 8 out of 20 - is
  there a significant difference between both ratios?</p>
</blockquote>

<p>The thing to do in R that comes to mind is the following:</p>

<pre><code>&gt; prop.test(c(17,8),c(25,20),correct=FALSE)

    2-sample test for equality of proportions without continuity correction

data:  c(17, 8) out of c(25, 20)
X-squared = 3.528, df = 1, p-value = 0.06034
alternative hypothesis: two.sided
95 percent confidence interval:
 -0.002016956  0.562016956
sample estimates:
prop 1 prop 2 
  0.68   0.40 
</code></pre>

<p>So this test says that the difference is not significant at the 95% confidence level.</p>

<p>Because we know that <code>prop.test()</code> is only using an approximation I want to make things more exact by using an exact binomial test - and I do it both ways around:</p>

<pre><code>&gt; binom.test(x=17,n=25,p=8/20)

    Exact binomial test

data:  17 and 25
number of successes = 17, number of trials = 25, p-value = 0.006693
alternative hypothesis: true probability of success is not equal to 0.4
95 percent confidence interval:
 0.4649993 0.8505046
sample estimates:
probability of success 
                  0.68 

&gt; binom.test(x=8,n=20,p=17/25)

    Exact binomial test

data:  8 and 20
number of successes = 8, number of trials = 20, p-value = 0.01377
alternative hypothesis: true probability of success is not equal to 0.68
95 percent confidence interval:
 0.1911901 0.6394574
sample estimates:
probability of success 
                   0.4 
</code></pre>

<p>Now this is strange, isn't it? The p-values are <em>totally</em> different each time! In both cases now the results are (highly) significant but the p-values seem to jump around rather haphazardly.</p>

<p><strong>My questions</strong></p>

<ol>
<li>Why are the p-values <em>that</em> different each time?</li>
<li>How to perform an exact two sample proportions binomial test in R correctly?</li>
</ol>
"
"0.0496291666985465","0.0485642931178632","123927","<p>I am currently working on a biological dataset where I am attempting to fit a detection function (Detection probability as a function of distance). I have been using the package geepack (in R-studio) to run the models. But I am now stuck with how to graph these models. I know to graph glms you just use plot(glm) but that doesnt seem to be working for the gee models. My models have a binomial distribution. </p>

<p>Can anyway point me in the right direction? I have searched the internet for hours and still struggling....</p>
"
"0.0496291666985465","0.0485642931178632","125477","<p>This question was given in class and I was wondering how to do this in R:</p>

<p><em>""Sixty percent of a large lot of old spark plugs are still usable, and they can be individually tested to determine this. Let Y be the number plugs to be tested in order to find 5 usable items.""</em></p>

<pre><code>Find the probability P[Y&lt;=10]
</code></pre>

<p>I wish to apply the Negative Binomial distribution. I know the answer to be 0.834. This is the R code that I was working on. This returns the wrong answer.</p>

<pre><code>sum(dnbinom(x=0:10,size=10,prob=0.6))
[1] 0.8724788
</code></pre>

<p>This set of parameters seem to return the correct answer, but I don't know why it is correct.</p>

<pre><code>sum(dnbinom(x=0:5,size=5,prob=0.6))
[1] 0.8337614
</code></pre>
"
"0.121566134770966","0.118957737857722","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.153770348887698","0.137931712208268","130264","<p>We conducted a study to predict deleterious mutations from a list of around 5000 mutations (which contains both neutral and deleterious mutations; the real state of each mutation is unknown), using four publicly available SNP classifiers (prediction tools) e.g. Classifier_1, Classifier_2, Classifier_3 and Classifier_4.   </p>

<p>Let's say, 
       Classifier_1 predicted 100 mutations as deleterious (i.e. remaining 4900 mutations as neutral) from the given 5000 mutations;  classifier_2 predicted 80 SNPs as deleterious (i.e. remaining 4920 mutations as neutral) from the given 5000 mutations; classifier_3 predicted 75 SNPs as deleterious (i.e. remaining 4925 mutations as neutral) from the given 5000 mutations, and classifier_4 predicted 95 SNPs as deleterious (i.e. remaining 4905 mutations as neutral) from the given 5000 mutations. </p>

<p>Then we calculated the prediction of deleterious SNPs from a combination of any two tools (e.g. deleterious SNPs from: classifier_1 &amp; classifier_2), any three tools (e.g. deleterious SNPs from: classifier_1 &amp; classifier_2 &amp; classifier_3), and a combination of all four tools (e.g. deleterious SNPs from: classifier_1 &amp; classifier_2 &amp; classifier_3 &amp; classifier_4). Predicted deleterious SNPs from these combinations are:</p>

<pre><code>   classifier_1 = 100 deleterious SNPs,
   classifier_2 = 80 deleterious SNPs,
   classifier_3 = 75 deleterious SNPs,
   classifier_4 = 95 deleterious SNPs,
   classifier_1&amp;2 = 44 deleterious SNPs,
   classifier_1&amp;3 = 27 deleterious SNPs,
   classifier_1&amp;4 = 32 deleterious SNPs,
   classifier_2&amp;3 = 38 deleterious SNPs,
   classifier_2&amp;4 = 32 deleterious SNPs,
   classifier_3&amp;4 = 20 deleterious SNPs,
   classifier_1&amp;2&amp;3 = 18 deleterious SNPs,
   classifier_1&amp;2&amp;4 = 17 deleterious SNPs,
   classifier_1&amp;3&amp;4 = 11 deleterious SNPs,
   classifier_2&amp;3&amp;4 = 13 deleterious SNPs,
   classifier_1&amp;2&amp;3&amp;4 = 10 deleterious SNPs.
</code></pre>

<p>Under this scenario, we want to calculate the probability of selecting these deleterious SNPs, in each level of prioritization (i.e. using one tool, a combination of two tools, a combination of three tools, and a combination of all four tools), simply by chance. </p>

<p>This analysis will assist us in inferring whether our prioritization scheme (i.e. application of one prediction tool, a combination of two tools, a combination of three tools, and a combination of all four tools) is effective or not.  </p>

<p>We tried pbinom(x, n, p) and binom.test(x, n, p) where x = number of predicted deleterious SNPs e.g. 100 SNPs by classifier_1, n = total number of SNPs considered e.g. 5000, and p = 0.5 (i.e. random guessing); but not sure whether it is correct or not, and how to address all the situations.  </p>

<blockquote>
  <p>##Example: </p>
</blockquote>

<pre><code> Classifier_1 = binom.test(c(100, 4900), p = 0.5)
   Classifier_1     
   Exact binomial test

   data:  c(100, 4900)
   number of successes = 100, number of trials = 5000, p-value &lt; 2.2e-16
   alternative hypothesis: true probability of success is not equal to 0.5
   95 percent confidence interval:
     0.01630168 0.02427257
   sample estimates:
   probability of success 
         0.02 

  Classifier_1 = pbinom(100, 5000, p = 0.5)
  Classifier_1
  [1] 0
</code></pre>

<p>I will really appreciate, if you guide me - how to calculate the probability of selecting these SNPs (from each tools, and a combination of all) simply by chance (i.e. prediction scheme is not effective).</p>

<p>Thank you all for your kind help .</p>
"
"0.193557098892035","0.211686845965254","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.121566134770966","0.118957737857722","130643","<p>I tried a regression in the form ${\rm logit}(Y) = {\rm coefficient}\times X + 0 + e$, where $Y$ is a binomial variable and $X$ is a factor variable with $n$ levels. I noticed that removing the intercept yields higher $p$ values. I'm wondering how to interpret it though.</p>

<p>Since removing the intercept makes it equal to $0$, I believe that the coefficients returned are relative to a $0$ probability of the event $Y$ and that all $X$ factors are in the $0$ state. But I think this is impossible isn't it?</p>

<p>$X$ are mutually exclusive factors, therefore it's impossible to have a case where no factor is $1$, at least in the presented observations. And it cannot be interpreted like the coefficient is relative to hypothetical cases in which really no one of the factors is present, because we have no data like that.</p>

<p>Regarding $Y$ having a $0$ intercept, wouldn't it mean forcing the probability of the event to $0$ when none of the factors is present? Again, this is an impossible case.</p>

<p>Nonetheless this kind of regression would allow me to retrieve pure probability range of the event Y given a factor by transforming the coefficients in the confidence intervals given as $\exp({\rm coefficient})/(1 + \exp({\rm coefficient}))$, and the $p$ values would test whether this probability is not $50\%$. This could also be a valuable result, since it would give independent probabilities for each factor.</p>

<p>Am I wrong?</p>
"
"0.32987533098149","0.343401409871723","132925","<p>I have data from the following experimental design: my observations are counts of the numbers of successes (<code>K</code>) out of corresponding number of trials (<code>N</code>), measured for two groups each comprised of <code>I</code> individuals, from <code>T</code> treatments, where in each such factor combination there are <code>R</code> replicates. Hence, altogether I have 2 * I * T * R <em>K</em>'s and corresponding <em>N</em>'s. </p>

<p>The data are from biology. Each individual is a gene for which I measure the expression level of two alternative forms (due to a phenomenon called alternative splicing). Hence, <em>K</em> is the expression level of one of the forms and <em>N</em> is the sum of expression levels of the two forms. The choice between the two forms in a single expressed copy is assumed to be a Bernoulli experiment, hence <em>K</em> out of <em>N</em> copies follows a binomial. Each group is comprised of ~20 different genes and the genes in each group have some common function, which is different between the two groups. For each gene in each group I have ~30 such measurements from each of three different tissues (treatments). I want to estimate the effect that group and treatment have on the variance of K/N.</p>

<p>Gene expression is known to be overdispersed hence the use of negative binomial in the code below.</p>

<p>E.g., <code>R</code> code of simulated data:</p>

<pre><code>library(MASS)
set.seed(1)
I = 20 # individuals in each group
G = 2  # groups
T = 3  # treatments
R = 30 # replicates of each individual, in each group, in each treatment

groups     = letters[1:G]
ids        = c(sapply(groups, function(g){ paste(rep(g, I), 1:I, sep=""."") }))
treatments = paste(rep(""t"", T), 1:T, sep=""."")
 # create random mean number of trials for each individual and 
 #  dispersion values to simulate trials from a negative binomial:
mean.trials = rlnorm(length(ids), meanlog=10, sdlog=1)
thetas      = 10^6/mean.trials
 # create the underlying success probability for each individual:
p.vec = runif(length(ids), min=0, max=1)
 # create a dispersion factor for each success probability, where the 
 #  individuals of group 2 have higher dispersion thus creating a group effect:
dispersion.vec = c(runif(length(ids)/2, min=0, max=0.1),
                   runif(length(ids)/2, min=0, max=0.2))
 # create empty an data.frame:
data.df = data.frame(id=rep(sapply(ids, function(i){ rep(i, R) }), T),
                     group=rep(sapply(groups, function(g){ rep(g, I*R) }), T),
                     treatment=c(sapply(treatments, 
                                        function(t){ rep(t, length(ids)*R) })),
                     N=rep(NA, length(ids)*T*R), 
                     K=rep(NA, length(ids)*T*R) )
 # fill N's and K's - trials and successes
for(i in 1:length(ids)){
  N     = rnegbin(T*R, mu=mean.trials[i], theta=thetas[i])
  probs = runif(T*R, min=max((1-dispersion.vec[i])*p.vec[i],0),
                max=min((1+dispersion.vec)*p.vec[i],1))
  K     = rbinom(T*R, N, probs)
  data.df$N[which(as.character(data.df$id) == ids[i])] = N
  data.df$K[which(as.character(data.df$id) == ids[i])] = K
}
</code></pre>

<p>I'm interested in estimating the effects that group and treatment have on the dispersion (or variance) of the success probabilities (i.e., <code>K/N</code>). Therefore I'm looking for an appropriate glm in which the response is K/N but in addition to modelling the expected value of the response the variance of the response is also modeled.</p>

<p>Clearly, the variance of a binomial success probability is affected by the number of trials and the underlying success probability (the higher the number of trials is and the more extreme the underlying success probability is (i.e., near 0 or 1), the lower the variance of the success probability), so I'm mainly interested in the contribution of  group and treatment beyond that of the number of trials and the underlying success probability. I guess applying the arcsin square root transformation to the response will eliminate the latter but not that of the number of trials.</p>

<p>Although in the simulated example data above the design is balanced (equal number of individuals in each of the two groups and identical number of replicates in each individual from each group in each treatment), in my real data it is not - the two groups do not have an equal number of individuals and the number of replicates varies. Also, I'd imagine the individual should be set as a random effect.</p>

<p>Plotting the sample variance vs. the sample mean of the estimated success probability (denoted as p hat = K/N) of each individual illustrates that extreme success probabilities have lower variance:
<img src=""http://i.stack.imgur.com/2AWFd.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/ArGnD.png"" alt=""enter image description here""></p>

<p>This is eliminated when the estimated success probabilities are transformed using the arcsin square root variance stabilizing transformation (denoted as arcsin(sqrt(p hat)):
<img src=""http://i.stack.imgur.com/Ktj4r.png"" alt=""enter image description here""></p>

<p>Plotting the sample variance of the transformed estimated success probabilities vs. the mean N shows the expected negative relationship:
<img src=""http://i.stack.imgur.com/i1CwL.png"" alt=""enter image description here""></p>

<p>Plotting the sample variance of the transformed estimated success probabilities for the two groups shows that group b has slightly higher variances, which is how I simulated the data:
<img src=""http://i.stack.imgur.com/lr5uo.png"" alt=""enter image description here""></p>

<p>Finally, plotting the sample variance of the transformed estimated success probabilities for the three treatments shows no difference between treatments, which is how I simulated the data:
<img src=""http://i.stack.imgur.com/xQlHD.png"" alt=""enter image description here""></p>

<p>Is there any form of a generalized linear model with which I can quantify the group and treatment effects on the variance of the success probabilities? </p>

<p>Perhaps a heteroscedastic generalized linear model or some form of a loglinear variance model?</p>

<p>Something in the lines of a model which models the Variance(y) = ZÎ» in addition to E(y) = XÎ², where Z and X are the regressors of the mean and variance, respectively, which in my case will be identical and include treatment (levels t.1, t.2, and t.3) and group (levels a and b), and probably N and R, and hence Î» and Î² will estimate their respective effects.</p>

<p>Alternatively, I could fit a model to the sample variances across replicates of each gene from each group in each treatment, using a glm which only models the expected value of the response. The only question here is how to account for the fact that different genes have different numbers of replicates. I think the weights in a glm could account for that (sample variances that are based on more replicates should have a higher weight) but exactly which weights should be set?</p>

<p>Note:
I have tried using the <code>dglm</code> R package:</p>

<pre><code>library(dglm)
dglm.fit = dglm(formula = K/N ~ 1, dformula = ~ group + treatment, family = quasibinomial, weights = N, data = data.df)
summary(dglm.fit)
Call: dglm(formula = K/N ~ 1, dformula = ~group + treatment, family = quasibinomial, 
    data = data.df, weights = N)

Mean Coefficients:
               Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) -0.09735366 0.01648905 -5.904138 3.873478e-09
(Dispersion Parameters for quasibinomial family estimated as below )

    Scaled Null Deviance: 3600 on 3599 degrees of freedom
Scaled Residual Deviance: 3600 on 3599 degrees of freedom

Dispersion Coefficients:
                Estimate Std. Error      z value  Pr(&gt;|z|)
(Intercept)  9.140517930 0.04409586 207.28746254 0.0000000
group       -0.071009599 0.04714045  -1.50634107 0.1319796
treatment   -0.001469108 0.02886751  -0.05089138 0.9594121
(Dispersion parameter for Gamma family taken to be 2 )

    Scaled Null Deviance: 3561.3 on 3599 degrees of freedom
Scaled Residual Deviance: 3559.028 on 3597 degrees of freedom

Minus Twice the Log-Likelihood: 29.44568 
Number of Alternating Iterations: 5 
</code></pre>

<p>The group effect according to dglm.fit is pretty weak. I wonder if the model is set right or is the power this model has.</p>
"
"0.149637567815527","0.146426853164356","135255","<p>I aim to estimate the annual proportion of patients (% of patients) that are smokers in a population whose age and sex must be taken into account. In other words, I want to calculate the adjusted prevalence (%) of smoking each year. I have repeated measurements on the same individuals and want to model the individual as a random effect, which is why I use the lme4 package, more precisely the glmer function. The variable of main interest is ""year"" (period 1996 to 2014), which I need to model as a fixed effect.</p>

<p><strong>Aim:</strong> Obtain adjusted proportions (%) of smokers each year.</p>

<p>Suppose the data set is named ""df"" and the year variable is converted to a factor.</p>

<p>I tried this code (<em>generated with a slightly different data set than the attached one</em>) to fit the model:</p>

<pre><code>&gt; smoke &lt;- glmer(smoker ~ biomarker + year + sex + age + (1 | id), data
&gt; = df, family = binomial, nAGQ = 1)

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -6.201632   0.231582 -26.779  &lt; 2e-16 ***
biomarker        -0.015364   0.008299  -1.851  0.06413 .  
yuar1997           0.648292   0.212400   3.052  0.00227 ** 
yuar1998          -0.586996   0.227217  -2.583  0.00978 ** 
yuar1999          -1.194309   0.216907  -5.506 3.67e-08 ***
yuar2000          -0.999889   0.217536  -4.596 4.30e-06 ***
yuar2001          -0.884453   0.203351  -4.349 1.37e-05 ***
yuar2002          -0.777464   0.199151  -3.904 9.47e-05 ***
yuar2003          -0.961869   0.194723  -4.940 7.83e-07 ***
yuar2004          -1.755470   0.197157  -8.904  &lt; 2e-16 ***
yuar2005          -1.207833   0.189753  -6.365 1.95e-10 ***
yuar2006          -1.072532   0.187504  -5.720 1.07e-08 ***
yuar2007          -1.494477   0.189467  -7.888 3.08e-15 ***
yuar2008          -2.441916   0.191069 -12.780  &lt; 2e-16 ***
yuar2009          -1.881562   0.187321 -10.045  &lt; 2e-16 ***
yuar2010          -2.254924   0.187254 -12.042  &lt; 2e-16 ***
yuar2011          -1.634935   0.184929  -8.841  &lt; 2e-16 ***
yuar2012          -2.405588   0.187349 -12.840  &lt; 2e-16 ***
yuar2013          -2.119775   0.186729 -11.352  &lt; 2e-16 ***
yuar2014          -2.241768   0.210259 -10.662  &lt; 2e-16 ***
sex              -0.071377   0.115975  -0.615  0.53826    
age              -0.012897   0.008011  -1.610  0.10742 
</code></pre>

<p>Using the predict function to obtain probability of being a smoker in 2005:</p>

<pre><code>predict(smoke, data.frame(age=mean(df$age), year=""2005"", sex=mean(df$sex), biomarker=mean(df$biomarker, na.rm=T)), type=""response"", re.form = NA)
</code></pre>

<p>I obtain much too low probabilities of being a smoker a particular year:</p>

<pre><code>0.0002233488
</code></pre>

<p>The same is true when using the lsmeans and effects package. Figures should be around 5â€“15% smokers.</p>

<p><strong>In short</strong>, in the data set I'm aiming to obtain the proportions of smokers during different years, adjusted for differences in age, sex and the biomarker while accounting of repeated measurements.</p>

<p>I'd be extremely grateful for a solution to these problems.</p>
"
"0.132344444529457","0.14569287935359","135691","<p>I am trying to figure out what test I should use in the following scenario: I know that there is a lot of room for improvement in a specific area at work - being extremely critical, let's say that sampling $52$ observations, $31$ could be improved. After instituting an improvement / QA program for six months, let me assume that out of a sample of $55$ cases, there are only $11$ with residual flaws. The two samples are independent. We are therefore comparing two proportions: $p_{initial} =\frac{31}{52}$ and $p_{final} = \frac{11}{55}$. </p>

<p>Although the numbers are exaggerated, I still want to see if the two proportions are statistically different, and I think I have a couple of options: I can run an exact binomial test to calculate the probability that the new proportion of flawed observations, 11/55, would occur if the actual underlying probability remained 31/52. Alternatively, I can run a chi-squared test.</p>

<p>The chi-squared is an approximation, and what I have read is that it is to be applied when the total number of observations is too high. This is clearly not the case in the example; however, playing with the numbers in R, I couldn't see any delay or problems with the results even after using numbers >10,000. And there was no indication of any normal approximation being used.</p>

<p>So, if this is all true, why shouldn't we always opt for an exact binomial test, rather than a chi square?</p>

<p>The code in R for the two test would be:</p>

<pre><code>    # Exact Binomial Test:
binom.test(c(11, 55 - 11), p = 31/52, alternative =""less"")

    #Chi-square Test:
prop.test(c(31, 11), c(52, 55), correct = FALSE, alternative = 'greater')
</code></pre>
"
"0.0350931203171798","0.0686802819743445","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"0.205769468971417","0.201354354114666","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.186109375119549","0.194257172471453","141412","<p>I am very confused with how weight works in glm with family=""binomial"". In my understanding, the likelihood of the glm with  family = ""binomial"" is specified as follows:
$$
f(y) = 
{n\choose{ny}} p^{ny} (1-p)^{n(1-y)} = \exp \left(n \left[ y \log \frac{p}{1-p} - \left(-\log (1-p)\right) \right] + \log {n \choose ny}\right)
$$
where $y$ is the ""proportion of observed success"" and $n$ is the known number of trials.</p>

<p>In my understanding, the probability of success $p$ is parametrized with some linear coefficients $\beta$ as $p=p(\beta)$ and glm function with family = ""binomial"" search for:
$$
\textrm{arg}\max_{\beta} \sum_i \log f(y_i).
$$ 
Then this optimization problem can be simplified as:</p>

<p>$$
\textrm{arg}\max_{\beta} \sum_i \log f(y_i)= 
\textrm{arg}\max_{\beta} \sum_i n_i \left[ y_i \log \frac{p(\beta)}{1-p(\beta)} - \left(-\log (1-p(\beta))\right) 
\right] + \log {n_i \choose n_iy_i}\\
=
\textrm{arg}\max_{\beta} \sum_i n_i \left[ y_i \log \frac{p(\beta)}{1-p(\beta)} - \left(-\log (1-p(\beta))\right) 
\right] \\
$$<br>
Therefore if we let $n_i^*=n_ic$ for all $i=1,...,N$ for some constant $c$, then it must also be true that:
$$
\textrm{arg}\max_{\beta} \sum_i \log f(y_i)
=
\textrm{arg}\max_{\beta} \sum_i n^*_i \left[ y_i \log \frac{p(\beta)}{1-p(\beta)} - \left(-\log (1-p(\beta))\right) 
\right] \\
$$
From this, I thought that <strong>Scaling of the number of trials $n_i$ with a constant does NOT affect the maximum likelihood estimates of $\beta$ given the proportion of success $y_i$</strong>. </p>

<p>The help file of glm says:</p>

<pre><code> ""For a binomial GLM prior weights are used to give the number of trials 
  when the response is the proportion of successes"" 
</code></pre>

<p>Therefore I expected that the scaling of weight would not affect the estimated $\beta$ given the proportion of success as response. However the following two codes return different coefficient values:</p>

<pre><code> Y &lt;- c(1,0,0,0) ## proportion of observed success
 w &lt;- 1:length(Y) ## weight= the number of trials
 glm(Y~1,weights=w,family=binomial)
</code></pre>

<p>This yields:</p>

<pre><code> Call:  glm(formula = Y ~ 1, family = ""binomial"", weights = w)

 Coefficients:
 (Intercept)  
      -2.197     
</code></pre>

<p>while if I multiply all weights by 1000, the estimated coefficients are different:</p>

<pre><code> glm(Y~1,weights=w*1000,family=binomial)

 Call:  glm(formula = Y ~ 1, family = binomial, weights = w * 1000)

 Coefficients:
 (Intercept)  
    -3.153e+15  
</code></pre>

<p>I saw many other examples like this even with some moderate scaling in weights. 
What is going on here?</p>
"
"0.112548371022619","0.128489042187512","147060","<p>I am studying an experiment of the kind: 
Let $n_{ij}$ be the number of fetuses, $X_{ij}$ the number of responses i.e. the number of fetuses with a malformation in the jth litter of the ith dose level for j=1,...,25 and i=1,...,5 . 
Then, $p_{ij}$ is the probability of response of in the jth litter of the ith dose level and hence we have:
$$
P(X_{ij} = x_{ij}|p_{ij}) \sim Bin(n_{ij},p_{ij})
$$
But the probability of response $p_{ij}$ follow a beta distribution hence
$$
P(p_{ij})=B^{-1}(\alpha_i , \beta_i )p_{ij}^{x_{ij}}(1-p_{ij})^{n_{ij}-x_{ij}}
$$
and hence, at the end, $X_{ij}$ follow a beta-binomial distribution. </p>

<p>My problem is that I have to generate the number of responses $X_{ij}$ but I'm having some troubles. </p>

<p>The data that I have are all the $n_{ij}$ and $p_1, p_2, p_3, p_4, p_5$ (and this probabilities follow a logistic model) i.e. the probability response for each dose group, hence I don't have $p_ij$. </p>

<p>What should I do? I think that I should first generate $p_{ij}$ using the fact that they follow a beta distribution. But in which way should I do? How to estimate the parameter $\alpha_i$ and $\beta_i$? </p>

<p>Maybe someone has some ideas.. 
Thank you in advance!</p>
"
"0.157593770141845","0.168231646227613","148699","<p>For a current piece of work Iâ€™m trying to model the probability of tree death for beech trees in a woodland in the UK. I have records of whether trees were alive or dead for 3 different census periods along with data on their diameter and growth rate. Each tree has an ID number so it can be identified at each time interval. However, the census intervals vary so that for the time between one survey and another is either 4, 12 or 18 years. Obviously the longer the census period the greater the probability a tree will have died by the time it is next surveyed. <strong>I had problems making a realistic reproducible example so you can find the <a href=""https://github.com/PhilAMartin/Denny_mortality/blob/master/Data/Stack_dead.csv"" rel=""nofollow"">data here</a>.</strong></p>

<p>The variables in the dataset are:</p>

<ol>
<li>ID - Unique ID for tree</li>
<li>Block - the ID for the 20x20m plot in which the tree was located</li>
<li>Dead - Status of tree, either dead (1) or alive (0)</li>
<li>GR - Annual growth rate from previous survey</li>
<li>DBH - diameter of tree at breast height</li>
<li>SL - Length of time between censuses in years</li>
</ol>

<p>Once a tree is recorded as dead it disappears from subsequent surveys.</p>

<p>Ideally I would like to be able to estimate the annual probability of mortality of a tree using information on diameter and growth rate. Having searched around for quite a while I have seen that logistic exposure models appear able to account for differences in census periods by using an altered version of logit link for binomial models as detailed by Ben Bolker <a href=""https://rpubs.com/bbolker/logregexp"" rel=""nofollow"">here</a>. This was originally used by Shaffer to determine the daily probability of bird nest survival where the age (and therefore exposure) of the nest differed. I've not seen it used outside of the context of models of nest survival but it seems like I should be able to use it to model survival/mortality where the exposure differs.</p>

<pre><code>require(MASS)
logexp &lt;- function(exposure = 1)
{
  linkfun &lt;- function(mu) qlogis(mu^(1/exposure))
  ## FIXME: is there some trick we can play here to allow
  ##   evaluation in the context of the 'data' argument?
  linkinv &lt;- function(eta)  plogis(eta)^exposure
  logit_mu_eta &lt;- function(eta) {
    ifelse(abs(eta)&gt;30,.Machine$double.eps,
           exp(eta)/(1+exp(eta))^2)
    ## OR .Call(stats:::C_logit_mu_eta, eta, PACKAGE = ""stats"")
  }
  mu.eta &lt;- function(eta) {       
    exposure * plogis(eta)^(exposure-1) *
      logit_mu_eta(eta)
  }
  valideta &lt;- function(eta) TRUE
  link &lt;- paste(""logexp("", deparse(substitute(exposure)), "")"",
                sep="""")
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, 
                 name = link),
            class = ""link-glm"")
}
</code></pre>

<p>At the moment my model looks like this, but I will incorporate more variables as I go along:</p>

<pre><code>require(lme4)
Dead&lt;-read.csv(""Stack_dead.csv"",)


M1&lt;-glmer(Dead~DBH+(1|ID),data=Dead,family=binomial(logexp(Dead$SL))) 
#I use (1|ID) here to account for the repeated measurements of the same individuals
    summary(M1)

plot(Dead$DBH,plogis(predict(M1,re.form=NA)))
</code></pre>

<p><strong>Primarily I want to know</strong>:</p>

<ol>
<li><strong>Does the statistical technique I am using to control for the difference in time between census seem sensible? If it isn't, can you think of a better way to deal with the problem?</strong></li>
<li><strong>If the answer to the first question is yes, is using the inverse logit (plogis) the correct way to get predictions expressed as probabilities?</strong></li>
</ol>

<p>Thanks in advance for any help!</p>
"
"0.131306432859723","0.128489042187512","151354","<p>While trying to determine power for a Poisson GLMM, I started by checking the probability of rejecting the null for a given parameter when the null is true (parameter is zero). I kept coming up with a rejection rate of approximately $0.1$ where I expected $\alpha = 0.05$. To check if my programming was faulty, I did the same with a binomial GLMM and a Gaussian LMM. Both of these, however, returned the expected percentage of rejections ($5\%$). Thus, I don't think this is a post for Stack Overflow, but maybe others will disagree.</p>

<p>I figured I'd post the code here and see if anyone can tell me why I'm seeing this unexpected result.</p>

<p>First, a function to simulate data with a single independent variable $X$ and the dependent variable $Y$. The data simulate $100$ individuals with $3$ measurements each. $Y$ is defined as a function of the intercept, $bX$, and the group specific intercept ($g$). Though, $b$ is zero, so $X$ doesn't come into play. The function also fits a GLMM to the data and checks/returns whether $b=0$ should be rejected.</p>

<pre><code>simPow.Pois &lt;- function(j=100, i=3, alpha=.05, b=0, tau=1, refRate=.2) {
    # refRate is referent group rate (intercept)

    # g is group level intercept
    g &lt;- rep(rnorm(j, 0, tau), each=i)

    # group identifies the groupings of the individuals
    group &lt;- rep(1:j, each=3)

    # randomly drawn x
    x &lt;- round(runif(i*j,-.5,5.499)) # approx uniform discrete 0-5

    # DV: a function of intercept, b*x, and group intercept
    y &lt;- exp(refRate + b*x + g)
    y &lt;- rpois(i*j, y)

    # fit the model with one of three options
    #ans &lt;- glmer(y ~ x + (1|group), family=poisson)
    ans &lt;- glmmPQL(y ~ x, random=~1|group, family=poisson)
    #ans &lt;- glmmadmb(y ~ x + (1|group), family = ""poisson"",link = ""log"")

    # extract z as [fixed effect] / [SE]
    z &lt;- abs(fixef(ans) / sqrt(diag(vcov(ans))))

    # check if z is too large to believe the null is true
    if(2*(1-pnorm(z['x'])) &lt; alpha) {
        return(1)
    } else { 
        return(0)
    }
}
</code></pre>

<p>The function above returns a <code>1</code> if the null hypothesis is rejected, and a <code>0</code> otherwise.</p>

<p>I then run the following script to do this many times. Unfortunately, this takes a few minutes to run, so I've limited it to 200 iterations. You can expand that if you doubt the result.</p>

<pre><code>require(MASS)
res.pois &lt;- replicate(200,simPow.Pois(b=0))
mean(res.pois)
#[1] 0.105
</code></pre>

<p>I appreciate any thoughts on this. Thank you.</p>
"
"NaN","NaN","151764","<p>I understand that the R-function </p>

<pre><code>pbinom(0, 100, 0.5, lower.tail=FALSE) 
</code></pre>

<p>returns the probability of getting 0 or more heads in 100 trials. R gives the correct outcome, 1 for this problem. But if I use very small values for the probability in a single trial, as in </p>

<pre><code>pbinom(0, 100, 0.0000000001, lower.tail=FALSE) 
</code></pre>

<p>I get an answer close to 0, actually: <code>1E-08</code>, while I expected a result close to 1. Is this a bug? A similar thing occurs in Excel. But the propbability of ZERO or more successes in a binomial experiment should always be equal to 1. For the example above it is not important, because I know the answer already, but I am using the function pbinom() in a program.</p>
"
"0.0496291666985465","0.0485642931178632","152091","<p>I am fairly new with logistic regression. I have a binary response. And did this plot. The binary response is:</p>

<p>Y = 0: The student fails</p>

<p>Y = 1: The student succeed</p>

<pre><code>library(ggplot2)
ggplot(data = both, aes(x = age, y = succeed)) + 
  stat_smooth(method = 'glm', family = 'binomial') +
  theme_bw()+xlab(""X"")+ylab(""The student succeed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xM9wS.png"" alt=""enter image description here""></p>

<p><strong>What is the y-axis? estimated probability that the student succeed? Or estimated log odds of a student succeed?
Feeling a little confused. Could someone explain what they are I have on the y-axis?</strong></p>
"
"0.0496291666985465","0.0485642931178632","152111","<p>I have a dataset with multiple samples of batches of observations (e.g. one batch of 20 obs., one batch of 50 obs,, etc.). There is a probability that the batches have contaminated observations, with the number of contaminated observations in each batch following a binomial distribution with a constant probability across batches. However--due to the testing equipment--one only observes whether a batch is contaminated (if there are 1 or latent contaminated samples) or not contaminated. Is there a standard model / R package / formula to calculate the maximum likelihood estimate of the probability?</p>
"
"0.099258333397093","0.0971285862357264","152208","<p>I have a problem where <code>glm.nb</code> (R version 3.1.0, MASS version 7.3.33) converges on some data, but adding only one 0 it does not converge any more. This is the data</p>

<pre><code>x &lt;- c(3908,2729,10,803,1893,27,1312,1457,4534,3420,3,1608,903,1702,
       3041,1267,1381,3983,203,2202,1021,1550,1293,2572,1868,877,2317,
       2442, 1174,2450,1183,349)
</code></pre>

<p><code>glm.nb(x~1)</code> converges fine, but when I run <code>glm.nb(c(x,0)~1)</code> it does not converge. <code>zeroinfl(c(x,0)~1, dist=""negbin""</code>) converges estimating the zero probability at 0.029 (this is roughly <code>1/(length(x)+1)</code>). It seems that the problem is with <code>theta.ml</code>, which <code>glm.nb</code> uses. More precisely <code>theta.ml(c(x,0), 1681)</code> (1681 is the poisson estimate of <code>mu</code>) does not converge and this fails <code>glm.nb</code>.</p>

<p>To me adding one 0 seems like a benign thing to do (in this case), for such a dramatic change in behavior. My problem is bigger than the indication above, because I have many other pieces of data where <code>glm.nb/theta.ml</code> does not converge (most have more than one 0) and I am not sure what to do. I am trying to compare the negative binomial fit with its zero-inflated version (<code>zeroinf</code>) and am getting foiled because of this. Is the failure of <code>glm.nb</code> an indication that negative binomial is not appropriate? This might be the case for the examples with more 0's, but the above example with only one 0 is confusing me, because it makes me think that the problem is with the <code>theta.ml</code> code. </p>

<p>Any comments/suggestions? <code>theta.ml</code> seems to employ a simple iterative procedure and perhaps someone who understands it better can comment on its convergence properties.</p>
"
"NaN","NaN","153514","<p>In a negative-binomial experiment with three independent trials, if I want the probability for 2 successes before the first failure. The probability of each trial is 0.6.</p>

<pre><code>dnbinom(c(1), 2, 0.6)
</code></pre>

<p>in R it gives me 0.288. But why? In my model, the only sequence that can meet my condition is HHT, it's 0.144. Where does the 0.288 come from?</p>
"
"0.0701862406343596","0.0686802819743445","154637","<p>I'm reading this book (Implementation: How Great Expectations in Washington Are Dashed in Oakland. 1973) and they discuss how hard it is to gain agreement of actors.  </p>

<p>They set up a conceptual problem to illustrate this.  </p>

<p>Assume that in any chain of decisions, the contracting parties have an 80% probability of reaching an agreement to proceed. Now assume that there are 70 separate agreements that much be reached.  What is the probability that all 70 agreements will be successful.  </p>

<p>They report that with an 80% probability of agreement, the probability of success after 70 agreements is 0.000000125 and that the number of agreements that reduce the probability to below 50 percent is 4.  </p>

<p>The thing is: they do not really explain how they arrive at these probabilities.  So, that's why I'm turning to you.</p>

<p>It seems like what is going on here is an exercise in the binomial distribution.  Given an 80% probability of a successful trial, what is the probability that 70 trials will return all successes? </p>

<p>To put this in the language of coin tosses: If we assume a coin has a probability of turning up heads of 80%, and we flip the coin 70 times, what is the probability that you would get heads 70 straight times?</p>

<p>Have I got this right?</p>

<p>Thanks for any insight. Thanks, Simon </p>

<p>I hope I've explained myself</p>
"
"0.122825921110129","0.120190493455103","155668","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has: 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>doing this below doesn't specify the individual age group, or can this be worked out from the summary stats? </p>

<pre><code>Model&lt;-glm(Maturity~Lgnth+age, family=binomial(logit)) 
</code></pre>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this. Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.122825921110129","0.120190493455103","155762","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has:
 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this.
Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.0573068255061253","0.0560772154092044","156766","<p>I have been having real issues trying to calculate the length at which certain probabilities of maturing are reached. This is NOT the same as the proportion of individuals that are mature as <code>dose.p</code> would calculate.</p>

<p>I know I have to run a logistic regression with binomial errors, something similar to the below code</p>

<pre><code>mylogit &lt;- glm(Maturity ~ Lngth, data = data, family = ""binomial"")
</code></pre>

<p>But from this how can I work out the length at which the probability of maturing equals 50%.
My data frame consists of Maturity (0,1) data and length data. (I have other data but believe this is all I need at this stage (I could be wrong!)
Any help would be greatly appreciated! </p>
"
"0.131306432859723","0.128489042187512","156883","<p>What is the difference between the density and probability?</p>

<p>I have tried R in which I can use both <code>pnorm</code> and <code>dnorm</code> for the normal distribution and <code>pbinom</code> and <code>dbinom</code> for the binomial distribution, etc.</p>

<p>I have tried reading the documentation but I don't think it's quite clear what the difference is between those two functions. I know that <code>pnorm(x)</code> gives me $P(X \leq x)$.</p>

<p>My guess is that the density is the value of $f(x)$ while the probability is $\int_\infty^x f(x) \, \mathrm{d}x$. If this is correct, I don't understand what the value of $f(x)$ gives me of information since only the area under the function gives me the probability.</p>

<p>I know that the interesting values are pdf=probability density function and cdf=cumulative density function. So maybe the pdf is the value from <code>dnorm</code> and is the area at a specific x while cdf is the value from <code>pnorm</code> and is the area from $\infty$ to the specific x. But I have learned that you cannot get the probability at a specific $x$ in continuous distributions so that explanation does not make sense.</p>
"
"0.0701862406343596","0.0686802819743445","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.140372481268719","0.137360563948689","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.0496291666985465","0.0485642931178632","167794","<p>I wrote a script that create a logistic model, for Email opening probability, for each user name.</p>

<pre><code>form&lt;-formula(OpenOrNor~as.factor(TimeSend)
              +OpenWithSmartphoneind)
models&lt;- dlply(Data, ""User_id"", 
               function(df) {
                 model&lt;-glm(formula = form,family = binomial(""logit""),data = df,control = glm.control(epsilon = 1e-9, maxit = 500))
                 return(model)})
</code></pre>

<p>for some users it return me </p>

<pre><code>Call:  glm(formula = form, family = binomial(""logit""), data = df, weights = HistoryWeights, 
    control = glm.control(epsilon = 0.000000001, maxit = 500))

Coefficients:
              (Intercept)            as.factor(TimeSend)2      as.factor(TimeSend)3   as.factor(TimeSend)4  
                -22.61106                   20.21853                    0.07738                   20.54737  
          as.factor(TimeSend)5       as.factor(TimeSend)6      as.factor(TimeSend)7   as.factor(TimeSend)9  
                  0.19292                   -0.03624                    0.22013                    0.11837  
          OpenWithSmartphoneind  
                       NA  

Degrees of Freedom: 83 Total (i.e. Null);  76 Residual
Null Deviance:      190.6 
Residual Deviance: 166.8    AIC: 182.8
</code></pre>

<p>We can see that for OpenWithSmartphoneind their  is NA. and this is because their are no Opening With Smartphone at all In this user history.</p>

<p>My question is how it will impact on predict?<br>
And doe's it make different if OpenWithSmartphoneind in the formula will be a factor type or not?</p>
"
"0.17243138545012","0.181710946077908","168167","<p>My dependent variable is a probability. As such, values lie between 0 and 1. The most common values are 0, 0.5, and 1 each occurring in 20% to 30% of the observations but any value in between is possible and some do occur. </p>

<p><strong>Question 1: Which regression model is best to explain such data?</strong></p>

<ul>
<li><p>Ordinary least squares (OLS, function <code>lm</code> in Râ€™s <code>stats</code> package) is not suitable as it does neither account for the limited interval nor the accumulation at the margins.</p></li>
<li><p>Logit regression (function <code>glm</code> with parameter <code>family=""binomial""</code> in Râ€™s <code>stats</code> package) accounts for the accumulation at 0 and 1 but does not allow intermediate values.</p></li>
<li><p>Ordered logit regression (function <code>polr</code> in Râ€™s <code>MASS</code> package) could be applied when I divide the [0, 1] interval in subintervals. However, I lose the continuous nature of the dependent variable.</p></li>
<li><p>For probit and ordered probit regressions, the same applies as for logit and ordered logit.</p></li>
<li><p>Left- and right-censored tobit regression (function <code>tobit</code> with parameters <code>left=0</code> and <code>right=1</code> in Râ€™s <code>AER</code> package) might be appropriate. However, I found the following quote: â€œSome researchers have considered using censored normal regression techniques such as tobit ([R] tobit) on proportions data that contain zeros or ones. However, this is not an appropriate strategy, as the observed data in this case are not censored: values outside the [0, 1] interval are not feasible for proportions data.â€ (p. 302 in Baum (2008), <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0147"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0147</a>). </p></li>
</ul>

<p>Below you find a code example </p>

<pre><code># Load libraries
library(stats, MASS, AER)
# Generate data
set.seed(123)
data &lt;- data.frame(x1 &lt;- runif(60, min = 0, max = 1), x2 &lt;- runif(60, min = 0, max = 1))
data$y  &lt;- -0.7 + data$x1 + 2 * data$x2 + rnorm(60, mean = 0, sd = 0.5)
    data$y  &lt;- ifelse(data$y &lt; 0, 0, data$y)
data$y  &lt;- ifelse(data$y &gt; 0.4 &amp; data$y &lt; 0.6, 0.5, data$y)
data$y  &lt;- ifelse(data$y &gt; 1, 1, data$y)
    data$yCat &lt;- data$y
    data$yCat &lt;- ifelse(data$yCat &gt; 0 &amp; data$yCat &lt; 0.5, 0.25, data$yCat)
    data$yCat &lt;- ifelse(data$yCat &gt; 0.5 &amp; data$yCat &lt; 1, 0.75, data$yCat)
    data$yCat &lt;- as.factor(data$yCat)
    hist(data$y, breaks=101)
# Different regression models
summary(lm(y ~ x1 + x2, data=data)) # OLS
summary(glm(y ~ x1 + x2, data=data, family=""binomial"")) # Logit
summary(polr(yCat ~ x1 + x2, data=data)) # Ordered logit
summary(tobit(y ~ x1 + x2, data=data, left=0, right=1)) # Tobit
</code></pre>

<p>To make matters worse, my data is panel data. I know how to handle individual, time, and mixed effects and random and fixed effects models using plm from Râ€™s plm package and F-test, LM-test, and Hausman test do decide which of these is best. </p>

<p><strong>Question 2: For the dependent variable described above, which panel regression model is best?</strong> </p>

<p>Below your find a code example for the data structure. This extends the prior example.</p>

<pre><code># Load library
library(plm)
# Generate data (builds on prior example)
data$id &lt;- rep( paste( ""F"", 1:15, sep = ""_"" ), each = 4)
    data$time &lt;- rep( 1981:1984, 15 )
pData &lt;- pdata.frame(data, c( ""id"", ""time"" ))
# Panel regression example
summary(plm(y ~ x1 + x2, data=pData, model=""within"", effect=""twoways"")) # Based on OLS
</code></pre>
"
"0.198516666794186","0.194257172471453","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.140372481268719","0.137360563948689","173511","<p>I'm trying a simple Monte Carlo example which led to some confusion about ways of generating random Geometrically distributed values in R.</p>

<p>""A supercomputer is shared by 250 independent subscribers. Each day, each subscriber uses the facility with probability 0.3. The number of tasks sent by each active user has Geometric distribution with parameter 0.15, and each task takes a Gamma(10, 3) distributed computer time (in minutes). Tasks are processed consecutively. What is the probability that all the tasks will be processed, that is, the total requested computer time is less than 24 hours? Estimate this probability, attaining the margin of error Â±0.01 with probability 0.99.""</p>

<p>The solution is given in pure MATLAB and I will spare you everything unrelated to my question. The Geometric distribution is calculated as</p>

<pre><code>Y=ceil( log(1-rand(X,1))/log(1-q) );
</code></pre>

<p>where <code>X</code> is a Binomial variable (the number of active subscribers a given day) and <code>q</code> is the parameter to the Geometric distribution. I translated this into R as</p>

<pre><code>Y &lt;- ceiling(log(1-runif(X))/log(1-q))
</code></pre>

<p>and got the expected result (~0.17) when I ran the simulation. However, as I understand it, R, unlike pure MATLAB, can generate Geometrically distributed random variables directly, like so</p>

<pre><code>Y &lt;- rgeom(X, q)
</code></pre>

<p>and when I tried that instead the simulation suddenly started returning values (~0.55).</p>

<p>I'm probably missing something obvious but can anyone explain what's going on here? I tried plotting the different distributions given by the expected value of X and they look similar enough but the results are obviously not equal. If any necessary details are missing tell me, and I will add them.</p>
"
"0.0744437500478198","0.0971285862357264","175079","<p>Is it so that:</p>

<ul>
<li>$y_i$ is not a discrete value, but a range with probability density function</li>
<li>Which means for the same predictor(s) value $y_i$ could have different results</li>
<li>In linear regression this distribution can only be normal</li>
<li>In GLM, this distribution can be any distribution from the exponential family</li>
<li>distribution of a single $y_i$ has nothing to do with distribution of all $y(s)$</li>
<li>$\mu_i$ is expected value of $y_i$</li>
<li>In practical use, $\mu_i$ is the predicted value $y_i$, specially if dataset has only one y for given predictor(s)</li>
</ul>

<p>Are above correct? Where am I wrong?</p>

<p>Based on the above I've tried simulating <code>glm</code> with <code>lm</code> in R, and it kinda works:</p>

<pre><code>library(boot)
download.file(""https://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""./ravensData.rda"",method=""curl"")
load(""./ravensData.rda"")
# download manually and loadhere if above fails
# load(""/yourpath/ravensData.rda"")

# calling logit(ravensData$ravenWinNum) results in 
# [1]  Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf
# [17] -Inf -Inf  Inf -Inf
# that's way too much, as inv.logit goes to 1 at 20
# so we'll write our own dummy ""logit"" routine
# this will give us 5 when winNum=1 and -5 when it's zero
win &lt;- ravensData$ravenWinNum*10-5

# now we can do a simple lm
fit &lt;- lm(win~ravensData$ravenScore)

# and get probability of win using inv.logit
fitwin &lt;- inv.logit(fit$fitted.values)
plot(ravensData$ravenScore, fitwin)

# now glm
fitglm &lt;- glm(ravensData$ravenWinNum ~ ravensData$ravenScore, family=""binomial"")
plot(ravensData$ravenScore,fitglm$fitted)
</code></pre>
"
"0.121566134770966","0.118957737857722","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.156941205143586","0.153573779208488","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.141247084629227","0.153573779208488","175773","<p>I have a hypothesis testing model in which I would like to know if environmental variables I collected influences the probability a bacteria on a given amphibian kills a pathogen. </p>

<p>Following Crawley's R intro to stats book on binomial models I do this:</p>

<pre><code>y &lt;- cbind(df$Antipathogen, df$Total_isolated - df$Antipathogen)
</code></pre>

<p>I still follow Crawley, but bring in a glmer model (using help from online) as I want to examine this at the Site level and not transect level. So, I make transect a random effect. I sampled three sites. One site has one transect, the second site has two transects, and the third site was sampled along an altitudinal gradient and has seven transects.</p>

<p>This is my model:</p>

<pre><code>model &lt;- glmer(y ~ Site + Species + sex + BodyCon + Leaf_litter + (1|Transect), 
               data = df, family = binomial)
</code></pre>

<p>I use the Anova function in car to see which terms are significant when they are introduced into the model</p>

<pre><code>Anova(model, type = ""III"", test.statistic = ""Chisq"")
</code></pre>

<p>I get this:</p>

<pre><code>Response: y
              Chisq Df Pr(&gt;Chisq)    
(Intercept) 21.3200  1  3.887e-06 ***
Site        12.0107  2   0.002466 **
Species      0.0617  2   0.969644  
sex          0.2313  2   0.890785    
BodyCon      0.7058  1   0.400851    
Leaf_litter  2.8763  1   0.089890 . 
</code></pre>

<p>I am starting to understand the function lsmeans in lsmeans package to look at pairwise comparisons to figure out how each of my sites differ from one another. </p>

<p>This is where my question comes in:</p>

<p>What is the appropriate approach -- use the full model and apply the lsmeans function to the full model </p>

<pre><code>lsmeans(model, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>OR -- remove non-significant terms in a step-wise manner following Crawleyâ€™s (2007, pg. 325) Principle of Parsimony to simplify the full model. And use </p>

<pre><code>model2 &lt;- update(model, ~.-Species)
anova(model, model2, test = ""Chisq"")
</code></pre>

<p>To make sure removing the terms is valid.</p>

<p>Then, I can do lsmeans on the final model that now only contains the only significant variable -- site. </p>

<pre><code>lsmeans(model5, pairwise~Site, adjust = ""tukey"")
</code></pre>

<p>Regardless of how I do it I still get a significant p-value for Site, and the pairwise comparisons give similar results. The p-value is only much lower when I do lsmeans on the reduced model. </p>

<p>I don't know what the better approach is, and doing some basic searching I could not find a similar question. </p>
"
"0.253233834800894","0.247800295647344","175834","<p>I have a distribution represented as a scatter plot (see image below). It is clear to me from looking at the plot that there is an L shaped curve that describes most of the data. I am interested in identifying the outliers from this distribution, the data points that are much higher on the y-axis relative to other points on the X axis. If I just set a hard cut off, such as take all values 2 SDs above the mean I will only get values with a mean > 0.6 on the Y-axis. But I am also interested in values with a lower mean, such as the data points further along the X axis that have means &lt; 0.3 but which are clearly distinguishable as sitting above the general distribution in the scatter plot.</p>

<p>Context, if it helps: Each point is a gene and I am trying to detect genes with a mean score from a test that is noticeably above the genomic average, conditioning on gene size, which is on the X axis. As genes get larger, we expect the mean of this test to decrease. So I want to identify all genes that have high means, given their size. </p>

<p>Is there a statistically robust way to identify which datapoints are outliers relative to their position in the distribution? Another way to put it is I want to create a curve that describes the distribution of the majority of the data, and then identify all the points that are outliers above this curve.</p>

<p>I thought about dividing the distribution into bins based on X axis value, then for each bin identifying values that are 2 SDs above the mean for that bin. But then I have no good criteria for defining bin width, which would influence the total number of outliers I detect. 
I saw that a kernal density approach can be used to identify outliers in a scatter plot, though I am not familiar with this. Also this seemed to also detect outliers below the mean. I am only interested in outliers above the mean.</p>

<p>It would be great if this could be done in R, where I have been analysing the data.</p>

<p>Please let me know if I can clarify my question, I am probably not using the right terminology to describe my problem.</p>

<p>Thanks in advance.</p>

<p><a href=""http://i.stack.imgur.com/ToJIR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ToJIR.png"" alt=""enter image description here""></a></p>

<pre><code>GENE    mean_score  total_number_snps
X1  0.1 3
X2  0.1466666667    30
X3  0.1375  8
X4  0.24    5
X5  0.2625  8
X6  0.2 1
X7  0.1466666667    15
X8  0.2 1
X9  0.1666666667    9
X10 0.1 1
X11 0.1928571429    14
X12 0.1 2
X13 0.1545454545    11
X14 0.1333333333    3
X15 0.1666666667    3
X16 0.2117647059    34
X17 0.1452380952    42
X18 0.16    5
X19 0.2 1
X20 0.25    2
X21 0.125   4
X22 0.2 13
X23 0.1714285714    7
X24 0.15    6
X25 0.2 3
X26 0.2894736842    19
X27 0.2352941176    17
X28 0.1333333333    6
X29 0.12    5
X30 0.2 3
X31 0.1 1
X32 0.1571428571    7
X33 0.2125  8
X34 0.18125 16
X35 0.26    10
X36 0.1368421053    19
X37 0.1333333333    6
X38 0.15    2
X39 0.14    5
X40 0.18    15
X41 0.14    5
X42 0.3 1
X43 0.1 2
X44 0.1 6
X45 0.1 4
X46 0.1 1
X47 0.1333333333    3
X48 0.1166666667    6
X49 0.225   4
X50 0.2 15
X51 0.125   12
X52 0.1 3
X53 0.1714285714    14
X54 0.175   4
X55 0.3404761905    42
X56 0.1 1
X57 0.25    2
X58 0.15    4
X59 0.1 1
X60 0.1666666667    3
X61 0.3 2
X62 0.225   4
X63 0.3076923077    13
X64 0.1 1
X65 0.1666666667    3
X66 0.1666666667    6
X67 0.1 3
X68 0.1 3
X69 0.1166666667    6
X70 0.125   8
X71 0.2 1
X72 0.2 2
X73 0.1333333333    42
X74 0.1 1
X75 0.2 8
X76 0.1444444444    9
X77 0.1666666667    15
X78 0.1 2
X79 0.176744186 43
X80 0.1275  40
X81 0.1666666667    3
X82 0.125   4
X83 0.2545454545    11
X84 0.1304347826    46
X85 0.21    10
X86 0.1571428571    7
X87 0.3 9
X88 0.275   16
X89 0.11    10
X90 0.1333333333    6
X91 0.2333333333    3
X92 0.2 2
X93 0.2866666667    15
X94 0.25    2
X95 0.1125  8
X96 0.4 11
X97 0.1 1
X98 0.2 2
X99 0.15    2
X100    0.1625  8
X101    0.24    5
X102    0.175   4
X103    0.15    4
X104    0.1333333333    3
X105    0.4 2
X106    0.2 3
X107    0.25    2
X108    0.32    5
X109    0.2333333333    3
X110    0.1714285714    7
X111    0.2 1
X112    0.225   4
X113    0.2 1
X114    0.1714285714    7
X115    0.15    2
X116    0.1166666667    6
X117    0.16875 16
X118    0.1555555556    9
X119    0.15    6
X120    0.12    5
X121    0.1 1
X122    0.1333333333    6
X123    0.2333333333    3
X124    0.1 1
X125    0.2333333333    3
X126    0.1333333333    3
X127    0.1 1
X128    0.1827586207    29
X129    0.25    8
X130    0.2 7
X131    0.25    6
X132    0.1 1
X133    0.125   4
X134    0.2 1
X135    0.1666666667    3
X136    0.1 3
X137    0.12    5
X138    0.1 1
X139    0.175   4
X140    0.1 1
X141    0.1666666667    3
X142    0.1666666667    3
X143    0.1 1
X144    0.1375  8
X145    0.1 9
X146    0.1 2
X147    0.125   4
X148    0.1333333333    3
X149    0.1769230769    13
X150    0.15    2
X151    0.1214285714    14
X152    0.1 1
X153    0.2555555556    18
X154    0.2 1
X155    0.1 1
X156    0.1 1
X157    0.1 1
X158    0.4 1
X159    0.14    5
X160    0.1 2
X161    0.1333333333    3
X162    0.375   8
X163    0.2263157895    19
X164    0.1636363636    11
X165    0.3 1
X166    0.1 3
X167    0.2 1
X168    0.3 1
X169    0.1428571429    7
X170    0.1 2
X171    0.1222222222    9
X172    0.1 8
X173    0.1 5
X174    0.1 8
X175    0.1666666667    3
X176    0.2 5
X177    0.1 4
X178    0.1166666667    6
X179    0.15    2
X180    0.3666666667    3
X181    0.25    4
X182    0.1 1
X183    0.1 2
X184    0.1 1
X185    0.1 1
X186    0.1 1
X187    0.184   25
X188    0.2333333333    3
X189    0.2333333333    3
X190    0.1 2
X191    0.32    5
X192    0.1 2
X193    0.12    5
X194    0.1 5
X195    0.2 1
X196    0.1 6
X197    0.1 2
X198    0.4 1
X199    0.2 2
X200    0.1 2
X201    0.2 1
X202    0.2333333333    6
X203    0.35    2
X204    0.1 1
X205    0.12    5
X206    0.14    5
X207    0.125   4
X208    0.3333333333    3
X209    0.1 2
X210    0.1 3
X211    0.1 1
X212    0.2 4
X213    0.15    8
X214    0.125   4
X215    0.1548387097    31
X216    0.2 7
X217    0.225   4
X218    0.125   4
X219    0.15    2
X220    0.4 1
X221    0.275   4
X222    0.325   4
X223    0.2 3
X224    0.175   4
X225    0.3 1
X226    0.1 1
X227    0.19    10
X228    0.25    4
X229    0.2666666667    9
X230    0.1 1
X231    0.2 1
X232    0.3 1
X233    0.2166666667    6
X234    0.26    5
X235    0.225   4
X236    0.1 1
X237    0.1857142857    7
X238    0.58    5
X239    0.25    10
X240    0.6066666667    15
X241    0.3 1
X242    0.5 2
X243    0.2333333333    3
X244    0.25    2
X245    0.1 4
X246    0.1 1
X247    0.1714285714    7
X248    0.16875 16
X249    0.2 1
X250    0.4 3
X251    0.1 1
X252    0.1666666667    6
X253    0.2 6
X254    0.3166666667    12
X255    0.1 1
X256    0.1 2
X257    0.4 1
X258    0.1333333333    3
X259    0.225   4
X260    0.2571428571    7
X261    0.4 5
X262    0.15    10
X263    0.1571428571    7
X264    0.2 11
X265    0.2285714286    7
X266    0.15    4
X267    0.3 1
X268    0.1384615385    13
X269    0.1 4
X270    0.1 1
X271    0.16    5
X272    0.1285714286    7
X273    0.1 1
X274    0.2222222222    9
X275    0.2083333333    12
X276    0.2153846154    13
X277    0.1888888889    9
X278    0.1 1
X279    0.1 2
X280    0.3 2
X281    0.17    10
X282    0.1 5
X283    0.2833333333    6
X284    0.1333333333    6
X285    0.1833333333    6
X286    0.1833333333    12
X287    0.1953488372    43
X288    0.2526315789    19
X289    0.1 1
X290    0.125   4
X291    0.26    5
X292    0.1 2
X293    0.2578947368    19
X294    0.2545454545    11
X295    0.1 1
X296    0.3666666667    3
X297    0.1714285714    7
X298    0.1833333333    6
X299    0.16    5
X300    0.2733333333    15
X301    0.275   4
X302    0.1 1
X303    0.2 7
X304    0.1583333333    12
X305    0.1666666667    3
X306    0.1 1
X307    0.1 6
X308    0.1642857143    14
X309    0.1 1
X310    0.1606060606    33
X311    0.1428571429    7
X312    0.1888888889    9
X313    0.2 2
X314    0.1388888889    18
X315    0.35    2
X316    0.3 2
X317    0.1 4
X318    0.15    16
X319    0.1166666667    12
X320    0.1888888889    9
X321    0.16    5
X322    0.2333333333    3
X323    0.1857142857    14
X324    0.31    20
X325    0.2 1
X326    0.1 1
X327    0.1952380952    21
X328    0.215625    32
X329    0.1 1
X330    0.1 1
X331    0.1307692308    13
X332    0.1 4
X333    0.1666666667    3
X334    0.2 14
X335    0.1583333333    12
X336    0.1961538462    26
X337    0.2222222222    9
X338    0.1 3
X339    0.1 2
X340    0.1285714286    14
X341    0.175   4
X342    0.125   4
X343    0.1 4
X344    0.1428571429    7
X345    0.1 4
X346    0.1 2
X347    0.15    2
X348    0.25    4
X349    0.22    5
X350    0.1 2
X351    0.1 3
X352    0.14    10
X353    0.1666666667    18
X354    0.1333333333    3
X355    0.2 3
X356    0.16    5
X357    0.3 1
X358    0.175   4
X359    0.5 1
X360    0.1111111111    9
X361    0.2333333333    6
X362    0.175   4
X363    0.227027027 37
X364    0.3857142857    7
X365    0.1 2
X366    0.2 3
X367    0.1916666667    12
X368    0.1428571429    14
X369    0.2666666667    3
X370    0.2 9
X371    0.25    2
X372    0.2 1
X373    0.1 2
X374    0.225   4
X375    0.1 1
X376    0.1 3
X377    0.3 2
X378    0.1 1
X379    0.1545454545    11
X380    0.1730769231    52
X381    0.1 3
X382    0.1333333333    3
X383    0.1814814815    27
X384    0.108   25
X385    0.2666666667    6
X386    0.1666666667    3
X387    0.25    8
X388    0.225   4
X389    0.24    25
X390    0.2666666667    6
X391    0.1 2
X392    0.15    4
X393    0.1666666667    6
X394    0.1 1
X395    0.2375  8
X396    0.125   4
X397    0.1 7
X398    0.1 7
X399    0.1 4
X400    0.1 2
X401    0.1625  8
X402    0.3 1
X403    0.3 2
X404    0.25    4
X405    0.2 1
X406    0.1285714286    7
X407    0.15    8
X408    0.5 1
X409    0.1 1
X410    0.1285714286    7
X411    0.1 1
X412    0.2166666667    30
X413    0.22    5
X414    0.2714285714    14
X415    0.1214285714    14
X416    0.2 8
X417    0.28    5
X418    0.24    35
X419    0.15    4
X420    0.1333333333    12
X421    0.125   4
X422    0.1 1
X423    0.1666666667    3
X424    0.2111111111    9
X425    0.3 4
X426    0.2 2
X427    0.2 3
X428    0.1 1
X429    0.1 1
X430    0.1617021277    47
X431    0.15    8
X432    0.1142857143    14
X433    0.15    4
X434    0.1384615385    13
X435    0.1 2
X436    0.1166666667    12
X437    0.1714285714    14
X438    0.2416666667    12
X439    0.1 1
X440    0.1428571429    7
X441    0.1 1
X442    0.1416666667    12
X443    0.3333333333    6
X444    0.2 1
X445    0.14    5
X446    0.2 3
X447    0.225   28
X448    0.1571428571    14
X449    0.1 1
X450    0.1583333333    12
X451    0.1518518519    27
X452    0.1363636364    11
X453    0.2 1
X454    0.1666666667    6
X455    0.1 1
X456    0.1333333333    3
X457    0.2368421053    19
X458    0.1222222222    9
X459    0.15    2
X460    0.2 1
X461    0.1625  24
X462    0.2 6
X463    0.1666666667    3
X464    0.1 3
X465    0.3 8
X466    0.1523809524    21
X467    0.1 3
X468    0.1 3
X469    0.15    4
X470    0.1 1
X471    0.1642857143    28
X472    0.1 5
X473    0.1 2
X474    0.12    15
X475    0.1 3
X476    0.1090909091    11
X477    0.1346153846    26
X478    0.125   4
X479    0.1444444444    9
X480    0.2 1
X481    0.1 1
X482    0.1 3
X483    0.2 3
X484    0.1375  8
X485    0.1 4
X486    0.12    5
X487    0.1739130435    23
X488    0.25    2
X489    0.1333333333    6
X490    0.3 1
X491    0.225   20
X492    0.175   4
X493    0.1 3
X494    0.1222222222    9
X495    0.1 1
X496    0.175   4
X497    0.2333333333    6
X498    0.1615384615    13
X499    0.15    8
X500    0.1666666667    6
X501    0.2 2
X502    0.1777777778    9
X503    0.15    4
X504    0.2666666667    3
X505    0.1 4
X506    0.1222222222    9
X507    0.15    2
X508    0.2 3
X509    0.1333333333    15
X510    0.14    5
X511    0.1 1
X512    0.4 1
X513    0.2125  8
X514    0.36    5
X515    0.34    5
X516    0.4 1
X517    0.1428571429    7
X518    0.3333333333    3
X519    0.1 3
X520    0.2277777778    18
X521    0.1916666667    12
X522    0.2 4
X523    0.1857142857    7
X524    0.1 2
X525    0.1 5
X526    0.2222222222    9
X527    0.1818181818    11
X528    0.2151515152    33
X529    0.1 3
X530    0.1214285714    14
X531    0.2 1
X532    0.1 2
X533    0.1 3
X534    0.1166666667    12
X535    0.1 2
X536    0.1 2
X537    0.1 1
X538    0.2379310345    29
X539    0.175   4
X540    0.1363636364    11
X541    0.1 1
X542    0.1479166667    48
X543    0.1928571429    28
X544    0.4 1
X545    0.1951219512    41
X546    0.1333333333    3
X547    0.15    4
X548    0.2833333333    6
X549    0.1547619048    42
X550    0.1555555556    9
X551    0.2363636364    11
X552    0.2142857143    7
X553    0.5 1
X554    0.15    4
X555    0.1709677419    31
X556    0.17    10
X557    0.1 2
X558    0.2866666667    15
X559    0.4 2
X560    0.15    2
X561    0.1424242424    66
X562    0.25    2
X563    0.1 3
X564    0.1285714286    7
X565    0.12    5
X566    0.25    4
X567    0.2263157895    19
X568    0.1 12
X569    0.1666666667    6
X570    0.5 1
X571    0.147826087 23
X572    0.1 1
X573    0.1818181818    11
X574    0.2 2
X575    0.15    2
X576    0.2 3
X577    0.16    15
X578    0.1621621622    37
X579    0.1333333333    3
X580    0.1333333333    12
X581    0.18    5
X582    0.1534482759    58
X583    0.1538461538    26
X584    0.1 9
X585    0.2142857143    7
X586    0.1 1
X587    0.1222222222    9
X588    0.1 1
X589    0.1 3
X590    0.1 6
X591    0.15    2
X592    0.1 2
X593    0.3 1
X594    0.1285714286    21
X595    0.2 2
X596    0.12    5
X597    0.1 1
X598    0.1 1
X599    0.1 2
X600    0.1153846154    13
X601    0.1 15
X602    0.1 1
X603    0.1 1
X604    0.1 4
X605    0.15    10
X606    0.15    4
X607    0.15    4
X608    0.2 1
X609    0.14    5
X610    0.2 1
X611    0.1 2
X612    0.1 3
X613    0.125   4
X614    0.172   25
X615    0.2 4
X616    0.1727272727    11
X617    0.2090909091    22
X618    0.1333333333    3
X619    0.1 7
X620    0.15    4
X621    0.1181818182    11
X622    0.1375  8
X623    0.1666666667    3
X624    0.1 3
X625    0.1090909091    11
X626    0.125   8
X627    0.1 2
X628    0.12    5
X629    0.1 8
X630    0.13    40
X631    0.1666666667    3
X632    0.34    5
X633    0.1714285714    7
X634    0.1636363636    11
X635    0.1 1
X636    0.1 1
X637    0.18125 16
X638    0.2 4
X639    0.2 8
X640    0.1 2
X641    0.1 1
X642    0.1166666667    6
X643    0.2 1
X644    0.6 1
X645    0.2666666667    9
X646    0.2666666667    3
X647    0.2 2
X648    0.1 2
X649    0.1 1
X650    0.1 2
X651    0.1 1
X652    0.125   4
X653    0.15    2
X654    0.1 1
X655    0.1 1
X656    0.35    4
X657    0.2666666667    3
X658    0.1 2
X659    0.1 1
X660    0.2 1
X661    0.1 2
X662    0.1 2
X663    0.1333333333    3
X664    0.1 2
X665    0.1 1
X666    0.225   4
X667    0.1666666667    6
X668    0.1 2
X669    0.1 3
X670    0.175   4
X671    0.1 3
X672    0.15    4
X673    0.1666666667    3
X674    0.1 3
X675    0.175   4
X676    0.25    8
X677    0.25    4
X678    0.2571428571    7
X679    0.1 1
X680    0.2571428571    7
X681    0.208   25
X682    0.325   12
X683    0.1 1
X684    0.25    2
X685    0.1 2
X686    0.3047619048    21
X687    0.24    5
X688    0.15    6
X689    0.1333333333    6
X690    0.3 1
X691    0.1 1
X692    0.15    2
X693    0.23    20
X694    0.2 2
X695    0.1666666667    6
X696    0.1342857143    35
X697    0.25    6
X698    0.2 8
X699    0.2 5
X700    0.5 1
X701    0.1333333333    6
X702    0.3 1
X703    0.15    2
X704    0.15    2
X705    0.1833333333    6
X706    0.15    6
X707    0.1493506494    77
X708    0.36    5
X709    0.3 2
X710    0.15    2
X711    0.38    5
X712    0.2666666667    3
X713    0.25    4
X714    0.225   4
X715    0.5 1
X716    0.1 2
X717    0.16    5
X718    0.3 2
X719    0.3538461538    13
X720    0.1 2
X721    0.175   4
X722    0.22    5
X723    0.175   4
X724    0.2333333333    6
X725    0.34    5
X726    0.2 7
X727    0.1 1
X728    0.3 3
X729    0.1 1
X730    0.1 3
X731    0.3 5
X732    0.35    6
X733    0.2875  8
X734    0.1 1
X735    0.1 2
X736    0.2 5
X737    0.1714285714    7
X738    0.375   4
X739    0.1 4
X740    0.3 1
X741    0.1 1
X742    0.1142857143    7
X743    0.1 1
X744    0.2285714286    7
X745    0.14    5
X746    0.15    6
X747    0.1 1
X748    0.125   4
X749    0.1666666667    6
X750    0.125   8
X751    0.1 1
X752    0.15    2
X753    0.2 1
X754    0.225   4
X755    0.3 1
X756    0.3 5
X757    0.175   4
X758    0.1 3
X759    0.1333333333    18
X760    0.1230769231    13
X761    0.2 1
X762    0.11    10
X763    0.1666666667    6
X764    0.1 1
X765    0.2090909091    11
X766    0.145   20
X767    0.14    5
X768    0.2375  8
X769    0.1571428571    7
X770    0.1 1
X771    0.1 2
X772    0.2 2
X773    0.16    5
X774    0.2 1
X775    0.1777777778    9
X776    0.1210526316    19
X777    0.2 1
X778    0.225   12
X779    0.1666666667    3
X780    0.1 6
X781    0.2333333333    6
X782    0.1692307692    13
X783    0.19    10
X784    0.2 3
X785    0.1489361702    47
X786    0.2 5
X787    0.45    2
X788    0.1666666667    6
X789    0.18    5
X790    0.3 1
X791    0.2 2
X792    0.11    10
X793    0.3333333333    3
X794    0.25    2
X795    0.2 1
X796    0.25    2
X797    0.2 2
X798    0.2 1
X799    0.1 3
X800    0.1333333333    18
X801    0.1473684211    19
X802    0.2 5
X803    0.14    5
X804    0.125   4
X805    0.1583333333    12
X806    0.1857142857    7
X807    0.1 1
X808    0.2 1
X809    0.1769230769    26
X810    0.1 1
X811    0.1 2
X812    0.1833333333    6
X813    0.1409090909    22
X814    0.1416666667    24
X815    0.1307692308    13
X816    0.1235294118    17
X817    0.1 1
X818    0.1 1
X819    0.18    30
X820    0.2514285714    35
X821    0.18    5
X822    0.2 4
X823    0.1 1
X824    0.2333333333    9
X825    0.1222222222    9
X826    0.15    2
X827    0.14    5
X828    0.1588235294    51
X829    0.15    2
X830    0.2 4
X831    0.1 2
X832    0.1391304348    23
X833    0.18    20
X834    0.15    2
X835    0.3 1
X836    0.1 8
X837    0.1666666667    9
X838    0.1954545455    22
X839    0.225   16
X840    0.1222222222    9
X841    0.1210526316    19
X842    0.1 2
X843    0.1 2
X844    0.125   4
X845    0.1 4
X846    0.1 1
X847    0.2 2
X848    0.275   4
X849    0.1 3
X850    0.2833333333    6
X851    0.175   4
X852    0.32    5
X853    0.1 1
X854    0.1428571429    7
X855    0.2277777778    18
X856    0.15    8
X857    0.12    5
X858    0.1 2
X859    0.175   4
X860    0.18    5
X861    0.16    5
X862    0.2333333333    6
X863    0.1 1
X864    0.3333333333    3
X865    0.1 2
X866    0.15    12
X867    0.1636363636    11
X868    0.4 1
X869    0.4 1
X870    0.1 3
X871    0.1555555556    9
X872    0.2 1
X873    0.3 1
X874    0.2 2
X875    0.15    12
X876    0.1 1
X877    0.1181818182    11
X878    0.1428571429    7
X879    0.1461538462    13
X880    0.3076923077    13
X881    0.2 2
X882    0.3 1
X883    0.205   20
X884    0.2 5
X885    0.1333333333    3
X886    0.15    2
X887    0.25    2
X888    0.15    4
X889    0.3 1
X890    0.125   4
X891    0.1875  8
X892    0.1428571429    7
X893    0.2333333333    3
X894    0.1 2
X895    0.1 1
X896    0.35    6
X897    0.1444444444    9
X898    0.2 2
X899    0.3 1
X900    0.1 2
X901    0.1 1
X902    0.25    2
X903    0.1 1
X904    0.1 1
X905    0.7 1
X906    0.2 1
X907    0.45    4
X908    0.25    2
X909    0.15    4
X910    0.1 2
X911    0.4 13
X912    0.1 2
X913    0.1842105263    19
X914    0.1 1
X915    0.1333333333    3
X916    0.2 2
X917    0.1 7
X918    0.1 1
X919    0.225   4
X920    0.2 1
X921    0.2 3
X922    0.18    5
X923    0.1 1
X924    0.1875  8
X925    0.2833333333    6
X926    0.5 3
X927    0.2 1
X928    0.1 1
X929    0.1 2
X930    0.2 3
X931    0.4 1
X932    0.2875  16
X933    0.1857142857    7
X934    0.1 1
X935    0.2 2
X936    0.1 1
X937    0.2 13
X938    0.2444444444    9
X939    0.1 1
X940    0.1714285714    7
X941    0.3 1
X942    0.1 1
X943    0.2857142857    7
X944    0.15    2
X945    0.1 1
X946    0.15625 16
X947    0.1666666667    3
X948    0.3 1
X949    0.2 2
X950    0.1 8
X951    0.1 1
X952    0.1 3
X953    0.3 1
X954    0.3 1
X955    0.1 3
X956    0.1125  8
X957    0.18    5
X958    0.2666666667    3
X959    0.2 1
X960    0.125   4
X961    0.1333333333    3
X962    0.2444444444    9
X963    0.25    10
X964    0.25    4
X965    0.2 1
X966    0.225   4
X967    0.1625  8
X968    0.1333333333    3
X969    0.1333333333    3
X970    0.1 1
X971    0.2 7
X972    0.3 10
X973    0.1 1
X974    0.3 2
X975    0.225   4
X976    0.1 1
X977    0.1 2
X978    0.4 1
X979    0.1333333333    3
X980    0.1333333333    9
X981    0.13125 16
X982    0.1 1
X983    0.2 1
X984    0.1782608696    23
X985    0.2225806452    31
X986    0.15    4
X987    0.1 3
X988    0.1 3
X989    0.15    4
X990    0.2285714286    14
X991    0.2384615385    26
X992    0.4 1
X993    0.4 2
X994    0.1 1
X995    0.1 1
X996    0.1666666667    3
X997    0.1 6
X998    0.13    20
X999    0.2666666667    3
</code></pre>

<p>I attempted to use a funnel plot, as this seems to be a good approach for my goal adapting code from an R tutorial <code>http://www.r-bloggers.com/power-tools-for-aspiring-data-journalists-r/</code></p>

<pre><code>number=mydata$total
    p=mydata$mean

p.se &lt;- sqrt((p*(1-p)) / (number))
df &lt;- data.frame(p, number, p.se)

## common effect (fixed effect model)
p.fem &lt;- weighted.mean(p, 1/p.se^2)

## lower and upper limits for 95% and 99.9% CI, based on FEM estimator
#TH: I'm going to alter the spacing of the samples used to generate the curves
number.seq &lt;- seq(1000, max(number), 1000)
number.ll95 &lt;- p.fem - 1.96 * sqrt((p.fem*(1-p.fem)) / (number.seq))
number.ul95 &lt;- p.fem + 1.96 * sqrt((p.fem*(1-p.fem)) / (number.seq))
number.ll999 &lt;- p.fem - 3.29 * sqrt((p.fem*(1-p.fem)) / (number.seq))
number.ul999 &lt;- p.fem + 3.29 * sqrt((p.fem*(1-p.fem)) / (number.seq))
dfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, number.ul999, number.seq, p.fem)

## draw plot
#TH: note that we need to tweak the limits of the y-axis
fp &lt;- ggplot(aes(x = number, y = p), data = df) +
geom_point(shape = 1) +
geom_line(aes(x = number.seq, y = number.ll95, colour = ""red""), data = dfCI) +
geom_line(aes(x = number.seq, y = number.ul95), data = dfCI) +
geom_line(aes(x = number.seq, y = number.ll999, linetype = factor(2)), data = dfCI) +geom_line(aes(x = number.seq, y = number.ul999, linetype = factor(2)), data = dfCI) +
geom_hline(aes(yintercept = p.fem), data = dfCI) +
xlab(""number"") + ylab(""p"") + theme_bw()
</code></pre>

<p>The result looks good, except that I the funnel plot lines are too short on both ends, not covering the X axis. Does anyone know the reason for this? I can't tell if it's a coding error or an analysis problem.</p>

<p><a href=""http://i.stack.imgur.com/f5fe0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/f5fe0.png"" alt=""enter image description here""></a></p>

<p>Version 2:</p>

<p>I also made a funnel plot using a different method with this code:</p>

<pre><code>x &lt;- Asianpig_data$total
    prob &lt;- Asianpig_data$mean

#generate 99% confidence intervals based on overall probability of pop_prob
alph &lt;- 0.01
seq &lt;- 1:(max(x)+5)

#via http://r.789695.n4.nabble.com/inverse-binomial-in-R-td4631935.html
invbinomial &lt;- function(n, k, p) { 
   uniroot(function(x) pbinom(k, n, x) - p, c(0, 1))$root 
} 
low &lt;- mapply(invbinomial,n=seq,k=seq*pop_prob,p=1-alph/2)
high &lt;- mapply(invbinomial,n=seq,k=seq*pop_prob,p=alph/2)

plot(x,prob) 

lines(low,col='red')              #low and high funnel lines
lines(high,col='red')
</code></pre>

<p>It looks like this: </p>

<p><a href=""http://i.stack.imgur.com/lCGFs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lCGFs.png"" alt=""enter image description here""></a></p>

<p>As far as I understand, the first funnel plot code calculates the standard error while the second code calculates the confidence intervals? I will investigate which is most appropriate for my data, any input is welcome. </p>

<p>Thanks in advance for your help.</p>
"
"0.110974190404619","0.108593060690767","176847","<p>I would like to find the effect size of a binomial test and relate it to other measures of effect size such as Cohenâ€™s <em>d</em> or Pearsonâ€™s <em>r</em>.</p>

<p>Say I have 13 successes for 18 trials, where the probability of obtaining one success is 0.333.  in R:</p>

<pre><code>binom.test(13, 18, 0.333)
</code></pre>

<p>which gives the following output:</p>

<pre><code>Exact binomial test

data:  13 and 18
number of successes = 13, number of trials = 18, p-value = 0.001526
alternative hypothesis: true probability of success is not equal to 0.333
95 percent confidence interval:
 0.4651980 0.9030508
sample estimates:
probability of success 
             0.7222222 
</code></pre>

<p>1) Can I calculate the effect size for this test as follows?  effect size:  0.722 â€“ 0.333 = 0.389? </p>

<p>2) How does this relate to Cohenâ€™s <em>d</em> or Pearsonâ€™s <em>r</em> ?</p>
"
"0.204942810591567","0.200545433019714","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.149637567815527","0.161069538480791","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.131306432859723","0.128489042187512","183772","<p>I'm learning about Probability with Binomial, Poisson and Normal Distributions. I came across some study examples on-line that ask you to do it within R. I was given 10 exercises which were:</p>

<p><a href=""http://i.stack.imgur.com/KRs1G.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KRs1G.png"" alt=""enter image description here""></a></p>

<p>I did all 10 within R with this code but I have no idea if the results are correct as I don't know how to check them. My code is: </p>

<pre><code>round(pbinom(5, size = 10, prob = 0.65, lower = F), 4)
round(1 - dbinom(30, size = 100, prob = .2), 4)                 
round(dbinom(15:30, size = 50, prob = .32), 4)
round(dpois(6, lambda = 6) - dpois(8, lambda = 6), 4)
round(ppois(35, lambda = 41, lower = F), 4)
round(sum(dpois(2:5, lambda = 1)), 4)
round(pnorm(12, mean = 7, sd = 2.5, lower = F), 4)
round(pnorm(9.8, mean = 10, sd = 1, lower = F), 4)
round(1 - pnorm(38, mean = 50, sd = 5, lower = F), 4)
round(pnorm(4, mean = 5, sd = 3.6, lower = F), 4)
</code></pre>

<p>and the results I got within R are as follows:</p>

<p><a href=""http://i.stack.imgur.com/KVW9N.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KVW9N.png"" alt=""enter image description here""></a></p>

<p>If anyone can indicate if my code is actually correct and I'm studying it correctly, it would really help me. Thanks.</p>
"
"0.156941205143586","0.153573779208488","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.0496291666985465","0.0485642931178632","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.0859602382591879","0.0841158231138067","187015","<p>First, I have read <a href=""http://stats.stackexchange.com/questions/10985/dispersion-parameter-of-negbin-distribution/"" title=""this post"">this post</a>, <a href=""http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r/"" title=""this post"">this post</a> and <a href=""http://stats.stackexchange.com/questions/66586/is-there-a-test-to-determine-whether-glm-overdispersion-is-significant/"" title=""this post"">this post</a>. All have very useful information. I have three other more specific questions.</p>

<p>I have estimated a negative binomial model using the glm.nb function of MASS and discovered the following parameters
Theta: 9.0487, S.E: 0.444</p>

<ol>
<li>Is it correct to assume that dispersion parameter has a standard deviation of 20.38?</li>
<li>Does this value correspond to the Poisson overdispersion that is corrected by the negative binomial model or is my model still overdispersed?</li>
<li>Joseph Hilbe states in his <a href=""http://www.cambridge.org/us/academic/subjects/statistics-probability/statistical-theory-and-methods/modeling-count-data/"" rel=""nofollow"" title=""book"">book</a> that R's glm.nb function employs an inverted relationship of the dispersion parameter, theta. Thus a Poisson model results when theta approaches infinity. Suppose now that my second glm.nb model had estimates of Theta: 19.0487, S.E: 0.444. Would this model be less overdispersed than the first model?</li>
</ol>
"
"0.099258333397093","0.0971285862357264","189627","<p>I am using the <code>survey</code> package and my model is:  </p>

<pre><code>modelsvy &lt;- svydesign(id =~ 1, data=temp12, weights=~WGT) 
model12s &lt;- svyglm(DEPVAR ~ var1 + var2 +... ,  modelsvy, family= quasibinomial)  
</code></pre>

<p>This has been going well for me. </p>

<p>I have a sample of the US population (N=4,343), with fractional weights (<code>WGT</code>) on each observation. The completed model is then scored as a probability: </p>

<pre><code>mdlg2012$DEPVARSCR &lt;- predict(model12s,mdlg2012)  
mdlg2012$DEPVARSCRP &lt;- (1 / (1 + exp(-1 * mdlg2012$DEPVARSCR)))  
</code></pre>

<p>The probabilities are applied to census block-group data to estimate the number of households interested in buying things like Life Insurance.  </p>

<p>In a variation, I built a couple models that did not use all N. For example, I subset to N=2,339, with DEPVAR = 1 (n=178) and DEPVAR = 0 (n=2161). The entire population was not included. The proportion of N was 178 / 2,339 = 0.076. For N overall, it would have been 178 / 4343 = 0.041 (stated on an unweighted basis, I do need to weight them).  </p>

<p>My question is, would you happen to know how I might calibrate the resulting new model probabilities back to the full US population? I down sampled, removing n = 2,004, which inflated the DEPVAR incidence. The logistic transform in this case overstates the probabilities. Is the calibrate(design,...) command useful here?</p>
"
"0.295940066441417","0.329998130794724","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0701862406343596","0.0686802819743445","199504","<p>I would like to compare an average probability to a fixed probability value in order to determine if there is a significant difference between the two.</p>

<p>My participants had to detect and point a target that appeared among three distractors. The possibility they answered hazardously therefore is 1 out of 4 (i.e., .25). My dependent variable being binary (Correct answer: 1 ; Incorrect answer: 0) I am using logistic regression for my analyses:</p>

<pre><code>model1 &lt;- glm(Accuracy ~ Task * Masking,
              data = DF,
              family = binomial(link = ""logit""))
</code></pre>

<p>âŸ¶ <a href=""http://i.stack.imgur.com/AyPzG.png"" rel=""nofollow"">Plot of results</a></p>

<p>More precisely, I would like to know if the probability of report in the Reach-Masked condition is significantly different from 0.25.</p>

<p>How do I test this possibility in R?</p>

<p>Thank you for your time. Have a very nice day.</p>
"
"0.0701862406343596","0.0686802819743445","200159","<p>Suppose that the diameter of trees of a certain type are normally distributed with $\mu = 8.8in $ and $\sigma = 1.75in$ If four trees are independently selected, what is the probability that at least one tree has a diameter exceeding than $10in$? </p>

<p>$X\sim{N(8.8,1.75^2)}$</p>

<p>So the probability for a single tree's diameter being greater than 10 is:</p>

<p>$P(X\ge 10) = 1 - P(X&lt;10)$</p>

<p>Standardize: $ z=\frac{10-8.8}{2.8} = 0.43$</p>

<p>$1 - P(z&lt;0.43) =$ <code>1 - pnorm(0.43) = 0.334</code></p>

<p>If $n = 4$ independent trees, then if I was to create a new distribution $Y$ with a diameter greater than 10, then $Y\sim binom(4,.334)$ I can conclude that:</p>

<p>$P(Y \ge 1) = 1 - P(Y \le 0) =$ <code>1-dbinom(0,4,.334) = 0.803</code></p>

<p>Is this correct? Would it make sense that this can be considered to be a binomial distribution? Under what conditions should I use <code>dbinom</code> over <code>pbinom</code> commands?   </p>
"
"0.110974190404619","0.108593060690767","200505","<p>Background: </p>

<p>We have a developed an alternative website which we believe will increase sales by a small percentage (1-4%). Although small this is very valuable. </p>

<p>Our Site:</p>

<p>Visitors are tracked by a cookie and take up to ten days to complete a purchase. 
Traffic and sales are clearly seasonal. </p>

<p>Despite this we are reasonably comfortable using the A/B testing described in detail on the web and here on Cross Validated. </p>

<p>To determine power, we are performing the following simulation:</p>

<ol>
<li>Estimate current site conversion using the last ten days of traffic and orders.</li>
<li>Assume that the new site will increase conversion by 2%</li>
<li>Simulate traffic using a binomial random generator. One group will have a probability of success equal to the measured conversion X, the other X * 1.02</li>
<li>Count the number of successes in each group and then run a pearson chi square test on the two proportions.</li>
<li>Repeat this 10,000 times.</li>
<li>Calculate what % of the 10,000 simulations give a significant result (p &lt; 0.05).</li>
</ol>

<p>This gives a far more conservative sample size to just using 
power.prop.test in R (say).</p>

<p>Is this a reasonable way to estimate power. Are we are being too conservative? It seems to me that power.prop.test is assuming that 'base' conversion is known, which is not the case in our test.  </p>

<p>R snippet: </p>

<pre><code>prop.test.power_analysis = function(runs, confidence_level, trials, successes) { 

 alpha = 1 - confidence_level

 result &lt;- replicate(
 n = runs,
 expr = {

   res &lt;- prop.test(
      c(sum(rbinom(n = trials[1],size = 1,prob = successes[1]/trials[1]))
       ,sum(rbinom(n = trials[2],size = 1,prob = successes[2]/trials[2]))),
     n = trials, alternative = 'greater')$p.value &lt; alpha

   res
 })

power &lt;- sum(result)/runs # power
power
}
</code></pre>
"
"0.110974190404619","0.108593060690767","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0.0496291666985465","0.0485642931178632","202211","<p>I have a logistic regression in R whose goal is to predict the probability of default on some test data. </p>

<p><code>glm(default ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>What I'd like to do is 'bin' this data so that bins 1 to n each have a certain rate of default. How can I bin the logistic regression results in this way? For example, the bins on a sample set of 1000 might look like:</p>

<pre><code>Bin# P(Default) Count
1         4%     400
2         2%     300
3         1%     300
</code></pre>

<p>That is, I set in advance the probabilities I want each bin to have (.04,.02,.01) and then bins are created based around those settings.</p>
"
"0.112548371022619","0.128489042187512","203693","<p>I get the same p-value for a one-tailed and two-tailed exact binomial test for some combinations of x, n, and p. Does anyone know why this happens? What formula does R use to calculate the p-value for two-tailed binomial tests?</p>

<pre><code>&gt;binom.test(x=2,n=9,p=.1, alternative=""greater"")

Exact binomial test

data:  2 and 9
number of successes = 2, number of trials = 9, p-value = 0.2252
alternative hypothesis: true probability of success is greater than 0.1
95 percent confidence interval:
0.04102317 1.00000000
sample estimates:
probability of success 
             0.2222222 

&gt; binom.test(x=2,n=9,p=.1, alternative=""two.sided"")

    Exact binomial test

data:  2 and 9
number of successes = 2, number of trials = 9, p-value = 0.2252
alternative hypothesis: true probability of success is not equal to 0.1
95 percent confidence interval:
0.02814497 0.60009357
sample estimates:
probability of success 
             0.2222222 
</code></pre>
"
"0.0859602382591879","0.0841158231138067","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.110974190404619","0.108593060690767","210317","<p>I'm performing binomial test to check whether the number of occurrences within buckets are not greater than 95%. I'm using <code>R</code> for that.</p>

<pre><code>pbinom(5-1,50,0.1)
</code></pre>

<p>In the above in one bucket I have 5 occurrences (n-1), out of 50 cases, the expected probability of occurrences out of the total 50 cases is here p=0.1.</p>

<p>I would like to perform a test that would just do the same however each occurrence represents random monetary values. Here for example the 5 occurrences could represent $a_{i}=5*rnorm(5, 100, 20)$ five monetary values. The 50 cases above is known absolute monetary value for example 10.000.</p>

<p>I would like to perform test similar to the binomial however not on counts (number of realisations).</p>

<p>It was suggested to me that this would be the appropriate test:</p>

<p>$P(\frac{a}{b}&lt;PD) = \textbf{N}\mid\mid{\frac{\sum_{i}^{n}{(a_{i})} - PD \times b}{\sqrt{b \times PD(1-PD)}}\mid\mid}$</p>

<p>Where the <strong>N</strong> represents cumulative distribution function. The sum over $a_{i}$ represent the sum of the realised monetary values as above. The PD is expected probability (of realisations) and $b$ is the total monetary value in given bucket.    </p>

<p>I tried to implement it but I'm not sure. The results seems wrong. </p>

<p><code>vfun &lt;- function(a,b,p){ (a-p*b)/sqrt(b*p*(1-p)) }</code></p>

<p><code>pnorm(abs(vfun(300, 1000, 0.1)))</code></p>
"
"0.121566134770966","0.099131448214768","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.099258333397093","0.0971285862357264","211174","<p>We are oversampling the data to use in logistic regression. Aim  is to predict CTR(click probability) which is rare event scenario.
I have predicted the probabilities of click but CTR results are inflated as we over sampled positive class.</p>

<p>model2&lt;-SMOTE(V61 ~ ., z2, perc.over = 600,perc.under=100, learner = 'glm',family=binomial())</p>

<p>Is there any way to undo oversampling results so that I can get exact probabilities ? Based on research so far, one easiest way to divide the output probability by the multiplier we used in over sampling. I dont feel it would be the exact way as I have used synthetic minority over sampling technique(SMOTE) in R.</p>
"
"0.0496291666985465","0.0485642931178632","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.112548371022619","0.128489042187512","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.180552614478965","0.200235710158407","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0859602382591879","0.0841158231138067","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.226525117591484","0.23053124319287","221426","<p>I am working on a problem in which I have multiple pairs of currently living males <code>i</code> that each have a presumed paternal ancestor <code>ni</code> generations ago (based on genealogical evidence) and where I have info on whether there is a mismatch in their Y chromosomal genotype (exclusively paternally inherited, <code>xi</code> = 1 for mismatch, 0 if there is a match). If there is no mismatch, they indeed have a common paternal ancestor, but if there is there must have been a kink in the chain as a result of one or more extra-marital affairs (I can only detect though if either none or at least one such extra-pair paternity event happened, ie the dependent variable is censored). What I am interested in is obtaining a maximum likelihood estimate (plus 95% confidence limits) not just of the mean extra-pair paternity (EPP) rate (probability that per generation a child would be derived from an extra-pair copulation), but also to try to infer how the extra-pair paternity rate may have changed as a function of time (as the nr of generations that separated the common paternal ancestor should have some info on this - when there is a mismatch I don't know though when the EPPs would have happened, as it could have been anywhere between the generation of that presumed ancestor and the present, but when there is a match we are sure there were no EPPs in any of the preceding generations). Hence, both my dependent binomial variable and independent covariate generation/time are censored. Based on a somewhat similar problem posted <a href=""http://stats.stackexchange.com/questions/152111/censored-binomial-model-log-likelihood"">here</a> I already figured out how I could make a maximum-likelihood estimate of the population and time-average extra-pair paternity rate <code>phat</code> plus 95% profile likelihood confidence intervals in R as follows :</p>

<pre><code># Function to make overall ML estimate of EPP rate p plus 95% profile likelihood confidence intervals, 
# taking into account that for pairs with mismatches multiple EPP events could have occured
#
# input is 
#     x=vector of booleans or 0 and 1s specifying if there was a mismatch or not (1 or TRUE = mismatch)
#     n=vector with nr of generations that separate common ancestor
# output is mle2 maximum likelihood fit with best-fit param EPP rate p

estimateP = function(x, n) {
  if (!is.logical(x[[1]])) x = (x==1)
  neglogL = function(p, x, n)  -sum((log(1 - (1-p)^n))[x]) -sum((n*log(1-p))[!x]) # negative log likelihood, see see http://stats.stackexchange.com/questions/152111/censored-binomial-model-log-likelihood
  require(bbmle)
  fit = mle2(neglogL, start=list(p=0.01), data=list(x=x, n=n))
  return(fit)
}
</code></pre>

<p>Example with some pilot data (from <a href=""https://bio.kuleuven.be/ento/pdfs/larmuseau_etal_procb_2013.pdf"" rel=""nofollow"">Larmuseau et al. ProcB 2010</a>):</p>

<pre><code>n = c(7, 7, 7, 7, 7, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 23, 23, 24, 24, 25, 27, 31) # number of generations/meioses that separate presumed common paternal ancestor
x = c(rep(0,6), 1, rep(0,7), 1, 1, 1, 0, 1, rep(0,20), 1, rep(0,13), 1, 1, rep(0,5)) # whether pair of individuals had non-matching Y chromosomal genotypes
</code></pre>

<p>Maximum-likelihood estimate of population and time-average extra-pair paternity rate plus 95% confidence limits :</p>

<pre><code>fit = estimateP(x,n)
c(coef(fit),confint(fit))*100 # estimated p and profile likelihood confidence intervals
#           p     2.5 %    97.5 % 
#   0.9415172 0.4306652 1.7458847
</code></pre>

<p>i.e. 0.9% [0.43-1.75% 95% C.L.s] of all kids were derived from a father that was different than the supposed one.</p>

<p>I then wanted to go a step further, and also try to estimate a possible temporal trend in the extra-pair paternity rate <code>p</code> as a function of generation <code>ni</code> (for simplicity assuming a linear relationship between the log odds of observing an extra-pair paternity event and generation), taking into account that if a mismatch occurs the EPP events could have taken place anywhere between the generation of the common ancestor <code>ni</code> and the present (generation 0), and that if there was no mismatch that no EPP event could have taken place in any of the previous generations for that particular pair of individuals.</p>

<p>If before we assumed the probability of a child being derived from an extra-pair copulation $p$ to be constant, and if $X$ was a random variable equal to $1$ when a Y chromosome mismatch was observed (corresponding to 1 or more EPP events) and $0$ otherwise, then the probability of observing no mismatch (that is, $X=0$) when the paternal ancestor lived $n$ generations ago ($n = 1, 2, 3, \ldots$) was $(1-p)^n$, whereas the chance of observing an EPP event was</p>

<p>$$\Pr(X=1\,|\, n) = 1 - (1-p)^n.$$</p>

<p>In a dataset of independent observations $\mathbf{x} = (x_1, x_2, \ldots)$ of $X$ with paternal ancestors living $\mathbf{n} = (n_1, n_2, \ldots)$ generations ago, the likelihood therefore was</p>

<p>$$L(p; \mathbf{x}, \mathbf{n}) = \prod_{x_i=1} \left(1 - (1-p)^{n_i}\right)\prod_{x_j=0} (1-p)^{n_j},$$</p>

<p>resulting in a log likelihood of</p>

<p>$$\Lambda(p) = \sum_{x_i=1} \log\left(1 - (1-p)^{n_i}\right) + \sum_{x_j=0} {n_j} \log (1-p).$$</p>

<p>Taking into account that in my more complex model incorporating temporal dynamics I want $p$ to be a function of $n$ now, with 
$$log (p/(1-p)) = a + b.n$$
, i.e. 
$$p(a,b,n) = \exp(a+b.n) / (1+\exp(a+b.n))$$</p>

<p>I then changed the definition of the likelihood function above accordingly and maximized it using function <code>mle2</code> from package <code>bbmle</code> :</p>

<pre><code># ML estimation, assuming that EPP rate p shows a temporal trend
# where log(p/(1-p))=a+b*n
# input is 
#     x=vector of booleans or 0 and 1s specifying if there was a mismatch or not (1 or TRUE = mismatch)
#     n=vector with nr of generations that separate common ancestor
# output is mle2 maximum likelihood fit with best-fit params a and b

estimatePtemp = function(x, n) {
  if (!is.logical(x[[1]])) x = (x==1)
  pfun = function(a, b, n) exp(a+b*n)/(1+exp(a+b*n)) # we now write p as a function of a, b and n
  logL = function(a, b, x, n)  sum((log(1 - (1-pfun(a, b, n))^n))[x]) + 
    sum((n*log(1-pfun(a, b, n)))[!x]) # a and b are params to be estimated, modified from http://stats.stackexchange.com/questions/152111/censored-binomial-model-log-likelihood
  neglogL = function(a, b, x, n)  -logL(a, b, x, n) # negative log-likelihood
  require(bbmle)
  fit = mle2(neglogL, start=list(a=-3, b=-0.1), data=list(x=x, n=n))
  return(fit)
}

# fitted coefficients
estfit = estimatePtemp(x, n)
cbind(coef(estfit),confint(estfit)) # parameter estimates and profile likelihood confidence intervals
#                      2.5 %      97.5 %
#   a -3.09054167 -5.3191406 -1.12078519
#   b -0.09870851 -0.2396262  0.02848305
summary(estfit)
# Coefficients:
#      Estimate Std. Error z value    Pr(z)   
#   a -3.090542   1.057382 -2.9228 0.003469 **
#   b -0.098709   0.067361 -1.4654 0.142819   
</code></pre>

<p>This gives me a reasonable looking historical estimate of the evolution of the EPP rate $p$ over time :</p>

<pre><code>pfun = function(a, b, n) exp(a+b*n)/(1+exp(a+b*n)) 
xvals=1:max(n)
par(mfrow=c(1,1))
plot(xvals,sapply(xvals,function (n) pfun(coef(estfit)[1], coef(estfit)[2], n)), 
     type=""l"", xlab=""Generations ago (n)"", ylab=""EPP rate (p)"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1WazP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1WazP.png"" alt=""enter image description here""></a></p>

<p>However, I am still a little stuck on how to calculate the 95% confidence intervals on the overall prediction of this model. Would anybody know how to do that by any chance? Maybe using <a href=""http://ms.mcmaster.ca/~bolker/emdbook/chap7A.pdf"" rel=""nofollow"">population prediction intervals</a> (by resampling parameters according to the fit following a multivariate normal distribution) (or would the delta method also work?)? And could somebody comment on whether my logic above is correct? I was also wondering if this kind of censored binomial model is known under some standard name in the literature, and if anyone knows of any published work on doing these kind of ML calculations under this kind of model? (I have a feeling that the problem should be fairly standard and correspond to something that's been done already, but can't seem to find anything...)</p>

<p>[PS Papers with more background on this topic/problem are available <a href=""https://bio.kuleuven.be/ento/pdfs/larmuseau_etal_procb_2013.pdf"" rel=""nofollow"">here</a> and <a href=""https://bio.kuleuven.be/ento/pdfs/larmuseau_etal_tree_2016.pdf"" rel=""nofollow"">here]</a></p>
"
"0.216599441022544","0.222549549284631","225311","<p>I need to realistically simulate study effect sizes and within-study variances for a random-effects meta-analysis in which the outcome is a relative risk. <strong>My question: why does this simulation lead to severe underestimation of $\tau^{2}$, and how can I fix it?</strong></p>

<h2>Simulation approach</h2>

<ul>
<li>Simulate total sample sizes for each of $k$ studies. Assume equal numbers in control and treatment groups for all studies.</li>
<li>Draw a population effect, $\mu_{i}$, for each study from a Normal with a specified mean and variance $V$. (Keep your eye on the variance -- it is the heart of my woes.) This is on the log-RR scale. </li>
<li>Draw the number of successes in the control group from a binomial with success probability $p_{0}$ (fixed across all studies). </li>
<li>Draw the number of success in the treatment group from a binomial with success probability $p_{1i} = \exp\{ \mu_{i} \} \cdot p_{0}$.</li>
<li>Compute observed effect sizes, $y_{i}$, for each study from the above data. </li>
<li>Fit a random-effects meta-analysis using Viechtbauer's <code>metafor</code>package.</li>
<li>Become upset that $\tau^{2} \ne V$.</li>
</ul>

<h2>Explanations I've considered</h2>

<p>Note that <code>metafor</code> by default estimates $\tau^{2}$ via REML. Using other options, such as Dersimonian-Laird, does not help. Also, the estimated SE of $\tau^{2}$ is also small, so it's not a precision problem. Finally, fiddling with the parameters doesn't help either. </p>

<h2>Reproducible example</h2>

<pre><code># set parameters
.k = c(1000)  # huge number of studies to eliminate concerns about asymptotics
.Mt = log(1.5)  # mean of true effects (log-RR)
.V = 0.3  # variance of true effects
p0=0.08  # P(success) in control group
seed = 131457

# simulate total N for each study
# right-skewed with minimum 20
N = round( rchisq(.k, df=2) * 30 + 20 )

# simulate population effect for each study
Mi = rnorm( n=.k, mean=.Mt, sd=sqrt(.V) )

##### Simulate Control Groups ##### 
# assume equal numbers in each treatment arm (so denom is N/2)
n0 = floor(N/2)

# simulate d0, number of successes in group 0
d0 = rbinom( n=.k, size=n0, prob=p0 )

##### Simulate Treatment Groups ##### 
# calculate n for this group
n1 = N - n0

# figure out p(success) in this group using population RR = exp(Mi)
# use pmin to ensure probability isn't above 1
# I used a small p0 and smallish .Mt to ensure that there is very little truncation
p1 = pmin( exp(Mi) * p0, 1 )  

# simulate d1, number of successes in group 1
d1 = rbinom( n=.k, size=n1, prob=p1 )

# calculate true ES using metafor
require(metafor)
temp = escalc( measure=""RR"", ai=d1, bi=n1-d1, ci=d0, di=n0-d0)
#head( log( (d1/n1) / (d0/n0) ) ); head(temp)  # yup, matches manual approach

# get observed effect sizes and within-study variances for each study
th.t = temp$yi
vyi = temp$vi

# see if true ES are hitting correct tau^2
RE = rma.uni( yi=th.t, vi=vyi, measure=""RR"")

# these should be the same (var of true ES):
print(RE$tau2); print(.V)
# LOOKS COMPLETELY HORRIBLE. WHY IS TAU^2 SO SMALL??
# ITS SE IS SMALL, TOO, SO THERE IS NO EXCUSE?
</code></pre>

<h2>Edit: A sort-of answer</h2>

<p>The culprit is the distribution of the sample sizes (which affect the within-study variances). The above code produces a highly right-skewed distribution of $n$. By generating large samples sizes from a more symmetric distribution, estimation of $\tau^{2}$ is unbiased.</p>

<p>Generating huge sample sizes from a Normal works:</p>

<pre><code>N = round( rnorm(.k, mean=10000, sd=800)   )
</code></pre>

<p>Or even from a uniform:</p>

<pre><code>N = round( runif(.k, 1000, 3000) )
</code></pre>

<p>But not generating small sample sizes from a uniform:</p>

<pre><code>N = round( runif(.k, 20, 200) )
</code></pre>

<p>I'd still very much appreciate a theoretical explanation for this behavior. As far as I know, the random-effects model does not make any assumptions on the within-study variances, so it's disturbing that this seems to matter so much.</p>
"
"0.185695338177052","0.181710946077908","226330","<p>Overall, I'd like to be able to say that, for the logistic prediction for this row, ColA was more influential in driving up the resultant probability (ie, y_hat) than ColB. (We'll use y_hat as it's usually defined for logistic.) But is this possible? Some data scientists I've talked to say yes, but I've also seen push-back.</p>

<p>From what I've read, it seems that GLMs make it easiest to get at a per-row variable importance (see <a href=""http://stats.stackexchange.com/q/190482"">this limited discussion</a> on logit in particular, including push-back). But can they actually do it?</p>

<p>If B1 and B2 are coefficients and the cols in X represent our features, it would seem that if <em>B1</em>*X1 is greater than <em>B2</em>*X2 then <em>B1</em>*X1 would drive the resultant probability towards 1 more than <em>B2</em>*X2. Here's an example (which brings in a factor col, for a full treatment).</p>

<p>We create features X1 and X2, where X1 is random and X2 (I think we can agree) has a large positive impact on y:</p>

<pre><code>set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(1,0,1,0,1,0,1,0,1,0)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)
</code></pre>

<p>Now we create the logit model:</p>

<pre><code>fit.logit = glm(
formula = y~.,
data = df,
family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:       -1.2353      22.0041
Wald statistic:      -0.267        0.003
</code></pre>

<p>Now if we multiply <em>B1</em> and <em>B2</em> by <em>X1</em> and <em>X2</em> respectively and print the results:</p>

<pre><code>coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)

          X1       X2
1  -0.55087679 22.00411
2  -0.48751729  0.00000
3  -0.59755734 22.00411
4  -1.13510089  0.00000
5  -1.04245907 22.00411
6  -0.63908954  0.00000
7  -0.53998690 22.00411
8  -0.42395777  0.00000
9  -0.01916833 22.00411
10 -0.14575621  0.00000
</code></pre>

<p>We see that in the rows where X2 = 1 then <em>B2</em>*X2 (ie the second column) is much higher than <em>B1</em>*X1 (ie the first column). So it would seem that we could say that for those rows that X2 would be the dominant feature driving up the resultant prediction towards 1.</p>

<p>If one reverses the y dependency on X2 by replacing zeros for ones in X2, then after doing the multiplication, <em>B2</em>*X2 has a much lower value than <em>B1</em>*X1 when X2 = 1, which makes sense (since X2 now pushes y_hat towards 0 when X2 = 1). Thus, for these rows, X1 is actually more ""responsible"" for driving y_hat towards 1. (Note that if both results are negative, then the least negative would be the feature more responsible for y_hat being as high as it is.) Because of this, it would seem that this method of per-row feature ranking still works. What am I missing? </p>

<p>In case it helps, the code for the latter (reversed dependency) case is below: </p>

<pre><code># Reverse y dependency on X2
set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(0,1,0,1,0,1,0,1,0,1)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)

fit.logit = glm(
  formula = y~.,
  data = df,
  family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:        -1.235      -22.004
Wald statistic:      -0.267       -0.003

coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)
multiply_res

            X1        X2
1  -0.55087679   0.00000
2  -0.48751729 -22.00411
3  -0.59755734   0.00000
4  -1.13510089 -22.00411
5  -1.04245907   0.00000
6  -0.63908954 -22.00411
7  -0.53998690   0.00000
8  -0.42395777 -22.00411
9  -0.01916833   0.00000
10 -0.14575621 -22.00411
</code></pre>

<p>Overall, for logistic, can we accurately say (for example) that feature A drives y_hat toward 1 more than feature B, for this individual prediction? </p>

<p>Thanks, all!</p>
"
"0.121566134770966","0.118957737857722","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
