"V1","V2","V3","V4"
"0.0640622132638473","0.0443133396029328","  1413","<p>It seems like the current revision of lmer does not allow for custom link functions.  </p>

<ol>
<li><p>If one needs to fit a logistic
linear mixed effect model with a
custom link function what options
are available in R?</p></li>
<li><p>If none - what options are available in other
statistics/programming packages?</p></li>
<li><p>Are there conceptual reasons lmer
does not have custom link functions,
or are the constraints purely
pragmatic/programmatic?</p></li>
</ol>
"
"0.110959008218296","0.115129433468","  3412","<p>I have an experiment that I'll try to abstract here.  Imagine I toss three white stones in front of you and ask you to make a judgment about their position.  I record a variety of properties of the stones and your response.   I do this over a number of subjects.  I generate two models.  One is that the nearest stone to you predicts your response, and the other is that the geometric center of the stones predicts your response.  So, using lmer in R I could write.</p>

<pre><code>mNear   &lt;- lmer(resp ~ nearest + (1|subject), REML = FALSE)
mCenter &lt;- lmer(resp ~ center  + (1|subject), REML = FALSE)
</code></pre>

<p><strong>UPDATE AND CHANGE - more direct version that incorporates several helpful comments</strong></p>

<p>I could try</p>

<pre><code>anova(mNear, mCenter)
</code></pre>

<p>Which is incorrect, of course, because they're not nested and I can't really compare them that way.  I was expecting anova.mer to throw an error but it didn't.  But the possible nesting that I could try here isn't natural and still leaves me with somewhat less analytical statements.  When models are nested naturally (e.g. quadratic on linear) the test is only one way.  But in this case what would it mean to have asymmetric findings?</p>

<p>For example, I could make a model three:</p>

<pre><code>mBoth &lt;- lmer(resp ~ center + nearest + (1|subject), REML = FALSE)
</code></pre>

<p>Then I can anova.</p>

<pre><code>anova(mCenter, mBoth)
anova(mNearest, mBoth)
</code></pre>

<p>This is fair to do and now I find that the center adds to the nearest effect (the second command) but BIC actually goes up when nearest is added to center (correction for the lower parsimony).  This confirms what was suspected.</p>

<p>But is finding this sufficient?  And is this fair when center and nearest are so highly correlated?</p>

<p>Is there a better way to analytically compare the models when it's not about adding and subtracting explanatory variables (degrees of freedom)?</p>
"
"0.0661631693578849","0.0686499305187969","  5270","<p>I have some data which, after lots of searching, I concluded would probably benefit from using a linear mixed effects model. I think I have an interesting result here, but I am having a little trouble figuring out how to interpret all of the results. This is what I get from the summary() function in R:</p>

<pre><code>&gt; summary(nonzero.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>My question involves the relationship between the dependent variable (""relative.sents.A"") and ""relative.sents.B"" once the random factors are factored out. I gather that the t-value of 5.357 for relative.sents.B should be significant.</p>

<p>But does this show what the direction of the effect is? I am thinking that because the coefficient for the slope is positive that this means that as relative.sents.B increases, so does my dependent variable. Is this correct?</p>

<p>The book I've been using briefly mentions that the correlation reported here is not a normal correlation, but goes into no details. Normally, I'd look there to figure out the direction and magnitude of the effect. Is that wrong?</p>

<p>If I'm wrong on both counts, then what is a good (hopefully reasonably straightforward) way to discover the direction and size of the effect?</p>
"
"0.111517999203046","0.11570943428471","  5333","<p>I have a linear mixed-effect model which I hope will answer the question of whether an increase in the frequency of use of one word leads to an increase of the frequency of use of that word by another person in a conversation, factoring out random effects of subject and topic of conversation. The basic model I've come up with looks like this:</p>

<pre><code>Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>The ""dependent"" variable is relative frequency of use by one person, and the fixed variable is relative frequency of use by another. I decided to see what the R^2 would be:</p>

<pre><code>&gt; cor(nonzero$relative.sents.A, fitted(nonzero.lmer))^2
[1] 0.6705905
</code></pre>

<p>To see what proportion of this is due to the fixed effect, I made a new model with only the random effects:</p>

<pre><code>&gt; summary(r.only.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ 1 + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -678.2 -666.3  343.1   -696.7  -686.2
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.2868e-04 0.0113435
 abstract (Intercept) 7.8525e-06 0.0028022
 Residual             3.7643e-04 0.0194017
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.029149   0.002088   13.96
</code></pre>

<p>...and tried the same thing:</p>

<pre><code>&gt; cor(nonzero$relative.sents.A, fitted(r.only.lmer))^2
[1] 0.6882534
</code></pre>

<p>To my surprise, without that fixed effect, R^2 seems to increase!</p>

<p>Does this mean my model is useless? If so, any suggestions on what might be wrong? Or am I somehow misinterpreting these results? </p>
"
"0.12266979912335","0.127280377713181","  6224","<p>I used the <code>lmer</code> function in the <code>lme4</code> package in order to assess the effects of 2 categorical fixed effects (1Âº Animal Group: rodents and ants; 2Âº Microhabitat: bare soil and under cover) on seed predation (a count dependent variable). I have 2 Sites, with 10 trees per site and 4 seed stations per tree. Site and Tree are my (philosophically) random factors, but given that I have only two level for Site, it must be treated as a fixed factor. I have questions about how to interpret the results:  </p>

<ol>
<li>I made a model selection criterion based on QAICc, but the best model (lower QAICc) does not result in any significant fixed effect and other models with higher QAIC (e.g. the Full Model) did find significant fixed factors. Does this make sense?  </li>
<li>Given a fixed factor that is important to the model, how do I distinguish which level of fixed factor is influencing the response variable?  </li>
</ol>

<p>Finally, correlation between the fixed factors implies an incorrect estimation of the model?  </p>

<pre><code>FullModel=lmer(SeedPredation ~ AnimalGroup*Microhabitat*Site + (1|Site:Tree) + 
                                   (1|obs), data=datos,  family=""poisson"") 

QAICc(FM)104.9896

    enterGeneralized linear mixed model fit by the Laplace approximation 
Formula: SP ~ AG * MH * Site + (1 | Site:Tree) + (1 | obs) 
   Data: datos 
   AIC   BIC logLik deviance
 101.8 125.6  -40.9     81.8
Random effects:
 Groups    Name        Variance Std.Dev.
 obs       (Intercept) 0.20536  0.45317 
 Site:Tree (Intercept) 1.19762  1.09436 
Number of obs: 80, groups: obs, 80; Site:Tree, 20

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)       0.01161    0.47608   0.024   0.9805  
AGR             -18.97679 3130.76500  -0.006   0.9952  
MHUC             -1.60704    0.63626  -2.526   0.0115 *
Site2            -0.91424    0.74506  -1.227   0.2198  
AGR:MHUC         19.92369 3130.76508   0.006   0.9949  
AGR:Site2         1.02241 4431.84919   0.000   0.9998  
MHUC:Site2        1.80029    0.86235   2.088   0.0368 *
AGR:MHUC:Site2   -3.49042 4431.84933  -0.001   0.9994  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) AGR    MHUC   Site2  AGR:MHUC AGR:S2 MHUC:S
AGR          0.000                                            
MHUC        -0.281  0.000                                     
Site2       -0.639  0.000  0.180                              
AGR:MHUC     0.000 -1.000  0.000  0.000                       
AGR:Site2    0.000 -0.706  0.000  0.000  0.706                
MHUC:Site2   0.208  0.000 -0.738 -0.419  0.000    0.000       
AGR:MHUC:S2  0.000  0.706  0.000  0.000 -0.706   -1.000  0.000 code here

BestModel=lmer(SP ~ AG * MH + (1|Site:Tree) + (1|obs), data=datos,  
               family = ""poisson"") 

QAICc(M) 101.4419

Generalized linear mixed model fit by the Laplace approximation 
Formula: SP ~ AG + AG:MH + (1 | Site:Tree) + (1 | obs) 
   Data: datos 
   AIC   BIC logLik deviance
 100.3 114.6 -44.15     88.3
Random effects:
 Groups    Name        Variance Std.Dev.
 obs       (Intercept) 0.76027  0.87194 
 Site:Tree (Intercept) 1.14358  1.06938 
Number of obs: 80, groups: obs, 80; Site:Tree, 20

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.5153     0.4061  -1.269    0.205
AGR          -18.7146  2603.4397  -0.007    0.994
AGA:MHUC      -0.7301     0.5045  -1.447    0.148
AGR:MHUC      17.7221  2603.4397   0.007    0.995
</code></pre>
"
"0.0827039616973562","0.0858124131484961","  6927","<p>I'm doing a simulation study which requires bootstrapping estimates obtained from a generalized linear mixed model (actually, the product of two estimates for fixed effects, one from a GLMM and one from an LMM). To do the study well would require about 1000 simulations with 1000 or 1500 bootstrap replications each time. This takes a significant amount of time on my computer (many days). </p>

<p><code>How can I speed up the computation of these fixed effects?</code></p>

<p>To be more specific, I have subjects who are measured repeatedly in three ways, giving rise to variables X, M, and Y, where X and M are continuous and Y is binary. We have two regression equations 
$$M=\alpha_0+\alpha_1X+\epsilon_1$$
$$Y^*=\beta_0+\beta_1X+\beta_2M+\epsilon_2$$
where Y$^*$ is the underlying latent continuous variable for $Y$ and the errors are not iid.<br>
The statistic we want to bootstrap is $\alpha_1\beta_2$. Thus, each bootstrap replication requires fitting an LMM and a GLMM. My R code is (using lme4)</p>

<pre><code>    stat=function(dat){
        a=fixef(lmer(M~X+(X|person),data=dat))[""X""]
        b=fixef(glmer(Y~X+M+(X+M|person),data=dat,family=""binomial""))[""M""]
        return(a*b)
    }</code></pre>

<p>I realize that I get the same estimate for $\alpha_1$ if I just fit it as a linear model, so that saves some time, but the same trick doesn't work for $\beta_2$.</p>

<p>Do I just need to buy a faster computer? :)</p>
"
"0.0739726721455309","0.0575647167340002"," 10429","<p>I'm wondering how to fit multivariate linear mixed model and finding multivariate BLUP in R. I'd appreciate if someone come up with example and R code. Thanks</p>

<p><strong>Edit</strong></p>

<p>I wonder how to fit multivariate linear mixed model with <code>lme4</code>. I fitted univariate linear mixed models with the following code:</p>

<pre><code>library(lme4)
lmer.m1 &lt;- lmer(Y1~A*B+(1|Block)+(1|Block:A), data=Data)
summary(lmer.m1)
anova(lmer.m1)

lmer.m2 &lt;- lmer(Y2~A*B+(1|Block)+(1|Block:A), data=Data)
summary(lmer.m2)
anova(lmer.m2)
</code></pre>

<p>I'd like to know how to fit multivariate linear mixed model with <code>lme4</code>. The data is below:</p>

<pre><code>Block A B    Y1    Y2
 1 1 1 135.8 121.6
 1 1 2 149.4 142.5
 1 1 3 155.4 145.0
 1 2 1 105.9 106.6
 1 2 2 112.9 119.2
 1 2 3 121.6 126.7
 2 1 1 121.9 133.5
 2 1 2 136.5 146.1
 2 1 3 145.8 154.0
 2 2 1 102.1 116.0
 2 2 2 112.0 121.3
 2 2 3 114.6 137.3
 3 1 1 133.4 132.4
 3 1 2 139.1 141.8
 3 1 3 157.3 156.1
 3 2 1 101.2  89.0
 3 2 2 109.8 104.6
 3 2 3 111.0 107.7
 4 1 1 124.9 133.4
 4 1 2 140.3 147.7
 4 1 3 147.1 157.7
 4 2 1 110.5  99.1
 4 2 2 117.7 100.9
 4 2 3 129.5 116.2
</code></pre>

<p>Thank in advance for your time and cooperation.</p>
"
"0.075498042361142","0.0783356573256404"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.0818727450060703","0.0849499549461966"," 18428","<p><strong>Edit Note:</strong> Since I posted this question, it has been suggested that I 
 read some documents and I am still thinking on the subject. I have
added some new understanding as marked between <em>*</em>* asterisks: please do
correct them if I am wrong and add remaining questions.*</p>

<p>I often confuse four symbols "":"", ""|"", ""/"" and ""*"" in the formula, particularly while doing mixed model. Can somebody explain clearly the differences between them? I have the following working example:</p>

<pre><code>require(lme4)
mydata &lt;- expand.grid(xvar1 =factor(1:10), xvar2 =factor(1:3),replication = factor(1:3))
mydata$yvar &lt;- rnorm(nrow(mydata), 10, 5)

fm1 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + (1|xvar1:xvar2), mydata)
***My understanding: Cross classification between xvar1, xvar2***  

fm2 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + (1|xvar1/xvar2), mydata)
***is not correct model, should remove 1|xvar1, 1|xvar2*** 
***** has fixed intercept 1 , I do not know if that is correct technically** 
**fm2 &lt;- lmer(yvar ~ 1 + (1|xvar1/xvar2), mydata)***** 
 *is xvar1 is nested in xvar2 and is essentially same as fm1*

fm3 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + (xvar1|xvar2), mydata)
Warning message:
In mer_finalize(ans) : singular convergence (7)
***I do not have idea on ""|"",*** 

fm4 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + xvar1:xvar2, mydata)
Error in mer_finalize(ans) : Downdated X'X is not positive definite, 31 ;
***xvar1:xvar2 interaction term, technically correct !***

fm5 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + (1|xvar1*xvar2), mydata)
Error in validObject(.Object) : 
  invalid class ""ngTMatrix"" object: all row indices must be between 0 and nrow-1
In addition: Warning message:
In Ops.factor(xvar1, xvar2) : * not meaningful for factors
    **again (1|xvar1) and (1|xvar2) are not necessary**
***fm5 &lt;- lmer(yvar ~  1 + (1|xvar1*xvar2), mydata) is equal to***
 ***fm5 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2)+ (1|xvar1*xvar2)***


fm6 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + (xvar1/xvar2), mydata)
 ***same as fm2***

fm7 &lt;- lmer(yvar ~ 1 + (1|xvar1) + (1|xvar2) + (0 + xvar1|xvar2), mydata)
   Warning message:
   In mer_finalize(ans) : singular convergence (7)
   ***I have no idea on xvar1|xvar2, 0 means no intercept I believe*** 
</code></pre>

<p><strong>Edit: Here is something I learned from R documentation on linear model formula</strong></p>

<pre><code>(1) yvar ~ xvar1 + xvar2 + xvar1:xvar2  - cross over classification
     is same as yvar ~ xvar1 * xvar2 

(2) yvar ~ xvar1 / xvar2  - nested classification (
    however means xvar1 + xvar2 + xvar1:xvar2)
     is same as yvar ~ xvar1 %in% xvar2
</code></pre>

<p>I am not sure those applicable to lm model hold true for mixed model. </p>

<p>I am still not on track on use of ""|"" which is I believe unique in mixed models. </p>
"
"0.105264957864947","0.10922137064511"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.110959008218296","0.115129433468"," 21600","<p>I have a data set of skewed nutrient intake values, from around 7800 individuals, of whom around 3000 had two measures of daily nutrient intake (the others only had one measure), so this is a repeated measures design, very unbalanced. The <code>boxcox</code> transformation in the <code>MASS</code> package in <code>R</code> identified a suitable lambda (0.4) to use for the transformation to linearity, and the nutrient value transformation takes the normal Box Cox form of <code>(x^L-1)/L</code> where <code>x</code> is the nutrient intake value (<code>&amp;response</code>) and <code>L</code> is lambda (0.4 in this case). The purpose of this transformation is to provide a linear dependent variable for a subsequent <code>nlmer</code> analysis in <code>R</code> although the original analysis was performed in <code>SAS</code>.</p>

<p><code>PROC NLMIXED</code> in <code>SAS</code>, unlike other <code>SAS</code> procedures, requires a user-specified <code>MODEL</code> statement. In the <code>SAS</code> code, a general log likelihood function has been provided, which is (rest of <code>SAS</code> syntax omitted, the <code>&amp;</code> are there as the syntax uses <code>SAS</code> macro language):</p>

<pre><code>ll1=log(1/(sqrt(2*pi*A_VAR_E)));
ll2=(-(boxcoxy-x2b2u2)**2)/(2*A_VAR_E)+(&amp;amtlambda-1)*log(&amp;response);
ll=ll1+ll2;
model &amp;response ~ general(ll);
</code></pre>

<p>I can follow some of this log likelihood specification, as I can see some relationship to the <a href=""http://en.wikipedia.org/wiki/Normal_distribution"" rel=""nofollow"">probability density function of the normal distribution</a>. But I don't understand the parameterisation to the dataset at hand. The values are:</p>

<ol>
<li><code>A_VAR_E</code> appears to be the variance of the residuals</li>
<li><code>boxcoxy</code> is the BoxCox-transformed values of <code>&amp;response</code></li>
<li><code>x2b2u2</code> is the fitted values based on the fixed and random effects in the model (incorporates age, race, weekend intake effects)</li>
<li><code>&amp;amtlambda</code> is 0.4 in this case</li>
<li><code>&amp;response</code> is the raw nutrient intake value, that was Box Cox transformed to <code>boxcoxy</code>, and is the dependent variable in this analysis.</li>
</ol>

<p>This likelihood function seems to have deviated from the usual form. In particular, I do not understand why <code>ll2</code> incorporates <code>(&amp;amtlambda-1)*log(&amp;response)</code> into the denominator. What is this part of the likelihood function doing? Why would the dependent variable <code>&amp;response</code> be on both sides of the equation in the model statement? In <code>ll2</code> the dependent variable appears to be included twice, once as its transformation <code>boxcoxy</code> in the numerator and once as its raw value in the denominator.</p>

<p>I would appreciate any help with understanding why this likelihood has been parameterised this way.</p>
"
"0.148201971273683","0.144726761101135"," 22363","<p>I asked this question on Stack Exchange, but I think it might be too specialized.  Hopefully someone in the mixed model group can help me out.</p>

<p>I want to be able to bootstrap the variance differences between two data sets obtained at different times while taking out the error in a random effect.</p>

<p>I have 2 sets of experimental data, where the data was measured at 2 time points (initial and final). I also have a set of simulation data. I want to compare the variance of the simulated date with the variance difference between the experimental data (final - initial). The idea is to get confidence intervals from the bootstrap to compare the experimental data with the simulation.</p>

<p>I am having trouble making the statistic for the bootstrap function in the <code>boot</code> package for R. So far I have.</p>

<pre><code>varcomp &lt;- function ( formula, data, indices ) {
    d &lt;- data[indices,] #sample for boot
    fit &lt;- lmer(formula, data=d) #linear model
    res.var = (attr (VarCorr(fit), ""sc"")^2) # variance estimation
    return(res.var)
    }
</code></pre>

<p>But this function only returns the variance of a single data set. I want to be able to input 2 sets of data and have it return the difference between the two data sets' variance.</p>

<p>When I try something like:</p>

<pre><code>varcomp &lt;- function ( formula, data1, data2, indices ) {
d1 &lt;- data1[indices,] #sample for boot
d2 &lt;- data2[indices,] #sample for boot
fit1 &lt;- lmer(formula, data=d1) #linear model
fit2 &lt;- lmer(formula, data=d2) #linear model
a = (attr (VarCorr(fit1), ""sc"")^2) #output variance estimation
b = (attr (VarCorr(fit2), ""sc"")^2) #output variance estimation
drv = a - b #difference between the variance estimations
return(drv)
}
</code></pre>

<p>I would then put it into boot such as:</p>

<pre><code>ip1.boot &lt;- boot ( data = ip1, statistic=varcomp, R=100, formula=CNPC~(1|Cell.line:DNA.extract)+Cell.line)
</code></pre>

<p>I can't do it this way because the boot function only allows for one data set to be inputted.</p>

<p><strong>Does anyone know how to create the correct statistic function for this?</strong></p>

<p>An example of the data can also be downloaded here (2 csv files zipped 1.22KB.)</p>

<p>My data looks something like the following:</p>

<p>Initial</p>

<pre><code>       Cell.line    Time DNA.extract   Gene      CNPC
1          9 initial           1 atubP1 1778.4589
2          9 initial           1 atubP1 2108.0552
3          9 initial           1 atubP1 2118.6725
4          9 initial           2 atubP1 2018.6593
5          9 initial           2 atubP1 1935.9008
6          9 initial           2 atubP1 1749.9158
7          9 initial           3 atubP1 1524.7475
8          9 initial           3 atubP1 1532.9781
9          9 initial           3 atubP1 1693.3098
10        17 initial           1 atubP1 1076.4720
11        17 initial           1 atubP1 1101.3315
12        17 initial           1 atubP1 1185.3606
13        17 initial           2 atubP1 1131.1118
14        17 initial           2 atubP1  892.7087
15        17 initial           2 atubP1 1028.5465
16        17 initial           3 atubP1  887.9972
17        17 initial           3 atubP1  732.9646
18        17 initial           3 atubP1  680.6724
</code></pre>

<p>Final</p>

<pre><code>   Cell.line  Time DNA.extract   Gene      CNPC
1          9 final           1 atubP1 1262.2378
2          9 final           1 atubP1 1261.9858
3          9 final           1 atubP1 1390.6873
4          9 final           2 atubP1 1539.7180
5          9 final           2 atubP1 1510.5405
6          9 final           2 atubP1 1443.1767
7          9 final           3 atubP1 1456.2050
8          9 final           3 atubP1 1578.6396
9          9 final           3 atubP1 1656.1822
10        17 final           1 atubP1 1462.5179
11        17 final           1 atubP1 1580.9956
12        17 final           1 atubP1 1255.9020
13        17 final           2 atubP1  886.7579
14        17 final           2 atubP1  581.8116
15        17 final           2 atubP1  722.0526
16        17 final           3 atubP1 4168.7895
17        17 final           3 atubP1 3266.2105
18        17 final           3 atubP1 4219.5645
</code></pre>
"
"0.0640622132638473","0.0664700094043992"," 22988","<p>I use lme4 in R to fit the mixed model</p>

<pre><code>lmer(value~status+(1|experiment)))
</code></pre>

<p>where value is continuous, status and experiment are factors, and I get</p>

<pre><code>Linear mixed model fit by REML 
Formula: value ~ status + (1 | experiment) 
  AIC   BIC logLik deviance REMLdev
 29.1 46.98 -9.548    5.911    19.1
Random effects:
 Groups     Name        Variance Std.Dev.
 experiment (Intercept) 0.065526 0.25598 
 Residual               0.053029 0.23028 
Number of obs: 264, groups: experiment, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.78004    0.08448   32.91
statusD      0.20493    0.03389    6.05
statusR      0.88690    0.03583   24.76

Correlation of Fixed Effects:
        (Intr) statsD
statusD -0.204       
statusR -0.193  0.476
</code></pre>

<p>How can I know that the effect of status is significant? R reports only $t$-values and not $p$-values.</p>
"
"0.152498570332605","0.148922608566615"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.123097967263292","0.127724638671999"," 24844","<p>I am running 3 models on 3 subsets of the same data.  The set up is as follows:</p>

<ol>
<li>Outcome (DV) is binary categorical</li>
<li>Time (IV) is repeated twice (pre and post)</li>
<li>Treatement (IV of interest) is binary categorical</li>
</ol>

<p>I am interested to know if at time 2 treatment has had an effect on outcome.  I used the lme4 package and used the following R code:</p>

<pre><code>tot.null&lt;-lmer(as.factor(outcome)~Time+(1|id), family=binomial(link='logit'),
             data=df.total)
tot.mod&lt;-lmer(as.factor(outcome)~trt*Time+(Time|id), 
             family=binomial(link='logit'), data=df.total)
anova(tot.null,tot.mod)
summary(tot.mod)
</code></pre>

<p><strong>Data head</strong></p>

<pre><code>   id             trt Time outcome
1   1 peer discussion   -1       1
2   2 peer discussion   -1       1
3   3 peer discussion   -1       0
4   4 peer discussion   -1       1
5   5 peer discussion   -1       1
</code></pre>

<p><strong>str of data</strong></p>

<pre><code>&gt; str(df.total)
'data.frame':   872 obs. of  4 variables:
 $ id     : int  1 2 3 4 5 6 7 8 9 10 ...
     $ trt    : Factor w/ 2 levels ""peer discussion"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Time   : num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
     $ outcome: num  1 1 1 1 1 1 1 0 1 0 ...
</code></pre>

<p>The problem is I get an error messoge on the <code>tot.mod</code>:</p>

<pre><code>&gt; tot.mod&lt;-glmer(as.factor(outcome)~trt*Time+(Time|id), family=binomial(link='logit'),
               data=df.total)
Warning message:
In mer_finalize(ans) : false convergence (8)
</code></pre>

<p>I think this is the reason the model is significant but none of the predictors are.  look at the inflated SEs.</p>

<p><strong>Comparison to the null model and the summary of full model</strong></p>

<pre><code>&gt; anova(tot.null,tot.mod)
Data: df.total
Models:
tot.null: as.factor(outcome) ~ Time + (1 | id)
tot.mod: as.factor(outcome) ~ trt * Time + (Time | id)
         Df    AIC    BIC  logLik  Chisq Chi Df            Pr(&gt;Chisq)    
tot.null  3 689.54 703.85 -341.77                                        
tot.mod   7 410.67 444.07 -198.34 286.86      4 &lt; 0.00000000000000022 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; summary(tot.mod)
Generalized linear mixed model fit by the Laplace approximation 
Formula: as.factor(outcome) ~ trt2 * Time + (Time | id) 
   Data: df.total 
   AIC   BIC logLik deviance
 410.7 444.1 -198.3    396.7
Random effects:
 Groups Name        Variance Std.Dev. Corr  
 id     (Intercept)  396.46  19.911         
        Time        1441.98  37.973   0.470 
Number of obs: 872, groups: id, 436

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) 10.09866    3.33921   3.024  0.00249 **
trt21        0.01792    5.10796   0.004  0.99720   
Time        -0.93753    5.79560  -0.162  0.87149   
trt21:Time  -0.84882   10.41073  -0.082  0.93502   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
           (Intr) trt21  Time  
trt21      -0.654              
Time        0.558 -0.365       
trt21:Time -0.311  0.473 -0.557
</code></pre>

<p>What's going on?  Why is the model significant but none of the betas?  In OLS I know this is an indicator of multi-colinearity among predictors.  I don't think that's the reason here.  Please help with understanding this problem as well as the error message (I think they may be connected).  What are some things I should check for?</p>

<p>The other two  models from the same data set (<code>split</code> on a different grouping variable) had no apparent problems.</p>

<p>Thank you in advance.</p>

<p><em>Using R 2.14.2, lme4 v. 0.999375-42 on a win 7 machine</em> </p>
"
"0.128505183463769","0.133335086600437"," 25371","<p>I am using the <code>glmer()</code> function from the lme4 package to run a GLMM using the poisson distribution.  In all the examples that I see, the random effects part of the output has a residual part that has been estimated from the data (surrounded by 2 asterisks on either side in the example below).  This information can then be used in interpreting the amount of variation explained by the random effect.  Here is an example:</p>

<pre><code>&gt; summary(M1)
Linear mixed model fit by REML 
Formula: Richness ~ NAP * fExp + (1 | fBeach) 
Data: RIKZ 
AIC   BIC    logLik   deviance REMLdev
236.5 247.3 -112.2    230.3    224.5
Random effects:
Groups   Name          Variance Std.Dev.
fBeach   (Intercept)   3.3072   1.8186  
**Residual             8.6605   2.9429**
Number of obs: 45, groups: fBeach, 9

Fixed effects:
              Estimate Std. Error t value
(Intercept)   8.8611     1.0208   8.681
NAP          -3.4637     0.6279  -5.517
fExp11       -5.2556     1.5451  -3.401
NAP:fExp11    2.0005     0.9461   2.114

Correlation of Fixed Effects:
           (Intr) NAP    fExp11
NAP        -0.181              
fExp11     -0.661  0.120       
NAP:fExp11  0.120 -0.664 -0.221
</code></pre>

<p>However, when I use my own data, I get output that does not include this information, and I am not sure why.  I want to know how much variation is explained by my random effects, but can't figure out how to access the information necessary to answer the question.  Any clues?  Is this a data/statistics issue or is this a knowing how to access the information issue?  I apologize if I'm asking in the wrong place.  The output I get looks similar to the following output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: y ~ z.score(x1) + z.score(x2) + z.score(x3) + z.score(x4) + z.score(x5) +      z.score(x6) + (1 | RE) 
Data: p 
AIC   BIC logLik deviance
419.5 454.7 -201.8    403.5
Random effects:
Groups Name        Variance Std.Dev.
RE     (Intercept) 0.021605 0.14699 
Number of obs: 600, groups: RE, 40

Fixed effects:
                  Estimate   Std. Error z value Pr(&gt;|z|)    
(Intercept)       1.70591    0.02911    58.60   &lt; 2e-16 ***
z.score(x1)       0.19087    0.03595    5.31    1.10e-07 ***
z.score(x2)      -0.14302    0.04083   -3.50    0.000460 ***
z.score(x3)      -0.16562    0.04020   -4.12    3.79e-05 ***
z.score(x4)       0.13229    0.03425    3.86    0.000112 ***
z.score(x5)      -0.10588    0.03985   -2.66    0.007885 ** 
z.score(x6)       0.17600    0.05798    3.04    0.002401 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) z.(x1) z.(x2 z.s(x3) z.(x4 z.(x5
z.scr(x1)  -0.051                                   
z.s(x2)     0.038  0.259                            
z.scr(x3)   0.045  0.156  0.113                     
z.(x4      -0.040  0.144 -0.052  0.044              
z.(x5       0.026 -0.368 -0.339 -0.072 -0.073       
z.scor(x6) -0.031 -0.020  0.002 -0.143 -0.004  0.004
</code></pre>

<p>Here is some sample data, to be fit with <code>glmer(y ~ x1 + (1|RE), data=d, family=poisson)</code>.</p>

<pre><code>d &lt;- data.frame(
  y  =  c(3, 5, 2, 6, 3, 7, 2, 3, 0, 4, 0,10, 1, 4, 0, 4, 2, 3, 0, 6, 
          3, 4, 2, 3, 2, 3, 3, 4, 0, 5, 6, 5, 4, 4, 0, 3, 1, 6, 0, 3, 2, 
          2, 1, 6, 2, 7, 0, 2, 0, 4, 0, 6, 4, 5, 1, 5, 1, 4, 1, 2, 3, 6, 
          6, 7, 0, 5, 0, 9, 1, 4, 5, 6, 1, 7, 1, 4, 1, 4, 0, 4, 1, 6, 1, 
          4, 0, 7, 1, 4, 0, 6, 0, 7, 2, 6, 0, 6, 1, 5, 0, 4, 1, 7, 2, 4, 
          1, 5, 1, 7, 2, 5, 0, 4, 3, 5, 1, 4, 0, 3, 0, 6, 0, 8, 3, 9, 0, 
          2, 3, 8, 0, 1, 0, 3, 0, 5, 0, 4, 4, 5, 0, 5, 1, 5, 3, 5, 1, 4, 
          3, 4, 4, 4, 4, 4, 4, 7, 1, 8, 1, 4, 0, 2, 2, 5, 1, 4, 1, 5, 1, 
          4, 2, 4, 2, 4, 0, 6, 1, 6, 0, 6, 1, 2, 1, 3, 1, 8, 1, 6, 1, 6, 
          0, 6, 1, 6, 2, 6, 2, 4, 0, 1, 1, 1, 1, 6, 5, 5, 1, 5, 2, 4, 2, 
          6, 1, 7, 1, 8, 2, 8, 1, 8, 2, 4, 1, 7, 3, 6, 4, 7, 3, 7, 1, 6, 
          3, 5, 1,10, 1, 7, 2, 5, 1, 5, 0, 6, 1, 8, 4, 7, 1, 6, 1, 9, 
          0, 9, 1, 3, 2, 5, 2, 9, 3, 5, 0, 2, 2, 3, 0, 5, 0, 5, 0, 4, 3, 
          6, 1,10, 2, 8, 0, 6, 0, 4, 2, 6, 2, 4, 2, 6, 1, 4, 0, 5, 2, 
          6, 1, 5, 2, 5, 1, 5, 1, 5),
  x1 = rep(c(0.1008, 0.0511, 0.1792, 1.0345), c(80, 80, 80, 60)),
  RE = rep(c(37, 88, 139, 190, 241, 292, 343, 394, 91, 142, 193, 244, 295, 
             346, 397, 40, 94, 145, 43, 196, 247, 298, 349, 400, 301, 352, 
             403, 250, 148, 199, 46, 97, 355, 406, 253, 304, 49, 100, 151, 
             202, 37, 88, 139, 190, 241, 292, 343, 394, 91, 142, 193, 244, 
             295, 346, 397, 40, 43, 94, 145, 247, 298, 349, 196, 400, 199, 
             250, 301, 352, 406, 46, 97, 148, 403, 49, 100, 151, 202, 253, 
             304, 355, 37, 88, 139, 190, 241, 292, 343, 394, 193, 244, 346, 
             397, 295, 40, 91, 142, 43, 94, 145, 196, 46, 97, 148, 151, 247, 
             400, 298, 349, 352, 199, 250, 301, 403, 253, 304, 355, 202, 406, 
             49, 100, 37, 88, 139, 190, 241, 292, 343, 394, 346, 397, 193, 
             244, 295, 40, 91, 142, 43, 94, 145, 196, 247, 298, 349, 400, 
             97, 148, 46, 199, 250, 301), each=2)
)
</code></pre>
"
"0.111517999203046","0.11570943428471"," 25714","<p>I've been using <code>nlme</code> and more recently <code>lmer</code> to fit multi-level models of time course data using orthogonal polynomials. My colleagues and I originally chose polynomials because we believed that ""nonlinear"" functions such as the logistic could not be used for multi-level modeling because they are not dynamically consistent. In at least one case this constraint is articulated very explicitly (Willett, 1997, p. 238-239):</p>

<blockquote>
  <p>In general, the individual growth modeling approach can accommodate any level-1 model that is <em>linear in the individual growth parameters</em>...Many common growth functions are dynamically consistent, including the quadratic model cited above and all other polynomial models, regardless of their order. Other potentially important individual growth models such as the logistic model (which provides an important theoretical representation of human development from the perspective of some psychological theories - see Fischer &amp; Pipp, 1984) is not linear in the individual growth parameters in its usual formulation.</p>
</blockquote>

<p>However, I recently discovered that, as I understand it, both <code>nlme</code> and <code>lmer</code> can use <code>SSfpl</code> to fit 4-parameter logistic functions in a multi-level modeling context. Did we misunderstand the dynamic consistency constraint? Perhaps <code>lmer</code> and/or <code>SSfpl</code> implements the 4-parameter logistic in a dynamically consistent way? If so, does anyone know how it is constrained to be dynamically consistent?</p>

<p>Thanks in advance.</p>
"
"0.0818727450060703","0.0849499549461966"," 25945","<p>Essentially, I have two collinear variables which could be seen as either random or as fixed effects, a dependent variable I'm fitting the model to, and a variable that's assuredly a random effect.</p>

<p><strong>Dependent var:</strong> Number of neuron spikes (<code>FiringRate</code>) in a specific region of mousebrain</p>

<p><strong>Fixed effects:</strong></p>

<p><strong>1)</strong> <code>Time</code> at which data sample was taken (on a linear scale in days -- so day two would be 2, day 5 would be 5, and so on)</p>

<p><strong>2)</strong> The <code>Age</code> of the mouse in days (so there's definitely collinearity between this and the <code>Time</code> variable, but there are enough mice of different ages to make this worthwhile as a separate variable)</p>

<p><strong>Random effect:</strong> <code>Subject</code> -- ""Name"" (ID number) of the mouse</p>

<p>Essentially, I'm wondering if it would be appropriate to run two LMEs. In the first, I'd treat <code>Age</code> and <code>Subject</code> as random variables in order to control for the effects of <code>Age</code> (and thus the collinearity between <code>Age</code> and <code>Time</code>) and see if Time is a significant predictor of the # of spikes (dependent variable). In the second, I'd enter <code>Time</code> and <code>Subject</code> as random variables to see if <code>Age</code> was a significant predictor.</p>

<pre><code>library(lme4)
a = lmer(FiringRate ~ Time + (1|Age) + (1|Subject))
b = lmer(FiringRate ~ Age + (1|Time) + (1|Subject))
</code></pre>
"
"0.105264957864947","0.10922137064511"," 26401","<p>Here are some simulated data:</p>

<pre><code>    library(mvtnorm)
    I &lt;- 3 # positions (fixed factor)
    J &lt;- 4 # tubes (random factor)
    K &lt;- 4 # repeats 
    n &lt;- I*J*K
    set.seed(123)
    tube &lt;- rep(1:J, each=I)
    position &lt;- rep(LETTERS[1:I], times=J)
    Mu_i &lt;- 3*(1:I)
    Mu_ij &lt;- c(t(rmvnorm(J, mean=Mu_i)) )  
    tube &lt;- rep(tube, each=K)
    position &lt;- rep(position, each=K)
    Mu_ij &lt;- rep(Mu_ij, each=K)
    dat &lt;- data.frame(tube, position, Mu_ij)
    sigmaw &lt;- 2
    dat$y &lt;- rnorm(n, dat$Mu_ij, sigmaw)
    dat$tube &lt;- factor(dat$tube)

&gt; str(dat)
'data.frame':   48 obs. of  4 variables:
 $ tube    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ...
     $ position: Factor w/ 3 levels ""A"",""B"",""C"": 1 1 1 1 2 2 2 2 3 3 ...
 $ Mu_ij   : num  2.44 2.44 2.44 2.44 6.13 ...
     $ y       : num  3.24 2.66 1.33 6.01 7.12 ...
&gt; head(dat)
  tube position    Mu_ij        y
1    1        A 2.439524 3.241067
2    1        A 2.439524 2.660890
3    1        A 2.439524 1.327842
4    1        A 2.439524 6.013351
5    1        B 6.129288 7.124989
6    1        B 6.129288 2.196053
</code></pre>

<p>I fit a mixed model with R, it works well:</p>

<pre><code>&gt; library(lme4)
&gt; lmer(y ~ position + (0+position|tube), data=dat)
Linear mixed model fit by REML 
Formula: y ~ position + (0 + position | tube) 
   Data: dat 
   AIC   BIC logLik deviance REMLdev
 212.6 231.3  -96.3    194.8   192.6
Random effects:
 Groups   Name      Variance Std.Dev. Corr          
 tube     positionA 0.30123  0.54885                
          positionB 0.68317  0.82654  -0.695        
          positionC 1.66666  1.29099  -0.408  0.940 
 Residual           3.14003  1.77201                
Number of obs: 48, groups: tube, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.3533     0.5211   6.435
positionB     3.1098     0.8923   3.485
positionC     5.6138     1.0144   5.534

Correlation of Fixed Effects:
          (Intr) postnB
positionB -0.753       
positionC -0.651  0.744
</code></pre>

<p>But the same model does not work well with SAS:</p>

<pre><code>PROC MIXED DATA=dat ;
CLASS POSITION TUBE ;
MODEL y = POSITION ;
RANDOM POSITION / subject=TUBE type=UN G GCORR ;
RUN; QUIT;
</code></pre>

<p>gives</p>

<pre><code> Estimated G matrix is not positive definite.
                         Estimated G Matrix

               Row    Effect      position    tube        Col1        Col2        Col3

                 1    position    A           1        0.08895     -0.5823     -0.1545
                 2    position    B           1        -0.5823      0.1455      1.2431
                 3    position    C           1        -0.1545      1.2431      1.4835
</code></pre>

<p>Is it possible to remedy this failure ?</p>
"
"0.0523065780965941","0.0542725354129257"," 27164","<p>I'm trying to fit a model under JAGS where I have a response, y, that depends linearly on eight predictors (with no intersect : I'm actually trying to evaluate the share of each socioeconomic statuts population that voted for a candidate, wth aggregated data). </p>

<p>My problem is that naive hierarchical modelling, under lmer() in R or in JAGS, gives me some negative estimates of my interest parameters, which is logically impossible. So I would like to constrain these parameters to take values between 0 and 1. Should I just cut off the posteriors ? I actually would like to integrate it in the model, but the problem is that since there are eight predictors, I use multivariate normal distribution, and I dont know how to use the dinterval() (or alternatively T() ?) distribution on it.</p>

<p>Here is the model : </p>

<pre><code>model{
   for (i in 1:n) {
    y[i] ~ dnorm(y.hat[i], tau.y)
    y.hat[i] &lt;- inprod(B[dpt[i],], X[i,])
   }

   tau.y &lt;- pow(sigma.y, -2)
   sigma.y ~ dunif(0, 100)

   for (j in 1:J) {
    for (k in 1:K) {
      B[j,k] &lt;- xi[k] * B.raw[j,k]
    }
    B.raw[j,1:K] ~ dmnorm(mu.raw[], Tau.B.raw[,])
   }

   for (k in 1:K) {
    is.interval[k] ~ dinterval(mu[k], lim)
    mu[k] &lt;- xi[k]*mu.raw[k]
    mu.raw[k] ~ dnorm(0, 0.0001)
    xi[k] ~ dunif(0,100)
   }

   Tau.B.raw[1:K,1:K] ~ dwish(W[,], df)
   df &lt;- K+1
   Sigma.B.raw[1:K,1:K] &lt;- inverse(Tau.B.raw[,])
   for (k in 1:K) {
    for (k.prime in 1:K) {
      rho.B[k, k.prime] &lt;- Sigma.B.raw[k, k.prime]/
        sqrt(Sigma.B.raw[k,k]*Sigma.B.raw[k.prime, k.prime])
    }
    sigma.B[k] &lt;- abs(xi[k])*sqrt(Sigma.B.raw[k,k])
    }
    }
</code></pre>

<p>with n, J, K(=8), y, dpt, X, W, lim, and is.interval provided as data. My main parameter of interest is B.</p>

<p>Thanks you very much for any help here !</p>
"
"0.104613156193188","0.09497693697262"," 29500","<p>Following grafts are taken from <a href=""http://personal.bgsu.edu/~jshang/AICb_assumption.pdf"">this article </a>. I'm newbie to bootstrap and trying to implement the parametric, semiparametric and nonparametric bootstrapping bootstrapping for linear mixed model with <code>R boot</code> package.</p>

<p><img src=""http://i.stack.imgur.com/b24bi.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/2fTO0.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/zANep.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/VfGOQ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/Y2Ng0.png"" alt=""enter image description here""></p>

<p><strong>R Code</strong></p>

<p>Here is my <code>R</code> code:</p>

<pre><code>library(SASmixed)
library(lme4)
library(boot)

fm1Cult &lt;- lmer(drywt ~ Inoc + Cult + (1|Block) + (1|Cult), data=Cultivation)
fixef(fm1Cult)


boot.fn &lt;- function(data, indices){
 data &lt;- data[indices, ]
 mod &lt;- lmer(drywt ~ Inoc + Cult + (1|Block) + (1|Cult), data=data)
 fixef(mod)
 }

set.seed(12345)
Out &lt;- boot(data=Cultivation, statistic=boot.fn, R=99)
Out
</code></pre>

<p><strong>Questions</strong></p>

<ol>
<li>How to do parametric, semiparametric and nonparametric bootstrapping for mixed models with <code>boot</code> package?</li>
<li>I guess I'm doing nonparametric bootstrapping for mixed model in my code.</li>
</ol>

<p>I found <a href=""http://www.r-project.org/conferences/useR-2009/slides/SanchezEspigares+Ocana.pdf"">these slides</a> but could not get the R package <code>merBoot</code>. Any idea where I can get this package. Any help will be highly appreciated. Thanks in advance for your help and time.</p>
"
"0.0986302295273746","0.102337274193778"," 30797","<p>I would like to get the posterior simulations of the variance components of a lmer() model with the mcmcsamp() function. How to do ?</p>

<p>For instance, below is the result of a lmer() fitting :</p>

<pre><code>&gt; fit
Linear mixed model fit by REML
Formula: y ~ 1 + (1 | Part) + (1 | Operator) + (1 | Part:Operator)
   Data: dat
   AIC   BIC logLik deviance REMLdev
 97.55 103.6 -43.78    89.18   87.55
Random effects:
 Groups        Name        Variance Std.Dev.
 Part:Operator (Intercept) 2.25724  1.50241
 Part          (Intercept) 3.30398  1.81769
 Operator      (Intercept) 0.00000  0.00000
 Residual                  0.42305  0.65043
Number of obs: 25, groups: Part:Operator, 15; Part, 5; Operator, 3
</code></pre>

<p>Now I run mcmcsamp() :</p>

<pre><code>&gt; mm &lt;- mcmcsamp(fit, n=15000) 
</code></pre>

<p>I expected that the simulations of the residual variance are stored in the ""sigma"" node but this does not seem to fit the results of lmer() :</p>

<pre><code>&gt; sigmasims &lt;- mm@sigma[1,-(1:5000)]  # discard first 5000 simulations (burn-in)
&gt; summary(sigmasims)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.8647  1.4960  1.7040  1.7460  1.9480  3.7920 
</code></pre>

<p>Similarly I expected that the simulations of the other variance components are stored in the ""ST"" node but I get a similar observation.</p>
"
"0.184931680363827","0.176531797984267"," 31118","<p>I performed an experiment where I raised different families coming from two different source populations, where each family was split up into a different treatments. After the experiment I measured several traits on each individual. 
To test for an effect of either treatment or source as well as their interaction, I used a linear mixed effect model with family as random factor, i.e.</p>

<pre><code>lme(fixed=Trait~Treatment*Source,random=~1|Family,method=""ML"")
</code></pre>

<p>so far so good,
Now I have to calculate the relative variance components, i.e. the percentage of variation that is explained by either treatment or source as well as the interaction.</p>

<p>Without a random effect, I could easily use the sums of squares (SS) to calculate the variance explained by each factor. But for a mixed model (with ML estimation), there are no SS, hence I thought I could use Treatment and Source as random effects too to estimate the variance, i.e.</p>

<pre><code>lme(fixed=Trait~1,random=~(Treatment*Source)|Family, method=""REML"")
</code></pre>

<p>However, in some cases, lme does not converge, hence I used lmer from the lme4 package:</p>

<pre><code>lmer(Trait~1+(Treatment*Source|Family),data=DATA)
</code></pre>

<p>Where I extract the variances from the model using the summary function:</p>

<pre><code>model&lt;-lmer(Trait~1+(Treatment*Source|Family),data=regrexpdat)
results&lt;-model@REmat
variances&lt;-results[,3]
</code></pre>

<p>I get the same values as with the VarCorr function. I use then these values to calculate the actual percentage of variation taking the sum as the total variation.</p>

<p>Where I am struggling is with the interpretation of the results from the initial lme model (with treatment and source as fixed effects) and the random model to estimate the variance components (with treatment and source as random effect). I find in most cases that the percentage of variance explained by each factor does not correspond to the significance of the fixed effect.</p>

<p>For example for the trait HD,
The initial lme suggests a tendency for the interaction as well as a significance for Treatment. Using a backward procedure, I find that Treatment has a close to significant tendency. However, estimating variance components, I find that Source has the highest variance, making up to 26.7% of the total variance.</p>

<p>The lme:</p>

<pre><code>anova(lme(fixed=HD~as.factor(Treatment)*as.factor(Source),random=~1|as.factor(Family),method=""ML"",data=test),type=""m"")
                                      numDF denDF  F-value p-value
(Intercept)                                1   426 0.044523  0.8330
as.factor(Treatment)                       1   426 5.935189  0.0153
as.factor(Source)                          1    11 0.042662  0.8401
as.factor(Treatment):as.factor(Source)     1   426 3.754112  0.0533
</code></pre>

<p>And the lmer:</p>

<pre><code>summary(lmer(HD~1+(as.factor(Treatment)*as.factor(Source)|Family),data=regrexpdat))
Linear mixed model fit by REML 
Formula: HD ~ 1 + (as.factor(Treatment) * as.factor(Source) | Family) 
   Data: regrexpdat 
    AIC    BIC logLik deviance REMLdev
 -103.5 -54.43  63.75   -132.5  -127.5
Random effects:
 Groups   Name                                      Variance  Std.Dev. Corr                 
 Family   (Intercept)                               0.0113276 0.106431                      
          as.factor(Treatment)                      0.0063710 0.079819  0.405               
          as.factor(Source)                         0.0235294 0.153393 -0.134 -0.157        
          as.factor(Treatment)L:as.factor(Source)   0.0076353 0.087380 -0.578 -0.589 -0.585 
 Residual                                           0.0394610 0.198648                      
Number of obs: 441, groups: Family, 13

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.02740    0.03237  -0.846
</code></pre>

<p>Hence my question is, is it correct what I am doing? Or should I use another way to estimate the amount of variance explained by each factor (i.e. Treatment, Source and their interaction). For example, would the effect sizes be a more appropriate way to go?</p>

<p>Thanks!</p>

<p>Kay Lucek</p>
"
"0.110959008218296","0.102337274193778"," 31204","<p>I want to model two different time variables, some of which are heavily collinear in my data (age + cohort = period). Doing this I ran into some trouble with <code>lmer</code> and and interactions of <code>poly()</code>, but it's probably not limited to <code>lmer</code>, I got the same results with <code>nlme</code> IIRC.</p>

<p>Obviously, my understanding of what the poly() function does is lacking. I understand what <code>poly(x,d,raw=T)</code> does and I thought without <code>raw=T</code> it makes orthogonal polynomials (I can't say I really understand what that means), which makes fitting easier, but doesn't let you interpret the coefficients directly.<br>
I <a href=""http://r.789695.n4.nabble.com/use-of-poly-td847784.html"">read</a> that because I'm using the predict function, the predictions should be the same.</p>

<p>But they aren't, even when the models converge normally. I'm using centered variables and I first thought that maybe the orthogonal polynomial leads to higher fixed effect correlation with the collinear interaction term, but it seems comparable. I've pasted two <a href=""https://gist.github.com/3002722"">model summaries over here</a>.</p>

<p>These plots hopefully illustrate the extent of the difference. I used the predict-function which is only available in the dev. version of lme4 (heard about it <a href=""http://stats.stackexchange.com/a/29749/2795"">here</a>), but the fixed effects are the same in the CRAN version (and they also seem off by themselves, e.g. ~ 5 for the interaction when my DV has a range of 0-4).</p>

<p>The lmer call was</p>

<pre><code>cohort2_age =lmer(churchattendance ~ 
poly(cohort_c,2,raw=T) * age_c + 
ctd_c + dropoutalive + obs_c + (1+ age_c |PERSNR), data=long.kg)
</code></pre>

<p>The prediction was fixed effects only, on fake data (all other predictors=0) where I marked the range present in the original data as extrapolation=F.</p>

<pre><code>predict(cohort2_age,REform=NA,newdata=cohort.moderates.age)
</code></pre>

<p>I can provide more context if need be (I didn't manage to produce a reproducible example easily, but can of course try harder), but I think this is a more basic plea: explain the <code>poly()</code> function to me, pretty please.</p>

<h3>Raw polynomials</h3>

<p><img src=""http://i.stack.imgur.com/2FIIO.jpg"" alt=""Raw polynomials""></p>

<h3>Orthogonal polynomials (clipped, nonclipped at <a href=""http://i.imgur.com/iFmLE.png"">Imgur</a>)</h3>

<p><img src=""http://i.stack.imgur.com/CvVdC.jpg"" alt=""Orthogonal polynomials""></p>
"
"0.0739726721455309","0.0767529556453336"," 31569","<p>I recently measured how the meaning of a new word is acquired over repeated exposures (practice: day 1 to day 10) by measuring ERPs (EEGs) when the word was viewed in different contexts. I also controlled properties of the context, for instance, its usefulness for the discovery of new word meaning (high vs. low). I am particularly interested in the effect of practice (days). Because individual ERP recordings are noisy, ERP component values are  obtained by averaging over the trials of a particular condition. With the <code>lmer</code> function, I applied the following formula:</p>

<pre><code>lmer(ERPindex ~ practice*context + (1|participants), data=base) 
</code></pre>

<p>and </p>

<pre><code>lmer(ERPindex ~ practice*context + (1+practice|participants), data=base) 
</code></pre>

<p>I've also seen the equivalent of the following random effects in the literature:</p>

<pre><code>lmer(ERPindex ~ practice*context + (practice|participants) + 
                (practice|participants:context), data=base) 
</code></pre>

<p>What is accomplished by using a random factor of the form <code>participants:context</code>? Is there a good source that would allow someone with just cursory knowledge of matrix algebra understand precisely what random factors do in linear mixed models and how they should be selected?</p>
"
"0.177534413149274","0.168856502419734"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.123097967263292","0.106437198893332"," 32994","<p>I've been using the <code>MCMCglmm</code> package recently. I am confused by what is referred to in the documentation as R-structure and G-structure. These seem to relate to the random effects - in particular specifying the parameters for the prior distribution on them, but the discussion in the documentation seems to assume that the reader knows what these terms are. For example:</p>

<blockquote>
  <p>optional list of prior specifications having 3 possible elements: R (R-structure) G (G-structure) and B (fixed effects)............ The priors for the variance structures (R and G) are lists with the expected (co)variances (V) and degree of belief parameter (nu) for the inverse-Wishart</p>
</blockquote>

<p>...taken from from <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=MCMCglmm%3aMCMCglmm"">here</a>.</p>

<p><strong>EDIT: Please note that I have re-written the rest of the question following the comments from Stephane.</strong></p>

<p>Can anyone shed light on what R-structure and G-structure are, in the context of a simple variance components model where the linear predictor is 
$$\beta_0 + e_{0ij} + u_{0j} $$
with $e_{0ij} \sim N(0,\sigma_{0e}^2)$ and $u_{0j} \sim N(0,\sigma_{0u}^2)$</p>

<p>I made the following example with some data that comes with <code>MCMCglmm</code></p>

<pre><code>&gt; require(MCMCglmm)
&gt; require(lme4)
&gt; data(PlodiaRB)
&gt; prior1 = list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 0.002)))
&gt; m1 &lt;- MCMCglmm(Pupated ~1, random = ~FSfamily, family = ""categorical"", 
+ data = PlodiaRB, prior = prior1, verbose = FALSE)
&gt; summary(m1)


 G-structure:  ~FSfamily

         post.mean l-95% CI u-95% CI eff.samp
FSfamily    0.8529   0.2951    1.455      160

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units         1        1        1        0

 Location effects: Pupated ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)   -1.1630  -1.4558  -0.8119    463.1 &lt;0.001 ***
---

&gt; prior2 = list(R = list(V = 1, nu = 0), G = list(G1 = list(V = 1, nu = 0.002)))
&gt; m2 &lt;- MCMCglmm(Pupated ~1, random = ~FSfamily, family = ""categorical"", 
+ data = PlodiaRB, prior = prior2, verbose = FALSE)
&gt; summary(m2)


 G-structure:  ~FSfamily

         post.mean l-95% CI u-95% CI eff.samp
FSfamily    0.8325   0.3101    1.438    79.25

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units    0.7212  0.04808    2.427    3.125

 Location effects: Pupated ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)   -1.1042  -1.5191  -0.7078    20.99 &lt;0.001 ***
---

&gt; m2 &lt;- glmer(Pupated ~ 1+ (1|FSfamily), family=""binomial"",data=PlodiaRB)
&gt; summary(m2)
Generalized linear mixed model fit by the Laplace approximation 
Formula: Pupated ~ 1 + (1 | FSfamily) 
   Data: PlodiaRB 
  AIC  BIC logLik deviance
 1020 1029   -508     1016
Random effects:
 Groups   Name        Variance Std.Dev.
 FSfamily (Intercept) 0.56023  0.74849 
Number of obs: 874, groups: FSfamily, 49

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9861     0.1344  -7.336  2.2e-13 ***
</code></pre>

<p>So based on the comments from Stephane I think the G structure is for $\sigma_{0u}^2$. But the comments also say that the R structure is for $\sigma_{0e}^2$ yet this does not seem to appear in the <code>lme4</code> output.</p>

<p>Note that the results from <code>lme4/glmer()</code> are consistent with both examples from MCMC <code>MCMCglmm</code>.</p>

<p>So, is the R structure for $\sigma_{0e}^2$ and why doesn't this appear in the output for <code>lme4/glmer()</code> ?</p>
"
"0.110959008218296","0.0895451149195559"," 34969","<p>Sorry if I'm missing something very obvious here but I am new to mixed effect modelling. </p>

<p>I am trying to model a binomial presence/absence response as a function of percentages of habitat within the surrounding area. My fixed effect is the percentage of the habitat and my random effect is the site (I mapped 3 different farm sites). </p>

<pre><code>glmmsetaside &lt;- glmer(treat~setas+(1|farm),
       family=binomial,data=territory)
</code></pre>

<p>When <code>verbose=TRUE</code>:</p>

<pre><code>0:     101.32427: 0.333333 -0.0485387 0.138083 
1:     99.797113: 0.000000 -0.0531503 0.148455  
2:     99.797093: 0.000000 -0.0520462 0.148285  
3:     99.797079: 0.000000 -0.0522062 0.147179  
4:     99.797051: 7.27111e-007 -0.0508770 0.145384  
5:     99.797012: 1.45988e-006 -0.0495767 0.141109  
6:     99.797006: 0.000000 -0.0481233 0.136883  
7:     99.797005: 0.000000 -0.0485380 0.138081  
8:     99.797005: 0.000000 -0.0485387 0.138083  
</code></pre>

<p>My output is this:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: treat ~ setasidetrans + (1 | farm) 

AIC   BIC logLik deviance
105.8 112.6  -49.9     99.8
Random effects:
 Groups Name        Variance Std.Dev.
farm   (Intercept)  0        0  
Number of obs: 72, groups: farm, 3

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.04854    0.44848  -0.108    0.914
setasidetrans  0.13800    1.08539   0.127    0.899

Correlation of Fixed Effects:
            (Intr)
setasidtrns -0.851
</code></pre>

<p>I basically do not understand why my random effect is 0? Is it because the random effect only has 3 levels? I don't see why this would be the case. I have tried it with lots of different models and it always comes out as 0.</p>

<p>It cant be because the random effect doesn't explain any of the variation because I know the habitats are different in the different farms.</p>

<p>Here is an example set of data using <code>dput</code>:</p>

<pre><code>list(territory = c(1, 7, 8, 9, 10, 11, 12, 13, 14, 2, 3, 4, 5, 
6, 15, 21, 22, 23, 24, 25, 26, 27, 28, 16, 17, 18, 19, 20, 29, 
33, 34, 35, 36, 37, 38, 39, 40, 30, 31, 32, 41, 45, 46, 47, 48, 
49, 50, 51, 52, 42, 43, 44, 53, 55, 56, 57, 58, 59, 60, 61, 62, 
54, 63, 65, 66, 67, 68, 69, 70, 71, 72, 64), treat = c(1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0), farm = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3), 
built = c(5.202332763, 1.445026852, 2.613422283, 2.261705833, 
2.168842186, 1.267473928, 0, 0, 0, 9.362387965, 17.55433115, 
4.58020626, 4.739300829, 8.638442377, 0, 1.220760647, 7.979990338, 
13.30789514, 0, 8.685544976, 3.71617163, 0, 0, 6.802926951, 
8.925512803, 8.834006678, 4.687723044, 9.878232478, 8.097800267, 
0, 0, 0, 0, 5.639651095, 9.381654651, 8.801754791, 5.692392532, 
3.865304919, 4.493438554, 4.826277798, 3.650995554, 8.20818417, 
0, 8.169597157, 8.62030666, 8.159474015, 8.608979238, 0, 
8.588288678, 7.185700856, 0, 0, 3.089524893, 3.840381223, 
31.98103158, 5.735501995, 5.297691011, 5.17141191, 6.007539933, 
2.703345394, 4.298077606, 1.469986793, 0, 4.258511595, 0, 
21.07029581, 6.737664009, 14.36176373, 3.056631919, 0, 32.49289428, 
0)
</code></pre>

<p>It goes on with around 10 more columns for different types of habitat (like <code>built</code>, <code>setaside</code> is one of them) with percentages in it.</p>
"
"0.0369863360727655","0.0383764778226668"," 36276","<p>I am current using the package (lme4) for a mixed effect model with random effects.</p>

<p>My model takes the form:</p>

<pre><code>mod &lt;- lmer(response ~ b1(predict1, bp) + b2(predict2, bp) + (1 | site), data = data)
</code></pre>

<p>The functions b1 and b2 come from the helpful advice in:</p>

<p><a href=""http://stats.stackexchange.com/questions/19772/estimating-the-break-point-in-a-broken-stick-piecewise-linear-model-with-rando"">Estimating the break point in a broken stick / piecewise linear model with random effects in R [code and output included]</a></p>

<pre><code>b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)
</code></pre>

<p>I am wondering, how would I set up my model so that the second part (line) of the broken stick model has a constant gradient? </p>

<p>So for example, I would like something like:</p>

<p>If X &lt; breakpoint, Y = m1X + C1</p>

<p>If X > breakpoint, Y = C2</p>

<p>Hope I am making sense.</p>

<p>Matt.</p>
"
"0.0905976508333704","0.0783356573256404"," 37805","<p>I have a GLMM of the form: </p>

<pre><code>lmer(present? ~ factor1 + factor2 + continuous + factor1*continuous + 
                (1 | factor3), family=binomial)
</code></pre>

<p>When I use <code>drop1(model, test=""Chi"")</code>, I get different results than if I use <code>Anova(model, type=""III"")</code> from the car package or <code>summary(model)</code>. These latter two give the same answers. </p>

<p>Using a bunch of fabricated data, I have found that these two methods normally do not differ. They give the same answer for balanced linear models, unbalanced linear models (where unequal n in different groups), and for balanced generalised linear models, but not for balanced generalised linear mixed models. So it appears that only in cases where random factors are included does this discord manifest.</p>

<ul>
<li>Why is there a discrepancy between these two methods?  </li>
<li>When using GLMM should <code>Anova()</code> or <code>drop1()</code> be used?  </li>
<li>The difference between these two is rather slight, at least for my data. Does it even matter which is used?</li>
</ul>
"
"0.128124426527695","0.132940018808798"," 37944","<p>I am currently using the R package <a href=""http://cran.r-project.org/web/packages/lme4/lme4.pdf"">lme4</a>.</p>

<p>I am using a linear mixed effects models with random effects:</p>

<pre><code>library(lme4)
mod1 &lt;- lmer(r1 ~ (1 | site), data = sample_set) #Only random effects
mod2 &lt;- lmer(r1 ~ p1 + (1 | site), data = sample_set) #One fixed effect + 
            # random effects
mod3 &lt;- lmer(r1 ~ p1 + p2 + (1 | site), data = sample_set) #Two fixed effects + 
            # random effects
</code></pre>

<p>To compare models, I am using the <code>anova</code> function and looking at differences in AIC relative to the lowest AIC model:</p>

<pre><code>anova(mod1, mod2, mod3)
</code></pre>

<p>The above is fine for comparing models. </p>

<p>However, I also need some simple way to interpret goodness of fit measures for each model. Does anyone have experience with such measures? I have done some research, and there are journal papers on R squared for the fixed effects of mixed effects models:</p>

<ul>
<li>Cheng, J., Edwards, L. J., Maldonado-Molina, M. M., Komro, K. A., &amp; Muller, K. E. (2010). Real longitudinal data analysis for real people: Building a good enough mixed model. Statistics in Medicine, 29(4), 504-520. doi: 10.1002/sim.3775  </li>
<li>Edwards, L. J., Muller, K. E., Wolfinger, R. D., Qaqish, B. F., &amp; Schabenberger, O. (2008). An R2 statistic for fixed effects in the linear mixed model. Statistics in Medicine, 27(29), 6137-6157. doi: 10.1002/sim.3429  </li>
</ul>

<p>It seems however, that there is some criticism surrounding the use of measures such as those proposed in the above papers.</p>

<p>Could someone please suggest a few easy to interpret, goodness of fit measures that could apply to my models?  </p>
"
"0.12266979912335","0.127280377713181"," 38177","<p>I have a GLMM with Poisson distribution and random spatial block. My experimental design is 2x2 factorial, with 4 blocks, resulting in 16 total data points. Here is the specification of the model in R using the lme4 package.</p>

<pre><code>lmer(rich ~ morph*caged + (1|block), family=poisson, data=bexData)
</code></pre>

<p>When I call summary on this object, I am returned</p>

<pre><code>   AIC   BIC logLik deviance
 18.58 22.44 -4.288    8.576
Random effects:
 Groups Name        Variance Std.Dev.
 block  (Intercept)  0        0      
Number of obs: 16, groups: block, 4
</code></pre>

<p>I have left out the fixed effect parameter tests and correlations for brevity.</p>

<p>Here are my primary questions:</p>

<ol>
<li><p>Can you use this output to calculate overdispersion?  </p>

<ul>
<li>I have read that overdispersion can be calculated as the residual deviance divided by the residual degrees of freedom. Is that 8.576 / (16 - 4)? (Zuur et al., Mixed Effects Models)</li>
</ul></li>
<li>If this calculation is correct, the estimator phi = 0.715. This indicates that there is not overdispersion in my data. 
<ul>
<li>Does this indicate that there is underdispersion? </li>
<li>Is this a problem? </li>
<li>Can anybody offer advice as to thresholds for over/underdispersion at which corrections to the models should be made? Zuur has said in one book that 5 is a common cutoff. Do people agree with that?</li>
<li>How can such corrections be made?</li>
</ul></li>
<li>I've also noticed here that the variance for the random effect is 0. 
<ul>
<li>Does this mean that there are precisely <em>no</em> error correlations between data points within my blocking factor?</li>
<li>If this is so, why would a generalised linear model of the form shown at bottom have an AIC substantially higher, around 55?</li>
<li>is AIC a reasonable method for choosing GLMM over GLM (as suggested by Zuur)?</li>
</ul></li>
</ol>

<p>.</p>

<pre><code>glm(rich ~ morph*caged, data=bexData, family=poisson)
</code></pre>
"
"0.0640622132638473","0.0664700094043992"," 38188","<p>I'm a new user of mixed models and through the material I've been reading there are always probability values (p>t) or (p>z) that estimate the importance of a level of a factor in the model. However, when using the <code>lmer()</code> function in R, which supposedly gives you those probabilities, I simply don't find them. Here is the output: </p>

<pre><code>Linear mixed model fit by REML 
Formula: Temp ~ depth + (1 | locality) 
   Data: qminmatrix 
   AIC   BIC logLik deviance REMLdev
 561.3 581.3 -273.7    551.5   547.3
Random effects:
 Groups   Name        Variance Std.Dev.
 locality (Intercept) 4.7998   2.1909  
 Residual             4.0433   2.0108  
Number of obs: 128, groups: locality, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.0103     1.1500  19.140
depth1        1.9564     0.6832   2.864
depth10       2.6624     0.5756   4.625
depth5        3.0209     0.4932   6.125
depthWS      -2.2585     0.5444  -4.149

Correlation of Fixed Effects:
        (Intr) depth1 dpth10 depth5
depth1  -0.157                     
depth10 -0.175  0.189              
depth5  -0.213  0.313  0.458       
depthWS -0.191  0.334  0.373  0.441
</code></pre>
"
"0.110959008218296","0.115129433468"," 38524","<p>I got completely different results from lmer() and lme()! Just look at the coefficients' std.errors. Completely different in both cases. Why is that and which model is correct?</p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai)
&gt; mix1d = lme(logInd ~ 0 + crit_i + Year:crit_i, random = ~ 1 + Year|Taxon, data = datai)
&gt; 
&gt; summary(mix1d)
Linear mixed-effects model fit by REML
 Data: datai 
       AIC      BIC    logLik
  4727.606 4799.598 -2351.803

Random effects:
 Formula: ~1 + Year | Taxon
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev       Corr  
(Intercept) 9.829727e-08 (Intr)
Year        3.248182e-04 0.619 
Residual    4.933979e-01       

Fixed effects: logInd ~ 0 + crit_i + Year:crit_i 
                 Value Std.Error   DF   t-value p-value
crit_iA      29.053940  4.660176   99  6.234515  0.0000
crit_iF       0.184840  3.188341   99  0.057974  0.9539
crit_iU      12.340580  5.464541   99  2.258301  0.0261
crit_iW       5.324854  5.152019   99  1.033547  0.3039
crit_iA:Year -0.012272  0.002336 2881 -5.253846  0.0000
crit_iF:Year  0.002237  0.001598 2881  1.399542  0.1618
crit_iU:Year -0.003870  0.002739 2881 -1.412988  0.1578
crit_iW:Year -0.000305  0.002582 2881 -0.118278  0.9059
 Correlation: 
             crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF       0                                              
crit_iU       0      0                                       
crit_iW       0      0      0                                
crit_iA:Year -1      0      0      0                         
crit_iF:Year  0     -1      0      0      0                  
crit_iU:Year  0      0     -1      0      0      0           
crit_iW:Year  0      0      0     -1      0      0      0    

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-6.98370498 -0.39653580  0.02349353  0.43356564  5.15742550 

Number of Observations: 2987
Number of Groups: 103 
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 2961 3033  -1469     2893    2937
Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 Taxon    (Intercept) 6.9112e+03 83.13360        
          Year        1.7582e-03  0.04193 -1.000 
 Residual             1.2239e-01  0.34985        
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403 18.0295239   1.611
crit_iF       0.1848404 12.3352135   0.015
crit_iU      12.3405800 21.1414908   0.584
crit_iW       5.3248537 19.9323887   0.267
crit_iA:Year -0.0122717  0.0090916  -1.350
crit_iF:Year  0.0022365  0.0062202   0.360
crit_iU:Year -0.0038701  0.0106608  -0.363
crit_iW:Year -0.0003054  0.0100511  -0.030

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -1.000  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -1.000  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -1.000  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -1.000  0.000  0.000  0.000
&gt; 
</code></pre>
"
"0.116961064294386","0.121357078494567"," 38591","<p>I'm trying to compute a mixed model using jags (R2jags) and I got very strange divergence. The chains started so well, very well according to the results expected (also see <code>lmer</code> output of the same model below). But at certain point, the chains went crazy. Just look at the traceplot for <strong>delta_tau</strong> variable - the chains start so well, but then the green chain goes crazy, then blue and finally red... </p>

<p>Any ideas why this happens? Can't be in initial values, because the chains started so well. Maybe the priors? Why is the system unstable?</p>

<p><img src=""http://i.stack.imgur.com/rn08d.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/v2jof.png"" alt=""enter image description here""></p>

<p><strong>EDIT:</strong> Variables <code>gamma_tau</code> and <code>delta_tau</code> don't fall to exact zero, as you can see on these zoomed-in figures:</p>

<p><img src=""http://i.stack.imgur.com/qkORJ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/gdEpz.png"" alt=""enter image description here""></p>

<p>This is the jags model:</p>

<pre><code>model {

# likelihood
for (i in 1:N) {
    logInd[i] ~ dnorm(mu[i], eps_tau)
    mu[i] &lt;- alpha[crit[i]] + (beta[crit[i]] + delta[species[i]])*year[i] + gamma[species[i]] # ekviv mix1b/c podle me
}

# priors
eps_tau ~ dgamma(1.0E-3, 1.0E-3) 

for (j in 1:no_crit) {
    alpha[j] ~ dnorm(0, 0.0001)
    beta[j] ~ dnorm(0, 0.0001) 
}

for (k in 1:no_species) {
    gamma[k] ~ dnorm(0, gamma_tau)
    delta[k] ~ dnorm(0, delta_tau)
}

gamma_tau ~ dgamma(1.0E-3, 1.0E-3) 
delta_tau ~ dgamma(1.0E-3, 1.0E-3)
}
</code></pre>

<p>Code used to run jags (using R2jags):</p>

<pre><code>no_crit = length(levels(crit))

win.data = list(logInd = mydata$logInd, crit = (as.integer(crit)), 
    	year = mydata$Year, species = (as.integer(mydata$Taxon)),
    	N = nrow(mydata), no_crit = no_crit, no_species = length(levels(mydata$Taxon))
)

inits = function () { list(
    alpha = rnorm(no_crit, 0, 10000),
    beta = rnorm(no_crit, 0, 10000)
)}  

params = c(""alpha"", ""beta"", ""eps_tau"", ""gamma_tau"", ""delta_tau"")

# ni: 1000 -&gt; .. sec
ni &lt;- 20000
nt &lt;- 8
nb &lt;- 8000
nc &lt;- 3

out &lt;- R2jags::jags(win.data, inits, params, ""model.txt"",
    nc, ni, nb, nt,  
    working.directory = paste(getwd(), ""/tmp_bugs/"", sep = """")
)
R2jags::traceplot(out, mfrow = c(4, 2))
</code></pre>

<p>Here is output from the equivalent <code>lmer</code> model:</p>

<pre><code>&gt; summary(lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai2))
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai2 
  AIC  BIC logLik deviance REMLdev
 8558 8630  -4267     8495    8534
Random effects:
 Groups   Name        Variance   Std.Dev.   Corr  
 Taxon    (Intercept) 1.1682e-12 1.0808e-06       
          Year        5.3860e-07 7.3389e-04 0.000 
 Residual             8.7038e-01 9.3294e-01       
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403  8.8116915   3.297
crit_iF       0.1848404  6.0286726   0.031
crit_iU      12.3405800 10.3326242   1.194
crit_iW       5.3248537  9.7416915   0.547
crit_iA:Year -0.0122717  0.0044174  -2.778
crit_iF:Year  0.0022365  0.0030222   0.740
crit_iU:Year -0.0038701  0.0051799  -0.747
crit_iW:Year -0.0003054  0.0048836  -0.063

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -0.999  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -0.999  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -0.999  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -0.999  0.000  0.000  0.000
</code></pre>

<p>Thanks in advance!</p>
"
"0.138390197576366","0.143591631723548"," 40459","<p>I conducted a computer-based assessment of different methods of fitting a particular type of model used in the palaeo sciences. I had a large-ish training set and so I randomly (stratified random sampling) set aside a test set. I fitted $m$ different methods to the training set samples and using the $m$ resulting models I predicted the response for the test set samples and computed a RMSEP over the samples in the test set. This is a single <strong>run</strong>.</p>

<p>I then repeated this process a large number of times, each time I chose a different training set by randomly sampling a new test set.</p>

<p>Having done this I want to investigate if any of the $m$ methods has better or worse RMSEP performance. I also would like to do multiple comparisons of the pair-wise methods.</p>

<p>My approach has been to fit a linear mixed effects (LME) model, with a single random effect for <strong>Run</strong>. I used <code>lmer()</code> from the <strong>lme4</strong> package to fit my model and functions from the <strong>multcomp</strong> package to perform the multiple comparisons. My model was essentially</p>

<pre><code>lmer(RMSEP ~ method + (1 | Run), data = FOO)
</code></pre>

<p>where <code>method</code> is a factor indicating which method was used to generate the model predictions for the test set and <code>Run</code> is an indicator for each particular <strong>Run</strong> of my ""experiment"".</p>

<p>My question is in regard to the residuals of the LME. Given the single random effect for <strong>Run</strong> I am assuming that the RMSEP values for that run are correlated to some degree but are uncorrelated between runs, on the basis of the induced correlation the random effect affords.</p>

<p>Is this assumption of independence <em>between</em> runs valid? If not is there a way to account for this in the LME model or should I be looking to employ another type of statical analysis to answer my question?</p>
"
"0.0915365116690398","0.0814088031193886"," 41510","<p>How do you explain that ? There's only one operator but the mixed model returns an estimate for the <code>operator</code> random effect. Furthermore the <code>sample</code> effect is confounded with the interaction <code>sample:operator</code>. Below is the R code.</p>

<pre><code>&gt; dd
   sample operator         y
9      10      SCF 0.9153188
10     10      SCF 0.9884982
19    100      SCF 2.0798781
20    100      SCF 2.0464027
29   1000      SCF 3.0401590
30   1000      SCF 3.0114448
39  10000      SCF 4.1348324
40  10000      SCF 4.0840063
49  1e+05      SCF 5.1235795
50  1e+05      SCF 5.1106381
59  1e+06      SCF 6.0803404
60  1e+06      SCF 6.2353263
&gt; str(dd)
'data.frame':   12 obs. of  3 variables:
 $ sample  : Factor w/ 6 levels ""10"",""100"",""1000"",..: 1 1 2 2 3 3 4 4 5 5 ...
     $ operator: Factor w/ 1 level ""SCF"": 1 1 1 1 1 1 1 1 1 1 ...
 $ y       : num  0.915 0.988 2.08 2.046 3.04 ...
&gt; lmer(y ~ (1|sample)+(1|operator)+(1|sample:operator), data=dd) 
Linear mixed model fit by REML 
Formula: y ~ (1 | sample) + (1 | operator) + (1 | sample:operator) 
   Data: dd 
  AIC   BIC logLik deviance REMLdev
 18.6 21.03 -4.302    9.932   8.605
Random effects:
 Groups          Name        Variance   Std.Dev.
 sample:operator (Intercept) 1.87954740 1.370966
 sample          (Intercept) 1.87954925 1.370967
 operator        (Intercept) 0.00063096 0.025119
 Residual                    0.00283931 0.053285
Number of obs: 12, groups: sample:operator, 6; sample, 6; operator, 1

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.5709     0.7921   4.508
</code></pre>

<p>For those who are more familiar with SAS the corresponding code is:</p>

<pre><code>PROC MIXED DATA=dd;
CLASS sample operator;
MODEL y=;
RANDOM sample operator sample*operator;
RUN;
</code></pre>

<p>This is nothing but the crossed 2-way ANOVA with random effects.</p>
"
"0.0905976508333704","0.0940027887907685"," 41637","<p>I'm trying to fit a multivariate (i.e., multiple response) mixed model in <code>R</code>. Aside from the <code>ASReml-r</code> and <code>SabreR</code> packages (which require external software), it seems this is only possible in <code>MCMCglmm</code>. In the <a href=""http://www.jstatsoft.org/v33/i02/"" rel=""nofollow"">paper</a> that accompanies the <code>MCMCglmm</code> package (pp.6), Jarrod Hadfield describes the process of fitting such a model as like reshaping multiple response variables into one long-format variable and then suppressing the overall intercept. My understanding is that suppressing the intercept changes the interpretation of the coefficient for each level of the response variable to be the mean for that level. Given the above, is it therefore possible to fit a multivariate mixed model using <code>lme4</code>? For example:</p>

<pre><code>data(mtcars)
library(reshape2)
mtcars &lt;- melt(mtcars, measure.vars = c(""drat"", ""mpg"", ""hp""))
library(lme4)
m1 &lt;- lmer(value ~ -1 + variable:gear + variable:carb + (1 | factor(carb)),
    data = mtcars)
summary(m1)
#  Linear mixed model fit by REML 
#  Formula: value ~ -1 + variable:gear + variable:carb + (1 | factor(carb)) 
#     Data: mtcars 
#   AIC   BIC logLik deviance REMLdev
#   913 933.5 -448.5    920.2     897
#  Random effects:
#   Groups       Name        Variance Std.Dev.
#   factor(carb) (Intercept) 509.89   22.581  
#   Residual                 796.21   28.217  
#  Number of obs: 96, groups: factor(carb), 6
#  
#  Fixed effects:
#                    Estimate Std. Error t value
#  variabledrat:gear  -7.6411     4.4054  -1.734
#  variablempg:gear   -1.2401     4.4054  -0.281
#  variablehp:gear     0.7485     4.4054   0.170
#  variabledrat:carb   5.9783     4.7333   1.263
#  variablempg:carb    3.3779     4.7333   0.714
#  variablehp:carb    43.6594     4.7333   9.224
</code></pre>

<p>How would one interpret the coefficients in this model? Would this method also work for generalized linear mixed models?</p>
"
"0.0827039616973562","0.0858124131484961"," 43634","<p>I have 299 surveys collected from 299 individuals working at 26 different locations. I want to understand how the location specific features relate to the individual survey scores.  The only inference I have as to location features is gathered from the individual survey scores.  Is it a valid strategy to calculate means for each location based on the individual scores, and include this as a level 2 variable?  Further, does it also make sense to include the same variable but as the level 1 variable, with slope varying freely between locations, if I want to compare the relative usefulness of the mean (best estimate of 'reality') to a persons individual score? (their perception of reality and response biases).  </p>

<p>I feel like I may have some circularity in the logic.  My implementation in R for one of the variables of interest follows, any feedback is welcome!</p>

<pre><code>lmer(X21~X25+meanX25+(X25|X1),data=datai)
Linear mixed model fit by REML 
Formula: X21 ~ X25 + meanX25 + (X25 | X1) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 1079 1105 -532.7     1056    1065
Random effects:
 Groups   Name        Variance Std.Dev. Corr   
 X1       (Intercept) 0.384983 0.62047         
          X25         0.012382 0.11127  -1.000 
 Residual             1.936068 1.39143 
Number of obs: 299, groups: X1, 26
Fixed effects:
            Estimate Std. Error t value
(Intercept)  1.13616    0.38013   2.989
X25          0.56683    0.05265  10.766
meanX25      0.33897    0.12213   2.775

Correlation of Fixed Effects:
        (Intr) X25   
X25     -0.119       
meanX25 -0.838 -0.389
</code></pre>
"
"0.138390197576366","0.143591631723548"," 45278","<p>I m working on a piecewise linear growth model and I need help to understand how to write my <code>lmer()</code> code and how to interpret the <code>R</code> output.</p>

<p>My data are the sales return of different IDs over a period of time. I want to know how the sales-return (growth) changes after a certain event (breakpoint).
To define the breakpoint I inserted a coded variable.</p>

<pre><code>df = data.frame (
  ID = c(1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2),
  sales = c(1,4,10,12,20,26,28,30,31,32,33,2,5,9,12,15,19,26,27,29,31,32,34,36),
  var1 = c(1,2,3,3,3,3,3,3,3,3,3,1,2,3,4,4,4,4,4,4,4,4,4,4),
  var2 = c(0,0,0,0,0,0,1,2,3,4,5,0,0,0,0,0,0,0,1,2,3,4,5,6))
</code></pre>

<p>I need to apply a Multi-Level Model -- a 2-level-model to be more precise -- using the <code>lme4</code> package. I'm looking for the correct <code>lmer()</code> code to estimate this equation:</p>

<p>$$Y_{ti} = \pi _{0i} + (\gamma _{00}+\varepsilon _{0i})a_{1ti} + (\gamma _{10}+\varepsilon _{1i})a_{2ti} + e_{ti}$$</p>

<p>My data and variables:</p>

<blockquote>
  <p>level 1: individual ID-level    </p>
  
  <p>level 2: inter-individual level </p>
  
  <p>$\varepsilon _{0i}$: var1 (this is my first coded variable, period 1)</p>
  
  <p>$\varepsilon _{1i}$: var2 (this is my second coded variable, period 2)</p>
  
  <p>sales: dependent variable   ID: random effect (is this correct?)  </p>
  
  <p>var1 and var 2: fixed effects (is this correct?)</p>
</blockquote>

<p>I think the code for my model should be:</p>

<pre><code>test &lt;- lmer(sales ~ 0 + var1 + var2 + (1| ID), data=df)
</code></pre>

<p><strong>Q1:</strong> is this code appropriate?</p>

<p>My Output</p>

<pre><code>&gt; summary(test)
Linear mixed model fit by REML 
Formula: sales ~ 0 + var1 + var2 + (1 | ID) 
   Data: df 
   AIC   BIC logLik deviance REMLdev
 154.1 158.8 -73.05    147.8   146.1
Random effects:
 Groups   Name        Variance Std.Dev.
 ID       (Intercept) 11.646   3.4127  
 Residual             25.902   5.0894  
Number of obs: 24, groups: ID, 2

Fixed effects:
     Estimate Std. Error t value
var1   5.5828     0.7759   7.195
var2   3.5039     0.5646   6.206

Correlation of Fixed Effects:
     var1  
var2 -0.433
</code></pre>

<p><strong>Q2:</strong> Interpretation (sorry, I m not really familiar with statistical interpretation :( )<br>
Is it correct that:   </p>

<ul>
<li>fixed effects: var1 Estimate is my slope parameter for period 1?  </li>
<li>fixed effects: var2 Estimate is my slope parameter for period 2?  </li>
</ul>

<p>So I could say that the sales return growth is smaller in period 2 than in period 1?</p>
"
"0.153350524281807","0.150739812452713"," 48582","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/48696/generalized-linear-mixed-model-in-r-with-repeated-measures"">Generalized Linear Mixed Model in R with repeated measures</a>  </p>
</blockquote>



<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.153350524281807","0.150739812452713"," 48696","<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.0905976508333704","0.0940027887907685"," 49623","<p>I am used to fitting 3-level HLM with HLM software, but now I am moving to <code>nlme</code> or <code>lme4</code> in <code>R</code>. Unfortunately, though, I am struggling a little bit with the proper specification/notation of the random parameters of the 2nd and the 3rd levels.</p>

<p>What I am trying to do is to estimate a 3-level linear model where some of the random effects are in the 2nd level, some are in the 3rd, but I do not know how to differentiate them in the ""random"" part of <code>lme()</code>. To make it clear, let me give you the specification equations:</p>

<pre><code>Y = b0 + b1*X1 + b2*X2 + e

    b0 = a00 + a01*X3 + r0
    b1 = a10 + a11*X4 + r1
    b2 = a20 + r2

         a00 = c00
         a01 = c10 + u1
         a10 = c20 + u2
         a11 = c30
         a20 = c40
</code></pre>

<p>and the final model is</p>

<pre><code>Y = c00 + c20*X1 + c40*X2 + c10*X3 + c30*X1*X4 + r0 + r1*X1 + r2*X2 + u2*X1 + u1*X3 + e
</code></pre>

<p>To estimate this model, so far I could only think of the following working codes: </p>

<pre><code>lme(Y ~ X1 + X2 + X3 + X1:X4, random= ~ X1 + X2 + X3 | level3 / level2, data=mydataset)
lmer(Y ~ X1 + X2 + X3 + X1:X4 + (1|level2) + (X1| level3/level2) + (X2|level2) + (1|level3) + (X3|level3), data=mydataset)
</code></pre>

<p>The <code>lme</code> seems to be somewhat wrong, as it estimates the random parameters of X1, X2 and X3 both in level 2 and in level 3, not the random compenents of X1 and x2 in level 2 and of X1 and X3 in level 3. In <code>lmer</code> I tried to do so, but I don't know if it is right to so in this way.</p>

<p>Please, could any of you give me some basic light here?</p>

<p>Thanks a ton!</p>
"
"0.0523065780965941","0.0542725354129257"," 49872","<p>I am currently running some analyses on a linguistic data set with a mixed effect model. The problem is, I think that one random factor should be excluded while my colleague thinks it should be included. The two options are:</p>

<pre><code>lmer(intdiff ~ stress * vowel_group + (1|speaker) + (1|word), data)
lmer(intdiff ~ stress * vowel_group + (1|speaker), data)
</code></pre>

<p>How do we check which model best fits our data set? It was suggested that I use a likelihood ratio test, but as far as I can tell, there isn't a function in R that can be applied to 2 linear mixed effects models. Is there another way to tell which model is more predictive?</p>

<p>Thanks</p>
"
"0.0739726721455309","0.0575647167340002"," 50726","<p>I'm using the <code>lme4</code> package in R to do some logistic mixed-effects modeling.<br>
My understanding was that sum of each random effects should be zero.</p>

<p>When I make toy linear mixed-models using <code>lmer</code>, the random effects are usually &lt; $10^{-10}$ confirming my belief that the <code>colSums(ranef(model)$groups) ~ 0</code>
But in toy binomial models (and in models of my real binomial data) some of the random effect sum to ~0.9. </p>

<p>Should I be concerned?  How do I interpret this?  </p>

<p>Here is a linear toy example
<code><pre>
toylin&lt;-function(n=30,gn=10,doplot=FALSE){
 require(lme4)
 x=runif(n,0,1000)
 y1=matrix(0,gn,n)
 y2=y1
 for (gx in 1:gn)
 {
   y1[gx,]=2*x*(1+(gx-5.5)/10) + gx-5.5  + rnorm(n,sd=10)
   y2[gx,]=3*x*(1+(gx-5.5)/10) * runif(1,1,10)  + rnorm(n,sd=20)
 }
 c1=y1*0;
 c2=y2*0+1;
 y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
 g=rep(1:gn,each=n,times=2)
 x=rep(x,times=gn*2)
 c=c(c1,c2)
 df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
 (m=lmer(y~x*c + (x*c|g),data=df))
 if (doplot==TRUE)
  {require(lattice)
   df$fit=fitted(m)
   plot1=xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1)
   plot2=xyplot(y ~ x|g,data=df,group=c)
   print(plot1+plot2)
  }
 print(colMeans(ranef(m)$g))
 m
}
</code></pre></p>

<p>In this case the colMeans always come out $&lt;10^{-6}$ </p>

<p>Here is a binomial toy example (I would share my actual data, but it is being submitted for publication and I am not sure what the journal policy is on posting beforehand):</p>

<p><pre><code>
toybin&lt;-function(n=100,gn=4,doplot=FALSE){
  require(lme4)<br>
  x=runif(n,-16,16)
  y1=matrix(0,gn,n)
  y2=y1
  for (gx in 1:gn)
  { com=runif(1,1,5)
    ucom=runif(1,1,5)
    y1[gx,]=tanh(x/(com+ucom) + rnorm(1)) > runif(x,-1,1)
    y2[gx,]=tanh(2*(x+2)/com + rnorm(1)) > runif(x,-1,1)
  }
  c1=y1*0;
  c2=y2*0+1;
  y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
  g=rep(1:gn,each=n,times=2)
  x=rep(x,times=gn*2)
  c=c(c1,c2)
  df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
  (m=lmer(y~x*c + (x*c|g),data=df,family=binomial))
  if (doplot==TRUE)
   {require(lattice)
    df$fit=fitted(m)
    print(xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1))
   }
  print(colMeans(ranef(m)$g))
  m
}
</pre></code></p>

<p>Now the colMeans sometimes come out above 0.3, and definitely higher, on average than the linear example.</p>
"
"0.0978566471559948","0.101534616513362"," 53427","<p>Lets take as an example a repeated measures design with 10 subjects that are all reading the same letter strings and pressing a button as soon as they determine whether the string is valid English word, producing reaction times (RT). I wish to determine whether word length has a significant effect on the produced RT (it should), using a linear mixed effect model. Using R and the lme4 package I construct the following model:</p>

<pre><code>m = lmer(RT ~ 1 + word.length + (1 + word.length|subject), data=rt.data)
</code></pre>

<p>As you can see, I allow both the intercept and the slope to vary randomly across subjects, as I suspect that the effect of word length might be larger for slow readers than fast readers. </p>

<p>Understanding that p-values are not trivial in these types of models, my approach is to construct a NULL model, containing only the random effects. But I'm not sure whether this should be:</p>

<pre><code>m.null = lmer(RT ~ (1 + word.length|subject), data=rt.data)
</code></pre>

<p>or:</p>

<pre><code>m.null = lmer(RT ~ (1 | subject), data=rt.data)
</code></pre>

<p>In the end, I wish to perform an anova between the model with word length and the NULL model like so:</p>

<pre><code>anova(m, m.null)
</code></pre>

<p>which should give me a p-value whether the addition of word length actually makes the model fit better and thus whether word length actually influences the RT.</p>
"
"0.12266979912335","0.127280377713181"," 54678","<p>We are investigating the effects of a treatment (versus untreated control) on insect abundance.  The study consists of 5 geographically separated study site replicates. Treatments were applied to 1 ~500 acre area at each location.  Insects were collected from 14 randomly selected locations within the treatment area at each site, and were compared to samples collected from 14 randomly selected untreated reference area locations at each site.  Samples were collected during 3 biweekly periods each year.  So there are 840 observations for the entire study (2 years x 5 sites x 2 treatment levels/site x 14 sampling plots/treatment x 3 sampling periods/year).</p>

<p>We are proposing to analyze these data with linear mixed effects models with treatment and years as the fixed effects, and site/sample location as random effects.  Year is included as a fixed effect primarily because there are only 2 levels.  I would appreciate feedback on whether either of the following models using lmer in the R lmer4 package are an appropriate starting point for analyzing these data:</p>

<pre><code>m1 &lt;- lmer(abundance ~ TRT*year + (period|site/sample))    
</code></pre>

<p>Or</p>

<pre><code>m2 &lt;- lmer(abundance ~ TRT*year + (TRT|site) + (period|site/sample)
</code></pre>

<p>where <code>sample</code> is the 14 sampling locations within treatment level, and <code>period</code> is the 3 bi-weekly samples within each year.</p>
"
"0.0905976508333704","0.0940027887907685"," 56380","<p>The <code>lme4</code> package in R includes the <code>cake</code> dataset. </p>

<pre><code>library(lme4)
head(cake[,2:4], 20)
   recipe temperature angle
1       A         175    42
2       A         185    46
3       A         195    47
4       A         205    39
5       A         215    53
6       A         225    42
7       B         175    39
8       B         185    46
9       B         195    51
10      B         205    49
11      B         215    55
12      B         225    42
13      C         175    46
14      C         185    44
15      C         195    45
16      C         205    46
17      C         215    48
18      C         225    63
19      A         175    47
20      A         185    29
</code></pre>

<p>I've analysed the <code>cake</code> dataset using two different models below. The first model is a 2 factor ANOVA:</p>

<pre><code>summary(aov(angle ~ temperature + recipe, cake))
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature   5   2100   420.1   6.918 4.37e-06 ***
recipe        2    135    67.5   1.112     0.33    
Residuals   262  15908    60.7                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...and the second is a mixed effects model, with <code>temperature</code> as a random effect:</p>

<pre><code>lmer(angle ~ recipe + (1| temperature), data=cake, REML=F)
Linear mixed model fit by maximum likelihood 
Formula: angle ~ recipe + (1 | temperature) 
   Data: cake 
  AIC  BIC logLik deviance REMLdev
 1893 1911 -941.7     1883    1877
Random effects:
 Groups      Name        Variance Std.Dev.
 temperature (Intercept)  6.4399  2.5377  
 Residual                60.2560  7.7625  
Number of obs: 270, groups: temperature, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   33.122      1.320  25.093
recipeB       -1.478      1.157  -1.277
recipeC       -1.522      1.157  -1.315

Correlation of Fixed Effects:
        (Intr) recipB
recipeB -0.438       
recipeC -0.438  0.500
</code></pre>

<p>Is someone able to provide a summary of what the mixed effect model has done differently to the ANOVA?</p>
"
"0.104613156193188","0.108545070825851"," 56600","<p>I have produced the following model: </p>

<pre><code>&gt;lmer(TotalPayoff~PgvnD*Type+Type*Asym+PgvnD*Asym+Game*Type+Game*PgvnD+Game*Asym+
                   (1|Subject)+(1|Pairing),REML=FALSE,data=table1)-&gt;m1

PgvnD=A percentage (numeric)
Asym= a factor 0 or 1
Type=a factor 1 or 2
Game= a factor 1 or 2
</code></pre>

<p>from this model the terms <code>Type</code>, <code>Game</code> and <code>PgvnD:Asym</code> were shown to be  significant by removal from the model. <code>PgvnD</code> and <code>Asym</code> on there own were not significant but were left in the model because the interaction between them was. The summary of this model is as follows;</p>

<pre><code>&gt; m7
Linear mixed model fit by maximum likelihood 
Formula: TotalPayoff ~ Type + PgvnD * Asym + Game + (1 | Subject) + (1 |Pairing) 
   Data: table1 
  AIC  BIC logLik deviance REMLdev
 1014 1038 -497.8    995.6   964.4
Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.000   0.0000 
 Pairing  (Intercept) 716.101  26.7601 
 Residual              89.364   9.4533 
Number of obs: 113, groups: Subject, 73; Pairing, 61

Fixed effects:
            Estimate Std. Error t value
(Intercept)   81.727      6.332  12.907
Type2          7.926      2.852   2.779
PgvnD         -8.466      7.554  -1.121
Asym1        -12.167      7.583  -1.604
Game2         15.374      7.147   2.151
PgvnD:Asym1   26.618      9.710   2.741

Correlation of Fixed Effects:
            (Intr) Type2  PgvnD  Asym1  Game2 
Type2       -0.188                            
PgvnD       -0.218 -0.038                     
Asym1       -0.620  0.081  0.189              
Game2       -0.483  0.009 -0.010 -0.015       
PgvnD:Asym1  0.233 -0.267 -0.766 -0.328 -0.011
</code></pre>

<p>Am I interpreting these results correctly?</p>

<ul>
<li><code>TotalPayoff</code> is higher when <code>Type=1</code> than in <code>Type=2</code>, it is also higher when <code>game=2</code> than when <code>game=1</code>. </li>
<li>Also <code>TotalPayoff</code> increases significantly with <code>PgvnD</code> if <code>Asym=1</code> but not if <code>ASym=0</code> (indicated by significant interaction term but non-significant single terms).</li>
</ul>

<p>Also I notice that the <code>Subject</code> random effect has SD and variance of 0. Can this then be removed from the model? What does this really mean?</p>
"
"0.173481293613825","0.180001636385951"," 56695","<p>I understand that we use random effects (or mixed effects) models when we believe that some model parameter(s) vary randomly across some grouping factor. I have a desire to fit a model where the response has been normalized and centered (not perfectly, but pretty close) across a grouping factor, but an independent variable <code>x</code> has not been adjusted in any way. This led me to the following test (using <em>fabricated</em> data) to ensure that I'd find the effect I was looking for if it was indeed there. I ran one <em>mixed</em> effects model with a random intercept (across groups defined by <code>f</code>) and a second <em>fixed</em> effect model with the factor f as a fixed effect predictor. I used the R package <code>lmer</code> for the mixed effect model, and the base function <code>lm()</code> for the fixed effect model. Following is the data and the results. </p>

<p>Notice that <code>y</code>, regardless of group, varies around 0. And that <code>x</code> varies consistently with <code>y</code> within group, but varies much more across groups than <code>y</code></p>

<pre><code>&gt; data
      y   x f
1  -0.5   2 1
2   0.0   3 1
3   0.5   4 1
4  -0.6  -4 2
5   0.0  -3 2
6   0.6  -2 2
7  -0.2  13 3
8   0.1  14 3
9   0.4  15 3
10 -0.5 -15 4
11 -0.1 -14 4
12  0.4 -13 4
</code></pre>

<p>If you're interested in working with the data, here is <code>dput()</code> output:</p>

<pre><code>data&lt;-structure(list(y = c(-0.5, 0, 0.5, -0.6, 0, 0.6, -0.2, 0.1, 0.4, 
-0.5, -0.1, 0.4), x = c(2, 3, 4, -4, -3, -2, 13, 14, 15, -15, 
-14, -13), f = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 
4L, 4L, 4L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")), 
.Names = c(""y"",""x"",""f""), row.names = c(NA, -12L), class = ""data.frame"")
</code></pre>

<p>Fitting the mixed effects model:</p>

<pre><code>&gt; summary(lmer(y~ x + (1|f),data=data))
Linear mixed model fit by REML 
Formula: y ~ x + (1 | f) 
   Data: data 
   AIC   BIC logLik deviance REMLdev
 28.59 30.53  -10.3       11   20.59
Random effects:
 Groups   Name        Variance Std.Dev.
 f        (Intercept) 0.00000  0.00000 
 Residual             0.17567  0.41913 
Number of obs: 12, groups: f, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.008333   0.120992   0.069
x           0.008643   0.011912   0.726

Correlation of Fixed Effects:
  (Intr)
x 0.000 
</code></pre>

<p>I note that the intercept variance component is estimated 0, and importantly to me, <code>x</code> is not a significant predictor of <code>y</code>.</p>

<p>Next I fit the fixed effect model with <code>f</code> as a predictor instead of a grouping factor for a random intercept:</p>

<pre><code>&gt; summary(lm(y~ x + f,data=data))

Call:
lm(formula = y ~ x + f, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.16250 -0.03438  0.00000  0.03125  0.16250 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.38750    0.14099  -9.841 2.38e-05 ***
x            0.46250    0.04128  11.205 1.01e-05 ***
f2           2.77500    0.26538  10.457 1.59e-05 ***
f3          -4.98750    0.46396 -10.750 1.33e-05 ***
f4           7.79583    0.70817  11.008 1.13e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1168 on 7 degrees of freedom
Multiple R-squared: 0.9484, Adjusted R-squared: 0.9189 
F-statistic: 32.16 on 4 and 7 DF,  p-value: 0.0001348 
</code></pre>

<p>Now I notice that, as expected, <code>x</code> is a significant predictor of <code>y</code>.</p>

<p><strong>What I am looking for</strong> is intuition regarding this difference. In what way is my thinking wrong here? Why do I incorrectly expect to find a significant parameter for <code>x</code> in both of these models but only actually see it in the fixed effect model?</p>
"
"0.0640622132638473","0.0443133396029328"," 57395","<p>I have the following model:</p>

<pre><code>&gt; model1&lt;-lmer(aph.remain~sMFS1+sAG1+sSHDI1+sbare+season+crop
  +(1|landscape),family=poisson)
</code></pre>

<p>...and this is the summary output.  </p>

<pre><code>&gt; summary(model1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: aph.remain ~ sMFS1 + sAG1 + sSHDI1 + sbare + season + crop 
         +      (1 | landscape) 
  AIC  BIC logLik deviance
 4057 4088  -2019     4039
Random effects:
 Groups    Name        Variance Std.Dev.
 landscape (Intercept) 0.74976  0.86588 
Number of obs: 239, groups: landscape, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.6613761  0.1344630  19.793  &lt; 2e-16 
sMFS1        0.3085978  0.1788322   1.726  0.08441   
sAG1         0.0003141  0.1677138   0.002  0.99851    
sSHDI1       0.4641420  0.1619018   2.867  0.00415 
sbare        0.4133425  0.0297325  13.902  &lt; 2e-16 
seasonlate  -0.5017022  0.0272817 -18.390  &lt; 2e-16 
cropforage   0.7897194  0.0672069  11.751  &lt; 2e-16
cropsoy      0.7661506  0.0491494  15.588  &lt; 2e-16 
</code></pre>

<p>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â </p>

<pre><code>Correlation of Fixed Effects:
           (Intr) sMFS1  sAG1   sSHDI1 sbare  sesnlt crpfrg
sMFS1      -0.007                                          
sAG1        0.002 -0.631                                   
sSHDI1      0.000  0.593 -0.405                            
sbare      -0.118 -0.003  0.007 -0.013                     
seasonlate -0.036  0.006 -0.006  0.003 -0.283              
cropforage -0.168 -0.004  0.016 -0.014  0.791 -0.231       
cropsoy    -0.182 -0.028  0.030 -0.001  0.404 -0.164  0.557
</code></pre>

<p>It is probably overdispersed, but how exactly do I calculate this? </p>

<p>Thanks very much.</p>
"
"0.161219701233018","0.167279188638034"," 58745","<p>EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.</p>

<p>The experiments I need to analyze look like this: </p>

<ul>
<li>Subjects were assigned to one of several treatment groups</li>
<li>Measurements of each subject were taken on multiple days</li>
<li>So:
<ul>
<li>Subject is nested within treatment</li>
<li>Treatment is crossed with day</li>
</ul></li>
</ul>

<p>(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)</p>

<p>My dataset contains the following information:</p>

<ul>
<li>Subject = blocking factor (random factor)</li>
<li>Day = within subject or repeated measures factor (fixed factor)</li>
<li>Treatment = between subject factor (fixed factor)</li>
<li>Obs = measured (dependent) variable</li>
</ul>

<p><strong>UPDATE</strong>
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:</p>

<p><strong>Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)</strong></p>

<p>Obviously his notation is different from the R syntax, but this model is supposed to account for:</p>

<ul>
<li>Treatment   (fixed)</li>
<li>Day   (fixed)</li>
<li>the Treatment*Day interaction</li>
<li>Subject nested within Treatment  (random)</li>
<li>Day crossed with ""Subject within Treatment""   (random)</li>
</ul>

<p>So, is this the correct syntax to use? </p>

<pre><code>m4 &lt;- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)
</code></pre>

<p>I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?</p>

<p>Here are my previous attempts at building a model and writing syntax (discussed in answers &amp; comments):</p>

<pre><code>m1 &lt;- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)
</code></pre>

<p>How do I deal with the fact that subject is nested within treatment?  How does <code>m1</code> differ from: </p>

<pre><code>m2 &lt;- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 &lt;- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)
</code></pre>

<p>and are <code>m2</code> and <code>m3</code> equivalent (and if not, why)?</p>

<p>Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like <code>correlation = corAR1</code>)?  According to <a href=""http://circ.ahajournals.org/content/117/9/1238.full"">Repeated Measures</a>, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. </p>

<p>When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?</p>

<p>Here's an example of what the data look like:</p>

<pre><code>mydata &lt;- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))
</code></pre>
"
"0.128124426527695","0.132940018808798"," 58900","<p>I have very recently started learning about Generalised Linear Mixed Models and was using R to explore what difference it makes to treat group membership as either fixed or random effect. In particular, I am looking at the example dataset discussed here:</p>

<p><a href=""http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm"">http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm</a></p>

<p><a href=""http://www.ats.ucla.edu/stat/r/dae/melogit.htm"">http://www.ats.ucla.edu/stat/r/dae/melogit.htm</a></p>

<p>As outlined in this tutorial, the effect of Doctor ID is appreciable and I was expecting the mixed model with a random intercept to give better results. However, comparing AIC values for the two methods suggest that this model is worse:</p>

<pre><code>&gt; require(lme4) ; hdp = read.csv(""http://www.ats.ucla.edu/stat/data/hdp.csv"")
&gt; hdp$DID = factor(hdp$DID) ; hdp$Married = factor(hdp$Married)
&gt; GLM = glm(remission~Age+Married+IL6+DID,data=hdp,family=binomial);summary(GLM)

Call:
glm(formula = remission ~ Age + Married + IL6 + DID, family = binomial, 
data = hdp)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5265  -0.6278  -0.2272   0.5492   2.7329  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.560e+01  1.219e+03  -0.013    0.990    
Age         -5.869e-02  5.272e-03 -11.133  &lt; 2e-16 ***
Married1     2.688e-01  6.646e-02   4.044 5.26e-05 ***
IL6         -5.550e-02  1.153e-02  -4.815 1.47e-06 ***
DID2         1.805e+01  1.219e+03   0.015    0.988    
DID3         1.932e+01  1.219e+03   0.016    0.987   

[...]

DID405       1.566e+01  1.219e+03   0.013    0.990    
DID405       1.566e+01  1.219e+03   0.013    0.990    
DID406      -2.885e-01  3.929e+03   0.000    1.000    
DID407       2.012e+01  1.219e+03   0.017    0.987    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 10353  on 8524  degrees of freedom
Residual deviance:  6436  on 8115  degrees of freedom
AIC: 7256

Number of Fisher Scoring iterations: 17


&gt; GLMM = glmer(remission~Age+Married+IL6+(1|DID),data=hdp,family=binomial) ; m

Generalized linear mixed model fit by the Laplace approximation 
Formula: remission ~ Age + Married + IL6 + (1 | DID) 
Data: hdp 
AIC  BIC logLik deviance
7743 7778  -3867     7733
Random effects:
Groups Name        Variance Std.Dev.
DID    (Intercept) 3.8401   1.9596  
Number of obs: 8525, groups: DID, 407

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.461438   0.272709   5.359 8.37e-08 ***
Age         -0.055969   0.005038 -11.109  &lt; 2e-16 ***
Married1     0.260065   0.063736   4.080 4.50e-05 ***
IL6         -0.053288   0.011058  -4.819 1.44e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
         (Intr) Age    Marrd1
Age      -0.898              
Married1  0.070 -0.224       
IL6      -0.162  0.012 -0.033


&gt; extractAIC(GLM) ; extractAIC(GLMM)

[1]  410.000 7255.962
[1]    5.000 7743.188
</code></pre>

<p>Thus, my questions are:</p>

<p>(1) Is it appropriate to compare the AIC values provided by the two functions? If so, why does the fixed effect model do better?</p>

<p>(2) What is the best way to identify if fixed or random effects are more important (ie to quantify that the variability due to the doctor is more important than patient characteristics?</p>
"
"0.143247463647051","0.138722695527068"," 59539","<p>I'm new to linear mixed modeling, and have some theory-driven questions that I'm not sure how to analytically resolve.</p>

<p>I am analyzing experimental data with a within-subjects factor (<code>discount</code>). My theory hypothesizes that the effect of this within-subjects factor is contingent upon a between-subjects characteristic of respondents (<code>iipm</code>). Because my data is in long form, and respondents are making 8 choices over time, I model my data as follows:</p>

<pre><code>library(lme4)
m1 &lt;- glmer(chose ~ iipm*discount + product + (1|id) + (1|time), data=long1, 
        family=""binomial"")
</code></pre>

<p>All that I'm trying to do here is fit a simple model that accounts for the dependence between observations for a single subject (<code>1|id</code>), and the potential effect of making several choices in a row (<code>1|time</code>).</p>

<p>However, my theory further specifies that this relationship <strong>should not be affected</strong> by the inclusion of other demographic variables in the model (let's say <code>ideology</code> and <code>partisanship</code>).  So, based on some reading I've done <a href=""http://stats.stackexchange.com/questions/3757/random-effect-slopes-in-linear-mixed-models"">(as well as previous answers on this site)</a>, I fit the following model:</p>

<pre><code>m2 &lt;- glmer(chose ~ iipm*discount + product + (1 |id) + (1|time) + (1|partisanship) +
       (1|ideology) , data=long1, family=""binomial"")
</code></pre>

<p>Because random slopes goes beyond my expertise, I'm just using random intercepts to see what happens when I account for baseline variation amongst individuals attributable to their partisanship and ideology. However, if I <strong>were</strong> to use random slopes to essentially say that the effects of partisanship and ideology vary on an individual basis, even after accounting for baseline variability, I should specify the following model:</p>

<pre><code>m3 &lt;- glmer(chose ~ iipm*discount + product + (1 + partisanship +ideology |id)  +
  (1|time) , data=long1, family=""binomial"")
</code></pre>

<p>To test the hypothesis that this baseline variability <strong><em>doesn't matter</em></strong>, I then run a likelihood ratio test comparing the two models:</p>

<pre><code>library(lmtest)
lrtest(m2,m1) # p=.349
lrtest(m3,m1) # p=.416
</code></pre>

<p>If there's no improvement in fit (p>.05), I (very tentatively) interpret this as support for my hypothesis that <code>demographics</code> and <code>ideology</code> don't matter.</p>

<p>Is this a right way to approach the data, or is there a more sophisticated way to test this hypothesis using multilevel modeling?  Any expertise and advice is greatly appreciated.</p>
"
"NaN","NaN"," 59702","<p>Can you suggest me any random effect linear model in R? At the moment I am using <strong>lmer</strong>, but I would like to know if there is any other similar function.</p>

<p>Thank you</p>
"
"0.12266979912335","0.127280377713181"," 59861","<p><strong>Data structure:</strong>
I have two datasets from two protected areas that differ in protection status. Both areas contain 43 and 37 sites each. </p>

<p><strong>Question:</strong>
I would like to know which test would be the best for testing whether the PA status has had an effect on:  </p>

<ol>
<li>the first axis of a PCoA (principal coordinates analysis) - i.e. species composition turnover (derived by constructing a bray curtis dissimilarity matrix) and </li>
<li>species richness per site (a continuous variable). </li>
</ol>

<p><strong>Problem:</strong>
I understand that there is pseudoreplication present in this as I only have two areas. From what I have read, it seems that I either have to use an ANCOVA / GLM / mixed-effect model, where I define PA status as both a random effect and a fixed effect. I intended to nest sites within PA, but it seems that as there is only one datapoint per site it will not work as a nested object. </p>

<p>For those familiar with R, here are some codes I have tried:</p>

<pre><code>pcoaPAanovadata1 &lt;- read.csv(""PCoA\\data\\
                              combined data PCoA axis 1 with distance variables.csv"", 
                              header=T)

str(pcoaPAanovadata1)
'data.frame': 80 obs. of 7 variables:
PCOA:    num -0.2215 -0.3521 -0.0611 0.3434 -0.3624 ...
PA.stat: Factor w/ 2 levels ""N"",""P"": 1 1 1 1 1 1 1 1 1 1 ...
village: num 33.6 33.7 39.9 37.9 34 ...
road:    num 4.18 3.8 0.89 0.1 3.43 5.49 1.86 5.04 0.79 0.88 ...
track:   num 8.11 6.48 3.11 2.71 4.49 5.35 1.25 4.03 7.62 6.77 ...
site:    Factor w/ 80 levels ""M1_11"",""M1_17"",..: 1 2 3 4 5 6 7 8 9 10 ...
rich:    num 3.27 1.79 7.31 0.82 1.79 1.82 2.45 0.82 5.47 2.79 ...
</code></pre>

<p>compare community composition turnover at different PAs:
below specifies a null model where the slope deviates as a result of the random effect </p>

<pre><code>z0 &lt;- lmer(rich ~ 1, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z0)
z1 &lt;- lme(rich ~ pastat, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z1)
anova(z0,z1)
</code></pre>

<p>impacts of distance variables:</p>

<pre><code>zz &lt;- lme(pcoa ~ road, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(zz)
</code></pre>

<p>The errors I get from the lme(linear mixed effect model):</p>

<pre><code>Warning message:
In pt(-abs(tTable[, ""t-value""]), tTable[, ""DF""]) : NaNs produced
</code></pre>

<p>The error I get from the ANOVA:</p>

<pre><code>Warning message:
In anova.lme(z0, z1) :
fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Firstly, I was hoping to just clarify whether the test I am running is correct. Secondly, it'd be great if someone could tell me what the errors mean. I apologise if my question is poorly phrased, I am relatively new to R and the statistics I am using. </p>
"
"0.177534413149274","0.161181206855201"," 59912","<p>So I often do little self-experiments where I blind &amp; randomize things; these can be formulated as your normal <em>t</em>-tests, but sometimes the measured metrics have extensive baselines which seem like they could be used for more accurate answers. A bunch of reading upon <em>n</em>-of-1 and single-subject designs suggested that people have been moving to mixed/hierarchical/multilevel models for analyzing such setups (eg. Nelson 2012 <a href=""http://etd.lsu.edu/docs/available/etd-04252012-152015/unrestricted/NelsonDiss.pdf"" rel=""nofollow"">""Hierarchical linear modeling versus visual analysis of single subject design data""</a> or <a href=""http://www.eric.ed.gov/PDFS/EJ800974.pdf"" rel=""nofollow"" title=""Van den Noortgate et al 2007"">""The Aggregation of Single-Case Results using Hierarchical Linear Models""</a>).</p>

<p>As I understand it, the idea is to split the subject's data into experiment vs baseline, and treat those as the groups. I'm trying to understand how sensible this is with a recent experiment, so hopefully someone can point out if I go wrong in using <code>lmer</code> here.</p>

<hr>

<p>We start with a regular linear model which examines purely the experimental data (the numeric <code>Response</code> vs the binary <code>Intervention</code> variables) and ignores the extensive baseline phase before, during, and after the experiment:</p>

<pre><code>R&gt; experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max
-1.0156 -0.8889 -0.0156  0.1111  1.1111

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    3.0156     0.0889    33.9   &lt;2e-16
Intervention  -0.1267     0.1262    -1.0     0.32

Residual standard error: 0.711 on 125 degrees of freedom
  (145 observations deleted due to missingness)
Multiple R-squared:  0.008, Adjusted R-squared:  6.73e-05
F-statistic: 1.01 on 1 and 125 DF,  p-value: 0.317

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 % 97.5 %
(Intercept)   2.8397  3.192
Intervention -0.3765  0.123
</code></pre>

<p>The estimated coefficient is not statistically-significant: -0.38-0.12. But it's definitely slanted towards being negative. So this is the 'conservative' case, where we ignore the baseline entirely. What's the optimistic case? Well, it seems to me that the optimistic case is when we take the entire baseline and assume it is exactly the same as the 'off'/0 intervention in the experiment, in which case we get a narrower CI (because our estimate of the intercept has halved its standard error):</p>

<pre><code>R&gt; experiment$Intervention[is.na(experiment$Intervention)] &lt;- 0
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max 
-1.9924 -0.8889  0.0076  1.0076  1.1111 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    2.9924     0.0375   79.88   &lt;2e-16
Intervention  -0.1036     0.1012   -1.02     0.31

Residual standard error: 0.746 on 458 degrees of freedom
Multiple R-squared:  0.00228,   Adjusted R-squared:  0.000101 
F-statistic: 1.05 on 1 and 458 DF,  p-value: 0.307

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 %  97.5 %
(Intercept)   2.9188 3.06607
Intervention -0.3025 0.09538
</code></pre>

<p>It's narrowed to -0.30-0.10; still not statistically-significant, but closer.</p>

<p>It seems to me that a hierarchical model ought to produce a CI intermediate between the pessimistic and optimistic cases: it loses some power because it's estimating how different the two phases are before it does any combining.</p>

<p>Here is my multilevel model, split between baseline and experimental phases:</p>

<pre><code>library(lme4)
experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
experiment$Phase &lt;- ifelse(is.na(experiment$Intervention), TRUE, FALSE)
model &lt;- lmer(Response ~ Intervention + (1|Phase), data=experiment); summary(model)

...
 AIC BIC logLik deviance REMLdev
 286 297   -139      273     278
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5057   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)     3.016      0.136    22.2
Intervention   -0.127      0.126    -1.0

Correlation of Fixed Effects:
            (Intr)
Interventin -0.461

m &lt;- mcmcsamp((lmer(Response ~ Intervention + (1|Phase), data=experiment)), n = 100000)
HPDinterval(m, prob=0.95)$fixef

                lower   upper
(Intercept)  -45.3107 56.6558
Intervention  -0.3742  0.1191
</code></pre>

<p>The estimated CI comes out exactly in the middle, as expected:</p>

<ol>
<li>pessimistic   -0.38 0.12</li>
<li>hierarchical  -0.37 0.11</li>
<li>optimistic    -0.30 0.10</li>
</ol>

<p>So, my basic question is: is this a sane approach to take? It's spitting out answers that seem intuitively correct, but that might just be a coincidence.</p>

<hr>

<p>Incidentally, one might be worried about time trends. The randomization/blocking would fix that in the experimental period but not the baseline. Fortunately, that doesn't seem to be an issue:</p>

<pre><code>experiment$Time &lt;- 1:nrow(experiment)
summary(lmer(Response ~ Intervention + Time + (1|Phase), data=experiment))

...
 AIC BIC logLik deviance REMLdev
 298 312   -144      272     288
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5055   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)   3.42517    0.42325    8.09
Intervention -0.12398    0.12621   -0.98
Time         -0.00132    0.00129   -1.02

Correlation of Fixed Effects:
            (Intr) Intrvn
Interventin -0.128       
Time        -0.947 -0.021
</code></pre>
"
"0.104613156193188","0.108545070825851"," 61008","<p>I'm a psycholinguistics student with few knowledge in statistics and I have some doubts about a Correlation of Fixed Effects in lmer function (lme4 package). So, if my question is stupidâ€¦ hummmâ€¦ I'm sorry!</p>

<p>My response variable is RT (Reaction Time in a self-paced reading experiment) and my independent variables are Ant (PP, NP) and Verbo (SG, PL). </p>

<p>I have modeled the data with intercepts for Sujeitos (the people who are doing the task) and Item (the sentences i've used), asking for principal effects and interactions between the variables. Here is the model <code>lmer(RT~Ant*Verbo+(1|Sujeitos)+(1|Item))</code> and that's the coefficients for fixed effects:</p>

<p><img src=""http://i.stack.imgur.com/MM27e.jpg"" alt=""Table Fixed Effects""></p>

<p>So, I have made a table of the coefficients for the interactions.</p>

<p>My problem is: that -0.705 and -0.716 correlation effects are a problem for my interaction terms? I'm saying this because the coefficients for the condition NP:SG came from the coefficients of SG only:</p>

<p><img src=""http://i.stack.imgur.com/vsYZo.jpg"" alt=""NP:SG""></p>

<p>and the coefficients for PP:PL came from the PP only:</p>

<p><img src=""http://i.stack.imgur.com/t0f8G.jpg"" alt=""PP:PL""></p>

<p>So, to me, there is no problem here, because I'm not contrasting PP:SG x PP (correlation = -0.705) and I'm not contrasting PP:SG x SG ((correlation = -0.716)). But I do that when getting the coefficients for PP:SG:</p>

<p>ï¿¼<img src=""http://i.stack.imgur.com/xBlx7.jpg"" alt=""PP:SG""></p>

<p>In the last case, there is a contrast between PP:SG x PP and SG. So, the correlation could be a problem. Is this correct? And, if so, how can I deal with this? I've read some thinks in <a href=""http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/"" rel=""nofollow"">Jaeger's blog</a> and in this book: <code>Howell, 2010. Statistical Methods for Psychology</code>, but it doesn't help much.</p>

<p>Thank you.</p>
"
"0.148201971273683","0.144726761101135"," 62000","<p>I've tried to create three models (using R): an intercept only linear regression, a simple mixed effects regression and a by-subject effects mixed effects regression.</p>

<p>An intercept only regression models the grand mean of a response variable plus error. In <code>mtcars</code>, the variable <code>drat</code> may be considered a response variable. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus error?</p>

<pre><code>interceptOnly &lt;- lm(drat ~ 1, data=mtcars)
</code></pre>

<p>A simple mixed effects regression models the grand mean of a response variable, plus subject deviation, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable and <code>cyl</code> a subject deviation. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus error?</p>

<pre><code>library(lme4)
simpleMixedEffects &lt;- lmer(drat ~ (1|cyl), data=mtcars)
</code></pre>

<p>A by-subject effects mixed effects regression models the grand mean of a response variable, plus subject deviation, plus condition effect, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable, <code>cyl</code> a subject deviation and <code>wt</code> a condition effect. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus the effect of <code>wt</code>, plus error?</p>

<pre><code>bySubjectMixedEffects &lt;- lmer(drat ~ (1|cyl) + wt, data=mtcars)
</code></pre>

<p>I have one further question: </p>

<p>How can I model a by-subject varying condition effect model. This is a mixed effects model which models the grand mean of a response variable, plus group deviation from grand mean (random effect), plus condition effect (fixed effect), plus group deviation from condition effect (random effect), plus error. Could someone provide R code that outputs a ""by-subject varying condition effect model""?</p>
"
"0.156919734289782","0.153772183669956"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"0.0978566471559948","0.101534616513362"," 63566","<p>I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.</p>

<p>My data-frame in short: A subject has one property, <code>is_frisian</code> which is either 0 or 1 depending on the subject. And it is tested for two conditions, <code>person</code> and <code>condition</code>. The measurement variable is <code>error</code>, which is either 0 or 1.</p>

<p>My mixed linear model in R is:</p>

<pre><code>&gt; model &lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)
</code></pre>

<p>However, the residuals plot of this model gives an unexpected (?) result.</p>

<p><img src=""http://i.stack.imgur.com/nz2KY.png"" alt=""Residuals lmer model""></p>

<p>I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?</p>

<p>The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable <code>error</code> is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?</p>

<p><strong>UPDATE:</strong></p>

<pre><code>&gt; model &lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')
&gt; binnedplot(fitted(model),resid(model))
</code></pre>

<p>Gives the following result:</p>

<p><img src=""http://i.stack.imgur.com/XMXFx.png"" alt=""binned residual plot""></p>

<p><strong>FINAL EDIT:</strong></p>

<p>The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), <a href=""https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/"" rel=""nofollow"">see here at statisticssolutions.com</a></p>
"
"0.133356131201899","0.117080918782665"," 63872","<p>I am think that it is possible to analyse <strong>a model with just random effects</strong> but I am not sure as I have never done it. I am looking for guidance on whether it is appropriate, what assumptions I need to be aware of, and how to do it properly.</p>

<p>From my study of an insect; </p>

<ul>
<li>I have a response variable (age at death, ""age"")  </li>
<li>Two treatments
(""Treat1"" and ""Treat2"") both of which have two levels (Treat1 has
""A"" and ""B"", and Treat2 has ""P"" and ""Q"")  </li>
<li>There is also 40 genotypes
(1-40)  </li>
<li>With four replicates (w,x,y,z) of each combination of
Genotype/Treat1/Treat2 </li>
<li>Each replicate contains 50 individuals</li>
</ul>

<p>Put simply, my data looks like 32000 rows of this:</p>

<pre><code>Treat1  Treat2  Genotype  Block  Individual   Age   
A       P       1         w      1            23
A       P       1         w      2            35
A       P       1         w      3            44
.       .       .         .      .            .
.       .       .         .      .            .
.       .       .         .      .            .
B       Q       40        z      50           76     
</code></pre>

<p>I would like to know if each combination of Treat1 and Treat2 (AP,AQ,BP,BQ) have genetic genetic variation - i.e. is there variation between my 40 genotypes within each treatment combination?</p>

<p>I think I need a model for each of AP, AQ, BP, and BQ, along the lines of </p>

<pre><code>Age ~ Genotype [ Treat1 == ""A"" &amp; Treat2 == ""P""] * Block [ Treat1 == ""A"" &amp; Treat2 == ""P""]
</code></pre>

<p>Where  Genotype and Block are random effects. I hear Gamma distribtions are better to use in lifespan (time to death) models.</p>

<p><strong>My questions are:</strong></p>

<p>a. Is this an appropriate way to show whether or not my genotypes have variation?</p>

<p>b. Can I build the four models as defined above or is that a really poor way of doing it?</p>

<p>c. If possible, what functions should I be using in R (lm, glm, lmer... &amp; summary, summary.lm, aov, anova...)?</p>

<p>d. What should I expect, if gamma is more suitable than gaussian, to see when I compare <code>plot(model)</code> for gamma compared to gaussian?</p>

<hr>

<p>This is currently my model...</p>

<pre><code>AP= df$Treat1==""A"" &amp; df$Treat2==""P""
apmodel&lt;- lmer(df$Age[AP]~(1|df$Genotype[AP])+(1|df$Block[AP]))
summary(apmodel)
</code></pre>

<p>Which I think is right but I'm not sure what to do with the output..</p>

<pre><code>&gt; summary(apmodel)
Linear mixed model fit by REML 
Formula: df$Age[AP] ~ (1 | df$Genotype[AP]) + (1 | df$Block[AP]) 
       AIC   BIC logLik deviance REMLdev
     57343 57371 -28667    57336   57335
    Random effects:
     Groups           Name        Variance Std.Dev.
     df$Genotype[AP]  (Intercept) 17.23798 4.15186 
     df$Block[AP]     (Intercept)  0.15416 0.39263 
     Residual                     93.18777 9.65338 
    Number of obs: 7757, groups: df$line[AP], 40; df$Block[AP], 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  49.9948     0.6939   72.05
</code></pre>

<p><strong>Is there genetic variance??</strong></p>
"
"0.105264957864947","0.10922137064511"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.0739726721455309","0.0767529556453336"," 64352","<p>I'm facing a problem with a binomial <code>glmer</code> model. I want to find if differences in flower presence in pine trees is due to procedence of the tree.
My model is as follows: <code>FlorMas ~ Proc + (1|Blq)</code>.
Proc is a factor with nine levels, one of it (<code>TAMR</code>) presents no flower at all (variable value for all <code>TAMR</code> trees is 0).
This model gives me this output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: FlorMas ~ Proc + (1 | Blq) 
   Data: flower.data 
 AIC   BIC logLik deviance
 593 647.7 -285.5      571
Random effects:
 Groups Name        Variance Std.Dev.
 Blq    (Intercept) 0.18476  0.42983 
Number of obs: 1067, groups: Blq, 8

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -1.7668     0.2958  -5.974 2.32e-09 ***
ProcTAMR     -16.8758  1080.5608  -0.016  0.98754    
ProcARMY      -0.3543     0.3910  -0.906  0.36490    
ProcASPE      -1.4891     0.5260  -2.831  0.00464 ** 
ProcCOCA      -2.4947     0.7619  -3.274  0.00106 ** 
ProcMIMI      -1.2040     0.4930  -2.442  0.01459 *  
ProcORIA      -1.5360     0.5739  -2.676  0.00744 ** 
ProcPLEU      -1.9437     1.0538  -1.845  0.06511 .  
ProcPTOV       0.1693     0.3508   0.483  0.62945    
ProcSCRI       0.5060     0.3346   1.512  0.13050    

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I don't understand that values for <code>TAMR</code> procedence, as if it has all zero values it should be different from the others.
Any help will be appreciated.</p>
"
"0.147945344291062","0.153505911290667"," 65489","<p>My problem can be summarized very simply: I'm using a linear mixed-effects model and I am trying to get p-values using pvals.fnc(). The problem is that this function seems to have some trouble estimating p-values directly from the t-values associated with model coefficients (Baayen et al., 2008), and I don't know what is going wrong with the way I do it (i.e. according to what I have read, it should work). So, I'm explaining my model below, and if you can point out what I am doing wrong and suggest changes I would really appreciate it!</p>

<p><strong>DESCRIPTION</strong>: I have a 2 by 2 within subjects design, fully crossing two <em>categorical</em> factors, ""Gram"" and ""Number"", each with two levels. This is the command I used to run the model:</p>

<pre><code>&gt;m &lt;- lmer(RT ~ Gram*Number + (1|Subject) + (0+Gram+Number|Subject) + (1|Item),data= data)
</code></pre>

<p>If I understand this code, I am getting coefficients for the two fixed effects (Gram and Number) and their interaction, and I am fitting a model that has by-subject intercepts and slopes for the two fixed effects, and a by-item intercept for them. Following Barr et al. (2013), I thought that this code gets rid of the correlation parameters. I don't want estimate the correlations because I want to get the p-values using pvals.fnc (), and I read that this function doesn't work if there are correlations in the model.</p>

<p>The command seems to work:</p>

<pre><code>&gt;m
Linear mixed model fit by REML 
Formula: RT ~ Gram * Number + (1 | Subject) + (0 + Gram + Number | Subject) + (1 |Item) 
   Data: mverb[mverb$Region == ""06v1"", ] 
   AIC   BIC logLik deviance REMLdev
 20134 20204 -10054    20138   20108
Random effects:
 Groups      Name        Variance  Std.Dev. Corr          
 Item       (Intercept)   273.508  16.5381               
 Subject     Gramgram        0.000   0.0000               
             Gramungram   3717.213  60.9689    NaN        
             Number1        59.361   7.7046    NaN -1.000 
 Subject     (Intercept) 14075.240 118.6391               
 Residual                35758.311 189.0987               
Number of obs: 1502, groups: Item, 48; Subject, 32

Fixed effects:
             Estimate Std. Error  t value
(Intercept)    402.520     22.321  18.033
Gram1          -57.788     14.545  -3.973
Number1         -4.191      9.858  -0.425
Gram1:Number1   15.693     19.527   0.804

Correlation of Fixed Effects:
            (Intr) Gram1  Numbr1
Gram1       -0.181              
Number1     -0.034  0.104       
Gram1:Nmbr1  0.000 -0.002 -0.011
</code></pre>

<p>However, when I try to calculate the p-values I still get an error message:</p>

<pre><code>&gt;pvals.fnc(m, withMCMC=T)$fixed
Error in pvals.fnc(m, withMCMC = T) : 
MCMC sampling is not implemented in recent versions of lme4
  for models with random correlation parameters
</code></pre>

<p>Am I making a mistake when I specify my model? Shouldn't pvals.fnc() work if I removed the correlations?</p>

<p>Thanks for your help!</p>
"
"0.236828104961361","0.239735956384022"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.0978566471559948","0.101534616513362"," 68106","<p>I'm having trouble understanding the output of my <code>lmer()</code> model. It is a simple model of an outcome variable (Support) with varying State intercepts / State random effects:</p>

<pre><code>mlm1 &lt;- lmer(Support ~ (1 | State))
</code></pre>

<p>The results of <code>summary(mlm1)</code> are:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Support ~ (1 | State) 
   AIC   BIC logLik deviance REMLdev
 12088 12107  -6041    12076   12082
Random effects:
 Groups   Name        Variance  Std.Dev.
 State    (Intercept) 0.0063695 0.079809
 Residual             1.1114756 1.054265
Number of obs: 4097, groups: State, 48

Fixed effects:
            Estimate Std. Error t value
(Intercept)  0.13218    0.02159   6.123
</code></pre>

<p>I take it that the variance of the varying state intercepts / random effects is <code>0.0063695</code>. But when I extract the vector of these state random effects and calculate the variance</p>

<pre><code>var(ranef(mlm1)$State)
</code></pre>

<p>The result is: <code>0.001800869</code>, considerably smaller than the variance reported by <code>summary()</code>.</p>

<p>As far as I understand it, the model I have specified can be written:</p>

<p>$y_i = \alpha_0 + \alpha_s + \epsilon_i, \text{ for } i = \{1, 2, ..., 4097\}$</p>

<p>$\alpha_s \sim N(0, \sigma^2_\alpha), \text{ for } s = \{1, 2, ..., 48\}$</p>

<p>If this is correct, then the variance of the random effects ($\alpha_s$) should be $\sigma^2_\alpha$. Yet these are not actually equivalent in my <code>lmer()</code> fit. </p>
"
"0.177380236469607","0.184047122095657"," 68363","<p>A coworker and I are trying to analyze agreement between two measurement methods.  I apologize in advance for needing some extra explanation due to the fact I'm an engineer whose statistics background is mostly geared toward the relationship between signal-to-noise ratio and bit error rates, and other analysis of random processes.</p>

<p>For method comparison, it is natural to create a Bland-Altman plot (and we've done so).  However our data has some additional characteristics that Bland-Altman style analysis doesn't account for.  Furthermore, we're trying to compare our results to an earlier study that published a correlation coefficient resulting from mixed-effect analysis, unfortunately this publication didn't say whether they were reporting Pearson correlation coefficient or Intra-class correlation (maybe there are others too?).</p>

<p>The characteristics of our data set are:</p>

<ul>
<li>Multiple test subjects</li>
<li>Multiple observation instants for each test subject, sequentially ordered and equally spaced in time</li>
<li>The subjects are time varying, but receiving treatment so that the time dependent changes are not monotonic</li>
<li>At each observation instant, one measurement is made using each methods</li>
</ul>

<p>A statistician here at our university warned us that a simple paired analysis wasn't appropriate because there's a subject-specific effect, and pointed us to mixed-effects analysis but couldn't help further.</p>

<p>I read several articles on mixed-effect analysis, but most of them are a comparison of groups, rather than a group of comparisons, if that makes sense.  The information I found on intra-class correlation said it treats the measurements within the class interchangeably, and that seems suspect here.</p>

<p>This article uses mixed-effect analysis for method comparison, but has repeated measurements instead of a series of time-separated measurements.  It also doesn't cover correlation coefficients on grouped data.</p>

<ul>
<li><a href=""http://www.degruyter.com/view/j/ijb.2008.4.1/ijb.2008.4.1.1107/ijb.2008.4.1.1107.xml"" rel=""nofollow"">Statistical Models for Assessing Agreement in Method Comparison Studies with Replicate Measurements</a></li>
</ul>

<p>Here's what I've done so far, using R:</p>

<p>Load the data</p>

<pre><code>data &lt;- read.table(filename, header=TRUE, sep="","");
</code></pre>

<p>Convert variables to cases, adding factors (is it correct to create a factor for the encounter, since the time indicators are independent for each test subject?):</p>

<pre><code>library(""reshape"")
mdata &lt;- within(melt(data, variable_name=""method"", id=c(""subject"", ""time"")), {
  subject &lt;- factor(subject)
  time &lt;- factor(interaction(subject, time))
  method &lt;- factor(method)
})
</code></pre>

<p>Run linear mixed-effects model.  I've chosen an autocorrelation structure for the random subject/time covariance matrices, because of the nice periodic measurements.</p>

<pre><code>library(nlme)
lm2 &lt;- lme(value ~ method, random = list( ~1|subject, ~1|time ), corr = corAR1(), data = mdata)
</code></pre>

<p>I'd like to know whether I've assigned the right factors to fixed and random effects.  Also, from my research I guess there should be a random effect on method*subject, but it shouldn't have AR(1) structure and I don't know how to give different structure to different random effects.</p>

<p>Finally, I did calculate a correlation coefficient, using intra-class correlation and encounter as the class to get measurements paired properly.  But I don't think this is using the subject grouping, and as I said earlier, I don't feel like treating class members interchangeably is right.</p>

<pre><code>library(psychometric)
r2 &lt;- ICC1.lme(value, time, mdata)
</code></pre>

<p>What would you do differently?  It seems like the <code>lmer</code> function was a bit easier to describe random effect nested groups, but I didn't find a way to control the correlation structure.</p>
"
"0.0978566471559948","0.101534616513362"," 70272","<p>I'm trying to generate an autoplot for mer objects in the same vein as the the <a href=""http://librestats.com/2012/06/11/autoplot-graphical-methods-with-ggplot2/"" rel=""nofollow"">autoplot.lm example</a>.</p>

<p>I can extract the original data frame, the residuals and the linear predictors directly from the returned object...</p>

<pre><code>&gt; random.model &lt;- lmer(a ~ b + c + (1 | d), data = example, family = binomial
&gt; diagnostics  &lt;- cbind(random.model@frame, random.model@eta, random.model@resid)
</code></pre>

<p>...and after reading a suggestion <a href=""http://stats.stackexchange.com/questions/54818/how-to-extract-compute-leverage-and-cooks-distances-for-linear-mixed-effects-mo"">here</a> I can calculate Cooks Distance using the influence.ME package...</p>

<pre><code>&gt; library(influence.ME)
&gt; cooks       &lt;- cooks.distance(influence.ME::influence(random.model))
&gt; diagnostics &lt;- cbind(diagnostics, cooks)
</code></pre>

<p>...and elsewhere (sorry can't find link) found that I could derive the standardised residuals using the HLMdiag package...</p>

<pre><code>&gt; library(HLMdiag)
&gt; stdresid    &lt;- HLMresid(random.model, level = 1, standardize = TRUE)
&gt; diagnostics &lt;- cbind(diagnostics, stdresid)
</code></pre>

<p>But I've hit a problem as the autoplot example calls ggplot2's <code>fortify()</code> function to calculate these and two additional measurements, sigma, the estimate of the residual SD when the corresponding observation is dropped from the model and hat, the diagonal of the hat matrix.</p>

<p>Reading around I thought the <code>hatTrace()</code> function would be one part of the solution, but found  posts on <a href=""http://r.789695.n4.nabble.com/Function-hatTrace-in-package-lme4-td4646071.html"" rel=""nofollow"">R-help</a> indicating that it was removed from the lme4 package.</p>

<p>Can anyone advise on how to calculate sigma and hat vectors?</p>

<p>Or if anyone has canned solutions for diagnostic plots for lme4 that would be useful too.</p>
"
"0.0827039616973562","0.0858124131484961"," 71070","<p>I'm modeling the amount of organic content in bird bones (a percentage) in two different conditions and also over two time periods. The design is repeated measures - observations in both conditions and time periods come from the same bone (divided into pieces). I want to test the hypotheses: 1) there is no difference in E(Y) across conditions, 2) there is no difference in E(Y) across time, 3) there is no difference in difference of E(Y) (i.e., time*condition interaction). I've tried the following (here with dummy data):</p>

<pre><code>set.seed(6753)
dat &lt;- data.frame(
    id = rep(1:15, each = 4),
    pc.organic = rnorm(60, 0.11, 0.055),
    condition = factor(rep(c(""raw"", ""advanced""), times = 30)),
    year = factor(rep(c(1, 1, 2, 2), times = 15))
    )

library(lme4)
fm1 &lt;- lmer(pc.organic ~ condition * year + (1 | id), data = dat)
summary(fm1)
</code></pre>

<p>This, I think, is an appropriate model to account for the non-independence of observations in the repeated measures design. I'm unsure, however, whether this is ok given the nature of the response variable. The response varies between about .01 (1%) and .2 (20%). It is bounded at zero (obviously), but also at 40% (this is the maximum amount of organic content in any bone - 60% is inorganic). Another option would be to use 40% as the denominator when I define the percentage, thus, the previous values would be .025 and .5 respectively. However, this would still leave the response bounded between 0 and 1.</p>

<p>I've read about beta regression and also about using a logit transformation to linearize the data. If possible, I'd  like to avoid going down these paths, as other researchers in my field are not familiar with these methods. Any suggestions are most welcome. </p>
"
"NaN","NaN"," 71194","<p>I am now looking for a GLMM, which could fitted a Poisson distribution with a log-link. From what I see until now, <strong>lme4</strong> allow to specify the family and the link function for <code>lmer()</code> model, but the <code>lme()</code> function in <strong>nlme</strong> package doesn't. Is there another way to specify it in <strong>nlme</strong>? </p>

<p>May we also do it when fitting a non linear mixed models with both pacakges? </p>

<p>Thanks in advance, your help, would be appreciated.</p>
"
"0.104613156193188","0.108545070825851"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.0554795041091482","0.0767529556453336"," 73346","<p>I'm analysing data from our experiment. We had participants in 4 groups, each participant was measured 4 times. We measured cortisol in saliva, so it leads us to the linear mixed models, because the individual cortisol levels have different slopes.
I have fitted following model:</p>

<pre><code>lmer1 &lt;- lmer(Cortisol ~ group*measurement + (1|id), data=df)
</code></pre>

<p>I used treatment codig for both categorical variables, because we are interested in differences between 1st measurement in first group with other measurements. </p>

<p>My problem is, that I get strong correlations between factor levels and I'm not sure, how to solve it. Contrast coding would be one solution, but it would answer different question (as I said, we want to compare differences between 1st group,1st measurement and all the others).</p>

<p>This is my correlation matrix for fixed effect from lmer method (lme4 package):</p>

<pre><code>          (Intr) group2 group3 groupP msrmn2 msrmn3 msrmn4 grp2:2 grp3:2 grpP:2 grp2:3 grp3:3 grpP:3 grp2:4 grp3:4
group2      -0.770                                                                                    
group3      -0.650  0.500                                                                             
groupP      -0.557  0.429  0.362                                                                      
measuremnt2 -0.602  0.464  0.391  0.335                                                               
measuremnt3 -0.598  0.460  0.388  0.333  0.521                                                        
measuremnt4 -0.602  0.464  0.391  0.335  0.524  0.521                                                 
grp2:msrmn2  0.461 -0.600 -0.299 -0.257 -0.765 -0.398 -0.401                                          
grp3:msrmn2  0.390 -0.300 -0.589 -0.217 -0.647 -0.337 -0.339  0.495                                   
grpP:msrmn2  0.329 -0.253 -0.214 -0.578 -0.546 -0.284 -0.287  0.418  0.353                            
grp2:msrmn3  0.461 -0.599 -0.300 -0.257 -0.402 -0.772 -0.402  0.519  0.260  0.220                     
grp3:msrmn3  0.383 -0.295 -0.579 -0.213 -0.333 -0.641 -0.333  0.255  0.501  0.182  0.495              
grpP:msrmn3  0.333 -0.256 -0.216 -0.585 -0.290 -0.557 -0.290  0.222  0.188  0.499  0.430  0.357       
grp2:msrmn4  0.462 -0.598 -0.300 -0.257 -0.402 -0.399 -0.767  0.518  0.260  0.220  0.518  0.256  0.223  
grp3:msrmn4  0.390 -0.300 -0.589 -0.217 -0.339 -0.337 -0.647  0.260  0.510  0.185  0.260  0.501  0.188  0.496
grpP:msrmn4  0.329 -0.253 -0.214 -0.578 -0.287 -0.284 -0.546  0.219  0.185  0.493  0.220  0.182  0.499  0.419  0.353
</code></pre>

<p>Do you have suggestions about how to solve this (reduce collinearity/ignore it)?</p>
"
"NaN","NaN"," 76594","<p>I am trying to fit a linear model (<code>lme4::lmer()</code>) to my data in R. I would like to look at a number of things, including ""<strong>scrambling</strong>"" of visual stimuli and ""<strong>intensity</strong>"" of the emotions portrayed therein. These things are stored the ""<strong>scrambling</strong>"" and ""<strong>intensity</strong>"" columns of my dataframe.</p>

<p>To ease your comprehension you may see a graphic plot of my data in <a href=""http://stats.stackexchange.com/questions/76134/determining-best-approximator-based-on-repeated-measurements"">this other thread</a>.</p>

<p>I have been told that linear model results can be compromised if category names are parsed as integers instead of strings by accident. But since these measures (<strong>scrambling</strong> and <strong>intensity</strong>) are kind-of quantitative, I am thinking it may be better to leave them as integers - or maybe even use both approaches separately.</p>

<p>I am however unsure how my interpretation of results should vary depending on whether or not my category IDs are passed as stings or ints.</p>

<p>Could anyone explain this to me?</p>

<p>Also would this differentiation still hold for when I use <code>stats::aov()</code> on the same data?</p>
"
"0.161219701233018","0.15847502081498"," 76980","<p>I'm trying to analyse some data I've recently gotten my hands on, but I'm not entirely sure which model to use. One suggestion has been a Mixed Model, Repeated Measurements ANOVA, but I'm not sure if that such kind of model can answer the questions of interest.</p>

<p><strong>The data</strong>: 
Two individual persons (A and B) have had a lot of different values (V1, V2, V3, ..., Vn) measured four times (At T0, T1, T2 and T3) - The spacing between times differs.</p>

<p>The different values have been grouped into categories (C1, C2, C3, ..., Cn). One value may belong to none, one or multiple categories. Each of the categories have a continuos value (Response_C1,Response_C1, ..., Response_Cn), which is the sum of the measured values belonging to that category. </p>

<p>In addition to this, person B was given a drug at T1.</p>

<p>What I would like to investigate now, is:</p>

<ol>
<li>Is there any observable effect after administering the drug</li>
<li>On which categories did the drug have an effect</li>
<li>If there is an effct on a category, what is the effect size</li>
<li>How does the effect vary over time</li>
<li>If there is an effect, is the effect observed from the drug at T1 still persistant at T3</li>
</ol>

<p>I realise one of the major pitfalls is the lack of both time points and samples, but it would be appreciated if you could suggest any articles/methods for this type of analysis.</p>

<p><strong>What I have tried so far</strong> is just Repeated Measurements ANOVA, using R:</p>

<pre><code>test.aov &lt;- aov(Response_C ~ Category * Timepoint * Treatment + Error(Sample), data=df)
</code></pre>

<p>But I am not sure that the model is correct, neither am I sure that it actually answers my questions, even if I try to model it as a mixed model. </p>

<p>Any help is much appreciated. Please let me know if any additional information is needed</p>

<p><strong>Edit 1:</strong> After doing some more reading, it seems a Generalised Linear Model with a negative binomial distribution (since this kind of data is usually over-dispersed) might be better suited for this kind of data, but I'm still not sure if such a model would answer the questions. Potentially I could fit a model to each individual category, but that would inflate the Type-I error I guess, and so we would need to correct for multiple testing.</p>

<p><strong>Edit 2:</strong> Some more reading, and I thought the <code>lme4</code> R package would be a good way to fit a Linear mixed model to my data, and just do individual comparisons of each category. Here's the model I tried to fit:</p>

<pre><code>lm1 &lt;- lmer(Response ~ Treatment * Timepoint + (1|Subject), data=my_data)
</code></pre>

<p>First off, I'm not sure whether Timepoint should be a factorial or a numerical value. As I mentioned, timepoints are not evenly distributed (To be precise, I have for time 0, 2days, 14 days, 90days), however, the design is balanced. If I enter the Timepoints as a numerical value, I don't get any estimate of what the value is at any given Timepoint, but just some numbers for Correlation of fixed effects, which I can't really use for anything. On the other hand, if I enter the Timepoints as factors, I do get an estimated value for the effect at each timepoint, but I'm not too sure how certain or reliable this value is.</p>
"
"0.0640622132638473","0.0664700094043992"," 77797","<p>I have some time course data which plotted looks like this:</p>

<p><img src=""http://i.stack.imgur.com/nkdV5.png"" alt=""enter image description here""></p>

<p>I have fitted a mixed model to my raw datapoints with R's <code>lme4::lmer</code>, as seen in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7669971/raw/e35bc10d5b6443d28eca44bce8cf17eaa7bf1a8a/TC_lme4"" rel=""nofollow"">this code</a>.</p>

<p>In essence I get the following output for my model:</p>

<p>measurement~condition*time:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 ID       (Intercept) 1118082  1057.4  
 Residual               58680   242.2  
Number of obs: 5784, groups: ID, 12

Fixed effects:
             Estimate Std. Error t value
(Intercept)  2480.379    305.375   8.122
CoIhard       -90.294     12.659  -7.133
Time          -17.326      3.884  -4.461
CoIhard:Time   62.931      5.492  11.458

Correlation of Fixed Effects:
            (Intr) CoIhrd Time  
CoIhard     -0.021              
Time        -0.025  0.611       
CoIhard:Tim  0.018 -0.864 -0.707
</code></pre>

<p>Now, what am I to make of these results? As I predicted in <a href=""http://stats.stackexchange.com/questions/77486/linear-model-or-component-analysis-on-timecourse-data"">this question</a> the factors only get one intercept value each, as if they would follow a steady linear increase/decrease. Obviously my time course does not.</p>

<p>What added information pertaining to the description of the difference between my conditions does fitting this model give me?</p>

<p>Cheers,</p>
"
"0.0827039616973562","0.0686499305187969"," 77891","<p>I ran a repeated design whereby I tested 30 males and 30 females across three different tasks. I want to understand how the behaviour of males and females is different and how that depends on the task. I used both the lmer and lme4 package to investigate this, however, I am stuck with trying to check assumptions for either method. The code I run is</p>

<pre><code>lm.full &lt;- lmer(behaviour ~ task*sex + (1|ID/task), REML=FALSE, data=dat)
lm.full2 &lt;-lme(behaviour ~ task*sex, random = ~ 1|ID/task, method=""ML"", data=dat)
</code></pre>

<p>I checked if the interaction was the best model by comparing it with the simpler model without the interaction and running an anova:</p>

<pre><code>lm.base1 &lt;- lmer(behaviour ~ task+sex+(1|ID/task), REML=FALSE, data=dat)
lm.base2 &lt;- lme(behaviour ~ task+sex, random= ~1|ID/task), method=""ML"", data=dat)
anova(lm.base1, lm.full)
anova(lm.base2, lm.full2)
</code></pre>

<p>Q1: Is it ok to use these categorical predictors in a linear mixed model?<br/>
Q2: Do I understand correctly it is fine the outcome variable (""behaviour"") does not need to be normally distributed itself (across sex/tasks)?<br/>
Q3: How can I check homogeneity of variance? For a simple linear model I use <code>plot(LM$fitted.values,rstandard(LM))</code>. Is using <code>plot(reside(lm.base1))</code> sufficient?<br/>
Q4: To check for normality is using the following code ok?</p>

<pre><code>hist((resid(lm.base1) - mean(resid(lm.base1))) / sd(resid(lm.base1)), freq = FALSE); curve(dnorm, add = TRUE)
</code></pre>
"
"0.143247463647051","0.138722695527068"," 79684","<p>Trying to fit a linear mixed effects model with 2 categorical predictors (group &amp; worker) where worker is a random effect and group a fixed effect.  I'm trying to figure out 1) whether I should specify intercept=0 and 2) why these 2 model results seem to give different conclusions about the effect of group.</p>

<p>Model1: tps ~ group + (1 | worker)</p>

<p>Model2: tps ~ group + (1 | worker) + 0</p>

<pre><code>summary(Model1):
Linear mixed model fit by REML ['merModLmerTest']
Formula: tps ~ group + (1 | worker) 
   Data: mydata 

REML criterion at convergence: 3489.872 

Random effects:
 Groups   Name        Variance Std.Dev.
 worker   (Intercept) 1866     43.20   
 Residual             3165     56.26   
Number of obs: 318, groups: worker, 18

Fixed effects:
            Estimate Std. Error     df t value Pr(&gt;|t|)    
(Intercept)    70.15      15.59  11.27   4.501 0.000848 ***
group phone   -20.85      21.75  10.83  -0.959 0.358586    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr)
group phone -0.717

summary(Model2):
Linear mixed model fit by REML ['merModLmerTest']
Formula: tps ~ group + (1 | worker) + 0 
   Data: mydata 

REML criterion at convergence: 3489.872 

Random effects:
 Groups   Name        Variance Std.Dev.
 worker   (Intercept) 1866     43.20   
 Residual             3165     56.26   
Number of obs: 318, groups: worker, 18

Fixed effects:
               Estimate Std. Error    df t value Pr(&gt;|t|)    
group computer    70.15      15.59 11.27   4.501 0.000848 ***
group phone       49.30      15.17 10.40   3.251 0.008291 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            grpcmp
group phone 0.000 
</code></pre>

<p>In the first model the 'phone' effect is the same as the difference between the two groups' effects in model 2 (this makes sense because in model1 the 'computer' group is the baseline).  In model2, both groups' effects are significant, whereas in model1 only the intercept is significant.  </p>

<p>Which is the ""right"" model for a situation where the group predictor is binary?  It must be only one or the other (seems to indicate that model1 is correct, because there the intercept ""is the same as"" the computer group, right? Model2 allows a ""zero"" value for group which doesn't make sense).  Am I right about this?</p>

<p>And how to interpret the fact that in model1 the intercept is significant but 'phone' is not?</p>
"
"0.152498570332605","0.158230271602029"," 80866","<p>After weeks of reading and trying I decided to post my question here because I could not find a convincing solution.
I radio tracked two animals for several months and now I want to find out 1) what influences the activity of the animals and 2) when (hour after sunset) they are showing the highest activity and 3) what influences the travel distance.</p>

<p>For question 1) I created a generalized linear mixed model, with animal as a random factor looking like this:
glmer(cbind(active,inactive)~offspring+season+observation_time+temperature+precipitation+season:temperature+season:precipitation+temperature:precipitation,family=binomial)</p>

<p>offspring and season are factors coded with 0 and 1,observation_time in minutes, temperature is in Â°C and precipitation in mm.</p>

<p>The first lines of the summary() are showing that:</p>

<p>AIC       BIC    logLik  deviance 
 438.3251  460.0690 -209.1625  418.3251 </p>

<p>Random effects:
 Groups Name        Variance Std.Dev.
 Name   (Intercept) 0.06594  0.2568<br>
Number of obs: 65, groups: Name, 2</p>

<p>So my question is: is this model build up correctly? Is there an improvement possible or neccessary? 
It is very difficult to work with this data, because most diagnostic plots which are usually used to evaluate models are different because of the random factor.
I also wanted to boxcox transform the response but one animal showed no activity one day (the activity is zero) and therefore this is not possible.
I try to eliminate variables or interaction terms but in most cases I can only eliminate one interaction term. After that all variables and interaction terms seems to be significant. For variable selection I use differend aproaches (AIC, BIC, AVOVA).</p>

<p>For the 3) question I created a LMM like this: 
lmer(sqrt(distance)~aktivity+season+offspring+observation_time+temperature+precipitation+season:temperature+...(other interaction terms)</p>

<p>The result from summary():</p>

<p>REML criterion at convergence: 513.0257 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Name     (Intercept)  11.6     3.405<br>
 Residual             295.8    17.199<br>
Number of obs: 62, groups: Name, 2</p>

<p>Does any of this values tell me something about the goodness of my model?</p>

<p>How can I check if data transformation is neccessary? And if yes, which one?</p>

<p>I'm honest, I'm quite new in this field and all I learnd about R and statistics do not really work with glmm or lmm. Or it's to complicated for me.
I also created a gam without the random factor to check the relationship between the variables and response but I don't know what to do with the results (seems to be no linear relationship between activity and observation_time and rainfall).
How do I fit variables to my model? 
An other idea is to fit the model without the random factor and add the random factor afterwards. Would it be ok to do it like this?</p>

<p>For the second question - at which hour after sunset they are showing the highest activity - I have no idea how the model could be build up...</p>

<p>Sorry for the amount of questions but I'm working for weeks on this and it is very frustrating...
Thanks in advance for all your ideas and help!
Iris </p>
"
"0.106770355439746","0.121861683908065"," 81430","<p>I have a mixed model and the data looks like this:</p>

<pre><code>&gt; head(pce.ddply)
  subject Condition errorType     errors
1    j202         G         O 0.00000000
2    j202         G         P 0.00000000
3    j203         G         O 0.08333333
4    j203         G         P 0.00000000
5    j205         G         O 0.16666667
6    j205         G         P 0.00000000
</code></pre>

<p>Each subject provides two datapoints for errorType (O or P) and each subject is in either Condition G (N=30) or N (N=33).  errorType is a repeated variable and Condition is a between variable.  I'm interested in both main effects and the interactions.  So, first an anova:</p>

<pre><code>&gt; summary(aov(errors ~ Condition * errorType + Error(subject/(errorType)),
                 data = pce.ddply))

Error: subject
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Condition  1 0.00507 0.005065   2.465  0.122
Residuals 61 0.12534 0.002055               

Error: subject:errorType
                    Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
errorType            1 0.03199 0.03199   10.52 0.001919 ** 
Condition:errorType  1 0.04010 0.04010   13.19 0.000579 ***
Residuals           61 0.18552 0.00304                     
</code></pre>

<p>Condition is not significant, but errorType is, as well as the interaction.</p>

<p>However, when I use lmer, I get a totally different set of results:</p>

<pre><code>&gt; lmer(errors ~ Condition * errorType + (1 | subject),
                    data = pce.ddply)
Linear mixed model fit by REML 
Formula: errors ~ Condition * errorType + (1 | subject) 
   Data: pce.ddply 
    AIC    BIC logLik deviance REMLdev
 -356.6 -339.6  184.3     -399  -368.6
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.000000 0.000000
 Residual             0.002548 0.050477
Number of obs: 126, groups: subject, 63

Fixed effects:
                       Estimate Std. Error t value
(Intercept)            0.028030   0.009216   3.042
ConditionN             0.048416   0.012734   3.802
errorTypeP             0.005556   0.013033   0.426
ConditionN:errorTypeP -0.071442   0.018008  -3.967

Correlation of Fixed Effects:
            (Intr) CndtnN errrTP
ConditionN  -0.724              
errorTypeP  -0.707  0.512       
CndtnN:rrTP  0.512 -0.707 -0.724
</code></pre>

<p>So for lmer, Condition and the interaction are significant, but errorType is not.</p>

<p>Also, the lmer result is exactly the same as a glm result, leading me to believe something is wrong.</p>

<p>Can someone please help me understand why they are so different?  I suspect I am using lmer incorrectly (though I've tried many other versions like (errorType | subject) with similar results.</p>

<p>(I have seen researchers use both approaches in the literature with similar data.)</p>
"
"0.261952873983836","0.271798445762429"," 82102","<p>I hope this is an appropriate forum to post this question. I recently upgraded my R software from 2.15.0 to 3.0.2. I also upgraded the lme4 package from .999999-0 to 1.1-2. After doing so, the results from one of my linear mixed models analyses have changed a bit unexpectedly. In some respects, I was expecting some change, as the lme4 developers very clearly stated that they had made some significant changes to some fundamental components in the package. However, the changes that I am seeing (described below) make me think that something else is awry. I will start by explaining the experimental design, which is quite simple and then the issue at hand.</p>

<p>My experiment is a basic repeated measures design. I used 24 ""Items"" that each appeared in three different ""Conditions"" (SmallClause_Som, NoSmallClause, SmallClause_NoSom). Levels of Condition were rotated across three presentation lists such that each Subject (45 total, each assigned to a particular list) only saw one level of each item.</p>

<p>I used lmer() for the analysis. Condition was entered in as a Fixed effect and ""Subject"" and ""Item"" were entered as Random effects.</p>

<p>The problem:
Using the current version of R 3.0.2 and lme4 1.1-2 with NoSmallClause as the reference level (and no weighting on any of the contrasts), the ConditionSmallClause_Som/NoSmallClause contrast produces a t value of 1.680. </p>

<p>But, when I change reference level to SmallClause_Som (to observe the one remaining contrast) I get not only a change in the polarity of the effect (plus to minus, as expected), but the values change as well.</p>

<p>When I use R 2.15.0 and lme4 .999999-0 (on another computer), I do not experience this issue. I get slightly different values, but they do not change (apart from the polarity) when I change reference level.</p>

<p>My colleague also tried my analysis for me using R 3.0.2 and a version of lme4 (pre version 1.0) (I don't know exactly which version, but it was before the major changes) and he also does not experience the issue.</p>

<p>R 2.15.0 lme4 1.1-2 (older) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
 AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance   Std.Dev. Corr          
 Subject  (Intercept)                0.98998765 0.994981               
          ConditionSmallClause_Som   0.00203374 0.045097 -1.000        
          ConditionSmallClause_NoSom 0.00019873 0.014097  1.000 -1.000 
 Item     (Intercept)                0.96231875 0.980978               
          ConditionSmallClause_Som   0.89924400 0.948285 -0.020        
          ConditionSmallClause_NoSom 0.62128577 0.788217 -0.256  0.361 
 Residual                            1.68810777 1.299272               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                       Estimate Std. Error t value
(Intercept)                  2.9583     0.2584  11.447
ConditionSmallClause_Som     0.3639     0.2165   1.680
ConditionSmallClause_NoSom   0.1472     0.1878   0.784

Correlation of Fixed Effects:
            (Intr) CnSC_S
CndtnSmlC_S -0.116       
CndtnSmC_NS -0.260  0.392

&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
  AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance  Std.Dev. Corr          
 Subject  (Intercept)                0.9023239 0.949907               
          ConditionNoSmallClause     0.0020340 0.045099 1.000         
          ConditionSmallClause_NoSom 0.0035039 0.059194 1.000  1.000  
 Item     (Intercept)                1.8238288 1.350492               
          ConditionNoSmallClause     0.8992237 0.948274 -0.687        
          ConditionSmallClause_NoSom 0.9804329 0.990168 -0.604  0.670 
 Residual                            1.6881050 1.299271               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3174  10.468
ConditionNoSmallClause      -0.3639     0.2165  -1.680
ConditionSmallClause_NoSom  -0.2167     0.2243  -0.966

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.588       
CndtnSmC_NS -0.521  0.638
</code></pre>

<p>R 3.0.2 and lme4 1.1-2 (newer) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3942.557  4022.312 -1955.278  3910.557 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.9522   0.9758              
          ConditionSmallClause_NoSom 0.1767   0.4204    0.03      
          ConditionSmallClause_Som   0.1760   0.4196   -0.15  0.92
 Item     (Intercept)                1.2830   1.1327              
          ConditionSmallClause_NoSom 0.7782   0.8822   -0.41      
          ConditionSmallClause_Som   1.4901   1.2207    0.09  0.41
 Residual                            1.6466   1.2832              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  2.9583     0.2814  10.512
ConditionSmallClause_NoSom   0.1472     0.2133   0.690
ConditionSmallClause_Som     0.3639     0.2741   1.327

Correlation of Fixed Effects:
            (Intr) CSC_NS
CndtnSmC_NS -0.357       
CndtnSmlC_S -0.007  0.451
&gt; #anova (test.lmer3, test.lmer4)
&gt; 
&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)
Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3951.357  4031.113 -1959.679  3919.357 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.88980  0.9433              
          ConditionNoSmallClause     0.04299  0.2073   0.83       
          ConditionSmallClause_NoSom 0.01562  0.1250   0.90  0.67 
 Item     (Intercept)                2.39736  1.5483              
          ConditionNoSmallClause     0.72053  0.8488   -0.04      
          ConditionSmallClause_NoSom 1.87804  1.3704   -0.16  0.53
 Residual                            1.65166  1.2852              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3525   9.425
ConditionNoSmallClause      -0.3639     0.2004  -1.816
ConditionSmallClause_NoSom  -0.2167     0.2963  -0.731

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.045       
CndtnSmC_NS -0.160  0.514
</code></pre>

<p>My question:
What is going on here? Why is changing the reference level producing a shift from 1.327 to -1.816 in the t scores for the new version of lme4 whereas it produces the same (disregarding sign) value of 1.680/-1.680 in the old version's t scores? Only the older version seems to make sense to me.</p>

<p>1) Am I specifying my model incorrectly for the new version of lme4?</p>

<p>2) Am I missing some basic fundamental fact about how contrasts work? That is, is it possible to get different values just from changing the reference level? (the correlation values look a bit odd in the newer output).</p>

<p>3) Is this a bug in lme4?</p>

<p>4) Some other explanation...?</p>

<p>I have had some other odd issues as well with this same analysis using lme4 1.1-2. For example, if I don't clear the workspace and re-run an analysis, the values also will change between analyses (and also within the analysis as I change the reference level). This never happened to me on the earlier version (and it still does not happen when I run it on the earlier version now).</p>

<p>I hope someone can help with this. I found two other similar questions online (after much searching) but neither had any informative responses.</p>

<p>Thanks DT</p>
"
"0.133697632737248","0.148631459493287"," 82379","<p>I want to test the fixed and random effects of some covariates on a discrete variable with non negative values. In exploratory analysis I fitted a null Poisson GLM and an null Poisson GLMM. However, the GLMM underestimated the mean value of the response variable even after inclusion of fixed and/or random covariates. I also tried Bayesian approaches, zero-inflated models and negative binomial distributions but the ""problem"" remains.</p>

<p>Response variable mean: 0.7804<br>
GLM intercept: 0.7803772<br>
GLMM intercept: 0.6595108</p>

<p>Is the estimated intercept of the GLMM an indicative of poor fitting of the model? </p>

<pre><code>summary(banco2$caes)  
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.   
 0.0000  0.0000  0.0000  0.7804  1.0000 12.0000  


mod1 &lt;- glm(caes ~ 1, poisson, banco2)  
Deviance Residuals:  
    Min       1Q   Median       3Q      Max    
-1.2493  -1.2493  -1.2493   0.2381   6.5689  
Coefficients:  
            Estimate Std. Error z value Pr(&gt;|z|)      
(Intercept) -0.24798    0.01078  -23.01   &lt;2e-16 ***  
---  
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1  
(Dispersion parameter for poisson family taken to be 1)  
    Null deviance: 15304  on 11027  degrees of freedom  
Residual deviance: 15304  on 11027  degrees of freedom  
AIC: 27654  
Number of Fisher Scoring iterations: 5  

exp(mod1$coefficients[1])  
(Intercept)  
  0.7803772  


(mod2 &lt;- lmer(caes ~ 1 + (1 | setor), poisson, data = banco2))  
Generalized linear mixed model fit by the Laplace approximation  
Formula: caes ~ 1 + (1 | setor)  
   Data: banco2  
   AIC   BIC logLik deviance  
 13575 13590  -6785    13571  
Random effects:  
 Groups Name        Variance Std.Dev.  
 setor  (Intercept) 0.39817  0.63101  
Number of obs: 11028, groups: setor, 559  
Fixed effects:  
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -0.41626    0.02937  -14.18   &lt;2e-16 ***  

exp(fixef(mod2))  
(Intercept)  
  0.6595108  
</code></pre>

<p>Best regards!</p>
"
"0.12266979912335","0.11570943428471"," 82984","<p>I am currently running some mixed effect linear models. </p>

<p>I am using the package ""lme4"" in R. </p>

<p>My models take the form:</p>

<pre><code>model &lt;- lmer(response ~ predictor1 + predictor2 + (1 | random effect))
</code></pre>

<p>Before running my models, I checked for possible multicollinearity between predictors.</p>

<p>I did this by:</p>

<p>Make a dataframe of the predictors</p>

<pre><code>dummy_df &lt;- data.frame(predictor1, predictor2)
</code></pre>

<p>Use the ""cor"" function to calculate Pearson correlation between predictors.</p>

<pre><code>correl_dummy_df &lt;- round(cor(dummy_df, use = ""pair""), 2) 
</code></pre>

<p>If ""correl_dummy_df"" was greater than 0.80, then I decided that predictor1 and predictor2 were too highly correlated and they were not included in my models.</p>

<p>In doing some reading, there would appear more objective ways to check for multicollinearity.   </p>

<p>Does anyone have any advice on this?</p>

<p>The ""Variance Inflation Factor (VIF)"" seems like one valid method. </p>

<p>VIF can be calculated using the function ""corvif"" in the AED package (non-cran). The package can be found at <a href=""http://www.highstat.com/book2.htm"">http://www.highstat.com/book2.htm</a>. The package supports the following book:</p>

<p>Zuur, A. F., Ieno, E. N., Walker, N., Saveliev, A. A. &amp; Smith, G. M. 2009. Mixed effects models and extensions in ecology with R, 1st edition. Springer, New York.</p>

<p>Looks like a general rule of thumb is that if VIF is > 5, then multicollinearity is high between predictors. </p>

<p>Is using VIF more robust than simple Pearson correlation?</p>

<p><strong>Update</strong></p>

<p>I found an interesting blog at: </p>

<p><a href=""http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/"">http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/</a></p>

<p>The blogger provides some useful code to calculate VIF for models from the lme4 package.</p>

<p>I've tested the code and it works great. In my subsequent analysis, I've found that multicollinearity was not an issue for my models (all VIF values &lt; 3). This was interesting, given that I had previously found high Pearson correlation between some predictors. </p>

<p>Kind Regards,
Matt.</p>
"
"0.0905976508333704","0.0940027887907685"," 83044","<p>I'm working with linear mixed models. I used <code>plotlmer.func</code>, in the package <code>lmerConvenienceFunctions</code> to build a graphical representation of my model. I simply want to ask what the â€œverbose=TRUEâ€ parameter tell me.</p>

<pre><code>plotLMER.fnc(model, xlabel=NA, xlabs=NA, ylabel=NA, ylimit=NA, ilabel=NA, fun=NA, 
             pred=NA, control=NA, ranefs=NA, n=100, intr=NA, lockYlim=TRUE, 
             addlines=FALSE, withList=FALSE, cexsize=0.5, linecolor=1, 
             addToExistingPlot=FALSE, verbose=TRUE, ...)
</code></pre>

<p>Here is the explanation of the parameter: </p>

<blockquote>
  <p>verbose: if TRUE (default), effect sizes and default transformations are reported.</p>
</blockquote>

<p>I want to know how this effect sizes is calculated and precisely what is referred to and what it can say for my data. In my analysis I have an interaction between two groups and a continuous factor. I have a statistical difference for the factor group x factor but I want to know what this information of the effect size can say  about the different trend of my groups and if it can be useful in the report of the experiment.</p>
"
"0.161219701233018","0.149670852991926"," 83458","<p>My question is about the best way to estimate the effect of a predictor on a dependent variable, while accounting for several other predictors that may correlate with the predictor of interest. I'm using a linear mixed-effects model, using the <code>lmer</code> function from the R <code>lme4</code> package. (Warning: I'm fairly new at this, so their may be some misunderstandings woven through my question.)</p>

<h2>The problem</h2>

<p>To make things a bit more specific, I'll just explain the actual data that I'm working with. I have eye-movement data of participants freely viewing natural scenes. I want to determine whether pupil size predicts the 'visual saliency' (i.e. the conspicuity) of the locations in the image that participants are looking at. But there are many other things that correlate with pupil size, such as luminosity, and this makes the analysis tricky (or does it?).</p>

<h2>Option 1 (simple): Looking at fixed effects</h2>

<p>One option would be to simply create a linear mixed-effects model that has all predictors of saliency that I can think of, including the predictor of interest (<code>pupil_size</code>), as fixed effects and <code>subject</code> and <code>scene</code> as random effects. (To keep things manageable, I'm using a purely additive model, although I suppose that this is a whole topic in itself.)</p>

<pre><code>my_lmer = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>This will give me a t-value for the fixed effect <code>pupil_size</code>. From what I understand, this fixed effect will already be partial, so it's the unique predictive power of pupil size, with any correlations between fixed effects already taken into account. Is my understanding correct?</p>

<h2>Option 2 (complex): Using model comparison</h2>

<p>An alternative approach, which I have from <a href=""http://www.sciencedirect.com/science/article/pii/S0749596X07001398"">Baayen et al. (2008)</a>, is to compare a model without pupil size as fixed effect (<code>simple_model</code>) to a model with pupil size as fixed effect (<code>complex_model</code>).</p>

<pre><code>simple_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + (1|subject) + (1|scene))
complex_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>Now I can use the <code>anova</code> function to compare these two models (see Baayen's paper for an example). This will give me a <code>Chisq Chi</code> value, and I can use this to determine whether adding <code>pupil_size</code> as fixed effect is a justified addition to the model.</p>

<p>Clearly, this model comparison approach is more complex than simply looking at the t-values for fixed effects in a single model. And it seems to me that if <code>pupil_size</code> is a significant predictor (per Option 1), then it must also be a significant addition to the model (per Option 2).</p>

<p>In sum, my question is: <em>Is there any reason to do a model comparison (Option 2), or am I better off just creating a single linear mixed-effects model and seeing whether the t-value associated with <code>pupil_size</code> as fixed effect is sufficiently high (Option 1)?</em></p>
"
"0.0739726721455309","0.0767529556453336"," 85921","<p>I am running a linear mixed effects model in R using (lme4). I have two independent variables: word types (five levels) and lexicality (two levels), and one dependent variable; reaction time. I used the following formula to look at the overall interaction between the dependent variables.</p>

<pre><code>mydata.mod1=lmer(RT~lexicality*wordType*(1|Item)+(1|Subject), mydata)
summary(mydata.mod1)
</code></pre>

<p>My question now is about the possibility of looking at each level under word types separately and compare it to lexicality based on reaction time.  For example, I want to take Type Two (under word types) and compare it to lexicality. What terms should I include to run this type of analysis. I have tried the following formula, but it did not show what I needed:</p>

<pre><code>mydata.mod2=lmer(RT~wordType*(1+lexicality|Subject)+(1|Item),mydata)
</code></pre>

<p>Is there something I missed?</p>
"
"0.199177515363906","0.192410991737755"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.166095693194512","0.172338446116409"," 86889","<p>I calculated the least-squares means and standard errors for a linear mixed model. I am attempting to plot the lsmeans and standard errors for the combinations of the two factors, but I notice a discrepancy in what is allegedly significant.</p>

<pre><code>&gt; library(lmerTest)

&gt; print(summary(data_dist))
   ID        interface    direction    error_dist      
12     : 18   fs     :108   depth :72   Min.   :-0.34375  
13     : 18   none   :108   height:72   1st Qu.:-0.13037  
14     : 18   RW_none:  0   width :72   Median :-0.04048  
15     : 18                             Mean   :-0.03464  
16     : 18                             3rd Qu.: 0.06022  
17     : 18                             Max.   : 0.36864  
(Other):108

&gt; print(summary(model_error_dist))
Linear mixed model fit by REML ['merModLmerTest']
Formula: error_dist ~ direction * interface + (1 | ID) + (1 | ID:direction) 
Data: data_master 

REML criterion at convergence: -320.9162 

Random effects:
Groups       Name        Variance Std.Dev.
ID:direction (Intercept) 0.005864 0.07658 
ID           (Intercept) 0.003395 0.05827 
Residual                 0.008463 0.09199 
Number of obs: 216, groups: ID:direction, 36; ID, 12

Fixed effects:
                                Estimate Std. Error         df t value Pr(&gt;|t|)  
(Intercept)                     0.019411   0.031728  35.080000   0.612   0.5446  
directionheight                -0.074568   0.038046  31.210000  -1.960   0.0590 .
directionwidth                 -0.058316   0.038046  31.210000  -1.533   0.1354  
interfacenone                  -0.037312   0.021683 176.810000  -1.721   0.0870 .
directionheight:interfacenone  -0.008412   0.030664 176.810000  -0.274   0.7841  
directionwidth:interfacenone    0.061797   0.030664 176.810000   2.015   0.0454 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) drctnh drctnw intrfc drctnh:
directnhght -0.600                             
directnwdth -0.600  0.500                      
interfacenn -0.342  0.285  0.285               
drctnhght:n  0.242 -0.403 -0.201 -0.707        
drctnwdth:n  0.242 -0.201 -0.403 -0.707  0.500 

&gt; st &lt;- step(model_error_dist)

&gt; print(st)

Random effects:
                   Chi.sq Chi.DF elim.num p.value
(1 | ID)             3.28      1     kept    0.07
(1 | ID:direction)  37.83      1     kept  &lt;1e-07

Fixed effects:
                    Sum Sq Mean Sq NumDF  DenDF F.value elim.num Pr(&gt;F)
direction           0.0446  0.0223     2  22.00  2.6379     kept 0.0940
interface           0.0206  0.0206     1 176.81  2.4308     kept 0.1208
direction:interface 0.0529  0.0265     2 176.81  3.1263     kept 0.0463

Least squares means:
                                 direction interface Estimate Standard Error   DF t-value Lower CI Upper CI p-value   
direction  depth                       1.0        NA   0.0008         0.0298 27.4    0.03  -0.0604   0.0619  0.9800   
direction  height                      2.0        NA  -0.0780         0.0298 27.4   -2.62  -0.1392  -0.0169  0.0143 * 
direction  width                       3.0        NA  -0.0267         0.0298 27.4   -0.89  -0.0878   0.0345  0.3790   
interface  fs                           NA       1.0  -0.0249         0.0229 12.8   -1.09  -0.0744   0.0246  0.2971   
interface  none                         NA       2.0  -0.0444         0.0229 12.8   -1.94  -0.0939   0.0051  0.0747 . 
direction:interface  depth fs          1.0       1.0   0.0194         0.0317 35.1    0.61  -0.0450   0.0838  0.5446   
direction:interface  height fs         2.0       1.0  -0.0552         0.0317 35.1   -1.74  -0.1196   0.0092  0.0909 . 
direction:interface  width fs          3.0       1.0  -0.0389         0.0317 35.1   -1.23  -0.1033   0.0255  0.2283   
direction:interface  depth none        1.0       2.0  -0.0179         0.0317 35.1   -0.56  -0.0823   0.0465  0.5762   
direction:interface  height none       2.0       2.0  -0.1009         0.0317 35.1   -3.18  -0.1653  -0.0365  0.0031 **
direction:interface  width none        3.0       2.0  -0.0144         0.0317 35.1   -0.45  -0.0788   0.0500  0.6523   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Differences of LSMEANS:
                                             Estimate Standard Error    DF t-value Lower CI Upper CI p-value   
direction depth-height                            0.1         0.0348  22.0    2.26   0.0066   0.1510   0.034 * 
direction depth-width                             0.0         0.0348  22.0    0.79  -0.0448   0.0996   0.439   
direction height-width                           -0.1         0.0348  22.0   -1.47  -0.1236   0.0209   0.154   
interface fs-none                                 0.0         0.0125 176.8    1.56  -0.0052   0.0442   0.121   
direction:interface  depth fs- height fs          0.1         0.0380  31.2    1.96  -0.0030   0.1521   0.059 . 
direction:interface  depth fs- width fs           0.1         0.0380  31.2    1.53  -0.0193   0.1359   0.135   
direction:interface  depth fs- depth none         0.0         0.0217 176.8    1.72  -0.0055   0.0801   0.087 . 
direction:interface  depth fs- height none        0.1         0.0380  31.2    3.16   0.0427   0.1979   0.004 **
direction:interface  depth fs- width none         0.0         0.0380  31.2    0.89  -0.0437   0.1114   0.381   
direction:interface  height fs- width fs          0.0         0.0380  31.2   -0.43  -0.0938   0.0613   0.672   
direction:interface  height fs- depth none        0.0         0.0380  31.2   -0.98  -0.1148   0.0403   0.335   
direction:interface  height fs- height none       0.0         0.0217 176.8    2.11   0.0029   0.0885   0.036 * 
direction:interface  height fs- width none        0.0         0.0380  31.2   -1.07  -0.1183   0.0368   0.292   
direction:interface  width fs- depth none         0.0         0.0380  31.2   -0.55  -0.0986   0.0566   0.585   
direction:interface  width fs- height none        0.1         0.0380  31.2    1.63  -0.0156   0.1396   0.113   
direction:interface  width fs- width none         0.0         0.0217 176.8   -1.13  -0.0673   0.0183   0.260   
direction:interface  depth none- height none      0.1         0.0380  31.2    2.18   0.0054   0.1606   0.037 * 
direction:interface  depth none- width none       0.0         0.0380  31.2   -0.09  -0.0811   0.0741   0.928   
direction:interface  height none- width none     -0.1         0.0380  31.2   -2.27  -0.1640  -0.0089   0.030 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Final model:
lme4::lmer(formula = error_dist ~ direction + interface + (1 | 
    ID) + (1 | ID:direction) + direction:interface, data = data_master, 
    REML = reml, contrasts = l)
</code></pre>

<p>Plotting the lsmeans and standard errors gives me this graph:
<img src=""http://i.stack.imgur.com/YBzp4.png"" alt=""plot of least-squares means and standard errors""></p>

<p>From this plot, I can see that my standard error is huge and most comparisons between means will likely be insignificant. However, in the ""Differences of LSMEANS"" section of the output above, I see four significant differences.</p>

<p>This one I expect, based on the plot above:</p>

<pre><code>direction:interface  depth fs- height none        0.1         0.0380  31.2    3.16   0.0427   0.1979   0.004 **
</code></pre>

<p>This one, for example, surprises me:</p>

<pre><code>direction:interface  height fs- height none       0.0         0.0217 176.8    2.11   0.0029   0.0885   0.036 *
</code></pre>

<p>It seems I don't actually understand what these outputs are telling me. Can somebody please explain the reason for this discrepancy? Thanks!</p>
"
"0.143247463647051","0.148631459493287"," 87050","<p>I want to fit a model without a correlation term between the random effects with <code>lme</code>. In <code>lmer</code> this is fairly straighforward....</p>

<pre><code># lmer without correlation term
m1 &lt;- lmer(distance ~ (1|Subject) + age + (0+age|Subject) + Sex, data = Orthodont)
VarCorr(m1)
# Groups    Name        Std.Dev.
# Subject   (Intercept) 1.474105
# Subject.1 age         0.099979
# Residual              1.402591
</code></pre>

<p>With <code>lme</code> I think I can drop the correlation term using the following specification...</p>

<pre><code># lme without correlation term?
m2 &lt;- lme(distance ~ age + Sex, data = Orthodont, random = list(~ 1 | Subject, ~-1+ age | Subject))
VarCorr(m2)
#             Variance            StdDev    
# Subject =   pdLogChol(1)                  
# (Intercept) 2.172946296         1.47409169
# Subject =   pdLogChol(-1 + age)           
# age         0.009996006         0.09998003
# Residual    1.967260819         1.40259075
</code></pre>

<p>I am not entirely convinced that these are the same models though, partly because I can not find any resources that details how to specify this specific form and partly because the output from <code>print</code> is a little mystifying to me...</p>

<pre><code>m2 
# Linear mixed-effects model fit by REML
#   Data: Orthodont 
#   Log-restricted-likelihood: -218.3227
#   Fixed: distance ~ age + Sex 
# (Intercept)         age   SexFemale 
#  17.5806928   0.6601852  -2.0117005 
# 
# Random effects:
#  Formula: ~1 | Subject
#         (Intercept)
# StdDev:    1.474092
# 
#  Formula: ~-1 + age | Subject %in% Subject
#                age Residual
# StdDev: 0.09998003 1.402591
# 
# Number of Observations: 108
# Number of Groups: 
#                Subject Subject.1 %in% Subject 
#                     27                     27 
</code></pre>

<p>In particular, what is <code>Subject %in% Subject</code> referring to? and why are the residual considered as part of the second random effect term?</p>
"
"0.0369863360727655","0.0383764778226668"," 87445","<p>I have fitted random coefficient Poisson analysis in R. I have obtained the following results: </p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )</p>

<p>Formula: frequency ~ 1 + cc + ageveh + make + (1 | AREA) </p>

<p>Data: x </p>

<pre><code>  AIC       BIC    logLik  deviance 
</code></pre>

<p>1359.1477 1389.7370 -672.5739 1345.1477 </p>

<p>Random effects:</p>

<p>Groups Name        Variance Std.Dev.</p>

<p>AREA   (Intercept) 1.323    1.15 </p>

<p>Number of obs: 584, groups: AREA, 8</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|) </p>

<p>(Intercept) -0.12902    0.44432  -0.290   0.7715 </p>

<p>ccL          0.05656    0.12371   0.457   0.6475</p>

<p>agevehO      0.02136    0.09264   0.231   0.8177</p>

<p>make2       -0.45454    0.20632  -2.203   0.0276 *</p>

<p>make3       -0.31799    0.21422  -1.484   0.1377 </p>

<h2>make4       -0.29708    0.14469  -2.053   0.0401 *</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr) ccL    agevhO make2  make3
</code></pre>

<p>ccL      0.052    </p>

<p>agevehO -0.179 -0.232   </p>

<p>make2   -0.171 -0.007 -0.001 </p>

<p>make3   -0.156  0.022 -0.078  0.366 </p>

<p>make4   -0.300 -0.235  0.167  0.544  0.522</p>

<p>However I am unable to interpret the results. </p>
"
"0.0827039616973562","0.0858124131484961"," 87510","<p>I have obtained the following results in R from a random coefficient Poisson analysis.</p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )
Formula: frequency ~ 1 + insgen + ageveh + make + area + (1 | ID) 
   Data: Panel </p>

<pre><code>  AIC       BIC    logLik   deviance 
</code></pre>

<p>1099.9670 1134.9262  -541.9835  1083.9670 </p>

<p>Random effects:</p>

<p>Groups Name          Variance   Std.Dev.</p>

<p>ID  (Intercept)     1.551e-11     3.939e-06</p>

<p>Number of obs: 584, groups: ID, 584</p>

<p>Fixed effects:</p>

<pre><code>          Estimate    Std. Error   z value   Pr(&gt;|z|)  
</code></pre>

<p>(Intercept)    -22.98292   8432.07738    -0.003     0.9978  </p>

<p>insgenM          0.02616     0.08806      0.297     0.7664  </p>

<p>ageveho          0.05889     0.08586      0.686     0.4928 </p>

<p>make             -0.10447    0.04126     -2.532    0.0113 *</p>

<p>area1             23.68571  8432.07738   0.003    0.9978  </p>

<p>area2             23.85969  8432.07738   0.003    0.9977 </p>

<p>area3             23.77374  8432.07738   0.003   0.9978  </p>

<hr>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr)  insgnM  ageveh  make    area1   area2 
</code></pre>

<p>insgenM   0.000  </p>

<p>ageveho   0.000  -0.037   </p>

<p>make      0.000   0.071 0.108 </p>

<p>area1    -1.000   0.000 0.000  0.000   </p>

<p>area2    -1.000  0.000 0.000  0.000  1.000</p>

<p>area3   -1.000  0.000  0.000  0.000  1.000  1.000       </p>

<p>I have included predictors:gender (Male,female), area which has 4 levels (area 1,2,3,4) and vehicle age (New and old) and make of car.</p>
"
"0.116961064294386","0.10922137064511"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0661631693578849","0.0858124131484961"," 87768","<p>My research objective is that I would like to test if some sounds in languages are similar to each other in terms of duration. </p>

<p>In my situation, I have 4 languages with 5 sounds. The first three languages have all 5 sounds but only one language has 3 sounds. My items are divided into 6 vowels and all languages have all these 6 vowels. </p>

<p>Then I have speakers of 4 languages but 2 languages are produced by the same group of speakers - so actually I have 3 groups of speakers instead of 4 (each group has both males and females). </p>

<p>As duration is continuous variable, I am using Linear mixed model (LMM) in R with lme4. My fixed effects are language + vowel + sex + sound. My random effects are speaker + item. </p>

<p>However, I am not sure what the proper model should look like in my case. I have tried this formula: 
<code>lmer4 &lt;- lmer(Duration ~ (1|item) + (1+sound*vowel|speaker) + vowel*sex*sound*Language, data=data.frame,REML=FALSE)</code> </p>

<p>but the model cannot be run because some sounds do not exist in some languages. Does anyone have any idea about the appropriate model for my case? Your help would be exactly grateful. </p>
"
"0.075498042361142","0.0940027887907685"," 87834","<p>I am doing linear mixed models using lme4 and this is the results of model comparison:</p>

<pre><code>&gt; anova(lmer5,lmer6,lmer32)

       Df   AIC   BIC logLik   Chisq Chi Df Pr(&gt;Chisq)    
lmer32  9 43172 43226 -21577                              
lmer6  21 43190 43315 -21574  6.3081     12     0.8998    
lmer5  26 43162 43317 -21555 37.9971      5  3.778e-07 ***
</code></pre>

<p>As you can see, the results show that one model is significantly better than the others and normally I will choose model with smallest logLik. However in this result, the logLik is negative. Do you think it is a good idea to choose model from logLik in this case, or should I choose it from AIC or BIC instead.</p>

<p>As no conclusion whether AIC is better than BIC, I am confused which one I should choose. What do you think?</p>
"
"0.0915365116690398","0.108545070825851"," 87920","<p>I am doing linear mixed model using lme4. According to Winter (2013, <a href=""http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf"" rel=""nofollow"">http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf</a>), as the new version of R does not give p-values due to inconclusion of degree of freedom, p-values of mixed models can be derived from model comparison. From the examples on page 12, he suggested to construct the null model:</p>

<pre><code>politeness.null=lmer (frequency ~ gender + (1|subject) + (1|scenario), data=politeness, REML=FALSE)
</code></pre>

<p>Then add the fixed effect that we are interested in:</p>

<pre><code>politeness.null=lmer (frequency ~ attitude + gender + (1|subject) + (1|scenario), data=politeness, REML=FALSE)
</code></pre>

<p>And then the p-values of attitude can be given from </p>

<pre><code>anova(politeness.null,politeness.model)
</code></pre>

<p>However, in my case, I have 3-way interaction: color*sex*food, and when I run the model I have 17 layers of fixed effects, such as white (compare to red), blue (compared to red), white:male(compared to female), etc.</p>

<p>Then my question is how I can get p-values for all these fixed effects? I am not sure if I should have some fixed effects that I am not interested in first:</p>

<pre><code>lmer1=lmer (duration ~ action + (1|subject) + (1|repetition), data=data.frame, REML=FALSE) 
</code></pre>

<p>Then add:</p>

<pre><code>lmer1=lmer (duration ~ action + color + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)  
</code></pre>

<p>Then add:</p>

<pre><code>lmer1=lmer (duration ~ action + color + sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE) 
</code></pre>

<p>or </p>

<pre><code>lmer1=lmer (duration ~ action + color*sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)
</code></pre>

<p>or</p>

<pre><code>lmer1=lmer (duration ~ action + sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)
</code></pre>

<p>Anyone can help me about this? or is there any other ways to get p-values easier than model comparison?</p>
"
"0.0369863360727655","0.0383764778226668"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"NaN","NaN"," 88905","<p>I am using <code>lmerTest</code> to run linear mixed models (LMM) to get the p-values. However, in the articles I have written, they will show both <code>lme4</code> and <code>lmerTest</code>. Then I am not sure if I also have cite <code>lme4</code> when I use only <code>lmerTest</code> to run LMM models. </p>

<p>Do you have any idea about this?</p>
"
"0.13869876027287","0.143911791835001"," 89089","<p>I want to fit a model with two crossed random factors that also allow heteroscedasticity. Whereas <code>nlme4</code> allows non-constant error variance, I was not sure how to fit a model with crossed random effects. Recently I found the example in Pinheiro and Bates (2000), p. 163, that suggest the syntax (I fitted only the random intercept, with the Data set ""Assay""):</p>

<pre><code>model.nlme &lt;- lme(logDens ~ 1, Assay, 
                  random = pdBlocked(list(pdIdent(~ 1), 
                                          pdIdent(~ sample - 1), 
                                          pdIdent(~ dilut - 1))))
</code></pre>

<p>, which gives the output </p>

<pre><code>&gt; model.nlme
Linear mixed-effects model fit by REML
  Data: Assay 
  Log-restricted-likelihood: 57.56285
  Fixed: logDens ~ 1 
(Intercept) 
   0.272999 

Random effects:
 Composite Structure: Blocked

 Block 1: (Intercept)
 Formula: ~1 | Block
         (Intercept)
StdDev: 7.369344e-06

 Block 2: samplea, sampleb, samplec, sampled, samplee, samplef
 Formula: ~sample - 1 | Block
 Structure: Multiple of an Identity
           samplea    sampleb    samplec    sampled    samplee    samplef
StdDev: 0.07433785 0.07433785 0.07433785 0.07433785 0.07433785 0.07433785

 Block 3: dilut1, dilut2, dilut3, dilut4, dilut5
 Formula: ~dilut - 1 | Block
 Structure: Multiple of an Identity
           dilut1    dilut2    dilut3    dilut4    dilut5   Residual
StdDev: 0.2642586 0.2642586 0.2642586 0.2642586 0.2642586 0.04745801

Number of Observations: 60
Number of Groups: 2
</code></pre>

<p>. However, when I fit the model using <code>lme4</code>, the result appears to be differet:</p>

<pre><code>model.lme4 &lt;- lmer(logDens ~ 1 + (1 | sample) + (1 | dilut), 
                   data = Assay)

&gt; model.lme4
Linear mixed model fit by REML ['lmerMod']
Formula: logDens ~ 1 + (1 | sample) + (1 | dilut) 
   Data: Assay 
REML criterion at convergence: -140.0874 
Random effects:
 Groups   Name        Std.Dev.
 sample   (Intercept) 0.07252 
 dilut    (Intercept) 0.27908 
 Residual             0.05137 
Number of obs: 60, groups: sample, 6; dilut, 5
Fixed Effects:
(Intercept)  
      0.273  
</code></pre>

<p>. The random effect standard deviations, especially the within-level one, is quite different (7.369344e-06 vs. 0.05137). </p>

<p>Question 1: Are the two models equivalent using the setup as above?</p>

<p>Question 2: If yes, why do they have different results?</p>
"
"0.152734453799701","0.167279188638034"," 89991","<p>Which binnedplot of the glmer should I use to check the model? The residuals against the predicted values without random part(REform=NA) or residuals against the predicted values with random part(REform=NULL)?</p>

<p>I have one binary response variable (y.10) derived from one continuous variable with around 50 to 75% of zeros. I want to model the probability to exceed the limit of 10.
For this example I used only one predictor ""fragments"" which is transformed by  taking the logarithm to get a normal distribution an later a better fit  . All variables are measured in tree regions (region). Within this regions are different plots (plot) and a set of samples were taken from some objects (object).</p>

<p>To inspect the residuals I used binnedplot like discribed in the answer of the question:
<a href=""http://stats.stackexchange.com/questions/63566/unexpected-residuals-plot-of-mixed-linear-model-using-lmer-lme4-package-in-r"">Unexpected residuals plot of mixed linear model using lmer (lme4 package) in R</a>.
To save calculation time with very complex models I modeled at first with 
glm {stats} and based on this results the model with less variables with glmer{lme4}. Doing this I could observe a big difference in the binnedplot of residuals.</p>

<p>To examination the differences I created this example with only one variable. Like you can see in the picture bellow the models of glm and glmer without the random part show a very similar behavior. At the end the random part is not for interest. I need the random part only during model selection.</p>

<p>Which binnedplot of the glmer should I use to check the model? The residuals against the predicted values without random part(REform=NA) or residuals against the predicted values with random part(REform=NULL)?</p>

<p>The code and the resulting picture is given here:</p>

<pre><code>fit.glm=glm(y.10 ~ x.t , data=data, family=""binomial"")
fit.glmer=glmer(y.10 ~ x.t + (1|region) + (1|plot) + (1|object), 
          data=data, family=""binomial"")

y.glm=predict(fit.glm, type =""response"")
y.glmer=predict(fit.glmer,REform=NA,type =""response"") 
y.glmer.ran=predict(fit.glmer,REform=NULL,type =""response"")

par(mfrow=c(2,2))
plot(y.10~x, data=data, type=""n"", main=""Models"")
points(y.glmer.ran~data$x, col=""green"", pch=4, cex=0.5)
    lines(y.glm~data$x, col=""red"")
lines(y.glmer~data$x, col=""darkgreen"")

binnedplot(fitted(fit.glm),resid(fit.glm), main=""Binned residual plot glm"")
binnedplot(y.glmer.ran,resid(fit.glmer), main=""Binned residual plot glmer(REform=NULL)"")
binnedplot(y.glmer,resid(fit.glmer), main=""Binned residual plot glmer(REform=NA)"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HoW04.jpg"" alt=""Shows in the first first Plot the glm fitted model (red) and glmer fitted models with  (green crosses) and without (dark green line) random part. The other plots show the corresponding residuals.""></p>
"
"0.0661631693578849","0.0858124131484961"," 90392","<p>I have run Generalized linear mixed model with glmer in lme4. I use R version 3.0.1. My dependent variable is binary (correct or wrong). And this is my results:</p>

<pre><code>&gt; glmer16 &lt;- glmer(result ~ (1|item) + (1|speaker) + vowel + sex + cat + dog + exposure + frequency + v00004 + v00024 + v00034 + v00044, data=data1.frame, family=binomial)
&gt; summary(glmer16)
Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: realisation ~ (1 | item) + (1 | speaker) + vowel + sex + cat +      dog + exposure + frequency + v00004 + v00024 + v00034 +      v00044 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
 881.7026  958.6402 -426.8513  853.7026 

Random effects:
 Groups  Name        Variance Std.Dev.
 speaker (Intercept) 7.0291   2.651   
 item    (Intercept) 0.5084   0.713   
Number of obs: 1800, groups: speaker, 50; item, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 15.52018    5.33634   2.908  0.00363 ** 
vowelhigh    0.16750    0.55907   0.300  0.76449    
vowellow     0.70981    0.63194   1.123  0.26134    
sexmale      1.37080    1.03228   1.328  0.18420    
cat         -0.11460    0.09537  -1.202  0.22953    
dog         -0.05460    0.03633  -1.503  0.13286    
exposure    -0.00404    0.01564  -0.258  0.79613    
frequency   -0.01709    0.15594  -0.110  0.91272    
v00004      -2.83445    0.66039  -4.292 1.77e-05 ***
v00024       0.29687    0.55868   0.531  0.59515    
v00034       0.43899    0.58656   0.748  0.45421    
v00044       0.36663    0.65130   0.563  0.57349    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My questions are: 1) Does v00004 decrease the result of 'wrong' or 'correct'? and 2) what does it mean by significant at the intercept?</p>
"
"0.0986302295273746","0.115129433468"," 90758","<p>I have run LMM models with different reference categories and this yield different results: </p>

<pre><code>&gt; summary(lmer3)
Linear mixed model fit by maximum likelihood ['merModLmerTest']
Formula: v000001 ~ (1 | item) + (1 + color | speaker) + Language * color *      sex 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
16279.975 16377.355 -8119.988 16239.975 

Random effects:
 Groups   Name        Variance  Std.Dev.  Corr       
 speaker  (Intercept) 8.904e+05 9.436e+02            
          colorblue   1.821e+05 4.267e+02 -0.35      
          colorred    3.428e+05 5.855e+02 -0.44  1.00
 item     (Intercept) 9.502e-06 3.083e-03            
 Residual             1.067e+06 1.033e+03            
Number of obs: 962, groups: speaker, 53; item, 10

Fixed effects:
                                  Estimate Std. Error       df t value Pr(&gt;|t|)
(Intercept)                       10664.67     318.69    38.45  33.464   &lt;2e-16
Languagel2_like                     391.48     421.40    42.13   0.917   0.3642
colorblue                          -179.31     211.02    44.50  -0.850   0.4000
colorred                            116.96     241.44    36.27   0.484   0.6310
sexmale                            -168.01     450.11    38.26  -0.373   0.7110
Languagel2_like:colorblue           758.22     301.01    54.20   2.519   0.0147
Languagel2_like:colorred            463.37     344.01    45.73   1.344   0.1857
Languagel2_like:sexmale            -811.49     607.85    43.49  -1.326   0.1917
colorblue:sexmale                   342.76     294.97    42.57   1.162   0.2517
colorred:sexmale                     13.25     337.44    34.81   0.039   0.9689
Languagel2_like:colorblue:sexmale  -721.37     438.78    54.19  -1.644   0.1059
Languagel2_like:colorred:sexmale   -605.76     497.75    45.29  -1.216   0.2304

(Intercept)                       ***
Languagel2_like                      
colorblue                            
colorred                            
sexmale                              
Languagel2_like:colorblue         *  
Languagel2_like:colorred            
Languagel2_like:sexmale              
colorblue:sexmale                    
colorred:sexmale                     
Languagel2_like:colorblue:sexmale    
Languagel2_like:colorred:sexmale     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And this is the second model:</p>

<pre><code>&gt; summary(lmer43)
Linear mixed model fit by maximum likelihood ['merModLmerTest']
Formula: v000001 ~ (1 | item) + (1 + color3 | speaker) + Language * color3 *      sex 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
16279.975 16377.355 -8119.988 16239.975 

Random effects:
 Groups   Name        Variance  Std.Dev.  Corr       
 speaker  (Intercept) 7.945e+05 8.913e+02            
          color3white 1.821e+05 4.268e+02 -0.11      
          color3red   2.761e+04 1.661e+02 -0.24 -0.94
 item     (Intercept) 4.961e-06 2.227e-03            
 Residual             1.067e+06 1.033e+03            
Number of obs: 962, groups: speaker, 53; item, 10

Fixed effects:
                                   Estimate Std. Error       df t value
(Intercept)                        10485.36     305.33    39.57  34.341
Languagel2_like                     1149.70     399.61    43.91   2.871
color3white                          179.31     211.03    44.50   0.850
color3red                            296.27     167.08   125.59   1.773
sexmale                              174.75     430.05    38.94   0.406
Languagel2_like:color3white         -758.22     301.01    54.10  -2.519
Languagel2_like:color3red           -294.85     244.46   159.09  -1.206
Languagel2_like:sexmale            -1532.85     577.70    44.74  -2.648
color3white:sexmale                 -342.76     294.98    42.57  -1.162
color3red:sexmale                   -329.51     228.57   113.99  -1.442
Languagel2_like:color3white:sexmale  721.36     438.78    54.10   1.644
Languagel2_like:color3red:sexmale    115.61     351.65   162.98   0.329
                                   Pr(&gt;|t|)    
(Intercept)                         &lt; 2e-16 ***
Languagel2_like                     0.00627 ** 
color3white                         0.40004    
color3red                           0.07862 .  
sexmale                             0.68671    
Languagel2_like:color3white         0.01477 *  
Languagel2_like:color3red           0.22953    
Languagel2_like:sexmale             0.01114 *  
color3white:sexmale                 0.25171    
color3red:sexmale                   0.15215    
Languagel2_like:color3white:sexmale 0.10602    
Languagel2_like:color3red:sexmale   0.74275    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Is it possible to report results from two models? (I know that very little of people would do this but these two models give different pictures). What should I do? Which one should I believe?    </p>
"
"0.118620169351171","0.102565451231105"," 90799","<p>I am having trouble training a model for nested data about house prices. Lets say my data looks like following:</p>

<pre><code>  logPrice bedCount bathCount                city
 0.6517920        4       2-3        Redwood City
 0.4402192        1       1-2 South San Francisco
 0.5922396        2       1-2           San Mateo
 0.4606918        3       1-2 South San Francisco
 0.7592523    5plus       3-4           San Mateo
 0.4710397        1       1-2        Redwood City
</code></pre>

<p><code>bedCount</code>, <code>bathCount</code> and <code>city</code> are factors.</p>

<p>As a baseline, I trained a simple linear model ignoreing nested structure of the data (houses are nested within cities).</p>

<pre><code>lm.model = lm(formula = logPrice ~ 1 + bedCount + bathCount + city)
</code></pre>

<p>which corresponds to following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where </p>

<p>$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $I$(<code>city</code>$_{j[i]}$) is the indicator function for city of the $i^{th}$ house (which is 1).</p>

<p>Now, I trained a 2-level hierarchical model:</p>

<pre><code>lmer.model = lmer(formula = logPrice ~ 1 + bedCount + bathCount + (1 | city))
</code></pre>

<p>which corresponds to the following assumption:</p>

<p><code>logPrice</code>$_i = \beta_0 + \beta_1\cdot$ <code>bedCount</code>$_i + \beta_2\cdot$ <code>bathCount</code>$_i + \beta_{3,j[i]}\cdot I$(<code>city</code>$_{j[i]}) + \epsilon_i$</p>

<p>where
$\epsilon_i \sim N(0, \sigma^2_{logPrice})$ and $\beta_{3,j} \sim N(0, \sigma^2_{\beta_3})$</p>

<p>Now, on the training data, <code>lm.model</code> gives me lesser average RMSE than <code>lmer.model</code> which shouldn't happen because linear regression is a special case of multilevel linear regression (I didn't care to check average RMSE on test data because that on training data itself should be lower for 2nd model than that for 1st model). In fact, my data has multiple levels (houses nested within subdivisions, which are nested within zipcodes, which in turn are nested within cities) and the performance gets worse as I add more and more levels to the model (i.e. model with random effect <code>(1 | subdivision)</code> does worse than that with random effect <code>(1 | zipcode) + (1 | zipcode:subdivision)</code>, which in turn does worse than a model with random effect <code>(1 | city) + (1 | city:zipcode) + (1 | city:zipcode:subdivision)</code>).</p>

<p>What am I missing?</p>
"
"0.177950592399576","0.18463891501222"," 93601","<p>I am a complete novice and dummy when it comes to statistics so I apologise in advance...</p>

<p>I have been asked to report the results of my GLMMs (I ran two) in a table. This table must state: effect, standard error, test statistic, and P value, for all fixed effects. </p>

<p>Unfortunately I am struggling to read my output. </p>

<p>The out put is as follows, if anyone would be kind enough to help I would be very grateful and will know for future reference which bit equates to what (also I have been told my degrees of freedom are different for both the tests, could someone explain why this is?).</p>

<pre><code>GLMM 1-run for predictors of step length. 
Response variable = step length. 
fixed effects = depth and direction threshold. 
random factor = individual

Models:
m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m2 3 373235 373259 -186615 373229 
m1 8 373225 373290 -186605 373209 19.767 5 0.001382 **
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>.</p>

<pre><code>GLMM 2 -run to investigate potential predictors of PDBA.
response variables = depth and step length. 
fixed effect = direction threshold.
random factor = Individual

Models:
m3: PDBA ~ Depth + (1 | ind) + thresholdepth
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m3 6 -48205 -48157 24109 -48217 
m2 11 -48430 -48341 24226 -48452 235.1 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Models:
m4: PDBA ~ step + (1 | ind) + step:threshold
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m4 6 -48206 -48158 24109 -48218 
m2 11 -48430 -48341 24226 -48452 233.81 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Hi, I think the package I used was was lme4? </p>

<p>I have run a summary for the first GLMM and this is what I got, I have no idea which parts are relevant though, I assume it doesn't all go in a table?! </p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                  Estimate Std. Error t value
(Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>The output from the Anova only gives me one p value for each GLMM and I think I need a p value for each of the fixed effects within the models?</p>

<p>Does anyone know what code I can run to get this?
Thank you</p>
"
"0.123097967263292","0.117080918782665"," 93892","<p>I need to get p values for the fixed effects in the following GLMM's I ran. Does anyone know of code that I can run that will give me the p values I need? At the moment the output from the ANOVA only gives me one p value and I believe I need a separate p value for each of the fixed effects in the models. </p>

<p>Thanks in advance.
Code is as follows -</p>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>When I ran GLMM 1 code this is what I got:</p>

<pre><code>m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
m2  3 373235 373259 -186615   373229                            
m1  8 373225 373290 -186605   373209 19.767      5   0.001382 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>summary</p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                 Estimate Std. Error t value
  (Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>OUTPUT FROM SUGGESTED CODE BY SETH (IN COMMENTS)</p>

<pre><code>Models:
m6: step ~ Depth + threshold + (1 | ind)
m5: step ~ Depth + threshold + Depth:threshold + (1 | ind)
   Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
m6  6 373227 373275 -186607   373215                           
m5  8 373225 373290 -186605   373209 5.2901      2      0.071 .
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"NaN","NaN"," 94000","<p>after doing the linear mixed model with lme4, I have used lsmeans for pairwise comparison with this command:</p>

<pre><code>lsmeans(lmer52, pairwise~color, adjust=""tukey"")
</code></pre>

<p>I am not sure if it is ""multiple pairwise comparisons, using Least Squares Means
 (LSMEANS) and Tukey's adjustment"". Anyone have any idea if it is exactly the same thing?</p>
"
"0.199288261836385","0.213671185591981"," 94057","<p>I have an agricultural field experiment (testing a plant protection agent):</p>

<p><strong>Split plot design</strong> with: </p>

<pre><code>2 whole plot treatments ""infestation"": ""high"" &amp; ""low"" 
8 split-plot treatments (""treat""): 

1. Untreated Control (""Ctrl1"")
2. Reference Product (""Ctrl2"")
3. 1 x Test-Product 1
4. 2 x Test-Product 1
5. 3 x Test-Product 1
6. 1 x Test-Product 2
7. 2 x Test-Product 2
8. 3 x Test-Product 2

and 4 replicates (""block""):
</code></pre>

<p>The parameter of interest in this example is grain (<strong>yield</strong>):</p>

<p>First, I could model this:</p>

<pre><code>lme(yield ~ infestation * treat, random = ~ 1 | block/infestation, data)
</code></pre>

<p>or</p>

<pre><code> lmer(yield ~ infestation * treat + (1 | block/infestation), data)
</code></pre>

<p>But as can be seen treatments 3-8 can and have to be recoded as 2 products (""prod"") being tested 1-3 times (""times""), so I have a 2x3 subdesign.</p>

<p>One possibility would be subsetting the data: </p>

<pre><code>  data2 &lt;- subset(data, !data$treat == ""Ctrl1"" &amp;  !data$treat == ""Ctrl2"")
</code></pre>

<p>and recode the resting treatments to ""prod"" = 1,2 and ""times"" = 1:3
then run:</p>

<pre><code>lme(yield ~ infestation * form * times, random = ~ 1 | block/infestation, data)
</code></pre>

<p>Afterwards I could still do contrasts to compare the control treatments with the treated ones. </p>

<p>But (here my actual problem starts): I read an article of</p>

<p><strong>H.P. Piepho</strong>: ""<em>A Note on the Analysis of Designed Experiments with Complex Treatment Structure</em>"", 
HortScience 41(2):446--452. 2006</p>

<p>The author wants to show ""<em>how a meaningful analysis can be obtained based on a linear model by appropiate coidng of factors. (...) Our main objective is to demonstrate that the introduction of dummy variables can conveniently solve a wide variety of inferential problems that would otherwise either require ... multiple linear contrasts... or not make fully eficient use of the data, e.g when only data from orthogonal subdesigns are analysed.</em>""  </p>

<p>A very similar example (Example 1 in the article) is discussed within, and an alternative analysis in SAS is proposed - which I wanted to try to realise in R. </p>

<p>The author adds a dummy variable (<strong>ctrl_vs_trt</strong>) to the data and codes it: ""control"", ""trt"" (in my case <strong>trt</strong>, <strong>Ctrl1</strong>, <strong>Ctrl2</strong>"". </p>

<p>The he uses: 
(in his case <strong>prod</strong> is <strong>form</strong> ulation, and <strong>times</strong> is <strong>conc</strong> entration)</p>

<pre><code>PROC GLM;
CLASS block contr_vs_trt form conc;   ## 
MODEL set = block contr_vs_trt
        contr_vs_trt * form
    contr_vs_trt * conc
    contr_vs_trt * form * conc;
RUN.                    
</code></pre>

<p>I cite a further paragraph: 
""<em>Of course, a test for contr_vs_trt is not produced with this model, and one cannot compute simple means or marginal means. Also, the Type I SS for <strong>form</strong>, <strong>conc</strong>, and <strong>form x conc</strong> are not the same as with Type III SS. With Type III SS, the test for form is adjusted for <strong>conc</strong>, as fitting <strong>conc</strong> takes out the control when coding factors as in Table ""xy"" (as I did here). Similarly, the test for <strong>conc</strong> is adjusted for <strong>form</strong>, because fitting of <strong>form</strong> takes out the control. As a result, the Type III ANOVA for the model <strong>form x conc</strong> turns out to be that for the 3x2 factorial subdesign. (...)
It seems much more stringent and transparent to use the nested model <strong>contr_vs_trt/(form x conc)</strong>, as this properly reflects all nesting and crossing features of the design.</em>""</p>

<p>Now, how to do that in <code>lme</code> or <code>lmer</code>?</p>

<p>lme does not run at all, even if I simplify to: </p>

<pre><code> lme(yield ~ prod * times, random = ~1|block, data), 
 I get
 Error in MEEM(object, conLin, control$niterEM) : 
 Singularity in backsolve at level 0, block 1
</code></pre>

<p>The term <strong>prod * times</strong> cannot be run (<strong>prod + times</strong> logically can). Eliminating both controls from the data set resolves this problem. </p>

<p><code>lmer</code> runs with <strong>prod * times</strong>, but always given the message:</p>

<pre><code> fixed-effect model matrix is rank deficient so dropping ""x"" columns / coefficients
</code></pre>

<p>I understand that the subdesign is not orthogonal and therefor dropping is occuring, but I cannot say if the analysis after dropping can still be right. </p>

<p>Also, I do not know how to specify the full model (leaving out the ""infestation"" whole plot for a second):</p>

<pre><code>lmer(yield ~ prod * times + (1|block/ctr_vs_trt), data)
</code></pre>

<p><strong>prod * times</strong> is nested inside <strong>ctr_vs_trt</strong> but both are nested inside the same block (or whole plot).
Is nesting of fixed effects possible in <code>lme</code> or <code>lmer</code> - does it work as I proposed?
Does it even make sense to run the full model?</p>

<p>With <code>aov()</code> I get the model running, even the partitioning of Df's is right. But due to strong non-orthogonality it is not possible to assume that the results are right.</p>

<p>I can get meaningful results subsetting and using contrasts, but I found the authors approach interesting and it would help in the analysis of some of my other trials. 
Thanks in advance for any help; I hope this question is not too long...</p>
"
"0.148201971273683","0.153772183669956"," 94888","<p>I'm analysing some behavioural data using <code>lme4</code> in <code>R</code>, mostly following <a href=""http://www.bodowinter.com/tutorials.html"" rel=""nofollow"">Bodo Winter's excellent tutorials</a>, but I don't understand if I'm handling interactions properly. Worse, no-one else involved in this research uses mixed models, so I'm a bit adrift when it comes to making sure things are right.</p>

<p>Rather than just post a cry for help, I thought I should make my best effort at interpreting the problem, and then beg your collective corrections. A few other asides are:</p>

<ul>
<li>While writing, I've found <a href=""http://stackoverflow.com/questions/17794729/test-for-significance-of-interaction-in-linear-mixed-models-in-nlme-in-r"">this question</a>, showing that <code>nlme</code> more directly give p values for interaction terms, but I think it's still valid to ask with relation to <code>lme4</code>.</li>
<li><code>Livius'</code> answer to <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">this question</a> provided links to a lot of additional reading, which I'll be trying to get through in the next few days, so I'll comment with any progress that brings.</li>
</ul>

<hr>

<p>In my data, I have a dependent variable <code>dv</code>, a <code>condition</code> manipulation (0 = control, 1 = experimental condition, which should result in a higher <code>dv</code>), and also a prerequisite, labelled <code>appropriate</code>: trials coded <code>1</code> for this should show the effect, but trials coded <code>0</code> might not, because a crucial factor is missing.</p>

<p>I have also included two random intercepts, for <code>subject</code>, and for <code>target</code>, reflecting correlated <code>dv</code> values within each subject, and within each of the 14 problems solved (each participant solved both a control and an experimental version of each problem).</p>

<pre><code>library(lme4)
data = read.csv(""data.csv"")

null_model        = lmer(dv ~ (1 | subject) + (1 | target), data = data)
mainfx_model      = lmer(dv ~ condition + appropriate + (1 | subject) + (1 | target),
                         data = data)
interaction_model = lmer(dv ~ condition + appropriate + condition*appropriate +
                              (1 | subject) + (1 | target), data = data)
summary(interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## Linear mixed model fit by REML ['lmerMod']
## ...excluded for brevity....
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  subject  (Intercept) 0.006594 0.0812  
##  target   (Intercept) 0.000557 0.0236  
##  Residual             0.210172 0.4584  
## Number of obs: 690, groups: subject, 38; target, 14
## 
## Fixed effects:
##                                Estimate Std. Error t value
## (Intercept)                    0.2518     0.0501    5.03
## conditioncontrol               0.0579     0.0588    0.98
## appropriate                   -0.0358     0.0595   -0.60
## conditioncontrol:appropriate  -0.1553     0.0740   -2.10
## 
## Correlation of Fixed Effects:
## ...excluded for brevity.
</code></pre>

<p>ANOVA then shows <code>interaction_model</code> to be a significantly better fit than <code>mainfx_model</code>, from which I conclude that there's a significant interaction present (p = .035).</p>

<pre><code>anova(mainfx_model, interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## ...excluded for brevity....
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## mainfx_model       6 913 940   -450      901                          
## interaction_model  7 910 942   -448      896  4.44      1      0.035 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>From there, I isolate a subset of the data for which the <code>appropriate</code> requirement is met (i.e., <code>appropriate = 1</code>), and for it fit a null model, and a model including <code>condition</code> as an effect, compare the two models using ANOVA again, and lo, find that <code>condition</code> is a significant predictor.</p>

<pre><code>good_data = data[data$appropriate == 1, ]
good_null_model   = lmer(dv ~ (1 | subject) + (1 | target), data = good_data)
good_mainfx_model = lmer(dv ~ condition + (1 | subject) + (1 | target), data = good_data)

anova(good_null_model, good_mainfx_model)
</code></pre>

<p>Output:</p>

<pre><code>## Data: good_data
## models:
## good_null_model: dv ~ (1 | subject) + (1 | target)
## good_mainfx_model: dv ~ condition + (1 | subject) + (1 | target)
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## good_null_model    4 491 507   -241      483                          
## good_mainfx_model  5 487 507   -238      477  5.55      1      0.018 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
"
"0.110959008218296","0.115129433468"," 95105","<p>I've been putting a lot of work over the last few days into bring mixed effects models to bear on some behavioural data I've collected for my thesis, but it's occurred to me that I'm not 100% sure that this kind of model is actually appropriate for my data (I only came across them after starting the experiment).</p>

<p>In an experiment, <code>60 participants</code> completed <code>28 trials</code> of a reasoning task, consisting of <code>14 problems</code> (call them ""A""-""N""), with participants completing each in <code>2 conditions</code>, <code>x</code> (conflict) and <code>y</code> (control).</p>

<pre><code>Participant  Problem
          1  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          2  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          3  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          4  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          5  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
        ...  ...      ...      ...      ...    ...  ...
         60  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
</code></pre>

<p>I'm interested in the difference in a trial-by-trial variable (let's call it <code>reaction time</code>) between the conflict and control conditions, and would expect it to be higher for the conflict (<code>y</code>) trials.</p>

<p>Obviously, I would expect to find a within-subject correlation - some subjects are generally fast, some generally slow. I would also expect to find a within-problem correlation - some problems are answered faster than others, regardless of condition. To account for these, I include random intercepts for these two factors: </p>

<p><code>(1|participant) + (1|problem)</code>.</p>

<p>The difference between conflict and control conditions may or may not turn out to be the same for each subject, and for each problem. For this reason, I consider including random slopes as well:</p>

<p><code>(1 + condition|participant) + (1 + condition|problem)</code>.</p>

<p>Putting this together, I'm testing a model that looks either like:</p>

<pre><code>null_model = lmer(reaction_time ~ condition(1|participant) + (1|problem), data=data)
condition_model = lmer(reaction_time ~ condition + (1|participant) + (1|problem), data=data)
</code></pre>

<p>or</p>

<pre><code>null_model = lmer(reaction_time ~ (1 + condition|participant) + (1 + condition|problem), data=data)
condition_model = lmer(reaction_time ~ condition(1|participant) + (1|problem), data=data)
</code></pre>

<p>.</p>

<p><strong>Please; have I horribly misunderstood how this is supposed to work?</strong></p>

<p><strong>Edit:</strong> The more traditional approach to analysing this data would be to average across problems within each participant, yielding two data points per participant: <code>conflict condition mean</code> and <code>control condition mean</code>, and then use a paired-samples t test. Reading <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">this question</a>, I thought that this approach should be largely the same as fitting </p>

<p><code>lmer(reaction_time ~ condition + (1|participant), data=data)</code>, </p>

<p>but trying this in R, it seems otherwise.</p>

<p><strong>Edit #2:</strong> Bounty added.</p>
"
"0.0661631693578849","0.0858124131484961","105582","<p>I am new to using linear mixed models and would greatly appreciate any help I can get.</p>

<p>I have an equation of the form $ y = X\beta + Zu + \epsilon$ where $u$ is a random effect whose covariance matrix I want to be a multiple of the identity matrix.</p>

<p>My matrix $Z$ has about 1000 columns. Trouble is I have only one group, i.e. not multiple observations per person, and I think <code>lmer</code> does not like this. </p>

<p>For example in the simple case, if I put in something like <code>lmer</code></p>

<p>$({\rm height} \sim 1 + (0+Z_1|ID) + (0+Z_2|ID))$ </p>

<p>where $Z_1$ and $Z_2$ are 2 columns of $Z$ and $ID$ is the <strong>unique ID for each person</strong> <code>lmer</code> throws up an error saying I need the number of groups to be less than the number of observations (a similar problem occurs if I set everyone belonging to the same group).</p>

<p>I had partial success with <code>lme</code> (in the <code>name</code> package) where I could provide the  covariance structure of $u$ as in <code>pdDiag</code>, and the package allows one group. Trouble there was that I ran into memory issues, and the time was too long. </p>

<p>Since <code>lmer</code> uses a lot of sparse matrix methods, I am guessing my flexibility in the <strong>covariance structure</strong> will be a lot lesser. That being said, my restriction seems like (probably?) one of the more obvious ones which would have been implemented in these models as is. </p>
"
"0.181195301666741","0.188005577581537","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.0905976508333704","0.0783356573256404","106063","<p>I ran linear mixed effects model in R.</p>

<pre><code>model&lt;-lmer(yld ~ rain + (1+rain|state),data=data,REML=FALSE)
</code></pre>

<p>Is there any way I can generate an R-squared for the model. I know we can derive R-squared for a generalised linear mixed effects model <a href=""http://jonlefcheck.net/2013/03/13/r2-for-linear-mixed-effects-models/"" rel=""nofollow"">http://jonlefcheck.net/2013/03/13/r2-for-linear-mixed-effects-models/</a> but I am not sure about a linear mixed effect model. Can anyone point me to the right direction/webpages/code?</p>

<p>Thanks a lot</p>

<p>Unfortunately, I cannot answer a question marked as duplicate and as I'm new here I cannot comment on questions due to my low reputation. But I can edit other peoples questions??? </p>

<p>You may want to have a look at Nakagawa &amp; Schielzeth (2013) A general and simple method for obtaining R 2 from generalized linear mixed-effects models. Methods in Ecology and Evolution 2013, 4, 133â€“142. doi: 10.1111/j.2041-210x.2012.00261.x Within the supplementary information of the online article they provide such an example. However, note that I had substitute ""mF@X"" by ""model.matrix(mF)"" in order to get the code working. 
Best Philipp</p>
"
"0.110959008218296","0.115129433468","106079","<p>I am estimating a random intercept and a random slope model using the following R code. My dependent and independent variable are both continuous.</p>

<pre><code>randominterceptfixedslope&lt;-lmer(y ~ x + (1|state),data=data,method=""ML"") # model with fixed slope but random intercept
randominterceptrandomslope&lt;-lmer(y ~ x + (1+x|state),data=data,method=""ML"") # model with random slope and random intercept

anova(randominterceptfixedslope,randominterceptrandomslope)
</code></pre>

<p>Anova tells me that my randominterceptrandomslope model is a better fit on the data. So far good, please correct me if I am wrong. </p>

<p>My question is: If I have another independent variable $x_1$, can I put two independent variables in the above model i.e. can I have a randominterceptfixedslope and randominterceptrandomslope model with two independent variables. If yes, how do I do that? As in what the code should look like?</p>

<p>Thanks for your response. I got a second query. lets say my full model is this: </p>

<pre><code>     randominterceptrandomslope&lt;-lmer(y ~ x1 + x2 + x3 + x4 (1+x1+x2+x3+x4|state),data=data,method=""ML"")
</code></pre>

<p>If some of my independent variables are correlated, what is the procedure of reducing the collinearity issue in a linear mixed effect model?  I could spot collinerity  using VIF and retain the most significant independent variables but I can do this for each factor level (levels of state) individually. But won't it result in retaining some independent variables in one factor level while deleting the same in other factor level? I guess the main question is how to spot collinearity in a mixed effect model and what to do with it when you have 5 or 6 independent variables?</p>
"
"0.0978566471559948","0.101534616513362","106112","<p>I have one dependent variable (continuous data) and 4 independent data (mix of continuous and count) collected over 35 years across several states. I am using a linear mixed effect models with a random intercept and a random slope.</p>

<pre><code>model&lt;-lmer(y ~ x1 + x2 + x3 + x4 (1+x1+x2+x3+x4|state),data=data,method=""ML"")
</code></pre>

<p>If some of my independent variables are correlated, what is the procedure of reducing the collinearity issue in a linear mixed effect model?  I could spot collinerity  using VIF and retain the most significant independent variables but I can do this for each factor level (levels of state) individually. But won't it result in retaining some independent variables in one factor level while deleting the same in other factor level? I guess the main question is how to spot collinearity in a mixed effect models and what to do with it when you have 5 or 6 independent variables?</p>
"
"0.0661631693578849","0.0686499305187969","107427","<p>I have a data set that I am attempting to analyse in R and I am relatively new to the environment.</p>

<p>My full data set contains 7 subjects (represented by Subject), that all receive 3 treatments (environmental conditions, represented by Altitude) and are measured 10 times within each treatment. Each participant received each treatment in a different order, represented  The measurements are power (from a treadmill, represented by Power) during repeated-sprint efforts (represented by Sprint). An example:</p>

<p><img src=""http://i.stack.imgur.com/lHITL.jpg"" alt=""enter image description here""></p>

<p>I also have characteristics about each subject that may influence their power effort, including weight, capacity tests and age. I am interested in the effect of each Environment on the Sprint results, plus the effect of order on the Power. </p>

<p>I believe a linear mixed model, with subject as a random effect, is an appropriate tool to investigate my dataset. I have attempted this using the following line:</p>

<p>alt.model = lmer(Power ~ Sprint + Altitude + VO2max + (1|Subject), data=ALTMM)   </p>

<p>However, I don't think this accounts for each individual sprint. How do I represent this?</p>

<p>Thank you.</p>
"
"0.0978566471559948","0.101534616513362","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"NaN","NaN","108161","<p>The <code>lmerTest</code> package provides an ANOVA function for linear mixed effects models with optionally Satterthwaite's (default) or Kenward-Roger's approximation of the degrees of freedom. What is the difference between these two approaches? When to choose which?</p>
"
"0.134557562058181","0.139614945531202","108647","<p>For answering my research question I am interested in the correlation between the random slopes and random intercepts in a multilevel model, estimated using the R library lme4. </p>

<p>The data I have is: Y (test-scores from students), SES (socio-economic status for each student) and schoolid (ID for each school). </p>

<p>I am using the following syntax to estimate random intercepts and slopes for the schools: </p>

<pre><code>library(lme4)
model3 &lt;- lmer(Y ~ SES + (1 + SES | schoolid))
</code></pre>

<p>The reference I used for this syntax is this pdf:</p>

<p><a href=""http://www.bristol.ac.uk/cmm/learning/module-samples/5-concepts-sample.pdf"" rel=""nofollow"">http://www.bristol.ac.uk/cmm/learning/module-samples/5-concepts-sample.pdf</a> </p>

<p>On page 19, a similar analysis is described. It is said that by defining the random intercepts and slopes toghether, it is indirectly specified that we want the random intercepts and slopes to covary. Therefore, also the correlation between random slopes and random intercepts is estimated. Basically, exactly what I need for answering my research hyptohesis. </p>

<p>However, when I look at the results: </p>

<pre><code> summary(model3)
</code></pre>

<p>I am getting the following output:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Y ~ SES + (1 + SES | schoolid)

REML criterion at convergence: 8256.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.1054 -0.6633 -0.0028  0.6810  3.5606 

Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 schoolid (Intercept) 0.6427924 0.80174      
      SES             0.0009143 0.03024  1.00
 Residual             0.3290902 0.57366      
Number of obs: 4376, groups: schoolid, 179

Fixed effects:
             Estimate Std. Error t value
(Intercept) -0.036532   0.060582  -0.603
SES          0.062491   0.009984   6.259

Correlation of Fixed Effects:
    (Intr)
SES 0.226 
</code></pre>

<p>As stated in the output, the correlation between the random slopes and random intercepts equals 1.00. I find this hard to believe. 
When I call in R: </p>

<pre><code>VarCorr(model3)$schoolid
</code></pre>

<p>I am getting the following output which gives the correlations and covariance matrix: </p>

<pre><code>                (Intercept)          SES
(Intercept)  0.64279243 0.0242429680
SES          0.02424297 0.0009143255

attr(,""stddev"")
(Intercept)         SES 
 0.80174337  0.03023782 

attr(,""correlation"")
        (Intercept) SES
(Intercept)           1   1
SES                   1   1
</code></pre>

<p>It seems as if the correlation between the slopes and intercepts was set to 1.00 by R. I did not see this in the output of anyone else when I was searching the internet on references on multilevel modelling. </p>

<p>Does anybody know what can be the cause of this correlation? 
Can it be that the correlation is set to 1.00 because otherwise the model would not be identified? 
Or is it because the variance of the random slopes is so small (0.0009) that the correlation can not be estimated? </p>

<p>I have tried to simulate data in order provide the code for a small reproducible dataset. I was however not yet able to reproduce this output by means of simulated data. As far as I have the code I will eidt my post and add the code.  </p>

<p>Edit: 
In response to a comment by Roman LuÅ¡trik, the following plot: </p>

<pre><code>ggplot(data[1:261,], aes(x = SES, y = Y)) + geom_point() + facet_wrap(~ schoolid) +
   geom_smooth(method=lm)
</code></pre>

<p>As there are in total 179 schools the plot becomes quite chaotic, therefore I included the first 10 schools only to make it readable: </p>

<p><img src=""http://i.stack.imgur.com/gSPI5.jpg"" alt=""enter image description here""></p>
"
"0.294686181013698","0.288036707419063","109215","<p>I try to compute the marginal and conditional $R^2$ for a GLMM using a negative binomial distribution by following the procedure recommended by Nakagawa &amp; Schielzeth (2013) . Unfortunately, the supplementary material of their article does not include an example of a negative binomial distribution (see the online version of the article stated below, I also added their code below). 
I fitted my model using the glmmPQL function from the MASS package.</p>

<pre><code>full_model  &lt;- glmmPQL ( Y~ a + b + c,  random = ~ 1 +  A | location  
, family = negative.binomial (1.4 ) ,data= mydata 
</code></pre>

<p>In particular, I do have the following problems:</p>

<ol>
<li><p>First, I need to extract the fixed effect design matrix of my model. However, full_model @X or model.matrix(full_model) does not work. I also tried to set the argument x=TRUE before extracting the matrix. Well, this should not be too tricky, but the following problems are. </p></li>
<li><p>Second, I need to specify the distribution-speciï¬c variance of my model. Examples in the article (see table 2 &amp; and the supplementary R code of the online article) specify this for a binomial and a Poisson distribution. With some deeper statistical knowledge, it should not be difficult to specify this for a negative binomial distribution. </p></li>
<li><p>Finally, I would need to know if glmmPQL uses additive dispersion or to multiplicative dispersion. In the paper, they state: ""we only consider additive dispersion implementation of GLMMs although the formulae that we present below can be easily modiï¬ed for the use with GLMMs that apply to multiplicative dispersion. "" Thus, in case glmmPQL uses multiplicative dispersion, I would need further help to adjust the formula.</p></li>
</ol>

<p>Can anybody help?</p>

<p>Thanks, best 
Philipp</p>

<p>P.S. R-code is welcome.</p>

<p>Nakagawa &amp; Schielzeth (2013) A general and simple method for obtaining R 2 from generalized linear mixed-effects models. Methods in Ecology and Evolution 2013, 4, 133â€“142. doi: 10.1111/j.2041-210x.2012.00261.x</p>

<p>Their R script:</p>

<pre><code>  #A general and simple method for obtaining R2 from generalized linear mixed-effects models
  #Shinichi Nakagawa1,2 and Holger Schielzeth3
  #1 National Centre of Growth and Development, Department of Zoology, University of    Otago, Dunedin, New Zealand
  #2 Department of Behavioral Ecology and Evolutionary Genetics, Max Planck Institute for Ornithology, Seewiesen, Germany
  #3 Department of Evolutionary Biology, Bielefeld University, Bielefeld, Germany
  #Running head: Variance explained by GLMMs
  #Correspondence:
  #S. Nakagawa; Department of Zoology, University of Otago, 340 Great King Street,    Dunedin, 9054, New Zealand
  #Tel:  +64 (0)3 479 5046
  #Fax: +64 (0)3 479 7584
  #e-mail: shinichi.nakagawa@otago.ac.nz 


  ####################################################
  # A. Preparation
  ####################################################
  # Note that data generation appears below the analysis section.
  # You can use the simulated data table from the supplementary files to reproduce exactly the same results as presented in the paper.

  # Set the work directy that is used for rading/saving data tables
  # setwd(""/Users/R2"")

  # load R required packages
  # If this is done for the first time, it might need to first download and install the package
  # install.package(""arm"")
  library(arm)
  # install.package(""lme4"")
  library(lme4)


  ####################################################
  # B. Analysis
  ####################################################

  # 1. Analysis of body size (Gaussian mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # Read body length data (Gaussian, available for both sexes)
  Data &lt;- read.csv(""BeetlesBody.csv"")

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 | Container), data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model
  # fixef() extracts coefficents for fixed effects
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3] + fixef(mF)[4] * mF@X[, 4]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM
  # Equ. 26, 29 and 30
  # VarCorr() extracts variance components
  # attr(VarCorr(lmer.model),'sc')^2 extracts the residual variance
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + attr(VarCorr(mF), ""sc"")^2)

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +    VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + (attr(VarCorr(mF), ""sc"")^2))

  # AIC and BIC needs to be calcualted with ML not REML in body size models
  m0ML &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data, REML = FALSE)
  mFML &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 | Container), data = Data, REML = FALSE)

  # View model fits for both models fitted by ML
  summary(m0ML)
  summary(mFML)


  # 2. Analysis of colour morphs (Binomial mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())
  # Read colour morph data (Binary, available for males only)
  Data &lt;- read.csv(""BeetlesMale.csv"")

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(Colour ~ 1 + (1 | Population) + (1 | Container), family = ""binomial"", data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(Colour ~ Treatment + Condition + (1 | Population) + (1 | Container), family = ""binomial"", data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model 
  # fixef() extracts coefficents for fixed effects 
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM
  # see Equ. 29 and 30 and Table 2
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + pi^2/3)

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +     VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + pi^2/3)


  # 3. Analysis of fecundity (Poisson mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # Read fecundity data (Poisson, available for females only)
  Data &lt;- read.csv(""BeetlesFemale.csv"")

  # Creating a dummy variable that allows estimating additive dispersion in lmer 
  # This triggers a warning message when fitting the model
  Unit &lt;- factor(1:length(Data$Egg))

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit), family = ""poisson"", data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(Egg ~ Treatment + Condition + (1 | Population) + (1 | Container) + (1 | Unit), family = ""poisson"", data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model 
  # fixef() extracts coefficents for fixed effects 
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM 
  # see Equ. 29 and 30 and Table 2 
  # fixef(m0) returns the estimate for the intercept of null model
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] + log(1 + 1/exp(as.numeric(fixef(m0)))))

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +    VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] + log(1 + 
                                                                       1/exp(as.numeric(fixef(m0)))))


  ####################################################
  # C. Data generation
  ####################################################

  # 1. Design matrices 
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # 12 different populations n = 960
  Population &lt;- gl(12, 80, 960)

  # 120 containers (8 individuals in each container)
  Container &lt;- gl(120, 8, 960)

  # Sex of the individuals. Uni-sex within each container (individuals are sorted at the pupa stage)
  Sex &lt;- factor(rep(rep(c(""Female"", ""Male""), each = 8), 60))

  # Condition at the collection site: dry or wet soil (four indiviudal from each condition in each container)
  Condition &lt;- factor(rep(rep(c(""dry"", ""wet""), each = 4), 120))

  # Food treatment at the larval stage: special food ('Exp') or standard food ('Cont')
  Treatment &lt;- factor(rep(c(""Cont"", ""Exp""), 480))

  # Data combined in a dataframe
  Data &lt;- data.frame(Population = Population, Container = Container, Sex = Sex, Condition = Condition, Treatment = Treatment)


  # 2. Gaussian response: body length (both sexes)
  #---------------------------------------------------

  # simulation of the underlying random effects (Population and Container with variance of 1.3 and 0.3, respectively)
  PopulationE &lt;- rnorm(12, 0, sqrt(1.3))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.3))

  # data generation based on fixed effects, random effects and random residuals errors
  Data$BodyL &lt;- 15 - 3 * (as.numeric(Sex) - 1) + 0.4 * (as.numeric(Treatment) - 1) + 0.15 * (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container] + 
    rnorm(960, 0, sqrt(1.2))

  # save data (to current work directory)
  write.csv(Data, file = ""BeetlesBody.csv"", row.names = F)


  # 3. Binomial response: colour morph (males only)
  #---------------------------------------------------

  # Subset the design matrix (only males express colour morphs)
  DataM &lt;- subset(Data, Sex == ""Male"")

  # simulation of the underlying random effects (Population and Container with variance of 1.2 and 0.2, respectively)
  PopulationE &lt;- rnorm(12, 0, sqrt(1.2))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.2))

  # generation of response values on link scale (!) based on fixed effects and random effects
  ColourLink &lt;- with(DataM, 0.8 * (-1) + 0.8 * (as.numeric(Treatment) - 1) + 0.5 *    (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container])

  # data generation (on data scale!) based on negative binomial distribution
  DataM$Colour &lt;- rbinom(length(ColourLink), 1, invlogit(ColourLink))

  # save data (to current work directory)
  write.csv(DataM, file = ""BeetlesMale.csv"", row.names = F)


  # 4. Poisson response: fecundity (females only)
  #---------------------------------------------------

  # Subset the design matrix (only females express colour morphs)
  DataF &lt;- Data[Data$Sex == ""Female"", ]

  # random effects
  PopulationE &lt;- rnorm(12, 0, sqrt(0.4))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.05))

  # generation of response values on link scale (!) based on fixed effects, random effects and residual errors
  EggLink &lt;- with(DataF, 1.1 + 0.5 * (as.numeric(Treatment) - 1) + 0.1 *   (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container] +   rnorm(480, 
                                                                                                                                                         0, sqrt(0.1)))

  # data generation (on data scale!) based on Poisson distribution
  DataF$Egg &lt;- rpois(length(EggLink), exp(EggLink))

  # save data (to current work directory)
  write.csv(DataF, file = ""BeetlesFemale.csv"", row.names = F)
</code></pre>
"
"0.0905976508333704","0.0940027887907685","110573","<p>Let's say we have this: </p>

<pre><code> model2 &lt;- lmer(milk.amount~(1|cow), data=milk, REML=FALSE)
 model1 &lt;- lmer(milk.amount~(1|cow), data=milk)
 summary(model2)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: milk.amount ~ (1 | cow)
    Data: milk
     AIC      BIC   logLik deviance df.resid 
   186.5    191.6    -90.2    180.5       37 

Scaled residuals: 
     Min      1Q  Median      3Q     Max 
 -2.0244 -0.4104  0.1795  0.6621  1.3879 

Random effects:
 Groups   Name        Variance Std.Dev.
 cow      (Intercept) 6.755    2.599   
 Residual             2.999    1.732   
 Number of obs: 40, groups: cow, 10

Fixed effects:
             Estimate Std. Error t value
 (Intercept)  27.0150     0.8663   31.18
</code></pre>

<p>then</p>

<pre><code>summary(model1)
Linear mixed model fit by REML ['lmerMod']

Formula: milk.amount ~ (1 | cow)
   Data: milk
 REML criterion at convergence: 178.9

 Scaled residuals: 
      Min      1Q  Median      3Q     Max 
  -1.9981 -0.4136  0.1775  0.6561  1.4021 

 Random effects:
  Groups   Name        Variance Std.Dev.
  cow      (Intercept) 7.589    2.755   
  Residual             3.000    1.732   
  Number of obs: 40, groups: cow, 10

 Fixed effects:
              Estimate Std. Error t value
  (Intercept)  27.0150     0.9132   29.58
</code></pre>

<p>Why model1 (with REML) doesn't show AIC, BIC, logLik, deviance coefficients? Is it possibly due to some kind of software dependency?</p>
"
"0.0915365116690398","0.09497693697262","111150","<p>I have been reading about calculating $R^2$ values in mixed models and after reading the R-sig FAQ, other posts on this forum (I would link a few but I don't have enough reputation) and several other references I understand that using $R^2$ values in the context of mixed models is complicated. </p>

<p>However, I have recently came across these two papers below. While these methods do look promising (to me) I am not a statistician, and as such I was wondering if anyone else would have any insight about the methods they propose and how they would compare to other methods that have been proposed. </p>

<blockquote>
  <p>Nakagawa, Shinichi, and Holger Schielzeth. <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00261.x/full"" rel=""nofollow"">""A general and simple method for obtaining R2 from generalized linear mixedâ€effects models.""</a> Methods in Ecology and Evolution 4.2 (2013): 133-142.</p>
  
  <p>Johnson, Paul CD. ""Extension of Nakagawa &amp; Schielzeth's R2GLMM to random slopes models."" Methods in Ecology and Evolution (2014).</p>
</blockquote>

<p>The is method can also be implemented using the r.squaredGLMM function in the <a href=""http://cran.r-project.org/web/packages/MuMIn/index.html"" rel=""nofollow"">MuMIn package</a> which gives the following description of the method.</p>

<blockquote>
  <p>For mixed-effects models, $R^2$ can be categorized into two types. Marginal $R^2$ represents the variance explained by fixed factors, and is defined as:
  $$R_{GLMM}(m)^2 = \frac{Ïƒ_f^2}{Ïƒ_f^2 + \sum(Ïƒ_l^2) + Ïƒ_e^2 + Ïƒ_d^2}$$
  Conditional $R^2$ is interpreted as variance explained by both fixed and random factors (i.e. the entire model), and is calculated according to the equation:
  $$R_{GLMM}(c)^2= \frac{(Ïƒ_f^2 + \sum(Ïƒ_l^2))}{(Ïƒ_f^2 + \sum(Ïƒ_l^2) + Ïƒ_e^2 + Ïƒ_d^2}$$
  where $Ïƒ_f^2$ is the variance of the fixed effect components, and $\sum(Ïƒ_l^2)$ is the sum of all variance components (group, individual, etc.), $Ïƒ_l^2$ is the variance due to additive dispersion and $Ïƒ_d^2$ is the distribution-specific variance.  </p>
</blockquote>

<p>In my analysis I am looking at longitudinal data and I am primarily interested in variance explained by the fixed effects in the model</p>

<pre><code>library(MuMIn) 
library(lme4)

fm1 &lt;- lmer(zglobcog ~ age_c + gender_R2 + ibphdtdep + iyeareducc + apoegeno + age_c*apoegeno + (age_c | pathid), data = dat, REML = FALSE, control = lmerControl(optimizer = ""Nelder_Mead""))

# Jarret Byrnes (correlation between the fitted and the observed values)
r2.corr.mer &lt;- function(m) {
   lmfit &lt;-  lm(model.response(model.frame(m)) ~ fitted(m))
   summary(lmfit)$r.squared
}

r2.corr.mer(fm1)
[1] 0.8857005

# Xu 2003
1-var(residuals(fm1))/(var(model.response(model.frame(fm1))))
[1] 0.8783479

# Nakagawa &amp; Schielzeth's (2013)
r.squaredGLMM(fm1)
      R2m       R2c 
0.1778225 0.8099395 
</code></pre>
"
"0.147945344291062","0.143911791835001","111535","<p>This is my first post, so sorry if it not optimally written. I have a paired samples at two time points in two groups, undergoing the same intervention. I want to test the effect of my intervention on weight.</p>

<p>I'm using R. Here's some data to illustrate: my data frame called <code>df</code>:</p>

<pre><code>      patients   timepoint        group          Weight
        102            1            1            107.30
        104            1            1             94.10
        117            1            1            110.80
        121            1            1            108.90
        153            1            1             95.40
        155            1            1            105.10
        161            1            1             97.70
        162            1            1             83.60
        167            1            1             82.40
        173            1            1             86.40
        176            1            1             81.90
        177            1            1             90.90
        179            1            1             95.30
        101            1            2             81.30
        108            1            2             72.30
        113            1            2             68.50
        170            1            2             89.20
        171            1            2             77.50
        172            1            2             94.50
        175            1            2             78.30
        181            1            2             71.40
        182            1            2             72.80
        183            1            2             73.50
        186            1            2             87.90
        187            1            2             83.50
        188            1            2             70.10
        102            2            1            102.70
        104            2            1             90.40
        117            2            1            107.30
        121            2            1            107.50
        153            2            1             95.00
        155            2            1            102.80
        161            2            1             95.40
        162            2            1             78.30
        167            2            1             81.90
        173            2            1             85.30
        176            2            1             83.10
        177            2            1             90.50
        179            2            1             97.50
        101            2            2             78.40
        108            2            2             72.00
        113            2            2             66.80
        170            2            2             90.20
        171            2            2             77.60
        172            2            2             93.40
        175            2            2             80.30
        181            2            2             72.60
        182            2            2             71.40
        183            2            2             74.20
        186            2            2             88.70
        187            2            2             80.50
        188            2            2             71.20
</code></pre>

<p>Since this is paired data (between time points) and unpaired (between groups), I guess I must use a mixed linear model. Im going for the lme4 package in R.</p>

<p>""timepoints"" and ""group"" will be my fixed effects (I exhaust both). ""patients"" will be my random effect, which also picks up that I have multiple responses per patient. </p>

<p>I will use a random intercept model, but I actually expect that my patients differ with how they react to my experimental manipulation, so a random slopes model would be nice also. However, it seams I over-parametrize the model if doing so.</p>

<p>I will use the likelighood ratio using anova() for a full model and a reduced model.</p>

<pre><code>full = lmer(Weight ~ timepoint + group + (1|patients), data=df,
        REML=FALSE)

reduced = lmer(Weight ~ group + (1|patients), data=df,
                   REML=FALSE)
</code></pre>

<p>""timepoint"" is the main factor in question. I now test using anova():</p>

<pre><code>&gt; anova(reduced,full)
Data: df
Models:
reduced: Weight ~ group + (1 | patients)
full: Weight ~ timepoint + group + (1 | patients)
        Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
reduced  4 309.72 317.52 -150.86   301.72                          
full     5 306.02 315.78 -148.01   296.02 5.695      1    0.01701 *
</code></pre>

<p>Question is, Im I doing it correctly? And how do I interpret the result?</p>

<p>BUT, what I really want is to test if the effect of time is different on the two groups. How do I do this?</p>

<p>Thank you.</p>
"
"0.115785546376299","0.163043584982143","111553","<p>I am trying to fit GLMM's to my data using the glmer function available in R's lme4 package. The data is available at: <a href=""https://onedrive.live.com/redir?resid=1B727FC7180E87DF%21118"" rel=""nofollow"">https://onedrive.live.com/redir?resid=1B727FC7180E87DF%21118</a></p>

<p>I keep getting warning messages. Can anybody help me get rid of them.</p>

<p>I am re-posting this from StackOverflow after someone's kind suggestion. That person also suggested that the main of the issue may be low number of virus positive samples n=12. Which I suspected. But I am also wandering if linear separation could be an issue, as all the virus positives occur in the low food group. Can these problems be resolved using GLMMs or should I think of other statistical tests?</p>

<p>Tried fitting the model:</p>

<pre><code>Food_Treatment.glmer &lt;- glmer(Virus_DNA~Food*Treatment+(1|Set),
                              family=binomial,data=data,method = ""ML"")
</code></pre>

<p>to get the warning messages</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge with max|grad| = 0.001101 (tol = 0.001, component 3)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge: degenerate Hessian with 4 negative eigenvalues
</code></pre>

<p>After running code with more iterations of the model, I still get the same warning messages: </p>

<pre><code>Food_Treatment.glmer &lt;- glmer(Virus_DNA~Food*Treatment+(1|Set),data=data,
                             family=binomial,control=glmerControl(optCtrl=list(maxfun=1e9)))
</code></pre>

<p>I then looked on-line and that people had similar problems and tried the optmizer <code>bobyqa</code>: </p>

<pre><code>Food_Treatment.glmer &lt;- glmer(Virus_DNA~Food*Treatment(1|Set),data=data,family=binomial,
                          control=glmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=1e9))) 
</code></pre>

<p>I then got the very similar warning messages:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge with max|grad| = 0.00393532 (tol = 0.001, component 2)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge: degenerate Hessian with 2 negative eigenvalues
</code></pre>

<p>I then thought of simplifying the model and tried no interactions between explanatory variables, with the code:  </p>

<pre><code>Food.Plus.Treatment.glmer&lt;-glmer(Virus_DNA~Food+Treatment(1|Set),family=binomial,
                                 data=data)
</code></pre>

<p>and </p>

<pre><code>Food.Plus.Treatment.glmer&lt;-glmer(Virus_DNA~Food+Treatment(1|Set),family=binomial,
                                 data=data,control=glmerControl(optCtrl=list(maxfun=1e9)))
</code></pre>

<p>Only to get the warning messages :</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
     Model failed to converge with max|grad| = 0.00248016 (tol = 0.001, component 2)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
      Model failed to converge: degenerate Hessian with 1 negative eigenvalues
</code></pre>

<p>So I tried this simplified model with the optimizers <code>bobyqa</code> and <code>Nelder_Mead</code>, as well as the optimzers <code>nlminb</code> and <code>L-BFGS-B</code> from the package <code>optimx</code>.</p>

<p>All but the <code>bobyqa</code> optimizers produce variations on the 2 warning messages. The <code>bobyqa</code> optimizer produces the 1 warning message:</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv, : 
    Model failed to converge with max|grad| = 0.00139574 (tol = 0.001, component 2)
</code></pre>

<p>P.S. This is my first post on here I hope I have provided enough information without being verbose and it is correctly formatted.</p>
"
"0.147945344291062","0.143911791835001","111569","<p>I have an experiment with a design in which subjects answer four items that are of four different types based on two factors (lets call the factors letter: ""a"" X ""b"" and big: ""A"" X ""a"", resulting in four types of questions A, a, B, b). The order of items (called here 1-4) is held constant and each subject answers one item of each type. The types are randomized. A subject can for example get question-type combinations: 1-a, 2-B, 3-b, 4-A; or 1-B, 2-b, 3-a, 4-A; etc.</p>

<p>I am interested in effects of question types, but expect that the random effects may play a role as well. I tried to use the following model:</p>

<pre><code>glmer(answer ~ (1|subject) + (big*letter|item) + big*letter, data = data, family = binomial(link = ""logit""))  
</code></pre>

<p>When I compare this model with one without random slopes:</p>

<pre><code>glmer(answer ~ (1|subject) + (1|item) + big*letter, data = data, family = binomial(link = ""logit""))
</code></pre>

<p>... the first model is not better in any way than the second:</p>

<pre><code>   Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)
m2  6 1242.1 1272.1 -615.04   1230.1                        
m1 15 1261.2 1336.2 -615.60   1231.2     0      9          1
</code></pre>

<p>So, my first question is whether the model is specified correctly given the design I have. The second question would be, why is it that including random slopes does not improve the model, even though it is possible to see from the data, that the effect of question type obviously differs between the items.</p>

<p>Edit:
Summary table for m1:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: answer ~ (1 | subject) + (big * letter | item) + big * letter 
   Data: data 

      AIC       BIC    logLik  deviance 
1261.2010 1336.2061 -615.6005 1231.2010 

Random effects:
 Groups  Name               Variance Std.Dev. Corr             
 subject (Intercept)        0.71862  0.8477                    
 item    (Intercept)        0.00000  0.0000                    
         bigTRUE            0.04241  0.2059     NaN            
         letterTRUE         0.10219  0.3197     NaN  1.00      
         bigTRUE:letterTRUE 0.05749  0.2398     NaN -1.00 -1.00
Number of obs: 1097, groups: subject, 275; item, 4

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          1.8297     0.1798  10.176  &lt; 2e-16 ***
bigTRUE             -0.9339     0.2413  -3.870 0.000109 ***
letterTRUE          -0.7073     0.2734  -2.587 0.009679 ** 
bigTRUE:letterTRUE   0.7458     0.3159   2.361 0.018212 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) bgTRUE ltTRUE
bigTRUE     -0.683              
letterTRUE  -0.602  0.698       
bgTRUE:TRUE  0.521 -0.786 -0.792
</code></pre>
"
"0.0905976508333704","0.0783356573256404","111915","<p>I just fitted the following linear mixed effects model:</p>

<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
 Formula: price ~ variable + (1 | product)
    Data: podzbior

       AIC       BIC    logLik  deviance  df.resid 
 130840.14 130868.85 -65416.07 130832.14      9674 

Scaled residuals: 
     Min      1Q  Median      3Q     Max 
 -6.2824 -0.3099 -0.0547  0.2201 12.4291 

Random effects:
 Groups           Name     Variance Std.Dev.
 product         (Intercept) 427375   653.7   
 Residual                     25930   161.0   
 Number of obs: 9678, groups: product, 1222

Fixed effects:
                  Estimate Std. Error  t value
 (Intercept)     9.362e+02  1.899e+01    49.29
  variable      -7.521e-04  1.171e-04    -6.42

Correlation of Fixed Effects:
              (Intr)
  variable    -0.050
</code></pre>

<p>That was output from <code>summary(lmerModel)</code>, after the run of <code>lmer</code> I got this warning:</p>

<pre><code>Warning:
  In checkScaleX(X, ctrl = control) :
  Some predictor variables are on very different scales: consider rescaling
</code></pre>

<p>Q1 Predictor variable is numeric from 0 to something like 100k, how It should be scaled? </p>

<p>Random effects with confidence intervals chart for this model looks like this, is it OK?:</p>

<p><img src=""http://i.stack.imgur.com/wR3Lp.png"" alt=""""></p>

<p>I am pretty sure residuals are not OK. What should I do in this case?</p>

<p><img src=""http://i.stack.imgur.com/8hzOE.png"" alt=""""></p>

<p>How can I go deeper with this model diagnostic, besides checking p-values?</p>
"
"0.110959008218296","0.115129433468","113322","<p>I have noticed that if there are interactions between hidden variables not in the model, then the variance estimates are inflated greatly compared to the predictive power of the model itself, and I'm trying to understand why this is.  Here is an illustrative example:</p>

<pre><code>library(lme4)
library(lmerTest)

x1 &lt;- sample(c(0,1),1000,replace=T)
x2 &lt;- sample(c(0,1),1000,replace=T)
y &lt;- x1*x2 + rnorm(1000,sd=0.1)
</code></pre>

<p>Now I execute lmer</p>

<pre><code>&gt; lmer(y~0+(1|x1))
Linear mixed model fit by maximum likelihood  ['merModLmerTest']
Formula: y ~ 0 + (1 | x1)
      AIC       BIC    logLik  deviance  df.resid 
 825.2395  835.0550 -410.6198  821.2395       998 
Random effects:
 Groups   Name        Std.Dev.
 x1       (Intercept) 0.3731  
 Residual             0.3626  
Number of obs: 1000, groups:  x1, 2
No fixed effect coefficients 
</code></pre>

<p>So the model says that x1 explains roughly 50% of the variance components</p>

<pre><code>&gt; my_model &lt;- lmer(y~0+(1|x1))
&gt; var_expl &lt;- as.data.frame(VarCorr(my_model))
&gt; var_expl &lt;- (var_expl[var_expl$grp == 'x1',4])/(var_expl[var_expl$grp == 'x1',4]+sum(var_expl[var_expl$grp != 'x1',4]))
&gt; var_expl
[1] 0.5143418
</code></pre>

<p>However, the model built using x1 has only ~35% linear variance explained in reality</p>

<pre><code>&gt; cor(predict(my_model,as.data.frame(x1)),y)^2
[1] 0.3464968
</code></pre>

<p>So if I were to interpret this as an experiment, I would say that there is a hidden confound x2 that affects the results of x1 directly... but I know that x1 and x2 are uncorrelated.  Again, trying to interpret this practically - there is a hidden factor that happens randomly if I do the experiment but I have not noticed it.  (Note, adding a main effect to x1 and an interaction term gives very comparable results)</p>

<p>Does this mean that interpreting the variance coefficients in a case with hidden confounds does not tell you anything at all about the predictive power of x1 in this case as a variable?  I would like to know if there's a way to correct for such hidden factors and when it is okay to interpret the variance components as percent variance explained (assuming that I only have one variable to look at, but potential confounders that I may not be able to know explicitly)</p>
"
"0.0905976508333704","0.0783356573256404","114213","<pre><code>library(lme4)
    out &lt;- glmer(cbind(incidence, size - incidence)
                 ~ period
                 + (1 | herd),
                 data = cbpp,
                 family = binomial,
                 contrasts = list(period = ""contr.sum""))

summary(out)
Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -2.32337    0.22129 -10.499  &lt; 2e-16 ***
period1      0.92498    0.18330   5.046 4.51e-07 ***
period2     -0.06698    0.22845  -0.293    0.769
period3     -0.20326    0.24193  -0.840    0.401
</code></pre>

<p>I was never in a situation where I needed to fit a generalised linear model with effect coding (<code>contr.sum</code> for <code>R</code> users). Can I apply the same interpretation as in the linear model case? In a normal linear model the intercept would be the grand mean and the $\beta$s (parameters for <code>period1</code>, <code>period2</code>, <code>period3</code> and <code>period4 = (Intercept) - period1 - period2 - period3</code> the effects i.e. how the factor levels deviate from the grand mean.</p>

<p>Here is how I think the analogous interpretation for generalised linear models goes. (I will exponentiate all parameters and hence transform the log-odds(-ratios) to odds(-ratios).) The intercept $\exp((\text{Intercept}))$ would then be the overall <strong>odds</strong> of success vs. failure (sticking here to classical binomial terminology) and the $\beta$s the <strong>log-odds-ratios</strong>. And we get the <strong>odds</strong> for e.g. <code>period1</code> by adding $\text{(Intercept)}+\text{period1}$ and then exponentiating: $\exp(\text{(Intercept)}+\text{period1})$. Is the $\text{(Intercept)}$ really the overall/medium <strong>odds</strong> and the $\beta$s <strong>odds-ratios</strong>?</p>
"
"0.0986302295273746","0.102337274193778","114291","<p>I am running generalised linear mixed effects models in R using the lme4 package. I am wondering if there are any post-hoc tests available for models built using the glmer function?</p>

<p>I know the package lmerTest has a function for post hoc testing lmer models (class merMod), but it won't work for objects of class glmerMod (method given in the <a href=""http://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf"" rel=""nofollow"">lmerTest manual</a>).</p>

<p>My models have either a numeric or integer response variable, two fixed effects (both a factor with 2 levels), and two random effects (both a factor, one nested within the other). I have used either a Gamma (for numeric response) or poisson (for integer response) error family. I want to know which combinations of levels of the fixed effects are statistically different to other combinations.</p>

<ol>
<li>Are there any options for post hoc testing this type of model?</li>
<li>If not, can anyone recommend another method for performing generalised linear mixed effects models in R that do allow for post hoc testing?</li>
</ol>

<p>Edit--</p>

<p>The model I am running for ANCOVA analysis is:</p>

<pre><code>m1&lt;-glmer(data=mydata,FLWR_MASS~BASE_MASS*F1TREAT*SO+
    (1 |LINE/MATERNAL_ID),family=Gamma(link=log))
</code></pre>

<p>Where FLWR_MASS and BASE_MASS are numeric, and F1TREAT and SO are both factors each with 2 levels.</p>

<p>The code I am using for the post hoc testing of the slope is:</p>

<pre><code>testInteractions(m1, custom=list(F1TREAT='control', SO=c(1,-1),
    slope='BASE_MASS', adjustment=""none""))
testInteractions(m1, custom=list(F1TREAT='stress', SO=c(1,-1),
    slope='BASE_MASS', adjustment=""none""))
testInteractions(m1, custom=list(SO='s', F1TREAT=c(1,-1),
    slope=""BASE_MASS"", adjustment=""none""))
testInteractions(m1, custom=list(SO='o', F1TREAT=c(1,-1),
    slope=""BASE_MASS"", adjustment=""none"")) 
</code></pre>

<p>However, as I've mentioned I get the same output regardless of what I specify as the slope (even if it is a term not included in the model)</p>
"
"0.128124426527695","0.132940018808798","114350","<p>Thanks to Rijmen et al.(2003), we can fit GRM to the data with <code>lme4::glmer</code>.</p>

<p>I think Rasch model is straightforward, with <code>data.frame</code> with columns like this</p>

<pre><code> response  person  item
 0         1      1
 0         1      2
 1         1      3
 ...
 1         2      1
 0         2      2
</code></pre>

<p>we can fit Rasch model like this</p>

<pre><code> glmer(response ~ -1 + item + (1|person), data=   , family=""binomial"")
</code></pre>

<p>But how about GRM? The data would be like this</p>

<pre><code> response  person  item
 2         1      1
 4         1      2
 3         1      3
 ...
 1         2      1
 4         2      2
 ...
</code></pre>

<p>For a Likert scale (1 to 5). I thought converting the data like this</p>

<pre><code> response  person item  category
 1          1    1       2
 0          1    1       3
 1          1    2       4
 0          1    2       5
</code></pre>

<p>Because for <code>person1</code>, <code>item1</code>, the response is 2, which means that for response 2, it's yes and for response 3, it's no.</p>

<p>The model would be</p>

<pre><code> response ~ item:category + (1|person)
</code></pre>

<p>But I am not quite sure this is the right way to do...</p>

<p><em>Note</em>: person, item, category variables are all factors</p>

<p>According to <a href=""http://www.jstatsoft.org/v39/i12/paper"" rel=""nofollow"">De Boeck et al. (2011)</a>, GRM cannot be fitted with <code>lmer</code>
which is rather in contrast to Rijmen et al(2003).</p>

<p>=== ADDED</p>

<p>Now I think I am pretty sure it will work, at least for GRM with no slope parameter.</p>

<p>Data should be coded like this.</p>

<pre><code>response  person item  category
 0          1    1       1
 1          1    1       2
 1          1    1       3
 1          1    1       4
 1          1    1       5  (which is always true so should be omitted.)
</code></pre>

<p>for 1-5 category(ordinal) answer.</p>

<p>Main benefit of using GLMM for IRT model is you can put other covariates
(person, item, person-item) into the model.</p>

<p>And for GRM, you can set the difference between the ordinal response is the same,
which can't be handled by ordinary GRM function, for example, ltm::grm.
(Oh, I see ordinal::clmm can handle this, but I doubt it can be useful for a model like this)</p>

<pre><code>  response ~ item + (1 + category|person)
</code></pre>

<p>or this</p>

<pre><code>  response ~ item + (-1 + category|item) + (1|person)
</code></pre>

<p>in this case, category is integer and would be better if coded as -2, -1, 0, 1, 2.</p>

<p><strong>References</strong></p>

<p>Rijmen, F., Tuerlinckx, F., De Boeck, P., &amp; Kuppens, P. (2003). A nonlinear mixed model framework for item response theory. Psychological methods, 8(2), 185.</p>

<p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A., Tuerlinckx, F., &amp; Partchev, I. (2011). The estimation of item response models with the lmer function from the lme4 package in R. Journal of Statistical Software, 39(12), 1-28.</p>

<p>====</p>

<p>Here's my source.</p>

<pre><code>library(ltm)
#Science[c(1,3,4,7)]
Sci.df &lt;- Science[c(1,3,4,7)] # Comfort, Work, Future, Benefit
Sci.df$id = 1:nrow(Sci.df)

Sci.long &lt;- reshape(Sci.df, varying=colnames(Sci.df[-5]), 
                v.names=""Response"", timevar=""item"", idvar=c(""id""), direction=""long"")
Sci.long$id &lt;- as.factor(Sci.long$id)
Sci.long$item &lt;- as.factor(Sci.long$item)

library(ordinal)
Sci.long.clmm &lt;- clmm(Response ~ (1|id)+item, data=Sci.long, threshold=""flexible"",     nAGQ=-21)
summary(Sci.long.clmm)

Positive1=as.integer(Sci.long$Response)&lt;=1
    Positive2=as.integer(Sci.long$Response)&lt;=2
Positive3=as.integer(Sci.long$Response)&lt;=3

Sci.long.sep1=Sci.long
Sci.long.sep1$Response=1; Sci.long.sep1$Positive=Positive1

Sci.long.sep2=Sci.long
Sci.long.sep2$Response=2; Sci.long.sep2$Positive=Positive2

Sci.long.sep3=Sci.long
Sci.long.sep3$Response=3; Sci.long.sep3$Positive=Positive3

Sci.long.sep = rbind(Sci.long.sep1, Sci.long.sep2, Sci.long.sep3)

Sci.long.sep$Response=as.factor(Sci.long.sep$Response)

Sci.long.sep.glmm &lt;- glmer(Positive ~ -1 + Response + item + (1|id), data=Sci.long.sep, family=binomial,
                       nAGQ=21, control=glmerControl(optimizer=""optimx"",
                       optCtrl=list(method=""nlminb""), check.conv.grad= .makeCC(""warning"", tol = 1e-4, relTol = NULL) ))
summary(Sci.long.sep.glmm)
</code></pre>

<p>I tried my best to make it same for clmm and glmer... but the log likelihood is different.</p>

<p>logLik = -1730.6 for glmer
logLik = -1633.5 for clmm</p>

<p>and the parameters r not the same but similar.</p>

<p>Does anyone know why the log likehoods are different?</p>
"
"0.0838771261337098","0.0870296712971673","114678","<p>Can anyone tell us how to evaluate the fit of our generalized linear model with a poisson distribution? We can't really tell if the model is a good fit or not. Do you use the deviance to answer this question? If so, what does it tell us in the following example? </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: poisson  ( log )
Formula: vok ~ factor(koen) + (1 | group) + factor(obs) + rid + aggr +  
    offset(log(min))
   Data: data

     AIC      BIC   logLik deviance df.resid 
   156.1    172.8    -70.0    140.1       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5286 -0.6338 -0.3348  0.5913  4.8183 

Random effects:
 Groups Name        Variance Std.Dev.
 group  (Intercept) 0        0       
Number of obs: 60, groups:  group, 60

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -5.40345    0.37230 -14.514  &lt; 2e-16 ***
factor(koen)1  1.13549    0.38823   2.925  0.00345 ** 
factor(obs)2   0.84057    0.51918   1.619  0.10544    
factor(obs)3   0.55973    0.24933   2.245  0.02477 *  
factor(obs)4  -1.24449    0.55967  -2.224  0.02617 *  
rid            0.10088    0.01939   5.203 1.96e-07 ***
aggr           0.05890    0.02868   2.053  0.04003 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) fct()1 fct()2 fct()3 fct()4 rid   
factor(kn)1 -0.705                                   
factor(bs)2 -0.275 -0.107                            
factor(bs)3 -0.342 -0.065  0.302                     
factor(bs)4 -0.206  0.005  0.157  0.355              
rid         -0.106 -0.343  0.304  0.304  0.248       
aggr        -0.106 -0.129  0.103 -0.313 -0.241 -0.287
</code></pre>
"
"0.218898583001937","0.220816893163752","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.133356131201899","0.127724638671999","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.0661631693578849","0.0514874478890977","115746","<p>I originally learned about random effects models when taking a course on Hierarchical Linear Models, which was taught using Raudenbush and Bryk's HLM book and software, and it sort of indoctrinated me to the subject in a very narrow way. I have a hard time thinking about random effects models in a way other than ""Level-1 formula is..."" and ""Level-2 formula is..."" Now I'm trying to migrate away from the HLM software and use instead <code>lme4</code>. But of course even if I specify the model correctly lme4 will give me slightly different estimates than the HLM software, so I'm not 100% sure that I'm doing this correctly. Could someone please tell me whether I am specifying the following model (from chapter 2 of the HLM help manual) correctly?</p>

<p>Level-1 model:</p>

<p>$$MATHACH_{ij} = \beta_{0j} + \beta_{1j}SES_{ij} + r_{ij}$$</p>

<p>Level-2 models:
$$\beta_{0j} = \gamma_{00} + \gamma_{01}SECTOR_j + \gamma_{02}MEANSES_j + u_{0j}$$
$$\beta_{1j} = \gamma_{10} + \gamma_{11}SECTOR_j + \gamma_{12}MEANSES_j + u_{1j}$$</p>

<p>This yields the following mixed model:</p>

<p>$$MATHACH_{ij} = \gamma_{00} + \gamma_{01}SECTOR_j + \gamma_{02}MEANSES_j + \gamma_{10}SES_{ij} + \gamma_{11}SECTOR_jSES_{ij} + \gamma_{12}MEANSES_jSES_{ij} + u_{0j} + u_{1j}SES_{ij} + r_{ij}$$</p>

<p>But seeing it in mixed model form doesn't quite help me with making sure I'm doing it right in <code>lme4</code>. Here's what I have so far:</p>

<pre><code>lmer(mathach ~ sector + meanses + ses + sector:ses + meanses:ses + (1+ses|school),data=dat)
</code></pre>

<p>We are measuring children (level 1) nested in schools (level 2). <code>ses</code> is a child-level predictor, while <code>sector</code> and <code>meanses</code> are school-level predictors. Subscript <code>i</code> is for children and subscript <code>j</code> is for schools.</p>
"
"0.0369863360727655","0.0383764778226668","116621","<p>Say, I fit a linear or generalised linear model in <code>R</code> with dummy coding (<code>contr.treatment</code> for <code>R</code> users) with a specified reference group:</p>

<pre><code>        library(lme4)
        out1 &lt;- glmer(cbind(incidence, size - incidence)
                      ~ C(period, contr.treatment(4, base=1))
                      + (1 | herd),
                      data = cbpp,
                      family = binomial)
</code></pre>

<p><code>period4</code> is the<code>(Intercept)</code>. And I see that the difference between <code>period1</code> and <code>period4</code> is significant. But what if I'm interested whether there is a difference between say <code>period2</code> and <code>period3</code>. It seems that when people encounter this case they e.g. use <code>glht()</code> from <code>multcomp</code> or <code>TukeysHD</code> to do pairwise comparisons. Couldn't I just refit the model and specify a different reference group? (I would obviously lose any p-value adjustments the aforementioned functions use but otherwise it should be similar.):</p>

<pre><code>        out2 &lt;- glmer(cbind(incidence, size - incidence)
                      ~ C(period, contr.treatment(4, base=2))
                      + (1 | herd),
                      data = cbpp,
                      family = binomial)
</code></pre>
"
"0.0827039616973562","0.0858124131484961","116994","<p>I learned in elementary statistics that, with a general linear model, for inferences to be valid, observations must be independent. When clustering occurs, independence may no longer hold leading to invalid inference unless this is accounted for. One way to account for such clustering is by using mixed models. I would like to find an example dataset, simulated or not, which demonstrates this clearly. I tried using one of the sample datasets on the <a href=""http://www.ats.ucla.edu/stat/stata/library/cpsu.htm"" rel=""nofollow"">UCLA site for analysing clustered data</a></p>

<pre><code>&gt; require(foreign)
&gt; require(lme4)
&gt; dt &lt;- read.dta(""http://www.ats.ucla.edu/stat/stata/seminars/svy_stata_intro/srs.dta"")

&gt; m1 &lt;- lm(api00~growth+emer+yr_rnd, data=dt)
&gt; summary(m1)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 740.3981    11.5522  64.092   &lt;2e-16 ***
growth       -0.1027     0.2112  -0.486   0.6271    
emer         -5.4449     0.5395 -10.092   &lt;2e-16 ***
yr_rnd      -51.0757    19.9136  -2.565   0.0108 * 


&gt; m2 &lt;- lmer(api00~growth+emer+yr_rnd+(1|dnum), data=dt)
&gt; summary(m2)

Fixed effects:
             Estimate Std. Error t value
(Intercept) 748.21841   12.00168   62.34
growth       -0.09791    0.20285   -0.48
emer         -5.64135    0.56470   -9.99
yr_rnd      -39.62702   18.53256   -2.14
</code></pre>

<p>Unless I'm missing something, these results are similar enough that I wouldn't think the output from <code>lm()</code> is invalid.  I have looked at some other examples (e.g. <a href=""http://www.bristol.ac.uk/cmm/learning/course-topics.html#m05"" rel=""nofollow"">5.2 from the Bristol University Centre for Multilevel Modelling</a>) and found the standard errors are also not terribly different (I am not interested in the random effects themselves from the mixed model, but it is worth noting that the ICC from the mixed model output is 0.42).</p>

<p>So, my questions are 1) under what conditions will the standard errors be markedly different when clustering occurs, and 2) can someone provide an example of such a dataset (simulated or not).</p>
"
"0.138390197576366","0.143591631723548","117641","<p><a href=""http://cran.r-project.org/web/packages/effects/index.html""><code>Effects</code> package</a> provides a very fast and convenient way for <a href=""http://www.r-bloggers.com/plotting-mixed-effects-model-results-with-effects-package/"">plotting linear mixed effect model results</a> obtained through <a href=""http://cran.r-project.org/web/packages/lme4/index.html""><code>lme4</code> package</a>. The <code>effect</code> function calculates confidence intervals (CIs) very quickly, but how <a href=""http://glmm.wikidot.com/faq"">trustworthy</a> are these confidence intervals?</p>

<p>For example:</p>

<pre><code>library(lme4)
library(effects)
library(ggplot)

data(Pastes)

fm1  &lt;- lmer(strength ~ batch + (1 | cask), Pastes)
effs &lt;- as.data.frame(effect(c(""batch""), fm1))
ggplot(effs, aes(x = batch, y = fit, ymin = lower, ymax = upper)) + 
  geom_rect(xmax = Inf, xmin = -Inf, ymin = effs[effs$batch == ""A"", ""lower""],
            ymax = effs[effs$batch == ""A"", ""upper""], alpha = 0.5, fill = ""grey"") +
  geom_errorbar(width = 0.2) + geom_point() + theme_bw()
</code></pre>

<p><img src=""http://i.stack.imgur.com/rEF4b.png"" alt=""enter image description here""></p>

<p>According to CIs calculated using <code>effects</code> package, batch ""E"" does not overlap with batch ""A"".</p>

<p>If I try the same using <code>confint.merMod</code> function and the default method:</p>

<pre><code>a &lt;- fixef(fm1)
b &lt;- confint(fm1)
# Computing profile confidence intervals ...
# There were 26 warnings (use warnings() to see them)

b &lt;- data.frame(b)
b &lt;- b[-1:-2,]

b1 &lt;- b[[1]]
b2 &lt;- b[[2]]

dt &lt;- data.frame(fit   = c(a[1],  a[1] + a[2:length(a)]), 
                 lower = c(b1[1],  b1[1] + b1[2:length(b1)]), 
                 upper = c(b2[1],  b2[1] + b2[2:length(b2)]) )
dt$batch &lt;- LETTERS[1:nrow(dt)]

ggplot(dt, aes(x = batch, y = fit, ymin = lower, ymax = upper)) +
  geom_rect(xmax = Inf, xmin = -Inf, ymin = dt[dt$batch == ""A"", ""lower""], 
            ymax = dt[dt$batch == ""A"", ""upper""], alpha = 0.5, fill = ""grey"") + 
  geom_errorbar(width = 0.2) + geom_point() + theme_bw()
</code></pre>

<p><img src=""http://i.stack.imgur.com/B7NTg.png"" alt=""enter image description here""></p>

<p>I see that all of the CIs overlap. I also get warnings indicating that the function failed to calculate trustworthy CIs. This example, and my actual dataset, makes me to suspect that <code>effects</code> package takes shortcuts in CI calculation that might not entirely be approved by statisticians. <strong>How trustworthy are the CIs returned by <code>effect</code> function from <code>effects</code> package for <code>lmer</code> objects?</strong></p>

<p>What have I tried: Looking into the source code, I noticed that <code>effect</code> function relies on <code>Effect.merMod</code> function, which in turn directs to <code>Effect.mer</code> function, which looks like this:  </p>

<pre><code>effects:::Effect.mer
function (focal.predictors, mod, ...) 
{
    result &lt;- Effect(focal.predictors, mer.to.glm(mod), ...)
    result$formula &lt;- as.formula(formula(mod))
    result
}
&lt;environment: namespace:effects&gt;
</code></pre>

<p><code>mer.to.glm</code> function seems to calculate Variance-Covariate Matrix from the <code>lmer</code>object: </p>

<pre><code>effects:::mer.to.glm

function (mod) 
{
...
mod2$vcov &lt;- as.matrix(vcov(mod))
...
mod2
}
</code></pre>

<p>This, in turn, is probably used in <code>Effect.default</code> function to calculate CIs (I might have misunderstood this part):</p>

<pre><code>effects:::Effect.default
...
     z &lt;- qnorm(1 - (1 - confidence.level)/2)
        V &lt;- vcov.(mod)
        eff.vcov &lt;- mod.matrix %*% V %*% t(mod.matrix)
        rownames(eff.vcov) &lt;- colnames(eff.vcov) &lt;- NULL
        var &lt;- diag(eff.vcov)
        result$vcov &lt;- eff.vcov
            result$se &lt;- sqrt(var)
        result$lower &lt;- effect - z * result$se
        result$upper &lt;- effect + z * result$se
...
</code></pre>

<p>I do not know enough about LMMs to judge whether this is a right approach, but considering the discussion around confidence interval calculation for LMMs, this approach appears suspiciously simple.</p>
"
"0.0739726721455309","0.0767529556453336","117853","<p>I've just gotten stuck in interpreting the output for a linear mixed effects model. My model includes Week as a time predictor, and scores on a depression scale as outcome. I have not worked with R before, and while the output is mostly clear to me, there's one part I don't understand. What does my correlation of -0.18 in the random effects mean?</p>

<p>Here's the summary for the lmer model I ran:</p>

<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: HAMD ~ 1 + week + (1 + week | id)
   Data: MD

Random effects:
 Groups   Name        Variance Std.Dev. Corr 
 id       (Intercept)  8.769   2.961         
          week         2.098   1.448    -0.18
 Residual             10.974   3.313         
Number of obs: 340, groups:  id, 60

Fixed effects:
            Estimate Std. Error t value
(Intercept)  23.4602     0.5006   46.87
week         -2.3518     0.2165  -10.86
</code></pre>

<p>Would it then be correct to state that <em>participants with a higher initial score decrease their depression score less rapidly over time</em>?</p>

<p>Thank you for taking the time to read this. I hope someone can help me out.</p>
"
"0.105264957864947","0.10922137064511","118475","<p>I have an unbalanced linear mixed effects model with three fixed factors of various levels and one random factor for my repeated measures data (<a href=""http://stats.stackexchange.com/questions/99742/how-to-analyze-interdependent-interaction-terms-of-lmer-model"">for details see here</a>).
Thanks to your help I managed to do post-hoc tests on the significant interaction terms using <code>lsm</code> from the <strong>lsmeans</strong> package. However, I need to report the F statistic (F value and degrees of freedom) for these post-hoc tests and wonder how???</p>

<p>Here is what I do:</p>

<ol>
<li><p>Model comparison using <code>anova()</code> resulting into the final model
<code>model_final</code>, which reads:</p>

<p><code>sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)</code> . </p></li>
<li><p>I analyze the significant interaction time:cond using <code>lsmeans</code>:</p>

<p><code>posthoc_1 &lt;- glht(model_final, lsm(pairwise ~ cond|time)</code></p>

<p><code>summary(posthoc_1)</code></p></li>
</ol>

<p>and get sth like below for each level of <code>time</code>, here is the example for <code>time1</code>.</p>

<pre><code>&gt; Note: df set to 268 
&gt;
&gt; $`time = time1`
&gt; 
&gt;    Simultaneous Tests for General Linear Hypotheses
&gt; 
&gt; Fit: lme4::lmer(formula = sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)
&gt; 
&gt; Linear Hypotheses:
&gt;                    Estimate Std. Error t value Pr(&gt;|t|) 
&gt; cond1 - cond2 == 0   3.1867     0.6797   4.688 4.39e-06 ***
</code></pre>

<p>This gives me t-values for the various levels of the interaction terms and their corresponding p-value, but no F stats!</p>

<p>My questions: </p>

<ol>
<li>Is there any way of obtaining the F stats? (F value and degree of
freedom) </li>
<li>Or am I stuck with the t-values? If so, is t(0.095;268) =
4.588, p &lt; 0.001 reporting the correct degrees of freedom?</li>
</ol>
"
"0.156919734289782","0.153772183669956","120421","<p>I am new to gam, and most of my knowledge comes from this document <a href=""http://www3.nd.edu/~mclark19/learn/GAMS.pdf"">http://www3.nd.edu/~mclark19/learn/GAMS.pdf</a>. Now I am using generalized addictive model with random effects to model some data, where I want to see how ""speedChange"" correlates with ""response"" in my dataset, with consideration of random effects ""user.id""</p>

<p>The code I run is shown as follows:</p>

<pre><code>speed.gammer &lt;- gamm4(response ~ s(speedChange) , data= t, random=~(1|user.id))
</code></pre>

<p>The gam can be plotted as follows:
<img src=""http://i.stack.imgur.com/xWkaU.jpg"" alt=""enter image description here""></p>

<p>I then try to interpret the gam:</p>

<pre><code>summary(speed.gammer$gam)
</code></pre>

<p>which gives the following :</p>

<pre><code>Family: gaussian 
Link function: identity 

Formula:
response ~ s(speedChange)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.30618    0.01482   155.6   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Approximate significance of smooth terms:
                 edf Ref.df     F p-value    
s(speedChange) 5.875  5.875 28.61  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

R-sq.(adj) =  0.0263   
lmer.REML =  14688  Scale est. = 0.57643   n = 5619
</code></pre>

<p>From what I understand from the output, I learned that speedChange is significantly correlates with response, and the non-linear relationship is as shown in the plot. I know the R-squared is small, but that's not what I want to ask. I actually don't understand the mer model.</p>

<p>If I run:</p>

<pre><code>summary(speed.gammer$mer)
</code></pre>

<p>I got the following results:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']

REML criterion at convergence: 14687.7

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.5908 -0.6500 -0.0454  0.5880  3.7110 

Random effects:
 Groups   Name           Variance Std.Dev.
 user.id  (Intercept)     0.2853  0.5342  
 Xr       s(speedChange) 56.4011  7.5101  
 Residual                 0.5764  0.7592  
 Number of obs: 5619, groups:  user.id, 3042; Xr, 8

Fixed effects:
                    Estimate Std. Error t value
X(Intercept)        2.306181   0.014823  155.58
Xs(speedChange)Fx1 -0.008977   0.115045   -0.08

Correlation of Fixed Effects:
X(Int)
Xs(spdCh)F1 0.004 
</code></pre>

<p>I understand this is an lmerMod. I understand the output for lmer function, but not here. I don't understand what ""X"" means in the fixed effects. From the t-value it seems that the Intercept is significant but not the speedChange. I want to report the result of my analysis, but what is the relationship between the gam results and this mer result? How can I interpret the mer result of  </p>

<pre><code>Xs(speedChange)Fx1 -0.008977   0.115045   -0.08
</code></pre>

<p>together with the gam result:</p>

<pre><code>s(speedChange) 5.875  5.875 28.61  &lt;2e-16 ***
</code></pre>

<p>I don't see any documents that help me to understand the output in order to report the result. Could someone help?</p>
"
"0.104613156193188","0.0814088031193886","120604","<p>After doing a model comparison with my mixed lmer model, I have a model with three main effects, no interaction, say <code>signal ~ factor A + factor B + factor C + (1|subj)</code>.</p>

<p>Factor C has three levels, so I want to do a post-hoc test to see how the levels differ from each other. I tried two methods:</p>

<p><strong>Method 1</strong>: mcp with Tukey (from multcomp package)</p>

<p><code>summary(glht(myModel, mcp(factorC=""Tukey""))</code></p>

<p>where I get the following result:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lme4::lmer(formula = signal ~ factorA + factorB + factor C + (1 | 
    subj), data = s)

Linear Hypotheses:
             Estimate Std. Error z value Pr(&gt;|z|)  
e1 - e2 == 0   0.8071     0.4681   1.724   0.1984    
e1 - e3 == 0   1.9926     0.4681   4.257   &lt;1e-04 ***
e2 - e3 == 0   1.1855     0.4681   2.533   0.0321 *    ---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<hr>

<p><strong>Method 2</strong>: lsm (from lsmeans package)</p>

<pre><code>summary(glht(myModel, lsm(pairwise ~ factorC)))
</code></pre>

<p>giving me the following result: </p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Fit: lme4::lmer(formula = signal ~ factorA + factorB + factorC + 
    (1 | ID), data = s)

Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
e1 - e2 == 0   0.8071     0.4681   1.724    0.198    
e1 - e3 == 0   1.9926     0.4681   4.257   &lt;1e-04 ***
e2 - e3 == 0   1.1855     0.4681   2.533    0.032 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<hr>

<p>The results are pretty similar, and I would guess that the lsm-results are more reliable, since lsmeans is explicitly suited for models with unequal observations. I still wonder, though, whether it is acceptable to do so and would appreciate any comment!</p>
"
"0.148867131055241","0.154462343667293","120650","<p>Field explains how to analyse repeated-measures data using linear mixed-effect models (LME). See Field et al., Discovering Statistics Using R, 2012, p. 573.</p>

<p>However, the way he specifies the model, there appears to be <strong>only one observation per level of the grouping factors</strong>. Is this a mistake in the textbook? If not, why not? It seems to me the random effects specify a full model and fit the data (almost) exactly.</p>

<p>The code is as follows:</p>

<pre><code>library(reshape2)
library(nlme)
# Load dataset:
dat.wide &lt;- read.delim(""http://www.sagepub.com/dsur/study/DSUR%20Data%20Files/Chapter%2013/Bushtucker.dat"")

dat.wide
#   participant stick_insect kangaroo_testicle fish_eye witchetty_grub
# 1          P1            8                 7        1              6
# 2          P2            9                 5        2              5
# 3          P3            6                 2        3              8
# 4          P4            5                 3        1              9
# 5          P5            8                 4        5              8
# 6          P6            7                 5        6              7
# 7          P7           10                 2        7              2
# 8          P8           12                 6        8              1


dat &lt;- melt(dat.wide, variable.name=""animal"", value.name=""retch"")
head(dat)
#   participant       animal retch
# 1          P1 stick_insect     8
# 2          P2 stick_insect     9
# ...

# set contrasts, not relevant to question but keeps example same as in book:
PartvsWhole &lt;- c(1, -1, -1, 1)
TesticlevsEye &lt;- c(0, -1, 1, 0)
StickvsGrub &lt;- c(-1, 0, 0, 1)
contrasts(dat$animal) &lt;- cbind(PartvsWhole, TesticlevsEye, StickvsGrub)

# Fit intercept term, then add ""animal"" term.
# NOTE: random effects are animal nested within participant, as in textbook.
# This would presumably give one observation per group?    
lme1 &lt;- lme(retch ~ 1, random=~1|participant/animal, data=dat, method=""ML"")
lme2 &lt;- lme(retch ~ 1 + animal, random=~1|participant/animal, data=dat, method=""ML"")

# I have checked these are the same results as in textbook
anova(lme1, lme2)    
summary(lme2)

# residuals are near-zero (e-05)
resid(lme2)

ran1 &lt;- random.effects(lme1)
ran2 &lt;- random.effects(lme2)
# same number of random effects as observations:
nrow(ran1$animal)
    nrow(ran2$animal)
nrow(dat)
</code></pre>

<p>The concern of one observation per level seems to be confirmed by using package lme4 instead:</p>

<pre><code>library(lme4)
# Using lme4 produces an error:
# ""Error in checkNlevels(reTrms$flist, n = n, control) : 
# number of levels of each grouping factor must be &lt; number of observations""    
lmer1 &lt;- lmer(retch ~ 1 + (1|participant/animal), data=dat, REML=F)
lmer2 &lt;- lmer(retch ~ 1 + animal + (1|participant/animal), data=dat, REML=F)
anova(lmer1, lmer2)

# see http://stackoverflow.com/questions/19713228/lmer-returning-error-grouping-factor-must-be-number-of-observations
# can force fit with lme4:
# control=lmerControl(check.nobs.vs.nlev = ""ignore"",
#                     check.nobs.vs.rankZ = ""ignore"",
#                     check.nobs.vs.nRE=""ignore""))

lmer1 &lt;- lmer(retch ~ 1 + (1|participant/animal), data=dat, REML=F,
              control=lmerControl(check.nobs.vs.nlev=""ignore"",
                                  check.nobs.vs.rankZ=""ignore"",
                                  check.nobs.vs.nRE=""ignore""))
lmer2 &lt;- lmer(retch ~ 1 + animal + (1|participant/animal), data=dat, REML=F,
              control=lmerControl(check.nobs.vs.nlev=""ignore"",
                                  check.nobs.vs.rankZ=""ignore"",
                                  check.nobs.vs.nRE=""ignore""))
# ignoring errors, we get the same results as with lme:
anova(lmer1, lmer2)
anova(lme1, lme2) # same
</code></pre>

<p>Should the random term should be 1|participant instead?</p>
"
"0.128124426527695","0.110783349007332","120768","<p>I'm using <code>glmer()</code> with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables ""flow"" and ""DNA"" are important but their p values in the model are well above 0.05?</p>

<p>Optimal model:</p>

<pre><code>a25 &lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,
             family=binomial, data=spore)
summary(a25)

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']  
Family: binomial  ( logit ) 
Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   
Data: spore
      AIC      BIC   logLik deviance df.resid 
     72.9     81.0    -32.4     64.9       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9318 -0.8163  0.4435  0.6848  1.6133 

Random effects:  
  Groups Name        Variance Std.Dev.  
  Root   (Intercept) 0.3842   0.6199   
  Number of obs: 56, groups:  Root, 9

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.97752    0.79252  -1.233    0.217   
Flow         3.82779    2.27165   1.685    0.092 . 
DNA          0.01616    0.01039   1.556    0.120  
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr) Flow   Flow -0.775        
     DNA    -0.576  0.227
</code></pre>

<p>Likelihood ratio test:</p>

<pre><code>a26 &lt;- update(a25,~.-DNA)
anova(a25,a26)

Data: spore 
Models: 
    a26: Status_qpcr ~ (1 | Root) + Flow 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
a26  3 74.802 80.878 -34.401   68.802                            
a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *

a27 &lt;- update(a25,~.-Flow)
anova(a25,a27)

Data: spore 
Models: 
    a27: Status_qpcr ~ (1 | Root) + DNA 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
a27  3 78.440 84.723 -36.220   72.440                             
a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **
</code></pre>
"
"0.193806787652043","0.220551495890586","122026","<p>I have a question regarding re-leveling in lme4 1.1-7. </p>

<p><strong>Experimental Design:</strong></p>

<p>Our experiment is an eyetracking while reading study (single sentence stimuli). We are analyzing four different continuous eyetracking DVs  over three different regions of interest.For all DVs, we first removed outliers, then took the log value, then residualized the result (subtracted the actual reading time from a predicted reading time per character - to control for word length differences).</p>

<p>The main manipulation is the categorical factor â€œconditionâ€ which has four levels. Condition is a within-subject (repeated measures) factor that represents four different versions of a single sentential item.</p>

<p>We also have a continuous predictor (Ospan) which is between-subject and is centered. I'm leaving that out of this question though since it does not seem to be related to my problem.</p>

<p>The experimental materials were distributed in a latin square rotation over four presentation lists. This ensured that each particular subject only saw one level of condition for each sentential item, but that each subject would also see an equal number of items representing each level of condition (thus this is a repeated measures design). There are 80 sentence items (each with four levels of condition). There were 45 participants across the four lists (somewhat unbalanced).</p>

<p><strong>My problem (well, one of them):</strong></p>

<p>Working with this model (which omits Ospan) on one eyetracking DV in one region (R02) of the sentence:</p>

<pre><code>(testFirstR02_lmer03 = lmer(RT2LogR ~ condition + (1 + condition | Subject) + (1 + condition | item), data = testFirst[testFirst$Region == ""R02"",],REML = FALSE))
</code></pre>

<p>I obtain convergence when I set the reference level (of condition) to StrongIs or RCE, but the model does not converge when I re-level to a reference level of PseudoC. It produces the following error messages:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.71338 (tol = 0.002, component 6)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>

<p>This is the releveling code that I am using (with variants in accordance to what I am setting the reference level to):</p>

<pre><code>testFirst$condition = factor(testFirst$condition,levels=c(""StrongIs"",""RCE"",""PseudoC"",""NonIs""))
</code></pre>

<p>If I remove the random slope specification of condition for the Item fixed effect, I can get convergence no matter how I set the reference level. </p>

<p>And if I remove only the intercept/slope interaction term for that fixed effect...</p>

<pre><code>(testFirstR02_lmer03 = lmer(RT2LogR ~ condition + (1 + condition | Subject) + (1 | item) + (0 + condition | item), data = testFirst[testFirst$Region == ""R02"",],REML = FALSE))
</code></pre>

<p>... then I get the following:</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p><strong>My Questions:</strong></p>

<p>1) Shouldn't the data behave similarly despite any re-leveling? I know that one can compute contrasts by hand without even using re-level - so I find this error message a bit confusing</p>

<p>2) I'm not sure how to interpret the final warning messages? What I would ""rescale"" in my variables - they are already scaled.</p>

<p>3) Does this behavior signal something inherently unstable in my data? The releveling problem does seem to only happen when I set the reference to PseudoC. I tried adding <code>complete.cases(testFirst$RT2LogR) &amp;</code> to my data specification, thinking that something was going wrong with NAs, but it did not help.</p>

<p>I have not attached any reproducible data, as it is a large data set. I can provide a link if necessary. Any help would be greatly appreciated.</p>

<p><strong>Edit (summary and singularity tests output)</strong></p>

<pre><code>&gt; #Change ref level
&gt; testFirst$condition = factor(testFirst$condition,levels=c(""PseudoC"",""NonIs"",""StrongIs"",""RCE""))
&gt; contrasts(testFirst$condition)
         NonIs StrongIs RCE
PseudoC      0        0   0
NonIs        1        0   0
StrongIs     0        1   0
RCE          0        0   1


&gt; #Model 3 Include condition slope for item

Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.71338 (tol = 0.002, component 6)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; summary(testFirstR02_lmer03.1)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT2LogR ~ condition + (1 + condition | Subject) + (1 + condition |      item)
   Data: testFirst[testFirst$Region == ""R02"", ]

     AIC      BIC   logLik deviance df.resid 
  2989.1   3159.3  -1469.5   2939.1     6670 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.6285 -0.5193  0.0227  0.5468  4.4020 

Random effects:
 Groups   Name              Variance Std.Dev. Corr             
 item     (Intercept)       0.000000 0.0000                    
          conditionNonIs    0.010466 0.1023    NaN             
          conditionStrongIs 0.010966 0.1047    NaN  0.17       
          conditionRCE      0.011935 0.1092    NaN  0.59  0.20 
 Subject  (Intercept)       0.014283 0.1195                    
          conditionNonIs    0.005491 0.0741   -0.61            
          conditionStrongIs 0.013175 0.1148   -0.47  0.75      
          conditionRCE      0.012684 0.1126   -0.55  0.75  0.75
 Residual                   0.083755 0.2894                    
Number of obs: 6695, groups:  item, 80; Subject, 45

Fixed effects:
                   Estimate Std. Error t value
(Intercept)       -0.444822   0.019226 -23.137
conditionNonIs    -0.014357   0.018888  -0.760
conditionStrongIs  0.043049   0.023169   1.858
conditionRCE      -0.002206   0.023181  -0.095

Correlation of Fixed Effects:
            (Intr) cndtNI cndtSI
conditnNnIs -0.476              
cndtnStrngI -0.442  0.496       
conditinRCE -0.489  0.626  0.553


&gt; #Model 3 Exclude condition slope for item

&gt; summary(testFirstR02_lmer03)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT2LogR ~ condition + (1 + condition | Subject) + (1 | item)
   Data: testFirst[testFirst$Region == ""R02"", ]

     AIC      BIC   logLik deviance df.resid 
  3091.7   3200.7  -1529.9   3059.7     6679 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.9890 -0.5127  0.0184  0.5609  4.5757 

Random effects:
 Groups   Name              Variance Std.Dev. Corr             
 item     (Intercept)       0.004827 0.06947                   
 Subject  (Intercept)       0.014292 0.11955                   
          conditionNonIs    0.004862 0.06973  -0.60            
          conditionStrongIs 0.012930 0.11371  -0.45  0.73      
          conditionRCE      0.012017 0.10962  -0.55  0.74  0.73
 Residual                   0.087334 0.29552                   
Number of obs: 6695, groups:  item, 80; Subject, 45

Fixed effects:
                    Estimate Std. Error t value
(Intercept)       -4.476e-01  2.082e-02 -21.503
conditionNonIs    -1.105e-02  1.468e-02  -0.753
conditionStrongIs  4.611e-02  1.992e-02   2.316
conditionRCE       7.436e-05  1.939e-02   0.004

Correlation of Fixed Effects:
            (Intr) cndtNI cndtSI
conditnNnIs -0.544              
cndtnStrngI -0.465  0.628       
conditinRCE -0.535  0.638  0.671
&gt; 


#Singularity test for both models

&gt; tt &lt;- getME(testFirstR02_lmer03.1,""theta"")
&gt; ll &lt;- getME(testFirstR02_lmer03.1,""lower"")
&gt; min(tt[ll==0])
[1] 0

&gt; tt &lt;- getME(testFirstR02_lmer03,""theta"")
&gt; ll &lt;- getME(testFirstR02_lmer03,""lower"")
&gt; min(tt[ll==0])
[1] 0.1889075
</code></pre>
"
"0.152498570332605","0.158230271602029","122336","<p>I have a problem with coding of a 2-level categorical predictor variable in R, and subsequently using it as a random slope in lmer().</p>

<p>I can keep the factor as numeric, coded using the treatment coding:</p>

<pre><code>&gt; unique (b$multi)
[1] 0 1
</code></pre>

<p>Running lmer() using a dataset coded in this way yields:</p>

<pre><code>&gt; l1 = glmer(OK ~ multi + (0 + multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
&gt; summary(l1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4806.5   4838.9  -2398.3   4796.5     4792 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-7.8294 -0.5560 -0.1548  0.5623 14.3342 

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 1.84379  1.3579  
 item    (Intercept) 2.40306  1.5502  
 item.1  multi       0.04145  0.2036  
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>Above there is only one random slope related to <code>multi</code>. However, something very different happens when I convert the variable into a factor:</p>

<pre><code>&gt; b$multi = as.factor(b$multi)
&gt; levels (b$multi)
[1] ""0"" ""1""
</code></pre>

<p>When I fit a model using <code>multi</code> as a random slope variable:</p>

<blockquote>
  <p>l2 = glmer(OK ~ multi + (0+multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
      Warning message:
      In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
        Model failed to converge: degenerate  Hessian with 1 negative eigenvalues</p>
</blockquote>

<p>... the model fails to converge and I get a very different random effects structure:</p>

<pre><code>&gt; summary(l2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4807.8   4853.1  -2396.9   4793.8     4790 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.3636 -0.5608 -0.1540  0.5627 15.2515 

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 subject (Intercept) 1.8375   1.3555       
 item    (Intercept) 0.9659   0.9828       
 item.1  multi0      1.5973   1.2638       
         multi1      1.0224   1.0111   1.00
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>The number of parameters in the model clearly change (reflected by the change in AIC, etc.), and I get two random slopes. </p>

<p>My question is which way of coding the categorical variable is better? Intuition tells me that it is the first one, but I have seen recommendations for both ways of coding in various tutorials and classes about running GLMMs in R and this is why it baffles me. Both types of the predictor variable work identically in ordinary regression using lm().</p>
"
"0.169996378813931","0.176385740092009","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.169492684732523","0.175863114528165","123997","<p>I have a dataset with growth rate as a response variable (<code>resp</code> in the example) and temperature, food availability, and salinity as predictor variables (<code>pred1</code> through <code>pred3</code> in the example). The predictor variables are ""continuous"" with weekly intervals and have different units. Measurements span weekly (with missing values for some samples) throughout a year (<code>week</code> in the example; defined from the beginning of the experiment). I have several samples and I want to quantify (over all samples):</p>

<ol>
<li>How much each predictor variable explains the variation in growth rate</li>
<li>The relative effect of each predictor variable on growth rate</li>
</ol>

<p>I understand that linear mixed models could be a solution for this problem as I have several samples and dependent measurements over time. My question is: <strong>What would be the optimal model formulations using <code>lme4</code> package for R?</strong></p>

<p>Example data is available <a href=""http://pastebin.com/SAHZeV8e"" rel=""nofollow"">here</a>. And here is an overview of it:</p>

<pre><code>library(ggplot2)
tmp &lt;- melt(X, id = c(""Sample"", ""weeks""))
ggplot(tmp, aes(x = weeks, y = value)) + geom_line() + facet_wrap(Sample ~ variable, scales = ""free_y"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/E7Dsv.png"" alt=""enter image description here""></p>

<p>I have tried following:</p>

<p>As a solution for point 1:</p>

<pre><code>library(""lme4"")
library(""MuMIn"")

p1 &lt;- lmer(resp ~ pred1 + (1|Sample) + (1|weeks), data = X)
p2 &lt;- lmer(resp ~ pred2 + (1|Sample) + (1|weeks), data = X)
p3 &lt;- lmer(resp ~ pred3 + (1|Sample) + (1|weeks), data = X)

margr2 &lt;- data.frame(Pred = c(""pred1"", ""pred2"", ""pred3""), marginal.R2 = c(r.squaredGLMM(p1)[[1]], r.squaredGLMM(p2)[[1]], r.squaredGLMM(p3)[[1]]))

ggplot(margr2, aes(x = Pred, y = marginal.R2)) + geom_bar(stat = ""identity"")
</code></pre>

<p>Marginal $R^2$ calculated by the method published <a href=""http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12225/abstract;jsessionid=32160C53D46AD1F97A3CBF86E65B95D4.f02t03"" rel=""nofollow"">here</a> should indicate the overall variance explained by each predictor variable as far as I have understood and assuming that my model formulations are correct.</p>

<p><img src=""http://i.stack.imgur.com/9JkwA.png"" alt=""enter image description here""></p>

<p>For the relative effect (point 2), I think that I first have to have the predictor variables on a same scale. Only then can I compare them by having them all in the model and removing the intercepts:</p>

<pre><code>Xs &lt;- X
Xs[4:6] &lt;- scale(Xs[4:6])

mod &lt;- lmer(resp ~ pred1 + pred2 + pred3 - 1 + (1|weeks) + (1|Sample), data = Xs)
cis &lt;- confint(mod)[4:6,]

releff &lt;- data.frame(par = rownames(cis), lower = cis[,1], est = fixef(mod), upper = cis[,2])
</code></pre>

<p>In order to make the interpretation more intuitive, I scale the effects to maximum absolute value of across confidence intervals (I am only interested in relative effect):</p>

<pre><code>tmp &lt;- c(releff$lower,releff$upper)

add &lt;- 100*releff[c(""lower"", ""est"", ""upper"")]/max(abs(tmp))
colnames(add) &lt;- paste0(""rel."", colnames(add))

releff &lt;- cbind(releff, add)

ggplot(releff, aes(x = par, y = rel.est, ymin = rel.lower, ymax = rel.upper)) + geom_pointrange() + geom_hline(yintercept = 0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/pfeqd.png"" alt=""enter image description here""></p>

<p>Predictor variables are ""significant"", where the CIs do not cross the horizontal line (to my understanding). I am not sure whether these approaches make much sense and that's why I am asking for help.</p>
"
"0.0978566471559948","0.101534616513362","124904","<p>I have 6 groups of fish made up of 8 individuals. Each group is tested three times under different treatments. These group level treatments are <code>hungry</code> , <code>satiated</code> and <code>mixed</code> in which half of the individuals were hungry and half satiated. I want to test for the effect of group treatment (3 levels, <code>hungry</code>, <code>satiated</code> and <code>mixed</code>) as well as individual state <code>hungry</code> or <code>satiated</code>. I've run a linear mixed model on just the <code>mixed</code> group treatment where <code>State</code> was the fixed factor and my random factors were Group and Individual ID nested inside Group. My dependent variable is <code>mean_speed</code>.</p>

<pre><code>mHvsS = lmer(mean_speed ~ State + (1|Group) + (1|Group/ID), REML=F, 
             data=HungryVsSatiated)
</code></pre>

<p>but how do appropriately add the Group level treatment (3 factors <code>hungry</code>, <code>satiated</code> and <code>mixed</code>) into the model? Is this correct?</p>

<pre><code>mHvsS=lmer(mean_speed ~ Treatment + State + Treatment:State + (1|Group) + ((1|Group/ID), 
           REML=F, data=HungryVsSatiated)
</code></pre>

<p>Or does <code>State</code> need to be nested within <code>Treatment</code> as a random factor? I probably also have to throw in <code>Day</code> as a random factor, correct?  </p>
"
"0.133356131201899","0.138368358561332","124944","<p>I am doing various analysis on a small sample. Basically, we have an experiment where 14 subjects (UID 1 ~ 14) used one of the 6 instruments (MID 1 ~ 6) on 3 occasions (Sequence 1 ~ 3). Each time an outcome score was registered (between 1 ~ 100). </p>

<p>The test was double blind. The subjects were told they are measuring 3 different conditions while in reality they were either measuring conditions A, B, A or B, A, B (randomly assigned to the machines and users). The objective was to see if <code>A</code> and <code>B</code> are different or not.</p>

<p>To see if there is any significant difference between the ratings for the conditions A and B, I tried to fit a simple, random intercept model using the nlme package in R. I tried:</p>

<pre><code>f.1 &lt;- lme(Score ~ Condition, random = ~1|UID, data)
</code></pre>

<p>However, for some reason <code>lme</code> fails to fit the model: it gives no error or warning but the variance of the fitted random effect is essentially zero:</p>

<pre><code>&gt; summary(f.1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC   logLik
  349.3259 356.0815 -170.663

Random effects:
 Formula: ~1 | UID
         (Intercept) Residual
StdDev: 0.0009303203 15.98295

Fixed effects: Score ~ Condition 
               Value Std.Error DF   t-value p-value
(Intercept) 77.47619  3.487766 27 22.213700  0.0000
ConditionA  -0.85714  4.932446 27 -0.173776  0.8633
 Correlation: 
           (Intr)
ConditionA -0.707

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.9704269 -0.4677603  0.2472873  0.7835730  1.4628682 

Number of Observations: 42
Number of Groups: 14
</code></pre>

<p>I tried doing the same thing using <code>lme4</code> and got the same results. The estimates for the intercept and the <code>Condition</code> factor is almost identical to a linear model if I use <code>lm</code>.</p>

<p>I am trying hard to understand what <code>lme</code> or <code>lmer</code> fail to estimate the random effect. I generated some data by simulation and both routines had no problem fitting the model so I doubt there is something wrong with the syntax of what I have used.</p>

<p>The data is here:</p>

<pre><code>   UID MID Seq Score Condition
1    1   1   1    90  B
2    1   1   2    85  A
3    1   1   3    75  B
4    2   4   1    75  A
5    2   4   2    95  B
6    2   4   3    85  A
7    3   6   1    60  A
8    3   6   2    82  B
9    3   6   3    85  A
10   4   3   1    60  A
11   4   3   2    70  B
12   4   3   3    75  A
13   5   2   1    85  B
14   5   2   2    85  A
15   5   2   3    85  B
16   6   5   1    90  B
17   6   5   2    95  A
18   6   5   3   100  B
19   7   2   1    90  B
20   7   2   2    70  A
21   7   2   3    50  B
22   8   1   1    70  B
23   8   1   2    75  A
24   8   1   3    80  B
25   9   3   1    90  A
26   9   3   2    30  B
27   9   3   3    90  A
28  10   6   1    50  A
29  10   6   2    85  B
30  10   6   3    92  A
31  11   4   1    50  A
32  11   4   2    85  B
33  11   4   3    92  A
34  12   5   1    65  B
35  12   5   2    50  A
36  12   5   3    90  B
37  13   4   1    65  A
38  13   4   2    70  B
39  13   4   3    80  A
40  14   2   1    60  B
41  14   2   2   100  A
42  14   2   3    80  B
</code></pre>
"
"0.215665546406877","0.223771396077568","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.0978566471559948","0.101534616513362","128750","<p>I am running a glmer model and I want to determine the total variance. My data is for survival and it is coded as 0 and 1, where 1 represents that the individual survived and 0 represents that the individual died. My data represents offspring from a full factorial cross where some individuals are full sibs or half sibs. </p>

<p>When running a glmer model, and there is no residual variance in the summary output. I have read that the residual variance should be (Ï€^2)/3 for generalized linear mixed models with binomial data and logit link function (Nakagawa, S., Schielzeth, H. 2010. Repeatability for Gaussian and non-Gaussian data: a practical guide for biologists. Biol. Rev. 85:935-956.).</p>

<p>Is this true? Or is there a different way to calculate the residual variance for glmer?</p>

<p>Here is my model and output:</p>

<pre><code>model6 = glmer(X09.Nov~(1|Dam)+(1|Sire)+(1|Sire:Dam), family=binomial, data=data)
summary(model6) 

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation 
      [glmerMod]
 Family: binomial  ( logit )
Formula: X09.Nov ~ (1 | Dam) + (1 | Sire) + (1 | Sire:Dam)
   Data: data

    AIC      BIC   logLik deviance df.resid 
 1274.4   1295.3   -633.2   1266.4     1375 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2747  0.3366  0.3931  0.4664  1.1090 

Random effects:
Groups   Name        Variance  Std.Dev. 
Sire:Dam (Intercept) 3.853e-01 6.207e-01
Sire     (Intercept) 4.181e-02 2.045e-01
Dam      (Intercept) 6.036e-09 7.769e-05
Number of obs: 1379, groups:  Sire:Dam, 49; Sire, 7; Dam, 7
Fixed effects:
            Estimate Std. Error z value     Pr    
(Intercept)   1.6456     0.1419    11.6 &lt;2e-16 *
</code></pre>
"
"0.0369863360727655","0.0383764778226668","129148","<p>I am doing linear mixed model and would like to check the assumptions using residual plot and QQ plot. Here is my code:</p>

<pre><code>data1.frame &lt;- read.delim(""height.txt"", fileEncoding=""UTF-16"")
lmer50      &lt;- lmer(response ~ (1|jumper) + group*gender, data=data1.frame, REML=FALSE, 
                    na.action=na.omit)
plot(fitted(lmer50), residuals(lmer50))
</code></pre>

<p>For the residual plot, here is the output:  </p>

<p><img src=""http://i.stack.imgur.com/j70nU.jpg"" alt=""enter image description here""></p>

<pre><code>qqnorm(data1.frame$response)
qqline(data1.frame$response)
</code></pre>

<p>For the qq plot, this is the output:  </p>

<p><img src=""http://i.stack.imgur.com/eLd1J.jpg"" alt=""enter image description here""></p>

<p>Could I ask if both of them look normal? Can I apply linear mixed model in my rating scale data?</p>
"
"0.165407923394712","0.163043584982143","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.0369863360727655","0.0383764778226668","130476","<p>This model is a simple linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>And this model adds <code>cyl</code> as a random effect:</p>

<pre><code>library(lme4)
mtcars_mixed_effects &lt;- lmer(mpg ~ wt + (1 | cyl), mtcars)
</code></pre>

<p>Is there a way to test whether adding <code>cyl</code> as random effect is worthwhile? I tried this but it threw an error:</p>

<pre><code>anova(mtcars_mixed_effects, mtcars_lm)
</code></pre>

<p>(please disregard the fact that <code>cyl</code> only has three groups, I'm just using one of R's built in datasets to make question reprodicible).</p>
"
"0.125587057920969","0.130307282495789","130714","<p>Consider the following data from a two-way within subjects design:</p>

<pre><code>df &lt;- ""http://personality-project.org/r/datasets/R.appendix4.data""
df &lt;- read.table(df,header=T)
head(df)

Observation Subject Task Valence Recall
1           1     Jim Free     Neg      8
2           2     Jim Free     Neu      9
3           3     Jim Free     Pos      5
4           4     Jim Cued     Neg      7
5           5     Jim Cued     Neu      9
6           6     Jim Cued     Pos     10
</code></pre>

<p>I would like to analyze this using mixed-linear models. Considering all possible fixed- and random-effects there are multiple possible models:</p>

<pre><code># different fixed effects with random-intercept
a0 &lt;- lmer(Recall~1 + (1|Subject), REML=F,df)
a1 &lt;- lmer(Recall~Task + (1|Subject), REML=F,df)
a2 &lt;- lmer(Recall~Valence + (1|Subject), REML=F,df)
a3 &lt;- lmer(Recall~Task+Valence + (1|Subject), REML=F,df)
a4 &lt;- lmer(Recall~Task*Valence + (1|Subject), REML=F,df)

# different fixed effects with random-intercept-random-slope
b0 &lt;- lmer(Recall~1 + (1|Subject) + (0+Task|Subject) + (0+Valence|Subject), REML=F,df)
b1 &lt;- lmer(Recall~Task + (1|Subject) + (0+Task|Subject) + (0+Valence|Subject), REML=F,df)
b2 &lt;- lmer(Recall~Valence + (1|Subject) + (0+Task|Subject) + (0+Valence|Subject), REML=F,df)
b3 &lt;- lmer(Recall~Task+Valence + (1|Subject) + (0+Task|Subject) + (0+Valence|Subject), REML=F,df)
b4 &lt;- lmer(Recall~Task*Valence + (1|Subject) + (0+Task|Subject) + (0+Valence|Subject), REML=F,df)

# different fixed effects with random-intercept-random-slope including variance-covariance matrix
c0 &lt;- lmer(Recall~1 + (1 + Valence + Task|Subject), REML=F,df)
c1 &lt;- lmer(Recall~Task + (1 + Valence + Task|Subject), REML=F,df)
c2 &lt;- lmer(Recall~Valence + (1 + Valence + Task|Subject), REML=F,df)
c3 &lt;- lmer(Recall~Task+Valence + (1 + Valence + Task|Subject), REML=F,df)
c4 &lt;- lmer(Recall~Task*Valence + (1 + Valence + Task|Subject), REML=F,df)
</code></pre>

<ol>
<li><p>What is the recommended way to select the best fitting model in this context? When using log-likelihood ratio tests what is the recommended procedure? Generating models upwards (from null model to most complex model) or downwards (from most complex model to null model)? Stepwise inclusion or exclusion? Or is it recommended to put all models in one log-likelihood ratio test and select the model with the lowest p-value? How to compare models that are not nested?</p></li>
<li><p>Is it recommended to first find the appropriate fixed-effects structure and then the appropriate random-effects structure or the other way round (I have found references for both options...)?</p></li>
<li><p>What is the recommended way of reporting results? Reporting the p-value from the log-likelihood ratio test comparing the full mixed-model (with the effect in question) to reduced model (without the effect in question). Or is it better to use log-likelihood ratio test to find the best fitting model and then use lmerTest to report p-values from the effects in the best fitting model?</p></li>
</ol>
"
"0.12266979912335","0.127280377713181","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.116961064294386","0.121357078494567","132841","<p>TL;DR: <code>lme4</code> optimization appears to be linear in the number of model parameters by default, and is <em>way</em> slower than an equivalent <code>glm</code> model with dummy variables for groups. Is there anything I can do to speed it up?</p>

<hr>

<p>I'm trying to fit a fairly large hierarchical logit model (~50k rows, 100 columns, 50 groups). Fitting a normal logit model to the data (with dummy variables for group) works fine, but the hierarchical model appears to be getting stuck: the first optimization phase completes fine, but the second goes through a lot of iterations without anything changing and without stopping.</p>

<p><strong>EDIT:</strong> I suspect the problem is mainly that I have so many parameters, because when I try to set <code>maxfn</code> to a lower value it gives a warning:</p>

<pre><code>Warning message:
In commonArgs(par, fn, control, environment()) :
  maxfun &lt; 10 * length(par)^2 is not recommended.
</code></pre>

<p>However, the parameter estimates aren't changing at all over the course of the optimization, so I'm still confused about what to do. When I tried to set <code>maxfn</code> in the optimizer controls (despite the warning), it seemed to hang after finishing the optimization.</p>

<p>Here's some code that reproduces the problem for random data:</p>

<pre><code>library(lme4)

set.seed(1)

SIZE &lt;- 50000
NGRP &lt;- 50
NCOL &lt;- 100

test.case &lt;- data.frame(i=1:SIZE)
test.case[[""grouping""]] &lt;- sample(NGRP, size=SIZE, replace=TRUE, prob=1/(1:NGRP))
test.case[[""y""]] &lt;- sample(c(0, 1), size=SIZE, replace=TRUE, prob=c(0.05, 0.95))

test.formula = y ~ (1 | grouping)

for (i in 1:NCOL) {
    colname &lt;- paste(""col"", i, sep="""")
    test.case[[colname]] &lt;- runif(SIZE)
    test.formula &lt;- update.formula(test.formula, as.formula(paste("". ~ . +"", colname)))
}

print(test.formula)

test.model &lt;- glmer(test.formula, data=test.case, family='binomial', verbose=TRUE)
</code></pre>

<p>This outputs:</p>

<pre><code>start par. =  1 fn =  19900.78 
At return
eval:  15 fn:      19769.402 par:  0.00000
(NM) 20: f = 19769.4 at           0     &lt;other numbers&gt;
(NM) 40: f = 19769.4 at           0     &lt;other numbers&gt;
</code></pre>

<p>I tried setting <code>ncol</code> to other values, and it appears that the number of iterations done is (approximately) 40 per column. Obviously, this becomes a huge pain as I add more columns. Are there tweaks I can make to the optimization algorithm that will reduce the dependence on the number of columns?</p>
"
"0.12266979912335","0.127280377713181","132971","<p>There is something I'm not quite understanding conceptually about the output from generalized linear mixed models. I have read that the target of inference in GLMMs is subject-specific. For example, the accepted answer to <a href=""http://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm"">this</a> question states that in a logistic GLMM the odds-ratios are conditioned on both the fixed and random effects. So, in a GLMM of pupils within classrooms, with random intercepts for classroom (i.e., the ""subject"" in this case), the odds-ratios will differ for each classroom as there will be many random intercepts. So far, this makes sense to me.</p>

<p>What I am confused about is that the typical output from the fixed effects part of such a model reports just one odds-ratio. For example, in the R example I provide below, the odds-ratio for the fixed effect of <code>sex</code> is .662. I have three questions:</p>

<ol>
<li><p><strong>How do I interpret this single fixed effect odds-ratio?</strong><br>
(Is it an odds-ratio ignoring the random effects? Is it an odds-ratio of the average random effect - in which case, isn't it a population average? Is it calculated assuming the random effect variance is zero?) </p></li>
<li><p><strong>Is it possible to calculate a population average odds-ratio using the output from a GLMM?</strong><br>
I know this can be done using a GEE, but what about a GLMM?</p></li>
<li><p><strong>How would I go about calculating the odds-ratio for a particular random effect (a particular classroom, lets say class 7 in the example below)?</strong><br>
Presumably this involves combining the fixed and random effect estimates somehow.</p></li>
</ol>

<p><strong>EDIT 1:</strong>
It seems after doing more reading (for example, this <a href=""http://stats.stackexchange.com/questions/32419/difference-between-generalized-linear-models-generalized-linear-mixed-models-i?lq=1"">post</a>), that since the fixed effect for <code>sex</code> in this example does not have its own random effect (e.g., a random slope), there will be no subject-level interpretation of this parameter. Does this mean that only the intercept term in the model below is subject-specific, while the <code>sex</code> term is a population average?</p>

<pre><code># dummy data:
set.seed(1)
dat &lt;- data.frame(Y         = factor(sample(rep(c(0, 1), 100))),
                  sex       = factor(sample(rep(c(""M"", ""F""), 100))),
                  classroom = factor(sample(rep(paste(""class"", 1:10), 20)))
) 

# model:
library(lme4)
fit &lt;- glmer(Y ~ sex + (1 | classroom), family=binomial, data=dat)

# summary(fit)
exp(fixef(fit))
# (Intercept)   sexM 
#  1.229       0.662 
</code></pre>
"
"0.0739726721455309","0.0767529556453336","134081","<p>I'm looking at the impact of dietary treatment and sex on weight.</p>

<p>My dataset comprises weight data for 3 dietary treatments and sex (male and female). The experimental design was run in duplicate, so that's 2 fixed factors (temperature and sex) alongside the duplicate tank from which they were sampled (random factor). There are 50 individual animals were weighed per tank. </p>

<pre><code>dataset &lt;- structure(list(Sex = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L), .Label = c(""Female"", ""Male""), class = ""factor""), Diet = structure(c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c(""A"", ""B"", ""C""), class = ""factor""), 
    Replicate = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L), Weight..g. = c(1.03, 1.02, 1.04, 1, 1.42, 
    0.93, 0.83, 1, 0.75, 1.02, 0.93, 0.72, 1.02, 0.88, 0.96, 
    1.23, 0.96, 0.95, 1.3, 0.99, 1.06, 0.79, 0.84, 0.88, 0.77, 
    0.57, 1.24, 1.05, 1.12, 0.8, 0.7, 1.46, 0.93, 1.22, 1.06, 
    0.97, 1.33, 1.11, 0.47, 1.59, 1.31, 0.96, 0.69, 1.27, 0.87, 
    0.41, 1.06, 0.95, 0.94, 1.33, 1.18, 1.34, 1.25, 1.44, 1.44, 
    1.5, 1.01, 0.9, 1.11, 0.82, 1.58, 1.08, 1.54, 1.13, 1.38, 
    1.28, 1.15, 1.13, 1.35, 1.2, 1.04, 1.44, 1.14, 1.37, 0.98, 
    1.43, 1.36, 1.16, 1.29, 1.23, 1.47, 0.89, 0.95, 1.14, 1.08, 
    1.11, 1.31, 1.02, 1.02, 1.47, 0.91, 1.21, 0.96, 1.08, 1.26, 
    0.96, 1.05, 1.27, 1.04, 1.26, 1.3, 1.26, 1.29, 1.34, 1.21, 
    1.23, 1.28, 0.73, 0.89, 0.95, 0.89, 1.22, 1.24, 0.78, 1.34, 
    0.86, 0.86, 1.16, 0.86, 0.8, 1.19, 0.44, 1.11, 0.76, 0.9, 
    0.91, 1.11, 1.29, 0.99, 1.31, 1.08, 1.21, 1.22, 1.23, 1.19, 
    1.53, 1.04, 0.94, 1.28, 0.85, 1.08, 1.23, 0.94, 0.94, 1.21, 
    1.16, 1.25, 0.9, 0.97, 1.08, 1.37, 1.09, 1.63, 1.2, 1.23, 
    1.3, 1.27, 1.03, 0.97, 1.33, 1.42, 1.05, 0.98, 1.38, 0.36, 
    0.94, 0.95, 0.91, 1.11, 1, 1.12, 0.98, 1.04, 1.17, 0.96, 
    1.3, 0.92, 0.93, 1.06, 1.16, 1.23, 1.07, 1.08, 0.92, 1.28, 
    1.11, 0.87, 1.02, 1.01, 1.14, 1.01, 1.05, 0.87, 1.22, 0.97, 
    1.16, 1.06, 0.81, 1.13, 0.88, 1.09, 1.27, 1.43, 1.17, 0.9, 
    0.79, 1.2, 1.36, 1.27, 0.68, 1.08, 0.86, 1.15, 1.33, 0.97, 
    1.39, 0.9, 0.77, 1.04, 0.92, 1.07, 1.12, 1.15, 0.93, 0.97, 
    1.21, 1.37, 0.82, 1.17, 0.89, 1.17, 1.18, 1.21, 1.09, 1.1, 
    0.72, 0.41, 1.27, 1.16, 1.23, 1.21, 1.2, 1.24, 1.3, 1.08, 
    1.16, 1.36, 0.63, 1.07, 1.01, 1.26, 1.57, 1.37, 1.38, 1.19, 
    1.31, 1.27, 1.2, 1.63, 1.43, 1.3, 0.96, 1.1, 1.43, 1.36, 
    1.14, 1.14, 1.01, 1.31, 1.3, 1.23, 1.19, 1.16, 1.3, 1.22, 
    1.15, 1.13, 1.34, 1.29, 1.41, 1.22, 1.42, 1.53, 1.43, 1.11, 
    1.21, 1.43, 1.01, 1.22, 1.05, 0.95, 1.4, 1.41, 0.69, 1.29, 
    1.36, 1.24, 1.42, 1.18, 1.2, 0.99, 1.09, 1.04, 0.92, 0.75, 
    0.8, 0.84, 1.09, 0.83, 0.96, 0.99, 0.76, 1.14, 0.84, 0.72, 
    0.98, 0.93, 1.06, 1.29, 0.77, 0.92, 0.72, 0.88, 1.42, 1.07, 
    0.73, 0.6, 0.81, 1.12, 0.81, 1.09, 0.89, 0.76, 0.82, 1.02, 
    0.93, 0.87, 0.68, 0.67, 0.77, 1, 1.17, 0.75, 0.72, 0.82, 
    0.6, 1.11, 0.78, 1.08, 0.48, 0.89, 0.69, 0.71, 0.88, 0.91, 
    0.92, 0.55, 0.84, 0.8, 0.43, 0.98, 0.67, 0.85, 1.11, 0.99, 
    0.89, 0.58, 0.9, 0.89, 0.85, 0.87, 0.72, 0.89, 1.06, 0.81, 
    0.83, 0.79, 0.9, 0.87, 0.81, 0.73, 0.77, 0.91, 0.79, 0.98, 
    0.77, 0.72, 0.81, 0.84, 0.75, 0.82, 1.05, 0.61, 0.93, 0.77, 
    0.86, 0.78, 0.77, 0.72, 0.76, 1.22, 0.79, 0.99, 0.99, 0.51, 
    0.96, 0.81, 1.07, 1.1, 0.83, 0.9, 0.9, 0.79, 0.79, 1.22, 
    1.03, 0.59, 1.05, 0.93, 0.72, 0.93, 0.64, 0.94, 0.81, 0.77, 
    0.62, 0.81, 0.98, 0.79, 0.92, 0.98, 0.66, 0.74, 0.91, 0.4, 
    1.05, 0.85, 0.9, 0.94, 0.84, 0.32, 0.87, 0.86, 0.87, 0.82, 
    0.9, 0.21, 0.55, 0.86, 0.87, 1.21, 1.07, 1.02, 1.52, 1.13, 
    1.17, 1.19, 1.21, 0.93, 0.92, 1.19, 0.96, 1.07, 0.93, 0.97, 
    1.15, 1.07, 1.31, 1.21, 1, 1.13, 0.94, 1.3, 1.02, 1.05, 0.87, 
    0.95, 0.5, 1.06, 1.23, 1.27, 0.76, 1, 0.75, 0.98, 0.91, 1.07, 
    0.97, 1.09, 1.17, 1.19, 0.87, 1.12, 1, 1.07, 1.06, 0.91, 
    0.82, 1.37, 1.05, 1.31, 1.28, 1.25, 1.27, 1.16, 0.58, 0.88, 
    0.97, 1.09, 0.59, 1.36, 1.27, 0.84, 0.81, 1.16, 1.14, 1.01, 
    1.31, 1.23, 0.87, 0.78, 0.74, 0.97, 1.18, 0.85, 0.53, 0.57, 
    0.95, 1.2, 1, 0.68, 0.85, 0.94, 0.96, 0.5, 1.36, 0.87, 1.25, 
    1.07, 0.68, 1.43, 1.02, 1.15, 1.41, 1.02, 1.03, 0.44, 1.09, 
    1.01, 1.15, 1.13, 1.25, 1.58, 1.29, 1.18, 1.26, 1.08, 1.29, 
    1.21, 1.36, 1.22, 0.84, 1.08, 1, 0.88, 0.8, 0.75, 1.36, 1.3, 
    1.16, 1.4, 1.26, 0.82, 0.98, 1.43, 0.97, 1.3, 1.15, 1.29, 
    1.12, 1.17, 1.14, 1.54, 0.97, 1.03, 1.28, 1.4, 1.12, 1.13, 
    1.21, 1.15, 1.03, 1.03, 1.15, 1.24, 0.89, 0.8, 1.22, 1.26, 
    1.19)), .Names = c(""Sex"", ""Diet"", ""Replicate"", ""Weight..g.""
), class = ""data.frame"", row.names = c(NA, -600L))
</code></pre>

<p>I've done some reading to establish that I need to do a linear mixed effects model. I have written the following code but keep getting error about rank deficiency and the dropping of 3 columns. The reasons for which this is happening is over my head and rank deficiencies are seemingly from limitless causes according to other posts!!! </p>

<p>Does anyone know where I might be going wrong here?</p>

<pre><code>lmer(Weight..g. ~ Diet * Sex + (1|Replicate), data = dataset)
</code></pre>
"
"0.128124426527695","0.121861683908065","134394","<p>I would like to fit a basic ""animal model"" (a kind of linear mixed model, see below) using the latest version of <a href=""http://cran.r-project.org/web/packages/lme4/"" rel=""nofollow"">lme4</a> on CRAN (v1.7). To check the results, I am fitting it on simulated data, and comparing the results with the <a href=""http://cran.r-project.org/web/packages/rrBLUP/index.html"" rel=""nofollow"">rrBLUP</a> package. However, the results of lme4 are really strange and I don't succeed in understanding why (even after reading the <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">vignette</a> and this <a href=""http://stackoverflow.com/a/19382162/597069"">answer</a>). My R code is available as a GitHub Gist <a href=""https://gist.github.com/timflutre/43daacf2c8868f609489"" rel=""nofollow"">here</a>. Any help would be much appreciated!</p>

<p>Notations:</p>

<ul>
<li><p>$N$: number of animals, known</p></li>
<li><p>$\mu$: global mean, unknown</p></li>
<li><p>$P$: number of fixed effects, known</p></li>
<li><p>$X$: $N \times P$ design matrix of fixed effects, known</p></li>
<li><p>$\boldsymbol{b}$: $P$-dimensional vector of fixed effects, unknown</p></li>
<li><p>$W = [ \boldsymbol{1} \; X]$: $N \times (P+1)$ matrix, known</p></li>
<li><p>$a = [\mu \; \boldsymbol{b}']'$: $(P+1)$ vector, unknown</p></li>
<li><p>$Q$: number of ""genetics"" random effects (in this document, $Q=N$), known</p></li>
<li><p>$Z$: $N \times Q$ design matrix of ""genetics"" random effects, known</p></li>
<li><p>$\sigma_u^2$: variance component of the ""genetics"" random effects, unknown</p></li>
<li><p>$A$: $Q \times Q$ matrix of additive relationships (obtained from pedigree or from molecular markers), known</p></li>
<li><p>$G = \sigma_u^2 A$: $Q \times Q$ covariance matrix of the ""genetic"" random effects, unknown</p></li>
<li><p>$\boldsymbol{u}$: $Q$-dimensional vector (often called ""breeding values""), unknown</p></li>
<li><p>$\sigma^2$: variance component of the errors, unknown</p></li>
<li><p>$R = \sigma^2 I_N$: $N \times N$ covariance matrix of the errors, unknown</p></li>
<li><p>$\boldsymbol{e}$: $N$-dimensional vector of errors, unknown</p></li>
<li><p>$\mathcal{N}_N$: multivariate Normal distribution of dimension $N$</p></li>
</ul>

<p>Likelihood:</p>

<p>$\boldsymbol{y} = W \boldsymbol{a} + Z \boldsymbol{u} + \boldsymbol{e}$ where $\boldsymbol{u} \sim \mathcal{N}_N(\boldsymbol{0}, G)$ and $\boldsymbol{e} \sim \mathcal{N}_N(\boldsymbol{0}, R)$</p>

<p>We also assume zero covariance between $\boldsymbol{u}$ and $\boldsymbol{e}$:</p>

<p>$\begin{pmatrix}
\boldsymbol{y} \\
\boldsymbol{u} \\
\boldsymbol{e}
\end{pmatrix}
= \mathcal{N}
\begin{pmatrix}
W \boldsymbol{a}, &amp; V &amp; ZG &amp; R \\
\boldsymbol{0}, &amp; GZ' &amp; G &amp; \boldsymbol{0} \\
\boldsymbol{0}, &amp; R &amp; \boldsymbol{0} &amp; R
\end{pmatrix}
$</p>

<p>where $V = Var(\boldsymbol{y}) = ZGZ' + R$</p>

<p>Primary goal: estimate $\sigma_u^2$ and $\sigma^2$</p>
"
"0.148201971273683","0.153772183669956","135416","<p>I have data from a series of psychology experiments in which human subjects completed one of many tasks. Multiple observations are taken per subject. Finally, tasks can be divided into one of two types. I would like to fit a model with a fixed effect for type of task, and then random intercepts for task and subject. Below are simulated data, with the lmer model I am trying to fit.</p>

<pre><code>N = 1000
subject = rep(rnorm(N/2), each=2)             # two obs per subject
task = rep(rnorm(10, sd=.5), each=N/10)       # 10 tasks
error = rnorm(N, sd=.25)
type = rep(c(1, 5), each=N/2)                 # fixed effects of 1 or 5
dv = subject + task + type + error


df = data.frame(subject = rep(1:(N/2), each=2), 
                task    = rep(letters[1:10], each=N/10),
                type    = rep(c('x','y'), each=N/2),
                dv      = dv
)

fit = lmer(dv ~ type + (1 | task:subject) + (1 | task), data=df)
</code></pre>

<p>Here, the outputs of <code>lmer</code> are consistent with the parameters set in the simulation.</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: dv ~ type + (1 | task:subject) + (1 | task)
   Data: df

REML criterion at convergence: 1851.7

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0617 -0.4636  0.0018  0.4857  2.4420 

Random effects:
 Groups       Name        Variance Std.Dev.
 task:subject (Intercept) 0.93276  0.9658  
 task         (Intercept) 0.20511  0.4529  
 Residual                 0.06843  0.2616  
Number of obs: 1000, groups:  task:subject, 500; task, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept)   1.0859     0.2119   5.125
typey         3.9994     0.2996  13.348

Correlation of Fixed Effects:
      (Intr)
typey -0.707
</code></pre>

<p>When I fit my data without the fixed-effect, it estimates the variance for <code>task:subject</code> and <code>task</code>..</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: ACC.ser ~ (1 | task:subid) + (1 | task)
   Data: dat.sub

REML criterion at convergence: -350.2

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.84997 -0.55776  0.01604  0.53404  2.03947 

Random effects:
 Groups     Name        Variance Std.Dev.
 task:subid (Intercept) 0.023145 0.15214 
 task       (Intercept) 0.003473 0.05893 
 Residual               0.008479 0.09208 

...
</code></pre>

<p>However, when I add in the fixed effect to the model, the variance estimate for task becomes 0. Moreover, when I examine the intercepts for task, they're all 0 as well. What disparities could be causing the addition of the fixed effect in <code>lmer</code> to reduce variance estimates to 0? Task is nested within type, as in the simulation. I'm happy to clarify any details that might be useful.</p>
"
"0.104613156193188","0.108545070825851","135681","<p>I'm wondering about the effect of true correlations among random effects on the standard error of my fixed effects in <code>lme4::lmer</code> models in R. </p>

<p>My assumption is that if there are true correlations--as indicated by a significant improvement in model fit when the correlation parameters are added to the model--then the inclusion of these parameters should improve the precision of the estimates somewhere else in the model. In particular, I would expect the standard error of the fixed effects to be smaller in the model in which the correlation parameters are contributing to the model fit.</p>

<p>However, a number of people have pointed out that the inclusion of correlation parameters do not improve the precision of the estimates of fixed effects even when they ""improve"" model fits:</p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/49832/in-a-multi-level-model-what-are-the-practical-implications-of-estimating-versus"">In a multi-level model, what are the practical implications of estimating versus not-estimating random effect correlation parameters?</a></li>
<li>I've conducted simulations of my own to the same end and presented them at a recent R user group meeting (Part 3: <a href=""http://github.com/pedmiston/visualizing-lmer"" rel=""nofollow"">http://github.com/pedmiston/visualizing-lmer</a>)</li>
<li>Shravan Vasishth has a few blog posts on a similar question, e.g., <a href=""http://vasishth-statistics.blogspot.com/2014/11/should-we-fit-maximal-linear-mixed.html"" rel=""nofollow"">http://vasishth-statistics.blogspot.com/2014/11/should-we-fit-maximal-linear-mixed.html</a></li>
</ol>

<p>I'm going to push the dialog even farther and challenge someone to demonstrate a situation in which including random effect correlation parameters does anything other than add complexity to the model. </p>

<p>My ignorance might be due to an over-interpretation of fixed effects as the best indicator of ""average behavior"", so I am interested to see the conditions under which random correlation parameters are useful to people who use these models to make inferences (as opposed to simply observing a correlation in the sample).</p>

<p>Thanks for your help.</p>
"
"0.0978566471559948","0.101534616513362","135840","<p>I have a data set that I expect there to be some variability among individuals; therefore, I chose to include <code>ID</code> as a random effect in the <code>glmer</code> model. However, when I run the model I get the following warning: </p>

<pre><code>model.5 &lt;- glmer(R0A1 ~ Dist_MP + (1|ID), data=secondorder, family=binomial)

Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
</code></pre>

<p>If I remove the random effect then the warning doesn't appear; therefore, I would assume that there is not enough variability among individuals (<code>ID</code>) for a random effect to be needed. Would you remove the random effect and just run a glm model? Also, how does the <code>family=binomial</code> code model the 0's and 1's in a data set? Does it consider 1's as the event? </p>

<pre><code>Summary output from glmer model:

Generalized linear mixed model fit by maximum
  likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: R0A1 ~ Dist_MP + (1 | ID)
   Data: secondorder

     AIC      BIC   logLik deviance df.resid 
 39451.7  39476.5 -19722.8  39445.7    28693 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.0876 -1.0372  0.2758  0.9567  1.7543 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 0        0       
Number of obs: 28696, groups:  ID, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.679e-01  1.505e-02   11.16   &lt;2e-16 ***
Dist_MP     -1.559e-03  8.771e-05  -17.77   &lt;2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr)
Dist_MP -0.614
</code></pre>
"
"0.0905976508333704","0.0940027887907685","136087","<p>So, building a LMM with the <code>lmer</code> function in <code>lme4</code>, you get the variance explained by the variance component.</p>

<pre><code>summary(esoph) # data on esophageal cancer from 'datasets' package

     agegp          alcgp         tobgp        ncases         ncontrols    
  25-34:15   0-39g/day:23   0-9g/day:24   Min.   : 0.000   Min.   : 1.00  
  35-44:15   40-79    :23   10-19   :24   1st Qu.: 0.000   1st Qu.: 3.00  
  45-54:16   80-119   :21   20-29   :20   Median : 1.000   Median : 6.00  
  55-64:16   120+     :21   30+     :20   Mean   : 2.273   Mean   :11.08  
  65-74:15                                3rd Qu.: 4.000   3rd Qu.:14.00  
  75+  :11                                Max.   :17.000   Max.   :60.00

# Just a hypothetical mixed model
m1 &lt;- lmer(ncase~ncontrols+(1|tobgp)+(1|agegp), data=esoph, REML=F)
summary(m1)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: ncases ~ ncontrols + (1 | tobgp) + (1 | agegp)
   Data: esoph

     AIC      BIC   logLik deviance df.resid 
   404.0    416.4   -197.0    394.0       83 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0267 -0.4770 -0.0704  0.3030  5.8282 

Random effects:
 Groups   Name        Variance Std.Dev.
 agegp    (Intercept) 2.2809   1.5103  
 tobgp    (Intercept) 0.0293   0.1712  
 Residual             4.4311   2.1050  
Number of obs: 88, groups:  agegp, 6; tobgp, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  1.65533    0.69209   2.392
ncontrols    0.05022    0.01882   2.668
</code></pre>

<p>I get the standard deviation for each of my variance components. Let's say I want to compute variance explained by my fixed effects (code from <a href=""http://jonlefcheck.net/2013/03/13/r2-for-linear-mixed-effects-models/"" rel=""nofollow"">here</a>):  </p>

<pre><code>(VarF &lt;- var(as.vector(lme4::fixef(m1) %*% t(m1@pp$X))))
# 0.4082163
</code></pre>

<p>Great! That's useful information! How do I go about calculating the standard deviation for this variance estimate?</p>
"
"0.219134487319818","0.215080409014993","136495","<p><b>Background:</b><br>
I am using linear mixed-effects models (LMMs) in order to determine how the interaction between two fixed effects influences measures of a response variable.  Since I am working with a dataset in which there are multiple samples from multiple individuals that could violate the assumption of independence of data points, I am treating ""individual"" as a random effect.  Thus, the generic model I am working with is:  </p>

<pre><code>lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind), dataset, REML=T)
</code></pre>

<p>Note: for my actual dataset, I used a likelihood ratio test to determine whether I needed to also nest the multiple trials within individual [i.e., lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind/Trial), dataset)], and failed to reject the null hypothesis that this ""fuller"" model contributed significantly to accounting for additional variation in the data.   </p>

<p><b>Problem to solve:</b><br>
Determine whether the results from my Tukey's post-hoc comparisons are reliable, given the interactions included in my LMM model.</p>

<p><b>Loading data and libraries:</b><br>
library(car) # for Soils dataset<br>
data(Soils)<br>
library(lme4) # for lmer()<br>
library(lsmeans) # for remaining functions  </p>

<p><b>Example code:</b><br>
     ## Create the LMM<br>
     ## ""Na"" is a numeric continuous response variable<br>
     ## ""Contour"" is a factor, with character categories, and is treated as a fixed effect<br>
     ## ""P"" is an integer variable, is treated as a fixed effect, and differs across the Contour groups<br>
     ## ""Group"" is a a numerical factor and is treated as a random effect  </p>

<pre><code>Na.LMER &lt;- lmer(Na ~ Contour*P + (1|Group), Soils, REML=T)
Na.LMER  

Linear mixed model fit by REML ['lmerMod']
Formula: Na ~ Contour * P + (1 | Group)
   Data: Soils
REML criterion at convergence: 190.4919
Random effects:
 Groups   Name        Std.Dev.
 Group    (Intercept) 2.514   
 Residual             1.063   
Number of obs: 48, groups:  Group, 12
Fixed Effects:
   (Intercept)    ContourSlope      ContourTop               P  ContourSlope:P    ContourTop:P  
    7.104951        4.381251       -0.260527       -0.006811       -0.026952       -0.006258  

### Conduct Tukey's post-hoc comparisons
Na.Tukey &lt;- lsmeans(Na.LMER, pairwise~Contour, adjust=""tukey"")
</code></pre>

<blockquote>
  <p>NOTE: Results may be misleading due to involvement in interactions  </p>
</blockquote>

<pre><code>Na.Tukey  

$lsmeans
 Contour      lsmean       SE   df lower.CL upper.CL
 Depression 5.973118 1.289466 8.15 3.008857 8.937380
 Slope      5.875929 1.286895 8.08 2.913697 8.838160
 Top        4.672639 1.294933 8.24 1.701416 7.643863

Confidence level used: 0.95 

$contrasts
 contrast             estimate       SE   df t.ratio p.value
 Depression - Slope 0.09718976 1.821763 8.11   0.053  0.9984
 Depression - Top   1.30047917 1.827450 8.19   0.712  0.7635
 Slope - Top        1.20328941 1.825636 8.16   0.659  0.7925

P value adjustment: tukey method for a family of 3 means 
</code></pre>

<p><b>So this is where the question comes in.</b><br>
Since I received the warning message (""NOTE: Results may be misleading due to involvement in interactions""), I want to verify whether I can reliably use the p-values output from lsmeans() to determine which contrasts were different from each other.  So how can I tell whether the interactions from this particular dataset could be problematic for interpreting the results from the Tukey's post-hoc comparisons.  </p>

<p><b>Here is what I have tried to investigate this issue.</b><br>
Based on the recommendations by Professor Russell Lenth (developer of the lsmeans R package), I used additional functions from the lsmeans R package to investigate what's going on with the data.</p>

<pre><code>### First, here are the F-tests of the fixed effects of the LMM.
anova(Na.LMER)   

Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
Contour    2  0.5696  0.2848  0.2520
P          1 10.4083 10.4083  9.2093
Contour:P  2  6.7070  3.3535  2.9672  
</code></pre>

<p>Does the Contour:P interaction seem relatively strong?  </p>

<p>Next, I'm going to evaluate whether this interaction is important by determining to what extent the values of P varies across the Contour groups, using lsmip().    </p>

<pre><code>Na.lsm &lt;- lsmeans(Na.LMER, ~Contour|P, at=list(P = c(75, 100, 200, 300, 400)))  
Na.lsm    

P =  75:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.594094 1.399580 10.70  3.5029413  9.685246
 Slope       8.953983 1.562754 13.53  5.5913341 12.316631
 Top         5.864180 1.511863 12.76  2.5917619  9.136598

P = 100:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.423808 1.355688  9.64  3.3876590  9.459957
 Slope       8.109909 1.429365 10.79  4.9562943 11.263524
 Top         5.537432 1.391548 10.16  2.4433848  8.631479

P = 200:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.742665 1.286244  8.08  2.7814923  8.703838
 Slope       4.733616 1.354120  9.32  1.6863856  7.780847
 Top         4.230440 1.384598 10.01  1.1459415  7.314939

P = 300:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.061522 1.396923 10.63  1.9738402  8.149204
 Slope       1.357323 1.960472 21.77 -2.7109112  5.425557
 Top         2.923449 2.025495 24.22 -1.2549312  7.101829

P = 400:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  4.380379 1.651907 17.57  0.9037052  7.857053
 Slope      -2.018970 2.841216 33.67 -7.7950921  3.757152
 Top         1.616457 2.914268 36.01 -4.2938885  7.526803

Confidence level used: 0.95  
</code></pre>

<blockquote>
  <h3>Plotting the interactions</h3>
  
  <p>Na.lsmip &lt;- lsmip(Na.lsm, Contour~P)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/9uDul.jpg"" alt=""Interaction of Contour and P""></p>

<blockquote>
  <h3>It seems like the levels of Contour vary at different values of P (especially for Slope), but I'm going to use pairs() to verify this using pairwise comparison at each value of P.</h3>
</blockquote>

<pre><code>pairs(Na.lsm)  
P =  75:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -2.3598888 2.097862 12.15  -1.125  0.5175
 Depression - Top    0.7299139 2.060232 11.74   0.354  0.9335
 Slope - Top         3.0898026 2.174381 13.15   1.421  0.3589

P = 100:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -1.6861012 1.970019 10.22  -0.856  0.6784
 Depression - Top    0.8863760 1.942755  9.90   0.456  0.8928
 Slope - Top         2.5724773 1.994865 10.47   1.290  0.4308

P = 200:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  1.0090489 1.867637  8.70   0.540  0.8539
 Depression - Top    1.5122246 1.889851  9.04   0.800  0.7122
 Slope - Top         0.5031757 1.936686  9.67   0.260  0.9636

P = 300:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  3.7041990 2.407248 16.78   1.539  0.2988
 Depression - Top    2.1380732 2.460493 18.21   0.869  0.6660
 Slope - Top        -1.5661258 2.818879 23.01  -0.556  0.8447

P = 400:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  6.3993492 3.286534 28.79   1.947  0.1439
 Depression - Top    2.7639218 3.349889 30.81   0.825  0.6905
 Slope - Top        -3.6354273 4.070070 34.89  -0.893  0.6481

P value adjustment: tukey method for a family of 3 means  
</code></pre>

<blockquote>
  <h3>Based on the pairs() output, it doesn't seem like Contour groups vary at these incremental values of P.</h3>
  
  <p><b>Since the Contour groups do not seem to vary at different levels of P, does that mean that the interaction strength is not that strong?  and thus, I am okay to ignore the warning message that ""NOTE: Results may be misleading due to involvement in interactions""?</b>  </p>
</blockquote>

<p>I would appreciate any feedback about interpreting these results, and whether there are additional analyses that I should be conducting in order to address my concern.  If there is any additional information that would be helpful in tackling this problem, please let me know.  </p>

<p>Thank you for your time!</p>

<p>UPDATE (2/6/15): I had a minor typo at the beginning, in which the first line of code read ""Dens.LMER &lt;- lmer(...)"".  The lmer product should have been named ""Na.LMER"", which was used in the remaining code.  Thus, the Dens.LMER product that rvl mentions is equivalent to Na.LMER.  I apologize for the inconvenience.  </p>
"
"0.104613156193188","0.108545070825851","137235","<p>Iâ€™m trying to fit a model with 4 parameters by not including all the interactions, just including the interaction which got biological sense.</p>

<p>This is my data</p>

<pre><code>&gt; str(data) 
'data.frame':   168 obs. of  7 variables:
 $ session  : Factor w/ 7 levels ""five"",""four"",..: 5 1 5 2 2 2 1 4 5 1 ...
     $ indiv    : chr  ""8"" ""21"" ""23"" ""26"" ...
 $ age      : Factor w/ 2 levels ""ad"",""juv"": 1 1 1 1 1 1 1 1 1 1 ...
     $ treatment: Factor w/ 2 levels ""control"",""fleas"": 1 1 1 1 1 1 1 1 1 1 ...
 $ daytype  : Factor w/ 2 levels ""after"",""before"": 1 1 1 1 1 1 1 1 1 1 ...
     $ time     : Factor w/ 2 levels ""dark"",""light"": 2 2 2 2 2 2 2 2 2 2 ...
 $ dayba    : int  947 286 480 948 135 112 616 274 543 237 ...
</code></pre>

<p>where session is the number of experiment, indiv is each one of the 42 inviduals, daytype is day before treatment or after time is night or day and movs is my response variable and is the only numerical, a number of movements </p>

<p>I try to fit repeated measures model (time series analysis) being â€˜indivâ€™ the random factor with 4 observations for each of the 42 individuals (=168 observations), using both, lme from package nlme and lmer from lme4 and this is what I obtained:</p>

<pre><code>&gt; lme.hp&lt;-lmer(movs~age+treatment+daytype+time+
    treatment:daytype+ daytype:age + 
     treatment:daytype:time+ treatment:daytype:age +  
       treatment:daytype:time:age+(1|indiv))

fixed-effect model matrix is rank deficient so dropping 1 column / coefficient

&gt; lme.hp&lt;-lme(movs~age+treatment+daytype+
  time+treatment:daytype+ daytype:age + 
   treatment:daytype:time+ treatment:daytype:age +
   treatment:daytype:time:age,random=~1|factor(indiv))

Error in MEEM(object, conLin, control$niterEM) : 
  Singularity in backsolve at level 0, block 1
</code></pre>

<p>I read that two reasons for that: </p>

<ol>
<li>It means that the fixed effects are multicollinear, i.e. there is some linear combination of them that's constant (<a href=""http://stackoverflow.com/questions/22788314/linear-mixed-model-matrix-is-rank-deficient"">http://stackoverflow.com/questions/22788314/linear-mixed-model-matrix-is-rank-deficient</a>)</li>
<li>cause of this error is apparently rank deficiency (<a href=""http://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it"">What is rank deficiency, and how to deal with it?</a>)</li>
</ol>

<p>However, this is no sense for me as I am able to fit a saturated model with all possible interactions, not just the ones I want it without receiving a warning message</p>

<pre><code> lme.hp &lt;- lmer(movs~age*treatment*daytype*time+(1|indiv))
 lme.hp &lt;- lme(dayba~age*treatment*daytype*time,random=~1|factor(indiv))
</code></pre>

<p>I think (not sure) that if I had any of the problems discussed above, I would not be able to fit a model saturated with all interactions.</p>

<p>Anybody knows? </p>

<p>Thanks in advance </p>
"
"0.0905976508333704","0.0940027887907685","138109","<p>I would like to obtain estimated $\theta$ from glmer.nb function in lme4 package. In my understanding this function fits the model:
$$
Y_{ij}|\boldsymbol{B}_{i}=\boldsymbol{b}_i \overset{ind.}{\sim} NB\Big(mean=\mu,var=\mu + \frac{\mu^2}{\theta}\Big)
$$
where $NB$ refers to the negative binomial distribution and:
$$
\mu = \exp(\boldsymbol{X}_{ij}^T \boldsymbol{\beta} + \boldsymbol{Z}_{ij}^T \boldsymbol{b}_i)
$$
and 
$$
\boldsymbol{B}_i \overset{i.i.d.}{\sim} MVN(mean=\boldsymbol{0},var=\Sigma).
$$
So glmer.nb must be estimating unknown parameters $\boldsymbol{\beta}$, $\Sigma$ and $\theta$ via maximizing its likelihood. The help file of glmer.nb little explains its functionality, and it says ""glmer() for Negative Binomial"". However, the negative binomial is NOT an exponential family when the parameter $\theta$ is unknown. So $\theta$ must be estimated in some other ways that generalized linear mixed effect models (GLMM) do not take, and the estimated $\theta$ must be obtained in some special ways that GLMM do not take. How can I access to the estimate of $\theta$ which should be one of glmer.nb output? </p>
"
"0.0640622132638473","0.0664700094043992","138132","<p>I have a model based on a dataset that respects all linear model assumptions except for homoscedasticity. When I just ignore the problem of heteroscedasticity, the p-value, for the interaction with group, in my model is &lt;.00001. I definitely know that there is something as per my previous studies and the literature in this field. However, I would like to be honest regarding my analyses and assumptions. Is this assumption really needed if the other 3 main ones are respected (independence, linearity, absence of collinearity) for the interpretation of the p-value in the mixed effects models? </p>

<p>When I run the following on my lmer model called mod:</p>

<pre><code>plot(fitted(mod),residuals(mod))
</code></pre>

<p><img src=""http://i.stack.imgur.com/yYkUm.png"" alt=""enter image description here""></p>

<p>I get a cone shape distribution. I then try to log transform it, and recheck the model, for the interaction with group in my model the p value goes to .40. Quite a jump! My data comes brain activity from patients and healthy individual, just to clarify.</p>

<p>This is my model: </p>

<blockquote>
  <p>lmer(value ~ dist*group + (1|patientnumber), dat1)</p>
</blockquote>

<p>This is how I obtained the p-value:</p>

<blockquote>
  <p>Anova(mod)</p>
</blockquote>

<p>Kindly advise.</p>
"
"0.116961064294386","0.121357078494567","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.0905976508333704","0.0626685258605123","139107","<p>I have a question about normal linear models vs mixed models.</p>

<p>Say I'm predicting prices for certain products, and I know two things: store and brand:</p>

<p>In a linear model (lm), this would be:</p>

<pre><code>price ~ 1 + store + brand
</code></pre>

<p>in a mixed effects model (lmer from lme4), this would be</p>

<pre><code>price ~ 1 + (1 | store) + (1 | brand)
</code></pre>

<p>I keep on reading that mixed effects models are great because e.g. different stores have different effects (think Whole Foods vs Costco, expensive vs cheap), but I don't see how a normal linear model doesn't track that anyways. If store and brand are factors, then doesn't each unique store get transformed into its own boolean variable? (For each price i, it was or it was not gathered from store j, so if there are ten different stores, that'll be turned into ten different indicator columns in the data matrix X).</p>

<p>How exactly does a mixed effects model do better than this?</p>
"
"0.157137527224977","0.154462343667293","140972","<p>Iâ€™m using a maximal logistic regression model to analyze some data. I would like to keep using this technique if possible, just include more data in the model. The main data Iâ€™m looking at is counts of a particular behavior over items in a sequence, and I would like my analysis to also include data from a post-experiment questionnaire (8 items, 1-9 Likert scored). Hereâ€™s some info about my data:</p>

<pre><code>'data.frame':
Pair          : Factor w/ 36 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
SpeakerID     : Factor w/ 72 levels ""10A"",""10B"",""11A"",..: 21 22 21 22 22 21 22 21 21 22 ...
Speaker       : Factor w/ 2 levels ""A"",""B"": 1 2 1 2 2 1 2 1 1 2 ...
Condition1     : Factor w/ 4 levels ""ANTI"",""CONTROL"",..: 1 1 1 1 1 1 1 1 1 1 ...
..- attr(*, ""contrasts"")= num [1:4, 1:3] -0.333 1 -0.333 -0.333 0.25 ...
.. ..- attr(*, ""dimnames"")=List of 2
.. .. .. : chr  ""ANTI"" ""CONTROL"" ""IN"" ""OUT""
.. .. .. : NULL
Condition2         : Factor w/ 3 levels ""0"",""90"",""180"": 2 3 1 1 2 1 1 2 2 3 ...
..- attr(*, ""contrasts"")= num [1:3, 1:2] 0 -0.5 0.5 -0.5 0.25 0.25
.. ..- attr(*, ""dimnames"")=List of 2
.. .. ..$ : chr  ""0"" ""90"" ""180""
    .. .. ..$ : NULL
Item         : Factor w/ 16 levels ""MAP1"",""MAP10"",..: 1 9 10 11 12 13 14 15 16 2 ...
Foo       : num  0.847 1.099 1.946 -1.099 -0.452 ...
wtsFoo          : num  0.952 0.889 2.286 0.889 0.468 ...
Close      : num  -1.798 0.202 -1.798 0.202 0.202 ...
Similar    : num  0.505 0.505 0.505 0.505 0.505 ...
Like       : num  -0.833 0.167 -0.833 0.167 0.167 ...
Task1Hard   : num  -0.89 4.11 -0.89 4.11 4.11 ...
Task2Hard: num  -1.02 2.98 -1.02 2.98 2.98 ...
</code></pre>

<p>My analysis is based on this guide to empirical logit analyses:
<a href=""http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html</a>
So far, so good. In my regression model, Iâ€™m testing the fixed effects of Condition1 (4 levels) and Condition2 (3 levels) on Foo (the behavior, expressed as a proportion converted into empirical logit form, see link for how and why). Pair, Pair:Subject (Subject nested within Pair) and Item are included as random effects. Condition1 is between-subjects/pairs and Condition2 is within-subjects. Hereâ€™s the model Iâ€™m using in R:</p>

<pre><code>model &lt;- lmer(Foo ~ Condition1*Condition2 + (1+Condition1 | Pair) 
+ (1+Condition1 | Pair:Subject) + (1+Condition2 | Item), weights=1/wtsFoo, data)
</code></pre>

<p>This all works fine, but hereâ€™s where it gets fun. Where should the questionnaire data go? </p>

<p>Bad idea #1: Each participant has one score for each questionnaire item, so each questionnaire item type should be included as a fixed effect, so that Foo can be predicted by any of the variables discovered in the post-experiment questionnaire (things like social closeness and task difficulty). This is a terrible idea because the questionnaire items are NOT independent variables from Condition1 and Condition2, and if I include them as fixed effects it will introduce a mess of multicollinearity and will just be flat-out wrong.</p>

<p>Bad idea #2: Analyzing the questionnaire data separately. Not such a bad idea, just one that my committee doesnâ€™t like. </p>

<p>Less bad ideas: please suggest a model that allows me to observe the effects of Condition1 and Condition2 on questionnaire items (Close, Similar, Like, Task1Hard, Task2Hard) AND allows me to observe the effects of questionnaire items on Foo. Failing that, explain to me why the only good thing to do is analyze the questionnaire separately from the observation data.</p>

<p>I've read around on Stackexchange and I haven't seen this particular problem covered, although some answers come close to looking useful, I don't yet have the R or stats chops to make them work for me. If I've missed something obvious, please clue me.</p>
"
"NaN","NaN","141582","<p>I tested whether different version-styles of a loading screen (hourglass vs. progress bar) in different progression patterns (linear, accelerate, decelerate, irregular, binary) affect time estimations within subjects.</p>

<p>By analyzing the data with a binomial linear mixed effects model I have found significant results for the interaction effect of ""versionStyle x DisplayDuration x progressionPattern"" I would like to run a post hoc analysis to test across which condition and Display duration the time estimation was significantly affected by the version Style""</p>

<p>This is the code I used for my analysis:</p>

<pre><code>(data1 &lt;- glmer(Long ~ DisplayDur * Pattern * Proggression+ (1 + DisplayDur + Pattern+ Progression| Subjects), dat=anadat,family=""binomial"",control=glmerControl(optimizer=""bobyqa"")))
</code></pre>

<p>Is there some command that I can use in R to do this?</p>
"
"0.173481293613825","0.180001636385951","141746","<p>Given three variables, <code>y</code> and <code>x</code>, which are positive continuous, and <code>z</code>, which is categorical, I have two candidate models given by:</p>

<pre><code> fit.me &lt;- lmer( y ~ 1 + x + ( 1 + x | factor(z) ) )
</code></pre>

<p>and</p>

<pre><code> fit.fe &lt;- lm( y ~ 1 + x )
</code></pre>

<p>I hope to compare these models to determine which model is more appropriate. It seems to me that in some sense <code>fit.fe</code> is nested within <code>fit.me</code>. Typically, when this general scenario holds, a chi-squared test can be performed. In <code>R</code>, we can perform this test with the following command,</p>

<pre><code> anova(fit.fe,fit.me)
</code></pre>

<p>When both models contain random-effects (generated by <code>lmer</code> from the <code>lme4</code> package), the <code>anova()</code> command works fine. Owing to boundary parameters, it is normally advisable to test the resulting Chi-Square statistic via simulation, nonetheless, we can still <em>use</em> the statistic in the simulation procedure.</p>

<p>When both models contain <em>only</em> fixed-effects, this approach---and, the associated <code>anova()</code> command---work fine.</p>

<p>However, when one model contains random effects and the reduced model contains <em>only</em> fixed-effects, as in the above scenario, the <code>anova()</code> command doesn't work.</p>

<p>More specifically, I get the following error:</p>

<pre><code> &gt; anova(fit.fe, fit.me)
 Error: $ operator not defined for this S4 class
</code></pre>

<p>Is there anything wrong with using the Chi-Square approach from above (with simulation)? Or is this simply a problem of <code>anova()</code> not knowing how to deal with linear models generated by different functions?</p>

<p>In other words, would it be appropriate to manually generate the Chi-Square statistic derived from the models? If so, what are the appropriate degrees of freedom for comparing these models? By my reckoning:</p>

<p>$$ F = \frac{\left((SSE_{reduced}-SSE_{full})/(p-k)\right)}{\left((SSE_{full})/(n-p-1)\right)} \sim F_{p-k,n-p-1} $$</p>

<p>We are estimating two parameters in the fixed effects model (slope and intercept) and two more parameters (variance parameters for the random slope and random intercept) in the mixed-effects model. Typically, the intercept parameter isn't counted in the degrees of freedom computation, so that implies that $k=1$ and $p=k+2=3$; having said that I'm not sure if the variance parameters for the random-effects parameters should be included in the degrees of freedom computation; the variance estimates for fixed-effect parameters are <em>not considered</em>, but I believe that to be because the parameter estimates for fixed effects are assumed to be <em>unknown constants</em> whilst they are considered to be <em>unknowable random variables</em> for mixed effects. I would appreciate some assistance on this issue.</p>

<p>Finally, does anybody have a more appropriate (<code>R</code>-based) solution to comparing these models?</p>
"
"0.0905976508333704","0.0940027887907685","142914","<p>I want to control for a nuisance covariate in a linear model. Since the covariate interacts significantly with one of the fixed factors, the homogeneity of regression slopes assumption is violated for an ANCOVA approach. My understanding is that this can be handled in a multi-level, or mixed effects, model.</p>

<p>My data:</p>

<pre><code>&gt; str(seeds)
'data.frame':   186 obs. of  4 variables:
 $ fixed.A : Factor w/ 31 levels ""A1"",""A2"",""A3"",..: 7 7 7 7 7 7 10 10 10 10 ...
     $ fixed.B : Factor w/ 2 levels ""Y"",""N"": 2 2 2 1 1 1 2 2 2 1 ...
 $ cov     : num  10.3 10.5 11 12.8 12.9 ...
     $ response: num  10.8 11 11.1 14.7 15.3 ...
</code></pre>

<p>The covariate <code>cov</code> has a significant interaction with <code>fixed.A</code>. To fit a random intercept and slope for <code>cov</code> conditioned on <code>fixed.A</code>, it seems an approach using <code>lmer</code> (in lme4) might be:</p>

<pre><code>&gt; lmer(response ~ fixed.A*fixed.B + (1 + cov | fixed.A), data = seeds)
</code></pre>

<p>I'm aware that this is not the usual approach in <code>lmer</code> since grouping factors tend to be random and therefore don't also appear as fixed factors in the model formula. However, since I'm interested in the interaction between my two fixed factors, I don't see how else to proceed. Any help is greatly appreciated.</p>
"
"0.0827039616973562","0.0858124131484961","143304","<p>I am trying to specify a formula for a linear mixed effect model (with <code>lme4</code>) for my experimental design, but I'm not sure I'm doing it right.</p>

<p>The design: basically I'm measuring a response parameter on plants. I have 4 levels of treatment, and 2 irrigation levels.  The plants are grouped in 16 plots, within each plot I sample 4 sub-plots. In each sub-plot I take between 15 and 30 observations (depending on the number of plants found). That is, there are a total of 1500 rows. </p>

<p><img src=""http://i.stack.imgur.com/6TVn0.png"" alt=""enter image description here""></p>

<p>Initially the subplot level was just here for sampling purposes, but I thought I'd like to take it into account in the model (as a 64-level variable) because I saw there was a lot of variability from one sub-plot to another, even inside the same plot (greater than the variability between whole plots).</p>

<p>My first idea was to write:</p>

<pre><code>library(lme4)
fit &lt;- lmer(y ~ treatment*irrigation + (1|subplot/plot), data=mydata)
</code></pre>

<p>or </p>

<pre><code>fit &lt;- lmer(y ~ treatment*irrigation + (1|subplot) + (1|plot), data=mydata)
</code></pre>

<p>Is that correct? I'm not sure if I must keep both plot/subplot levels in my formula. 
No fixed effect is significant but the random effects are very significant.</p>
"
"0.104613156193188","0.108545070825851","143556","<p>I would like to model a treatment effect in two different groups, controlled for some co-variates (like age and education), and I assume that a two-way repeated-measure Anova would be the right approach - if yes, I have some questions on how to model this design.</p>

<p>I'm a bit confused on how to do this with R (and the  <code>lme4</code> package), because I found different approaches for the same design. Let's say, I have following variables:</p>

<ul>
<li>subject</li>
<li>group (control vs treatment group)</li>
<li>time (t0 vs t1, i.e. two measures for each subject)</li>
<li>age (co-variate)</li>
<li>education (co-variate)</li>
</ul>

<p>Am I right, that, according to <a href=""http://stats.stackexchange.com/questions/58745/using-lmer-for-repeated-measures-linear-mixed-effect-model"">this posting on Cross Validated</a>, my model would look like this?</p>

<ol>
<li>model: <code>lmer(DV ~ group * time + age + education + (1+time|subject), mydata)</code> </li>
</ol>

<p>Then I found <a href=""http://www.uni-kiel.de/psychologie/rexrepos/posts/anovaMixed.html#mixed-effects-analysis-1"">this tutorial</a>. Following these instructions, my model would look like this?</p>

<ol start=""2"">
<li>model: <code>lmer(DV ~ group * time + age + education + (1|subject) + (1|group:subject) + (1|time:subject), data=mydata)</code></li>
</ol>

<p>Now I have two questions:</p>

<p>a) which of the two above models is correct? or do both work?</p>

<p>b) my data is in long format, how should my variable <code>subject</code> look like? the same value for each measured person, i.e. a value appears twice in this variable (for <em>person A in group X</em> at <strong>t0</strong> and <em>person A in group X</em> at <strong>t1</strong> the same value), or should each row/observation be indicated by a new, unique ID?</p>
"
"0.0640622132638473","0.0664700094043992","143922","<p>I'm having trouble in R with my Linear Mixed-Effects Model. I'm working with yeast in nectar. This is a part of my data just so you can see what is going on:
<img src=""http://i.stack.imgur.com/ufVt3.png"" alt=""enter image description here""></p>

<p>For the condition sucrose, I have 4 different samples (you can only see data for sample 4 here). For each sample I did 2 replica's (so <code>replica</code> is either 1 or 2). <code>sp</code> tells you which species it is and <code>condition</code> tells you whether the two yeast species were mixed together or just grew alone (single). I linked the two variables <code>condition</code> and <code>sp</code> together in <code>treatment</code>. <code>host</code> specifies the host plant of the species and <code>cells1</code> is the number of yeast cells.</p>

<p>It is the number of cells (<code>cells1</code>) that I want to compare for the different treatments. So I started off by making a mixed model with nested effects.</p>

<pre><code>suc &lt;- read.csv(file=file.choose(),header = TRUE,sep = "";"")
attach(suc)
## load packages 'lme4', 'lsmeans', 'pbkrtest and 'Rcpp'
fit1 &lt;- lmer(cells1~treatment+sp+treatment:sp+
    (1|cont/replica)+(1|replica/Sample)+(1|Sample/host), suc)
</code></pre>

<p>Next, I wanted to do a post hoc test. <code>TukeyHSD</code> wouldn't work. Error said something about not being able to use it with <code>lmer</code>. So after doing some research, I used the function <code>lsmeans</code>.</p>

<pre><code>library(""lsmeans"")
lsmeans(fit1, pairwise~treatment, adjust=""tukey"")
</code></pre>

<p>When looking at the output, I get <code>NA</code> as outcome everywhere and I have no clue what is wrong or how to resolve this.
<img src=""http://i.stack.imgur.com/r7LPN.png"" alt=""enter image description here""></p>

<p>Does anyone know how I can fix this?</p>
"
"0.0523065780965941","0.0542725354129257","144147","<p>I have to solve a problem using a linear mixed model (<code>lmer</code>). Six subjects performed two tests, (<code>test1</code>, <code>test2</code>) in two different locations (<code>on holiday</code>, <code>at work</code>). These are my data:</p>

<pre><code>subj     &lt;- c(1,1,2,2,3,3,4,4,5,5,6,6)
location &lt;- c(""holiday"",""holiday"",""holiday"",""holiday"",""holiday"",""holiday"", 
              ""work"",""work"",""work"",""work"",""work"",""work"")
test     &lt;- c(""test1"", ""test2"",""test1"", ""test2"",""test1"", ""test2"",""test1"", 
              ""test2"",""test1"", ""test2"",""test1"", ""test2"") 
value    &lt;- c(56,32,89,32,56,34,23,98,32,120,41,67)
data     &lt;- data.frame(subj, location, test, value)
</code></pre>

<p>Using <code>lmer</code>, I have to verify if there is an effect of the variable <code>location</code>, if there is an effect of the variable <code>test</code>, and if there is an interaction, <code>location X test</code>.</p>

<p>I thought a model like this might be appropriate:</p>

<pre><code>lmer(value ~ location*test + (1|subj))
</code></pre>

<p>Could it work?</p>

<p>Furthermore, I have to verify if there is a difference between <code>holiday.test1</code> and <code>work.test1</code>, and between <code>holiday.test2</code> and <code>work.test2</code>. I also need p-values.</p>

<p><img src=""http://i.stack.imgur.com/5EEbz.png"" alt=""boxplot(value ~ location*test)""></p>
"
"0.0554795041091482","0.0575647167340002","144597","<p>I have <strong>9 sites</strong>. Within each site, plant life was sampled to represent 70% of the basal area. Of the sampled plants, I know the corresponding <strong>family</strong>, <strong>genus</strong>, and <strong>species</strong>. For this project, I extracted leaf waxes and determined the <strong>total wax loading</strong> (<strong>TWL</strong>), <strong>average chain length</strong> (<strong>ACL</strong>), and the <strong>carbon preference index</strong> (<strong>CPI</strong>). My goal is to determine how much of the observed variation in TWL, ACL, and CPI, is explained by either the site, family, genus, or species. This would allow me to determine the major factors affecting the chemical composition of plant leaf waxes. Please note that most species, genera, and families occur several times within the data. The data look like this:</p>

<pre><code>     SITE              FAM           GEN                        SPEC     TWL   CPI   ACL
1   TAM05         Fabaceae     Tachigali        Tachigali polyphylla   34.65  6.89 30.06
2   TAM05    Caryocaraceae   Anthodiscus        Anthodiscus peruanus   68.68 13.00 30.57
3   TAM05         Moraceae      Clarisia           Clarisia racemosa   46.85 12.22 29.68
4   TAM05    Lecythidaceae  Bertholletia        Bertholletia excelsa   69.64  6.27 30.52
5   TAM05         Moraceae  Pseudolmedia      Pseudolmedia laevigata  126.33 17.34 29.61
6   TAM05         Moraceae  Pseudolmedia         Pseudolmedia laevis   83.58 13.50 30.07
7   TAM05         Linaceae     Roucheria          Roucheria punctata  160.62 13.98 29.71
8   TAM05         Fabaceae    Cedrelinga     Cedrelinga cateniformis  151.12 10.82 30.17
9   TAM05       Urticaceae      Pourouma              Pourouma minor   61.47  1.41 29.64
10  TAM05         Fabaceae  Sclerolobium     Sclerolobium bracteosum  163.28 12.22 29.53
...
</code></pre>

<p>By reviewing similar studies, I eventually ended up with the use of a nested linear mixed-effects model in R. My analysis here would likely consist of running the same analysis three times, once for every variable (TWL, CPI, ACL).</p>

<p>I am, however, unsure of how to proceed. I am currently at the point where I believe the formulas to use would be:</p>

<pre><code>TWL1 &lt;- lmer(TWL ~ SITE + (1|FAM/GEN/SPEC))
CPI1 &lt;- lmer(CPI ~ SITE + (1|FAM/GEN/SPEC))
ACL1 &lt;- lmer(ACL ~ SITE + (1|FAM/GEN/SPEC))
</code></pre>

<p>Is this correct? And is it correct that I would have to run reduced models as well, and compare them with a chi-squared test?</p>
"
"0.128124426527695","0.132940018808798","144815","<p>I'm encountering problems with the results of a <code>glmer</code> model (<code>lme4</code>-package).
Im trying to answer the question, whether a beaver is more likely to be present (<code>Status == 1</code>) or absent (<code>Status == 0</code>) with changing geomorphic and vegetation variables. My model formula looks like this:</p>

<pre><code>model1 &lt;- glmer(Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                         (1 | Location), family=""binomial"", data=Daten12, 
                control=glmerControl(optimizer=""Nelder_Mead""))
</code></pre>

<p>My output looks OK, as far as I can tell, the only peculiar thing being the high estimates of <code>slopecatCentered</code>:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                  (1 | Location)
Data: Datentest
Control: glmerControl(optimizer = ""Nelder_Mead"")

AIC      BIC     logLik    deviance   df.resid 
62.7     77.4    -25.3     50.7       80 

Scaled residuals: 
  Min        1Q    Median        3Q       Max 
-0.095917 -0.003971  0.000000  0.002706  0.079395 

Random effects:
Groups   Name        Variance Std.Dev.
Location (Intercept) 3682     60.68   
Number of obs: 86, groups:  Location, 43

Fixed effects:
                            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -18.5782     7.0847  -2.622 0.008734 ** 
SlopecatCentered             20.4162     5.6060   3.642 0.000271 ***
Canal_width                   0.4763     0.1584   3.007 0.002638 ** 
Distance1:Resource_biotopes   1.0442     0.4717   2.214 0.026861 *  
Distance2:Resource_biotopes   1.0379     0.4662   2.226 0.026010 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) SlpctC Cnl_wd Ds1:R_
SlopctCntrd -0.632                     
Canal_width -0.902  0.698              
Dstnc1:Rsr_ -0.663  0.560  0.458       
Dstnc2:Rsr_ -0.677  0.538  0.461  0.787    
</code></pre>

<p>My qqplot looks weird, though, and so does my residual vs. fitted plot:  </p>

<p><img src=""http://i.stack.imgur.com/8SJjD.jpg"" alt=""qqnorm plot with sjp.glmer(model,...)""></p>

<p><img src=""http://i.stack.imgur.com/wyZp7.jpg"" alt=""fitted vs. residual plot using plot(model)""></p>

<p>edit: I just had a closer look on my data: The <code>SlopecatCentered</code>variable is not a perfect predictor, but my random factor <code>Location</code>is causing this problem. In my raw data set, it denotes 43 different locations. One location has two <code>distance</code> in which most of the variables were measured, so my <code>location</code>variable has 43 * 2 = 86 entrys (in fact, that's the length of the data frame): </p>

<pre><code> &gt;Daten12$Loc
[1] 1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9  10 10 11 11 12 12 13 13 14 14 15 15 16 16 17
[34] 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33
[67] 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43
43 Levels: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ... 43
</code></pre>

<p>I changed that to 1-86 and ran a test model and the plot looked ok (I know that the random effect was futile in that test model, but I wanted to get to the root of the problem).</p>

<p>So apparantly, my raw data frame layout is wrong. But I got samples online to compare, and their layout looks similar, so I just don't know how to fix it.   </p>
"
"0.0739726721455309","0.0767529556453336","144904","<p>I have a data set containing various vegetation and geomorphic variables sampled in 3 <code>distances</code> on both <code>sides</code> of 43 drainage ditches (<code>Location</code>). Roughly half of these ditches are occupied by a beaver, the other half is empty. Now I want to run a model with the binomial response variable <code>Status</code> (""beaver == 1"" / ""beaver == 0"")
I'm struggeling with the order and layout of the nested and interaction effects using <code>glmer</code>. So far I've got</p>

<pre><code>fit &lt;- glmer(Status ~ BankslopeScaled + Connectivity + 
                      Canal_width + Distance:Food_crops + 
                      Distance:Edible_trees + 
                (1 | Distance/Side/Location), 
              data, family=binomial(link=""logit"")
</code></pre>

<p>but I'm not sure ifI still have pseudoreplication in my data or whether I correctly applied the formuly in order to estimate the influence of the predictors in every <code>distance</code> on both <code>sides</code> in each <code>Location</code>. </p>

<p>Like, if <code>food_crops</code> in the 3rd <code>distance</code> on the left <code>side</code> is lower than <code>edible_trees</code> in the 2nd <code>distance</code> on the right <code>side</code>, then ...</p>

<p>I kinda feel like there's something wrong with my random effects-term.</p>

<p>My out put looks like this:</p>

<pre><code>summary(fit)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ BankslopeScaled + Connectivity + Canal_width + Distance:Food_crops +  
Distance:Edible_trees + (1 | Distance/Side/Location)
Data: Satz

     AIC      BIC   logLik deviance df.resid 
   314.6    360.8   -144.3    288.6      245 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.18541 -0.71205  0.07243  0.82483  1.75303 

Random effects:
 Groups                   Name        Variance  Std.Dev. 
 Location:(Side:Distance) (Intercept) 2.834e-02 1.683e-01
 Side:Distance            (Intercept) 2.074e-10 1.440e-05
 Distance                 (Intercept) 2.085e-10 1.444e-05
 Number of obs: 258, groups:  Location:(Side:Distance), 258; Side:Distance, 6; Distance, 3

 Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)            -2.86517    0.79747  -3.593 0.000327 ***
 BankslopeScaled         1.76475    0.62541   2.822 0.004776 ** 
 Connectivity            0.10394    0.02729   3.809 0.000140 ***
 Canal_width             0.19138    0.11089   1.726 0.084364 .  
 Distance1:Food_crops    0.03667    0.09366   0.391 0.695441    
 Distance2:Food_crops    0.10852    0.08996   1.206 0.227694    
 Distance3:Food_crops    0.06303    0.08502   0.741 0.458510    
 Distance1:Edible_trees  0.02273    0.01327   1.712 0.086818 .  
 Distance2:Edible_trees -0.01750    0.02992  -0.585 0.558738    
 Distance3:Edible_trees  0.09769    0.07986   1.223 0.221201    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 [correlation of fixed effects snipped]    
</code></pre>

<p>A point into the right direction is much appreciated!</p>
"
"0.0827039616973562","0.0858124131484961","146118","<p><em>I have added a new example here for clarity, see original question below</em></p>

<p>Eg. I have 10 schools in 5 countries, ten students from each school is sampled.</p>

<p>Prediction variables: student test marks for Language, Math and Science
 Response variable: school fee</p>

<p>I want to know what subject (ie Math) correlates with the schools fees.</p>

<p>lmer(fees~math+language+science+(1|country/school)) *each row is a student</p>

<p>But now I have the same fees for students within the same school, and school is added as    a random effect. Is this allowed? Should I just take the average subject marks per school and drop the school random effect? <em>See original question below</em></p>

<hr>

<p>I have a dependent variable that depends on one of my random effects, as such:</p>

<pre><code>Dep   R1   R2   X1   X2   X3
30    a    g    4    43   21
30    a    g    7    46   18
20    b    g    5    31   22
20    b    g    4    37   17
60    c    h    9    50   26
60    c    h    7    34   21

lmer(Dep~X1+X2+X3+(1|R2/R1))   (R2=Genus, R1=Species)
</code></pre>

<p>I need the random effect, as I have independent data for each specimen, but I know this setup cannot be correct. Plus some of my models fail to converge. I can use the average values of traits for each R1 and then drop the R1 random effect, but then I lose lots of data.</p>

<p>Can I use a linear mixed effects model for this? or should I be using another technique?</p>

<hr>

<p>I have since decided to use a phylogeny with a PGLS, because taxonomic level random effects are too rough.</p>

<p>At the moment I am looking into pgls.Ives in phytools to account for within species level variation (see <strong>Helmus, M. R., Bland, T. J., Williams, C. K., &amp; Ives, A. R. (2007). Phylogenetic measures of biodiversity. The American Naturalist, 169(3)</strong>).</p>
"
"0.0496223770184137","0.0686499305187969","147532","<p>I have two questions about implemetation og generalized linear mixed models for continuous response variable in R:</p>

<ol>
<li>I am trying to fit glmm models for a continuous response variable that has a right skewed distribution. I try to fit gamma GLMM and Gaussian GLMM with glmmPQL and glmer (Laplace and quadrature) but I always have a convergence problem specially with Laplace, glmer:
Error: Downdated X'X is not positive definite, 1. 
Error: MEestimate(lmeSt, grps) :   Singularity in backsolve at level 0, block 1</li>
</ol>

<p>Do you have any idea about problem? I think it is not about data set because I can fit gee, glm and lmm without any problem. </p>

<ol start=""2"">
<li>I fit both PQL and Laplace to a smaller data set in random intercept GLMM case. While PQL gives 0.2632854 for standard deviation of random intercept, glmer (laplace) gives std. dev 1028.16. Why is so huge difference between two methods?</li>
</ol>

<p>Thanks for any help.</p>
"
"0.096093319895771","0.0997050141065988","147770","<p>I need to fit a linear mixed model in the ""Laird and Ware"" framework. This type of model is usually specified by (as you may know):</p>

<p>$\mathbf{y}_i = X_i \beta + Z_i \mathbf{b}_i + \mathbf{\epsilon}_i $, where</p>

<ul>
<li>$\mathbf{y}_i$ is the response for group $i$</li>
<li>$X_i$ is the design matrix for the fixed effects, with coefficients $\beta$</li>
<li>$Z_i$ is the design matrix for the random effects, with coefficients $\mathbf{b}_i$</li>
<li>$\mathbf{\epsilon}_i$ is normally distributed error, mean 0 and covariance structure $R_i$</li>
</ul>

<p>This specific type of model is usually quite easy to fit in <code>R</code> using the <code>lmer</code> function in <code>lme4</code>. </p>

<p>If we were to stack the $\mathbf{y}_i$ vectors into a larger vector $\mathbf{y}$ that contains all profile information, we would be able to write our model as</p>

<p>$\mathbf{y} = \begin{bmatrix} X_1  \\ \vdots \\ X_n \end{bmatrix} \beta + \begin{bmatrix} Z_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; Z_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; Z_n \end{bmatrix} \begin{bmatrix} \mathbf{b}_1 \\ \mathbf{b}_2 \\ \vdots \\ \mathbf{b}_n \end{bmatrix} + \mathbf{\epsilon}$ </p>

<p>Now, my problem is that I do not necessarily have this nice form for my model. I have something like the following:</p>

<p>$\mathbf{y}_i = X_i \beta + S_i \mathbf{u} + Z_i \mathbf{b}_i + \mathbf{\epsilon}_i $, where $\mathbf{u}$ is a random effect, but is not indexed by group.</p>

<p>This leads my model to be something like this:</p>

<p>$\mathbf{y} = \begin{bmatrix} X_1  \\ \vdots \\ X_n \end{bmatrix} \beta + \begin{bmatrix} S_1 &amp; Z_1 &amp; 0 &amp; \cdots &amp; 0 \\ S_2 &amp; 0 &amp; Z_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ S_n &amp; 0 &amp; 0 &amp; \cdots &amp; Z_n \end{bmatrix} \begin{bmatrix} \mathbf{u} \\ \mathbf{b}_1 \\ \mathbf{b}_2 \\ \vdots \\ \mathbf{b}_n \end{bmatrix} + \mathbf{\epsilon}$ </p>

<p>This type of model is seen, for example, on page 6 of <a href=""http://www.stat.vt.edu/research/Technical_Reports/TechReport10-2.pdf"" rel=""nofollow"">http://www.stat.vt.edu/research/Technical_Reports/TechReport10-2.pdf</a>.</p>

<p>I want to know how I would produce this type of model in <code>R</code>, with or without the <code>lme4</code> package. The main issue I am having is that to specify random effects in <code>lmer</code>, we are required to group by a certain variable. However, I need to specify a random effect <em>without</em> grouping by any variable. Can anyone help? Thanks</p>
"
"0.173645497430627","0.180172011848973","149732","<p>I'm trying to analyze the data from an experiment I conducted, and could use some guidance in relation to fixed vs. random effects.</p>

<p>The experiment was related to risk-seeking behavior in the context of hypothetical gambles, and implemented a 3 (Response Scale: Control vs. RI vs. ABR) x 3 (Stakes) X 5 (Endowment) factorial design. Response Scale was a between-subjects manipulation, and the levels of Stakes and Endowment were combined factorially to produce 15 different gamble scenarios, all of which were evaluated by each participant (i.e. gamble evaluation was within-subjects). The DV of interest for the particular analysis I'm working on is a binary indicator variable called ""Would.Play"" that describes whether a participant would choose to play the gamble if they were to encounter it in real life.</p>

<p>As a preliminary analysis, I'd like to be able to claim that there were no [or, as the data seem to indicate, <em>were</em>] meaningful differences in Would.Play as a result of random assignment to a particular Response Scale condition (designated by the factor variable ""Response.Scale"", ref=""Control"").</p>

<p>I can obviously do this with a binary logit for each of the 15 gambles (designated by the variable ""Gamble.Num""), but I'd like to avoid issues with multiple testing. My preference, therefore, is to fit a single model that accounts for the heterogeneity in gambles by fitting a separate intercept for each gamble.</p>

<p>I've come across two ways to do this, each of which seems to give different results: Dummy ""Fixed Effects"" modeling in glm() and ""random effects"" modeling in glmer() (see output below).</p>

<p>It seems possible that the difference in the estimated coefficients could be the result of the Dummy ""Fixed Effects"" approach taking Gamble.Num==1 as a reference level, but I don't have a very deep understanding of the math underlying these two techniques. I was hoping someone would be able to give me a quick explanation of (a) why the these two models appear to give different results; and (b) whether one of these approaches is better suited to answering my question of interest: is there a unique effect of Response.Scale on Would.Play, taking heterogeneity in gambles into account?</p>

<p>Below is a quick look at the data I'm using, and the output of the two models:</p>

<pre><code>## Data ##
head(analysis.0.data)
 Local.ID Condition Response.Scale RS.Code Gambles.First Gamble.Num Endowment Stakes
1        8         4             RI       1             0          1      -150     10
2        8         4             RI       1             0          2      -150     50
3        8         4             RI       1             0          3      -150    200
4        8         4             RI       1             0          4       -25     10
5        8         4             RI       1             0          5       -25     50
6        8         4             RI       1             0          6       -25    200
  Would.Play Perc.Risk
1          0         4
2          0         6
3          0         5
4          0         3
5          0         5
6          0         7


## Dummy ""Fixed Effects"" Model ##
summary(glm(Would.Play ~ Response.Scale + factor(Gamble.Num), family=""binomial"",     
data=analysis.0.data))

Call:
glm(formula = Would.Play ~ Response.Scale + factor(Gamble.Num), 
    family = ""binomial"", data = analysis.0.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7766  -0.7204  -0.4678   0.7006   2.5394  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          -1.14906    0.21987  -5.226 1.73e-07 ***
Response.ScaleRI     -0.06749    0.12815  -0.527  0.59844    
Response.ScaleABR    -0.91035    0.13843  -6.576 4.82e-11 ***
factor(Gamble.Num)2  -0.94090    0.35886  -2.622  0.00874 ** 
factor(Gamble.Num)3  -1.12416    0.37769  -2.976  0.00292 ** 
factor(Gamble.Num)4   0.31966    0.28379   1.126  0.25999    
factor(Gamble.Num)5  -0.63953    0.33303  -1.920  0.05482 .  
factor(Gamble.Num)6  -0.85860    0.35120  -2.445  0.01449 *  
factor(Gamble.Num)7   1.42100    0.26770   5.308 1.11e-07 ***
factor(Gamble.Num)8   0.35620    0.28268   1.260  0.20765    
factor(Gamble.Num)9  -0.51138    0.32379  -1.579  0.11425    
factor(Gamble.Num)10  2.10754    0.27298   7.720 1.16e-14 ***
factor(Gamble.Num)11  0.28248    0.28496   0.991  0.32154    
factor(Gamble.Num)12 -1.02908    0.36760  -2.799  0.00512 ** 
factor(Gamble.Num)13  2.49612    0.28133   8.873  &lt; 2e-16 ***
factor(Gamble.Num)14  1.72839    0.26867   6.433 1.25e-10 ***
factor(Gamble.Num)15  0.08524    0.29204   0.292  0.77039    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2649.2  on 2249  degrees of freedom
Residual deviance: 2096.4  on 2233  degrees of freedom
AIC: 2130.4

Number of Fisher Scoring iterations: 5


## GLMER ""Random-Effects"" Model##
summary(glmer(Would.Play ~ Response.Scale + (1|Gamble.Num), family=""binomial"", 
data=analysis.0.data))
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
[glmerMod]
 Family: binomial  ( logit )
Formula: Would.Play ~ Response.Scale + (1 | Gamble.Num)
   Data: analysis.0.data

     AIC      BIC   logLik deviance df.resid 
  2169.3   2192.1  -1080.6   2161.3     2246 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9011 -0.5461 -0.3522  0.5439  4.6708 

Random effects:
 Groups     Name        Variance Std.Dev.
 Gamble.Num (Intercept) 1.291    1.136   
Number of obs: 2250, groups:  Gamble.Num, 15

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -0.90254    0.30722  -2.938  0.00331 ** 
Response.ScaleRI  -0.06682    0.12707  -0.526  0.59897    
Response.ScaleABR -0.90170    0.13727  -6.569 5.07e-11 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Rs.SRI
Rspns.SclRI -0.202       
Rspns.ScABR -0.183  0.456
</code></pre>

<p>Thanks!</p>
"
"0.13056511110323","0.165577423237548","151079","<p>I have a data set of 2430 observations, with a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I want to to a mixed effects model using glmer. Here is what I entered into R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I then get the following warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>This is what my summary looks like:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting this bit in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So here's the new command:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>I get one less warning, but the other one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks weird:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>Does anyone have an idea what I can do to solve this? Or tell me what this warning even means? Please explain in a way that an R-newbie like myself can understand!</p>

<p>Any help is much appreciated!</p>
"
"0.138390197576366","0.143591631723548","151200","<p>I have been trying to figure out how to do a fairly basic repeated measures analysis using linear mixed effects in R, and then analysing it using post-hoc tests. The problem is that I'm not sure whether the output I get is statistically sound?</p>

<p>The response variable: <code>weighted</code>- an index of habitat preference (prop. individuals on habitatA / prop. of total habitat that is A). A value above 1 indicates the habitat is being used more than what you would expect from its availability. this was repeatedly  measured on the same colony through time over several weeks</p>

<p>Fixed variables: <code>Type</code> - habitat type (live/dead), <code>weeks</code> - the time variable</p>

<p>Random variables: <code>colony</code> - because each measurement of colony violates independence assumption.</p>

<p>Here's what the data loss like plotted over time (orange=live habitat, blue=dead habitat):</p>

<p><img src=""http://i.stack.imgur.com/atUVm.jpg"" alt=""enter image description here""> </p>

<p>i run the analysis using the <code>lmer()</code> function from the <code>lme4</code> package:</p>

<pre><code>results_full=lmer(weighted~type*weeks+(weeks|colony), data=Pos, REML=F)
</code></pre>

<p>My reasoning is that i have no reason to expect a random intercept, they should all start on 1 at time 0, and then individuals will start avoiding the dead habitat and favouring the live habitat. The <code>(weeks|colony)</code> term allows the slope of each colony to be random across time?</p>

<p>So to my question:</p>

<p>I compare the likelihood of two models with each other, in a likelihood ratio test to get p-values of the fixed effects using a reduced model:</p>

<pre><code>results_null=lmer(weighted~type+weeks+ (weeks|colony), data=Pos, REML=F)
anova(results_null, results_full)
</code></pre>

<p>But what I'm really interested in is at what time point (week) do the individuals start avoiding the dead habitat. as you can see from the figure this happens at week 1 so comparing live-dead habitat week by week ""should"" generate a n/s result at week 0 and sig result from then on (I'm not trying to force a statistically significant result, but the fig is pretty clear...)</p>

<p>I tried converting the weeks into a factor, and then performing </p>

<pre><code>lsmeans(results_full, pairwise~type+weeks)
</code></pre>

<p>But it didn't generate anything that seemed meaningful, the output didn't make sense in relation to the data. </p>

<p>Does anyone have any thoughts on A) whether my model and test is appropriate to this data, and B) how I can perform a post hoc test to compare habitat type over time?</p>

<p>Would it be appropriate to use a Dunetts post hoc test to compare preferences to a reference value (=1) rather than to each other?</p>

<p>Grateful for any ideas or pointers!</p>
"
"0.110959008218296","0.115129433468","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.0915365116690398","0.108545070825851","153611","<p>I'm revising a paper on pollination, where the data are binomially distributed (fruit matures or does not). So I used <code>glmer</code> with one random effect (individual plant) and one fixed effect (treatment). A reviewer wants to know whether plant had an effect on fruit set -- but I'm having trouble interpreting the <code>glmer</code> results.</p>

<p>I've read around the web and it seems there can be issues with directly comparing <code>glm</code> and <code>glmer</code> models, so I'm not doing that. I figured the most straightforward way to answer the question would be to compare the random effect variance (1.449, below) to the total variance, or the variance explained by treatment. But how do I calculate these other variances? They don't seem to be included in the output below. I read something about residual variances not being included for binomial <code>glmer</code> -- how do I interpret the relative importance of the random effect?</p>

<pre><code>&gt; summary(exclusionM_stem)
Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: cbind(Fruit_1, Fruit_0) ~ Treatment + (1 | PlantID)

     AIC      BIC   logLik deviance df.resid 
   125.9    131.5    -59.0    117.9       26 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0793 -0.8021 -0.0603  0.6544  1.9216 

Random effects:
 Groups  Name        Variance Std.Dev.
 PlantID (Intercept) 1.449    1.204   
Number of obs: 30, groups:  PlantID, 10

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  -0.5480     0.4623  -1.185   0.2359   
TreatmentD   -1.1838     0.3811  -3.106   0.0019 **
TreatmentN   -0.3555     0.3313  -1.073   0.2832   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) TrtmnD
TreatmentD -0.338       
TreatmentN -0.399  0.509
</code></pre>
"
"0.138390197576366","0.133335086600437","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.138390197576366","0.143591631723548","153846","<p>I am familiar with linear regression models, but I am in the process of learning about linear mixed effects models.</p>

<p>My data consists of measurements for each month for a set of subjects over a long period of time (~15 years). The subjects and time frames are partially crossed - subjects do not appear for each time point. I also have a number of covariates measured at the per date per subject level, and a single boolean variable indicating a whether a <code>count</code> is before or after a particular time point. The point of this particular model is to measure whether or not a particular event (occurring at the ""mid date"") had an effect on the <code>count</code> variable. Due to the partially crossed, longitudinal nature of my data and the general discontinuity of my data over time, I don't believe that simple paired t-tests can properly answer this question. My data frame is as follows:</p>

<p><code>head</code></p>

<pre><code>   subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
1:          0   2013-05-01     3        2011-07-01                           1     afteronly                TRUE                    22.33333                    195.7986
2:          0   2013-04-01     1        2011-07-01                           1     afteronly                TRUE                    21.33333                    194.7986
3:          0   2013-02-01    19        2011-07-01                           1     afteronly                TRUE                    19.36806                    192.8333
4:          0   2013-12-01     3        2011-07-01                           1     afteronly                TRUE                    29.46806                    202.9333
5:          0   2013-10-01     4        2011-07-01                           1     afteronly                TRUE                    27.43333                    200.8986
</code></pre>

<hr>

<p><code>tail</code></p>

<pre><code>       subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
22407:       6911   2013-08-01     3        2011-08-01                           1     afteronly                TRUE                    24.36667                    198.8653
22408:       6911   2013-07-01     1        2011-08-01                           1     afteronly                TRUE                    23.33333                    197.8319
22409:       6911   2013-06-01     1        2011-08-01                           1     afteronly                TRUE                    22.33333                    196.8319
22410:       6931   2009-05-01     7        2009-05-01                           1    beforeonly               FALSE                     0.00000                    147.0986
22411:        238   2013-09-01     1        2012-10-01                           1     afteronly                TRUE                    11.16667                    199.8986
</code></pre>

<p><code>count</code> is the response I am looking to model.</p>

<p>I've read through all of Bates' lme4 paper, but I am still confused as to how to specify the random effects part of my model.</p>

<p>My attempt at a model specification is:</p>

<pre><code>lmer(log(count) ~ covar1_per_subject_per_date + covar2_per_subject_per_date + 
covar3_per_subject_per_date + after_mid_date_bool + 
subject_group + subject_join_date + (1|subject_id) + (1|date_monthly),
data=df, REML=F)
</code></pre>

<p>Which ""works"" (no errors from <code>lmer</code>). However, my primary question is:</p>

<p>Is this the correct specification for a mixed effects model with random, uncorrelated intercepts for <code>subject_id</code> and <code>date_monthly</code>? Correct here means that we model independent fixed effects for each of the fixed effects specified in the model, accounting for multiple trials of the same subject over time with subjects not appearing at every time point.</p>

<p>A secondary but related question is:</p>

<p>Have I organized my data frame in the proper way? My worry is that the <code>after_mid_date</code> column may be specified improperly.</p>

<p>I apologize if this is long-winded or too-specific of a question. My intention of providing my exact data is to be as clear as possible with my question.</p>
"
"0.133356131201899","0.127724638671999","154037","<p>The <code>metafor</code> package in R can be used to fit the random effects model with the exact binomial likelihood as described in Stijnen <em>et al.</em> (2010). The <code>metafor</code> website also shows how to reproduce the examples from that paper (<a href=""http://www.metafor-project.org/doku.php/analyses:stijnen2010"" rel=""nofollow"">link</a>). One of the benefits of using the binomial likelihood instead of normal approximations is that studies with zero events can be properly modelled instead of adding a continuity correction of 0.5 to those studies. </p>

<p>The documentation for the appropriate <code>rma.glmm</code> function says that the continutity correction is added by default, trough the <code>add</code> argument. Here is the problem: Trying to do the analysis without the continuity correction gives an error. From the <code>metafor</code> webpage we can do the analysis from the Stijnen <em>et al</em> (2010) paper that works fine:</p>

<pre><code>library(metafor) # version 1.9-7
dat &lt;- get(data(dat.nielweise2007))
rma.glmm(measure=""PLO"", xi=ci, ni=n2i, data=dat) # add=1/2 by default.
</code></pre>

<p>If instead the continuity correction is removed an error occurs:</p>

<pre><code>rma.glmm(measure=""PLO"", xi=ci, ni=n2i, data=dat, add=0) # add=0 means no correction.

Error in model.frame.default(formula = yi ~ X - 1, drop.unused.levels = TRUE) : 
  variable lengths differ (found for 'X')
In addition: Warning messages:
1: In escalc.default(measure = measure, xi = xi, mi = mi, add = add,  :
  Some yi and/or vi values equal to +-Inf. Recoded to NAs.
2: In rma.glmm(measure = ""PLO"", xi = ci, ni = n2i, data = dat, add = 0) :
  Some yi/vi values are NA.
</code></pre>

<p>I am able to reproduce the analysis by using the <code>lme4</code> package without the correction and get the same estimates:</p>

<pre><code>library(lme4) # version 1.1-7

# nAGQ=7 is what the metafor package uses 
mm &lt;- glmer(cbind(ci, n2i-ci) ~ (1|study), family=binomial(link='logit'), data=dat, nAGQ=7)
summary(mm)
</code></pre>

<p>Why does it not work to remove the continuity correction in the <code>rma.glmm</code> function? I also tried to add a smaller correction (<code>add=0.0000001</code>) and the estimates for tau, I^2 and H^2 changed a bit, but the numbers under the <em>model results</em> heading did not. What is going on? </p>

<p>References:</p>

<p>Stijnen, T., Hamza, T. H., &amp; Ozdemir, P. (2010). Random effects meta-analysis of event outcome in the framework of the generalized linear mixed model with applications in sparse data. Statistics in Medicine, 29(29), 3046â€“3067.</p>
"
"0.184931680363827","0.184207093548801","154263","<p>I'm trying to plot confidence intervals for linear mixed effects models trained with lme4 and lmerTest in R. I am using <a href=""https://drive.google.com/file/d/0B_jcmrV1IADGX1BPaVU1TnR4anM/view?usp=sharing"" rel=""nofollow"" title=""Zip file on Google Docs"">this data file</a>, which I've shared via Google Drive.</p>

<p>Here is my trained model. The data consists of 8 subjects (SID) and 580 items per subject (UID).</p>

<pre><code>&gt; df &lt;- readRDS(file=""data.Rda"")
&gt; summary(df)
      UID            SID             Y                  X          
 U1     :   8   H1     : 580   Min.   :-1.75000   Min.   :0.00000  
 U10    :   8   H2     : 580   1st Qu.:-0.13330   1st Qu.:0.00000  
 U100   :   8   H3     : 580   Median :-0.02470   Median :0.08054  
 U101   :   8   H4     : 580   Mean   :-0.08563   Mean   :0.14070  
 U102   :   8   H5     : 580   3rd Qu.: 0.00000   3rd Qu.:0.21053  
 U103   :   8   H6     : 580   Max.   : 0.50000   Max.   :1.20000  
 (Other):4592   (Other):1160 
&gt; my.model &lt;- lmer(Y ~ X + (1|UID) + (1|SID), data=df)
&gt; summary(my.model)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: Y ~ X + (1 | UID) + (1 | SID)
   Data: df

REML criterion at convergence: -10980.2

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-16.4681  -0.2699   0.0042   0.3194   6.7467 

Random effects:
 Groups   Name        Variance  Std.Dev.
 UID      (Intercept) 4.573e-03 0.067624
 SID      (Intercept) 2.185e-06 0.001478
 Residual             4.109e-03 0.064099
Number of obs: 4640, groups:  UID, 580; SID, 8

Fixed effects:
              Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)  8.501e-04  3.255e-03  3.056e+02   0.261    0.794    
X           -6.146e-01  8.861e-03  1.644e+03 -69.362   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
X -0.383
</code></pre>

<p>I've tried generating confidence intervals for <code>X</code> using a number of approaches, to no success. <strong>With <code>lsmeans</code>, I don't get any output</strong>.</p>

<pre><code>&gt; lsmeans(my.model)
Least Squares Means table:
     Estimate Standard Error DF t-value Lower CI Upper CI p-value
</code></pre>

<p>I can generate confidence intervals using <code>confint</code> with Wald statistics, <strong>but using the default method runs indefinitely</strong>.</p>

<pre><code>&gt; confint(my.model, method=""Wald"")
                   2.5 %       97.5 %
(Intercept) -0.005530357  0.007230525
X           -0.631989857 -0.597255116
&gt; confint(my.model) # This runs indefinitely
Computing profile confidence intervals ...

&gt;
&gt; effect(c(""X""), my.model) # This also runs indefinitely
</code></pre>

<p>I have no problem getting confidence intervals on the example datasets.</p>

<pre><code>&gt; m1 &lt;- lmer(Informed.liking ~ Gender*Information +(1|Consumer), data=ham)
&gt; lsmeans(m1)
Least Squares Means table:
                        Gender Information Estimate Standard Error  DF t-value Lower CI Upper CI p-value    
Gender  1                    1          NA    5.854          0.183  79    32.0     5.49     6.22  &lt;2e-16 ***
Gender  2                    2          NA    5.609          0.185  79    30.3     5.24     5.98  &lt;2e-16 ***
Information  1              NA           1    5.632          0.155 154    36.4     5.33     5.94  &lt;2e-16 ***
Information  2              NA           2    5.831          0.155 154    37.7     5.53     6.14  &lt;2e-16 ***
Gender:Information  1 1      1           1    5.707          0.218 154    26.2     5.28     6.14  &lt;2e-16 ***
Gender:Information  2 1      2           1    5.556          0.220 154    25.2     5.12     5.99  &lt;2e-16 ***
Gender:Information  1 2      1           2    6.000          0.218 154    27.6     5.57     6.43  &lt;2e-16 ***
Gender:Information  2 2      2           2    5.662          0.220 154    25.7     5.23     6.10  &lt;2e-16 ***
</code></pre>

<p>Here is my session info:</p>

<pre><code>&gt; sessionInfo()
R version 3.1.1 (2014-07-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] effects_3.0-3   pbkrtest_0.4-2  lmerTest_2.0-20 lme4_1.1-7      Rcpp_0.11.6     Matrix_1.2-0   

loaded via a namespace (and not attached):
 [1] acepack_1.3-3.3     bitops_1.0-6        caTools_1.17.1      cluster_2.0.1       colorspace_1.2-6    digest_0.6.8        foreign_0.8-63     
 [8] Formula_1.2-1       gdata_2.16.1        ggplot2_1.0.1       gplots_2.17.0       grid_3.1.1          gridExtra_0.9.1     gtable_0.1.2       
[15] gtools_3.4.2        Hmisc_3.16-0        KernSmooth_2.23-14  lattice_0.20-29     latticeExtra_0.6-26 magrittr_1.5        MASS_7.3-40        
[22] minqa_1.2.4         munsell_0.4.2       nlme_3.1-120        nloptr_1.0.4        nnet_7.3-9          numDeriv_2014.2-1   parallel_3.1.1     
[29] plyr_1.8.2          proto_0.3-10        RColorBrewer_1.1-2  reshape2_1.4.1      rpart_4.1-9         scales_0.2.4        splines_3.1.1      
[36] stringi_0.4-1       stringr_1.0.0       survival_2.38-1     tools_3.1.1
</code></pre>

<p>Can someone explain why <code>lsmeans</code> won't work on my model? Thanks in advance.</p>

<p><strong>Update:</strong> Thanks to @aosmith, I now understand that <code>lsmeans</code> only displays confidence intervals on factors. So here's a related question.</p>

<p>I also tried computing confidence intervals on the fixed effects using the <code>effects</code> package. However, this seems to run indefinitely.</p>

<pre><code>lvls &lt;- c(1:10) / 10
Effect(c(""X""), my.model, xlevels=lvls)
</code></pre>

<p>I don't think it's related to the fact that <code>X</code> is numeric. I tried the following example, and I got </p>

<pre><code>str(mtcars)
m &lt;- lmer(mpg ~ 1 + wt + hp + (1 + wt |gear), data=mtcars)
str(m)
Effect(""wt"", m)
&gt; str(mtcars)
'data.frame':   32 obs. of  11 variables:
 $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
     $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
 $ disp: num  160 160 108 258 360 ...
     $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
 $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
     $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
 $ qsec: num  16.5 17 18.6 19.4 17 ...
     $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
 $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
     $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
 $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
&gt; m &lt;- lmer(mpg ~ 1 + wt + hp + (1 + wt |gear), data=mtcars)
&gt; Effect(""wt"", m)

 wt effect
wt
       2        3        4        5 
24.59905 20.20403 15.80902 11.41400
</code></pre>

<p>Any suggestions why it won't converge in my dataset?</p>
"
"0.128124426527695","0.132940018808798","154488","<p>We measured temperatures of a pond repeatedly every day at each hour for a month at two different depths (i.e., top and bottom). We want to see if the temperatures at the top of the pond are significantly different from the bottom and if so at what hours. Initially I did a two-way anova in R:</p>

<pre><code>aov.result &lt;- aov(temp ~ depth * hour, data = pondtemp)
</code></pre>

<p>followed by post hoc tukey hsd test:</p>

<pre><code>tukey.result &lt;- TukeyHSD(aov.result)
</code></pre>

<p>The pairwise comparison of the depth*hour interaction term is what I need to see which hours have significantly different temperatures between top and bottom. This worked out well but someone pointed out that since it is a repeated measure it does not satisfy the assumption of independence. Therefore I tried using a linear mixed model. I took the depth as the fixed variable and figured the hour (since it has multiple observations in a month) should be the random variable:</p>

<pre><code>pondmdl &lt;- lmer(temp ~ depth + (1+variable|hour), data = pondtemp)
</code></pre>

<p>And used glht package for post-hoc:</p>

<pre><code>summary(glht(pondmdl, mcp(depth = ""Tukey"")))
</code></pre>

<p>However, this does not allow me to do the pair wise comparison I want to do (i.e., comparing Top Hour 1 to Bottom Hour 0 -23)</p>

<p>I found one way by introducing an interaction factor:</p>

<pre><code>pondtemp$depth.hour &lt;- interaction (pondtemp$depth, pondtemp$hour)
</code></pre>

<p>and then using this in my model and glht function:</p>

<pre><code>pondmdl &lt;- lmer(temp~depth.hour + (1+depth|hour), data = pondtemp)
summary(glht(pondmdl, mcp(depth.hour = ""Tukey"")))
</code></pre>

<p>However, I'm not sure if I can allow fixed and random variables to interact like that and still use the same random variable in the random variable error term.</p>

<p>Please advise what is my best option.</p>
"
"0.116961064294386","0.121357078494567","154700","<p>Are following 2 models really the same? </p>

<pre><code>&gt; library(lme4)
&gt; library(lmerTest)
&gt; lmod = lmer(Reaction ~ Days + (Days|Subject), data=sleepstudy)    
&gt; summary(lmod)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: Reaction ~ Days + (Days | Subject)
   Data: sleepstudy

REML criterion at convergence: 1743.6

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9536 -0.4634  0.0231  0.4634  5.1793 

Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.09   24.740       
          Days         35.07    5.922   0.07
 Residual             654.94   25.592       
Number of obs: 180, groups:  Subject, 18

Fixed effects:
            Estimate Std. Error      df t value             Pr(&gt;|t|)    
(Intercept)  251.405      6.825  17.000  36.838 &lt; 0.0000000000000002 ***
Days          10.467      1.546  17.000   6.771           0.00000326 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Days -0.138
</code></pre>

<p>And: </p>

<pre><code>&gt; laov = aov(Reaction ~ Days + Error(Subject/Days), data=sleepstudy)   
&gt; summary(laov)

Error: Subject
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 17 250618   14742               

Error: Subject:Days
          Df Sum Sq Mean Sq F value     Pr(&gt;F)      
Days       1 162703  162703   45.85 0.00000326 ***  
Residuals 17  60322    3548                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: Within
           Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 144  94312   654.9               
</code></pre>

<p>Both are showing similar P values for Days variable. What is the difference between two methods?</p>
"
"0.110959008218296","0.115129433468","154846","<p>I'm analyzing a three-way mixed linear model using lmer:</p>

<p><code>Y ~ Factor1 * Factor2 * Factor3 + (1|sensor)</code></p>

<p>However, different sensors have different gains, uniformly scaling the response of each sensor across conditions. Therefore, a multiplicative random effect seems more appropriate than the additive one I currently use. Can something like this be implemented in R?</p>

<p>Y ~ [Factor1 * Factor2 * Factor3] $\cdot$ (1|sensor)</p>

<p>(The dot stands for multiplication)</p>

<p>My dependent measure can be negative, so using log(Y) doesn't seem like a good solution.</p>

<p><strong>Edit:</strong></p>

<p>I'd like to try a simplified, generalized formulation of this problem, using scalar notation. We'll start with a simple (fixed effects) linear model:
$$y_i=X_i^1\beta_1+X_i^2
\beta_2+X_i^3
\beta_3+\epsilon$$
In this model, given $X$ and $y$, the $\beta$ coefficients can be easily estimated by ordinary least squares. Now, we'll complicate things by assuming that different subsets of observations are sampled by different sensors and each sensor has some random additive contribution $u$. This contribution is normally distributed with 0 expectancy and an unknown variance across sensors.
$$y_i=X_i^1\beta_1+X_i^2
\beta_2+X_i^3
\beta_3+u_{sensor(i)}+\epsilon$$
This model can be estimated as a mixed effects model: <code>Y~X1+X2+X3+(1|sensor)</code>.
However, I'd like to consider the case of a multiplicative random effect:
$$y_i=X_i^1\beta_1u_{sensor(i)}+X_i^2
\beta_2u_{sensor(i)}+X_i^3
\beta_3u_{sensor(i)}+..+\epsilon$$</p>

<p>Note that for each sensor, there's a single scalar gain. I wish to estimate this model by R. To my understanding, the standard random slope approach won't do it, since <code>Y~X1+X2+x3+(X1+X2+X3|sensor)</code> would estimate the model
$$
y_i=X_i^1\beta_1u_{sensor(i)}^1+X_i^2
\beta_2u_{sensor(i)}^2+X_i^3
\beta_3u_{sensor(i)}^3+..+\epsilon
$$
, estimating three different random gains for each sensor instead of one.<code>Y~0+(X1+X2+X3|sensor)</code> won't help either.</p>

<p>Any ideas?</p>
"
"0.128124426527695","0.132940018808798","155040","<p>I am struggling with interpreting coefficients from a multiple regression analysis with multiple categorical (dummy) variables. I am running a linear mixed model with biodiversity (<code>LnS_Add1</code>) as independent variable, and several continuous and categorical dependent variables.</p>

<p>With a single categorical/dummy variable (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3</code>; where <code>AREA_AM_2.5</code> is continuous and <code>System_Type3</code> is categorical with 3 levels, i.e. <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em>) this is pretty straightforward. In this case the intercept represents the mean of the reference dummy variable (e.g. <em>Arable</em>) and the mean of the 2nd and 3rd levels <em>Grassland</em> and <em>Orchard</em> can be calculated manually by adding intercept to the slope coefficient.</p>

<pre><code>globmod1 &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + 
     (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1)
</code></pre>

<p>Which returns</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
(Intercept)            0.3585534  0.1238470   2.895
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Grassland -0.5227684  0.0915722  -5.709
System_Type3Orchard   -0.4057969  0.5477567  -0.741
</code></pre>

<p>To get a summary output that shows the means of both <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em> in R I suppress the intercept by adding a -1 (or +0) to the model.</p>

<pre><code>globmod1.coef &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 -1 +
                   (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1.coef)
</code></pre>

<p>Which returns:</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Arable     0.3585534  0.1238470   2.895
System_Type3Grassland -0.1642149  0.1341851  -1.224
System_Type3Orchard   -0.0472434  0.5457304  -0.087
</code></pre>

<p>But what do I do if I have multiple categorical variables (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + Habitat2</code>; where Habitat2 is a categorical variable with 3 levels, i.e. <em>Farm aggregated</em>, <em>Outside field</em>, and <em>Within field</em>)?. Now the intercept represents the mean of the reference level of a combination of <code>System_Type3</code> and <code>Habitat2</code> (e.g. all data in arable systems and measured at farm aggregate level). But what I am interested in are the means for the different levels of each of my 2 categorical variables, holding everything else constant.</p>

<p>How do I create a summary table that contains means of all levels of all categorical variables in my model? The -1 command doesnt help me anymore, as it removes the intercept but the intercept now represents a mean of 2 reference dummy variables. I am only interested here in the fixed effect estimates, not in any hypothesis testing.</p>
"
"0.0838771261337098","0.0870296712971673","155702","<p>For school, I'm tasked with investigating the effect of beta carotene on the prevention of skin cancer.
For this, I have data on several patients that are examined over the course of 5 years in medical centers. In essence, each year the patient goes to his examination center, and the number of new skin cancers since the previous examination are counted.
So we have a placebo controlled multilevel study (patient's in centers).
My outcome is the number of new skin cancers since the previous checkup.</p>

<p>I want to fit a Linear Mixed Model using the GMLER method of the LME4 package, and opt for a poisson model because our outcome is a count variable.</p>

<pre><code>mod1 &lt;- glmer(Y ~ 1 + Treatment*Year + (1 | Center/ID),
              data = skin, family = poisson(link = ""log""))
</code></pre>

<p>I found that, to investigate the BLUPs, I have to do the following</p>

<pre><code>ranef(mod1)
</code></pre>

<p>I'm assuming this gives me BLUPs on the patient level, and on the center level.</p>

<p>The problem however, is that these most of these BLUPs are negative (between -1 and 0), which is not what I expect as I'm working with count data.
It occurred to me that I'm using the log-link in my model, so I then exponentiated the BLUPs from <code>ranef</code>, and, because my original BLUPs are between -1 and +inf, I now obtain only positive values.
But is this correct, or do I have to specify my model differently or do something else to get the BLUPs.</p>
"
"0.110959008218296","0.102337274193778","156237","<p>We are implementing multilevel models in lme4 and have a question about how to handle cross-level predictors. This is a psychology experiment where individual participants come into the lab and complete multiple trials of the same task (e.g., judging how much they like a picture). To describe our dataset, we have trials nested within participants. These trials also have a trial-level predictor (e.g., how happy the participant rated they were before they made the judgment), and we might be interested in the relationship between happiness and liking (both rated on a 1-7 scale and treated as a linear variable). Modeling this with a random intercept for participant would be:</p>

<pre><code>lmer(liking~happiness + (1|participant), data)
</code></pre>

<p>Now, in these data we also have three distinct races completing  the experiment (e.g., participants that self-identify as white-only, black-only, or hispanic-only). Each participant only belongs to 1 race, and each race contains multiple participants. </p>

<p>We hypothesize that trial-level happiness will interact with participant-level race to predict liking. To test this model, we believe lme4 will detect that race is a group-level factor (since only one value exists for each participant) and that we would run:</p>

<pre><code>lmer(liking~happiness*race + (1|participant), data)
</code></pre>

<p>However, based on other reading, we're wondering if this should instead be treated as a nested or random slope. For instance, should we instead use:</p>

<pre><code>lmer(liking~happiness*race + (1| race/participant), data)
</code></pre>

<p>or </p>

<pre><code>lmer(liking~happiness*race + (1 + happiness | race/participant), data)
</code></pre>

<p>Again, we are interested in the interaction between race and happiness in predicting liking, and each participant only belongs to one race. Thank you in advance for your help!</p>

<p>PS: We have looked at <a href=""http://stats.stackexchange.com/questions/120517/specifying-cross-level-interactions-in-lmer"">Specifying Cross-Level Interactions in LMER</a> but this seems to represent a different data structure. </p>
"
"0.133356131201899","0.138368358561332","157851","<p>In the <code>lmer</code> function within <code>lme4</code> in <code>R</code> there is a call for constructing a model matrix of random effects, $Z$, as explained <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">here</a>, pages 7 - 9.</p>

<p>Calculating $Z$ entails KhatriRao and/or Kronecker products of two matrices, $J_i$ and $X_i$.  </p>

<p>The matrix $J_i$ is a mouthful: ""Indicator matrix of grouping factor indices"", but it seems to be a sparse matrix with dummy coding to select which unit (for example, subjects in repetitive measurements) corresponding to higher hierarchical levels are ""on"" for any observation. The $X_i$ matrix seems to act as a selector of measurements in the lower hierarchical level, so that the combination of both ""selectors"" would yield a matrix, $Z_i$ of the form illustrated in the paper via the following example:</p>

<pre><code>(f&lt;-gl(3,2))

[1] 1 1 2 2 3 3
Levels: 1 2 3

(Ji&lt;-t(as(f,Class=""sparseMatrix"")))

6 x 3 sparse Matrix of class ""dgCMatrix""
     1 2 3
[1,] 1 . .
[2,] 1 . .
[3,] . 1 .
[4,] . 1 .
[5,] . . 1
[6,] . . 1

(Xi&lt;-cbind(1,rep.int(c(-1,1),3L)))
     [,1] [,2]
[1,]    1   -1
[2,]    1    1
[3,]    1   -1
[4,]    1    1
[5,]    1   -1
[6,]    1    1
</code></pre>

<p>Transposing each of these matrices, and performing a Khatri-Rao multiplication:</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp;. &amp;. &amp;. &amp;.\\.&amp;.&amp;1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;1&amp;1 \end{smallmatrix}\right]\ast \left[\begin{smallmatrix}\,\,\,\,1 &amp; 1 &amp;\,\,\,\,1 &amp;1 &amp;\,\,\,\,1 &amp;1\\-1&amp;1&amp;-1&amp;1&amp;-1&amp;1 \end{smallmatrix}\right]=
\left[\begin{smallmatrix}\,\,1 &amp; 1 &amp;.&amp;.&amp;.&amp;.\\\,\,\,\,-1 &amp;1&amp;.&amp;.&amp;.&amp;.\\ .&amp;.&amp;\,\,\,\,\,1 &amp;1&amp;.&amp;.\\.&amp;.&amp;\,\,-1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;\,\,\,1&amp;1\\.&amp;.&amp;.&amp;.&amp;-1&amp;1 \end{smallmatrix}\right]$</p>

<p>But $Z_i$ is the transpose of it:</p>

<pre><code>(Zi&lt;-t(KhatriRao(t(Ji),t(Xi))))

6 x 6 sparse Matrix of class ""dgCMatrix""

[1,] 1 -1 .  . .  .
[2,] 1  1 .  . .  .
[3,] .  . 1 -1 .  .
[4,] .  . 1  1 .  .
[5,] .  . .  . 1 -1
[6,] .  . .  . 1  1
</code></pre>

<p>It turns out that the authors make use of the database <code>sleepstudy</code> in <code>lme4</code>, but don't really elaborate on the design matrices as they apply to this particular study. So I'm trying to understand how the made up code in the paper reproduced above would translate into the more meaningful <code>sleepstudy</code> example.</p>

<p>For visual simplicity I have reduced the data set to just three subjects - ""309"", ""330"" and ""371"":</p>

<pre><code>require(lme4)
sleepstudy &lt;- sleepstudy[sleepstudy$Subject %in% c(309, 330, 371), ]
rownames(sleepstudy) &lt;- NULL
</code></pre>

<p>Each individual would exhibit a very different intercept and slope should a simple OLS regression be considered individually, suggesting the need for a mixed-effect model with the higher hierarchy or unit level corresponding to the subjects:</p>

<pre><code>    par(bg = 'peachpuff')
    plot(1,type=""n"", xlim=c(0, 12), ylim=c(200, 360),
             xlab='Days', ylab='Reaction')
    for (i in sleepstudy$Subject){
                fit&lt;-lm(Reaction ~ Days, sleepstudy[sleepstudy$Subject==i,])
            lines(predict(fit), col=i, lwd=3)
            text(x=11, y=predict(fit, data.frame(Days=9)), cex=0.6,labels=i)
        }
</code></pre>

<p><img src=""http://i.stack.imgur.com/opwVvm.png"" alt=""enter image description here""></p>

<p>The mixed-effect regression call is:</p>

<pre><code>fm1&lt;-lmer(Reaction~Days+(Days|Subject), sleepstudy)
</code></pre>

<p>And the matrix extracted from the function yields the following:</p>

<pre><code>parsedFormula&lt;-lFormula(formula= Reaction~Days+(Days|Subject),data= sleepstudy)
parsedFormula$reTrms

$Ztlist
    $Ztlist$`Days | Subject`
6 x 12 sparse Matrix of class ""dgCMatrix""

309 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . . . . . . . . . . . .
309 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . . . . . . . . . . . .
330 . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . .
330 . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . .
371 . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1
371 . . . . . . . . . . . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9
</code></pre>

<p>This seems right, but if it is, what is linear algebra behind it? I understand the rows of <code>1</code>'s being the selection of individuals like. For instance, subject <code>309</code> is on for the baseline + nine observations, so it gets four <code>1</code>'s and so forth. The second part is clearly the actual measurement: <code>0</code> for baseline, <code>1</code> for the first day of sleep deprivation, etc.</p>

<p><strong>But what are the actual</strong> $J_i$ <strong>and</strong> $X_i$ <strong>matrices and the corresponding</strong> $Z_i= (J_i^{T}âˆ—X_i^{T})^âŠ¤$ <strong>or</strong> $Z_i= (J_i^{T}\otimes X_i^{T})^âŠ¤$, <strong>whichever is pertinent?</strong></p>

<p>Here is a possibility,</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.  &amp;. &amp;. &amp;. &amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.\\
.&amp;.&amp;.&amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.\\&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1\end{smallmatrix}\right]\ast \left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1&amp;1&amp;1&amp;1 &amp; 1 &amp; 1 &amp; 1\\0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right]=$</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\0 &amp; 1 &amp; 2 &amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\ &amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0 &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\\&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right] $</p>

<p>The problem is that it is not the transposed as the <code>lmer</code> function seems to call for, and still is unclear what the rules are to create $X_i$.</p>
"
"0.138390197576366","0.133335086600437","158319","<p>I have used a repeated-measures ANOVA in SPSS to analyse some of my data. It's the typical approach in my area, but I think it might be more appropriate to use a mixed effect model. However, I struggle with both building the model as well as interpreting it.</p>

<p><strong>Experimental design</strong></p>

<p>300+ participants from two different samples have rated on a continuous scale a stimulus at seven different manipulation levels. I want to test whether individual differences in the participants (recorded as ordinal or binary variables) interact with that rating score. In particular, I'm interested in whether the rating score changes as a function of stimulus level differently in people that, for example, feel mainly attracted to men or women.</p>

<p>Thus,  I have a within-subjects factor (stimulus level), a between-subjects factor (such as being attracted to men or women), and a random effect of participant nested in sample.</p>

<p>I've been using <code>lmer()</code> from the lme4 package and lmerTest and have come up with the following model</p>

<pre><code>model &lt;- lmer (rating.score ~ stim.level + factor + stim.level*factor +
                                                     (1|participant) + (1|sample), mydata)
</code></pre>

<p><strong>Analysis</strong></p>

<ol>
<li>Is lmer() the right package to work with?</li>
<li>Am I appropriately accounting for the random effects of participant and sample, or do I need something like <code>(1|sample/participant)</code>? I followed the <a href=""http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf"" rel=""nofollow"">Pastes data example</a>, but am not sure that's the right thing to do in this context.</li>
<li>Based on previous literature, I expect the relationship of <code>rating.score</code> and <code>stim.level</code> to be quadratic - should/could I enter <code>stim.level</code> as squared term?</li>
</ol>

<p><strong>Interpretation</strong></p>

<p>In SPSS, I find a significant interaction of <code>stim.level</code> x <code>factor</code>. By visualizing the interaction and running post-hoc tests I can then interpret the nature of that interaction. In R, I get estimates of the interaction at each level of <code>stim.level</code>, some of which are significant, some of which are not. Can I still make the conclusion that <code>factor</code> affects the relationship of <code>rating.score</code> and <code>stim.level</code> (even though not necessarily to the same extent at each level)?</p>

<p><strong>EDIT:</strong> I just realized I had entered <code>stim.level</code> as a factor. I think it is appropriate to enter it as a linear variable - the different levels correspond to the same manipulation applied with increasing extent (the steps between each level are the same). This also resolves one of my earlier questions regarding an error message when trying to model random slopes which I have thus now removed.</p>
"
"0.0523065780965941","0.0542725354129257","158524","<p>I'm testing various specifications of linear mixed effects models with <code>lmer()</code> in <code>R</code>.  The data are fiscal year firm-level, so each data point is uniquely identified by the firm's <code>id</code> and <code>fyear</code>.</p>

<p>Is it viable to use <code>id</code> and <code>fyear</code> as separate (i.e., non-nested) grouping variables -- or is the model being over-identified?  For example, are</p>

<pre><code>M1 &lt;- lmer(investment ~ 1 + interest + (1 | id) + (1 | fyear), data=df)
M2 &lt;- lmer(investment ~ 1 + interest + (1 + interest | id) + (1 + interest | fyear), data=df)
</code></pre>

<p>feasible?</p>

<p>So far testing LMMs like this seems to be fine -- the models converge and give reasonable estimates compared to versions in which I only use one grouping variable.</p>

<p>I'm still relatively new with mixed effects modelling, so any elaboration on why this non-nested structure is or is not feasible would be appreciated. Thnx!</p>
"
"0.0905976508333704","0.0940027887907685","160445","<p>I have computed GLMM using glmer in R. My response variable is species richness and my explanatory variable is grazing treatment (with three categories: cattle, sheep and ungrazed). In the model I have included site as a fixed variable and also a new object with the same number of variations as I have to attempt to account for underdispersal (<code>obs</code>):</p>

<pre><code>model2&lt;-glmer(VegRichness~Grazing+(1|Site)+(1|obs),family=""poisson"",data=veg.rich)
</code></pre>

<p>My output is below and the questions I have about it are:</p>

<p>How do I interpret the fixed effects section?
Cattle grazing seems to be missing in the oputput, is this because it is somehow incorporated into the intercept?</p>

<pre><code>&gt; summary(model2)

Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]

Family: poisson  ( log ) 

Formula: VegRichness ~ Grazing + (1 | Site) + (1 | obs)
   Data: veg.rich


     AIC      BIC   logLik deviance df.resid 
    178.8    185.2    -84.4    168.8       22 

Scaled residuals: 

Min..........           1Q............           Median....       3Q.........        Max

-1.4936...      -0.5698.....       -0.1928...      0.4923...   1.3646 

Random effects:

 Groups  ... Name......        Variance..... Std.Dev.

 obs.........      (Intercept).. 0.00000....  0.0000 

 Site.........     (Intercept).. 0.03596....  0.1896

Number of obs: 27, groups:  obs, 27; Site, 3

Fixed effects:
                .......Estimate.... Std. Error..... z value... Pr(&gt;|z|)    
(Intercept)............      3.55358.......    0.12309.......  28.869.....  &lt; 2e-16 ***                                                                 
GrazingSheep......     0.01242......    0.07876........   0.158.......  0.87467    
GrazingUngrazed -0.27526.....    0.08503........  -3.237......  0.00121 ** 

---
Signif. codes:  0 x***x 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr)....GrzngS

GrazingShe......................... -0.322       
GrzngUngrzd...................... -0.298...  0.466
</code></pre>
"
"0.128505183463769","0.133335086600437","161358","<p>I am a medical doctor and definitely not an expert in statistics, although I think I do okay and I am familiar with R.<br></p>

<p>I have discussed the following issue with a statistician but I am still not sure whether we came up with the right solution. I hope one of you can help.</p>

<p>THE AIM:<br>
I want to estimate the precision (repeatability) of a continuous variable measured on x-rays (PT) and evaluate the influence of different raters by specifying the variance between and within raters (inter- and intra-rater variance).<br></p>

<p>THE DESIGN:<br>
I have had 4 raters measure the continuous variable on 67 x-rays twice, thus each x-ray has been measured 8 times in total.</p>

<p>THE DATA (Pre):<br></p>

<pre><code>'data.frame':   536 obs. of  4 variables:
 ID    : Factor w/ 67 levels 
 Rater : Factor w/ 4 levels 
 Time  : Factor w/ 2 levels 
 PT    : num  40.4 29.3 36 58.8 40.5 ...
</code></pre>

<p>THE MODEL we came up with was a linear mixed effect fit:<br></p>

<pre><code>a &lt;- lmer(PT~ (1|ID) + (1|Rater:ID), data=Pre) 
</code></pre>

<p>From exploring the web I found that "":"" is rarely used and that the model is equivalent to:<br></p>

<pre><code>b &lt;- lmer(PT~ (1|ID/Rater), data=Pre)
</code></pre>

<p>THE RESULT of either model a or b is:<br></p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: PT ~ (1 | ID/Rater)
   Data: Pre

REML criterion at convergence: 2729.2

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.7958 -0.2266 -0.0165  0.2214  4.8076 

Random effects:
 Groups   Name        Variance Std.Dev.
 Rater:ID (Intercept)  4.630   2.152   
 ID       (Intercept) 97.178   9.858   
 Residual              2.695   1.642   
Number of obs: 536, groups:  Rater:ID, 268; ID, 67

Fixed effects:
            Estimate Std. Error t value
(Intercept)   18.611      1.214   15.34
</code></pre>

<p>THE INTERPRETATION according to the statistician was that:<br>
The residual variance represents the intra-rater variance and 
the inter-rater variance is the sum of Rater:ID and residual variance.</p>

<p>THE QUESTION:<br>
Are these interpretations valid?<br>
My concern is especially if the residual variance is truly an expression of the intra-rater variance or if I should somehow include the ""Time"" variable in the model to specify the variance between the first and second measurements within raters within subjects.<br></p>
"
"0.138390197576366","0.143591631723548","161569","<h2>The data structure</h2>

<p>I have 2 groups with 30 subjects each. Each subject has a different number of fibers (approximately 46000 +/- 3000) of different length (see histogram). My goal is to determine how much of the observed variation in fibre length is explained by the factor group (A or B) correcting for the influence of subjects' age. I'd avoid aggregating the data over subjects, since I would reduce 46000 fiber lengths to a single value.</p>

<p>The data look like this:</p>

<pre><code>line length     ID      age group
1     57.21179  a001    18  A
2     68.86046  a001    18  A
3    141.03307  a001    18  A
4     87.74376  a001    18  A
5     59.00859  a001    18  A
6    119.93846  a002    18  A
7     58.46695  a002    22  A
8    121.13653  a002    22  A
</code></pre>

<h2>This is what i have tried so far using R:</h2>

<ol>
<li><p>2-sample t-test:</p>

<p><code>t.test(length~group, data=longdfr)</code></p>

<p>Result: t = -12.4432; df = 2772102; p-value &lt; 2.2e-16<br>
The fact that the data was derived from different subjects is not considered in this apporach. Therefore I guess this is not a valid approach.</p></li>
<li><p>2-sample Kolmogorov-Smirnov test:</p>

<p><code>ks.test(longdfr$length[longdfr$group=='A'],longdfr$length[longdfr$group=='B'])</code></p>

<p>Result: D = 0.0096, p-value &lt; 2.2e-16<br>
The variances apparently are not equaly distributed. Like in the t-test i cannot correct for the subjects' age.</p></li>
<li><p>So I tried linear mixed-effects models using the <code>lme4</code> package:</p>

<p><code>lme.length &lt;- lmer(length ~ group + age + (1|ID), data=longdfr)</code></p>

<pre><code>REML criterion at convergence: 26425485
Random effects:
Groups    Name         Std.Dev.
ID        (Intercept)  1.696
Residual               28.289  
Number of obs: 2774925, groups:  ID, 60
Fixed Effects:
(Intercept)     groupntc  
81.4934         0.3779  
</code></pre></li>
</ol>

<p>With this approach I have two concerns. First, does lmer realy consider all fibres or does it use the same amount of fibres for all subjects. Second, does it consider subseqeunt fibres as repeated measures or as independent measures? </p>

<ol start=""4"">
<li><p>Since I like to get p-values I further tried <code>afex</code> package:</p>

<p>afe.length &lt;- mixed(length ~ group + age + (1|ID), data=longdfr)</p></li>
</ol>

<p>With <code>mixed</code> again I have the same concerns as with the lmer model. Worse still, running the model results in the abortion of the R Session while obtaining 1 p-value.</p>

<h2>My questions:</h2>

<p><strong>Is there a better statistic to test how much of the observed variation in fibre length (considering all fibers differing in number across subjects) is explained by the factor group?</strong> -- I am afraid <code>lmer</code> or <code>mixed</code> might interpret different fibers as repeated measures.</p>

<p><strong>How can I deal with different fiber numbers across different subjects without aggregation +/- 46000 fiber measures to a single mean value?</strong> </p>

<p><img src=""http://i.stack.imgur.com/GmFrh.png"" alt=""length histogram""></p>
"
"0.110959008218296","0.102337274193778","162701","<p>I am attempting to fit a linear mixed-effect model in R using <code>lme4</code> that is quite a bit more complex than any example I've seen in forums or in textbooks. I am having trouble finding the correct code for the random effects in particular. I have two fixed factors (parental environment and germination treatment) and random factor (genetic line); I want to test all 2 and 3-way interactions between these factors. I have two additional fixed factors that I want to test without interactions (provisioning and block). I want to specify random intercepts and slopes for genetic line, but I am not sure how to do this. The dependent variable is biomass. </p>

<p>Is this model specification correct?:</p>

<pre><code>fullmod &lt;- lmer(biomass ~ parental.environment* germination.treatment*
   (1+parental.environment*germination.treatment|genetic.line)
                + provisioning + block, biomass.data)
</code></pre>

<p>I am also interested in testing the significance of the 3-way interaction parental environment x germination treatment x genetic line by comparing the fit of the reduced model to the full model. What would be the correct model specification for the full model minus this 3-way interaction?</p>
"
"0.173481293613825","0.180001636385951","163326","<p>Assume that I have a linear mixed model of the following form, specified using lme4:</p>

<pre><code>fit &lt;- lmer(A ~ B + C + (1|D) + (1|E), data=data)
</code></pre>

<p>I am struggling with the best way to isolate the net impact of the fixed effects (B or C) on the value of D in the overall model, as well as the impact of random effect E on D.</p>

<p>To isolate the net effect of random effect D, normally I would do something like this, to predict outputs both with and without D:</p>

<pre><code>data$w_D &lt;- predict(fit, data, type='response')
data$wo_D &lt;- predict(fit, data, re.form=~ (1|E), type='response')
</code></pre>

<p>Let's call this a ""with and without"" D analysis.  I would then use something like the aggregate function to compile the sum difference of D in the predicted columns of ""w_D"" and ""wo_D"", subtract the sums of of those two columns from each other, and deposit the answer for all subjects in D to a new vector.  Let's call the new vector D_net, and it contains the net value of D, which is what I care about.  There may be other ways to do this, but this is the approach that makes sense to me.</p>

<p>But this gets a little interesting if one wants to isolate the impact of a random effect AND start isolating the impact of another variable, whether it be a fixed effect or a random effect.</p>

<p>Let's start with B, a fixed effect. One option could be to convert all observations of B to their mean, otherwise predict the same ""with and without"" D sequence above, and subtract the values of this new D_net from the original D_net.  My concern is that effectively removing a fixed effect like this could meaningfully affect the residual, meaning that the original random effect assignments might no longer be valid.</p>

<p>To ensure the random effects are recalculated, we could also re-specify the model by dropping B (let's call this <strong>fit2</strong>), execute the model otherwise the same, do a ""with and without"" sequence for D in this updated equation, and again subtract these D_net values from the original D_net values. This would ensure a recalculation of the random effects, but now we've again affected the residual (by removing B). This also feels a bit backwards because now B has been removed entirely, and the remaining values, instead of being held constant to reflect the value of B, may themselves change as a result of the modified residual.</p>

<p>Perhaps a third option would still involve a second model for comparison, but to instead add an interaction between D and B.  So we end up with something like this:</p>

<pre><code>fit3 &lt;- lmer(A ~ B + C + (1|D) + (1|E) + (1+B:D), data=data)  
</code></pre>

<p>The net difference of the ""with and without"" predictions of <strong>fit3</strong> could again be calculated and then subtracted from the original model's net D value.   Or, perhaps, with the interaction already accounting for the interaction of B:D, there may be no need to subtract from the original D_net at all. There are two further issues with this approach, however: if one of the fixed effects is itself an interaction between two fixed effects, it's unclear how that interaction can itself be interacted with a random effect. (1|B:C:D)? And of course, for large data sets, interactions with random effects can be computationally challenging, and sometimes require impractical amounts of memory.</p>

<p>If I wanted to deal with the effect of random effect E, instead of a fixed effect like B, I would be inclined to consider the same options.</p>

<p>Are any these options for investigating the effect of B (or E) on D actually satisfactory for the objective I've stated? If not, is there still another alternative? This is an aspect of mixed model interpretation I have yet to see addressed in any publications.</p>

<p>Thanks.</p>
"
"0.135242450223678","0.169867801811242","164457","<p>I have seen questions about this on this forum, and I have also asked it myself in a previous post but I still haven't been able to solve my problem. Therefore I am trying again, formulating the question as clearly as I can this time, with as much detailed information as possible. </p>

<p>My data set has a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I am using a mixed effects model using glmer. Here is what I entered in R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I get 2 warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>My summary looks like this:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting the following in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So that's what I did:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>Now, the second warning is gone, but the first one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks odd:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>What I can do to solve this? Or can anyone tell me what this warning even means? (in a way that an R-newbie like myself can understand)</p>
"
"0.128124426527695","0.132940018808798","164551","<p>Consider a linear random intercept model:  </p>

<p>\begin{align}
y_{ij} &amp;= A_{i} + \varepsilon_{ij}  \\
A_{i} &amp;\sim N(0,\tau^2)  \\
\varepsilon_{ij} &amp;\sim N(0,\sigma^2)
\end{align}</p>

<p>where, $A_i$ and $\varepsilon_{ij}$ are iid and independent of each other. I would fit this in R with something like <code>lmer(y ~ (1|id))</code>, where <code>id</code> is the group index ($i$ in the previous sentence). What are the fitted values I get by calling <code>fitted()</code> on the object returned by <code>lmer</code>?</p>

<p>I have heard it said (perhaps the speaker was speaking loosely) that making an effect random instead of fixed doesn't affect the point estimate, just its variance. Well if I used a fixed intercept above I would expect OLS to give the group means as the fitted values, which is not what I get when I run <code>fitted</code> on the random effects model. Also the remark I quoted doesn't seem quite right, because in the fixed effects case you would be fitting a term for each group whereas in the random effects case you would just have 1 df, right, the variance of the group effect? Is that right? Does the random effect likelihood equation integrate out the intercept values so just the group effect variance needs to be estimated? If my understanding is correct, and intercept terms are never estimated, how does one interpret fitted values in a random effects model? </p>
"
"0.128505183463769","0.123078541477327","167649","<p>I have a dataset with one continuous response variable (time), a 'treatment' explanatory variable and 5 other fixed factors:</p>

<pre><code>summary(dat)
area         date    day_time    treatment       time        trials  
A:31   05.04.14: 8   A:23     Control :32   Min.   :0.0000   Orca2_1 :  2  
B: 8   06.04.14: 8   B:28     Orca2 ON:15   1st Qu.:0.0000   Orca2_14: 2  
C:26   07.04.14: 8   C:14     SR2 ON  :18   Median :0.0278   Orca2_16: 2  
       08.04.14: 8                          Mean   :0.1272   Orca2_17: 2  
       25.04.15: 6                          3rd Qu.:0.2023   Orca2_18: 2  
       28.04.15: 6                          Max.   :0.9216   Orca2_19: 2  
       (Other) :21                                           (Other) :53  
</code></pre>

<p>I would like to build a linear mixed effect model like:</p>

<pre><code>M1 = lmer(time ~ treatment + (1|date) + (1|area) + (1|trials) + (1|day_time) , data=dat)
</code></pre>

<p>However, the data is zero-truncated as seen on the histogram:
<a href=""http://i.stack.imgur.com/WHoSG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WHoSG.png"" alt=""histogram of time""></a></p>

<p>When I run the model, I get an abnormal distribution of residuals:</p>

<p><a href=""http://i.stack.imgur.com/g2znO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/g2znO.png"" alt=""norma qqplot""></a></p>

<p>I tried to transformed the time variable with an inverse hyperbolic sine transformation:</p>

<pre><code>transformed_time &lt;- log(dat$time + sqrt(dat$time^dat$time +1))
</code></pre>

<p>and then run the model again:</p>

<pre><code>M2 = lmer(transformed_time ~ treatment + (1|date) + (1|area) + (1|trials) + (1|day_time) , data=dat)
qqnorm(residuals(M2))
</code></pre>

<p>It gets a bit better but still not convincing.</p>

<p>I am getting confused with everything that I am reading on zero-truncated models for continuous data... I was wondering if someone could help me find a way to build a valid model. </p>

<p>Here is the data:</p>

<p><strong>EDIT</strong> : added a time_cat column for time categories A-I</p>

<pre><code>&gt; dput(dat)
structure(list(area = structure(c(1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 1L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
3L, 3L, 3L, 3L, 1L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
1L, 1L, 1L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c(""A"", ""B"", 
""C""), class = ""factor""), date = structure(c(1L, 1L, 1L, 1L, 2L, 
2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 5L, 5L, 6L, 6L, 
6L, 8L, 7L, 9L, 10L, 10L, 10L, 11L, 12L, 12L, 12L, 3L, 3L, 3L, 
4L, 4L, 4L, 4L, 8L, 10L, 10L, 10L, 11L, 12L, 12L, 12L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 9L, 9L, 9L
), .Label = c(""05.04.14"", ""06.04.14"", ""07.04.14"", ""08.04.14"", 
""11.04.14"", ""25.04.15"", ""26.04.15"", ""26.05.15"", ""27.04.15"", ""28.04.15"", 
""29.04.15"", ""30.04.15""), class = ""factor""), day_time = structure(c(1L, 
1L, 2L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 1L, 2L, 2L, 3L, 
1L, 2L, 1L, 2L, 3L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 
2L, 3L, 1L, 2L, 2L, 3L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 
2L, 3L, 1L, 2L, 3L, 3L, 1L, 2L, 2L, 1L, 2L, 3L, 1L, 1L, 1L, 2L
), .Label = c(""A"", ""B"", ""C""), class = ""factor""), treatment = structure(c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L
), .Label = c(""Control"", ""Orca2 ON"", ""SR2 ON""), class = ""factor""), 
    time = c(0.2148, 0.1814, 0.106300000000005, 0.248799999999999, 
    0.129899999999999, 0.109099999999998, 0, 0.145200000000003, 
    0.1522, 0, 0.202300000000001, 0.921599999999998, 0.580300000000001, 
    0.1327, 0.617799999999995, 0.3309, 0.3127, 0.311299999999999, 
    0.151499999999999, 0, 0, 0, 0, 0.0806000000000004, 0.262699999999995, 
    0, 0, 0, 0, 0.2224, 0, 0, 0.136900000000004, 0.0743999999999971, 
    0, 0, 0.0784999999999982, 0.360700000000001, 0, 0, 0.0277999999999992, 
    0, 0, 0, 0, 0, 0, 0.238399999999999, 0.169600000000003, 0.394000000000005, 
    0, 0, 0, 0, 0.152200000000008, 0.151499999999999, 0.440600000000003, 
    0.331499999999998, 0, 0, 0, 0, 0, 0.296800000000005, 0), 
    time_cat = structure(c(4L, 3L, 3L, 4L, 3L, 3L, 1L, 3L, 3L, 
    1L, 4L, 9L, 7L, 3L, 8L, 5L, 5L, 5L, 3L, 1L, 1L, 1L, 1L, 2L, 
    4L, 1L, 1L, 1L, 1L, 4L, 1L, 1L, 3L, 2L, 1L, 1L, 2L, 5L, 1L, 
    1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 4L, 3L, 5L, 1L, 1L, 1L, 1L, 
    3L, 3L, 6L, 5L, 1L, 1L, 1L, 1L, 1L, 4L, 1L), .Label = c(""A"", 
    ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""J""), class = ""factor""), 
    trials = structure(c(16L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 
    33L, 33L, 1L, 7L, 11L, 12L, 13L, 14L, 15L, 17L, 19L, 20L, 
    21L, 22L, 23L, 2L, 25L, 3L, 4L, 5L, 6L, 8L, 9L, 10L, 1L, 
    7L, 11L, 12L, 13L, 14L, 15L, 2L, 3L, 4L, 5L, 6L, 8L, 9L, 
    10L, 16L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 17L, 18L, 19L, 
    20L, 21L, 22L, 23L, 24L, 24L, 25L), .Label = c(""Orca2_1"", 
    ""Orca2_14"", ""Orca2_16"", ""Orca2_17"", ""Orca2_18"", ""Orca2_19"", 
    ""Orca2_2"", ""Orca2_20"", ""Orca2_21"", ""Orca2_22"", ""Orca2_3"", 
    ""Orca2_4"", ""Orca2_5"", ""Orca2_6"", ""Orca2_7"", ""SR2_1"", ""SR2_10"", 
    ""SR2_11"", ""SR2_12"", ""SR2_14"", ""SR2_15"", ""SR2_16"", ""SR2_17"", 
    ""SR2_18"", ""SR2_19"", ""SR2_2"", ""SR2_3"", ""SR2_4"", ""SR2_5"", ""SR2_6"", 
    ""SR2_7"", ""SR2_8"", ""SR2_9""), class = ""factor"")), .Names = c(""area"", 
""date"", ""day_time"", ""treatment"", ""time"", ""time_cat"", ""trials""
), class = ""data.frame"", row.names = c(NA, -65L))
</code></pre>
"
"0.0369863360727655","0.0383764778226668","167780","<p>I am getting the exact same results for a probit regression and post-hoc tests (simultaneous tests for linear hypotheses) - is this because I have used a dummy variable in the probit model and so it is effectively comparing each factor level to the reference group thus when I run the post-hoc, which is comparing differences between the two groups, that I get the same answers?</p>

<p>This is the model I fitted:</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>

<p>and this is the post hoc test that I did:</p>

<pre><code> summary(glht(m1, lsm(pairwise ~ Name.Origin)))
</code></pre>
"
"0.0978566471559948","0.101534616513362","169115","<p>I have a dataset that has measurements of resource consumption in buildings for a number of years. I am interested in the differences in resource consumption of buildings in my study area between years (as opposed to differences between individual buildings). I've fitted a Linear Mixed Model to my data with the lme4 package in R using the formula: <code>model = lmer(resource.consumption ~ year + (1|building.id))</code></p>

<p>I would like to put this into a formula or equation format that will allow those unfamiliar with R to be able to understand what is being estimated by this model. However, I am having some trouble figuring out how to go about this given that 'year' is a factor in this scenario. The summary() function gives the following output:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: resource.consumption. ~ year + (1 | building.id)
   Data: year.comp

REML criterion at convergence: 122.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.1312 -0.4170 -0.0711  0.3419  5.0172 

Random effects:
 Groups      Name        Variance Std.Dev.
 building.id (Intercept) 0.07294  0.2701  
 Residual                0.04537  0.2130  
Number of obs: 368, groups:  building.id, 107

Fixed effects:
               Estimate Std. Error t value
(Intercept)     1.32746    0.05565  23.855
year2007       -0.24504    0.06029  -4.064
year2008       -0.36634    0.05817  -6.298
year2009       -0.44730    0.05551  -8.057
year2010       -0.47449    0.05391  -8.801
year2011       -0.53752    0.05524  -9.730

Correlation of Fixed Effects:
            (Intr) i.2007 i.2008 i.2009 i.2010
yr2007      -0.696                            
yr2008      -0.710  0.657                     
yr2009      -0.775  0.697  0.714              
yr2010      -0.803  0.720  0.735  0.802       
yr2011      -0.801  0.704  0.722  0.800  0.825
</code></pre>

<p>From <a href=""http://au.mathworks.com/help/stats/fitlme.html"" rel=""nofollow"">here</a> and <a href=""http://au.mathworks.com/help/stats/prepare-data-for-linear-mixed-effects-models.html"" rel=""nofollow"">here</a> I think I've narrowed my options to the following (you'll have to excuse these, they'll be messy but hopefully readable):
$$y_{im} = \beta_0 + \beta_1 year_{im} + b_{0m} +\epsilon_{im} $$
where $i$ is the # of obs., and $m$ is the grouping variable (building.id in this case)</p>

<p>OR</p>

<p>$$ y_{imj} = \beta_0 + \Sigma\beta_{1m}[year]_{im} + b_{0j}[building.id]_j + \epsilon_{imj} $$
where $i$ is the # of obs., $m$ corresponds to year, and $j$ corresponds to building.id.</p>

<p>Are either of these correct? Any help would be hugely appreciated! </p>
"
"0.0369863360727655","0.0383764778226668","171327","<p>I have a study where I am testing for a linear effect of the continuous variable <strong>treatment</strong> (1-10) on the response variable of <strong>fish abundance</strong> repeatedly observed in <strong>lakes</strong> (A-J). My concern is that there is one treatment level per lake and each treatment level only occurs at that one lake. Dummy data is shown below.</p>

<p>Is it valid for me to analyze this data with:</p>

<pre><code>lmer(fish.abundance~treatment+(1|lake),data=dat)
</code></pre>

<p>? </p>

<p>(Averaging values within lakes and fitting a simple linear model is not an option in the real analysis because I will also be accounting for seasonal trends in fish abundance, e.g.,  </p>

<pre><code>gamm(fish.abundance~treatment+s(day.of.year),random=list(lake=~1),data=dat)
</code></pre>

<p>)  </p>

<pre><code>treatment   lake    fish.abundance
1   A   1
1   A   2
1   A   3
2   B   2
2   B   3
2   B   4
3   C   3
3   C   4
3   C   5
4   D   4
4   D   5
4   D   6
5   E   5
5   E   6
5   E   7
6   F   6
6   F   7
6   F   8
7   G   7
7   G   8
7   G   9
8   H   8
8   H   9
8   H   10
9   I   9
9   I   10
9   I   11
10  J   10
10  J   11
10  J   12
</code></pre>
"
"0.0554795041091482","0.0575647167340002","172027","<p>We are dealing with house price estimation model in R. We think using mixed linear model with lme4 package and nlme package. (which package is better to use?)
We have explanotary variables which are commercial density in environment of house, social density in environment of house, quality of inside of house, size, technical equipment (elevator etc.), house type(duplex, house complex etc.),age of house, sunniness (north, south etc.), transportation, floor, prestige(lux brand apartments), garage. We try to do our model like this in R</p>

<pre><code>lmer(price~quality+sunny+size+garage+technic+age+floor+(1|prestige)+(1|house.type)+(1|social)+(1|commercial)+(1|transportation),data=house)
</code></pre>

<p>1)We are not sure about which factors are fixed which factors are random, can you help about this? 
2)social,commercial and transportation are continuous variables can we use them as a random? 
3)Should we normalize Price and size?</p>
"
"0.12266979912335","0.127280377713181","172953","<p>I have to compare two different methods of gene quantification. I have two matrices:</p>

<ul>
<li>the first matrix comes from the first method and has on the column the name of the samples and on the row the name of the gene;</li>
<li>the second matrix come from the second method and has on the column the name of the samples and on the row the name of the gene;</li>
</ul>

<p>Now from these two matrices I have to create one data frame for each gene where in the column I have: first column quantification, second column method, third column sample. The row correspond to the name of the gene. In total, I have to create a data frame for each gene according to these features.</p>

<p>Finally I have to use this data frame to run the linear mixed model to check the variability among samples and methods as here:</p>

<pre><code># Consider different source of variability, i.e., the samples and the methods
library(""lme4"")

# build the data.frame
df &lt;- data.frame(express, method, sample)

# fit the lmm
res &lt;- lmer(express~(1|method)+(1|sample), data=df)

# variance components
var_random_effect &lt;- as.numeric(VarCorr(res))
var_residual &lt;- attr(VarCorr(res),""sc"")^2
</code></pre>

<p>Do you have any idea how to create these data frames and run the linear mixed model? I need to create a data frame for each gene.</p>
"
"0.133697632737248","0.128813931560849","173315","<p>I would appreciate some assistance with setting the statistical analyses of my experiment with fish.</p>

<p>The experiment was the following: Fish larvae of approximately the same weight from four different experimental crosses (WW, WD, DW, DD) were randomly allocated to four experimental tanks; each tank containing an equal representation of WW, WD, DW and DD crosses. Incidentally, two of these tanks happened to have a clock-wise, and the other two anti-clockwise water flows. An acute stressor was applied to two of the tanks (one with clockwise and one with anti-clockwise water flow) whereas the other two were used as controls (again, one of each water flow directions).</p>

<p>My variable of interest was the final weight of fish at the end of the trial and I would like to know whether it differed between crosses or was affected by the stress. However, I would also like to account for the direction of the water flow as a factor if possible. It is also noteworthy, that although equal number of fish from each cross were mixed in each tank, the number of individuals with available weight data from the crosses and tanks varied. This was due to efforts made to reduce the cost involved in matching fish back to its cross of origin. Not all fish that were sampled were assigned to crosses, because the assignment of fish to their crosses was terminated when a sufficient number of fish from each cross and from each tank were identified. Thus the number of fish per cross per tank available for analysis varies from 10 to 22.</p>

<p>I have not much experience with stats or mixed models, however after a bit of reading I have reached the conclusion that the best approach might be using a linear mixed model with:</p>

<ol>
<li>Weight as dependent variable</li>
<li>Stress and crosses as fixed factors</li>
<li>Tank as random factor</li>
<li><p>Water flow as random factor nested within tank</p>

<pre><code>require(lme4)
lmm &lt;- lmer(Weight ~ Stress*Cross + (1|Tank) + (1|WaterFlow/Tank), data = my.data, REML = FALSE)
summary(lmm)
</code></pre></li>
</ol>

<p>I am not 100% sure that I am doing the right thing and I would appreciate your advice in this matter; especially comments/suggestions regarding the following:</p>

<ol>
<li>Identification of fixed and random variables</li>
<li>Validity of the script</li>
<li>Whether there are other nested factors that I could consider (e.g. should I nest the crosses within the tanks? What about nesting the stress within the tanks?)</li>
<li>Would it be preferable to have a more balanced design and exclude some of the data points to limit the number of individuals analysed to 10 in each of the crosses from each of the tanks? Or is it better to have a larger number of data points, even though it leads to less balanced data?</li>
</ol>

<p>Thank you for your time and consideration. Every comments/suggestions you may have are greatly appreciated.</p>
"
"0.138390197576366","0.143591631723548","173813","<p>I have a data obtained through forest inventory conducted yearly (1994-2015) in a West African country. 10 plots of equal sizes (1 ha each) were selected from  unmanaged natural forest and then species of trees and shrubs were identified and counted. Biodiversity indices like Abundance, Shannon, Simpson were calculated. I have chosen only 9 years in which data were collected in all the 10 plots and I discarded the incomplete years and considered ""Year"" as factor. </p>

<p>The data is structured as:</p>

<pre><code>str(BIData)
'data.frame':   90 obs. of  9 variables:
 $ Year          : Factor w/ 9 levels ""1994"",""1995"",..: 1 1 1 1 1 1 1 1 1 1 ...
     $ Plot          : Factor w/ 10 levels ""Bas Kolel"",""Bougou"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Richness      : int  8 21 13 14 8 10 6 10 8 20 ...
     $ Abundance     : int  286 1471 1121 466 242 97 250 790 208 2015 ...
 $ Shannon       : num  1.33 1.79 1.55 1.68 1.44 1.71 1.35 1.27 1.27 1.86 ...
     $ Simpson       : num  0.656 0.71 0.682 0.694 0.665 0.714 0.66 0.647 0.649 0.718 ...
 $ InverseSimpson: num  2.91 3.45 3.14 3.28 2.99 3.52 2.95 2.83 2.86 3.54 ...
     $ Topography    : Factor w/ 3 levels ""Plateau"",""Slope"",..: 3 1 1 3 3 2 2 2 3 1 ...
 $ Land_use      : Factor w/ 2 levels ""Cultivated"",""Pasture"": 1 2 2 2 1 1 2 2 1 2 ...
</code></pre>

<p>In addition, plots are located in different topography (slope, valley, plateau) and land use (cultivated, pasture).</p>

<p>I have the following two models in lmer and lme:</p>

<pre><code>model=lmer(Abundance~Year+Topography+Land_use+(1|Plot), method=""ML"", data=BIData)
model=lme(Abundance~Year+Topography+Land_use, random=~1|Plot, method=""ML"", data=BIData)
</code></pre>

<p>I got totally different results: My questions?</p>

<p>I m not an expert but I found that <code>lme</code> provides a kind of ""beautiful"" results with p-values. I can see many significant factors like years, topography and land use whereas in <code>lmer</code> only t-values without p-values. I don't know which one is correct for my data. In both cases, it shows good and acceptable residuals plots.</p>

<p>Please help me to understand which one is correct to my data.</p>

<hr>

<p>Thank you @fcoppens. No, I did not try that parameter. Here are the output of both <code>lme</code> and <code>lmer</code>.</p>

<p><strong><code>lmer</code></strong></p>

<pre><code>model=lmer(Abundance~Year+Topography+Land_use+(1|Plot), method=""ML"", data=BIData)
summary(model)
Linear mixed model fit by REML ['lmerMod']
Formula: Abundance ~ Year + Topography + Land_use + (1 | Plot)
   Data: BIData

REML criterion at convergence: 1106.2

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.5754 -0.5024 -0.0186  0.4015  3.4341 

Random effects:
 Groups   Name        Variance Std.Dev.
 Plot     (Intercept) 51753    227.5   
 Residual             48592    220.4   
Number of obs: 90, groups:  Plot, 10

Fixed effects:
                 Estimate Std. Error t value
(Intercept)       1073.15     252.41   4.252
Year1995             0.40      98.58   0.004
Year1996           -32.70      98.58  -0.332
Year1998          -198.10      98.58  -2.010
Year1999          -341.90      98.58  -3.468
Year2002          -295.80      98.58  -3.001
Year2004          -324.90      98.58  -3.296
Year2010          -291.60      98.58  -2.958
Year2015          -371.00      98.58  -3.763
TopographySlope   -756.87     206.36  -3.668
TopographyValley  -645.82     236.71  -2.728
Land_usePasture    178.07     200.85   0.887
</code></pre>

<p><strong><code>lme</code></strong></p>

<pre><code>model=lme(Abundance~Year+Topography+Land_use, random=~1|Plot, method=""ML"", data=BIData)
summary(model)
Linear mixed-effects model fit by maximum likelihood
 Data: BIData 
       AIC      BIC    logLik
  1264.675 1299.673 -618.3377

Random effects:
 Formula: ~1 | Plot
        (Intercept) Residual
StdDev:    171.5578 209.1232

Fixed effects: Abundance ~ Year + Topography + Land_use 
                     Value Std.Error DF   t-value p-value
(Intercept)      1073.1495  213.5506 72  5.025271  0.0000
Year1995            0.4000  100.4595 72  0.003982  0.9968
Year1996          -32.7000  100.4595 72 -0.325504  0.7457
Year1998         -198.1000  100.4595 72 -1.971938  0.0525
Year1999         -341.9000  100.4595 72 -3.403360  0.0011
Year2002         -295.8000  100.4595 72 -2.944469  0.0044
Year2004         -324.9000  100.4595 72 -3.234138  0.0018
Year2010         -291.6000  100.4595 72 -2.902661  0.0049
Year2015         -371.0000  100.4595 72 -3.693029  0.0004
TopographySlope  -756.8671  171.7008  6 -4.408058  0.0045
TopographyValley -645.8214  196.9543  6 -3.279041  0.0168
Land_usePasture   178.0654  167.1213  6  1.065486  0.3276
Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.6851599 -0.5159528 -0.0222693  0.4401886  3.6493837 

Number of Observations: 90
Number of Groups: 10 
</code></pre>
"
"0.0739726721455309","0.0767529556453336","173972","<p>Iâ€™m trying to perform a linear mixed effects analyses using the lme4 package on R. I need your help because of a recurrent errorâ€¦ 
I have 17 profiles (that participants had to rate) as repeated measures (theyâ€™ve seen every profil1,2,3 but only one version of them everytime, either 1a or 1b or 1c as in a classic latin squares design). Iâ€™m doing a score difference between profil-a type and profil-b ratings as fixed continuous effects as long as fixed categorical effects â€œovulâ€.  </p>

<p><a href=""http://i.stack.imgur.com/u4DXn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/u4DXn.png"" alt=""Sample""></a></p>

<pre><code>DF$X&lt;-rowMeans(cbind(DF$profil_1a,DF$profil_2a,DF$profil_3a,DF$profil_4a,DF$profil_5a,DF$profil_6a,DF$profil_10a,DF$profil_11a,DF$profil_12a,DF$profil_13a,DF$profil_14a,DF$profil_15a,DF$profil_16a, DF$profil_1b,DF$profil_2b,DF$profil_3b,DF$profil_4b,DF$profil_5b,DF$profil_6b,DF$profil_10b,DF$profil_11b,DF$profil_12b,DF$profil_13b,DF$profil_14b,DF$profil_15b,DF$profil_16b, DF$profil_1c,DF$profil_2c,DF$profil_3c,DF$profil_4c,DF$profil_5c,DF$profil_6c,DF$profil_10c,DF$profil_11c,DF$profil_12c,DF$profil_13c,DF$profil_14c,DF$profil_15c,DF$profil_16c),na.rm=TRUE)
    DF$likertfort&lt;-rowMeans(cbind(DF$profil_1a,DF$profil_2a,DF$profil_3a,DF$profil_4a,DF$profil_5a,DF$profil_6a,DF$profil_10a,DF$profil_11a,DF$profil_12a,DF$profil_13a,DF$profil_14a,DF$profil_15a,DF$profil_16a),na.rm=TRUE)
    DF$likertmoyen&lt;-rowMeans(cbind(DF$profil_1b,DF$profil_2b,DF$profil_3b,DF$profil_4b,DF$profil_5b,DF$profil_6b,DF$profil_10b,DF$profil_11b,DF$profil_12b,DF$profil_13b,DF$profil_14b,DF$profil_15b,DF$profil_16b),na.rm=TRUE)
    DF$Wdif1&lt;-(DF$likertfort-DF$likertmoyen)
        mean(DF$Wdif1, na.rm=TRUE)
    DF$ovul&lt;-   -0.5 *(DF$ovul==""Oui"") + 0.5 *(DF$ovul==""Non"")
</code></pre>

<p>When I insert the scenario (DF$X) and the subject as random variables as in:</p>

<pre><code>DF.model = lmer(DF$Wdif1 ~ DF$ovul +(1|subject) + (1|X), data=DF)
</code></pre>

<p>I got the error: Error: number of levels of each grouping factor must be &lt; number of observations. I understand the model is overparametrized but is this fixable? Or my number of participants (n=38) is just too small to perform such a model? Btw, when I perform without the random parameter ""subject"" it works apparently.
Thanks in advance for all your help,
j</p>
"
"0.106770355439746","0.121861683908065","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.105264957864947","0.10922137064511","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.129452176254679","0.124723552923667","174203","<p><strong>The problem:</strong></p>

<p>I have read in other <a href=""http://stats.stackexchange.com/a/4202/67822"">posts (a bit old)</a> that <code>predict</code> is not available for mixed effects <code>lmer</code> {lme4} models in [R]. <strong>EDIT</strong>: Although I know now, thanks to @EdM, that this exists in more recent versions, the question still is unresolved in terms of the actual algebra from $intercepts$ and $slopes$ <code>-&gt;</code> $predicted$ values.</p>

<p>However, in trying to get a more plastic sense of linear effects, I came across a situation where the <code>predict</code> call did seem to incorporate the random effects in the model, yielding a plausible output. </p>

<p><strong>Background:</strong></p>

<p>I'm working with an extremely massaged toy dataset. For the sake of intellectual honesty, I think I got the idea of the set from a Princeton on-line class, but any similarity to the original is at this point coincidental. In unsuccessfully looking for the original, I did come across <a href=""http://data.princeton.edu/wws509/datasets/"" rel=""nofollow"">this</a> that can serve as indirect credit and much more.</p>

<p>If anybody is so inclined to take a look directly it can be retrieved directly:</p>

<pre><code>require(gsheet)
data &lt;- read.csv(text = 
     gsheet2text('https://docs.google.com/spreadsheets/d/1QgtDcGJebyfW7TJsB8n6rAmsyAnlz1xkT3RuPFICTdk/edit?usp=sharing',
        format ='csv'))
head(data)

  Subject Auditorium Education Time  Emotion Caffeine Recall
1     Jim          A        HS    0 Negative       95 125.80
2     Jim          A        HS    0  Neutral       86 123.60
3     Jim          A        HS    0 Positive      180 204.00
</code></pre>

<p>So we have some repeated observations (<code>Time</code>) of a continuous measurement of the <code>Recall</code> rate of some words, and several $\small DV$'s, including random effects (<code>Auditorium</code> where the test took place; <code>Subject</code> name); and explanatory or fixed effects, such as <code>Education</code>, <code>Emotion</code> (the emotional connotation of the word to remember), or $\small mgs.$ of <code>Caffeine</code> ingested prior to the test.</p>

<p>The idea is that it's easy to remember for hyper-caffeinated wired subjects, but the ability decreases over time, perhaps due to tiredness. Words with negative connotation are more difficult to remember. Education has an effect that is intuitive, and ""surprisingly"" the Auditorium also plays a role (perhaps one was more noisy, or less comfortable). Here're a couple of exploratory plots:</p>

<p><img src=""http://i.stack.imgur.com/17TBp.png"" width=""400"" height=""500""></p>

<p>I know... so cool... Have you noticed that Tina doesn't drink coffee? How can anybody spend the night on CV, and be able to function in the morning without coffee?</p>

<p><a href=""http://i.stack.imgur.com/R1Jli.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/R1Jli.png"" alt=""enter image description here""></a></p>

<p><strong>The issue:</strong></p>

<p>When fitting lines on the data cloud for the call:</p>

<p><code>fit1 &lt;- lmer(Recall ~ (1|Subject) + Caffeine, data = data)</code></p>

<p>I get this plot:</p>

<p><img src=""http://i.stack.imgur.com/mFMN2.png"" width=""600"" height=""500""></p>

<p>with the following code (notice the call for $\small predict(fit1)$) in it:</p>

<pre><code>library(ggplot2)
p &lt;- ggplot(data, aes(x = Caffeine, y = Recall, colour = Subject)) +
  geom_point(size=3) +
  geom_line(aes(y = predict(fit1)),size=1) 
print(p)
</code></pre>

<p>while the following model:</p>

<p><code>fit2 &lt;- lmer(Recall ~ (1|Subject/Time) + Caffeine, data = data)</code></p>

<p>incorporating <code>Time</code> and a parallel code gets a surprising plot:</p>

<pre><code>p &lt;- ggplot(data, aes(x = Caffeine, y = Recall, colour = Subject)) +
  geom_point(size=3) +
  geom_line(aes(y = predict(fit2)),size=1) 
print(p)
</code></pre>

<p><img src=""http://i.stack.imgur.com/zZvX5.png"" width=""600"" height=""500""></p>

<p><strong>The question:</strong></p>

<p><strong>How does the <code>predict</code> function operate in this <code>lmer</code> model?</strong> Evidently it's taking into consideration the <code>Time</code> variable, resulting in a much tighter fit, and the zig-zagging that is trying to display this third dimension of <code>Time</code> portrayed in the first plot.</p>

<p>If I call <code>predict(fit2)</code> I get <code>132.45609</code> for the first entry, which corresponds to the first point: <code>Subject = Jim</code>, <code>Auditorium = A</code>, <code>Education = HS</code>, <code>Time = 0</code>, <code>Emotion = Negative</code>, <code>Caffeine = 95</code> and <code>Recall = 125.8</code></p>

<p>The coefficients for <code>fit2</code> are:</p>

<pre><code>$`Time:Subject`
         (Intercept)  Caffeine
0:Jason     75.03040 0.2116271
0:Jim       94.96442 0.2116271
0:Ron       58.72037 0.2116271
0:Tina      70.81225 0.2116271
0:Victor    86.31101 0.2116271
1:Jason     59.85016 0.2116271
1:Jim       52.65793 0.2116271
1:Ron       57.48987 0.2116271
1:Tina      68.43393 0.2116271
1:Victor    79.18386 0.2116271
2:Jason     43.71483 0.2116271
2:Jim       42.08250 0.2116271
2:Ron       58.44521 0.2116271
2:Tina      44.73748 0.2116271
2:Victor    36.33979 0.2116271

$Subject
       (Intercept)  Caffeine
Jason     30.40435 0.2116271
Jim       79.30537 0.2116271
Ron       13.06175 0.2116271
Tina      54.12216 0.2116271
Victor   132.69770 0.2116271
</code></pre>

<p>My best bet was <code>94.96442 + 0.2116271 * 95 = 115.0689945</code>... Not too far off, but wrong. <strong>What is the formula to get instead to <code>132.45609</code>?</strong></p>
"
"0.11744739098372","0.121861683908065","174532","<p>I have been working on my PC to analyse my multilevel data. I am now working on a Mac and have run the same model. Some of the output is the same but some is quite different. I can't seem to work out why. Here is the model:</p>

<pre><code>&gt; loss.2 &lt;- glmer.nb(Loss_across.Chain ~ Posn.c*Valence.c + (Valence.c|mood.c/Chain), data = FinalData_forpoisson, control = glmerControl(optimizer = ""bobyqa"", check.conv.grad = .makeCC(""warning"", 0.05)))
</code></pre>

<p>On the PC I got this output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 8.782e-15 9.371e-08     
              Valence.c   9.608e-15 9.802e-08 0.48
 mood.c       (Intercept) 0.000e+00 0.000e+00     
              Valence.c   1.654e-14 1.286e-07  NaN
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -0.19255    0.04794  -4.016 5.92e-05 ***
Posn.c           -0.61011    0.04122 -14.800  &lt; 2e-16 ***
Valence.c        -0.27372    0.09589  -2.855  0.00431 ** 
Posn.c:Valence.c  0.38043    0.08245   4.614 3.95e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>On the Mac I got this output:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 1.242e-13 3.524e-07     
              Valence.c   4.724e-13 6.873e-07 0.98
 mood.c       (Intercept) 7.998e-16 2.828e-08     
              Valence.c   3.217e-14 1.793e-07 1.00
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       2.947e-05  4.794e-02   0.001    1.000
Posn.c            7.441e-05  4.122e-02   0.002    0.999
Valence.c        -4.011e-05  9.589e-02   0.000    1.000
Posn.c:Valence.c -6.672e-05  8.245e-02  -0.001    0.999

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>Does anyone know why the output might be different across the two platforms and how I might be able to get them to align?</p>
"
"0.128124426527695","0.121861683908065","174992","<p>I am about to describe the Statistical Analysis methodology that I have used to produce my results.</p>

<p>I used linear mixed models (lme4) and least square means (lmerTest) to present the results.</p>

<p>In particular since I had an unbalance dataset with repeated measures I fitted first a model of the type:</p>

<pre><code>Response ~ Fixed1 + Fixed2 + Fixed3 + (1|Random)
Response2 ~ Fixed1 + Fixed2 + Fixed3 + (1|Random)
Response3 ~ Fixed1 + Fixed2 + Fixed3 + (1|Random)
</code></pre>

<p>and then I computed the population means for the fixed effect (Fixed1).</p>

<blockquote>
  <p><strong>Statistical Analysis</strong></p>
  
  <p>To study whether and to what extent Responses are different between the
  groups contained in Fixed1 linear mixed-effect models (LMMs) were
  constructed and backward elimination of non-significant effects
  performed. Elimination of the fixed part is done by the principle of
  marginality for which the highest order interactions are tested first
  and, if they are significant, the lower order effects are not tested
  for significance. The p-values for the fixed effects are calculated
  from F test based on Sattethwaiteâ€™s approximation, p-values for the
  random effects are based on likelihood ratio test. We present the
  least-square mean [<strong>IS A REF NECESSARY?</strong>] (LS-means) and LS-means differences of the
  considered outcome variables for the significant fixed factors of the
  models together with their p-values and standard error of measurement
  (SE). Level of significance is set at p&lt;0.05. Analyses were carried
  out using MATLAB R2015a (The MathWorks, Inc., Natick, Massachusetts,
  United States) and R (R Core Team, 2012) softwares.</p>
</blockquote>

<p>My results are figure showing the population means and significant differences between the groups of Fixed1.</p>

<p>Is the paragraph correct/clear? Can I use linear mixed effect models just to compute ls means? Is it the right way to describe my statistical analysis?</p>

<p>Thanks</p>
"
"0.128505183463769","0.133335086600437","175444","<p>I'm wondering if a treatment has an effect on my mite population. Therefore I've got a dataset with repeated measurements, some data is missing.</p>

<p>data:</p>

<pre><code>ID   Treatment    Mites   Time    Location    StartPopulation    X1bib
ID1  Control      7       t1      Loc1        5                  10000
ID1  Control      8       t2      Loc1        5                  10000
ID1  Control      10      t3      Loc1        5                  10000
ID1  Control      11      t7      Loc1        5                  10000
ID2  Control      12      t1      Loc2        11                 13000
ID2  Control              t2      Loc2        11                 13000
ID2  Control      14      t3      Loc2        11                 13000
ID3  Treatment    20      t1      Loc1        20                 12000
ID3  Treatment    22      t2      Loc1        20                 12000
ID3  Treatment            t3      Loc1        20                 12000
ID4  Treatment    20      t1      Loc11       18                 11500
and so on..
totally: 110 IDs; 7 different measurements (Time)
</code></pre>

<p>variables:</p>

<pre><code>ID:              factor, unique ID for each population
Treatment:       factor (""Treatment"" or ""Control"")
Mites:           numeric, the variable I'm interested in
Time:            factor with total 7 levels
Location:        factor with total 11 levels
StartPopulation: numeric (mean of Mites for t=-3, -2, -1 -&gt; before Treatment started)
X1bib:           numeric
</code></pre>

<p>I'm interested if my <code>Treatment</code> changed the <code>Mites</code> count - and if yes if there's an increase in it's effect over time. <code>StartPopulation</code> sure had an influence on <code>Mites</code>, <code>otherFactor</code> and <code>Location</code> could've had also.</p>

<p>As I use a mixed model I'd like to use <code>glmer</code> in <code>R</code>. My syntax looks like this:
(changed it, thank you for your answers so far)</p>

<pre><code>PPP &lt;- glmer(Mites ~ Treatment * Time + StartPopulation + X1bib + (1|ID) + (1|Location), data=vat_database, family=poisson)
</code></pre>

<p>which outputs:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Mites ~ Treatment * Time + StartPopulation + X1bib + (1 | ID) +      (1 | Location)
   Data: vat_database
     AIC      BIC   logLik deviance df.resid 
     Inf      Inf     -Inf      Inf      349 
Random effects:
 Groups   Name        Std.Dev.
 ID       (Intercept) 1       
 Location (Intercept) 1       
Number of obs: 367, groups:  ID, 78; Location, 9
Fixed Effects:
                  (Intercept)             TreatmentTreatment                     Timevmf_A2                     Timevmf_A3                     Timevmf_K1                     Timevmf_K2  
                    2.418e-01                      5.342e-01                      3.252e-01                      5.389e-01                      5.725e-01                      1.102e+00  
                   Timevmf_K3                     Timevmf_K4                StartPopulation                          X1bib  TreatmentTreatment:Timevmf_A2  TreatmentTreatment:Timevmf_A3  
                    1.079e+00                      7.893e-01                      1.486e-01                     -1.331e-06                     -4.664e-01                     -5.453e-01  
TreatmentTreatment:Timevmf_K1  TreatmentTreatment:Timevmf_K2  TreatmentTreatment:Timevmf_K3  TreatmentTreatment:Timevmf_K4  
                   -4.513e-01                     -5.476e-01                     -4.477e-01                     -6.858e-01  
fit warnings:
Some predictor variables are on very different scales: consider rescaling
convergence code 0; 1 optimizer warnings; 81500 lme4 warnings
</code></pre>

<p>Am I right considering that on <code>Time=""vmf_K1""</code> my <code>Treatment</code> Mite Population was -4.513e-01 smaller than my <code>Control</code> Mite Population? How about significances?</p>
"
"0.104613156193188","0.108545070825851","175652","<p>I am familiar with fixed-effects linear regression models, and have done reading on mixed-effects models.</p>

<p>I am attempting to fit a model based on observational data, where treatments come at varying times and do not exist at all for a majority of subjects.</p>

<p>I am interested in whether or not the treatment has an effect on the trajectory of a subject's response over time. Graphically:</p>

<p><img src=""http://i.imgur.com/szfwOL6.png"" alt=""Varying treatment time mixed model""></p>

<p>The most relevant analogous model I have found would be the one specified <a href=""http://www.ats.ucla.edu/stat/seminars/mlm_longitudinal/"" rel=""nofollow"">here</a>, specifically Part 3. However, this example does not use R. I have read through all of Bates' lme4 paper, but I am still uncertain how to specify this effect.</p>

<p>An excerpt of my data:</p>

<pre><code>     ID RESPONSE ID.CONST.1 ID.VAR.1 ID.VAR.2 TREATMENT_ACTIVE RESPONSE.TIME
1077415        7         41        0        5            FALSE           314
1077415        8         41        1        6            TRUE            316
1077415        9         41        10       7            TRUE            319
1077688        1         59        0        1            FALSE           313
1079475        1         85        0        1            FALSE           313
1080811        1         24        0        1            FALSE           314
1081156        1        502        0        1            FALSE           314
1082437        1         50        0        0            FALSE           315
1083154        1        257        0        0            FALSE           315
1083154        2        257        0        0            TRUE            316
1083527        1         69        0        0            FALSE           315
1086283        1         31        0        0            FALSE           316
1088810        1        120        2        1            FALSE           317
1090019        1         93        2        1            TRUE            317
1091048        1         27        0        0            FALSE           317
1091114        1         62        0        1            FALSE           317
</code></pre>

<p>Each subject (<code>ID</code>) has time-varying measurements (<code>ID.VAR.X</code>), constant measurements (<code>ID.CONST.X</code>), as well as the time of observation (<code>RESPONSE.TIME</code>). <code>TREATMENT_ACTIVE</code> indicates whether or not the treatment is active for a given subject at the corresponding <code>RESPONSE.TIME</code>. Some subjects have a single observation, others have multiple observations, and treatment times are rarely the same between subjects.</p>

<p>I've attempted to fit models as:</p>

<pre><code>lmer(RESPONSE ~ ID.CONST.1 + ID.VAR.1 + ID.VAR.2 + TREATMENT_ACTIVE + RESPONSE.TIME + (1|ID) + (1|RESPONSE.TIME)
lmer(RESPONSE ~ ID.CONST.1 + ID.VAR.1 + ID.VAR.2 + RESPONSE.TIME + (1|ID) + (1+TREATMENT_ACTIVE|RESPONSE.TIME)
</code></pre>

<p>However, I'm fairly certain this is misspecified. I am not sure how to specify the random effects to ensure that the <code>TREATMENT_ACTIVE</code> variable is interpreted as I intend. I am interested in testing both an intercept-only model as well as a intercept+slope model for the treatment effect.</p>
"
"0.0905976508333704","0.0940027887907685","176294","<p>I have tried to read documentation with no luck. </p>

<p>Let suppose i have a hundred mice and 50 of those mice has a condition X. I am studying the deliveries of the animals and want to know if an incidence of the event Y is more common among the animals with the condition X during the delivery. Some individuals of the animals has only one delivery but some of the animals can have more than one delivery. I want also standardise other factors(nominal and linear). </p>

<p>I have the following variables: 
ID (Same ID can occur on several rows if there is many deliveries), 
Condition.X, 
Color, 
Age, 
Weight, 
Event.Y(0/1)</p>

<p>I am using the following R-code:</p>

<blockquote>
  <p>model &lt;- glmer(Event.Y ~ Condition.X + Color + Age + Weight + (1 | ID), family = binomial)
  nullmodel &lt;- glmer(Event.Y ~  Color + Age + Weight + (1 | ID), family = binomial)
  anova(nullmodel, model)</p>
</blockquote>

<p>If I got it right the method for approximation above is Laplace. </p>

<p>I have also analysed data with SAS and by using RSPL or MMPL methods the significance is better. </p>

<p><strong>My question:</strong> Is it possible to use similar methods to RSPL/MMPL with glmer-function in R? </p>
"
"0.108735155238574","0.133335086600437","176766","<p>I'm running an analysis in R for a mixed model (using <code>lmer</code>).</p>

<pre><code>go&lt;-lmer(dN~treatment*time+(1|replicate), data=crab, REML=FALSE)
</code></pre>

<p>I have used AIC model selection to identify a top model set. Firstly I standardised input variables to a mean of zero and SD of 2, and then dredged the models to produce a candidate model list, and then reduced this list to a top model set using a deltaAIC cut off of >7. </p>

<p>My top model set includes 3 models (the interaction model, the model with two fixed effects, and a model with one fixed effect). I model-averaged across these models to produce coefficient estimates for each variable that I can report.</p>

<p>However, whilst I understand it is important to use standardised models to allow fair comparison for variables when averaging across models, I would also like to back-transform the coefficient/parameter estimates as the standardised values are not directly understandable when comparing with Figures showing the linear relationships i.e. I want the true slope values.</p>

<p>I have three questions:</p>

<p>(1) Presumably the values I need to back-transform (using inverse logit?) are the model-averaged coefficient estimates? (Or is the back-transformation carried out at an earlier stage?)</p>

<p>(2) What is the code and how do I back-transform these values? </p>

<p>(3) Now that I have both standardised and true coefficient estimates, how to report them?! Is it neccessary / good practice to report the standardised values given that model averaging was used, or can I just report the back-transformed values? </p>

<p>Below, if it is helpful, is my data and code.</p>

<pre><code>   time    treatment dN      replicate
    3.0       ice 11.6669         1
    3.0       ice 12.1120         2
    3.0       ice 11.3132         3
    3.0       ice 11.6912         4
    3.0       air 11.6373         5
    3.0       air 12.4235         6
    3.0       air 11.6117         7
    3.0       air 12.5151         8
   56.5       ice 11.5028         1
   56.5       ice 12.1031         2
   56.5       ice 11.2783         3
   56.5       ice 11.8608         4
   56.5       air 12.3022         5
   56.5       air 13.4229         6
   56.5       air 12.9646         7
   56.5       air 13.0974         8
  120.0       ice 11.6082         1
  120.0       ice 12.0306         2
  120.0       ice 11.2716         3
  120.0       ice 11.8967         4
  120.0       air 12.4769         5
  120.0       air 13.3197         6
  120.0       air 12.6280         7
  120.0       air 12.7871         8

library(lme4)
library(MuMIn)
library(arm)

stdz.model&lt;-standardize(go, standardize.y=FALSE)
model.set&lt;-dredge(stdz.model)
top.models&lt;-get.models(model.set, subset=delta &lt; 7)
avs&lt;-model.avg(top.models)
summary(avs)
</code></pre>

<p>Below is my coefficients output - I presume these are standardised, but I would like them to be ""true"" values instead:</p>

<pre><code>Model-averaged coefficients: 
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         12.1467     0.1146 106.015  &lt; 2e-16 ***
c.treatment         -0.9042     0.2292   3.946 7.94e-05 ***
z.time               0.3053     0.1176   2.597  0.00942 ** 
c.treatment:z.time  -0.5995     0.2235   2.682  0.00732 ** 
</code></pre>
"
"0.161219701233018","0.167279188638034","178152","<p>I want to fit diurnal cortisol profiles using linear mixed models, as was done by previous researchers (e.g. <a href=""http://www.sciencedirect.com/science/article/pii/S0306453005000491"" rel=""nofollow"" title=""Estimating between- and within-individual variation in cortisol levels using multilevel models"">Estimating between- and within-individual variation in cortisol levels using multilevel models</a>). </p>

<p>I have 4 samples of each individual for 1 to 3 days and the exact time of taking. I am especially interested in the individual intercepts and slopes of these profiles (i.e. the effect of time), which I NEED for other analyses, as intercept and slope are indicators of different aspects of stress regulation, theoretically.
Because cortisol is not normally distributed most researches use the natural logarithm before estimation. But when I look at the actual distribution, it looks like a poisson distribution (after rounding up). However, after fitting the model I checked for overdispersion, and unfortunately it is there. Thus, a negative binomial distribution might be more adequate.</p>

<p>So for sake of comparison I utilized all three models using the lme4 package.</p>

<p>For the log model i used</p>

<pre><code>lmer(log(cortisol) ~ time + (time|id) + (1|day/id), data=data)
</code></pre>

<p>This model gives me an unconditional RÂ² of 0.48 and a conditional RÂ² of 0.57. Unfortunately, the random effects are perfectly negative correlated, which is probably due to the low variance of random effects. From a statistical perspective a random intercept random slope model seems not appropriate. 
I also tried setting the random effects being independent of each other by using (zeit||id) instead, but this just gives me no variation for the random intercept.</p>

<p>For the second model i used</p>

<pre><code> glmer(round(cortisol, digits=0) ~ time + (time|id) + (1|day/id), data=data, family = poisson(link=log), control=glmerControl(optimizer=""bobyqa"")
</code></pre>

<p>This model gives me a better unconditional RÂ² of 0.56 and a much better conditional RÂ² of 0.94. Also, the correlation between random effects is -.44, which is what I would expect, and would also would like it to be around. However, I have the big problem of overdispersion (or do I???), which can be accounted for by defining an individual-level random effect as is suggested <a href=""http://r.789695.n4.nabble.com/Mixed-effects-model-for-overdispersed-count-data-td3010455.html"" rel=""nofollow"">here</a>. But this will result in the model been similar to first model; almost no variance of random effects and extreme correlation.</p>

<p>And lastly for the negative binomial model is used</p>

<pre><code>glmer.nb(round(cortisol, digits=0) ~ time + (time|id) + (1|day/id), data=data, control=glmerControl(optimizer=""bobyqa"")
</code></pre>

<p>This model gives me the worst unconditional RÂ² of 0.44 and a conditional RÂ² of 0.48. Also, the variance of the random intercept is zero, therefore correlation between random effects is not to be calculated.</p>

<p>So my questions are</p>

<ol>
<li><p>How wrong are the predictions in the second model (poisson distribution) under the consideration of overdispersion? How bad is overdispersion? [for me it is the best model in terms of variance of random effects, expected correlation of random effects and model fit]</p></li>
<li><p>How could I force random effects to have a considerable variance in the first model (and not being totally correlated)?</p></li>
</ol>

<p>Thank you for your help, and please let me know if I can provide additional information. </p>
"
"0.114597970917641","0.118905167594629","178551","<p>I'm using a binomial glmer mixed effects model using and I have two questions. </p>

<ol>
<li>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</li>
</ol>

<p>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")</p>

<blockquote>
  <p>summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + stimulus + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5154.3   5259.5  -2562.2   5124.3     8193 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-25.0764  -0.2706  -0.1939   0.2472  10.5131 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.743    1.32<br>
Number of obs: 8208, groups:  Listener, 228</p>

<p>Fixed effects:
              Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)     2.7561     0.2657  10.371  &lt; 2e-16 <strong>*
ListgpTA        0.1741     0.3147   0.553 0.580128<br>
ListgpTQ        0.0810     0.2575   0.315 0.753094<br>
stimulushaaDD  -5.4415     0.2071 -26.272  &lt; 2e-16 <em></strong>
stimulushad    -4.2953     0.1822 -23.569  &lt; 2e-16 <strong></em>
stimulushaDD   -5.4946     0.2086 -26.337  &lt; 2e-16 <em></strong>
stimulushid    -5.1519     0.1994 -25.832  &lt; 2e-16 <strong></em>
stimulushiDD   -0.6708     0.1801  -3.724 0.000196 <em></strong>
stimulushiid   -5.8124     0.2186 -26.593  &lt; 2e-16 <strong></em>
stimulushiiDD  -5.5101     0.2091 -26.353  &lt; 2e-16 <em></strong>
stimulushud    -0.2016     0.1915  -1.053 0.292345<br>
stimulushuDD   -5.6188     0.2123 -26.462  &lt; 2e-16 <strong></em>
stimulushuud   -5.6107     0.2121 -26.450  &lt; 2e-16 *</strong></p>

<h2>stimulushuuDD  -5.3207     0.2038 -26.109  &lt; 2e-16 ***</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
              (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD
ListgpTA      -0.613<br>
ListgpTQ      -0.755  0.636<br>
stimulushaaDD -0.394 -0.007  0.004<br>
stimulushad   -0.440 -0.006  0.005  0.605<br>
stimulushaDD  -0.392 -0.007  0.003  0.555         0.601<br>
stimulushid   -0.407 -0.007  0.004  0.572         0.624       0.569<br>
stimulushiDD  -0.414  0.000  0.001  0.534         0.606       0.530<br>
stimulushiid  -0.376 -0.006  0.003  0.536         0.578       0.533<br>
stimulushiiDD -0.391 -0.007  0.003  0.554         0.600       0.551<br>
stimulushud   -0.386  0.000  0.000  0.497         0.564       0.493<br>
stimulushuDD  -0.385 -0.007  0.003  0.548         0.592       0.545<br>
stimulushuud  -0.386 -0.007  0.003  0.548         0.593       0.545<br>
stimulushuuDD -0.400 -0.007  0.004  0.564         0.613       0.561<br>
              stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD   0.554<br>
stimulushiid   0.549       0.506<br>
stimulushiiDD  0.568       0.529        0.533<br>
stimulushud    0.516       0.569        0.471        0.492<br>
stimulushuDD   0.562       0.521        0.527        0.544         0.484<br>
stimulushuud   0.562       0.522        0.528        0.545         0.485<br>
stimulushuuDD  0.579       0.543        0.542        0.560         0.505<br>
              stimulushuDD stimulushuud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD<br>
stimulushiid<br>
stimulushiiDD<br>
stimulushud<br>
stimulushuDD<br>
stimulushuud   0.539<br>
stimulushuuDD  0.554        0.554 </p>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<blockquote>
  <p>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"")
  summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5218.3   5253.4  -2604.2   5208.3     8203 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-21.9276  -0.2804  -0.2059   0.2740   9.4275 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.676    1.294<br>
 stimulus (Intercept) 4.949    2.225<br>
Number of obs: 8208, groups:  Listener, 228; stimulus, 12</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)  -1.3754     0.6792  -2.025   0.0429 *
ListgpTA      0.2284     0.3073   0.743   0.4572  </p>

<h2>ListgpTQ      0.1432     0.2513   0.570   0.5687</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
         (Intr) LstgTA
ListgpTA -0.235<br>
ListgpTQ -0.288  0.636</p>

<blockquote>
  <p></p>
</blockquote>

<p>Appreciating your help,</p>

<p>Shad</p>
"
"0.120205592236488","0.124723552923667","178682","<p>I'm using a binomial glmer mixed effects model using and I have two questions.</p>

<p>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")

summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + stimulus + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5154.3 5259.5 -2562.2 5124.3 8193

Scaled residuals: Min 1Q Median 3Q Max -25.0764 -0.2706 -0.1939 0.2472 10.5131

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.743 1.32
Number of obs: 8208, groups: Listener, 228

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) 2.7561 0.2657 10.371 &lt; 2e-16 * ListgpTA 0.1741 0.3147 0.553 0.580128
ListgpTQ 0.0810 0.2575 0.315 0.753094
stimulushaaDD -5.4415 0.2071 -26.272 &lt; 2e-16 stimulushad -4.2953 0.1822 -23.569 &lt; 2e-16 stimulushaDD -5.4946 0.2086 -26.337 &lt; 2e-16 stimulushid -5.1519 0.1994 -25.832 &lt; 2e-16 stimulushiDD -0.6708 0.1801 -3.724 0.000196 stimulushiid -5.8124 0.2186 -26.593 &lt; 2e-16 stimulushiiDD -5.5101 0.2091 -26.353 &lt; 2e-16 stimulushud -0.2016 0.1915 -1.053 0.292345
stimulushuDD -5.6188 0.2123 -26.462 &lt; 2e-16 stimulushuud -5.6107 0.2121 -26.450 &lt; 2e-16 *

stimulushuuDD -5.3207 0.2038 -26.109 &lt; 2e-16 ***

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD ListgpTA -0.613
ListgpTQ -0.755 0.636
stimulushaaDD -0.394 -0.007 0.004
stimulushad -0.440 -0.006 0.005 0.605
stimulushaDD -0.392 -0.007 0.003 0.555 0.601
stimulushid -0.407 -0.007 0.004 0.572 0.624 0.569
stimulushiDD -0.414 0.000 0.001 0.534 0.606 0.530
stimulushiid -0.376 -0.006 0.003 0.536 0.578 0.533
stimulushiiDD -0.391 -0.007 0.003 0.554 0.600 0.551
stimulushud -0.386 0.000 0.000 0.497 0.564 0.493
stimulushuDD -0.385 -0.007 0.003 0.548 0.592 0.545
stimulushuud -0.386 -0.007 0.003 0.548 0.593 0.545
stimulushuuDD -0.400 -0.007 0.004 0.564 0.613 0.561
stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD 0.554
stimulushiid 0.549 0.506
stimulushiiDD 0.568 0.529 0.533
stimulushud 0.516 0.569 0.471 0.492
stimulushuDD 0.562 0.521 0.527 0.544 0.484
stimulushuud 0.562 0.522 0.528 0.545 0.485
stimulushuuDD 0.579 0.543 0.542 0.560 0.505
stimulushuDD stimulushuud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD
stimulushiid
stimulushiiDD
stimulushud
stimulushuDD
stimulushuud 0.539
stimulushuuDD 0.554 0.554
</code></pre>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"") summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5218.3 5253.4 -2604.2 5208.3 8203

Scaled residuals: Min 1Q Median 3Q Max -21.9276 -0.2804 -0.2059 0.2740 9.4275

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.676 1.294
stimulus (Intercept) 4.949 2.225
Number of obs: 8208, groups: Listener, 228; stimulus, 12

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.3754 0.6792 -2.025 0.0429 * ListgpTA 0.2284 0.3073 0.743 0.4572

ListgpTQ 0.1432 0.2513 0.570 0.5687

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA ListgpTA -0.235
ListgpTQ -0.288 0.636
</code></pre>
"
"0.0739726721455309","0.0767529556453336","180055","<p>I was wondering whether it is possible to combine a cumulative link mixed model (clmm2) with the linear mixed model (lme4/nlme) when running a mediation (package mediation). I have been working with linear mixed models and mediation for a while, however now I am working with an ordinal variable (A) and two continuous variables (B, C). ID is the random variable for the same patients. My ordinal variable (A) has 4 levels.</p>

<p>Here comes the formulas that I have prepared:</p>

<pre><code>outcome &lt;- clmm2(A ~ B + C + (1|ID), data=test, Hess=TRUE, nAGQ=10)
mediator &lt;- lmer(B ~ C + (1|ID), data=test)
total &lt;- mediate(mediator, outcome, treat = ""C"", mediator = ""B"", sims=10000)
</code></pre>

<p>Although I get the models from the two first formulas, when I run the mediation command I get the following error:</p>

<pre><code>Error in formula.default(object, env = baseenv()) : invalid formula
</code></pre>

<p>Any help is welcome..</p>

<p>Kostas </p>
"
"0.129452176254679","0.143911791835001","180288","<p>I am trying to understand the effect of a covariate (COVAR) in a linear mixed effects model with 2 categorical IVs (IV1, IV2). In order to illustrate where I am struggling, I had to paste the rather long <code>dput()</code> here:</p>

<pre><code>df &lt;- structure(list(ID=c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L),
IV1=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L),.Label=c(""412A"",""415D"",""512A"",""515A"",""615A""),class=""factor""),
IV2=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L),.Label=c(""24"",""27"",""2403""),class=""factor""),
DV=c(NA,NA,NA,17,19,27,14,21,21,31,34,NA,22,29,32,16,18,NA,NA,NA,39,33,27.5,28,27,NA,18,NA,24,38,27,15,NA,NA,22,27,17,52,NA,19,35,37,38,30,29,44,74,60,31,54,66,61,60,35,49,NA,52,53,30,36.5,46,57,54,59,NA,41,45,53.5,39,48,43,58.5,50,31,46,23,46,44,25,51,49,32.5,51,37,53,34,52,56,50.5,10,33,31,35,39,27,22,36,21,39,26,35,24,NA,28,39,28,35,21,39,34,30,NA,25,13,NA,31,28,29,32,NA,21,18,32,34,33.5,55,46,26.5,57,29,37,NA,23,52,31,32,41,25,29.5,47,37.5,30,NA,NA,NA,NA,43,43,43,29,42,31,NA,36,16,55,11,30,50,49,38,33,42,45,43.5,35,28,NA,44,36.5,34,41,35,17,38.5,24,49,42,40.5,37.5,15,37.5,32,30,44,25,38,39.5,37.5,43,25,28.5,26,32,43,NA,19,35,19,40.5,33,13,39,39,32,39,44,7,39,40,16,35,52,33,NA,54,24,52,37,31,27,24,31,18,50,16,31,NA,43,NA,42,39,NA,NA,51,36,38,28,NA,30,27,30,31,31,19,NA,38,35,38,21,29,31,8,32,19,23,18,NA,22,30,31,44,31,14,NA,28,25,34,32,39,30,27,33,44,47,16,46.5,12,24,17,40,29,21,47,6,19.5,39,32,28,43,51,42,44,36,48,37,32,37,43,41,10,5,37,28,10,35,45.5,51,22,35,38,39,45,44,46,24,41,37.5,30,NA,33,21,24,NA,25,27,18,NA,22,42,19,30,31,36,19,18,42,25,12,30,32,36.5,27,36,39,37,36,43,35,30.5,11,36,15,43,37,38,23,34,NA,14,39,35,42,38,45,31,41,37,36,37,33,12,44,42,45,39.5,36,44.5,38,14,14,36.5,36,32,43,39,35,38,51,43,48,35,25,49,46,26,46,51.5,35,45.5,NA,53,38.5,45,53,34,51,31,13,36,NA,32,37,43,43,19,35.5,45,41,28,42,44,43,44,34,30,46,43,45,37,33.5,47,23,19,36,38.5,26,41,NA,34,35.5,25,11,38,34,47,9,47,16,20,31,9,9,35,32,NA,34.5,31,NA,32,39,NA,NA,NA,NA,32,26,10,11,NA,37,44,25,15,37,25,10,NA,15,32,NA,24,27,NA,25,31,23,41.5,27,40,31,32,11,NA,14,25,29,36,37,31.5,37,27,21,NA,27,38,NA,NA,25,23,25,40,NA,47,35,33,39,35,38,43,27,35.5,33,28,NA,40,30,48,39,11,35,42.5,42.5,42,42,38,48,46,41,NA,32.5,43.5,34,29,35,NA,38,NA,NA,31,36,31,28.5,15,25,34,30,36,26,35,39,19,NA,NA,31,22,NA,NA,35,35,15,23,38.5,38,NA,36,16,18,26,30,28,NA,25,27,26,25,5,41,29,37,28,34,43,38,29,45,NA,41,32,37,50,31,NA,35,40,41,36,25,34,38,32,38,42,33,34,39,34,39,31,46,8,NA,36,48,25,32,37,NA,40,32,17,37,29,NA,37.5,NA,38,39,NA,44,48,40,NA,20,NA,36.5,20,33,31,41,32.5,28,43,39,29,23,37,32,39,26,36,15,37,31,11,38,29,42,38.5,32,30,37,38,32,33),
COVAR=c(5.2,5.2,5.87,5.68,5.49,7.67,6.3,8.34,7.01,5.51,5.8,4.35,3.95,5.23,6.32,4.01,3.16,3.61,4.67,3.44,5.27,4.59,4.18,4.64,3.97,4.11,3.68,7.57,3.97,5.9,6.02,4.79,5.14,5.84,7.61,4.99,4.18,7.25,3.92,6.3,6.04,5.02,8.01,4.14,8.24,6.21,7.44,5.69,6.31,5.9,6.7,4.96,5.08,4.93,6.4,7.2,7.38,9.59,6.37,8.24,5.6,5.87,4.99,3.64,3.44,5.72,4.52,6.5,4.78,5.18,5.92,8.79,7.65,4.5,4.3,5.76,8.53,4.38,4.46,8.7,8.26,8.89,5.85,6.98,6.65,7.27,8.92,7.43,5.91,5.49,7.64,7.15,6.8,5.74,4.63,4.62,7.02,5.43,9.59,5.42,6.13,8.9,4.66,6.87,6.83,8.38,8.96,5.25,5.54,6.95,8.03,4.33,7.76,6.35,4.99,7.41,6.13,4.67,4.1,4.51,4.6,3.71,6.72,5.37,8.21,6.5,5.46,5.6,7.83,5.08,5.42,3.9,4.88,6.63,4.21,5.3,4.57,8.56,3.84,7.07,4.84,6.19,5.15,3.73,5.32,8.32,7.09,6.06,5.42,7,6.65,5.28,6.08,4.84,4.73,5.15,5.44,6.38,7.4,6.28,4.96,5.14,5.53,8.46,6.93,5.34,5.03,4.4,6.68,7.31,6.17,5.5,9.65,4.36,4.64,6.77,6.95,7.56,8.47,4.68,3.9,4.33,4.77,3.65,5.17,4.44,6.37,4.35,4.55,7.09,4.06,7.78,4.49,6.37,9.03,2.67,3.89,4.38,5.56,6.77,4.48,4.69,4.94,6.17,4.32,4.25,8.11,3.79,5.62,3.99,5.19,4.47,7.07,8.32,8.79,4.27,4.55,4.5,4.15,5.12,10.11,7.68,4.01,6.53,5.66,6.52,5.99,6.62,9.44,5.44,11.1,8.62,5.85,3.82,9.46,8.69,10.36,6.95,6.27,8.37,6.35,7.12,3.71,8.21,5.98,5.49,7.62,6.31,7.98,8.26,6.93,7.03,3.4,3.35,4.74,5.84,7.99,5.07,7.35,7.88,7.44,9.32,7.22,6.47,5.32,5.98,6.61,8.26,7.79,8.19,7.05,3.24,6.5,3.94,7.33,4.4,6.22,5.95,3.56,6.13,6.98,5.2,5.67,5.29,3.6,4.71,5.88,4.27,4.52,5.44,5.39,6.07,6.51,3.24,7.55,4.52,4.19,6.41,5.43,5.48,4.08,5.26,6.99,3.66,5.4,6.13,7.24,10.57,5.92,6.78,6.47,7.78,12.14,8.49,8.77,4.74,8.49,8.03,9.02,5.42,8.22,4.95,5.77,7.49,4.52,4.8,4.62,7,9.01,9.36,4.73,5.14,6.63,7.44,6.91,5.47,7.24,7.46,4.52,6.35,9.13,9.56,8.11,8.97,12.03,8.16,10.79,7.8,6.39,5.8,3.97,7.44,5.03,8.35,6.94,8.44,4.04,6.6,6.04,4.61,5.9,7.72,7.57,6.25,6.96,5.55,9.01,7.44,5.09,5.56,9.17,8.97,7.99,10.16,11.04,6.33,6.96,7,5.08,5.37,4.4,5.49,6.17,6.97,7.65,6.48,5.54,7.79,8.42,7,8.11,5.02,3.9,5.09,4.4,4.63,7.92,9.47,7.05,9.63,4.93,8.36,7.83,10.81,11.58,5.68,11.66,8.01,4.35,5.43,9.3,6.01,5.7,7.64,8.03,7.8,5.9,9.05,6.9,6.36,9.57,6.58,7.66,7.14,5.75,3.58,10.36,6.4,6.09,7.46,7.16,8.78,5.12,4.66,4.61,4.48,4.66,8.11,4.18,5.93,5.97,6.36,6.07,7.4,4.78,8.51,5.21,8.44,5.25,4.68,4.1,3.92,3.57,4.7,5.54,4.5,5.88,5.42,4.45,4.86,6.48,4.71,4.67,4.29,4.71,3.71,5.23,5.64,4.67,3.93,4.79,4.21,4.39,3.4,4.41,4.81,3.85,4.72,4.58,3.09,5.58,4.84,5.19,6.39,3.82,3.89,4.04,4.53,5.8,4.6,4.49,4.35,5.85,4.67,5.44,3.83,5.28,4.33,5.14,3.92,4.37,6.03,6.1,6.38,6.04,5.98,5.26,5.44,3.76,5.37,5.36,6.33,5.52,4.56,4.6,5.58,5.1,4.21,5.03,4.85,4.56,5.79,4.22,3.77,3.34,4.03,6.53,6.97,4.49,6.4,4.49,5.98,5.41,5.03,5.28,4.92,6.92,4.91,4.7,6.6,4.98,6.81,4.8,4.1,4.09,4.87,4.83,4.77,4.4,4.89,4.55,4.55,4.65,5.12,4.85,5.78,5.49,4.58,5.25,5.09,4.93,4.9,5.42,5.33,4.81,4.61,6.67,4.46,5.33,8.05,5.99,4.35,5.06,5.31,4.29,4.29,3.48,4.32,3.86,4.64,4.03,4.18,5.39,4.35,3.54,4.22,3.65,4.63,4.61,4.14,3.4,4.28,5.98,3.48,3.68,5.54,4.22,4.78,3.49,5.84,6.52,6.1,3.9,4.77,4.59,5.31,4.45,4.44,3.97,4.24,3.75,3.84,5.66,4.15,4.35,5.62,5.09,5.65,4.57,4.97,3.53,3.64,3.87,5.49,5.33,4.66,5.85,3.69,6.43,4.73,4.67,4.76,4.7,5.05,8.12,4.53,9.82,3.97,5.24,11.78,5.09,4.94,4.33,5,6.49,7.02,5.1,5.98,4.56,4.06,5.76,4.51,6.56,5.41,4.35,3.76,3.91,3.77,4.69,3.97,4.83,4.78,4.75,4.39,3.46,8.21,3.85,3.48,9.49,3.91,5.19,4.52,4.2,4.7,4.95)),.Names=c(""ID"",""IV1"",""IV2"",""DV"",""COVAR""),class=""data.frame"",row.names=c(NA,675L))
</code></pre>

<p>Model fit with the covariate:</p>

<pre><code>require(lmerTest)
require(car)

m1&lt;-lmer(DV ~ COVAR*IV1*IV2 + (1|IV1:ID), data=df)
</code></pre>

<p>Then I wanted to test whether COVAR is significant and whether an interaction between COVAR and the IVs exists. I used the <code>anova()</code> function provided by <code>lmerTest</code>. Here the covariate is significant as well as the interaction between COVAR and IV2:</p>

<pre><code>anova(m1)
Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
          Sum Sq Mean Sq NumDF  DenDF F.value    Pr(&gt;F)    
COVAR         4589.8  4589.8     1 567.74  56.506 2.197e-13
IV1            610.4   152.6     4 548.49   1.879  0.112731    
IV2           1223.2   611.6     2 562.64   7.529  0.000593
COVAR:IV1      208.7    52.2     4 560.52   0.642  0.632594    
COVAR:IV2      703.4   351.7     2 563.35   4.330  0.013613  
IV1:IV2        776.8    97.1     8 561.48   1.195  0.299305    
COVAR:IV1:IV2  680.6    85.1     8 561.47   1.047  0.399018
</code></pre>

<p>However when I use <code>Anova(m1, type=3)</code>, just to check and compare different outputs, it comes out like this:</p>

<pre><code>Analysis of Deviance Table (Type III Wald chisquare tests)

Response: DV
           Chisq Df Pr(&gt;Chisq)   
(Intercept)   7.7308  1   0.005429
COVAR         1.9850  1   0.158866   
IV1           6.5038  4   0.164549   
IV2           2.0069  2   0.366610   
COVAR:IV1     0.3739  4   0.984554   
COVAR:IV2     1.6527  2   0.437654   
IV1:IV2       9.5635  8   0.297007   
COVAR:IV1:IV2 8.3786  8   0.397383 
</code></pre>

<p>When I run the <code>Anova(m1)</code> it looks again closer to what <code>anova(m1)</code> produced, however, IV1 is now ""highly"" significant (which is what I would have expected a priori given the nature of IV1), plus there is also an interaction between IV1 and IV2. That being said and also given the discussions regarding type 2 and type 3 SS, I would opt for going ahead with type 2 SS:</p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: DV
             Chisq Df Pr(&gt;Chisq)    
COVAR          97.1301  1  &lt; 2.2e-16 
IV1           104.2557  4  &lt; 2.2e-16 
IV2            20.0292  2  4.474e-05 
COVAR:IV1       0.2244  4  0.9941594    
COVAR:IV2       9.1881  2  0.0101119   
IV1:IV2        28.5092  8  0.0003865 
COVAR:IV1:IV2   8.3786  8  0.3973834 
</code></pre>

<p><strong>Question 1:</strong> What is the explanation for these substantial variations between these outputs (especially <code>anova(m1)</code> vs. <code>Anova(m1, type=3)</code> which are both <code>type=3</code> calculations)?</p>

<p>Given the fact that COVAR interacts with IV2 and also that <code>m2 &lt;- lmer(COVAR ~ IV1*IV2 + (1|IV1:ID), data=df); Anova(m2)</code> turns out to be significant (again for IV2), I cannot sell this anlysis as ANCOVA since both additional assumptions for ANCOVA are violated. </p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: COVAR
      Chisq Df Pr(&gt;Chisq)    
IV1       3.093  4     0.5424    
IV2     160.317  2  &lt; 2.2e-16
IV1:IV2  34.734  8  2.989e-05
</code></pre>

<p>However, COVAR seems to play an important role and therefore should be kept in the model nonetheless.</p>

<p><strong>Question 2:</strong> Is this reasonable? And if yes, how do I go on and interpret the output of such a model, especially the interaction between COVAR and IV2?</p>

<p>What I would do is plot the interactions for IV1:IV2 and for COVAR:IV2 first:</p>

<pre><code>with(na.omit(df), interaction.plot(IV1,IV2,DV))
require(ggplot2)
ggplot(df,aes(x=COVAR, y=DV))+geom_point(aes(colour=IV2))+
geom_smooth(aes(colour=IV2), method=lm)
</code></pre>

<p>and then start discussing.</p>

<pre><code>&gt; sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
[1] ggplot2_1.0.1   car_2.1-0       lmerTest_2.0-29
[4] lme4_1.1-10     Matrix_1.2-2
</code></pre>
"
"0.0827039616973562","0.0858124131484961","181481","<p>I've been looking all around the webs but cant find a conclusive answer.  I have count data for a longitudinal study where subjects were grouped into three treatment groups (A,B,C) and blocked by litter and starting weight category (high, med, low).  I'd like to use a linear mixed model (<code>lme4</code> R package) to ask questions like: which factors are most indicative in differentiating each group of subjects?</p>

<p>I've only found examples using mixed models that use either a continuous response variable or a dichotic (0/1) response. In my case, my response is categorical with three groups.  Is it possible to use mixed models (and more specifically <code>glmer()</code>) with a categorical response of more than two outcomes?  Do I simply specify a binomial family (probit or logit)?</p>
"
"0.128124426527695","0.132940018808798","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0.161219701233018","0.167279188638034","181687","<p>I am just dipping my toe into the ocean that is linear effects models and am working through Barr et al.'s 'Keeping it Maximal' paper, trying to figure out the best way to fit a lmem for my experiment. Say you have three groups given three different types of drug over three days: 100mg on first day, 50mg on second day, 10mg on last day. The outcome measure is how they feel that next day on some scale (e.g. mood), before they are given their daily dose (i.e. so we are measuring the effects of the previous day's dose). However participants don't come in at exactly the same time each day, thus as each time of measurement the drug will have had less time to take effect. </p>

<p>I would like to know how best to include that random effect of 'time elapsed since dose' into this model, and just how best to fit the model really.</p>

<p>This is a toy dataset. I have not built any trends into it. </p>

<pre><code>dose100mg &lt;- c(6,2,9,4,6,5,2,4,6,7,3,2)
dose50mg &lt;- c(1,2,4,3,6,1,3,3,2,1,4,1)
dose10mg &lt;- c(8,9,7,9,6,7,8,9,8,7,1,3)
timeD1 &lt;- c(24.2,20.5,26,30,22,26,19,23,29,30,24,16)
timeD2 &lt;- c(24,16,28,20,19,28,30,20,18,15,27,32)
timeD3 &lt;- c(21,28,29,30,29,17,23,18,24,16,28,21)
subject &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12)
group &lt;- factor(c(0,1,0,2,1,2,0,2,1,2,1,0))

df &lt;- data.frame(subject, group, dose100mg, dose50mg, dose10mg, cov)
</code></pre>

<p>Turn it from wide to long</p>

<pre><code>require(tidyr)

df &lt;- gather(df, dose, score, dose100mg:dose50mg:dose10mg)
</code></pre>

<p>Now add the 'hours elapsed since last dose' variable to the dataframe (btw: if anyone knows how to build this into the gather function above I'd appreciate it) </p>

<pre><code>df$hrsElapsed &lt;- c(timeD1, timeD2, timeD3) 
</code></pre>

<p>Now fit a model. First group*dose plus with random intercepts for subject. </p>

<p>require(lme4)</p>

<pre><code># random intercepts

anDf_randomintercepts &lt;- lmer(score ~ group*dose + (1|subject), data = df)

anova(anDf_randomintercepts)
</code></pre>

<p>Next random slopes, and my first question. Is it better to include hrsElasped as a covariate, like this?</p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + hrsElapsed + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>Or to include it as a random effect? like this</p>

<pre><code>nDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>I know it's not the latter because I get an error message. </p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0450795 (tol = 0.002, component 1)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>But I don't know WHY this doesn't work. I would have thought time elapsed would be exactly the sort of variable you'd want to assign to random effects.</p>

<p>What am I doing wrong?</p>

<p>An ancillary question pertains to fitting random slopes for the group-by-subject effect </p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+group|subject), data = df)
</code></pre>

<p>When I run this i get the error message</p>

<pre><code>Error: number of observations (=36) &lt;= number of random effects (=36) for term (1 + group | subject); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable
</code></pre>

<p>Why doesn't this work? What does it mean?</p>
"
"0.0827039616973562","0.0858124131484961","181844","<p>I'm trying to extract the value of the Variance from a Random Effects model built in R, using the lme4 package:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: PH ~ (1 | gen) + rep
   Data: tab

REML criterion at convergence: 591.3

Scaled residuals: 
 Min       1Q   Median       3Q      Max 
-2.09030 -0.53055  0.04938  0.66689  2.02929 

Random effects:
Groups   Name        Variance Std.Dev.
 gen      (Intercept) 34.99    5.915   
 Residual             75.16    8.670   
Number of obs: 80, groups:  gen, 40

Fixed effects:
        Estimate Std. Error t value
(Intercept)  296.268      3.205   92.45
rep            8.550      1.939    4.41

Correlation of Fixed Effects:
(Intr)
rep -0.907
</code></pre>

<p>I'm interested in the Random effects section - the Variance for the ""gen"" Group (the value is 34.99). Can anyone help?</p>
"
"0.075498042361142","0.0940027887907685","182315","<p>I'm using R to fit a generalized linear model with random effects using a negative binomial distribution. One of the main issues is that to run glmer() with a negative binomial, the dispersion parameter $\theta$ needs to be specified. <a href=""http://stats.stackexchange.com/questions/138109/r-glmer-nb-output-how-to-get-hat-theta"">From what I understand</a>, the glmer.nb() function makes an initial guess for $\theta$ and then runs the glmer() function. This is then iterated until convergence is achieved. This becomes a huge issue when the dataset is large with big number of factors. My question is: is there any other reasonable way of guessing the $\theta$ parameter and running the glmer() function once? For example, if I were to remove all random effects, and run glmer.nb() on the fixed effects alone, would that be a decent guess of $\theta$?</p>
"
"NaN","NaN","184767","<p>Here's the output from my the summary of lmer function which I used to calculate the ICC.</p>

<pre><code>   Linear mixed model fit by REML ['lmerMod']
   Formula: CareChange ~ 1 + (1 | PROVIDER)
      Data: MEA_data_1

   REML criterion at convergence: 35.2

   Scaled residuals: 
       Min      1Q  Median      3Q     Max 
   -0.3093 -0.2829 -0.2711 -0.2599  3.6206 

   Random effects:
    Groups            Name        Variance  Std.Dev.
    PROVIDER          (Intercept) 0.0003096 0.01759 
    Residual                      0.0660667 0.25703 
   Number of obs: 239, groups:  PROVIDER_CALENDAR, 14

   Fixed effects:
        Estimate Std. Error t value
        (Intercept)  0.07119    0.01742   4.086

   ICC = 0.0003096 /(0.0003096 +0.0660667 ) = 0.004664315
</code></pre>

<p>I did not find any literature that shows how to calculate the CI for this    ICC value. Any help on this issue is much appreciated.</p>
"
"0.0640622132638473","0.0664700094043992","185031","<p>I have a model of the following structure: </p>

<pre><code>lme4::lmer(formula = signal ~ factorA + factorB + factor C + (1 | 
    subj), data = s)
</code></pre>

<p>and find a significant main effect for factorA.</p>

<p>I then compute a post hoc test using <code>lsmeans::lsm</code></p>

<pre><code>summary(glht(finalModel, lsm(pairwise ~ factorA)))
</code></pre>

<p>and get the following result:</p>

<pre><code>Note: df set to 132

     Simultaneous Tests for General Linear Hypotheses

Fit: lme4::lmer(formula = signal ~ factorA + factorB + factorC + (1 | subj), data = s)

Linear Hypotheses:
               Estimate Std. Error t value Pr(&gt;|t|)   
f1 - f2 == 0  -1.4293     0.6743  -2.120  0.21647   
f1 - f3 == 0  -2.3160     0.6743  -3.434  0.00694 **
f1 - f4 == 0  -1.9429     0.6750  -2.878  0.03661 * 
f1 - f5 == 0  -1.0871     0.7796  -1.394  0.63049   
f2 - f3 == 0  -0.8867     0.6373  -1.391  0.63240   
f2 - f4 == 0  -0.5136     0.6668  -0.770  0.93822   
f2 - f5 == 0   0.3423     0.7742   0.442  0.99192   
f3 - f4 == 0   0.3731     0.6668   0.560  0.98039   
f3 - f5 == 0   1.2289     0.7742   1.587  0.50619   
f4 - f5 == 0   0.8558     0.7327   1.168  0.76805
</code></pre>

<p>I actually just wanted to check, whether the statistics were really computed with df=132, so that t(132)=-3.434, p=0.007.</p>

<p>So I computed: <code>2*pt(3.434, 132, lower=F)</code>, but I get a much smaller p-value instead.</p>

<p>Any suggestions why this goes wrong? </p>

<p>Is it, because the df in a mixed model are not so straightforward as in simple models or even not appropriate, that the pt()-formula fails?</p>
"
"0.0640622132638473","0.0664700094043992","185499","<p>I am using R for statistical analysis of my data. For some experiments I have to construct a linear mixed effect model using the <em>lmer</em> function from the <strong>lme4</strong> package. For other experiments an ANCOVA model is sufficient. After backwards elimination of the comprehensive model I want to compare the effects of treatments using Dunnett's test from the <strong>multcomp</strong> package. When I perform Dunnett's test for the lmer model it returns z-values, while for the ANCOVA models (constructed with <code>lm()</code>) the returned statistic is a t-value. What is the reason for this?</p>

<p>I also have another question about this. When I try to calculate the p-value from the t-value returned using the <code>pt(q,df)</code> function, I get a different value. Is this because of the correction made for the multiple comparisons?</p>

<p>Thanks for helping!</p>
"
"0.116961064294386","0.121357078494567","185936","<p><strong>When we use R code for a linear mixed model like this: <code>model=lmer(y~x1+x2+(1|x3),data)</code>, how can we calculate the variance explained by each fixed variable?</strong></p>

<p>To solve this problem, I tried my best to search on the internet.
My understanding is the following : </p>

<ol>
<li><p>After we run the code : <code>model=lmer(y~x1+x2+(1|x3),data)</code>, we run the function summary(model), then we get the variance explained by the random effect ï¼ˆnamely <code>rvariance</code>ï¼‰. </p></li>
<li><p>we run function  <code>anova(model)</code>, we get the sum of square for each fixed variable; Variance of each fixed variable can be obtained by dividing sum of square into n (the number of observation)  ï¼ˆ<code>fvariance1</code>, <code>fvariance2</code>ï¼‰.</p></li>
<li><p>We use function <code>resquredLR</code> in the MunMIn package to get the whole variance explained by the whole model (whole variance)</p></li>
<li><p>To calculate variance explained by each fixed vaiable,  we use this function</p>

<p><code>fixedvaiance1= whole variance*fvariance1/(rvariance+fvariance1+fvariance2)</code></p></li>
</ol>

<p>This is my current understanding. Can anyone give some comments about my understanding? If there are problems with this approach, I'd be happy if you could point them out.</p>
"
"0.0905976508333704","0.0783356573256404","186322","<p>I have a model M calculated via lme4's glmer function, with random effects (""Customer ID"") and fixed effects for each customer ID. My dataset is very large, so I would like to select a sample of Customer IDs, calculate the corresponding fixed and random effect coefficients and then use this model to estimate random effects for the other Customer IDs that haven't been sampled yet.</p>

<p>My main question is, what techniques or packages in R could I use to <em>validate</em> my model? In other words, I want to make sure that my fixed effect estimate is good enough before calculating the random effects of the rest of my dataset.</p>

<p>In general, what techniques exist for splitting datasets and doing gneralized linear models on each subset, finally combining the models?</p>

<p>I would sincerely appreciate references. The best I could find <a href=""http://www.stat.columbia.edu/~gelman/research/unpublished/comp7.pdf"" rel=""nofollow"">is this</a>. </p>
"
"0.116961064294386","0.121357078494567","186825","<p>I am trying to run a mixed model on over-dispersed non-integer data. My data are not counts, but are zero-inflated and over dispersed. The variable is distance (how far a gps point is from a central location) and as such looks like: 0.33, 64.73, 5.2 etc. I have been using a quasi-Poisson distribution as I have read that quasi can handle non-integer data (both Poisson and negative binomial cannot). I am using the <code>glmmPQL</code> function in package MASS as this allows quasi distributions with a random term (the identity of the individual that the gps point comes from).The functions <code>glmm</code> and <code>lmer</code> do not work with a quasi-Poisson distribution. Plotting the residuals indicates a lack of fit of this model.log-transforming the data to try and make it normal before hand also fails (the Shapiro-test for normality is significant). I am unsure how to fix this, as I seemingly have to use a quasi-distribution (link=""log"") because my data is not counts, non-integer and not normal but there is still overdispersion and lack of fit when using this distribution. </p>

<p>My question therefore is: <strong>How to model over-dispersed, non-integer data in a mixed model when quasi-Poisson does not seem to work?</strong>   </p>

<p>My code so far is:</p>

<pre><code>summary(glmmPQL(distance_from_centroid~Chick.Juv.Adult+Summer_winter, 
                random=~1|markingnumber, family=quasipoisson(link=""log""),
                data=centroid_distances))
</code></pre>

<p>Which results in:  </p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
 Data: centroid_distances 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 `Formula: ~1 | markingnumber
        (Intercept) Residual
StdDev:    1.157381 2.136811

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: distance._from_centroid ~ Chick.Juv.Adult + Summer_winter 
                      Value  Std.Error  DF   t-value p-value
(Intercept)       2.0670095 0.09403952 695 21.980221  0.0000
Chick.Juv.AdultC -0.2945360 0.06686399 695 -4.405002  0.0000
Chick.Juv.AdultJ -0.2005831 0.06727181 695 -2.981682  0.0030
Summer_winterW    0.1207721 0.04324588 695  2.792684  0.0054
 Correlation: 
                 (Intr) C.J.AC C.J.AJ
Chick.Juv.AdultC -0.565              
Chick.Juv.AdultJ -0.512  0.736       
Summer_winterW   -0.267  0.134  0.043

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.53759073 -0.48277169 -0.31041612  0.06314122  7.48672836 

Number of Observations: 1009
Number of Groups: 311 
</code></pre>

<p>Which when plotting the residuals gives me:</p>

<p><a href=""http://i.stack.imgur.com/3SKVU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3SKVU.jpg"" alt=""plot of residuals""></a></p>
"
"0.105264957864947","0.10922137064511","186836","<p>Based on a <a href=""http://stats.stackexchange.com/questions/182988/plotting-to-check-homoskedasticity-assumption-for-repeated-measures-anova-in-r"">previous question</a> that I asked about checking assumptions of repeated-measures ANOVAs in R (which turns out to be not so trivial), I'm wondering about the relationship between a repeated-measures ANOVA and a linear mixed model on the same data.</p>

<p>In an excellent exploration of my data, it was suggested to me that I switch to linear mixed models for my full analysis, which I have already done. However, since for reasons of completeness I still need to run a respeated-measures ANOVA, I'm specifically wondering <strong>whether assumption checks on a linear mixed model can be used to infer assumption violations of assuptions for a repeated-measures ANOVA using the same data</strong>.</p>

<p>For example, using the example data below:</p>

<pre><code>set.seed(12)

#Generate variables and data frame
subj &lt;- sort(factor(rep(1:20,8)))
x1 &lt;- rep(c('A','B'),80)
x2 &lt;- rep(c('A','B'),20,each=2)
x3 &lt;- rep(c('A','B'),10, each=4)
outcome &lt;- rnorm(80,10,2)

d3 &lt;- data.frame(outcome,subj,x1,x2,x3)

#Repeated measures ANOVA
m.aov &lt;- aov(outcome ~ x1*x2*x3 + Error(subj/(x1*x2*x3)), d3)

#Linear mixed model assumption checks
require(lme4)
#`subj` as random term to account for the repeated measurements on subject.
m.lmer&lt;-lmer(outcome ~ x1*x2*x3 + (1|subj), data = d3)

# Check for heteroscedasticity
plot(m.lmer)
# or
boxplot(residuals(m.lmer) ~ d3$x1 + d3$x2 + d3$x3)
# Check for normality
qqnorm(residuals(m.lmer))
</code></pre>

<p><strong>Can the assumption plots on m.lmer be used to test assumption violations for m.aov?</strong> For example, if m.lmer displays heteroskedasticity, would that suggest that m.aov is afflicted with heteroskedasticity as well?</p>

<p>Thanks for any insight!</p>
"
"0.105264957864947","0.10922137064511","186962","<p>Let's suppose I measure weights of eight male and eight female mice, from four litters, that were subjected to two different treatments: a and b</p>

<p>Here are my data:</p>

<pre><code>set.seed(1)
df &lt;- data.frame(sex = c(rep(""m"",8),rep(""f"",8)), treatment = rep(c(rep(""a"",4),rep(""b"",4)),2))
df$weight &lt;- c(rnorm(4,1),rnorm(4,2),rnorm(4,3),rnorm(4,4))
    df$litter &lt;- rep(c(rep(""l1"",2),rep(""l2"",2),rep(""l3"",2),rep(""l4"",2)),2)
</code></pre>

<p>Now I fit a mixed effects model to the interaction between sex and treatment, which are categorical fixed effects, and litter is defined as a random effect:</p>

<pre><code>df$sex = as.factor(df$sex)
df$treatment = as.factor(df$treatment)
df$litter = as.factor(df$litter)


fit &lt;- lmer(weight ~ sex*treatment + (1|litter), data = df)
</code></pre>

<p>And here's the output:</p>

<pre><code>summary(fit)
Linear mixed model fit by REML ['lmerMod']
Formula: weight ~ sex * treatment + (1 | litter)
   Data: df

REML criterion at convergence: 38.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.47793 -0.54377 -0.01737  0.48467  1.45905 

Random effects:
 Groups   Name        Variance Std.Dev.
 litter   (Intercept) 0.4318   0.6571  
 Residual             0.7574   0.8703  
Number of obs: 16, groups:  litter, 4

Fixed effects:
                Estimate Std. Error t value
(Intercept)      3.54300    0.63662   5.565
sexm            -2.46379    0.61541  -4.004
treatmentb       0.01801    0.90031   0.020
sexm:treatmentb  1.08648    0.87032   1.248

Correlation of Fixed Effects:
            (Intr) sexm   trtmnt
sexm        -0.483              
treatmentb  -0.707  0.342       
sxm:trtmntb  0.342 -0.707 -0.483
</code></pre>

<p>Obviously the ""f"" sex and the ""a"" treatment are set as dummy variables to a zero baseline.
My question relates to the interpretation: is there a way to extract the effect size of the dummy variables? 
Alternatively, if I add an additional sex and treatment variable, for which I set weight to 0, will they therefore serve as dummy variables that really have a zero effect size?</p>
"
"0.0661631693578849","0.0858124131484961","187581","<p>I have fitted a linear mixed effects model to my data. I first determined that it was appropriate to log transform my response variable. Here is the equation:</p>

<pre><code>logAreaChl=lmer(log(mgChl_m2)~ Day*Trmnt+(1|Tank), data=aggsums)
</code></pre>

<p>If someone wishes to publish this data, what is the best way to visualize the data?</p>

<p>-I have been using lsmeans to generate ""predicted"" values from my model </p>

<pre><code>logAreals=lsmeans(logAreaChl, pairwise~Day:Trmnt, data=aggsums)
</code></pre>

<p>Is it appropriate to use the lsmean to plot the data? I assume this is taking into account my random effect (specified as tank) which is why the data has much less variance than plotting just the raw data.</p>

<p>Also, because it is log transformed, the lsmean and confidence intervals are also on log scale. Can these simply be squared to generate values more appropriate to the original data?</p>
"
"0.116961064294386","0.121357078494567","187996","<p>I am attempting to analyze the effect of two categorical variables (<code>landuse</code> and <code>species</code>) on a continuous variable (<code>carbon</code>) though a linear mixed model analysis. Study sites are included as the random effect in the model (with the random slope and random intercept). Landuse, species (and their interaction) are included as fixed effects.</p>

<p>the model is this  - </p>

<pre><code>model1 = lmer(carbon ~ species*landuse + (1+landuse|site), data)
</code></pre>

<p>I know that there may be interaction between landuse and species. I know that presence of interaction can change the interpretation of the main effects. I want to know what should I do if there is a significant interaction between landuse and species? In that case, do I study the effect of landuse for each species seprately with the following model -</p>

<pre><code>model.sp1 = lmer(carbon ~ landuse + (1+landuse|site), data.sp1)
</code></pre>

<p>and repeat this for all the six species? Do I need any form of corrections (of p-values) due to running multiple tests?</p>

<p>Another question is that if the interaction term is not significant, can I interpret the main effects from <code>model1</code> or do I run another model (<code>model2</code>) without the interaction effect and interpret the main effects from there?</p>

<pre><code>model2 = lmer(carbon ~ species + landuse + (1+landuse|site), data)
</code></pre>

<p>I am fairly new to mixed model and R, so please excuse my naivety!</p>

<p>PS - just to clarify, I have a fairly good idea of what an interaction mean and how to interpret it. I do not know, how to interpret main effects in the presence of an interaction - whether I need to run a seprate analysis to interpret main effects.</p>
"
"0.0978566471559948","0.101534616513362","188301","<p>Following some reading I have tried for the first time to use multilevel poisson model to look at risk factors for disease cases in sheep in 5 English regions. </p>

<p>The overall aim is to find out if the factors could be predictive for within and between region disease cases (5 regions in total). The R output shows that some factors are significant. I also read that multicollinearity can be a problem while the correlation of fixed effects show that there is multicollinearity something cant figure out how to fix.</p>

<p>Could someone help explain in simpler terms how to interpret results from the output. Does the p-values which are significant shown on fixed effects indicate that these factors are the predictors for disease in every region from the 5 regions? </p>

<p>I have also included sample from the dataset.</p>

<pre><code>Postcode    Region.Coding   maxtemp meantemp    mintemp Cases2011
YO7 4DH     1               13.45   9.75        6.05    50
YO62 7JL    1               13.45   9.75        6.05    0
YO62 6RW    1               13.45   9.75        6.05    10
YO62 5HX    1               13.45   9.75        6.05    0
TN27 0DA    2               15.32   11.22       7.13    98
TN26 3TF    2               15.32   11.22       7.13    0
TN26 3EU    2               15.32   11.22       7.13    30
TN25 6AS    2               15.32   11.22       7.13    0
TN25 5PD    2               15.32   11.22       7.13    28
TR7 3HU     3               14.17   10.6        7.06    115
TR27 5EF    3               14.17   10.6        7.06    0
TR10 9DL    3               14.17   10.6        7.06    0
TQ9 7LN     3               14.17   10.6        7.06    23
TQ9 6NQ     3              14.17    10.6        7.06    50

mod1=glmer(Cases2011~maxtemp11+mintemp11+meantemp11+rain2011+Altitude+Flock2011+(1|Region.Coding/Postcode), family=poisson, data=orf)

  Min      1Q  Median      3Q     Max 
-5.6160 -0.1209 -0.0658  0.0206  2.9740 

Random effects:
 Groups                 Name        Variance  Std.Dev.
 Postcode:Region.Coding (Intercept) 44.647031 6.68184 
 Region.Coding          (Intercept)  0.001741 0.04173 
Number of obs: 756, groups:  Postcode:Region.Coding, 752; Region.Coding, 5

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.2474876  3.7713537  -2.452  0.01421 *  
maxtemp11   -1.0416300  0.3833977  -2.717  0.00659 ** 
mintemp11    0.2317166  0.2231959   1.038  0.29919    
meantemp11   1.4316707  0.5645888   2.536  0.01122 *  
rain2011     0.0034593  0.0013515   2.560  0.01048 *  
Altitude     0.0011370  0.0030738   0.370  0.71147    
Flock2011    0.0018647  0.0001878   9.929  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) mxtm11 mintmp11 mentmp11 rn2011 Altitd
maxtemp11   0.028                                       
mintemp11  -0.331 -0.246                                
meantemp11 -0.473 -0.851  0.140                         
rain2011   -0.536 -0.268  0.217    0.409                
Altitude   -0.249 -0.010  0.139    0.062    0.010       
Flock2011  -0.062  0.044 -0.046   -0.048   -0.012 -0.019
</code></pre>
"
"0.200058619511786","0.188117452377264","188361","<p>I'm pretty new in using <code>lmer</code> and be confused about different p-values in Tukey post hoc tests associated with exactly the same estimates. I built a linear mixed model with monetary contributions of human subjects as response variable and their wealth and number of children as explanatory variables. The experiment was designed in a way to contribute for future generations. I don't have repeated measurements of the same individual but some individuals played within the same group. There are several subsets and additional random factors but here I only want to consider the following model where <code>totalcontSubject</code> means contribution of a subject over the entire game, <code>poverty</code> is a factor with 2 levels (rich and poor), and <code>children</code> is a factor with 2 levels (child or noChild). Particularly I'm interested in understanding the fixed effects part of the model.</p>

<pre><code> &gt; summary(TC1)
Linear mixed model fit by REML ['lmerMod']
Formula: totalcontSubject ~ poverty * children + (1 | group_2)
   Data: data

REML criterion at convergence: 414.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.42955 -0.45554 -0.09361  0.45228  2.33159 

Random effects:
 Groups   Name        Variance  Std.Dev. 
 group_2  (Intercept) 8.611e-15 9.280e-08
 Residual             1.042e+02 1.021e+01
Number of obs: 58, groups:  group_2, 10

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    16.200      3.228   5.019
povertyrich                     8.600      3.953   2.175
childrennoChild                 2.800      4.565   0.613
povertyrich:childrennoChild    -4.489      5.642  -0.796

Correlation of Fixed Effects:
            (Intr) pvrty chldrC
povertyrch  -0.816              
chldrnnChld -0.707  0.577       
pvrtyrch:C   0.572 -0.701 -0.809
</code></pre>

<p>If I interpret fixed effects of the summary table in the right way, my intercept denotes poor people with children. The estimate also corresponds to the mean value of this combination in my data. According to my calculations the difference to rich people (shown as <code>povertyrich</code>) actually shows the difference of the intercept to rich people with children, even if not explicitly mentioned by <code>povertyrich</code>. This is the first issue I'm a bit confused. A reduced model only with fixed factor poverty is significant better by <code>anova()</code> but it seems data including children are used for this evaluation.</p>

<p>If I run a Tukey post hoc test by means of my TC1 model, I get a significant difference between rich and poor. But the estimates in the summary actually include children. Estimates of intercept and slope are the means of poor people with children and the difference to rich people with children. They don't correspond to the means of poor or rich data irrespective of parenthood. </p>

<pre><code>summary(glht(TC1, linfct=mcp(povertry=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ poverty * children + (1 | 
    group_2), data = data)

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)  
rich - poor == 0    8.600      3.953   2.175   0.0296 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>I get even more confused when I run a Tukey post hoc test for a subset where I coded interactions in a column such as poor people with children and rich people with children. In this output I have exactly the same estimates and parameters for these categories (like in the summary shown before for rich poor people exclusively) but the p-values are different. A visual check indicates that there is a significant difference between <code>richChild</code> and <code>poorChild</code> but outputs of <code>glht</code> <code>Interak</code> shows me it is not. Also, a comparison between models <code>anova()</code> with fixed factor poverty vs. fixed factors poverty and children indicates that I can get rid of the variable children in my model. Before I do so, I would like to understand the outputs better. I also worry about the high value for Residual and the correlations in the summary table. </p>

<pre><code>&gt; summary(glht(TC1_2, linfct=mcp(Interak=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ Interak + (1 | group_2), data = data)

Linear Hypotheses:
                               Estimate Std. Error z value Pr(&gt;|z|)
poorNoChild - poorChild == 0      2.800      4.565   0.613    0.927
richChild - poorChild == 0        8.600      3.953   2.175    0.129
richNoChild - poorChild == 0      6.911      4.026   1.717    0.312
richChild - poorNoChild == 0      5.800      3.953   1.467    0.454
richNoChild - poorNoChild == 0    4.111      4.026   1.021    0.735
richNoChild - richChild == 0     -1.689      3.316  -0.509    0.956
(Adjusted p values reported -- single-step method)
</code></pre>
"
"0.0978566471559948","0.101534616513362","189115","<p>I'm hoping somebody can help with what I think is a relatively simple question, and I think I know the answer but without confirmation it has become something I just can't be certain of. </p>

<p>I have some count data as a response variable and I want to measure how that variable changes with the proportional presence of something.</p>

<p>In more detail, the response variable is counts of the presence of an insect species in a number of sites, so for example a site is sampled 10 times and this species may occur 4 times. </p>

<p>I want to see if this correlates with the proportional presence of a group of plant species in the overall commmunity of plants at these sites. </p>

<p>This means my data looks as follows (this is just an example)</p>

<pre><code>Site, insectCount, NumberOfInsectSamples, ProportionalPlantGroupPresence
1, 5, 10, 0.5
2, 3, 10, 0.3
3, 7, 9, 0.6
4, 0, 9, 0.1
</code></pre>

<p>The data also includes a random effect for location. </p>

<p>I thought of two methods, one would be an linear model (<code>lmer</code>) with the insects converted to a proportion e.g. </p>

<pre><code> lmer.model&lt;-lmer(insectCount/NumberOfInsectSamples~
 ProportionalPlantGroupPresence+(1|Location),data=Data)
</code></pre>

<p>The second would be a binomial GLMM (<code>glmer</code>)
e.g.</p>

<pre><code>glmer.model &lt;- glmer(cbind(insectCount,NumberOfInsectSamples-insectCount)~
 ProportionalPlantGroupPresence+(1|Location),
 data=Data,family=""binomial"")
</code></pre>

<p>I believe the binomial glmer to be the correct method, however they produce fairly different results. I cant seem to find a definitive answer on the net without still feeling slightly uncertain, and wish to make sure I am not making a mistake. </p>

<p>Any help or insight into alternative methods on this would be much appreciated. </p>
"
"0.075498042361142","0.0626685258605123","189835","<p>I would appreciate some assistance with setting the statistical analyses of my experiment.</p>

<p>During my experiment 14 participant's motor responses (<code>ptp</code>) were tested once before and twice (<code>time</code>: <code>pre</code>, <code>post15</code>, <code>post60</code>) after a certain intervention (<code>condition</code>). During each test 7 different intensities were applied, and the stimulus was replied 10 times at each <code>intensity</code>.</p>

<p>This gives me the following data <code>io</code>:</p>

<pre><code>&gt; io
       subject condition   time intensity        ptp
1         Sbj1        MI    pre       90%  33.006978
...
10        Sbj1        MI    pre       90%         NA
11        Sbj2        MI    pre       90%  44.005610
...
11760    Sbj14       ERS post60      150%   415.1405
</code></pre>

<p>My variable of interest was the motor responses (<code>ptp</code>) and I would like to know whether I have an effect of intervention (<code>condition</code>) on response amplitude over time and a difference across interventions.
It is also noteworthy, that I had to rejects some of the motor responses due to preactivation. Thus the number of stimuli per <code>intensity</code> varies from 0 to 10.</p>

<p>After a lot of reading by my own, I reached the conclusion that the best approach is a linear mixed model with:</p>

<ul>
<li>dependent variable: <code>ptp</code></li>
<li>fixed factors: <code>condition</code>, <code>time</code> and <code>intensity</code></li>
<li>random effect: <code>subject</code></li>
</ul>

<p>and worked with the following code:</p>

<pre><code>io.model = lmer(ptp ~ condition + time + intensity + (1 | subject), data=io, REML=FALSE)
io.null = lmer(ptp ~ time + intensity + (1|subject), data=io, REML = FALSE)
anova(io.null, io.model)
</code></pre>

<p>However, I am not fully convinced with my setup, also as I don't include any interaction which might happen due to some of the variables.</p>
"
"0.184931680363827","0.184207093548801","189933","<p>I am seeking advice on how to effectively eliminate autocorrelation from a linear mixed model. My experimental design and explanation of fixed and random factors can be found here from an earlier question I asked: </p>

<p><a href=""http://stats.stackexchange.com/questions/188929/crossed-fixed-effects-model-specification-including-nesting-and-repeated-measure"">Crossed fixed effects model specification including nesting and repeated measures using glmm in R</a></p>

<p>I have treated day as numeric even though I only have four sampling time points (so I could treat it as a categorical predictor as well). Aside: Although four sample points is very few, I donâ€™t think that this is the root of the problem as this same dataset is giving me this residual autocorrelation issues using a different response variable that has 24 time points.</p>

<p>My issue is that I have tried a number of different autocorrelation structures and canâ€™t seem to achieve the random, non-significant residuals needed to confirm a lack of autocorrelation. I am using the function <code>lme</code> in the R package <code>nlme</code> to deal with autocorrelation. </p>

<p>I have tried the various autocorrelation classes  with variations to form</p>

<p>1) <code>corAR1</code> (autoregressive process of order 1).</p>

<p>2) <code>corARMA</code> (autoregressive moving average process)</p>

<p>3) <code>corCAR1</code> (continuous autoregressive process)</p>

<p>4) <code>corGaus</code> (Gaussian spatial correlation)</p>

<p>With form varying in the following ways with these different autocorrelation classes:</p>

<pre><code>form=~1
form=~1| TankNumb/RecruitID2
form=~Day| TankNumb/RecruitID2
</code></pre>

<p>If we look at a model without the time factor ""Day"" added, the ACF and PACF plots look like this. </p>

<pre><code>lme4_lognormal_notime&lt;-lmer(Arealog~Temperature*Culture+(1|TankNumb/RecruitID2), data=growthSR_noNA)

acf(residuals(lme4_lognormal_notime, retype=""normalized""))
pacf(residuals(lme4_lognormal_notime, retype=""normalized""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/xVdrb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xVdrb.jpg"" alt=""enter image description here""></a></p>

<p>Also, if I look at the residuals of the model without â€œDayâ€ included, I do not see any strong pattern in the residuals that would make me think there is a temporal autocorrelation problem.</p>

<pre><code>plot(residuals(lme4_lognormal_Ben_notime, retype=""normalized"")~growthSR_noNA$Day)
</code></pre>

<p><a href=""http://i.stack.imgur.com/wU1ZC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wU1ZC.jpg"" alt=""enter image description here""></a></p>

<p>Now for two different models with autocorrelation structure to hopefully eliminate autocorrelation:</p>

<pre><code>nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/Oe3et.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oe3et.jpg"" alt=""enter image description here""></a></p>

<pre><code>nlme_lognormal_mult_cortime&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~Day|TankNumb/RecruitID2), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pUgA1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUgA1.jpg"" alt=""enter image description here""></a></p>

<pre><code>ARMA_nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corARMA(form=~1, p=0, q=1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/6TMeL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6TMeL.jpg"" alt=""enter image description here""></a></p>

<p>The AIC suggests that the simplest correlation structure is the best. </p>

<pre><code>AIC(nlme_lognormal_mult,nlme_lognormal_mult_cor, nlme_lognormal_mult_cortime,ARMA_nlme_lognormal_Ben_mult_cor)

                               df      AIC
nlme_lognormal_mult              15 1233.997
nlme_lognormal_mult_cor          16 1184.389
nlme_lognormal_mult_cortime      16 1235.997
ARMA_nlme_lognormal_Ben_mult_cor 16 1198.451
</code></pre>

<p>As I mentioned above, I have tried a number of different <code>cor</code> functions (the four listed above) and different <code>form</code> specifications. They all end up with ACF/PCF plots like the last two models with a first lag at below 0.2 in the ACF plot and a PCF plot with the first three lags around 0.10.</p>

<p>I have also read a number of sites describing how to specify corARMA models based on diagnosing the ACF plots and have tried a number of variations of p and q parameters. </p>

<p>Questions: </p>

<ol>
<li>Does anyone have some advice on which type of correlation structure that might elimate this autocorrelation problem based on the patterns in my ACF/PCF plots? Should I be diagnosing based on a model with or without Day included?</li>
</ol>

<p>2.Is there ever an acceptable level of autocorrelation? 
This post (<a href=""http://stats.stackexchange.com/questions/80823/do-autocorrelated-residual-patterns-remain-even-in-models-with-appropriate-corre"">Do autocorrelated residual patterns remain even in models with appropriate correlation structures, &amp; how to select the best models?</a>) states that small amounts of autocorrelation probably won't impact the model coefficients very much. ""The estimate is slightly larger than zero so will have negligible effect on the model fit and hence you might wish to leave it in the model if there is a strong a priori reason to assume residual autocorrelation."" Potentially there is some autocorrelation that is not being caused by temporal autocorrelation, like outliers? Is there a cut-off, for example, autocorrelation below 0.1? I have extremely small 95% confidence intervals, so it doesn't take a lot of autocorrelation in my models to be significantly too much.</p>

<p>Any advice is appreciated! </p>
"
"0.258904352509358","0.259041225303001","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0863014508364527","0.102337274193778","191869","<p>I have a glmer model that I am performing contrasts on. My model has four factors (emotion, gender, filter, ageGroup). No interactions. I have been using <code>lsmeans(x, spec, contr='pairwise'))</code> (and) <code>lsm</code> within <code>glht</code>) to get lsmeans and to do pairwise contrasts within-factors. I am also interested in comparing the means from one factor against a level from a different factor.</p>

<pre><code>&gt; summary(m7.only_within)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )

 Formula: FaceStimulus.RESP.binary ~ emotion + emotion.Sample + gender +  
    gender.Sample + ageGroup + ageGroup.Sample + filter + filter.Sample + (1 | Subject)

Fixed effects:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -0.823451   0.264442  -3.114  0.00185 ** 
emotiondisgust               1.504896   0.092532  16.264  &lt; 2e-16 ***
emotionfear                  1.555492   0.091832  16.938  &lt; 2e-16 ***
emotion.Sample              -0.106948   0.044364  -2.411  0.01592 *  
gendermale                   0.012718   0.074198   0.171  0.86391    
gender.Sample                0.007021   0.010046   0.699  0.48462    
ageGroupold                  0.364239   0.076379   4.769 1.85e-06 ***
ageGroup.Sample             -0.013519   0.009577  -1.412  0.15808    
filtercropAdaptiveThreshold  0.419095   0.075906   5.521 3.37e-08 ***
filter.Sample                0.070147   0.029658   2.365  0.01802 *  
---
</code></pre>

<p>I understand that lsmeans calculates standard errors using variable variances and covariances from vcov() to do pairwise comparisons. Can I just compare one model variable against another? Like so:</p>

<pre><code>summary(glht(m3.only_within, linfct = c(""filtercropAdaptiveThreshold - ageGroupold = 0"")))

Linear Hypotheses:
                                               Estimate Std. Error z value Pr(&gt;|z|)    
filtercropAdaptiveThreshold - ageGroupold == 0  0.05486    0.10798   0.508        1    
</code></pre>

<p>That contrast is the test of the difference between the betas in the model, not between the lsmeans. Naturally, all my other contrasts compare the lsmeans since I'm using <code>lsm()</code>. Can I just manually take the difference between the lsmeans, and then use the variances and covariance SE's from the model vcov for the two model effects to get the SE for the <code>lsmeans</code> difference? Naturally, those SEs are much lower than the SEs reported from the <code>lsmeans</code> output.</p>

<pre><code>$lsmeans
 ageGroup     lsmean        SE df  asymp.LCL asymp.UCL
 young    0.01043529 0.2466025 NA -0.4728967 0.4937673
 old      0.37467465 0.2474967 NA -0.1104101 0.8597594

$lsmeans
 filter                     lsmean        SE df   asymp.LCL asymp.UCL
 cropDesat37           -0.01699274 0.2462214 NA -0.49957782 0.4655923
 cropAdaptiveThreshold  0.40210268 0.2478032 NA -0.08358268 0.8877880
</code></pre>

<p>So it would be: 
$$
\displaystyle \begin{array}{l}
    \bar{X_1}-\bar{X_2}= lsmean\_of\_cropAdaptiveThreshold - lsmean\_of\_old \\
= 0.40210268 - 0.37467465 = 0.0274\\ \\ 
SE_{\bar{X_1}-\bar{X_2}}= \sqrt{SE_1^2 + SE_2^2 + 2*cov}\\ 
= \sqrt{0.075906^2 + 0.076379^2 + 2*-3.225794e-05}\\ 
= 0.10798
\end{array}
$$</p>

<p>...which is the same SE as the SE in the beta-contrast.</p>

<p>Is it legal to compare cross-factor level lsmeans like this?</p>
"
"0.114597970917641","0.118905167594629","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"0.143247463647051","0.148631459493287","192845","<p>I'm analyzing an experiment that has 50 subjects and 50 items. Each item can occur in two possible conditions. Every experimental subject produces a (continuous) response to every item, but only once per item (individually, they only respond to one condition per item).</p>

<p>I hypothesize the following effects:</p>

<ol>
<li>item condition should affect the mean response: higher response in one condition than the other</li>
<li>item condition should affect the variance of the response: higher variance in one condition than the other</li>
<li>there is idiosyncratic variation between subjects: some subjects produce a higher response than others, regardless of item or condition. In addition, some subjects are more sensitive to the item condition than others.</li>
<li>there is idiosyncratic variation between items: some items get higher responses than others, regardless of condition. In addition, some items are more sensitive to the item condition than others.</li>
</ol>

<p>I want to model this with a linear mixed-effects model, and test the significance of (1) and (2). If I were using <code>lme4</code>, I would specify a model like this, with a fixed effect of <code>condition</code>, plus random intercepts and <code>condition</code> slopes for subjects and for items.</p>

<pre><code>lmer(response ~ condition + (1 + condition | subject) + (1 + condition | item))
</code></pre>

<p>As far as I can tell, there's no way to test (2) with <code>lme4</code>. If someone can suggest a straightforward way to test (2) while also accounting for the other components, that would be very helpful!</p>

<p>This is what I've been able to do so far:</p>

<p><a href=""http://stats.stackexchange.com/questions/171187/comparing-the-within-subject-variance-between-two-groups-of-subjects"">This question</a> suggests how to use <code>nlme</code> to allow the variance to differ between fixed factor levels. I found via googling that I can specify crossed random effects with a hack using <code>pdBlocked</code> and <code>pdIdent</code>. My current model looks like this:</p>

<pre><code>my_data$one = rep(1, nrow(my_data))

lme(response ~ condition, 
    random=list(one = pdBlocked(list(pdIdent(~ subject - 1),
                                     pdIdent(~ item - 1)))
                      ),
     weights=varIdent(form=~1|condition),
     data=my_data)
</code></pre>

<p>However, as far as I can tell from the model output (and comparison to lme4), this includes random intercepts but not slopes for condition, so requirements (3) and (4) of the model aren't fully satisfied. If you know how to specify condition slopes using the nlme syntax, that would also answer this question.</p>
"
"0.104613156193188","0.108545070825851","194451","<p>When I fit any model in <code>lmer()</code>, <code>summary</code> identifies a correlation between fixed effect(s) and the (fixed) intercept.  How should I interpret this, and why is it not $0$? In my way of thinking any correlation between the intercept and a fixed effect should be zero, since, in the design matrix $X$, the intercept term is just a column of $1$s, and therefore constant. There is a related post on this topic (<a href=""http://stats.stackexchange.com/questions/49082/lmer-interpretation-of-correlation"">lmer interpretation of correlation</a>), but it doesn't help me much.  Below is model <code>summary</code> output, so you can see what I mean. See the block labeled <code>Correlation of Fixed Effects:</code>, at the very bottom.</p>

<pre><code>>summary(exp2modFull)
Linear mixed model fit by REML ['lmerMod']
Formula: RT_log ~ Condition + (Condition | Subject) + (Condition | Item)
   Data: exp2

REML criterion at convergence: -1978.6

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.7080 -0.5775 -0.0634  0.4801  7.8186 

Random effects:
 Groups   Name            Variance  Std.Dev. Corr       
 Subject  (Intercept)     0.0261472 0.16170             
          Conditionoddman 0.0028821 0.05369  -0.21      
          Conditionhetero 0.0037356 0.06112  -0.46  0.80
 Item     (Intercept)     0.0018914 0.04349             
          Conditionoddman 0.0002885 0.01699  -0.97      
          Conditionhetero 0.0010140 0.03184  -0.65  0.81
 Residual                 0.0320147 0.17893             
Number of obs: 3600, groups:  Subject, 20; Item, 12

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      6.583310   0.038622  170.46
Conditionoddman -0.009109   0.014883   -0.61
Conditionhetero  0.021487   0.018018    1.19

Correlation of Fixed Effects:
            (Intr) Cndtnd
Conditnddmn -0.309       
Conditinhtr -0.472  0.722</code></pre>
"
"0.0640622132638473","0.0664700094043992","194569","<p>What I have is a generalized linear mixed model of the log OR of a rater (random effect)  giving a response above a certain level on an ordinal scale, given a specification of what the rater was presented with (acoustic parameters of the stimulus). Now, the residuals look fine, but I cat wrap my head around the residuals v.s. fitted values plot. Presumably, heteroscedasticity should to should not be a huge problem for in a logit model, and the results are more or less equidistant across the range of predicted values, but why the odd shape? </p>

<p><a href=""http://i.stack.imgur.com/X64Za.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X64Za.png"" alt=""Residuals vs. fitted values plot for logit GLMM""></a></p>

<p>Is this what you usually see (this is my first logit GLMM)? Is this due to the link function,or what could it be? </p>

<p>You can download the model here (R save format) <a href=""https://www.dropbox.com/s/rkygt5a8eat8qpc/gv.glmer02.rda?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/rkygt5a8eat8qpc/gv.glmer02.rda?dl=0</a></p>

<p>if you want to play with it.</p>
"
"0.0827039616973562","0.0858124131484961","194837","<p>I am having trouble using the correct test and r code for my experiment. Essentially I measured insect emergence daily from artificial streams with 3 treatments.:</p>

<pre><code>CONTROL - With 5 replicate streams  
TREAT 1 - With 5 Replicate streams  
TREAT 2 - With 5 Rep streams.  
TREAT 2- With 5 rep streams  
</code></pre>

<p>I think what I basically want to do is this: Emergence = Treatment + Day + Treatment*Day</p>

<p>Looking for an effect of treatment on insect emergence over time (day) </p>

<p>Update, I have just ran the model below, but it seems to be dropping a treatment group?</p>

<p>Ran this model:</p>

<pre><code>&gt; model6 &lt;- lmer(Emerg ~ Day + Treatment + Day:Treatment + (Day | Stream), insect)
&gt; summary(model6)

Linear mixed model fit by REML ['lmerMod']
Formula: 
Emerg ~ Day + Treatment + Day:Treatment + (Day | Stream)
   Data: insect

REML criterion at convergence: 2070.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.0321 -0.4694 -0.0445  0.3883  4.8618 

Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 Stream   (Intercept) 672.73598 25.9372      
          Day           0.01573  0.1254  1.00
 Residual             594.41059 24.3805      
Number of obs: 224, groups:  Stream, 16

Fixed effects:
                        Estimate Std. Error t value
(Intercept)              68.7060    14.6813   4.680
Day                      -0.7632     0.8106  -0.941
TreatmentControl        -26.5467    20.7625  -1.279
TreatmentFluoxetine     -14.0357    20.7625  -0.676
TreatmentMix            -15.0879    20.7625  -0.727
Day:TreatmentControl      0.6181     1.1464   0.539
Day:TreatmentFluoxetine   1.5500     1.1464   1.352
Day:TreatmentMix          1.3808     1.1464   1.204

Correlation of Fixed Effects:
            (Intr) Day    TrtmnC TrtmnF TrtmnM Dy:TrC Dy:TrF
Day         -0.343                                          
TrtmntCntrl -0.707  0.243                                   
TrtmntFlxtn -0.707  0.243  0.500                            
TreatmentMx -0.707  0.243  0.500  0.500                     
Dy:TrtmntCn  0.243 -0.707 -0.343 -0.172 -0.172              
Dy:TrtmntFl  0.243 -0.707 -0.172 -0.343 -0.172  0.500       
Dy:TrtmntMx  0.243 -0.707 -0.172 -0.172 -0.343  0.500  0.500
&gt; 
</code></pre>
"
"0.219499219232481","0.22774915856482","197435","<p>This is my first time posting. I hope I've included an appropriate amount of info. I have many questions, but try to highlight the obstacles I've been facing.</p>

<p>I am trying to run three GLMMs in R (3.2.2) on three separate response variables (various measures of sociality between pairs of animals), but with the same set of fixed effects and interactions. Two of the response variables are integers, but the other is a continuous numeric. The response variables are:</p>

<ol>
<li>traveling together- out of all the times I followed my subject, how many of those times were the other individuals present (<strong>Travel</strong>)</li>
<li>proximity- out of all the instantaneous scans to see who was within 5m of my subject, how often was that other individual within 5m (<strong>Within_Five</strong>) </li>
<li>touching- out of all the hours/minutes I watched my subject how many minutes spent touching (<strong>total_touch</strong>)</li>
</ol>

<p>The fixed effects are features of the pair, such as age difference (<strong>Age_Diff</strong>) and whether the partner is the <strong>mother</strong>, <strong>brother</strong>, or <strong>cousin</strong>. I want to have age of the subject (<strong>subject_age</strong>) as an interaction as the social behavior may change with age.</p>

<p>Given Subjects (a subset of the population that I observed) and Partners (anyone in the population who they may be interacting with, including ) are repeated, I am treating both as random effects, e.g. (1 | Partner). </p>

<pre><code>&gt; str(dat)
'data.frame':   954 obs. of  29 variables:
 $ Pair          : Factor w/ 954 levels ""Abrams_Barron"",..: 159 268 269 378     700 334 601 920 179 75 ...
 $ Subject       : Factor w/ 18 levels ""Abrams"",""Barron"",..: 3 6 6 8 14 7 12 18 4 2 ...
  $ Partner       : Factor w/ 54 levels ""Abrams"",""Barron"",..: 54 3 4 7 11 16    18 19 21 23 ...
 $ mother        : Factor w/ 2 levels ""NoMom"",""Mom"": 1 1 1 1 1 1 1 1 1 1 ...
 $ brother       : Factor w/ 2 levels ""NoBro"",""bro"": 2 2 2 2 2 2 2 2 2 2 ...
 $ cousin        : Factor w/ 2 levels ""NoCuz"",""cuz"": 2 1 1 1 1 1 1 1 1 1 ...
 $ subject_age   : num  14.3 17.7 17.7 18.7 19.7 ...
 $ partner_age   : num  8.67 41.69 31.69 12.39 24.68 ...
 $ Age_Diff      : num  -5.59 24.01 14.01 -6.29 5 ...
 $ Total_Scans   : int  314 309 309 313 289 314 310 321 305 283 ...
 $ Total_Hours   : num  43.2 44.3 44.3 45.1 40.9 ...
 $ Total_Minutes : num  2593 2656 2656 2707 2456 ...
 $ Total_Follows : int  45 46 46 47 43 48 46 48 46 45 ...
 $ Within_Five   : int  9 13 12 20 26 10 6 4 30 9 ...
 $ total_touch   : num  0 0 1.77 6.19 31.07 ...
 $ Touch_Rate_Min: num  0 0 0.000667 0.002287 0.012649 ...
 $ Touch_Rate    : num  0 0 0.04 0.137 0.759 ...
 $ Travel        : int  25 28 27 24 30 21 23 5 26 17 ...
 $ Travel_Rate   : num  0.556 0.609 0.587 0.511 0.698 ...
</code></pre>

<p>I have used lme4 in the past, but since some of my response variables have lots of zeros, it seems appropriate to run a zero-inflation model, and I have been trying glmmADMB. With some of the models, I have been getting warning messages.</p>

<p>For the Travel model, I don't think I need a zero-inflation model, but I get a warning when I run a poisson model with glmer</p>

<pre><code>&gt; travel.poisson.FULL2 &lt;- glmer(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = poisson)
Warning messages:
1: Some predictor variables are on very different scales: consider rescaling 
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
&gt; summary(travel.poisson.FULL2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Travel ~ subject_age * brother + subject_age * Age_Diff + subject_age *      cousin + subject_age * mother + log(Total_Follows) + (1 |  
    Subject) + (1 | Partner)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
  8989.2   9052.4  -4481.6   8963.2      941 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5702 -1.7767 -0.3958  0.9003 16.4475 

Random effects:
 Groups  Name        Variance Std.Dev.
 Partner (Intercept) 0.25078  0.5008  
 Subject (Intercept) 0.01283  0.1133  
Number of obs: 954, groups:  Partner, 54; Subject, 18

Fixed effects:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -4.9997116  1.7689865  -2.826  0.00471 ** 
subject_age             0.0246246  0.0138692   1.775  0.07582 .  
brotherbro              1.2107468  0.4163344   2.908  0.00364 ** 
Age_Diff                0.0226314  0.0086427   2.619  0.00883 ** 
cousincuz               1.0076641  0.3756843   2.682  0.00731 ** 
motherMom               1.9488415  0.6762811   2.882  0.00396 ** 
log(Total_Follows)      1.7389194  0.4464744   3.895 9.83e-05 ***
subject_age:brotherbro -0.0362557  0.0260557  -1.391  0.16408    
subject_age:Age_Diff   -0.0002216  0.0003799  -0.583  0.55972    
subject_age:cousincuz  -0.0635028  0.0236412  -2.686  0.00723 ** 
subject_age:motherMom  -0.1105007  0.0413857  -2.670  0.00758 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) sbjct_ brthrb Ag_Dff cosncz mthrMm l(T_F) sbjct_g:b s_:A_D sbjct_g:c
subject_age -0.354                                                                     
brotherbro  -0.014  0.058                                                              
Age_Diff    -0.071  0.443 -0.001                                                       
cousincuz   -0.007  0.095  0.011  0.109                                                
motherMom   -0.001  0.000  0.023 -0.126 -0.004                                         
lg(Ttl_Fll) -0.990  0.227  0.006  0.001 -0.006  0.002                                  
sbjct_g:brt  0.015 -0.056 -0.988  0.005 -0.012 -0.025 -0.008                           
sbjct_g:A_D  0.027 -0.192  0.008 -0.714 -0.150  0.168 -0.002 -0.013                    
sbjct_g:csn  0.006 -0.092 -0.012 -0.106 -0.988  0.004  0.006  0.013     0.147          
sbjct_g:mtM  0.004 -0.002 -0.024  0.123  0.003 -0.991 -0.004  0.028    -0.169 -0.003   
fit warnings:
Some predictor variables are on very different scales: consider rescaling
convergence code: 0
Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>Whereas, when I run with glmmadmb, I don't get a warning</p>

<pre><code>&gt; travel.poisson.FULL &lt;- glmmadmb(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = F)
&gt; summary(travel.poisson.FULL)

Call:
glmmadmb(formula = Travel ~ subject_age * brother + subject_age * 
    Age_Diff + subject_age * cousin + subject_age * mother + 
    log(Total_Follows) + (1 | Subject) + (1 | Partner), data = dat, 
    family = ""poisson"", zeroInflation = F)

AIC: 7430.9 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)            -9.52e+00   6.32e+00   -1.50    0.132  
subject_age             3.28e-02   4.56e-02    0.72    0.471  
brotherbro              9.49e-01   4.29e-01    2.21    0.027 *
Age_Diff                2.70e-02   1.28e-02    2.11    0.035 *
cousincuz               5.36e-02   4.27e-01    0.13    0.900  
motherMom              -1.21e+00   8.39e-01   -1.45    0.148  
log(Total_Follows)      2.75e+00   1.60e+00    1.72    0.086 .
subject_age:brotherbro -2.85e-02   2.66e-02   -1.07    0.284  
subject_age:Age_Diff    2.64e-05   4.11e-04    0.06    0.949  
subject_age:cousincuz  -3.11e-03   2.70e-02   -0.12    0.908  
subject_age:motherMom   7.76e-02   4.99e-02    1.56    0.120  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=954, Subject=18, Partner=54 
Random effect variance(s):
Error in VarCorr(x) : 
  could not find symbol ""rdig"" in environment of the generic function
</code></pre>

<p>For the touching model (which has lots of 0s, so it seemed appropriate to run a zero inflation model), I also got a warning.</p>

<pre><code>touch.poisson.FULL &lt;- glmmadmb(total_touch ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Scans) +  (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = T)

&gt;Warning messages:
1: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  non-integer response values in discrete family
2: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  Convergence failed:log-likelihood of gradient= -1.15888
</code></pre>

<p>What accounts for the differences in warnings produced by glmmadmb and glmer?</p>

<p>Some related questions:</p>

<p>Is it appropriate to put all of these effects and interactions into one model? Do these warnings and differences between packages suggest my model is unstable?</p>

<p>I have been trying to learn as much as possible about GLMMs and using them in R. I know there are a lot of different statistical approaches, but I want to ensure I am modeling my data appropriately and producing reliable results. Any and all help or advice is appreciated, and I am happy to provide more information.</p>
"
"0.104613156193188","0.108545070825851","198737","<p>I have a dataset with the following variables:</p>

<ul>
<li>proportion of species present, between 0 and 1 (called speciesProp)</li>
<li>a binomial (0,1) presence/absence of the same species (called PA in the model)</li>
<li>year</li>
</ul>

<p>The dataset has many 0s in the proportion and binomial columns. These are actual 0 values collected in the field.</p>

<p>I want to know if the proportion of species is increasing over the year (controlling for random effects)</p>

<p>I logit transformed my proportional data, and then originally I thought of running a linear mixed effects model in lme4 as follows:</p>

<pre><code>m01 &lt;- lmer(speciesPropLOGIT ~ year + (1|referenceID), data = speciesAll)
</code></pre>

<p>But then wondered if the following was more appropriate:
1) a logistic model of the binomial presence / absence data as follows:</p>

<pre><code>model &lt;- glm(PA ~ year , family = binomial(link = ""cloglog""), data = speciesAll)
</code></pre>

<p>followed by the following linear mixed effects model, where the proportion is the response variable and excluding the 0s:</p>

<pre><code>m01 &lt;- lmer(speciesPropLOGIT ~ year + (1|referenceID), data = speciesAll)
</code></pre>

<p>Someone suggested that I think about model multiplication, of the two outputs, the first estimating the proportion of species over year with the 1s and 0s, and the second looking at the positive data over time. I then would like to plot one line of model fit.</p>

<p>When i run the models separately, year comes out as significant in all of them.</p>

<p>I have spent a long time looking for advice on how to do it, but can't seem to find any. </p>

<p>Also - do I need to have family = binomial somewhere if I have logit transformed the proportional data?</p>

<p>Hope you can help?</p>
"
"0.0661631693578849","0.0858124131484961","201250","<p>I have the following <code>lmer()</code> syntax:</p>

<pre><code> lmer(Y ~ A*B*C + COV + A:COV + B:COV + C:COV + (1|A:BLOCK))
</code></pre>

<p>where <code>A</code>, <code>B</code> and <code>C</code> are the fixed effects and <code>COV</code> is a continuous covariate. In a manuscript I am writing, I explained that I am using a linear mixed effects model with <code>A</code>, <code>B</code> and <code>C</code> as the fixed effects, <code>COV</code> as the covariate, also used in interactions with <code>A</code>, <code>B</code> and <code>C</code>. I also modelled a random intercept for every observed combination of <code>A</code> and <code>BLOCK</code> to account for variation in blocks. I also list the outputs and <code>lmer</code> model description in the appendix. The reviewer however, want the model to be described in a ""conventional way"" in the text. I am not 100% sure what the reviewer means by  ""conventional"" but I am assuming he/she refers to the mathematical representation of the <code>lmer</code> syntax above? </p>

<p>Alternatively, I was thinking something along this way (but I am not sure how to add the covariate and the random term, see the ""$\dots$""):</p>

<p>$$Y_{ijk}=\mu+A_i+B_j+C_k+(AB)_{ij}+(AC)_{ik}+(BC)_{jk}+ ... +\epsilon_{ijk}$$</p>

<p>I appreciate any help regarding the translation of this <code>lmer</code> syntax.</p>
"
"0.0523065780965941","0.0542725354129257","201800","<p>I have the following linear mixed effects model I am running in the package lme4 in R:</p>

<pre><code>lmer(speciesPerc ~ year + season + (1|refID), data = df)
</code></pre>

<p>I want to include country in my model testing if possible, but my data isn't very well distributed between countries. Country is not the main question in my analysis, and I already have a studyID as a random effect. </p>

<p>speciesPerc per country is distributed between the country levels as follows:</p>

<pre><code>                Botswana                     Cameroon     Central African Republic 
                       2                           31                           11 
                   Congo Democratic Republic of Congo            Equatorial Guinea 
                      12                           27                            5 
                   Gabon                        Ghana                      Liberia 
                       5                            5                            4 
              Mozambique                      Nigeria                     Tanzania 
                       1                            3                           38 
                  Uganda                     Zimbabwe 
                       4                            1 
</code></pre>

<p>My question is - can I / should I include country as a fixed or random effect, if at all?</p>

<p>Cheers.</p>
"
"0.0986302295273746","0.102337274193778","202781","<p>I do have a 2 level data set with 3 observations nested in one person. I am fitting a mixed model including 71 predictors and 28 random slopes in the following manner:</p>

<pre><code>model = lmer(var1 ~ a + b + c + (1|PersID) + (0+a|PersID) + (0+b|PersID) + (0+c|PersID)
</code></pre>

<p>I am using the step() function of the lmerTest package to do backwards elimination of random and fixed effects of the model and get the following model:</p>

<pre><code>Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: Fufaksc1 ~ MusZ + HauZ + ArbZ + SpoZ + AusZ + TraZ + AAYYZ +  
AMYZ + AMSZ + TNAZ + AuZ + MusM + HauM + EmoM + SpoM + TraM +  
AMNM + WSM + AuM + SAE + Ex + RE + So + UC + (1 | PersID) +  
(0 + HauZ | PersID) + (0 + SpoZ | PersID) + (0 + VaZ | PersID) + (0 + AuZ | PersID) Data: Sitsort2

REML criterion at convergence: 2703.1

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-3.6274 -0.4721  0.0070  0.4884  4.1122 

Random effects:
 Groups   Name        Variance Std.Dev.
 PersID   (Intercept) 0.259072 0.50899 
 PersID.1 HauZ        0.088366 0.29726 
 PersID.2 SpoZ        0.285073 0.53392 
 PersID.3 VaZ         0.008581 0.09263 
 PersID.4 AuZ         0.008177 0.09043 
 Residual             0.209756 0.45799 
Number of obs: 1300, groups:  PersID, 555

Fixed effects:
             Estimate Std. Error        df t value Pr(&gt;|t|)    
(Intercept)  -1.00698    0.19540 515.50000  -5.154 3.65e-07 ***
MusZ          0.55662    0.07930 627.30000   7.019 5.80e-12 ***
HauZ         -0.26976    0.06780 456.30000  -3.979 8.06e-05 ***
ArbZ          0.30543    0.07223 704.20000   4.229 2.66e-05 ***
SpoZ         -0.45474    0.09816 130.70000  -4.633 8.62e-06 ***
AusZ          0.17835    0.08102 653.20000   2.201 0.028067 *  
TraZ         -0.10944    0.05563 648.90000  -1.967 0.049574 *  
AAYYZ        -0.20878    0.05301 769.00000  -3.939 8.93e-05 ***
AMYZ          0.10063    0.04699 767.70000   2.141 0.032550 *  
AMSZ          0.80907    0.16299 501.90000   4.964 9.49e-07 ***
TNAZ         -0.09069    0.04457 719.90000  -2.035 0.042261 *  
AuZ           0.06479    0.01266 455.80000   5.118 4.57e-07 ***
MusM          0.49393    0.18577 522.40000   2.659 0.008082 ** 
HauM         -0.47557    0.14806 517.50000  -3.212 0.001401 ** 
EmoM         -0.63551    0.30854 490.60000  -2.060 0.039950 *  
SpoM         -0.88607    0.20915 519.20000  -4.236 2.69e-05 ***
TraM         -0.35308    0.14697 513.00000  -2.402 0.016645 *  
AMNM         -0.41692    0.20316 530.50000  -2.052 0.040644 *  
WSM           0.04994    0.01685 530.80000   2.964 0.003172 ** 
AuM           0.09425    0.02577 527.50000   3.657 0.000281 ***
SAE           0.15065    0.02430 515.10000   6.198 1.17e-09 ***
Ex           -0.05206    0.02576 519.20000  -2.021 0.043836 *  
RE            0.07022    0.02686 514.70000   2.614 0.009201 ** 
So            0.09297    0.02611 523.20000   3.561 0.000404 ***
UC           -0.11635    0.02710 510.80000  -4.293 2.11e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The model contains 5 significant random effects including the intercept. The only thing I do not understand is why there is a significant random effect for VaZ but no significant fixed effect for VaZ included in the model. As far as I understand this it would only make sense if the fixed effect would be exact 0 (which is realisticly impossible). From my point of view it does not make sense to include a random effect without an equivalent fixed effect. Can anyone explain this to me or is it a bug in the step() function? </p>

<p>Thanks in advance! </p>
"
"0.189077005474193","0.210196625810714","203295","<p>I want to use linear mixed effects models (<code>lme4::lmer</code>) for the selection of features (dependent variables, >1000) on significant differences between specific groups (combinations of independent variables). Therefore I would make a model for each feature (gaining over 1000 models). I then look for significant differences by using contrasts in a posthoc test (<code>multcomp::glht</code>). My question is: given my data does it make sense to use LME models or is a repeated measure ANOVA more appropriate from statistical point of view? And if appropriate which of the models below (if any) should I use? Am I using the right grouping variables and reference levels? </p>

<p>My example data (included at the end of this post) with a representative feature (out of >1000) (<em>Mrkr</em>) looks as follows: The dependent variable (<em>Mrkr</em>) is a continuous variable.</p>

<p>As for the independent variables I have two different species (<em>Spcs</em>: C450, DX20) that undergo two different challenges (<em>Chal</em>: mck, inf). A sample is taken from each subject (<em>Subj</em>: B1 ... B10, D1 ... D10) at different time points (<em>Day</em>: day 0 ... day 30, but differs between species) after the challenge. This let me to believe that I have a random effect for the species since we are following the effect of the challenge trough time on each individual subject.</p>

<p>Because I am firstly interested in the differences between challenges for the same time points for each individual species I constructed a new group (Grp) which is a combination of <em>Chal</em>, <em>Spcs</em> and <em>Day</em> (<em>Grp</em>: I_C_00...M_D_05). </p>

<p>The model I initially used to select features is as follows:</p>

<pre><code>library(""lme4"")
library(""multcomp"")
library(""ggplot2"")

#mydata and K are defined at the end of this post for clarity
mydata$Grp &lt;- relevel(mydata$Grp, ref = ""M_C_03"")   #Set reference level.
mod01 &lt;- lmer(Mrkr ~ Grp + (1|Subj), data = mydata) #Make model with random intercept for Subj.
summary(glht(mod01, linfct = K))                    #Gives the p vals for my specified contrasts (yes, I am aware of the dangers of using p values).

#plots time courses for each individual subject for visualization
ggplot(mydata, aes(x = Day, y = Mrkr)) + 
  geom_point() + 
  geom_line(aes(group = 1)) + 
  facet_wrap(Spcs  + Chal ~  Subj , nrow = 4, ncol = 5) +
  theme(axis.text.x = element_text(angle = 90))
</code></pre>

<p>First I set the benchmark level at the C450/mck/day03 because this is the only time point that all <em>Spcs</em> and <em>Chal</em> combinations have in common. I don't know if this is necessary but intuitively it seemed the right thing to do. If I understand the literature correctly <code>mod01</code> allows for a random intercept for each individual subject. The contrasts were checked with <code>glht()</code> and contrast matrix <code>K</code> (included at the end of this post).</p>

<p>However, after some contemplation and wrestling trough the literature I figured that slopes of the effects might well be dependent on species and challenge and so I came up with the next model:</p>

<pre><code>mod02 &lt;-  lmer(Mrkr ~ Grp + (1 + Spcs|Subj) + (1 + Chal|Subj), data = mydata)
summary(glht(mod02, linfct = K))
</code></pre>

<p>Finally I realized that the subjects are nested in species so my final model would be:</p>

<pre><code>mod03 &lt;-  lmer(Mrkr ~ Grp + (1 + Spcs|Subj/Spcs) + (1 + Chal|Subj/Spcs), data = mydata)
summary(glht(mod03, linfct = K))
</code></pre>

<p>It turns out that the p values for the posthoc tests for all three models are very similar and by using <code>mod01</code> I found already a set of interesting features. However I want to know which model, if any, is the one that describes my random effects structure most accurate. </p>

<p>Kind regards</p>

<pre><code>#Data frame with exampel data
mydata &lt;- structure(list(Subj = structure(c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 6L, 7L, 8L, 
9L, 10L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 11L, 12L, 
13L, 14L, 15L, 11L, 12L, 13L, 14L, 15L, 11L, 12L, 13L, 14L, 15L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 16L, 17L, 18L, 
19L, 20L), .Label = c(""B1"", ""B2"", ""B3"", ""B4"", ""B5"", ""D1"", ""D2"", 
""D3"", ""D4"", ""D5"", ""B6"", ""B7"", ""B8"", ""B9"", ""B10"", ""D6"", ""D7"", 
""D8"", ""D9"", ""D10""), class = c(""ordered"", ""factor"")), Chal = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""mck"", 
""inf""), class = ""factor""), Day = structure(c(1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 
4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 3L, 
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 
6L, 6L, 6L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L), .Label = c(""day00"", 
""day03"", ""day05"", ""day08"", ""day18"", ""day30""), class = ""factor""), 
Spcs = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""C450"", 
""DX20""), class = ""factor""), Grp = structure(c(1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 
4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 
7L, 7L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 
10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 
12L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 15L, 16L, 16L, 16L, 16L, 16L), .Label = c(""I_C_00"", 
""I_C_03"", ""I_C_05"", ""I_C_08"", ""I_C_18"", ""I_C_30"", ""I_D_00"", 
""I_D_03"", ""I_D_05"", ""M_C_03"", ""M_C_05"", ""M_C_08"", ""M_C_18"", 
""M_C_30"", ""M_D_03"", ""M_D_05""), class = ""factor""), Mrkr = c(2399.849218, 
1762.777866, 1774.939084, 1461.419699, 1368.804546, 1126.699114, 
1557.100579, 1369.699809, 2146.155143, 1006.337489, 856.6567507, 
856.775057, 683.6396713, 459.4223325, 651.9368177, 276.29906, 
559.5347751, 294.9815688, 304.0486838, 325.3924639, 814.1927642, 
1424.429248, 949.7589963, 1469.905312, 1319.214754, 1268.595709, 
1184.70564, 870.8718067, 682.4456494, 1177.223394, 512.4325239, 
360.1808537, 525.5669889, 488.9804713, 541.2128606, 1475.036591, 
1132.173062, 1256.048921, 1843.616592, 1892.594627, NA, NA, 
NA, NA, 1100.36921, 1566.125524, 720.1838491, 930.9203894, 
1069.445235, 866.415662, 1021.757551, 1310.491871, 1459.588906, 
1081.572941, 871.4666637, 511.329317, 1010.567794, 513.5011174, 
1005.356367, 734.6804492, 1144.873026, 1467.333437, 1496.635963, 
1519.662963, 1105.464233, 916.0012586, 1248.81632, 591.8699979, 
887.1439846, 610.6604304, 376.610192, 317.2069945, 479.5381028, 
279.0847122, 410.3471923, 491.626902, 331.8743751, 632.6303274, 
588.0827988, 513.1653612)), .Names = c(""Subj"", ""Chal"", ""Day"", 
""Spcs"", ""Grp"", ""Mrkr""), row.names = c(NA, -80L), class = ""data.frame"")

#Contrast matrix for glht()
K &lt;- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 
0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1), .Dim = c(11L, 
16L), .Dimnames = list(c(""I_C_00 vs I_C_03"", ""M_C_03 vs I_C_03"", 
""M_C_05 vs I_C_05"", ""M_C_08 vs I_C_08"", ""M_C_18 vs I_C_18"", ""M_C_30 vs I_C_30"", 
""I_D_00 vs I_D_03"", ""M_D_03 vs I_D_03"", ""I_C_00 vs I_D_00"", ""M_C_03 vs M_D_03"", 
""M_C_05 vs M_D_05""), c(""M_C_03"", ""I_C_00"", ""I_C_03"", ""I_C_05"", 
""I_C_08"", ""I_C_18"", ""I_C_30"", ""I_D_00"", ""I_D_03"", ""I_D_05"", ""M_C_05"", 
""M_C_08"", ""M_C_18"", ""M_C_30"", ""M_D_03"", ""M_D_05"")))
</code></pre>
"
"0.0640622132638473","0.0664700094043992","203717","<p>I am trying to do model simplification looking at how different factors may affect distance. So I have snails kept in several habitats and I wanted to see if that affects how closely another snail may follow that snail. So I start off with this model: </p>

<pre><code>  model1 &lt;- lmer(sqrt(dist+6)~  (1|snail)+food+stress+food:stress+
       weight+OriginalL+FollowedL)
summary(model1)
</code></pre>

<p>and the summary is this: </p>

<pre><code>  Linear mixed model fit by REML ['lmerMod']
  Formula: sqrt(dist + 6) ~ (1 | snail) + food + stress + food:stress +  
weight + OriginalL + FollowedL

REML criterion at convergence: 561.1

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2941 -0.7698 -0.3347  0.7515  1.9564 

Random effects:
 Groups   Name        Variance Std.Dev.
 snail    (Intercept) 0.000    0.000   
 Residual             2.334    1.528   
Number of obs: 148, groups:  snail, 37

Fixed effects:
                               Estimate Std. Error t value
(Intercept)                    4.960927   0.662947   7.483
foodSweetPotato               -0.219039   0.357768  -0.612
stressshelter                 -0.246649   0.355999  -0.693
weight                         0.002520   0.063259   0.040
OriginalL                      0.015549   0.013072   1.189
FollowedL                     -0.008044   0.005972  -1.347
foodSweetPotato:stressshelter -0.300143   0.503215  -0.596

Correlation of Fixed Effects:
            (Intr) fdSwtP strsss weight OrgnlL FllwdL
foodSwetPtt -0.309                                   
stressshltr -0.315  0.502                            
weight      -0.615  0.008  0.009                     
OriginalL   -0.617 -0.021  0.032  0.123              
FollowedL   -0.470  0.118  0.059  0.087 -0.004       
fdSwtPtt:st  0.230 -0.707 -0.708 -0.008 -0.024 -0.055
</code></pre>

<p>Should I remove the least significant factor or remove the interactions first?</p>

<p>And after this is it a simple anova between my first model and most simplified model?</p>
"
"0.0369863360727655","0.0383764778226668","204146","<p>I am running a negative binomial generalised linear mixed model - glmer.nb()from the {lmer} package - to investigate the extent to which elevation (elev) can predict changes in the density of herbaceous plants (herb_den), with site (site) as a random effect to account for heteroscedasticity due to different sampling efforts across elevation:</p>

<pre><code>tmp_glmer.nb &lt;- glmer.nb(herb_den ~ elev + (1|site))
</code></pre>

<p>For basic linear mixed models I can use: </p>

<pre><code>r.squaredGLMM(lmer(tmp_lmer))
</code></pre>

<p>from the {MuMIn} package to generate a pseudo-r-squared value to demonstrate goodness of fit of the model.</p>

<p>Can I do something similar for glmer.nb()?</p>
"
"0.177380236469607","0.168043024522122","204741","<p>I'm analyzing a data set using a mixed effects model with one fixed effect (condition) and two random effects (participant due to the within subject design and pair). The model was generated with the <code>lme4</code> package: <code>exp.model&lt;-lmer(outcome~condition+(1|participant)+(1|pair),data=exp)</code>. </p>

<p>Next, I performed a likelihood ratio test of this model against the model without the fixed effect (condition) and have a significant difference. There are 3 conditions in my data set so <strong>I want to do a multiple comparison but I am not sure which method to use</strong>. I found a number of similar questions on CrossValidated and other forums but I am still quite confused. </p>

<p>From what I've seen, people have suggested using</p>

<p><strong>1.</strong> The <code>lsmeans</code> package - <code>lsmeans(exp.model,pairwise~condition)</code> which gives me the following output:</p>

<pre><code>condition     lsmean         SE    df  lower.CL  upper.CL
 Condition1 0.6538060 0.03272705 47.98 0.5880030 0.7196089
 Condition2 0.7027413 0.03272705 47.98 0.6369384 0.7685443
 Condition3 0.7580522 0.03272705 47.98 0.6922493 0.8238552

Confidence level used: 0.95 

$contrasts
 contrast                   estimate         SE    df t.ratio p.value
 Condition1 - Condition2 -0.04893538 0.03813262 62.07  -1.283  0.4099
 Condition1 - Condition3 -0.10424628 0.03813262 62.07  -2.734  0.0219
 Condition2 - Condition3 -0.05531090 0.03813262 62.07  -1.450  0.3217

P value adjustment: tukey method for comparing a family of 3 estimates 
</code></pre>

<p><strong>2.</strong> The <code>multcomp</code> package in two different ways - using <code>mcp</code> <code>glht(exp.model,mcp(condition=""Tukey""))</code> resulting in</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = outcome ~ condition + (1 | participant) + (1 | pair), 
    data = exp, REML = FALSE)

Linear Hypotheses:
                             Estimate Std. Error z value Pr(&gt;|z|)  
Condition2 - Condition1 == 0  0.04894    0.03749   1.305    0.392  
Condition3 - Condition1 == 0  0.10425    0.03749   2.781    0.015 *
Condition3 - Condition2 == 0  0.05531    0.03749   1.475    0.303  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>and using <code>lsm</code> <code>glht(exp.model,lsm(pairwise~condition))</code> resulting in</p>

<pre><code>Note: df set to 62

     Simultaneous Tests for General Linear Hypotheses

Fit: lmer(formula = outcome ~ condition + (1 | participant) + (1 | pair), 
    data = exp, REML = FALSE)

Linear Hypotheses:
                             Estimate Std. Error t value Pr(&gt;|t|)  
Condition1 - Condition2 == 0 -0.04894    0.03749  -1.305   0.3977  
Condition1 - Condition3 == 0 -0.10425    0.03749  -2.781   0.0195 *
Condition2 - Condition3 == 0 -0.05531    0.03749  -1.475   0.3098  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>As you can see, the methods give different results. This is my first time working with R and stats so something might be going wrong but I wouldn't know where. My questions are:</p>

<p>What are the differences between the presented methods? I read in an answer to a related questions that it's about the degrees of freedom (<code>lsmeans</code> vs. <code>glht</code>).
<strong>Are there some rules or recommendations when to use which one, i.e., method 1 is good for this type of data set/model etc.?</strong> <strong>Which result should I report?</strong> Without knowing better I'd probably just go and report the highest p-value I got to play it safe but it would be nice to have a better reason. Thanks</p>
"
"0.105264957864947","0.10922137064511","205151","<p>I am using a generalized Linear Mixed-Effects model to look at the effects of different treatments on a density of trichomes.</p>

<p>The model is :</p>

<pre><code>fitPoisson = glmer(Count_trichomes ~ Treatment1*Treatment2*Treatment3 + 
                         (1 | Block/Code) + offset(log(Length)), family=poisson(), data=dataset)
</code></pre>

<p>Treatment 1 and 2 has 2 levels (0 and 1) and Treatment 3 has 3 levels (0,1,2). Block accounts for the replicates and Code, for each individual. Length is in cm.</p>

<p>An anova(fitPoisson) told me that treatments 1 and 3 are significant and that there is no interactions. What I want now is to know what the density is for each level of treatments.</p>

<p>So I used a lsmeans to look at the differences : </p>

<pre><code>    &gt; lsmeans(fitPoisson, ~ Treatment1)

     Treatment1   lsmean         SE df asymp.LCL asymp.UCL
     0           5.309106 0.06113705 NA  5.189280  5.428933
     1           5.471452 0.06114033 NA  5.351619  5.591285

     Results are averaged over the levels of: Treatment2, Treatment3
     Results are given on the log (not the response) scale. 
     Confidence level used: 0.95
</code></pre>

<p>I can see that the density of level 0 is lower than the density of level 1, but I dont understand what are the units used. It doesn't seems like it is for trichomes/cm, since the mean for level 0 is 107 trichomes/cm and the mean for level 1 is 131 trichomes/cm (calculated in excel).</p>

<p>When I transform back from the log scale, it gives me : </p>

<pre><code>    &gt; summary(lsmeans(fitPoisson, ~ Treatment1), type = ""response"")

     Treatment1   rate       SE df asymp.LCL asymp.UCL
     0           202.1694 12.36004 NA  179.3393  227.9058
     1           237.8053 14.53949 NA  210.9496  268.0799

    Results are averaged over the levels of: Treatment2, Treatment3 
    Confidence level used: 0.95 
    Intervals are back-transformed from the log scale 
</code></pre>

<p>Which is still far from the means I found in excel.</p>

<p>Maybe I just don't understand the information lsmeans is giving me, or I am not using the right function.</p>
"
"0.0905976508333704","0.0940027887907685","207101","<p>I have the following linear mixed effects model:</p>

<pre><code>m01 &lt;- lmer(man ~ year + (1|refID/stuID) + (1|country), data = hunt, weights = harv)
</code></pre>

<p>I am trying to plot a model line using the predict() function, and have the following code:</p>

<pre><code>pframe &lt;- data.frame(year= seq(1972,2014))

pframe$man &lt;- predict(m01,re.form=NA, newdata=pframe, type = ""response"")

mm &lt;- model.matrix(terms(m01),pframe)

pvar1 &lt;- diag(mm %*% tcrossprod(vcov(m01),mm))

cmult &lt;- 1.96

pframe &lt;- data.frame(
  pframe
  , plo = pframe$man-cmult*sqrt(pvar1)
      , phi = pframe$man+cmult*sqrt(pvar1) 
)
</code></pre>

<p>...and then my plotting code.</p>

<p>I can get predict() to work when plotting a line, however it does not include the information from the random effects, so it estimates the effect at more than the model predicts.</p>

<p>Does anybody know how I can plot the model fit line to include the random effects?</p>

<p>I think the edit would need to come in this line:</p>

<pre><code>pframe$man &lt;- predict(m01,re.form=NA, newdata=pframe, type = ""response"")
</code></pre>

<p>Hope someone can help - I am having a lot of trouble figuring this one out!</p>

<p>Cheers!</p>
"
"0.156919734289782","0.162817606238777","207395","<p>I have bird nesting data and I am trying to see whether the nest treatment has any significant effects on the survival of the nestling. My original data set is relatively small (n=101). The response variable is binomial (No treatment = 0,  treatment = 1) as is the fixed effect (survived = 1, died = 0). </p>

<p>A copy of my original data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZed1pwMFh4ekhGV00/view?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p>I have obtained the following results from my model:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood  (Laplace Approximation)
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_original
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

  AIC      BIC   logLik deviance df.resid 
109.8    120.2    -50.9    101.8       97 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9725  0.1557  0.2853  0.3653  1.2021 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 3.2860   1.8127  
Year   (Intercept) 0.5109   0.7148  
Number of obs: 101, groups:  Nest, 39; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   1.6228     0.7258   2.236   0.0254 *
Treatment     0.9063     0.7676   1.181   0.2377  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment -0.152
</code></pre>

<p>To account for the possible influence of small sample size, I produced a bootstrap data set using the following code:</p>

<pre><code>bootstrapdata &lt;- data.frame()
for (i in 1:1000){
  boot &lt;- sample(1:nrow(Treatment_original), replace=TRUE)
  bootdata &lt;- Treatment_original[boot,]
  bootstrapdata &lt;- rbind(bootstrapdata, bootdata)
}
</code></pre>

<p>The bootstrap data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZeS3VQVHcwMmw4MkE/view?usp=sharing"" rel=""nofollow"">here</a>.</p>

<p>I then ran the above model on the bootstrap data set, which produced the following results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_bootstrap
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

 AIC      BIC   logLik deviance df.resid 
2957.0   2985.8  -1474.5   2949.0     9996 

Scaled residuals: 
  Min      1Q  Median      3Q     Max 
-3.5915  0.0001  0.0002  0.0026  2.4168 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 511.888  22.625  
Year   (Intercept)   4.251   2.062  
Number of obs: 10000, groups:  Nest, 38; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  16.0123     1.9144   8.364   &lt;2e-16 ***
Treatment     1.5813     0.1465  10.795   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment 0.009 
</code></pre>

<p>I would like to know how to interpret the bootstrap model results. Can I now say that the nest treatment had a positive effect on nestling survival?</p>

<p>The original data set showed no significant effect of nest treatment. Should I rather be using these results and adjusting the p value for false discovery rate?</p>

<p>I am unsure as to which results are correct. Should I report both results? What inferences can I make from these results? </p>
"
"0.0905976508333704","0.0783356573256404","209939","<p>I would be extremely grateful for some advice on how to correctly fit linear mixed effects models with my repeated measures design!</p>

<p>In my experiment, subjects completed a task with 3 difficulty conditions: easy, medium, and hard. In addition, I have assessed subjects' depressive symptoms on a continuous scale. The outcome measure is accuracy.</p>

<p>""Medium"" here serves as a comparison condition. I hypothesized that depressive symptom severity would moderate the impact of difficulty on accuracy, such that for individuals who are low in depressive symptoms, difficulty would have little impact on accuracy. By contrast, I hypothesized that individuals who are high in depressive symptoms would perform worse during hard rounds and better during easy rounds. Thus, I planned to examine the interaction between difficulty and depressive symptoms.</p>

<p>To accomplish this analysis, my thought was to fit linear mixed effects models using lme4 package in R -- a full model and a reduced model. Then I would implement a likelihood ratio test. I planned to model both random slopes and random intercepts for subjects.</p>

<p>Here's how I would have thought to examine the interaction of a 2-level within-subjects factor (dummy coded) and a centered continuous predictor:</p>

<pre><code>full.model &lt;- lmer(accuracy ~ dummy_difficulty * depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
reduced.model &lt;- lmer(accuracy ~ dummy_difficulty + depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
anova(reduced.model, full.model)
</code></pre>

<p>However, my difficulty factor actually has 3 levels.  Since the ""medium"" condition is the comparison condition, I created two dummy variables as follows:</p>

<blockquote>
  <p>dummy_1: easy = 1, medium = -1, hard = 0</p>
  
  <p>dummy_2: easy = 0, medium = -1, hard = 1</p>
</blockquote>

<p>But now, with the two dummy variables, I'm at a loss as to how to model random slopes and random intercepts.  Can anyone help me out?  I would really, really appreciate any advice you might have to offer!</p>
"
"0.0739726721455309","0.0767529556453336","209980","<p>I have read similar posts in this website to help me assess whether my diagnostic plots are too far away from normal and if they are showing heteroscedasticity (<a href=""http://stats.stackexchange.com/questions/182316/interpretation-of-residuals-vs-fitted-plot"">Interpretation of residuals vs fitted plot</a>, <a href=""http://stats.stackexchange.com/questions/121490/interpretation-of-plotglm-model/139624#139624"">Interpretation of plot(glm.model)</a>) and I researched other sources as well. I haven't been able to find plots that might resemble mine. </p>

<p>I have concerns about my residuals not having a normal distribution (qqplot) as they depart from the line quite a bit, but I am even more concerned about my data showing heteroscedasticity. Are my concerns well founded here?[1]</p>

<p>The plots belong to this linear mixed model with one random effect (individual id) and a fixed effect that is a three way interaction: </p>

<pre><code>lmer(log.prop.out ~ 1 + time*season*sex + (1|id), REML=FALSE, data=in.out)
</code></pre>

<p>where <code>log.prop.out</code> is the log of a proportion, whereas time, season and sex are all categorical predictors. Should I be concerned about the log transformation not being enough to normalize my response variable? </p>

<p>I am struggling to decide how bad can these plots look without raising concern.</p>

<p>Thank you very much for your kind guidance!</p>

<p><a href=""http://i.stack.imgur.com/tXpqF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tXpqF.jpg"" alt=""enter image description here""></a></p>

<p>Without the transformation, using directly the proportions, this is how the residuals vrs fitted plot looks like:</p>

<p><a href=""http://i.stack.imgur.com/DQwh1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DQwh1.jpg"" alt=""enter image description here""></a></p>
"
"0.177534413149274","0.168856502419734","210757","<p>I have repeated measures of <code>happiness</code> for a sample of participants, and a single measure of <code>satisfaction</code> for each of the participants.</p>

<p>I want to predict <code>satisfaction</code> from the repeated measures of <code>happiness</code>. To do so, I want to create a new variable, called <code>happiness.change</code>, which measures the degree of change/trend in <code>happiness</code> from the first measurement to the last, for each participant (for example, a negative <code>happiness.change</code> if there is a decrease in <code>happiness</code> over time). Then I want to predict <code>satisfaction</code> from <code>happiness.change</code>.</p>

<p>Below (using R) is an excerpt from my data (a sample of 9 participants):</p>

<pre><code>ids &lt;- c(rep(seq(1:5), each = 2), rep(6:9, each = 5))
time &lt;- c(rep(1:2, 5), rep(1:5, 4))
happiness &lt;- c(0.80,0.00,0.75,0.00,0.80,0.00,2.75,2.50,0.40,0.20,
               3.80,0.40,0.00,0.20,3.40,3.00,2.60,3.40,3.80,0.00,
               3.60,3.60,0.20,0.40,1.00,0.40,0.20,1.20,1.20,0.00)
satisfaction &lt;- c(6,6,2,2,3,3,2,2,2,2,5,5,5,5,5,7,7,7,7,7,1,1,1,1,1,2,2,2,2,2)
data &lt;- as.data.frame(matrix(c(ids, time, happiness, satisfaction),
                         nrow = 30,
                         ncol = 4,
                         dimnames = list(c(),c(""id"", ""time"",
                                               ""happiness"", ""satisfaction""))))
print(data)

#    id time happiness satisfaction
# 1   1    1      0.80            6
# 2   1    2      0.00            6
# 3   2    1      0.75            2
# 4   2    2      0.00            2
# 5   3    1      0.80            3
# 6   3    2      0.00            3
# 7   4    1      2.75            2
# 8   4    2      2.50            2
# 9   5    1      0.40            2
# 10  5    2      0.20            2
# 11  6    1      3.80            5
# 12  6    2      0.40            5
# 13  6    3      0.00            5
# 14  6    4      0.20            5
# 15  6    5      3.40            5
# 16  7    1      3.00            7
# 17  7    2      2.60            7
# 18  7    3      3.40            7
# 19  7    4      3.80            7
# 20  7    5      0.00            7
# 21  8    1      3.60            1
# 22  8    2      3.60            1
# 23  8    3      0.20            1
# 24  8    4      0.40            1
# 25  8    5      1.00            1
# 26  9    1      0.40            2
# 27  9    2      0.20            2
# 28  9    3      1.20            2
# 29  9    4      1.20            2
# 30  9    5      0.00            2
</code></pre>

<p>To create <code>happiness.change</code> I was advised to use the coefficients produced by either of these equations:</p>

<pre><code>require(lme4)
model1 &lt;- lmer(happiness ~ time + (time | id), data = data)

require(nlme)
model2 &lt;- lme(happiness ~ time, random = ~1 + time | id, data = data)
</code></pre>

<p>For example, running <code>coef(model1)</code> produces the following coefficients (column <code>time</code>):</p>

<pre><code># $id
#   (Intercept)        time
# 1   0.9936158 -0.05991770
# 2   0.9739595 -0.05674569
# 3   0.9936158 -0.05991770
# 4   2.5539086 -0.31170766
# 5   0.8998610 -0.04478815
# 6   2.0446747 -0.22953078
# 7   3.2906564 -0.43059926
# 8   2.7120530 -0.33722798
# 9   1.0027053 -0.06138450
# 
# attr(,""class"")
# [1] ""coef.mer""
</code></pre>

<p>And then connecting between <code>satisfaction</code> and the coefficients:</p>

<pre><code>coefs1 &lt;- as.data.frame(unlist(coef(model1))[10:18])
satisfactionData &lt;- reshape(data,
                            direction = ""wide"",
                            idvar = ""id"",
                            timevar = ""time"")[c(1,3)]
newData &lt;- cbind(satisfactionData, coefs1)
colnames(newData) &lt;- c(""id"", ""satisfaction"", ""happiness.change"")
print(newData)

#    id satisfaction happiness.change
# 1   1            6      -0.05991770
# 3   2            2      -0.05674569
# 5   3            3      -0.05991770
# 7   4            2      -0.31170766
# 9   5            2      -0.04478815
# 11  6            5      -0.22953078
# 16  7            7      -0.43059926
# 21  8            1      -0.33722798
# 26  9            2      -0.06138450
</code></pre>

<hr>

<p><strong>I have several questions:</strong></p>

<ol>
<li><p>Running <code>model2</code> generates the following error:</p>

<pre><code>Error in lme.formula(happiness ~ time, random = ~1 + time | id, data = data) : 
  nlminb problem, convergence error code = 1
  message = iteration limit reached without convergence (10)
</code></pre>

<p>I don't remember where, but I read that running <code>lme(happiness ~ time, random = ~1 + time | id, control = list(opt = ""optim""), data = data)</code> bypasses this error, and indeed it does. But what exactly does it do? Do the coefficients produced by using this model still represent the change in <code>happiness</code> over time?</p></li>
<li><p>Running <code>model2</code> (with <code>control = list(opt = ""optim"")</code>) produces slightly (<em>very</em> slightly) different coefficients than <code>model1</code>; why? What is the difference between the models?</p></li>
<li><p>What would be the suitable method for testing the relationship between <code>happiness.change</code> and <code>satisfaction</code>? I tried <code>cor.test(newData$satisfaction, newData$happiness.change)</code>, which produced the following results, but I'm not sure how to interpret them (they are significant in the complete data set):</p>

<pre><code>    Pearson's product-moment correlation

data:  newData$satisfaction and newData$happiness.change
t = -0.72735, df = 7, p-value = 0.4906
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.7901064  0.4843018
sample estimates:
       cor 
-0.2650786 
</code></pre></li>
<li><p>As you can see, <code>model1</code> produces negative coefficients for all the participants (<code>model2</code> as well). Even for participants whose change over time is purely positive (for example, a participant with 2 <code>happiness</code> measures: <code>time1 = 0</code> and <code>time2 = 0.8</code>; no such example in the sample above, but it is in my data).</p>

<p>This may be a problem, because I want to be able to distinguish between participants whose change in <code>happiness</code> is positive (<code>happiness</code> increases over time) and participants whose change in <code>happiness</code> is negative (<code>happiness</code> decreases over time); and then see whether there's a difference between these participants in their relationship between <code>happiness.change</code> and <code>satisfaction</code>. </p>

<p>So my question is this: Is it statistically ""legit"" to divide my participants beforehand into groups based on their ""raw"" change in <code>happiness</code> (for example, I would label a participant with <code>time1 = 0</code> and <code>time2 = 0.8</code> as having a positive change), and then model the coefficient for these groups separately?</p>

<p>However, creating sub-groups this way may be difficult with participants with more than 2 measures of <code>happiness</code>, which brings me to my final question:</p></li>
<li><p>If I understand correctly, <code>model1</code> and <code>model2</code> assume there is a linear change over time. However, I checked the whole sample (424 participants), and a cubic model (<code>happiness ~ time</code>) actually explains more of the variance in <code>happiness</code>. So I suppose a cubic change over time is more appropriate. How can I create a new variable reflecting such a change in <code>happiness</code> over time?</p></li>
</ol>

<p>I realize this is quite long and these are a lot of questions, and so any help will be greatly appreciated. If anyone can answer even one of these questions, I will be very grateful.</p>

<p>Thanks!</p>
"
"0.0640622132638473","0.0664700094043992","211677","<p>I am studying linear mixed model recently, and my data have only 6 subjects and those are ranked groups of observations (Tier 1 customers > Tier 2 customers > Tier 3 > ... > Tier 6)</p>

<p>The formula looks like this:</p>

<pre><code>lmerfit &lt;- lmer(data=data, y ~ ( (1 + x1) | Tier ) + x2 )
</code></pre>

<p>In this case, intercepts of random effects are random, but the slopes (sensitivity to x1) are ranked corresponding to the groups:</p>

<p><a href=""http://i.stack.imgur.com/kVWzh.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kVWzh.gif"" alt=""enter image description here""></a></p>

<p>I do worry about this: usually the random effects should be randomly distributed, but apparently the observations are too few and they are not random in my model. </p>

<p>If I already know the subjects are going to have ranked sensitivity to x1, but not sure the exact values of betas, can I use linear mixed model as an unsupervised (or ""lazy"") model to cluster the data?</p>
"
"0.0905976508333704","0.0940027887907685","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.133697632737248","0.138722695527068","212397","<p>I would like to test the effect of a treatment (""crop"") on species richness. I would rather use a glm for richness as it is a kind of count data.</p>

<p>Besides, I have a nested sampling design (5 values per plot, 5 plot per treatment). Thus I should use a GLMM.</p>

<p>So I write my model :</p>

<pre><code>&gt; GLMM_ric = glmer(richness ~ Crop + (1| Plot),  family=poisson)
&gt; summary(GLMM_ric)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
 glmerMod]
 Family: poisson ( log )
 Formula: richness ~ Crop + (1 | Plot)
 Data: Com_agg

 AIC      BIC   logLik deviance df.resid 
433.8    446.9   -211.9    423.8       95 

Scaled residuals: 
   Min       1Q   Median       3Q      Max 
-1.33174 -0.41445 -0.08382  0.39853  1.73324 

Random effects:
 Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.08432  0.2904  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   1.9621     0.1503  13.056   &lt;2e-16 ***
 CropM        -0.5351     0.2211  -2.420   0.0155 *  
 CropYR       -0.3814     0.2181  -1.748   0.0804 .  
 CropOR       -0.3393     0.2175  -1.560   0.1188    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Correlation of Fixed Effects:
        (Intr) CropM  CropYR
 CropM  -0.678              
 CropYR -0.686  0.467       
 CropOR -0.687  0.468  0.475
</code></pre>

<p>and then a simpler model to compare with :</p>

<pre><code> &gt; GLMM_ric0 = glmer(richness ~ (1| Plot), data=Com_agg, family=poisson,    glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))

 &gt;summary(GLMM_ric0)

 Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod]
  Family: poisson ( log )
 Formula: richness ~ (1 | Plot)
    Data: Com_agg
 Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

 AIC      BIC   logLik deviance df.resid 
 433.3    438.5   -214.7    429.3       98 

 Scaled residuals: 
 Min       1Q   Median       3Q      Max 
 -1.27211 -0.39830 -0.03309  0.38204  1.66734 

 Random effects:
  Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.1251   0.3537  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)  1.64739    0.09114   18.07   &lt;2e-16 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And then I compare both models :</p>

<pre><code>&gt; anova(GLMM_ric0, GLMM_ric)
Data: Com_agg
Models:
GLMM_ric0: richness ~ (1 | Plot)
GLMM_ric: richness ~ Crop + (1 | Plot)
              Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
GLMM_ric0  2 433.32 438.53 -214.66   429.32                         
GLMM_ric   5 433.84 446.86 -211.92   423.84 5.4851      3     0.1395
</code></pre>

<p>So according to my anova, the factor ""crop"" is not significant. Yet in the summary of my model some of the modalities appear to be significant. How should I interpret this ?</p>

<p>I have looked around for a while (e.g. <a href=""http://stats.stackexchange.com/questions/9587/glmm-test-of-significance"">here</a> or <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">here</a>) but I could not find much for this precise situation.</p>
"
"0.143247463647051","0.128813931560849","213072","<p>Iâ€™m using the <code>glmer</code> function from the <code>lme4</code> package in <code>R</code> to model species richness adjacent to aquaculture sites. I have 6 sites: 2 in production, 2 were in production the last years but not anymore at the time of the sampling (fallow), and 2 that were never under production (references). Photographs along transects away from the aquaculture sites were taken each 20-40 m from 0 to 200 m and reference sites were at 1500 m from aquaculture sites. These transects were repeated 7 times over a period of 2 years to determine if the community changed over time.</p>

<p>Iâ€™ve followed the steps described in the excellent book from Zuur et al. (2009) <em><a href=""http://rads.stackoverflow.com/amzn/click/1441927646"" rel=""nofollow"">Mixed Effects Models and Extensions in Ecology with R</a></em> and my best model is:</p>

<p>(<em>Note that predictors</em> <code>Distance</code>, <code>Depth</code> <em>and</em> <code>Beggiatoa.sp.</code> <em>have been standardized to remove an</em> <code>lme4</code> <em>error message.</em>)</p>

<pre><code>glmm.8 &lt;- glmer(sr ~ Distance+Depth+fSubstrate+Beggiatoa.sp.+
                     Distance:Beggiatoa.sp.+(1|fSite),
                glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)),
                family=poisson, data=datsc)

summary(glmm.8)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
   ['glmerMod']
Family: poisson  ( log )
Formula: sr ~ Distance + Depth + fSubstrate + Beggiatoa.sp. + 
   Distance:Beggiatoa.sp. +      (1 | fSite)
Data: datsc
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

   AIC      BIC   logLik deviance df.resid 
2279.7   2328.8  -1129.9   2259.7      992 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5171 -0.6376 -0.2008  0.4326  4.9375 

Random effects:
Groups Name        Variance Std.Dev.
fSite  (Intercept) 0.1831   0.4279  
Number of obs: 1002, groups:  fSite, 6

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             1.49171    0.50388   2.960 0.003072 ** 
Distance                2.05809    0.59940   3.434 0.000596 ***
Depth                  -0.09093    0.02966  -3.066 0.002171 ** 
fSubstrateCoarse       -0.09929    0.08299  -1.196 0.231514    
fSubstrateFine         -0.62376    0.08606  -7.248 4.24e-13 ***
fSubstrateFloc         -1.75314    0.30211  -5.803 6.51e-09 ***
fSubstrateMedium       -0.35201    0.07625  -4.617 3.90e-06 ***
Beggiatoa.sp.           2.42190    1.09521   2.211 0.027011 *  
Distance:Beggiatoa.sp.  3.30995    1.37755   2.403 0.016271 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Distnc Depth  fSbstC fSbstrtFn fSbstrtFl fSbstM Bggt..
Distance     0.885                                                       
Depth        0.052  0.027                                                
fSubstrtCrs -0.076 -0.028 -0.024                                         
fSubstratFn -0.177 -0.058 -0.145  0.325                                  
fSubstrtFlc  0.047  0.132 -0.022  0.066  0.155                           
fSubstrtMdm -0.088 -0.029 -0.102  0.314  0.380     0.097                 
Beggiat.sp.  0.927  0.947  0.039 -0.024 -0.088     0.080    -0.027       
Dstnc:Bgg..  0.925  0.950  0.037 -0.024 -0.089     0.119    -0.027  0.996
</code></pre>

<p><strong>My question is: How do I validate this model to see if it meets the required assumptions?</strong></p>

<p>I did a series of plots but I'm not sure if they are the appropriate ones and if they are, if they violate the assumptions.</p>



<pre><code>EP &lt;- residuals(glmm.8,type=""pearson"")
plot(EP~fitted(glmm.8))
</code></pre>

<p><a href=""http://i.stack.imgur.com/K8YnQ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/K8YnQ.jpg"" alt=""enter image description here""></a></p>



<pre><code>qqnorm(EP)
qqline(EP)
</code></pre>

<p><a href=""http://i.stack.imgur.com/owp47.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/owp47.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Distance, EP, xlab=""Distance"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/4X05s.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4X05s.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Depth, EP, xlab=""Depth"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/cKdYC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cKdYC.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$fSubstrate, EP, xlab=""Substrate"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/LUgG8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LUgG8.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Beggiatoa.sp., EP, xlab=""Beggiatoa.sp."", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/cg3oO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cg3oO.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(fitted(glmm.8)~predict(glmm.8))
</code></pre>

<p><a href=""http://i.stack.imgur.com/LgHDV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LgHDV.jpg"" alt=""enter image description here""></a></p>



<p>I looked on this and other websites and I couldn't find a ""perfect"" method to validate Poisson GLMM models. I believe a good answer to my question would be relevant to many people.  If needed I can provide a subset of my data but this question can probably be answered without it. Still, let me know if need it.</p>


"
"0.104613156193188","0.09497693697262","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.075498042361142","0.0940027887907685","213214","<p>I'm currently running a time series analysis which requires me to fit lmer models to each of my data points. Here is my code :</p>

<pre><code># import data
dt &lt;- h5read('file.h5', 'd1/table')
dt$cond &lt;- as.factor(dt$cond)
dt$sub &lt;- as.factor(dt$sub)

# close file
H5close()

# apply model to each time point
fitted &lt;- by(dt, dt$tp, function(x) lmer(size ~ cond + (1 + cond | sub), data = x))
</code></pre>

<p>In this model, <code>cond</code> is a 2 levels fixed effect factor and <code>sub</code> is a 14 levels random effect factor. For some reason, the model doesn't yield a t-value in its ouput :</p>

<pre><code>&gt; fitted[1]
$`0`
Linear mixed model fit by REML ['lmerMod']
Formula: size ~ cond + (1 + cond | sub)
   Data: x
REML criterion at convergence: -7694.797
Random effects:
 Groups   Name        Std.Dev.  Corr 
 sub      (Intercept) 1.388e-02      
          cond1       1.169e-05 -1.00
 Residual             7.922e-02      
Number of obs: 3467, groups:  sub, 14
Fixed Effects:
(Intercept)        cond1  
   0.023743    -0.001154
</code></pre>

<p>Is it the output I should have expected? It seems that R doesn't take the difference between both levels of <code>cond</code> into account.</p>
"
"0.110959008218296","0.102337274193778","213470","<p><strong>Background</strong></p>

<p>Although my data should have a multinomial dependent variable, I have settled for a binary as I could not understand too much of MCMCglmm. The data is a time series cross sectional, so am looking at each individual outcome vis-a-vis the others. I don't have much experience with statistics, so I really need to know if am on the right path of actually coming up with values for a dynamic linear model or way off. Since i need effects from the independent variables.</p>

<p><strong><em>The data sample</em></strong></p>

<p>The data is for students who applied for university courses and were admitted within a period of 3 years. The data mainly has the grades in the subjects done in their pre-entry level exams. Each student can do a maximum of 4, but in the model below <code>NA</code> values are filled with <code>0</code> (Not sure if a correct assumption). </p>

<p><strong>The problem</strong></p>

<ol>
<li>How do I get time varying effects?</li>
<li>How do i extract the effect of time? </li>
<li>what does it imply when time is expressed as a random effect?</li>
</ol>

<p>Every input is highly appreciated, Thank you.</p>

<p>Below is a result from the <code>glmer</code> function with formula</p>

<p><code>glmllb2 &lt;- glmer(logi ~ history + c.r.e + economics + geography + literature + f.art + entrepreneurship + luganda + kiswahili + french + i.r.e + historyc + historycsq + (1 | called), family = binomial(""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 100, data = data.apriori.llb2)</code></p>

<p>where <code>history+c.r.e + ... + i.r.e</code> are subject grades that predict student admission into a course <code>logi</code> (as a binary) while <code>historyc</code> is a grand mean centered variable for history and <code>historycsq</code> is the squared variable for <code>historyc</code>. <code>called</code> is time in years re-scaled to <code>1,2 and 3</code></p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Adaptive Gauss-Hermite Quadrature, nAGQ = 100) [glmerMod]
 Family: binomial  ( logit )
Formula: logi ~ history + c.r.e + economics + geography + literature +  
    f.art + entrepreneurship + luganda + kiswahili + french +      i.r.e + historyc + historycsq + (1 | called)
   Data: data.apriori.llb2
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  1778.2   1874.8   -875.1   1750.2     7317 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
 -3.843  -0.123  -0.054  -0.025 213.612 

Random effects:
 Groups Name        Variance Std.Dev.
 called (Intercept) 0.09975  0.3158  
Number of obs: 7331, groups:  called, 3

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -13.27747    0.52128 -25.471  &lt; 2e-16 ***
history            0.55942    0.04656  12.016  &lt; 2e-16 ***
c.r.e              0.45941    0.03652  12.580  &lt; 2e-16 ***
economics          0.69835    0.04509  15.489  &lt; 2e-16 ***
geography          0.49442    0.04137  11.950  &lt; 2e-16 ***
literature         0.77936    0.04129  18.877  &lt; 2e-16 ***
f.art              0.50219    0.04387  11.447  &lt; 2e-16 ***
entrepreneurship   0.46377    0.04504  10.297  &lt; 2e-16 ***
luganda            0.49340    0.07643   6.456 1.08e-10 ***
kiswahili          0.52691    0.10498   5.019 5.20e-07 ***
french             0.65225    0.09133   7.142 9.22e-13 ***
i.r.e              0.59269    0.08265   7.171 7.44e-13 ***
historycsq         0.03721    0.01794   2.075    0.038 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) histry c.r.e  ecnmcs ggrphy litrtr f.art  entrpr lugand kiswhl french i.r.e 
history     -0.559                                                                             
c.r.e       -0.608  0.143                                                                      
economics   -0.340 -0.083 -0.002                                                               
geography   -0.569  0.187  0.624 -0.059                                                        
literature  -0.625  0.169  0.540  0.086  0.709                                                 
f.art       -0.614  0.231  0.588  0.118  0.525  0.585                                          
entrprnrshp -0.554  0.191  0.564 -0.031  0.583  0.654  0.596                                   
luganda     -0.305  0.086  0.289  0.065  0.305  0.337  0.283  0.281                            
kiswahili   -0.242  0.158  0.173  0.073  0.161  0.195  0.195  0.186  0.099                     
french      -0.265  0.079  0.314 -0.002  0.257  0.268  0.290  0.305  0.144  0.094              
i.r.e       -0.256  0.038  0.349  0.041  0.253  0.251  0.231  0.242  0.049  0.083  0.137       
historycsq   0.153 -0.253 -0.235 -0.107 -0.275 -0.204 -0.202 -0.231 -0.106 -0.112 -0.119 -0.102
fit warnings:
fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
</code></pre>
"
"0.135763958933068","0.149670852991926","214645","<p>I'm studying the effect of pH and cross-types on mortality of fish. Treatment is categorical (2 levels: control and low pH) and cross-types is also categorical (4 levels: parents wild male x wild female (WMWF), wild male x farmed female (WMFF), farmed male x wild female (FMWF), and farmed male x farmed female (FMFF)). There was 6 tanks in total (3 control and 3 at low pH) and each tank had 15 fish of each cross-type (60 fish total/tank). Since mortality is a count and that there was higher mortality in one of the control tank, I used Poisson GLMM to account for the tank effect.</p>

<p>Here's the model and summary results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: poisson  ( log )
Formula: mortality.count ~ Tr * Cross + (1 | Tank)
Data: pHdat

 AIC      BIC   logLik deviance df.resid 
93.8    104.4    -37.9     75.8       15 

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-1.1311 -0.4171 -0.2554  0.1608  1.2889 

Random effects:
Groups Name        Variance Std.Dev.
Tank   (Intercept) 2.225    1.492   
Number of obs: 24, groups:  Tank, 6

Fixed effects:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)       -1.666e+00  1.377e+00  -1.210   0.2264  
TrLOWpH            3.053e+00  1.647e+00   1.854   0.0637 .
CrossFMWF          9.810e-01  6.770e-01   1.449   0.1473  
CrossWMFF          9.810e-01  6.770e-01   1.449   0.1474  
CrossWMWF          2.248e-05  8.165e-01   0.000   1.0000  
TrLOWpH:CrossFMWF -1.754e+00  8.378e-01  -2.094   0.0363 *
TrLOWpH:CrossWMFF -1.243e+00  7.970e-01  -1.560   0.1188  
TrLOWpH:CrossWMWF -6.190e-01  9.415e-01  -0.658   0.5109  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) TrLOWH CrFMWF CrWMFF CrWMWF TLOWH:CF TLOWH:CWMF
TrLOWpH     -0.835                                                
CrossFMWF   -0.358  0.299                                         
CrossWMFF   -0.358  0.299  0.727                                  
CrossWMWF   -0.296  0.248  0.603  0.603                           
TLOWH:CFMWF  0.289 -0.297 -0.808 -0.588 -0.487                    
TLOWH:CWMFF  0.304 -0.313 -0.618 -0.849 -0.512  0.614             
TLOWH:CWMWF  0.257 -0.265 -0.523 -0.523 -0.867  0.520    0.547 
</code></pre>

<p>As you can see, the fish in low pH tanks of the cross FMWF died (weakly but still) significantly less than the baseline (FMFF).</p>

<p>Now I wanted to see if there was significant differences between the cross for each treatment (not only compared to the baseline) so I used <code>lsmeans</code>. Here's the results:</p>

<pre><code>lsmeans(glmm.0,pairwise~Tr*Cross,adjust=""tukey"")
$lsmeans
Tr    Cross     lsmean        SE df  asymp.LCL asymp.UCL
CTRL  FMFF  -1.6658411 1.3770371 NA -4.3647842  1.033102
LOWpH FMFF   1.3873820 0.9065822 NA -0.3894865  3.164251
CTRL  FMWF  -0.6848201 1.2991734 NA -3.2311531  1.861513
LOWpH FMWF   0.6141089 0.9548037 NA -1.2572719  2.485490
CTRL  WMFF  -0.6848677 1.2991756 NA -3.2312050  1.861470
LOWpH WMFF   1.1251162 0.9192163 NA -0.6765147  2.926747
CTRL  WMWF  -1.6658186 1.3770335 NA -4.3647547  1.033117
LOWpH WMWF   0.7683761 0.9422428 NA -1.0783859  2.615138

Results are given on the log (not the response) scale. 
Confidence level used: 0.95 

$contrasts
contrast                     estimate        SE df z.ratio p.value
CTRL,FMFF - LOWpH,FMFF  -3.053223e+00 1.6467764 NA  -1.854  0.5828
CTRL,FMFF - CTRL,FMWF   -9.810210e-01 0.6770180 NA  -1.449  0.8342
CTRL,FMFF - LOWpH,FMWF  -2.279950e+00 1.6738073 NA  -1.362  0.8744
CTRL,FMFF - CTRL,WMFF   -9.809735e-01 0.6770224 NA  -1.449  0.8342
CTRL,FMFF - LOWpH,WMFF  -2.790957e+00 1.6537652 NA  -1.688  0.6954
CTRL,FMFF - CTRL,WMWF   -2.248243e-05 0.8165311 NA   0.000  1.0000
CTRL,FMFF - LOWpH,WMWF  -2.434217e+00 1.6666742 NA  -1.461  0.8284
LOWpH,FMFF - CTRL,FMWF   2.072202e+00 1.5822427 NA   1.310  0.8956
LOWpH,FMFF - LOWpH,FMWF  7.732732e-01 0.4935656 NA   1.567  0.7704
LOWpH,FMFF - CTRL,WMFF   2.072250e+00 1.5822445 NA   1.310  0.8956
LOWpH,FMFF - LOWpH,WMFF  2.622658e-01 0.4206134 NA   0.624  0.9986
LOWpH,FMFF - CTRL,WMWF   3.053201e+00 1.6467732 NA   1.854  0.5828
LOWpH,FMFF - LOWpH,WMWF  6.190059e-01 0.4688054 NA   1.320  0.8914
CTRL,FMWF - LOWpH,FMWF  -1.298929e+00 1.6103573 NA  -0.807  0.9928
CTRL,FMWF - CTRL,WMFF    4.754102e-05 0.4999815 NA   0.000  1.0000
CTRL,FMWF - LOWpH,WMFF  -1.809936e+00 1.5895153 NA  -1.139  0.9483
CTRL,FMWF - CTRL,WMWF    9.809985e-01 0.6770119 NA   1.449  0.8342
CTRL,FMWF - LOWpH,WMWF  -1.453196e+00 1.6029417 NA  -0.907  0.9855
LOWpH,FMWF - CTRL,WMFF   1.298977e+00 1.6103590 NA   0.807  0.9928
LOWpH,FMWF - LOWpH,WMFF -5.110073e-01 0.5164052 NA  -0.990  0.9760
LOWpH,FMWF - CTRL,WMWF   2.279928e+00 1.6738042 NA   1.362  0.8744
LOWpH,FMWF - LOWpH,WMWF -1.542672e-01 0.5563606 NA  -0.277  1.0000
CTRL,WMFF - LOWpH,WMFF  -1.809984e+00 1.5895171 NA  -1.139  0.9483
CTRL,WMFF - CTRL,WMWF    9.809510e-01 0.6770162 NA   1.449  0.8343
CTRL,WMFF - LOWpH,WMWF  -1.453244e+00 1.6029435 NA  -0.907  0.9855
LOWpH,WMFF - CTRL,WMWF   2.790935e+00 1.6537621 NA   1.688  0.6954
LOWpH,WMFF - LOWpH,WMWF  3.567401e-01 0.4927939 NA   0.724  0.9963
CTRL,WMWF - LOWpH,WMWF  -2.434195e+00 1.6666710 NA  -1.461  0.8284

Results are given on the log (not the response) scale. 
P value adjustment: tukey method for comparing a family of 8 estimates 
Tests are performed on the log scale
</code></pre>

<p>Now I don't find the significant difference identified by the GLMM.</p>

<p><strong>Why the GLMM indicates a significant difference and lsmeans not, and why do I get NAs for my df in lsmeans?</strong></p>
"
"0.0523065780965941","0.0542725354129257","217915","<p>I'm trying to recalculate the REML log-likelihood given by the <code>logLik</code> function from a linear mixed model with one random effect. I use the <code>lmer</code> function in the <code>lme4</code> package and the sleepstudy dataset from the package.</p>

<pre><code>model &lt;- lmer(Reaction ~ Days + (1|Subject), sleepstudy)
logLik(model, REML=F)
# 'log Lik.' -897.0497 (df=4)
</code></pre>

<p>My calculation :</p>

<pre><code>Z      &lt;- getME(model, 'Z')
sigma2 &lt;- summary(model)$devcomp$cmp[[9]]^2
Xbeta  &lt;- getME(model, 'X') %*% fixef(model)
V      &lt;- as.data.frame(VarCorr(model))$vcov[1] * Z %*% t(Z) + 
                       sigma2 * diag(1, nrow(sleepstudy))
sum(log(dnorm(sleepstudy$Reaction, Xbeta, sqrt(V[1:10, 1:10]))))
# -962.0263
</code></pre>

<p>Does anyone know what am I doing wrong?</p>

<h2><strong>EDIT</strong></h2>

<p>Using the appropriate function <code>dmnorm</code> from package <code>mnormt</code> to obtain the density for the multivariate normal distribution, I get : </p>

<pre><code>dmnorm(x = sleepstudy$Reaction, mean = as.vector(Xbeta), varcov = V, log = TRUE)
# -897.0553
</code></pre>

<p>It is not always the right solution.</p>
"
"0.124055942546034","0.137299861037594","218707","<p>I have successfully calculated RÂ²c as a goodness-of-fit measure for GLMM's using the <code>r.squaredGLMM</code> function implemented in <code>R</code>'s <code>MuMIn</code> package, as well as calculating RÂ²GLMM with the step-by-step guide given in Nakagawa &amp; Schielzeth 2013 (A general and simple method for obtaining RÂ² from generalized linear mixed-effects models 4:133-142. doi: 10.1111/j.2041-210x.2012.00261.x). </p>

<p>Successfully anyway, when my models followed a Gaussian or Poisson error structure. I am having trouble, though, to calculate RÂ² for a Gamma-GLMM: </p>

<p>1) <code>r.squaredGLMM</code> apparently does not work for Gamma GLMM's:</p>

<pre><code>Error in r.squaredGLMM.merMod(mod1) : do not know how to calculate variance for this family/link combination`
</code></pre>

<p>2) Nakagawa and Schielzeth give examples on how to calculate RÂ²GLMM for GLMM's with Gaussian, Poisson or Binomial error structures, but not for GLMM's with Gamma error structure. </p>

<p>This following is part of the supplemental R code to the paper:</p>

<pre><code># Fit null model without fixed effects (but including all random effects)
m0 &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data)

# Fit alternative model including fixed and all random effects
mF &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 |     Container), data = Data)

# View model fits for both models
summary(m0)
summary(mF)

# Extraction of fitted value for the alternative model
# fixef() extracts coefficents for fixed effects
# mF@X returns fixed effect design matrix
Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]  + fixef(mF)[4] * mF@X[, 4]

# Calculation of the variance in fitted values
VarF &lt;- var(Fixed)

# An alternative way for getting the same result
VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

# R2GLMM(m) - marginal R2GLMM
# Equ. 26, 29 and 30
# VarCorr() extracts variance components
# attr(VarCorr(lmer.model),'sc')^2 extracts the residual variance
VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] +    attr(VarCorr(mF), ""sc"")^2)

# R2GLMM(c) - conditional R2GLMM for full model
# Equ. XXX, XXX
(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +  VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + (attr(VarCorr(mF), ""sc"")^2))
</code></pre>

<p>Can anybody give me a solution on how to calculate RÂ²GLMM for a GLMM with Gamma error structure?</p>

<p>The real problem is that I don't know which is the correct specification for the very last part of the very last formula in the above example. This part is really the only one which changes depending on error structure of the model.</p>

<p>For Gaussian it is <code>(attr(VarCorr(mF), ""sc"")^2)</code>, for Binomial it is <code>pi^2/3</code> and for Poisson they use <code>log(1 + 1/exp(as.numeric(fixef(m0))))</code>.</p>

<p>Thanks a lot.</p>
"
"0.12266979912335","0.127280377713181","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.0978566471559948","0.101534616513362","218970","<p>For a specific analysis I want to calculate the <em>variance partition coefficient</em> (VPC). I am using the following formula:</p>

<pre><code> test &lt;- glmer(SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek + 
             (1|POSCODN), data = dataScaled, family = binomial)

&gt; summary(test)
     Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
       Family: binomial  ( logit )
       Formula: SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek +  (1 | POSCODN)
       Data: dataScaled

   AIC      BIC   logLik deviance df.resid 
  43707.5  43757.9 -21847.8  43695.5    32684 

Scaled residuals: 
   Min      1Q  Median      3Q     Max 
 -1.3263 -0.8378 -0.7164  1.1263  2.4788 

  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Number of obs: 32690, groups:  POSCODN, 173

Fixed effects:
          Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   -0.56437    0.01798 -31.387  &lt; 2e-16 ***
 Herkomst1      0.49571    0.02980  16.633  &lt; 2e-16 ***
 OuderPersoon1  0.29911    0.02433  12.295  &lt; 2e-16 ***
 statusscore14 -0.09900    0.01353  -7.316 2.56e-13 ***
 M_SpoWeek     -0.08658    0.01225  -7.067 1.58e-12 ***

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   Correlation of Fixed Effects:
        (Intr) Hrkms1 OdrPr1 stts14
Herkomst1   -0.436                     
OuderPersn1 -0.553  0.168              
statusscr14 -0.068  0.233  0.013       
M_SpoWeek   -0.044 -0.029  0.138 -0.034
</code></pre>

<p>Because I only have information in the outcome about POSCODN at random effects I am not sure how to calculate the VPC. How can I get an extra row there with information about residuals. For example: </p>

<pre><code>  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Residual             258.357 16.0735 
 Number of obs: 32690, groups:  POSCODN, 173
</code></pre>

<p>Or do you have other suggestions how to calculate the VPC?</p>

<p>Thanks!</p>
"
"0.174744012778562","0.174059342594335","219122","<p><strong>The CV Question</strong></p>

<p>I'm trying to give (a) detailed and concise mathematical representation(s) of a mixed effects model. I am using the <code>lme4</code> package in R. What is the correct mathematical representation for my model?</p>

<hr>

<p><strong>The Data, Science Question, and R Code</strong></p>

<p>My data set consists of species in different regions. I'm testing if a species' prevalence changes in the time leading up to an extinction (extinctions aren't necessarily permanent; it can recolonize), or following a colonization.</p>

<p><code>lmer(prevalence ~ time + time:type + (1 + time + type:time | reg) + (1 + time + type:time | reg:spp))</code></p>

<ul>
<li><strong>Prevalence</strong> is the proportion of strata occupied by a species in a region-year</li>
<li><strong>Time</strong> is a continuous variable that indicates the time to either extinction or colonization; it is always positive</li>
<li><strong>Type</strong> is a categorical variable with two levels. These two levels are â€œ-â€ and â€œ+â€. When type is -, itâ€™s a colonization (default level). When type is +, itâ€™s an extinction.</li>
<li><strong>Reg</strong> is a categorical variable with nine levels, indicating the region</li>
<li><strong>Spp</strong> is a categorical variable; the number of levels varies among regions, and varies between 48 levels and 144 levels.</li>
</ul>

<p>In words: response variable is prevalence (proportion of strata occupied). Fixed effects included 1) and intercept, 2) time from event, and 3) the interaction between time to event and the type of event (colonization or extinction). Each of these 3 fixed effects varied randomly among regions. Within a region, each of the effects varied randomly among species.</p>

<p><strong>I'm trying to figure out how to write the mathematical equation for the model.</strong> I think I understand what's going on in the R code (although, I'm sure I have some knowledge gaps, and hopefully writing out the formal mathematical expression will improve my understanding).</p>

<p>I have searched through the web and through these forums quite a bit. I found tons of useful information, to be sure (and maybe I'll link to some of these in an edit to this question). However, I couldn't quite find that ""Rosetta Stone"" of R-code translated to math (I'm more comfortable with code) that would really help me confirm I've got these equations right. In fact, I know there are some gaps already, but we'll get to that.</p>

<hr>

<p><strong>My Attempt</strong></p>

<p>The basic form of a mixed effects model, in matrix notation is (to my understanding): $$ Y = X \beta + Z \gamma + \epsilon$$</p>

<p>$$ X = \begin{bmatrix} 1 &amp; \Delta t &amp; \Delta t_{+} \\ \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; \Delta t_n &amp; \Delta t_{+,n} \end{bmatrix} $$
$$ \beta^{'} = \begin{bmatrix} \beta_0 &amp; \beta_1 &amp; \beta_2 \end{bmatrix}$$
$$ Z = \begin{bmatrix} 1 I(r_1) &amp; \Delta t I(r_1) &amp; \Delta t_{+} I(r_1) &amp; \dots  &amp; 1 I(r_9) &amp; \Delta t I(r_9) &amp; \Delta t_{+} I(r_9) \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\ 1 I(r_{1,n}) &amp; \Delta t_n I(r_{1,n}) &amp; \Delta t_{+,n} I(r_{1,n}) &amp; \dots  &amp; 1 I(r_{9,n}) &amp; \Delta t I(r_{9,n}) &amp; \Delta t_{+,n} I(r_{9,n}) \\ \end{bmatrix}$$ 
$$ \gamma^{'} =  \begin{bmatrix} \gamma_{0,1} &amp; \gamma_{1,1} &amp;\gamma_{2,1} &amp; \dots  &amp;  \gamma_{0,9} &amp; \gamma_{1,9} &amp;\gamma_{2,9} \end{bmatrix}$$
$$ \epsilon \sim \mathcal{N}(0,\Sigma)$$ </p>

<ul>
<li>$X$ is the design matrix for the fixed effects, $\Delta t$ is the time after colonization (<code>time</code>), and $\Delta t_{+}$ is the time after extinction (<code>time:type</code>)</li>
<li>$Z$ is the design matrix for the random effects (level 1?), I() is the indicator function giving 1 if the sample belongs to the designated region and 0 otherwise, r is indexed to indicates one of the nine regions.</li>
<li>$\beta$ and $\gamma$ contain parameters</li>
<li>$\epsilon$ is errors; I'm not entirely sure how to explain $\Sigma$, though I realize one of these variance/covariance matrices will express covariances among slopes and intercepts, e.g.</li>
</ul>

<p>Assuming things so far are ~correct, that means I'm good at the top level. However, explaining the species-specific variation on the parameters, which is nested within each region, stumped me even more.</p>

<p>But I took a crack at something that maybe makes sense ...</p>

<p>Each of the parameters in $\gamma$ is derived from a linear combination of species-specific predictors and parameters within a region. For each region , there are 3 rows of , corresponding to the 3 predictor variables. Each $\gamma$ can be individually expressed as</p>

<ul>
<li>$  \gamma_{p,r} = U_{p,r}  b_{p,r}  +  \eta_{p,r} $

<ul>
<li>where  $U_{p,r}$ is a design matrix specific to region $r$ and predictor $p$, $b_{p,r}$ is a 1 by S matrix of parameters for the region (richness in the region = $S$, e.g. 48 or 144), and $\eta_{p,r}$ is a matrix of error terms</li>
</ul></li>
</ul>

<p>Specifically, for a given region, each of the $\gamma_{p,r}$ would be:</p>

<p>$$ \gamma_{0,r} = U_{0,r} b_{0,r} + \eta_{0,r} $$
$$  \gamma_{0,r} = \begin{bmatrix} 1 I(s_1) \dots 1 I(s_S) \end{bmatrix} + \begin{bmatrix} b_{0,1}\\ \vdots \\ b_{0,S} \end{bmatrix} + \eta_{0,r} $$
$$ \gamma_{1,r} = U_{1,r} b_{1,r} + \eta_{1,r} $$
$$  \gamma_{1,r} = \begin{bmatrix} \Delta t I(s_1) \dots  \Delta t I(s_S) \end{bmatrix} + \begin{bmatrix} b_{1,1}\\ \vdots \\ b_{1,S} \end{bmatrix} + \eta_{1,r} $$
$$ \gamma_{2,r} = U_{2,r} b_{2,r} + \eta_{2,r} $$
$$  \gamma_{2,r} = \begin{bmatrix} \Delta t_+ I(s_1) \dots  \Delta t_+ I(s_S) \end{bmatrix} + \begin{bmatrix} b_{2,1}\\ \vdots \\ b_{2,S} \end{bmatrix} + \eta_{2,r} $$</p>

<p>That would be repeated for each region. Then,  $ \eta \sim \mathcal{N}(0,\Sigma_{\eta}) $, like $\epsilon$. Although, perhaps instead of $\Sigma$, there is another letter, like $G$, that is commonly used.</p>

<hr>

<p><strong>Edit: other Q/A's that were somewhat helpful</strong></p>

<ul>
<li><a href=""http://stats.stackexchange.com/a/90734/25184"">This Q/A was nice, but didn't write things out in the full matrix form</a></li>
</ul>
"
"0.0369863360727655","0.0383764778226668","219232","<p>I have a study where subjects have received treatment to one eye, the other eye is untreated and is acting as a control. There are 3 measures (of retinal thickness) taken in each eye. </p>

<p>The model I am using is:  </p>

<pre><code>fit &lt;- lmer(Thickness~tx + Region + (1|ID) + (1|ID:Eye), data)
</code></pre>

<p>Where <code>tx</code> is treated / untreated and <code>Region</code> is the repeated measure.</p>

<p>I need a sentence to describe this for publication!</p>

<p>My current attempt looks like this:  </p>

<blockquote>
  <p>Changes of retinal thickness between treated and untreated eyes was examined using a linear mixed effects model[6]. Each subject was considered to have an independent intercept and the three regions within each eye were treated as repeated measures.</p>
</blockquote>

<p>Is this correct, could it be better?</p>
"
"0.19571329431199","0.195816760418627","219259","<h2>The Setup</h2>

<p>I am employing a linear mixed model in <em>R</em> using the packages ""lme4"" and ""lmerTest."" In modeling my predicted variable, I have two time indicators set as fixed and random effects: one time variable signifying change within a day of measurement of the predicted variable, and one time variable distinguishing between measurements taken before and after an intervention. Both indicators are dichotomous (0/1; reflecting respectively in the morning versus the evening, and pre-to-post intervention). The interaction between these time variables would reflect (I believe) the <em>change in the change</em> from pre-to-post treatment. For the purpose of this question, let's say I have two predictors of interest that interact with both time variables and their combination, ""expectations"" and ""anxietychange"". </p>

<p>My code looks something like this:</p>

<pre><code>lmer(predicted ~ (1 + prepost + withindaytime | subjectid) + expectations * prepost * withindaytime + anxietychange * prepost * withindaytime, data=data)
</code></pre>

<p>Let's say that I find that there are significant interactions between each of my (continuous) predictor variables of interest and various combinations of the time variables. Hooray!</p>

<h2>The Main Question: Confidence intervals around predicted change, predicted change in change, given particular levels of the predictor variable?</h2>

<p>For the purposes of constructing a figure illustrating these results, I would like to be able to construct the confidence intervals of the model's predicted change in the Y variable pre-to-post intervention, given particular levels of the predictor variables (i.e., expectations and anxietychange). I would like any given approach I use to take into account the fact that the confidence intervals would be different for different individuals with different values of the predictor variables, ""expectations"" and ""anxietychange"". </p>

<p>To be even more specific: Given specific levels of the predictor variables, I would like to create confidence intervals around the prediction of change in the Y variable pre-to-post intervention for a given time-point in the day (e.g., the Y variable in the morning) <em>and</em> the prediction of change in change (!) for individuals (i.e., within-day time interacting with pre-to-post intervention time). Essentially, I would like to predict these values for an (imaginary) 3x3 cell representing individuals relatively low (-1SD), average (mean), and relatively high (+1SD) in each predictor variable. </p>

<p>For example, imaginary individual 1 who has mean values for each predictor would have value X for their pre-to-post intervention change in their Y variable as measured in the morning [e.g., within-day time 0; 95% CI: -aaa to aaa], and value Z for how much their Y variable change within a day of measurement has shifted after an intervention [95% CI: -bbb to bbb]. Imaginary individual 2 would have a +1SD value for one of the predictors, but mean level for the other predictor, and would have value G for their pre-to post change in their Y variable as measured in the morning, et cetera. (Again, my actual hypothesis testing does not impose this artificial trichotomy, this would simply be for illustrative purposes.)</p>

<p>While I can easily acquire confidence intervals for the fixed effect coefficients, I'm not sure how to acquire prediction confidence intervals for a given prediction of change. With my limited knowledge, accomplishing the above is complicated by the fact that I am not feeding the model pre-to-post change scores in Y, but rather using all available data and having the mixed model estimate slopes of change (and slopes of change in change). In other words, if I were to use a simple predict() function, I (believe) I would get predictions for each time point in the data (e.g., for prepost 0/withinday 0 for a given level of ""anxietychange"" and ""expectations"") I feed the model, not predictions of the change in the predicted variable as reflected by the fixed effect time indicators. </p>

<p>The following graphic indicates the core features of the data structure. Each pre-treatment and post-treatment (i.e., prepost 0/1) set of measurements is taken twice on two concurrent days, and are indicated in this data structure as having different subscripts.</p>

<p><a href=""http://i.stack.imgur.com/J8dei.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J8dei.png"" alt=""""></a></p>

<p>Is there any way to create such a complicated-seeming confidence interval around a prediction in R? My knowledge is pretty limited when it comes to functionality out of ""plug and play"" packages. Thank you so much for any help that you can provide! I am ready to be schooled.</p>
"
"0.0986302295273746","0.102337274193778","219343","<p>I am having problems with building a generalised linear model with random effects.</p>

<p>I am modelling how a sensitivity ratio between various taxa and cyanobacteria (logSR) is effected by the taxa and the class of chemicals (Class), taking into account the compounds and endpoints as random factors...</p>

<pre><code>model1 &lt;- lmer(logSR ~ Taxa * Class + (1|Compound) + (1|Endpoint), REML = TRUE, data = MET2, na.action = na.exclude)
</code></pre>

<p>Ive created the log sensitivity ratio between the different taxa and cyanobacteria for each compound where data is available. This approach allows me to compare various reproductive endpoints together that would otherwise not be comparable. However, it does mean that I have many 0's in the data where the cyanobacteria has been ratio'd with itself. this means that my model is not normal although the residuals appear nicely spaced around the mean - its just that the many 0's makes the peak in the centre too high for normal gaussian distribution. And my QQ plot is distorted at the ends.</p>

<p>I tried to remove the cyanobacteria 0s from the dataset but obviously the model then compares everything to the next taxa on the list and i need to compare with cyanobacteria.</p>

<p>There an error family that will allow me to incorporate this data into the model? I kind of zero-inflated gaussian error family type of thing?</p>

<p>Or is there another approach that I can consider?</p>

<p>I posted the above text in stack exchange and was suggested I repost here.  They also sugested that I try using the package cplm.  So I did....Its beginning to go past my statistical knowledge but it appears that maybe it might be able to deal with this spike at 0?  But when I try making the model</p>

<pre><code>model1 &lt;- cpglmm(logSR ~ Taxa * ABClass + (1|API) + (1|Endpoint), data = Met1)
</code></pre>

<p>I get this error message</p>

<pre><code>Error in if (!(validmu(mu) &amp;&amp; valideta(eta))) stop(""cannot find valid starting values: please specify some"",  : 
missing value where TRUE/FALSE needed
</code></pre>

<p>Is this an appropriate way to try and fix my problem and if so how do I got about fixing this error?</p>
"
"0.075498042361142","0.0783356573256404","219364","<p>I am having problems with building a generalised linear model with random effects.</p>

<p>I am modelling how a sensitivity ratio between various taxa and cyanobacteria (logSR) is effected by the taxa and the class of chemicals (Class), taking into account the compounds and endpoints as random factors...</p>

<pre><code>model1 &lt;- lmer(logSR ~ Taxa * Class + (1|Compound) + (1|Endpoint), REML = TRUE, data = MET2, na.action = na.exclude)
</code></pre>

<p>Ive created the log sensitivity ratio between the different taxa and cyanobacteria for each compound where data is available.  This approach allows me to compare various reproductive endpoints together that would otherwise not be comparable.  However, it does mean that I have many 0's in the data where the cyanobacteria has been ratio'd with itself.  this means that my model is not normal although the residuals appear nicely spaced around the mean - its just that the many 0's makes the peak in the centre too high for normal gaussian distribution.  And my QQ plot is distorted at the ends.</p>

<p>I tried to remove the cyanobacteria 0s from the dataset but obviously the model then compares everything to the next taxa on the list and i need to compare with cyanobacteria.</p>

<p>There an error family that will allow me to incorporate this data into the model?  I kind of zero-inflated gaussian error family type of thing?</p>

<p>Or is there another approach that I can consider?</p>
"
"0.116961064294386","0.121357078494567","220022","<p>I'm working on revising stats for a manuscript involving male reproductive success of deer. We measured three variables (<code>body size</code>, <code>antler size</code>, and <code>age</code>) of male deer in a captive population on an annual basis over a 6 year period, and goal was to determine the relative influence of each variable on annual reproductive success (i.e., number of fawns produced each year / not lifetime reproductive success).  I understand that my predictors are collinear and the issues created; however, there really is no way around including collinear predictors in our model as PCA's, etc. would essentially destroy the core question of our research.  We plan to use model averaging to evaluate predictors in the end so ran global model first.  Here is current code:</p>

<pre><code>global.model = glmer(Fawn ~ Age + I(Age^2) + Age*AvgAge + BodySize + I(BodySize^2) + 
                          BodySize*AvgAge + SSCM + I(SSCM^2) + SSCM*AvgAge + AvgAge + 
                          (1|Sire) + (1|Year), 
                     data=datum, family=poisson, na.action=""na.fail"")
</code></pre>

<p>Quadratic effects were included for predictors due to expected non-linearities.  Two random terms were included to account for the individual potential sires being sampled multiple times during the study and year (input as a factor) effects.  <code>AvgAge</code> is a term related to population demographics, and interactions with predictors are included.</p>

<p>So here are my questions:</p>

<ol>
<li><p>A reviewer suggested that there is temporal correlation in my data (e.g. male age at year one is correlated with male age at year two) that needs to be addressed.  He suggested including a temporal autocorrelation structure, or including Year as a numeric predictor to deal with this.  Am I missing something here or isn't this correlation the whole purpose of including the random effect for each male, coded as <code>(1|Sire)</code> in this case?  Also, including year as a numeric predictor really mucks things up because: </p>

<ol>
<li>I'm not interested in the specific effect of <code>Year</code>,  </li>
<li>there was a good bit of variability in the number of males we measured each year, and </li>
<li>it makes an already complex model more complex.</li>
</ol></li>
<li><p>Is the following code sufficient to use to screen for overdispersion?  So I assume here that if the ratio is &lt; 1 then you likely do not have issues with overdispersion?  </p>

<pre><code>overdisp.glmer(test)
# Residual deviance: 84.153 on 105 degrees of freedom (ratio: 0.801)
</code></pre></li>
</ol>
"
"0.128505183463769","0.123078541477327","220957","<p>I want to run a linear mixed model on a dependent variable DV that is collected under two different <code>Condition</code> at three different <code>Timepoint</code>. The data is:</p>

<pre><code>## dput(head(RawData,5))
    structure(list(Participant = structure(c(2L, 2L, 2L, 2L, 4L), 
  .Label = c(""Jessie"", ""James"", ""Gus"", ""Hudson"", ""Flossy"", 
 ""Bobby"", ""Thomas"", ""Alfie"", ""Charles"", ""Will"", ""Mat"", ""Paul"", ""Tim"", 
  ""John"", ""Toby"", ""Blair""), class = ""factor""), 
 xVarCondition = c(1, 1, 0, 0, 1), 
 Measure = structure(c(1L, 2L, 3L, 4L, 1L), 
.Label = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", 
""9"", ""10"", ""11"", ""12""), class = ""factor""), 
Sample = structure(c(1L, 2L, 1L, 2L, 1L), 
.Label = c(""1"", ""2""), class = ""factor""), 
 Condition = structure(c(2L, 2L, 1L, 1L, 2L),
 .Label = c(""AM"", ""PM""), class = ""factor""),
 Timepoint = structure(c(2L, 2L, 2L, 2L, 1L), 
.Label = c(""Baseline"", ""Mid"", ""Post""), class = ""factor""),
 DV = c(83.6381348645853, 86.9813802115179, 69.2691666620429, 
 71.3949807856125, 87.8931998204771)), 
.Names = c(""Participant"", ""xVarCondition"", ""Measure"", 
   ""Sample"", ""Condition"", ""Timepoint"", ""DV""), 
 row.names = c(NA, 5L), class = ""data.frame"")
</code></pre>

<p>Each <code>Participant</code> performs two trials per <code>Condition</code> across three <code>Timepoint</code> as depicted by <code>Measure</code>; however, there are missing data so not necessarily 12 levels per participant. The column <code>xVarCondition</code> is simply a dummy variable that includes a 1 for each entry of AM in <code>Condition</code>. The column <code>Sample</code> refers to the 2 trials for each Condition at each <code>Timepoint</code>.</p>

<p>I am an R user but the statistician is a SAS user who uses the following model, which is giving sensible answers:</p>

<pre><code>proc mixed data=RawData covtest cl alpha=&amp;alpha;
class Participant Condition Timepoint Measure Sample;
model &amp;dep=Condition Timepoint/s ddfm=sat outp=pred residual noint;
random int xVarCondition xVarCondition*TimePoint*Sample 
          TimePoint/subject=Participant s;
</code></pre>

<p>I believe the resulting lme4 syntax for the above SAS model to be:</p>

<pre><code>TestModel = lmer(DV ~ Condition + Timepoint + 
              (1 | Participant/Timepoint) +
              (0 + xVarCondition | Participant) +
              (1 | Participant:xVarCondition:Measure), data = RawData)
</code></pre>

<p>However, I get the following error when running this model:</p>

<pre><code>Error: number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>How do I correctly write the R syntax for lme4 to replicate the SAS model?</p>
"
"0.165595780267742","0.171819743822953","221037","<p>I'm analyzing the data for my master thesis; it's about photosynthetic efficiency for 10 different genotypes of a certain plant species (genotype=Accession in my dataset).</p>

<p>For each Accession, I've been measuring PPFD (light intensity) that is the continuous independent variable, and rETR (photosynthetic efficiency) that is the continuous dependent variable in the linear model I would like to run. </p>

<p>For the light intensities I've been measuring and the species I'm using, we can in fact consider the relation between PPFD and rETR approximately linear.</p>

<p>What I'm interested in is to see if the slopes given by the interaction between PPFD and Accession differ. In particular these Accessions come from Asia and Africa, so another thing would be to check if the ""Africa-slopes"" are different from the ""Asia-slopes"".</p>

<p>It's a given fact, that the origin of my lines should be in (0;0), since without light there is no electron transport in the plant and it cannot be negative. Therefore I'm after a model with no intercept.</p>

<p>I had a complete randomized block design, with 5 Blocks (for a total of 5x10 plants). In a block, one of the Accessions died out, so I'm temporarily excluding the whole block from the analysis (I measured the Blocks one by one, but always changing order).</p>

<p>I measured three leaves per plant, with all the Blocks measured twice per day (Mo and Af) and I repeated the measurements for 6 days.
After a quick analysis I decided it was ok to average rETR for the 3 leaves within one plant, since it didn't look like having a significant effect.</p>

<p>Therefore I remain with 3 blocking factors nested within each other:
Date (6 levels)
MoAf (2 levels)
Block (4 levels)
I thought I could use these to control for environmental noise.</p>

<p>What I did first was to run a linear model with lm(), without considering my blocking factors, so to have a general idea of what was going on:</p>

<pre><code>    LM3E = lm(rETR ~ 0 + PPFD:Accession, data=avpsN, na.action=na.omit)
</code></pre>

<p>Then I tried to fit a mixed effect model with lmer(), to take into account my blocking factors and compare it with the previous one. I was only able to ask for a random intercept, but it is not really what I want:</p>

<pre><code>    lER3C = lmer(rETR ~ 0 + PPFD:Accession + (1|Date:MoAf:Block), data=avps,   na.action=na.omit)
</code></pre>

<p>And then, out of curiosity, I tried to fit something similar to LM3E, but using lmer to see if they were going to give the same outcome. Since lmer() does not work without random effects, I had to specify as random what I would like to keep as fixed:</p>

<pre><code>    lER3 = lmer(rETR ~ 0 + (PPFD-1|Accession), data=avps, na.action=na.omit)
</code></pre>

<p>My concerns are:</p>

<ol>
<li><p>What are the differences between lER3 and lER3E? The slope coefficients given by the two models are exactly the same. Is it better to specify Accession as a random effect (so the opposite of what I would like to do?) or in this case it does not matter, but it would if I also included my blocking factors?</p></li>
<li><p>If I'm trying to specify a linear model with origin in (0;0), lER3C is somehow pushing me away from what should be ""reality"" (no rETR in darkness), but I think it also better corrects for variation between days etc. (Maybe not really a question for stackexchange?)</p></li>
</ol>

<p>This is my first question posted here, so I'm sorry if it's far too long and verbose :)</p>
"
"0.138390197576366","0.133335086600437","221161","<p>I have been having trouble with the predict function underestimating (or overestimating) the predictions from an lmer model with some polynomials. Hopefully my edits make it clearer. I have scaled data that looks like this:</p>

<pre><code>Terr      Date     Year            Age  
T.092     123      0.548425     -0.86392            
T.104     102      1.2072       -0.48185            
T.104     105      1.075445     -0.86392            
T.104     112      0.94369      -1.24599            
T.040     116     -0.2421        2.192652           
T.040     114     -0.37386       1.810581           
T.040     119     -0.50561       1.428509           
T.040     128      0.15316      -0.09978            
T.040     113      0.021405     -0.48185
</code></pre>

<p>Iâ€™m trying to determine how Year affects lay date after controlling for Age, with Terr (territory) as a random variable. I usually include polynomials and do model averaging, but whether I use a single model or do model averaging, the predict function gives predictions that are a bit lower or higher than they should be. I realize that the model below would not be a good model for this data, Iâ€™m just trying to provide a simplified example.  </p>

<p>Below is my code  </p>

<pre><code>library(lme4
m1 &lt;- lmer(Date ~ (1|Terr) + Year + Age + I(Age^2), data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictions=predict(m1, newdata = new.dat, re.form=NA)
pred.l&lt;-cbind(new.dat, Predictions)
pred.l  

      Year          Age Predictions
    1   -2 2.265676e-16    124.4439
    2   -1 2.265676e-16    123.2124
    3    0 2.265676e-16    121.9810
    4    1 2.265676e-16    120.7496
</code></pre>

<p>When plotted with the means, the graph looks like this:</p>

<p><a href=""http://i.stack.imgur.com/mwIpJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mwIpJ.jpg"" alt=""graph1""></a></p>

<p>When I use effects, I get a much better fit  </p>

<pre><code>library(effects)
ef.1c=effect(c(""Year""), m1, xlevels=list(Year=-2:1))
pred.lc=data.frame(ef.1c)
pred.lc

      Year      fit        se    lower    upper
    1   -2 126.0226 0.6186425 124.8089 127.2363
    2   -1 124.7911 0.4291211 123.9493 125.6330
    3    0 123.5597 0.3298340 122.9126 124.2068
    4    1 122.3283 0.3957970 121.5518 123.1048
</code></pre>

<p><a href=""http://i.stack.imgur.com/SvI3f.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SvI3f.jpg"" alt=""graph2""></a></p>

<p>After much trial and error, I have discovered that the problem is with the Age polynomial, because when the Age polynomial is not included, the predicted and fitted are equal and both fit well. Below is the same  model but with Age as a linear term.  </p>

<pre><code>m2 &lt;- lmer(Date ~ (1|Terr) + Year + Age, data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsd=predict(m2, newdata = new.dat, re.form=NA)  
pred.ld&lt;-cbind(new.dat, Predictionsd)
pred.ld

      Year          Age Predictionsd
    1   -2 2.265676e-16     125.9551
    2   -1 2.265676e-16     124.7653
    3    0 2.265676e-16     123.5755
    4    1 2.265676e-16     122.3857

library(effects)
ef.1e=effect(c(""Year""), m2, xlevels=list(Year=-2:1))
pred.le=data.frame(ef.1e)
pred.le

      Year      fit        se    lower    upper
    1   -2 125.9551 0.6401008 124.6993 127.2109
    2   -1 124.7653 0.4436129 123.8950 125.6356
    3    0 123.5755 0.3406741 122.9072 124.2439
    4    1 122.3857 0.4093021 121.5827 123.1887
</code></pre>

<p>I do many similar analyses, and this issue with the predictions being slightly lower (or higher) than they should be often happens when Age is included as a polynomial. When I include a polynomial for Year, there is no problem and the predicted and fitted are equal, so I know the problem is not with all polynomials.</p>

<pre><code>m3 &lt;- lmer(Date ~ (1|Terr) + Year + I(Year^2) + Age, data=data)

new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsf=predict(m3, newdata = new.dat, re.form=NA)  
pred.lf&lt;-cbind(new.dat, Predictionsf)
pred.lf

      Year          Age Predictionsf
    1   -2 2.265676e-16     125.6103
    2   -1 2.265676e-16     124.8494
    3    0 2.265676e-16     123.7483
    4    1 2.265676e-16     122.3070

library(effects)
ef.1g=effect(c(""Year""), m3, xlevels=list(Year=-2:1))
pred.lg=data.frame(ef.1g)
pred.lg

      Year      fit        se    lower    upper
    1   -2 125.6103 0.8206625 124.0003 127.2203
    2   -1 124.8494 0.4615719 123.9438 125.7549
    3    0 123.7483 0.4275858 122.9094 124.5871
    4    1 122.3070 0.4262110 121.4708 123.1431
</code></pre>

<p>I've looked for answers (e.g., <a href=""http://stats.stackexchange.com/questions/180010/overestimated-and-underestimated-predictions-in-regression"">here</a>) but haven't found anything that is directly helpful. I can provide the whole data set if needed. Does anyone have any insight?</p>
"
"0.0827039616973562","0.0858124131484961","221516","<p>Let's say we have two models specified by the following formulas in R's <code>lmer()</code>:</p>

<p>i)   <code>Y ~ A + B + (A:B|SUBJECT)</code></p>

<p>ii)  <code>Y ~ A + B + (SUBJECT|A:B)</code></p>

<p>For the random effects, equation i) specifies a random slope and intercept for each level of <code>A:B</code> by subject, as equation ii) specifies a random slope and intercept for each subject by level of <code>A:B</code> (if i am not interpreting this wrongly).</p>

<p>For both these model formulas, what are the differences in the linear mixed effects model equation regarding the random effects? And is the model ii) wrongly specified? </p>
"
"0.133356131201899","0.138368358561332","223008","<p>Good morning all!I am trying to run a binomial gmler model.
My response variable is a binomial variable:  extra pair paternity -->( 1 or 0) I am looking at several continuous variables like weight, tarsus and number of eggs lost. However, I am having problems with my random effects. I would appreciate any help, because I am already 2 days trying to figure out!! Thanks a lot!!!</p>

<p>my data:      </p>

<pre><code>ring_id    nest nest_id number_eggs number_chicks lost_eggs ring_year tarsus weight
1 BD29285 WH00060       6          10            10         0      2016    210   1700
2 BD29286 WH00060       6          10            10         0      2016    200   1510
3 BD29287 WH00060       6          10            10         0      2016    199   1540
4 BD29288 WH00060       6          10            10         0      2016    209   1780
5 BD29289 WH00060       6          10            10         0      2016    199   1670
6 BD29290 WH00060       6          10            10         0      2016    199   1670
  number_epy epy_wpy EPP_nest Epfather
1          0     WPY        0         
2          0     WPY        0         
3          0     WPY        0         
4          0     WPY        0         
5          0     WPY        0         
6          0     WPY        0         
&gt; 
</code></pre>

<p>This is my code</p>

<pre><code>m &lt;- lmer(EPP_nest ~ weight + tarsus + lost_eggs + (1|nest_id) + (1| ring_id) ,family = 'binomial', data=chicks)

summary (m)
</code></pre>

<p>output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod
]
 Family: binomial  ( logit )
Formula: EPP_nest ~ weight + tarsus + lost_eggs + (1 | nest_id) + (1 |      ring_id)
   Data: chicks
Control: structure(list(optimizer = c(""bobyqa"", ""Nelder_Mead""), calc.derivs = TRUE,  
    use.last.params = FALSE, restart_edge = FALSE, boundary.tol = 1e-05,  
    tolPwrss = 1e-07, compDev = TRUE, nAGQ0initStep = TRUE, checkControl = structure(list( 
        check.nobs.vs.rankZ = ""ignore"", check.nobs.vs.nlev = ""stop"",  
        check.nlev.gtreq.5 = ""ignore"", check.nlev.gtr.1 = ""stop"",  
        check.nobs.vs.nRE = ""stop"", check.rankX = ""message+drop.cols"",  
        check.scaleX = ""warning"", check.formula.LHS = ""stop"",  
        check.response.not.const = ""stop""), .Names = c(""check.nobs.vs.rankZ"",  
    ""check.nobs.vs.nlev"", ""check.nlev.gtreq.5"", ""check.nlev.gtr.1"",  
    ""check.nobs.vs.nRE"", ""check.rankX"", ""check.scaleX"", ""check.formula.LHS"",  
    ""check.response.not.const"")), checkConv = structure(list( 
        check.conv.grad = structure(list(action = ""warning"",  
            tol = 0.001, relTol = NULL), .Names = c(""action"",  
        ""tol"", ""relTol"")), check.conv.singular = structure(list( 
            action = ""ignore"", tol = 1e-04), .Names = c(""action"",  
        ""tol"")), check.conv.hess = structure(list(action = ""warning"",  
            tol = 1e-06), .Names = c(""action"", ""tol""))), .Names = c(""check.conv.grad"",  
    ""check.conv.singular"", ""check.conv.hess"")), optCtrl = list()), .Names = c(""optimizer"",  
""calc.derivs"", ""use.last.params"", ""restart_edge"", ""boundary.tol"",  
""tolPwrss"", ""compDev"", ""nAGQ0initStep"", ""checkControl"", ""checkConv"",  
""optCtrl""), class = c(""glmerControl"", ""merControl""))

     AIC      BIC   logLik deviance df.resid 
    56.0     77.5    -22.0     44.0      259 

Scaled residuals: 
      Min        1Q    Median        3Q       Max 
-0.001717 -0.001158 -0.000051  0.020852  0.041496 

Random effects:
 Groups  Name        Variance Std.Dev.
 ring_id (Intercept)    0      0.00   
 nest_id (Intercept) 6613     81.32   
Number of obs: 265, groups:  ring_id, 265; nest_id, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.106e+01  2.222e+01  -0.498  0.61854    
weight      -6.211e-04  7.607e-03  -0.082  0.93493    
tarsus      -6.362e-03  1.157e-01  -0.055  0.95615    
lost_eggs   -6.395e+00  1.777e+00  -3.599  0.00032 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
          (Intr) weight tarsus
weight    -0.252              
tarsus    -0.857 -0.272       
lost_eggs  0.212 -0.170 -0.070
convergence code: 0
Model failed to converge with max|grad| = 0.00997492 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>So I am quite lost.... and I would really apreciate any help!!!!! </p>

<p>Thank you very much!!1
Best, Mara</p>
"
"0.0739726721455309","0.0767529556453336","223296","<p>I am attempting to model the yield of various crops as a function of weather data, namely one temperature variable and 7 moisture-related variables (measuring different aspects of moisture content). The moisture readings exhibited a significant degree of collinearity and were all using different units, and so as recommended by some other answers on this site, I scaled the moisture variables and applied Principal Component Analysis, picking the PCs that accounted for > 95% of the variance cumulatively.</p>

<p>However, I now have a question regarding when to scale the data prior to applying machine learning techniques. I'm trying to build a mixed effects model with <code>lmer</code> in <code>lme4</code> package. Since the PCs were obtained by scaling <strong>only</strong> the moisture data, if I wanted to make a model of the form 
<code>yield ~ temperature + PC1 +... + PCN + (1|categorical vars)</code>, would I need to re-scale the dataset consisting of <code>temperature</code>, <code>PC1</code>,...,<code>PCN</code>?</p>

<p>Also, is it recommended to scale the response variable as well? Any clarification and help would be much appreciated; I'm only just getting started on this path.</p>
"
"0.152734453799701","0.15847502081498","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.219499219232481","0.233742557474421","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.0523065780965941","0.0542725354129257","224165","<p>I built a mixed linear regression model which includes a dependent variable 'dv', independent variable 'v1' &amp; 'v2', and subject ID 'subject'. </p>

<p>The R syntax is shown below:</p>

<p>output &lt;-lmer(dv ~ v1 + v2 + v1*v2 + (1|subject)+(0+ v1|subject)+(0+ v2 |subject), data=matrix, control=lmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=2e5)))</p>

<p>The dataset is here:
<a href=""https://1drv.ms/t/s!AitdBHtSjoIpiCXWmzLEnvqX1iuE"" rel=""nofollow"">dataset</a></p>

<p><em>My question:</em></p>

<p>There is a point very strange, the significance of interaction is not consistent with the data pattern (as shown on attached plot), the fixed effect of interaction shows a very small p-value, meaning the slope of two lines should be significantly different, however, the plot does not show an expected pattern. I am worrying there is something wrong, which one (mixed model output or plot result) is correct?</p>

<p><a href=""http://i.stack.imgur.com/IMWEm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMWEm.png"" alt=""enter image description here""></a></p>
"
"0.104613156193188","0.09497693697262","224372","<p>I have a two-part question that includes issues with generalized linear mixed models and failure to converge. </p>

<p>First, a little bit about my experimental design. I have data where I am trying to test the effects of population and genotype of a tree on the propensity to flower. </p>

<ul>
<li>I went to multiple locations (i.e. populations) to collect cuttings from trees.  </li>
<li>At each tree I took multiple cuttings (each tree is considered a genotype, hence the multiple cuttings from each tree were my genotype replicates). </li>
<li>I then planted all of my cuttings at a single site. Trees were planted in plots of 16 trees, where all 16 trees came from the same population (this is the focus of one of my questions). </li>
</ul>

<p>To analyze the data, I am trying to use the <code>bglmer</code> function in the ""blme"" package of R (as per <a href=""http://stackoverflow.com/questions/25985970/generalised-linear-mixed-model-error-binary-response"">http://stackoverflow.com/questions/25985970/generalised-linear-mixed-model-error-binary-response</a>). </p>

<p>My model contains <code>Population</code>, <code>Genotype</code> nested within <code>Population</code> and <code>Plot</code> as factors. <code>Population</code> and <code>Genotype</code> are treated as random effects and <code>Plot</code> as a fixed effect. </p>

<p>My R code is:</p>

<pre><code>Flower_bayesfull &lt;- bglmer(data=Tam, Flower ~ Plot + (1| Population/GenotypeB), family=binomial, cov.prior=NULL, fixef.prior=normal)
</code></pre>

<p>I am getting warnings about failure to converge with <code>max|grad| = 4.74867</code>.</p>

<p>So my two <strong>questions</strong> are: </p>

<ol>
<li>By including <code>Plot</code> in my analysis, am I properly accounting for my plot design (described above)?</li>
<li>How can I get my model to converge? Or is it impossible, considering my plot structure described above.</li>
</ol>
"
"0.166095693194512","0.164504880383845","225241","<p>Consider a mixed model as follows.</p>

<pre><code>library(lme4)
# Load data
data &lt;- structure(list(blk = c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3L),
                       gent = c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10L),
                       yld = c(83, 77, 78, 78, 70, 75, 74, 79, 81, 81, 91, 79, 78, 92, 79, 87, 81, 96, 89, 82L),
                       syld = c(250, 240, 268, 287, 226, 395, 450, 260, 220, 237, 227, 281, 311, 258, 224, 238, 278, 347, 300, 289L)),
                  .Names = c(""blk"", ""gent"", ""yld"", ""syld""), class = ""data.frame"", row.names = c(NA, -20L))
data$blk &lt;- as.factor(data$blk)
data$gent &lt;- as.factor(data$gent)
</code></pre>

<p>The data is unbalanced.</p>

<pre><code># Mixed effect model
frmla &lt;- ""syld ~ 1 + gent + (1|blk)""
library(lme4)
model &lt;- lmer(formula(frmla), data = data)

model
Linear mixed model fit by REML ['merModLmerTest']
Formula: syld ~ 1 + gent + (1 | blk)
   Data: data
REML criterion at convergence: 73.9572
Random effects:
 Groups   Name        Std.Dev.
 blk      (Intercept)  9.385  
 Residual             16.919  
Number of obs: 20, groups:  blk, 3
Fixed Effects:
(Intercept)        gent2        gent3        gent4        gent5        gent6        gent7        gent8        gent9  
    256.000      -28.000       -8.333        8.000       32.127       43.678      -36.805       90.678       62.127  
     gent10       gent11       gent12  
     32.678      132.195      187.195  
</code></pre>

<p>Primarily I want to compare the <code>gent</code> levels by LS means.</p>

<pre><code>library(""lmerTest"")
lsmeans(model)
Least Squares Means table:
         gent Estimate Standard Error   DF t-value Lower CI Upper CI p-value    
gent  1   1.0    256.0           11.2  6.9    22.9      229      283  &lt;2e-16 ***
gent  2   5.0    228.0           11.2  6.9    20.4      201      255  &lt;2e-16 ***
gent  3   6.0    247.7           11.2  6.9    22.2      221      274  &lt;2e-16 ***
gent  4   7.0    264.0           11.2  6.9    23.6      237      291  &lt;2e-16 ***
gent  5   8.0    288.1           18.5  8.0    15.6      245      331  &lt;2e-16 ***
gent  6   9.0    299.7           18.5  8.0    16.2      257      342  &lt;2e-16 ***
gent  7  10.0    219.2           18.5  8.0    11.8      177      262  &lt;2e-16 ***
gent  8  11.0    346.7           18.5  8.0    18.8      304      389  &lt;2e-16 ***
gent  9  12.0    318.1           18.5  8.0    17.2      275      361  &lt;2e-16 ***
gent  10  2.0    288.7           18.5  8.0    15.6      246      331  &lt;2e-16 ***
gent  11  3.0    388.2           18.5  8.0    21.0      346      431  &lt;2e-16 ***
gent  12  4.0    443.2           18.5  8.0    24.0      401      486  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In addition I am interested in variance partitioning.</p>

<p>The variance component due to random effect and residual can be estimated as follows.</p>

<pre><code>VCrandom &lt;- VarCorr(model)
print(VCrandom, comp = ""Variance"")
 Groups   Name        Variance
 blk      (Intercept)  88.083 
 Residual             286.250
</code></pre>

<p>How to partition the total variance into components due to each of the factors <code>gent</code> and <code>blk</code> along with the residual ? Something similar to the output given by <code>PROC MIXED</code> of <code>SAS</code>, where MSE is computed even when estimation is by ML or REML instead of least squares.</p>

<p>Should I treat the fixed effect as random just for the purpouse of getting variance component ?</p>

<pre><code>frmla2 &lt;- ""syld ~ 1 + (1|gent) + (1|blk)""
model2 &lt;- lmer(formula(frmla2), data = data)
model2

VCrandom2 &lt;- VarCorr(model2)
print(VCrandom2, comp = ""Variance"")
 Groups   Name        Variance
 gent     (Intercept) 4152.08 
 blk      (Intercept)  116.11 
 Residual              274.92 
</code></pre>

<p>If there is no random effect, variance components can be estimated using the least squares approach (ANOVA, Sum of squares, MSE).</p>

<p>The package <code>mixlm</code> has provision for variance partitioning using SS in case of mixed models.</p>

<pre><code>library(mixlm)

mixlm &lt;- lm(syld ~ 1 + r(gent) + r(blk), data)

Anova(mixlm, type=""III"")

Analysis of variance (unrestricted model)
Response: syld
          Mean Sq   Sum Sq Df F value Pr(&gt;F)
gent      5360.49 58965.36 11   18.73 0.0009
blk        638.58  1277.17  2    2.23 0.1886
Residuals  286.25  1717.50  6       -      -

            Err.term(s) Err.df VC(SS)
1 gent              (3)      6 3044.5
2 blk               (3)      6   52.8
3 Residuals           -      -  286.3
(VC = variance component)

               Expected mean squares
gent      (3) + 1.66666666666667 (1)
blk       (3) + 6.66666666666667 (2)
Residuals (3)                       

WARNING: Unbalanced data may lead to poor estimates
</code></pre>

<p>The estimates are different</p>

<pre><code># Total variance
var(data$syld)

|source   |  model1|  model2|  mixlm|
|:--------|-------:|-------:|------:|
|gent     |      NA| 4152.08| 3044.5|
|blk      |  88.083|  116.11|   52.8|
|Residual | 286.250|  274.92|  286.3|
</code></pre>

<p>Can fixed effect variance be extracted using <code>predict</code> function as suggested here <a href=""https://sites.google.com/site/alexandrecourtiol/what-did-i-learn-today/inrhowtoextractthedifferentcomponentsofvarianceinalinearmixedmodel"" rel=""nofollow"">In R: How to extract the different components of variance in a linear mixed model!</a> ?</p>

<pre><code>var(predict(model))
</code></pre>

<p>Which is the most appropriate method compatible with <code>(RE)ML</code> estimates in lme4 ?</p>
"
"0.128505183463769","0.133335086600437","226505","<p>I run two <code>lmer</code> tests, one with and one without the interaction term between fixed effects. The problem is that the former gives an output result that makes no sense to the actual data (i.e. negative slope instead of positive), whereas the latter shows the expected output. Why does this happen and even though the interaction is significant (and also makes sense) does it mean that I should not include it in the model due to wrong output? Would it be better to run a model with only the fixed factors and another with the interaction term alone?</p>

<p>Below is the models and their outputs. Thank you!</p>

<p>(WITHOUT INTERACTION TERM)</p>

<pre><code>mTEST&lt;- lmer(amp.sqrt~ treatment + time + axis + (1+treatment|ID))
summary(mTEST)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: amp.sqrt ~ treatment + time + axis + (1 + treatment | ID)

REML criterion at convergence: 5682.2

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2769 -0.7678 -0.0236  0.6049  3.5182 

Random effects:
 Groups   Name        Variance Std.Dev. Corr       
 ID       (Intercept)  602.8   24.55               
          treatment2  1028.9   32.08    -0.14      
          treatment3   283.2   16.83    -0.03  0.52
 Residual             2027.6   45.03               
Number of obs: 540, groups:  ID, 21

Fixed effects:
            Estimate Std. Error      df t value Pr(&gt;|t|)    
(Intercept)  115.184      7.546  36.300  15.265  &lt; 2e-16 ***
treatment2     2.644      8.571  18.400   0.308  0.76117    
treatment3    23.365      6.139  19.200   3.806  0.00117 ** 
time7         13.958      4.707 474.800   2.965  0.00318 ** 
time8         21.799      4.787 478.500   4.554  6.7e-06 ***
axis2         60.458      4.746 474.800  12.737  &lt; 2e-16 ***
axis3        128.456      4.746 474.800  27.063  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) trtmn2 trtmn3 time7  time8  axis2 
treatment2 -0.287                                   
treatment3 -0.299  0.506                            
time7      -0.312  0.000  0.000                     
time8      -0.314  0.013  0.008  0.492              
axis2      -0.315  0.000  0.000  0.000  0.000       
axis3      -0.315  0.000  0.000  0.000  0.000  0.500
</code></pre>

<p>(WITH INTERACTION TERM)</p>

<pre><code>mTEST2&lt;- lmer(amp.sqrt~ treatment * time + axis + (1+treatment|ID))
summary(mTEST2)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: amp.sqrt ~ treatment * time + axis + (1 + treatment | ID)

REML criterion at convergence: 5615.6

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.7117 -0.7237 -0.0390  0.6140  3.3017 

Random effects:
 Groups   Name        Variance Std.Dev. Corr       
 ID       (Intercept)  619.0   24.88               
          treatment2  1061.1   32.58    -0.16      
          treatment3   296.4   17.22    -0.06  0.54
 Residual             1879.0   43.35               
Number of obs: 540, groups:  ID, 21

Fixed effects:
                 Estimate Std. Error      df t value Pr(&gt;|t|)    
(Intercept)       130.587      8.417  55.500  15.515  &lt; 2e-16 ***
treatment2         -3.766     10.713  44.500  -0.352   0.7269    
treatment3        -14.929      8.851  83.600  -1.687   0.0954 .  
time7              -7.697      8.120 471.000  -0.948   0.3436    
time8              -2.628      8.120 471.000  -0.324   0.7464    
axis2              60.458      4.569 471.000  13.232  &lt; 2e-16 ***
axis3             128.456      4.569 471.000  28.113  &lt; 2e-16 ***
treatment2:time7    9.697     11.206 471.000   0.865   0.3873    
treatment3:time7   53.206     11.206 471.000   4.748 2.73e-06 ***
treatment2:time8    8.554     11.396 473.700   0.751   0.4532    
treatment3:time8   62.411     11.289 473.300   5.528 5.35e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) trtmn2 trtmn3 time7  time8  axis2  axis3  trt2:7 trt3:7 trt2:8
treatment2  -0.448                                                               
treatment3  -0.479  0.515                                                        
time7       -0.482  0.379  0.459                                                 
time8       -0.482  0.379  0.459  0.500                                          
axis2       -0.271  0.000  0.000  0.000  0.000                                   
axis3       -0.271  0.000  0.000  0.000  0.000  0.500                            
trtmnt2:tm7  0.349 -0.523 -0.332 -0.725 -0.362  0.000  0.000                     
trtmnt3:tm7  0.349 -0.275 -0.633 -0.725 -0.362  0.000  0.000  0.525              
trtmnt2:tm8  0.344 -0.514 -0.327 -0.356 -0.712  0.000  0.000  0.492  0.258       
trtmnt3:tm8  0.347 -0.272 -0.628 -0.360 -0.719  0.000  0.000  0.261  0.496  0.512
</code></pre>
"
"0.128505183463769","0.133335086600437","228800","<p>Here is how I have understood nested vs. crossed random effects:  </p>

<p><strong>Nested random effects</strong> occur when a lower level factor appears only within a particular level of an upper level factor.  </p>

<ul>
<li>For example, pupils within classes at a fixed point in time.  </li>
<li><p>In <code>lme4</code> I thought that we represent the random effects for nested data in either of two equivalent ways:  </p>

<pre><code>(1|class/pupil)  # or  
(1|class) + (1|class:pupil)
</code></pre></li>
</ul>

<p><strong>Crossed random effects</strong> means that a given factor appears in more than one level of the upper level factor.  </p>

<ul>
<li>For example, there are pupils within classes measured over several years.  </li>
<li><p>In <code>lme4</code>, we would write: </p>

<pre><code>(1|class) + (1|pupil)
</code></pre></li>
</ul>

<p>However, when I was looking at a particular nested dataset, I noticed that both model formulas gave identical results (code and output below). However I have seen other datasets where the two formulas produced different results.  So what is going on here? </p>

<pre><code>mydata &lt;- read.csv(""www-personal.umich.edu/~bwest/classroom.csv"")
# Crossed version: 
Linear mixed model fit by REML ['lmerMod']
Formula: mathgain ~ (1 | schoolid) + (1 | classid)
   Data: mydata

REML criterion at convergence: 11768.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.6441 -0.5984 -0.0336  0.5334  5.6335 

Random effects:
 Groups   Name        Variance Std.Dev.
 classid  (Intercept)   99.23   9.961  
 schoolid (Intercept)   77.49   8.803  
 Residual             1028.23  32.066  
Number of obs: 1190, groups:  classid, 312; schoolid, 107


# Nested version:
Formula: mathgain ~ (1 | schoolid/classid)

REML criterion at convergence: 11768.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.6441 -0.5984 -0.0336  0.5334  5.6335 

Random effects:
 Groups           Name        Variance Std.Dev.
 classid:schoolid (Intercept)   99.23   9.961  
 schoolid         (Intercept)   77.49   8.803  
 Residual                     1028.23  32.066  
Number of obs: 1190, groups:  classid:schoolid, 312; schoolid, 107
</code></pre>
"
"0.118620169351171","0.133335086600437","228958","<p>I calculated a linear mixed model using the packages lme4 and lsmeans with the lmer-function, where i have one dependent variable rv and the interacting factors treatment, time, age and race. I'm interested in the <strong><em>response variable change over time</em></strong>, that's why i use the <strong><em>lstrends</em></strong>-function. So far so good. The problem is, i have to square root the response variable in order to fit the model properly. But the pairs-function only gives out a response to the square root of the rv, hard to interpret!</p>

<p>So i tried to back-transform the response variable after pairs:     </p>

<pre><code>model.lmer &lt;- lmer(sqrt(rv) ~ treat*time*age*race + (1|individual), data=mydata)
model.lst &lt;- lstrends(model.lmer, ~treat | age*race , var = ""time"", type=""response"")
pairs(mouse.lst, type=""response"")
</code></pre>

<p>This obviously doesn't work, as stated by the package itself:</p>

<pre><code># Transformed response
sqwarp.rg &lt;- ref.grid(update(warp.lm, sqrt(breaks) ~ .))
summary(sqwarp.rg)

# Back-transformed results - compare with summary of 'warp.rg'
summary(sqwarp.rg, type = ""response"")

# But differences of sqrts can't be back-transformed
summary(pairs(sqwarp.rg, by = ""wool""), type = ""response"")

# We can do it via regrid
sqwarp.rg2 &lt;- regrid(sqwarp.rg)
summary(sqwarp.rg2)  # same as for sqwarp.rg with type = ""response""
pairs(sqwarp.rg2, by = ""wool"")
</code></pre>

<p>Anybode got an idea how to solve this particular problem? Thanks in advance!</p>

<p>edit1:</p>

<p>It could look like the following code:</p>

<pre><code>summary(pairs(lsmeans(rg.regrid, ~ treat | race*age, trend=""time"")), type=""response"")
</code></pre>

<p>The problem is, i can't alter the reference grid for lstrends, just for lsmeans, because the first argument in lstrends or lsmeans with trend=""time"" requires the linear mixed effect model (model.lmer) intead of just the reference grid like in lsmeans, without the trend-argument... That's probably why i can't back-transform the data with </p>

<p>edit2: This here sums up my problem pretty well:</p>

<pre><code>model.sqrt &lt;- lmer(sqrt(rv) ~ time*treat*race*age, data=mydata)
rg &lt;- ref.grid(model.sqrt)
rg.regrid &lt;- regrid(rg)
summary(pairs(lsmeans(rg.regrid, ~treat | race*age*time), type = ""response""))
</code></pre>

<p>works perfectly.</p>

<pre><code>summary(pairs(lsmeans(rg.regrid, ~treat | race*age, trend=""time""), type = ""response""))
</code></pre>

<p>Gives the following error:</p>

<pre><code>Error in summary(pairs(lsmeans(rg.regrid, ~treat | race * age, trend = ""time""),  : 
error in evaluating the argument 'object' in selecting a method for function 'summary': Error in data[[var]] : subscript out of bounds
</code></pre>

<p>How to avoid the error and still be able to back-transform my data?</p>

<p>edit3:</p>

<pre><code>model &lt;- lme(sqrt(dv) ~ time*treat*race*age, random=~1|individual, data=mydata, weights=varPower(0.19, form = ~time|individual), method=""ML"")

lsms &lt;- summary(pairs(model, ~treat | time*race*age, at=list(time=4))))$estimate

slome &lt;- summary(pairs(lstrends(model, ~treat | race*age, var=""time"")))$estimate
slose &lt;- summary(pairs(lstrends(model, ~treat | race*age, var=""time"")))$SE

for(i in 1:4){
    eslo[i] &lt;- 2 * lsms[i] * slome[i]
    ese[i] &lt;- abs(2*lsms[i]) * slose[i]
    }
</code></pre>

<p>i = 1 is race1 at age1; </p>

<p>i = 2 is race1 at age2; </p>

<p>i = 3 is race2 at age1; </p>

<p>i = 4 is race2 at age2; </p>

<p>slose: slope-SE from lstrends for sqrt(dv)-difference between treated and untreated group </p>

<p>slome: slope from lstrends for sqrt(dv)-difference per time between treated and untreated group</p>

<p>eslo and ese: estimated slope and se for dv-difference per time between treated and untreated group</p>
"
"0.0905976508333704","0.0940027887907685","229370","<p>I built a generalized linear mixed-effects model (GLMM) using <code>glmer</code> function from the <code>lme4</code> package in <code>r</code> to model species richness around aquaculture sites based on significant explanatory variables using Zuur et al. (2009) <em><a href=""http://rads.stackoverflow.com/amzn/click/1441927646"" rel=""nofollow"">Mixed Effects Models and Extensions in Ecology with R</a></em>. The dataset used to build the model has ~ 1000 samples and my best model is:</p>

<pre><code>Mod1 &lt;- glmer(Richness ~ Distance + Depth + Substrate + Beggiatoa + 
        Distance*Beggiatoa + (1|Site/transect), family = poisson, data = mydata)
</code></pre>

<p>Now I have a full data set collected at different sites and I want to assess how this model performs on the new data set.</p>

<p>I never assessed model performance on a new data set before and my first approach would be to compare R^2 values between data sets and p-values of each explanatory variables. However, Iâ€™m sure there are much better options. <strong>What technique would you use to assess model performance on a new data set?</strong></p>
"
"0.0369863360727655","0.0383764778226668","230721","<p>I have the following model  </p>

<pre><code>fit1 &lt;- glmer(Res~FA+FB+FC+(1|fsite), family=binomial(), data=DATA)
</code></pre>

<p>the result of <code>summary()</code> is:  </p>

<pre><code>summary(fit1)
Generalized linear mixed model fit by maximum likelihood 
 (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Res ~ FA + FB + FC + (1 | fsite)
   Data: DATA

     AIC      BIC   logLik deviance df.resid 
   202.3    229.9    -92.1    184.3      150 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.1768 -0.6167 -0.4967  0.6815  2.0132 

Random effects:
 Groups Name        Variance Std.Dev.
 fsite  (Intercept) 0        0       
Number of obs: 159, groups:  fsite, 28

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.55573    0.55830   2.787 0.005327 ** 
FA2         -0.11914    0.37344  -0.319 0.749692    
FB2         -1.38652    0.39967  -3.469 0.000522 ***
FC2         -0.14976    0.61984  -0.242 0.809076    
FC3         -0.06794    0.63171  -0.108 0.914350    
FC4         -1.20114    0.61670  -1.948 0.051452 .  
FC5         -1.44951    0.62817  -2.308 0.021025 *  
FC6         -1.13590    0.65427  -1.736 0.082538 .  
---
Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1

Correlation of Fixed Effects:
        (Intr) fspcs2    FB2    FC2    FC3    FC4    FC5 
FA2     -0.466                                          
FB2     -0.456  0.169                                   
FC2     -0.572 -0.021  0.017                            
FC3     -0.596  0.050  0.036  0.506                     
FC4     -0.582 -0.005  0.020  0.519  0.509              
FC5     -0.558  0.019 -0.038  0.508  0.500  0.511       
FC6     -0.391 -0.101 -0.288  0.485  0.467  0.486  0.492
</code></pre>

<ul>
<li>Why are the variance and Std.Dev of the random effects zero?</li>
<li>How do I check for overdispersion in this model?</li>
<li>What should do if there is overdispersion?</li>
</ul>
"
"0.11744739098372","0.121861683908065","230802","<p>I am analyzing data from cohort of 500 calves investigating the impact of disease on growth.</p>

<p>My outcome variables are normally distributed, continuous data. I am using hierarchical models with calf nested within farms and testing for the longer term impacts of disease.</p>

<p>The problem I am having is with how to include disease data. I have variables for the number of weeks a calf had disease and the total score over a validated threshold for diagnosis</p>

<p><img src=""https://drive.google.com/file/d/0B7BdsyR1JIOjemZlNDlvREtFOVU/view?usp=sharing"" alt=""Histograms of Calf Disease Data""></p>

<p>As I am inexperienced in uploading images, here are the tabulated results of the data above:</p>

<hr>

<pre><code>Disease Duration (weeks) 0   1    2   3   4   5   6 
Frequency               266 128  50  33   8   5   2
</code></pre>

<hr>

<pre><code>Total Score  0   1   2   3    4   5   6   7   8   9  10  13  14  15 
Frequency   266  88  51  30  20  13   2   6   4   5   3   1   2   1 
</code></pre>

<hr>

<p>Obviously, this data is far from normal. But there a lot of levels to use a dummy coded categorical variable, and I think an ordinal scale better represents the data. What do you think it the best way to include this data as an independent variable in my LME models? (n.b. I don't include both in the same model just one or the other)</p>

<p>The models do return results without convergence errors or other warnings when I include these variables but it doesn't feel like very good practice and I am unsure of what sort of transformation I could do to make this data better (e.g. log transformation leaves the data looking very odd and plots of the raw data make it look like a linear relationship is the most likely) </p>

<p>Here is an example of what I would like to improve:</p>

<p>(adj_w_63 - calf weight,
weeks_brd - weeks with disease (as described above),
rid - a normally distributed continuous variable,
milksolids_total - a normally distributed continuous variable)</p>

<pre><code>library(lme4)
model1&lt;-lmer(adj_w_63 ~ weeks_brd + rid + milksolids_total + (1|farm_ac),
 data=comp)
summary(model1)

Linear mixed model fit by REML ['lmerMod']
Formula: adj_w_63 ~ weeks_brd + rid + milksolids_total + (1 | farm_ac)
   Data: comp

REML criterion at convergence: 3247

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5180 -0.5525 -0.0458  0.5945  6.1674 

Random effects:
 Groups   Name        Variance Std.Dev.
 farm_ac  (Intercept) 30.10    5.487   
 Residual             83.37    9.131   
Number of obs: 443, groups:  farm_ac, 11

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      68.06279    3.30996  20.563
weeks_brd        -1.00200    0.42089  -2.381
rid               0.11010    0.04981   2.210
milksolids_total  0.19904    0.07679   2.592

Correlation of Fixed Effects:
            (Intr) wks_br rid   
weeks_brd   -0.174              
rid         -0.285  0.141       
mlkslds_ttl -0.795  0.038 -0.016
</code></pre>

<p>Thank you so much for your help.</p>
"
"0.12266979912335","0.127280377713181","230847","<p>So I have read many textbooks and so many R tutorials that I am going crazy here. How do you decide on which model to use? I really hope this comes with experience but with the amount of modern techniques coming out and evidence for and against transformations, etc., how is anyone supposed to actually create a model that produces the correct result?</p>

<p>All I want to know is if there is a significant difference between the number of points in a plot covered with wood between two treatments (Low and High elephant impact). I would also like to know if any of the effects are significant. Each site has 5 plots (1,2,3,4,5). The number of points covered with wood were counted in each plot in 2013 and then again in 2014 and 2015. Therefore I have repeated measures.</p>

<p>My response variable is <code>Number</code> = number of points covered with wood
My fixed effects or predictor variable are <code>Year</code> (2013,2014,2015) and <code>Site</code> (High and Low)
To account for the repeated measure, <code>Year</code> and <code>Site</code> are also my random effects. Or should this actually be <code>Plot</code> (1,2,3,4,5)?</p>

<p>The first option is to use a GLMM, as I have both random and fixed effects; because I have count data, I selected the Poisson family:</p>

<pre><code>model&lt;-glmer(Number~Year*Treatment+(1|Year:Treatment),data=data,family=poisson)
</code></pre>

<p>Firstly, can <code>Year</code> and <code>Treatment</code> act as both fixed and random effects in the same model? I haven't included plot as I'm assuming the repeated measure is actually YearL is that correct?
Secondly, if my data is not normally distributed, should I log-transform it and then run the GLMM?</p>

<p>Or should I rather leave it untransformed and use a linear mixed effects model (LME) instead?</p>

<pre><code>model1&lt;-lmer(Number~Year*Treatment+(1+Year|Treatment),data=data,REML=FALSE)
</code></pre>

<p>For the LME, should I stipulate a distribution? Or does it automatically use the Gaussian distribution (Normal distribution)?
Again, can <code>Year</code> and <code>Treatment</code> be both fixed and random effects?</p>

<p>Could this actually be non-linear?</p>
"
"0.128124426527695","0.121861683908065","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.202582505865207","0.196183517423333","231765","<p>Please bear with me and this potentially misinformed question.</p>

<p>Can I re-code the averaged model as a normal linear model to check r^2?</p>

<p>I know that with averaged models AIC is considered the main important value as per everyone important</p>

<p>From my understanding AIC is basically letting us know that our modelâ€™s line of best fit is in the right place, and tilted correctly (that we have the best line for the data based on the covariates included in the model), without telling us how close the model is to the points itâ€™s trying to fit.</p>

<p>Conversely R2 is telling us how close to the line the points actually fall (how well our model fits the data).  </p>

<p>Why donâ€™t we look at r2 for averaged models? 
Is it just because thereâ€™s no easy R code translation from averaged model object in MuMIn to normal linear model that can have r2 evaluated?</p>

<p>I think I have a work-around:   </p>

<p>My understanding of the MuMIn function is that the mod.avg() function averages models and returns an RVI (relative variable importance to the model) which gives the proportional influence each covariate should have in the optimized model and the summed AIC weights of the models going in to the averaged model give us how confident we are that the averaged model contains the best model.</p>

<p>However, being confident that the averaged model contains the best model doesnâ€™t necessarily mean that the averaged model actually fits the response variable. You can be confident that you have the best out of a bad lot of models and still not fit the data very well. Thus, Iâ€™m not entirely convinced that AIC tells us everything we need to know about the fit of an averaged model.</p>

<p>Getting an r2 from a regular linear model object is easy in the MuMIn package the r.squaredGLMM() and r.squaredLR() functions do this quite nicely.
However you canâ€™t calculate an r2 for an averaged model object because MuMIn doesnâ€™t know what to do with it.</p>

<p>For example: </p>

<pre><code>##### in R

#starting with a global model

Test&lt;-lme(y ~a + b + c + d + e, random=~1|f),method=""ML"")

summary(Test)


# dredge global model, get the top models, and average them

all&lt;-dredge(Test) 

all 

top&lt;-get.models(all,delta&lt;2)

top

modavg&lt;-model.avg(top)

summary(modavg)

#####
</code></pre>

<p>The optimal averaged model returned by the model averaging process has omitted covariate â€œeâ€, and assigned the following RVI weightings to the retained covariates:</p>

<pre><code>averaged model&lt;-lme(y ~a + b + c + d , random=~1|f),method=""ML"")


RVI a=1 b=0.81, c=0.32, d=0.12
</code></pre>

<p>But the model averaged object is not a linear model object, so I canâ€™t perform other model investigating functions on it (i.e. plotting it etc.), like I can on a normal linear model object.</p>

<p>As far as I can tell, an averaged model is essentially just a subset of a regular linear model with only the important covariates retained and then weighted by the model averaging process.</p>

<p>So can I re-code the covariates retained by the model averaging to incorporate the RVI weighting, and replicate the averaged model as a regular model object?</p>

<p>For example:</p>

<pre><code>##### in R

#recode the averaged model identified covariates weighted by the RVI determined by the averaged model


a1&lt;-(a*1)


b1&lt;-(b*0.81)


c1&lt;-(c*0.32)


d1&lt;-(d*0.12)


avg.model.recode&lt;-lme(y ~a1 + b1 + c1 + d1 , random=~1|f),method=""ML"")


# re-fit to lme4 for R2 examination


avg.model.lmer&lt;-lmer(y ~a1 + b1 + c1 + d1 +(1|f),data=dataset)

summary(avg.model.lmer)


r.squaredGLMM(avg.model.lmer)


r.squaredLR(avg.model.lmer)


sem.model.fits(avg.model.lmer, aicc = TRUE)
#####
</code></pre>

<p>Conversely, can I check the averaged model fit by plotting the re-coded averaged model fitted values  against the response variable to see how well my fitted values  predicts the response variable ?  </p>

<pre><code>##### In R

plot(avg.model.recode)


Fitted&lt;-fitted(avg.model.recode)


plot(y~Fitted) ## Plot model fitted points versus observed response variable 
values.


fits&lt;-lm(y~Fitted)


abline(fits,lty=2,lwd=2)


# add confidence intervals?


confin&lt;-confint(fits,full=TRUE) 


confin


abline(coef=confin[,1],lty=2)


abline(coef=confin[,2],lty=2)


##this summary also gives r2 values


summary(fits)

#####
</code></pre>

<p>What do you think? Iâ€™m not a statistician, and just use R without really understanding how the math works, so Iâ€™m sorry if this was a stupid question because the math is all wrong. </p>
"
"0.110959008218296","0.115129433468","231980","<p>I am trying to recreate a PROC GLIMMIX command in R using glmer.  Here is a link to the data (from SAS product support GLIMMIX documentation): <a href=""https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_glimmix_a0000001403.htm"" rel=""nofollow"">https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_glimmix_a0000001403.htm</a></p>

<p>The SAS code is</p>

<pre><code> proc glimmix data=multicenter;

      class center group;

      model sideeffect/n = group / solution;

      random intercept / subject=center;

   run;
</code></pre>

<p>The coefficients are:</p>

<pre><code>Intercept: -0.8071

Group A : -0.4896

Group B: 0
</code></pre>

<p>Here is the R command, after swapping 1 and 0 in the <code>sideeffect</code> column to align the defaults in R and SAS:</p>

<pre><code>mt &lt;- glmer(sideeffect/n ~ group + (1|center), data = test, family = binomial, 
    weights=n, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 10)
</code></pre>

<p>The coefficients are:</p>

<pre><code>Intercept: 1.3379

Group B: -0.4966
</code></pre>

<p>The different coefficients are not concerning on their face, since I transformed the data. However, when I compute the associated probabilities, I get from SAS:</p>

<pre><code>P(sideeffect | A) = 0.2147

P(sideeffect | B) = 0.3085
</code></pre>

<p>and from R:</p>

<pre><code>P(sideeffect | A) = 0.2078556

P(sideeffect | B) = 0.3018929
</code></pre>

<p>These estimates have a discrepancy of approximately 2%.  I know that R and SAS use slightly different approaches to the generalized linear models - is this enough to explain this discrepancy?  If not, what should I do to get R to conform to the SAS code?  I have seen an example online using different data where R code exactly replicated the SAS results: <a href=""https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004002.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004002.html</a></p>
"
"0.116961064294386","0.10922137064511","232031","<p>I am analyzing my data using a generalized linear mixed model in R. My design has three categorical variables:  </p>

<ul>
<li><code>language</code> (three levels: English vs French vs Japanese),a between subject effect, that is, three groups of participants</li>
<li><code>function</code> (two levels: radical vs component), a within-subject effect</li>
<li><code>position</code> (five levels: left, right, top, bottom, inside), a within-subject effect</li>
<li>and the dependent variable <code>response</code> is binomial (0 or 1) </li>
</ul>

<p>I model these three categorical variables as fixed effects while <code>subject</code>(the participants) and <code>item</code>(the experimental trail) are random effects. The R code looks like this:</p>

<pre><code>glmm &lt;- glmer(formula=response~proficiency*function*position+
                 (1|item)+(1|subject), 
              family=binomial, data=data1,
          control=glmerControl(optimizer=""bobyqa""))
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>I would like to see the interaction between these three IVs (<code>proficiency*function*position</code>). Is the R code suitable? How could I get the $p$-value of this interaction?</p></li>
<li><p>How do I interpret the results? Since the model sets English, component and bottom as the baseline levels by default, I cannot find the comparison of the baseline. So the question is, how could I do the comparison between different levels? (For example, <code>English.component.bottom</code> vs <code>English.radical.bottom</code>, since English, component and bottom were set as the baseline, I could not find them in the output of the model.) </p></li>
<li><p>After looking though the websites, somebody suggested that by <code>relevel</code> the baseline then can find the results of different levels, but I found that the estimate and the $p$-value change by releveling the baseline.</p></li>
<li><p>Somebody suggested using the <code>glht</code> function, but the output is general linear hypotheses rather than generalized linear mixed model. Does that influence the validity of the results?</p></li>
</ol>
"
"NaN","NaN","232252","<p>I am in the last stage of my paper review and it is very close to getting submitted. However, there is a comment I am highly struggling with:</p>

<p>The reviewer wants to know what the <strong>ICC and the s.e. of the ICC</strong> is.</p>

<p>The model I have is a linear mixed effect model looking like:</p>

<p>mixedmodel&lt;-lmer(y~x1+x2+x3+ (1 | countries), weights=weight,Alldata)</p>

<p>Before I had never heard about the ICC. When looking for documentation, I do find information on ICC, but not on how to calculate the s.e. of the ICC. </p>

<p>Can anyone please help me with this? I am using R.</p>
"
"0.0978566471559948","0.101534616513362","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.144249206366384","0.140866685168871","233071","<p>After many months of lurking, this is my first post (fingers crossed).</p>

<p>I have a 'large data' set (~ 5m observations of runs at UK parkrun events) to which I am trying to fit a multilevel model (because of repeated measures from specific runners over time) using R. Here is a quick summary of the variables I am working with:</p>

<p>Outcome: 5k run <code>time</code> (integer in minutes, e.g. 21.5)</p>

<p>Predictors:
<code>gender</code> (two level factor);
<code>age</code> (integer);
previous <code>runs</code> (integer);
<code>friends</code> present during run (integer);
athlete number or <code>athnumber</code> (factor; level 2 random effect in the multilevel model)</p>

<p>Using <code>lmer</code>, I fit a model as follows:</p>

<pre><code>model &lt;- lmer(time ~ gender + age + runs + friends + (1 | athnumber), data = mydata)
</code></pre>

<p>I am concerned with the fitted v. residuals plot that this model produces. With <code>plot(fitted(model), resid(model))</code> I get the plot on the left below.</p>

<p>Using the same model, log transforming (and scaling) the <code>time</code> variable (which has a positive skew) does not seem to help, in fact it makes it worse (middle plot).</p>

<p>And I get a similar output (the plot on the right) again from the same model if I scale and log transform <code>time</code>, <code>age</code> and the positively skewed predictors (i.e., <code>runs</code> and <code>friends</code>):</p>

<p><a href=""http://i.stack.imgur.com/Dw4gY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Dw4gY.jpg"" alt=""enter image description here""></a></p>

<p>For these plots: x = fitted, y = residuals (sorry they're small but I only get two images since I have less than 10 reputation points!)</p>

<p>I have also messed around with trimming the <code>time</code> variable and trimming the age variable, but, if anything, this just makes the fitted v. residual plots look more like diagonal rectangles (i.e., they become less fuzzy, but are still the same shape). </p>

<p>However, if I run a non-hierarchical linear model (with scaled and log transformed versions of <code>time</code>, <code>runs</code>, <code>friends</code>, and <code>age</code>) on a subset of the data obtained from randomly sampling one run from each athlete (thus removing the necessity of including <code>athnumber</code> in a multilevel model), I get a much more reasonable looking fitted v. residual plot. I get the following from <code>model &lt;- lm(time ~ gender + age + runs + friends, mydata)</code> and <code>plot(fitted(model), resid(model))</code>:</p>

<p><a href=""http://i.stack.imgur.com/knNSw.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/knNSw.jpg"" alt=""enter image description here""></a></p>

<p>So, the <code>lm()</code> seems okay in this regard. But the way I understand it, the multilevel model is predicting <code>time</code> scores that are too fast (positive residuals) for fast runs and <code>time</code> scores that are too slow (negative residuals) for slow runs, is this correct? </p>

<p>So, my questions are: (1) why am I getting this strange-looking fitted v. residual plot for the multilevel model? And (2) surely this means the model is biased and makes any inference invalid, correct? And (3) how can I fix this problem?</p>

<p>Thanks very much for the help and understanding in advance - I'm self-taught with this stuff so please forgive any mistakes. Cheers!</p>
"
"0.0369863360727655","0.0383764778226668","233800","<p>I need to fit a linear mixed model but my dependent variable is right-skewed with some big outliers. Thus I used the <code>rlmer</code> function of the <code>robustlmm</code> package. it works quite nicely, however what I am missing now is confidence intervals for the fixed effects. Is anyone working with this package and has some tips for me? Otherwise I would also be interested in other packages to fit robust linear mixed models.</p>
"
"0.0523065780965941","0.0542725354129257","233995","<p>I am analyzing EEG spectral data using a linear mixed effect model fitted with the lme4 and lmerTest packages in R. </p>

<p>I have two groups with 20 and 21 subjects respectively. The dependent variable is Gamma activity and the predictors in the model are Condition (4 level), Group (2 level), Hemisphere (2 level), Caudality (2 level) and Time (3 level). All the predictors are <em>within-subjects</em> factors, with the exception of Group, which is a <em>between-subject</em> factor.</p>

<p>When I try to do post-hoc contrasts on significant effects using the <strong>lsmeans</strong> package, I found that for between-subject factors the degrees of freedom are quite small (almost equal to the number of subjects) compared to the degree of freedom for the within-subjects factor.</p>

<p>I am wondering why there is such a difference, since the calculation of the t statistics depends on it.</p>

<p>Many thanks in advance.</p>
"
"0.116961064294386","0.121357078494567","234028","<p>I am trying to use a binomial generalized linear mixed model to analyze binary data of an experiment. Just few details on the experiment that could be useful:  </p>

<ul>
<li>The dependent variable is <code>Score</code>: 0 (incorrect); 1 (correct)</li>
<li>The two predictors are <code>CongRec</code>: -1 (Congruent); +1 (Incongruent) and
<code>TempsExp</code>: 6 levels (33ms,50ms,67ms,..117ms), used as a categorical
predictor.</li>
</ul>

<p>For each combination of the levels of the two predictors, we have 40 observations for 29 participants.</p>

<p><strong>The issue</strong>: When I try to fit a model with only the main effects of the predictors (+intercept) and by-subject random slope for both <code>Congruency</code> and <code>ExposureTime</code>, the model fails to converge.</p>

<p>In such cases, I have been told to remove the random effect with the smallest variance. However depending on the way I define the predictor <code>Congruence</code>, i.e., as a categorical predictor with 2 levels (<code>Congruence</code>) or as continuous variable (<code>CongRec</code>), the random effect with the smallest variance differs.</p>

<p>Here is the R command for the (second) model:  </p>

<pre><code>Model4_Categbis = glmer(formula = Score~1+CongRec+TempsExp+(1+CongRec+TempsExp|Sujet), 
                        data=donneestestCN, family=""binomial"", REML=F)
</code></pre>

<p>Here are the random effects for both cases:  </p>

<ol>
<li><p><code>Congruency</code> as categorical predictor  </p>

<p><a href=""http://i.stack.imgur.com/cRLas.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cRLas.png"" alt=""enter image description here""></a></p></li>
<li><p><code>Congruency</code> as continuous predictor  </p>

<p><a href=""http://i.stack.imgur.com/rWRZJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rWRZJ.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>So basically, in the first case, I should remove the random slope for <code>CongRec</code> while, in the second case, I should remove the random intercept by subject. Normally, I think the way I define the predictor <code>Congruence</code> should not have any influence on model main characteristics but, here, it does because of the non-convergence.</p>

<p>So, which random effects should I remove in your opinion and, that being done, which type of variable should I use for Congruency?</p>
"
"0.0905976508333704","0.0940027887907685","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.169668052275276","0.168043024522122","235168","<p>I'm studying Design and Analysis of Experiments, 8th Edition. Douglas C. Montgomery is the author. I'm trying to replicate the first example he gives in Chapter 13, Experiments with Random Factors.</p>

<p>In this example, there are measurements in a critical dimension on a part. 20 parts are randomly selected and measured by 3 operators, also selected at random. I want to fit two models to this data. The first one I call full model and it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + (\tau\beta)_{ij} + \varepsilon_{ijk}$$</p>

<p>The other model I call reduced model ant it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}$$</p>

<p>Both $\tau_i, i=1, \cdots, 20$ and $\beta_j, j=1, 2, 3$ are random effects. The code I'm using to analyze my problem is below:</p>

<pre><code>gauge &lt;- structure(list(part = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 
18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 
19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 
20L), operator = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), replication = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L), measurement = c(21L, 24L, 20L, 27L, 
19L, 23L, 22L, 19L, 24L, 25L, 21L, 18L, 23L, 24L, 29L, 26L, 20L, 
19L, 25L, 19L, 20L, 23L, 21L, 27L, 18L, 21L, 21L, 17L, 23L, 23L, 
20L, 19L, 25L, 24L, 30L, 26L, 20L, 21L, 26L, 19L, 20L, 24L, 19L, 
28L, 19L, 24L, 22L, 18L, 25L, 26L, 20L, 17L, 25L, 23L, 30L, 25L, 
19L, 19L, 25L, 18L, 20L, 24L, 21L, 26L, 18L, 21L, 24L, 20L, 23L, 
25L, 20L, 19L, 25L, 25L, 28L, 26L, 20L, 19L, 24L, 17L, 19L, 23L, 
20L, 27L, 18L, 23L, 22L, 19L, 24L, 24L, 21L, 18L, 25L, 24L, 31L, 
25L, 20L, 21L, 25L, 19L, 21L, 24L, 22L, 28L, 21L, 22L, 20L, 18L, 
24L, 25L, 20L, 19L, 25L, 25L, 30L, 27L, 20L, 23L, 25L, 17L)), .Names = c(""part"", 
""operator"", ""replication"", ""measurement""), class = ""data.frame"", row.names = c(NA, 
-120L))

###############
# full model
fit.full &lt;- lmer(measurement ~ (1|part) + (1|operator) + (1|part:operator), data=montgomery)
summary(fit.full)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator) + (1 | part:operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups        Name        Variance Std.Dev.
 part:operator (Intercept)  0.00000 0.0000  
 part          (Intercept) 10.25127 3.2018  
 operator      (Intercept)  0.01063 0.1031  
 Residual                   0.88316 0.9398  
Number of obs: 120, groups:  part:operator, 60; part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95

###############
# reduced model
fit.reduced &lt;- lmer(measurement ~ (1|part) + (1|operator), data=montgomery)
summary(fit.reduced)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups   Name        Variance Std.Dev.
 part     (Intercept) 10.25127 3.2018  
 operator (Intercept)  0.01063 0.1031  
 Residual              0.88316 0.9398  
Number of obs: 120, groups:  part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95    
</code></pre>

<p>However, I'm getting different estimates from the ones in the book. Montgomery used Minitab to fit its model and here are his results for the full model:</p>

<p><a href=""http://i.stack.imgur.com/1aeGL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1aeGL.png"" alt=""Anova Table for the Full Model""></a></p>

<p>They are different from mine. Notice how his <code>part*operator</code> has a negative estimation, while mine is zero. However, his estimates for the reduced model are the same as mine:</p>

<p><a href=""http://i.stack.imgur.com/SaGVu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SaGVu.png"" alt=""Anova Table for the Reduced Model""></a></p>

<p>So, my question about his problem are:</p>

<ol>
<li><p>Why our estimates differ for the full model? I understand that I can't have a negative variance like the one he got, but why does Minitab doesn't set it to zero? </p></li>
<li><p>Using R, where (or how) can I get an ANOVA table like the one Minitab presents? I couldn't test my hypothesis in this problem because I can't find the p-values associated with the parameters I'm testing.</p></li>
</ol>
"
