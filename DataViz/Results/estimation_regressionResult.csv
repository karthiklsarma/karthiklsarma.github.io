"V1","V2","V3","V4"
"0.107211253483779","0.104257207028537"," 10697","<p>I'm studying R package dlm. So far it seems very powerful and flexible package, with nice programming interfaces and good documentation.</p>

<p>I've been able to successfully use dlmMLE and dlmModARMA to estimate the parameters of AR(1) process:</p>

<pre><code>u &lt;- arima.sim(list(ar = 0.3), 100)
fit &lt;- dlmMLE(u, parm = c(0.5, sd(u)),
              build = function(x)
                dlmModARMA(ar = x[1], sigma2 = x[2]^2))
fit$par
</code></pre>

<p>Now I'm trying to use similar code to estimate the parameters of simple linear regression model:</p>

<pre><code>r &lt;- rnorm(100)
u &lt;- -1*r + 0.5*rnorm(100)
fit &lt;- dlmMLE(u, parm = c(0, 1),
              build = function(x)
                dlmModReg(x[1]*r, FALSE, dV = x[2]^2))
fit$par
</code></pre>

<p>I expect fit$par to be close to c(-1, 0.5), but I keep getting something like</p>

<pre><code>[1] -0.0002118851  0.4884367070
</code></pre>

<p>The coefficient -1 is not estimated correctly. However, the strange thing is that the variance of the noise is returned correctly.</p>

<p>I understand that max-likelihood estimation might fail given bad initial values, but I observed that the likelihood function returned by dlmLL is very flat in the first coordinate.</p>

<p>So I wonder: can such model be estimated at all using dlm? I believe the model is ""non-singular"", however I'm not sure how the likelihood function is calculated inside the dlm.</p>

<p>Any hint greatly appreciated.</p>
"
"NaN","NaN"," 11703","<p>Assume the following easy example of a glm regression with an offset:</p>

<pre><code>numberofdrugs&lt;-rpois(84, 10)
healthvalue&lt;-rpois(84,75)
age&lt;-rnorm(84,50,5)
test&lt;-glm(healthvalue~age, family=poisson, offset=log(numberofdrugs))
summary(test)
fitted(test) #how to get one of these values manually?
</code></pre>

<ul>
<li>How can I compute the fitted values manually? </li>
<li>Also, why is there no estimation of log(numberofdrugs)? 
<ul>
<li>In the book <a href=""http://rads.stackoverflow.com/amzn/click/0412317605"" rel=""nofollow""><em>Generalized Linear Models</em></a> on page 205-207 there is an example where the offset is estimated. It was done to see if the coefficient is close to one. It's 0.903 (see page 207 if you've this classic book) and from this follows, that there is nearly a constant rate in the number of damage incident!</li>
</ul></li>
</ul>

<p>Previous related questions asked: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression"">When to use an offset?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/11595/whether-to-use-an-offset-in-a-poisson-regression-when-predicting-total-career-goa"">Whether to use an offset when predicting hockey scores?</a></li>
</ul>
"
"0.0618984460590173","0.0601929265428846"," 13069","<p>I am very interested about the potential of statistical analysis for simulation/forecasting/function estimation, etc. </p>

<p>However, I don't know much about it and my mathematical knowledge is still quite limited -- I am a junior undergraduate student in software engineering. </p>

<p>I am looking for a book that would get me started on certain things which I keep reading about: linear regression and other kinds of regression, bayesian methods, monte carlo methods, machine learning, etc.
I also want to get started with R so if there was a book that combined both, that would be awesome. </p>

<p>Preferably, I would like the book to explain things conceptually and not in too much technical details -- I would like statistics to be very intuitive to me, because I understand there are very many risky pitfalls in statistics. </p>

<p>I am off course willing to read more books to improve my understanding of topics which I deem valuable.</p>
"
"0.186630836985285","0.199637352376173"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.0618984460590173","0.0601929265428846"," 17811","<p><strong>Context.</strong> I'd like to fit a regression line to study to relation between some response variable $y$ and some continuous covariate $x$. Because of the presence of bad leverage points, I have opted for an MM-estimator instead of the usual LS-estimator. </p>

<p><strong>Methodology.</strong> Basically, MM-estimation is M-estimation initialised by an S-estimator. Hence, two loss functions have to be picked. I have chosen the widely used Tukey Biweight's loss function</p>

<p>$\rho ( u ) = \left\{
\begin{array}{ll}
1 - \left[ 1 - \left( \tfrac{u}{k} \right)^{2} \right]^{3} &amp; \textrm{if } | u | \leq k \\
1 &amp; \textrm{if } | u | &gt; k,
\end{array}
\right.$</p>

<p>with $k = 1.548$ at the preliminary S-estimator (which gives a breakdown point equal to $50 \%$), and with $k = 2.697$ at the M-estimation step (to guarantee $70\%$ Gaussian efficiency).</p>

<p>I'd like to use R to fit my robust regression line. </p>

<p><strong>Question.</strong> </p>

<pre><code>library(MASS)
rlm(y~x, 
    method=""MM"",
    k0=1.548, c=2.697,
    maxit=50)
</code></pre>

<ul>
<li>Is my code consistent with the previous paragraph?  </li>
<li>Would you use other optional arguments?</li>
</ul>

<p><strong>EDIT.</strong> Following my discussion with @Jason Morgan, I realise that my previous code is wrong. (@Jason Morgan: Thank you very much for this!) However, I am still not convinced by his proposal. Instead, here is what I propose now:</p>

<pre><code>library(robustbase)
lmrob(y~x, 
      tuning.chi=1.548, tuning.psi=2.697)
</code></pre>

<p>I think it sticks to the methodology now. <strong>Do you agree?</strong></p>

<p>Thanks!</p>
"
"0.206010481049842","0.217028683070608"," 20417","<p>I am working on a Monte Carlo study of bootstrapping in an AR(1) model for a homework assignment (I'm using Matlab). The goal is to say something about the empirical rejection probabilities of the bootstrap in this specific context. However, I've been running into some computational problems. </p>

<p>Some context first. Suppose one has a regression $y_t = \gamma + \rho y_{t-1} + \varepsilon_t$, and wants to test the two-tailed hypothesis $\rho = \rho_0$ by the bootstrap. To do this, one first estimates the regression by, say, OLS, and then uses the obtained residuals $\hat \varepsilon_t$ and coefficients $\hat \gamma, \; \hat \rho$ for constructing the bootstrap samples. However, in this case, the bootstrap sample has to be constructed recursively. What is even worse for the computations, I have to repeat these calculations many times using different simulated data sets <em>and</em> different values of $\rho$. (The reason is that this allows one to get some sense of the empirical level of the test at different values of $\rho$.) </p>

<p>So far I have been doing it in the naive way of just estimating many regressions, generating the bootstrap samples, and so on. I was able to eliminate quite a few loops by using the <code>filter</code> function when generating bootstrap samples but still computational times are quite huge. I compared it with the built-in bootstrap in <a href=""http://gretl.sourceforge.net/"" rel=""nofollow"">gretl</a>, and it is many times slower. (I'm aware that gretl is written in C, but I sense that there is more to this than that.)</p>

<p>Hence, my question would be: what general advice could you give to implement bootstrapping efficiently in a matrix based language? Are there some smart ways to avoid estimation of many OLS regressions and generation of the bootstrap samples? Although I'm using Matlab for the computations, but if you have some general advice on say, doing this with R, all the help would be very much appreciated. Also, since this is a homework assignment I have to do, using existing libraries is not really an option. :)</p>
"
"0.0875376219064817","0.0851256530758749"," 20725","<p>I have a model that looks like </p>

<pre><code>lm(y ~ lag(x, -1) + lag(z, -1))
</code></pre>

<p>So basically, this is a time series regression with exogenous variables, and I want to carry out a rolling analysis of sample forecasts, meaning that:
I first used a subsample (e.g., 1990-1995) for estimation, then I performed a one step ahead forecast, then I added one observation and made another one step ahead forecast, and so on.</p>

<p>I have tried to work with <code>rollapply</code>, defining the model as <code>arima(0,0,0)</code> with <code>xreg=lags</code> of the other variables, but that doesn't work. </p>

<p>Your help would be much appreciated!</p>
"
"0.0618984460590173","0.0601929265428846"," 24572","<p>In my previous <a href=""http://stats.stackexchange.com/questions/24380/how-to-get-ellipse-region-from-bivariate-normal-distributed-data"">question</a> I needed to help with ellipse region extraction and determine if point lies in that region or not.
I ended up with this code:</p>

<pre><code>library(ellipse)
library(mvtnorm)
require(spatstat)

netflow &lt;- read.csv(file=""data.csv"",head=FALSE,sep="" "")
#add headers
names(netflow)&lt;-c('timestamps','flows','flows_tcp','flows_udp','flows_icmp','flows_other','packe ts','packets_tcp','packets_udp','packets_icmp','packets_other','octets','octets_tcp','octets_udp','octets_icmp','octets_other')
attach(netflow)

#load library
library(sfsmisc)
#plot
plot(packets,flows,type='p',xlim=c(0,500000),ylim=c(0,50000),main=""Dependence number of flows on number of packets"",xlab=""packets"",ylab=""flows"",pch = 16, cex = .3,col=""#0000ff22"",xaxt=""n"")
#Complete the x axis
eaxis(1, padj=-0.5, cex.axis=0.8)

pktsFlows=subset(na.omit(netflow),select=c(packets,flows))
head(pktsFlows)
#plot(pktsFlows,pch = 16, cex = .3,col=""#0000ff22"")

cPktsFlows &lt;- apply(pktsFlows, 2, mean)
elpPktsFlows=ellipse::ellipse(var(pktsFlows),centre=cPktsFlows,level=0.8)

png(file=""graph.png"")
plot(elpPktsFlows,type='l',xlim=c(0,500000), ylim=c(0,50000))
points(pktsFlows,pch = 19, cex = 0.5,col=""#0000FF82"")
grid(ny=10,nx=10)
dev.off()

W &lt;- owin(poly=elpPktsFlows)
inside.owin(100000,18000,W)
</code></pre>

<p>This produces this <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png"" rel=""nofollow"">graph</a>.</p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png"" alt=""graph ellipse""></p>

<p>Here is the same data with the <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png"" rel=""nofollow"">regression line plotted</a></p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png"" alt=""Plot all with linear regression line"">.</p>

<p>Can you explain me, why the ellipse has this shape? I expected that main axe of ellipse will have the same direction with linear regression line, but it hasn't.</p>

<p>Btw. <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png"" rel=""nofollow"">kernel density estimation</a> also points to 100000 althought there are no points...</p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png"" alt=""kernel density estimation""></p>
"
"0.185695338177052","0.180578779628654"," 24857","<p>Much like with regression, handling binary dependent variables in SEM requires special considerations. In particular, some of these are noted on Dave Garson's <a href=""http://faculty.chass.ncsu.edu/garson/PA765/structur.htm"" rel=""nofollow"">Structural Equation Modeling</a> and include:</p>

<blockquote>
  <ol>
  <li><p>Polychoric correlation. LISREL/PRELIS uses polyserial, tetrachoric, and polychoric correlations to create the input correlation matrix,
  combined with ADF estimation (see below), for variables which cannot
  be assumed to have a bivariate normal distribution.</p>
  
  <ul>
  <li>Sample size issue. ADF [Asymptotically distribution-free] estimation in turn requires a very large sample size. Yuan and Bentler (1994)
  found satisfactory estimates only with a sample size of at least 2,000
  and preferably 5,000. Violating this requirement may introduce
  problems greater than treating ordinal data as interval and using ML
  estimation. This is also a reason cited for preferring the Bayesian
  estimation approach to ordinal data taken by Amos since Bayesian
  estimation can handle smaller samples than ML or ADF.</li>
  </ul></li>
  </ol>
</blockquote>

<p>I'm currently trying to use the package <a href=""http://cran.r-project.org/web/packages/sem/index.html"" rel=""nofollow"">sem</a> in R to test my model, and the author of the model suggests using polychoric correlations on <a href=""http://r.789695.n4.nabble.com/Link-functions-in-SEM-td859182.html"" rel=""nofollow"">R-help</a>. The problems are: </p>

<ol>
<li>I don't know what estimation method is being used with these correlations (i.e., ADF or ML). </li>
<li>My sample size is small (N = 173). </li>
<li>I'm not familiar with how to interpret polychoric associations (in the case that it is appropriate for me to use them). All the other variables in my model are continuous in nature. </li>
</ol>

<p>Any help and/or links would be greatly appreciated. I'm also considering using other software like OpenMX, but I'm still reading about how it handles binary data. Help with what other software I might want to use would also be appreciated.</p>
"
"0.140372481268719","0.159255514317652"," 25538","<p>I am looking into time series data compression at the moment.</p>

<p>The idea is to fit a curve on a time series of n points so that the maximum deviation of any of the points is not greater than a given threshold. In other words, none of the values that the curve takes at the points where the time series is defined should be ""further away"" than a certain threshold from the actual values.</p>

<p>Till now I have found out how to do nonlinear regression using the least squares estimation method in R (<code>nls</code> function) and other languages, but I haven't found any packages that implement nonlinear regression with the L-infinity norm.</p>

<p>I have found papers on <a href=""http://www.jstor.org/discover/10.2307/2006101?uid=3737864&amp;uid=2&amp;uid=4&amp;sid=21100693651721"">""Non-linear curve fitting in the $L_1$ and $L_{\infty}$ norms""</a>, by Shrager and Hill and <a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a080454.pdf"">""A linear  programming algorithm  for curve fitting in the $L_{\infty}$ norm""</a>, by Armstrong and Sklar.</p>

<p>I could try to implement this in R for instance, but I first looking to see if this hasn't already been done and that I could maybe reuse it.</p>

<p>I have found a solution that I don't believe to be ""very scientific"": I use nonlinear least squares regression to find the starting values of the parameters which I subsequently use as starting points in the R <code>optim</code> function that minimizes the maximum deviation of the curve from the actual points.</p>

<p>The idea is to be able to find out if this type of curve-fitting is possible on a given time series sequence and to determine the parameters that allow it.</p>
"
"0.0875376219064817","0.0851256530758749"," 26326","<p>I am running a regression with several independent variables with 32 observations (from 1975 to 2006 and they are yearly data). The issue here is that there does not exist any observation for one of the variables prior to 1980. Consequently, that variable has 5 missing observations (from 1975 to 1979).
Is there any method in R to provide an estimation for these missing values? By the way, the explanatory variable here is ""total labor force"" and it has a very pronounced trend. Therefore, I know very well that it is statistically very possible to estimate the past values.</p>
"
"0.232119172721315","0.240771706171538"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"NaN","NaN"," 28882","<p>Is there any function for $M$-estimation in multivariate linear regression model in <code>R</code>. I can estimate the $\beta$'s in my model by using the <code>rlm()</code> by rewriting the $y$-variables into one column but, I would like to use one function to get the $\beta$'s. </p>
"
"0.321633760451338","0.312771621085612"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"NaN","NaN"," 30243","<p>I've recently embarked on fitting regression mixed models in the Bayesian framework, using a MCMC algorithm (function MCMCglmm in R actually).</p>

<p>I believe I have understood how to diagnose convergence of the estimation process (trace, geweke plot, autocorrelation, posterior distribution...).</p>

<p>One of the thing that strikes me in the Bayesian framework is that much effort seems to devoted to do those diagnostics, whereas very little appears to be done in terms of checking the residuals of the fitted model. For instance in MCMCglmm the residual.mcmc() function does exist but is actually not yet implemented (ie.returns: ""residuals not yet implemented for MCMCglmm objects""; same story for predict.mcmc()). It seems to be lacking from other packages too, and more generally is little discussed in the literature I've found (apart from DIC which is quite heavily discussed too).</p>

<p>Could anyone point me to some useful references, and ideally R code I could play with or modify? </p>

<p>Many thanks.</p>
"
"0.0875376219064817","0.0851256530758749"," 37973","<p>I am fitting a simple linear regression model with 4 predictors:</p>

<p><code>lm(Outcome ~ Predictor1 + Predictor2 + Predictor3 + Predictor4, data=dat.s)</code></p>

<p>I'm finding that the model predictions are consistently off as shown in this graph:
<img src=""http://i.stack.imgur.com/CNLJz.png"" alt=""scatterplot of predictions and residuals""></p>

<p>The model clearly overestimates the low values and underestimates the high values, but the miss-estimation is very linear -- it seems like the model should be able to just adjust the slope and fit the data better. Why is that not happening? In case it helps, here are scatterplots of the the Outcome against each of the four Predictors:
<img src=""http://i.stack.imgur.com/uc55e.png"" alt=""enter image description here""></p>

<p>Using the <code>car</code> package <code>outlierTest</code> function did not identify any outliers.</p>
"
"0.232119172721315","0.240771706171538"," 40453","<p>Perpendicular offset least square fitting has a lot of advantages compared to the native least square fitting scheme. The following figure illustrates the difference between there, and for a more detailed comparison of these two methods, we refer to <a href=""http://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html"" rel=""nofollow"">here</a>. </p>

<p><img src=""http://i.stack.imgur.com/Aue8i.gif"" alt=""enter image description here""></p>

<p>Perpendicular offset least square fitting, however, is not robust to outliers( points that are not supposed to be used for model estimation). Therefore, I am now considering to use a weighted perpendicular offset least square regression method. The method has two steps:</p>

<ol>
<li>Calculate the weighting factor for each points that are going to be used for line estimation;</li>
<li>Perform perpendicular offset in a weighted least square regression scheme. </li>
</ol>

<p>For the time being, my biggest problem comes from step 2. Suppose the weighting factors are given, how can I get the formula to estimate the parameters of the line? Many thanks!</p>

<p><strong>EDIT:</strong></p>

<p>Based on the kind suggestion of @MvG I have implemented the algorithm in MATLAB:</p>

<pre><code>function  line =  estimate_line_ver_weighted(pt_x, pt_y,w);
% pt_x  x coordinate
% pt_y  y coordinate
% w     weighting factor


pt_x = pt_x(:);
pt_y = pt_y(:);
w    = w(:);


% step 1: calculate n
n = sum(w(:));

% step 2: calculate weighted coordinates 
y_square = pt_y(:).*pt_y(:);
x_square = pt_x(:).*pt_x(:);
x_square_weighted = x_square.*w;  
y_square_weighted = y_square.*w;  
x_weighted        = pt_x.*w;
y_weighted        = pt_y.*w;

% step 3: calculate the formula
B_upleft = sum(y_square_weighted)-sum(y_weighted).^2/n;
B_upright = sum(x_square_weighted)-sum(x_weighted).^2/n;
B_down = sum(x_weighted(:))*sum(y_weighted(:))/n-sum(x_weighted.*pt_y);
B = 0.5*(B_upleft-B_upright)/B_down;

% step 4: calculate b
if B&lt;0
    b       = -B+sqrt(B.^2+1);
else
    b       = -B-sqrt(B.^2+1);
end

% Step 5: calculate a
a = (sum(y_weighted)-b*sum(x_weighted))/n;

% Step 6: the model is y = a + bx, and now we transform the model to 
% a*x + b*y + c = 0;
c_ = a;
a_ = b;
b_ = -1;

line = [a_ b_ c_];
</code></pre>

<p>The result is as good as we can expect, which is illustrated in the following script:</p>

<pre><code>%% Procedure 1: given the data
pt_x = [   692   692   693   692   693   693   750];
pt_y = [ 919         971        1022        1074        1126        1230        1289];

% Procedure 2: draw the point 
 close all; figure; plot(pt_x,pt_y,'b*');

% Procedure 3: estimate the line based on the weighted vertical offset
% least square method.
 weighting = ones(length(pt_x),1);
 weighting(end) = 0.01;  % we give the last point a low weighting because obvously it is an outlier
 myline =    estimate_line_ver_weighted(pt_x,pt_y,weighting); 
 a = myline(1); b = myline(2); c= myline(3);

 % Procedure 4: draw the line
 x_range = [min(pt_x):0.1:max(pt_x)];
 y_range = [min(pt_y):0.1:max(pt_y)];
 if length(x_range)&gt;length(y_range)
        x_range_corrspond = -(a*x_range+c)/b;
        hold on; plot(x_range,x_range_corrspond,'r');
 else
        y_range_correspond = -(b*y_range+c)/a;
        hold on; plot(y_range_correspond,y_range,'r');
 end
</code></pre>

<p>The following figure corresponds to the above script:
<img src=""http://i.stack.imgur.com/IIz2k.png"" alt=""enter image description here"">.</p>
"
"NaN","NaN"," 44584","<p>I have been playing around with a seemingly unrelated regression (SUR) estimation. However, for dynamic SUR models it is known that -- analogous to the ARIMA case -- an OLS/GLS estimate is biased. For example <a href=""http://www.sciencedirect.com/science/article/pii/030440769401670U"" rel=""nofollow"">this article</a> provides a correction. So here's my question: Is there a <em>R</em> package or some other implementation that provides this correction? Thanks in advance!</p>
"
"0.163767894813506","0.159255514317652"," 45882","<p>I have got a question regarding ordered choice regressions in <strong>R</strong>. </p>

<p>I have several demographic variables with which I want to explain the ordered choice of individuals within a survey in an <strong>ordered choice</strong> (<strong>probit</strong> or <strong>logit</strong>, this is not important) framework. Standard ordered choice estimations of course just give me aggregate parameter estimates. For my task it would however be useful to estimate or extract ""hypothetical"" individual-level parameter estimates (betas) for a certain independent variable and each individual in the survey. </p>

<p>I have experimented with hierarchical Bayes algorithms provided by the <strong>bayesm</strong> and <strong>ChoiceModelR</strong>. Correct me if I am wrong but I think these techniques also demand that individuals appear several times within a survey and are confronted with different choice situations, so that one can estimate the influence of certain attributes on the individuals choices.</p>

<p>My data however don't have any panel structure. I was also experimenting with Bayesian inference in example by the <strong>MCMCoprobit</strong> function in the <strong>MCMCpack</strong> package, but this function just simulates betas. I can't however, as far as I know, attribute them to certain individuals in the survey, which would be good. I would be very glad if somebody could give me a hint, sometimes already a catchword is helpful to google the correct solution!</p>
"
"0.215059722360368","0.209134071928531"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.163767894813506","0.159255514317652"," 48922","<p>I am trying to estimate a selection model of the form:</p>

<p>$Z_i = 1[\alpha_0 + \alpha_1X_{1,i} + \alpha_2X_{2,i} + \delta_i$ > 0]</p>

<p>$Y_i = \beta_0 + \beta_1X_{1,i} + Z_i + \epsilon_i$</p>

<p>where $1[]$ denotes the indicator function.</p>

<p>The purpose of the model is to calculate the indirect effect of $X_1$ on $Y$ through $Z$, as well as the the direct effect.</p>

<p>My first question is how to go about estimating this type of model, and how this estimation can be achieved in R. As far as I see it I have a few possible approaches:</p>

<p>(1) Use a standard Heckman selection model, using OLS for both the reduced form and structural equations, using ivreg() in R. This will obviously ignore the constraint that $Z$ is bounded between 0 and 1.</p>

<p>(2) Estimate the first stage with a probit model (i.e. $\delta_i \sim N(0,1)$), and the second stage using standard OLS. I understand that I could do this via manual 2SLS, but as far as I am aware the standard errors will be incorrect? Am I right in that this model is feasible, and if so, can you direct me to a method of achieving this in R?</p>

<p>(3) Build a switching regression model (tobit-5) using the selection() function from sampleSelection package in R. I believe this model will estimate two equations for  $Y$, one for where $Z_i=0$ and one where $Z_i=1$, and with a unique intercept and coefficients for each of the regressors in the outcome equations.</p>

<p>The question then is how to get an estimate of the indirect effect of $X_1$ for each of these methods.</p>

<ul>
<li><p>If I use (1) or (2) then I imagine it might be possible to calculate the average marginal effect of $Z$ on $Y$, and the average marginal effect of $X_1$ on $Z$, then approximate the indirect effect by multiplying the two values?</p></li>
<li><p>If (3) then could I take the fitted value under the estimated model for $Y$ where $Z=0$, and compare the mean to the mean of the fitted values under the estimated model for $Y$ where $Z=1$? This would then give me an estimate of the marginal effect of $Z$? Then use the same method as above and multiple this effect by the marginal effect of $X_1$ on $Z$?</p></li>
</ul>

<p>Many thanks in advance!</p>
"
"0.0875376219064817","0.0851256530758749"," 51006","<p><strong>Context</strong>: Hierarchical regression with some missing data.</p>

<p><strong>Question</strong>: How do I use full information maximum likelihood (FIML) estimation to address missing data in R?  Is there a package you would recommend, and what are typical steps?  Online resources and examples would be very helpful too.</p>

<p><strong>P.S.</strong>: I'm a social scientist who recently started using R.  Multiple imputation is an option, but I really like how elegantly programs like Mplus handles missing data using FIML.  Unfortunately Mplus doesn't seem to compare models in the context of hierarchical regression at the moment (please let me know if you know a way to do that!).  I wondered whether there is anything similar in R?  Many thanks!</p>
"
"NaN","NaN"," 54824","<p>Curious whether anyone knows a package, or has written an implementation themselves, for conducting instrumental variables regressions using LIML in R. All of the R packages I have seen for IV regressions seem to resort to 2SLS/GMM as opposed to LIML, which may have more desirable finite sample qualities compared to 2SLS <a href=""http://press.princeton.edu/titles/6946.html"" rel=""nofollow"">(Hayashi 2000)</a></p>

<p>For additional context, stata's ivregress command includes options to use LIML estimation, and hoping someone has already implemented something similar in R so I don't have to write it myself.</p>
"
"0.283947800642868","0.288675134594813"," 56237","<p>I'm a beginner in R and Im wondering how to interprete my results.....
My question is about the results that I got after I did a regression on the Translog production function for panel data:
$ log(y)=log(A) + \alpha_{K} log(K) + \alpha_{L} log(L) + \beta_{KL} log(K)log(L) + \beta_{L^2} log^2(L) + \beta_{K^2} log^2(K)$</p>

<p>L stands for labour and K for Kapital.</p>

<p>The results I got for the Within, Random and first difference a the following:
Within:</p>

<pre><code>  #Within
    Coefficients :
  Estimate  Std. Error  t-value Pr(&gt;|t|)    
     K   1.0902e-05  1.0654e-06  10.2326   &lt;2e-16 ***
     L  -2.4009e-06  1.5086e-07 -15.9150   &lt;2e-16 ***
     LK  1.9788e-03  3.6069e-03   0.5486   0.5833    
     LL  3.0511e-02  1.3141e-03  23.2173   &lt;2e-16 ***
     KK  5.0333e-02  2.6650e-03  18.8868   &lt;2e-16 ***
     ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

    Total Sum of Squares:    6886.3
        Residual Sum of Squares: 1983.9
  R-Squared      :  0.71191 
  Adj. R-Squared :  0.69692 
    F-statistic: 10729.1 on 5 and 21709 DF, p-value: &lt; 2.22e-16


&gt; #regression random translog
&gt; tl.random&lt;-plm(Y ~ K + L + LK + LL + KK, data=panel, model=""random"")
 &gt; summary(tl.random)
 Oneway (individual) effect Random Effect Model 
(Swamy-Aroras transformation)

 Call:
 plm(formula = Y ~ K + L + LK + LL + KK, data = panel, model = ""random"")

  Balanced Panel: n=462, T=48, N=22176

  Effects:
               var std.dev share
  idiosyncratic 0.09139 0.30230 0.397
   individual    0.13856 0.37224 0.603
  theta:  0.8836  

  Residuals :
Min.  1st Qu.   Median  3rd Qu.     Max. 
 -3.16000 -0.14200  0.00724  0.15400  4.89000 

   Coefficients :
                 Estimate  Std. Error  t-value Pr(&gt;|t|)    
   (Intercept)  1.6266e+00  3.9030e-02  41.6763   &lt;2e-16 ***
    K            9.0932e-06  1.0552e-06   8.6178   &lt;2e-16 ***
    L           -2.5192e-06  1.5023e-07 -16.7684   &lt;2e-16 ***
   LK           2.7566e-03  3.6102e-03   0.7636   0.4451    
   LL           2.9491e-02  1.3138e-03  22.4474   &lt;2e-16 ***
   KK           4.8817e-02  2.6659e-03  18.3117   &lt;2e-16 ***
   ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

  Total Sum of Squares:    7183.6
  Residual Sum of Squares: 2070.2
  R-Squared      :  0.71181 
  Adj. R-Squared :  0.71162 
  F-statistic: 10951.9 on 5 and 22170 DF, p-value: &lt; 2.22e-16

  &gt; #regression first difference translog
   &gt; tl.fd&lt;-plm(Y ~ K + L + LK + LL + KK-1, data=panel, model=""fd"")
   &gt; summary(tl.fd)
    Oneway (individual) effect First-Difference Model


       #First difference regression
     Call:
      plm(formula = Y ~ K + L + LK + LL + KK - 1, data = panel, model = ""fd"")

      Balanced Panel: n=462, T=48, N=22176

      Residuals :
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    -1.4900 -0.0321  0.0199  0.0202  0.0715  0.9860 

          Coefficients :
          Estimate  Std. Error t-value  Pr(&gt;|t|)    
      K   2.3847e-07  2.8965e-06  0.0823 0.9343856    
      L  -8.0238e-07  2.3128e-07 -3.4693 0.0005229 ***
     LK -2.6986e-02  6.7755e-03 -3.9829 6.831e-05 ***
     LL  5.6920e-02  2.3933e-03 23.7830 &lt; 2.2e-16 ***
     KK  3.7811e-02  5.1254e-03  7.3773 1.674e-13 ***
    ---
       Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

        Total Sum of Squares:    426.54
       Residual Sum of Squares: 269.92
        R-Squared      :  0.38799 
          Adj. R-Squared :  0.3879 
</code></pre>

<p>My question are:</p>

<p>1) Is there a reason why the estimation for coefficient for LK is not significant in both within and random? but in first diff?</p>

<p>2) Why give within and random so similar results, and why first difference is different from them?</p>

<p>3)Can I interpret Standard error and R squared?
Is there anything else I can interpret? Which is the best model of the three?</p>

<p>Thank you so much for your help! </p>
"
"0.0618984460590173","0.0601929265428846"," 57551","<p>Was wondering if anyone knows of an R package to estimate the Cauchy-M estimator of regression (see for example the end of this <a href=""http://www.brnt.eu/phd/node13.html#SECTION00712200000000000000"" rel=""nofollow"">section</a>, but with simultaneous estimation of the scale parameter as in section 2 of (1)). </p>

<blockquote>
  <p>(1) Mizera, I. MÃ¼ller, C. H. (2002).  Breakdown points of Cauchy
  regression-scale estimators, Statistics &amp; Probability Letters, Volume
  57, Issue 1, Pages 79-89.</p>
</blockquote>
"
"0.165062522824046","0.160514470781026"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.232119172721315","0.240771706171538"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"0.232119172721315","0.240771706171538"," 62125","<p>I'm interested in the effect on beta estimation of including/excluding independent variables in linear regression. </p>

<p>I've made this data below:</p>

<pre><code>set.seed(50)
predictor1 &lt;- rnorm(10, 3, 1)
predictor2 &lt;- rnorm(10, 6, 1)
</code></pre>

<p>I've also simulated a model using these data, with an intercept of 2, a beta of 50 for <code>predictor1</code> and a beta
of 2 for <code>predictor2</code>:</p>

<pre><code>response &lt;- 2 + (50 * predictor1) + (2*predictor2)
</code></pre>

<p>A linear regression in <code>R</code> correctly calculates the intercept and both the betas:</p>

<pre><code>summary(lm(response ~ predictor1 + predictor2))

Call:
lm(formula = response ~ predictor1 + predictor2)

Residuals:
       Min         1Q     Median         3Q        Max 
-4.150e-14 -6.833e-15  1.100e-15  9.879e-15  2.757e-14 

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 2.000e+00  8.194e-14 2.441e+13   &lt;2e-16 ***
predictor1  5.000e+01  7.730e-15 6.468e+15   &lt;2e-16 ***
predictor2  2.000e+00  1.353e-14 1.479e+14   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.081e-14 on 7 degrees of freedom
Multiple R-squared:      1,  Adjusted R-squared:      1 
F-statistic: 2.095e+31 on 2 and 7 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>However, a linear regression with only <code>predictor2</code> in the model calculates a beta of -3.467 for <code>predictor2</code>:   </p>

<pre><code>summary(lm(response ~ predictor2))

Call:
lm(formula = response ~ predictor2)

Residuals:
    Min      1Q  Median      3Q     Max 
-75.201 -24.049   5.193  38.423  56.074 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  171.133    177.588   0.964    0.363
predictor2    -3.467     30.872  -0.112    0.913

Residual standard error: 47.58 on 8 degrees of freedom
Multiple R-squared:  0.001574,  Adjusted R-squared:  -0.1232 
F-statistic: 0.01261 on 1 and 8 DF,  p-value: 0.9134
</code></pre>

<p>Here is plot of <code>response</code> against <code>predictor1</code> and <code>predictor2</code>, with line of best fit calculated through regression:</p>

<pre><code> library(ggplot2)
ggplot(dat, aes(predictor2 , response )) + geom_point() + 
  geom_point(aes(predictor1 , response )) + 
  geom_abline(intercept = 2, slope = 2) + 
  geom_abline(intercept = 2, slope = 50) + 
  ylim(0, 300) + 
  xlim(0,10)
</code></pre>

<p><img src=""http://i.stack.imgur.com/fXEE8.jpg"" alt=""enter image description here""></p>

<p>And here is plot of just <code>response</code> against just <code>predictor2</code>, against with line of best fit calculated through regression:</p>

<pre><code> ggplot(dat, aes(predictor2 , response )) + 
      geom_point() + 
      geom_abline(intercept = 171.133, slope = -3.467) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/svEXN.jpg"" alt=""enter image description here""></p>

<p>I have two questions:</p>

<ol>
<li><p>How did the linear regression arrive at a beta of -3.467 in the second model, when the real beta is 2? Or in other words, how has excluding <code>predictor1</code> from the second model caused <code>predictor2</code>'s beta to drop by over 5?</p></li>
<li><p>Is someone able to provide a visual display of how linear regression calculates betas?</p></li>
</ol>
"
"0.247593784236069","0.240771706171538"," 63222","<p>How do I get p-values using the <code>multinom</code> function of <code>nnet</code> package in <code>R</code>?</p>

<p>I have a dataset which consists of â€œPathology scoresâ€ (Absent, Mild, Severe) as outcome variable, and two main effects: Age (two factors: twenty / thirty days) and Treatment Group (four factors: infected without ATB; infected + ATB1; infected + ATB2; infected + ATB3).</p>

<p>First I tried to fit an ordinal regression model, which seems more appropriate given the characteristics of my dependent variable (ordinal). However, the assumption of odds proportionality was severely violated (graphically), which prompted me to use a multinomial model instead, using the <code>nnet</code> package.  </p>

<p>First I chose the outcome level that I need to use as baseline category: </p>

<pre><code>Data$Path &lt;- relevel(Data$Path, ref = ""Absent"")
</code></pre>

<p>Then, I needed to set baseline categories for the independent variables:</p>

<pre><code>Data$Age &lt;- relevel(Data$Age, ref = ""Twenty"")
Data$Treat &lt;- relevel(Data$Treat, ref=""infected without ATB"") 
</code></pre>

<p>The model:</p>

<pre><code>test &lt;- multinom(Path ~ Treat + Age, data = Data) 
# weights:  18 (10 variable) 
initial value 128.537638 
iter 10 value 80.623608 
final  value 80.619911 
converged
</code></pre>

<p>The output:</p>

<pre><code>Coefficients:
         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   -2.238106   -1.1738540      -1.709608       -1.599301        2.684677
Severe     -1.544361   -0.8696531      -2.991307       -1.506709        1.810771

Std. Errors:
         (Intercept)    infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   0.7880046    0.8430368       0.7731359       0.7718480        0.8150993
Severe     0.6110903    0.7574311       1.1486203       0.7504781        0.6607360

Residual Deviance: 161.2398
AIC: 181.2398
</code></pre>

<p>For a while, I could not find a way to get the $p$-values for the model and estimates when using <code>nnet:multinom</code>. Yesterday I came across a post where the author put forward a similar issue regarding estimation of $p$-values for coefficients (<a href=""http://stats.stackexchange.com/questions/9715/how-to-set-up-and-estimate-a-multinomial-logit-model-in-r"">How to set up and estimate a multinomial logit model in R?</a>). There, one blogger suggested that getting $p$-values from the <code>summary</code> result of <code>multinom</code> is pretty easy, by first getting the $t$values as follows: </p>

<pre><code>pt(abs(summary1$coefficients / summary1$standard.errors), df=nrow(Data)-10, lower=FALSE) 

         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate 0.002670340   0.08325396      0.014506395     0.02025858       0.0006587898
Severe   0.006433581   0.12665278      0.005216581     0.02352202       0.0035612114
</code></pre>

<p>According to Peter Dalgard, ""There's at least a factor of 2 missing for a two-tailed $p$-value. It is usually a mistake to use the $t$-distribution for what is really a $z$-statistic; for aggregated data, it can be a very bad mistake.""
According to Brian Ripley, ""it is also a mistake to use Wald tests for <code>multinom</code> fits, since they suffer from the same (potentially severe) problems as binomial fits. 
Use profile-likelihood confidence intervals (for which the package does provide software), or if you must test, likelihood-ratio tests (ditto).""</p>

<p>I just need to be able to derive reliable $p$-values.</p>
"
"0.123796892118035","0.120385853085769"," 66419","<p>Is it a good or a bad practice to use R packages from CRAN for research? I'm talking about the common packages like: simple models for regression, estimation, econometrics.<br>
Most of them use function that can be written easily on your own.</p>

<p>My yes arguments:</p>

<ul>
<li>Time to focus on the main part of the research</li>
<li>The community is a good quality control</li>
<li>R offers a lot of modern methods</li>
<li>Some models are too complicated to write them on your own</li>
</ul>

<p>My con arguments:</p>

<ul>
<li>Not every package of R has been created in an academic environment</li>
<li>There could be bugs that influence the outcome and I do not know it</li>
<li>Most models can be written in a short time period from scratch</li>
</ul>

<p>How can we use open source without risking failures in the outcome? Are there certain quality indicators for packages in general and for R?</p>
"
"0.0875376219064817","0.0851256530758749"," 67470","<p>I want to predict a categorical variable using also categorical predictors. Currently, I am looking at classification and regression trees (CART).</p>

<p>The prediction quality is ""good enough"", except for the presence of impossible combinations. In the following minimal example, the combination <code>a==2, b==2</code> is impossible, yet the estimation decides not to use <code>b</code> for splitting.</p>

<pre><code>&gt; library(rpart)
&gt; d &lt;- data.frame(a=rep(factor(c(1,1,2)), 100000), b=factor(c(1,2,1)))
&gt; xtabs(~., d)
   b
a       1     2
  1 1e+05 1e+05
  2 1e+05 0e+00
&gt; (tr &lt;- rpart(a~b, d))
n= 300000 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 300000 1e+05 1 (0.6666667 0.3333333) *
</code></pre>

<p>When simulating stochastically from this model (by choosing the leaf value by sampling using the annotated probability vector, here $(2/3, 1/3)$, as weights), the combination <code>2, 2</code> will occur:</p>

<pre><code>&gt; prob.m &lt;- predict(tr, d, type=""prob"")
&gt; d$a.sim &lt;- apply(prob.m, 1, function(x) sample.int(length(x), size=1, prob=x))
&gt; xtabs(~a.sim+b, d)
     b
a.sim      1      2
    1 133041  66615
    2  66959  33385
</code></pre>

<p>Is there a way to avoid this, perhaps using another method?</p>

<p>This is just a small example for a more general case. I have around 10 predictors, and I want to exclude all combinations of two (or perhaps three) attributes that have no observation in the sample.</p>

<p>I am aware of the ""loss matrix"" that can be specified as a parameter to <code>rpart</code>, but this is prohibitive if many predictors are used.</p>
"
"0.0618984460590173","0.0601929265428846"," 67473","<p>I'm using <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""nofollow"">kernlab package</a></p>

<p>Here are two examples:<br/>
First:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.vanilla &lt;- rvm(x, y, kernel='vanilladot')
</code></pre>

<p>Got error:</p>

<pre><code>Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) :
the leading minor of order 2 is not positive definite
</code></pre>

<p>Second:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.rbf &lt;- rvm(x[1:1000], y[1:1000], kernel='rbfdot')
print(model.rbf)
py.rbf &lt;- predict(model.rbf, x[1001:1020])
print(paste(""MSE: "", sum((py.rbf - y[1001:1020]) ^ 2) / length(py.rbf)))
</code></pre>

<p>OK:</p>

<pre><code>Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Relevance Vector Machine object of class ""rvm"" 
Problem type: regression 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  5.44268665122008e-06 

Number of Relevance Vectors : 247 
Variance :  4.368e-06
Training error : 3.418e-06 
[1] ""MSE:  4.921706631013e-05""
</code></pre>

<p>Why doesn't using linear kernel work here? <code>polydot</code> (polynomial kernel function) doesn't work either.</p>

<p>Can this be fixed?</p>
"
"0.0618984460590173","0.0601929265428846"," 67662","<p>I am a research scholar and monitoring phenological events of timber line at a Himalayan region from past 4 years. During data analysis I found a research paper ""Estimation and comparison of flowering curve"" similar to my work.</p>

<p>In this paper the <code>R</code> package <code>bbmle</code> was used and five parameters $(\beta_{0},\ldots,\beta_{4})$ which describe: (i) the height; (ii) the peak date; (iii) the range; (iv) the symmetry; and (v) the peakedness of the regression curve were calculated. I also read the appendix table and followed the code to estimate these parameter but as a newbie I failed to do the calculations.</p>

<p>The full appendix table is here:
<img src=""http://i.stack.imgur.com/wOkDn.jpg"" alt=""Appendix table""></p>

<p>I am also posting the <code>startvals</code> for the year 2007 as mentioned in the appendix. Look at the values in figure below for year 2007.
<img src=""http://i.stack.imgur.com/ZPtKH.jpg"" alt=""Start values for year 2007""></p>

<p>My main problem is how to calculate <code>startvals</code> for example data of year 2007 as shown in figure where $(\beta_{0},\ldots,\beta_{4})$ describe: </p>

<ol>
<li>the height</li>
<li>the peak date</li>
<li>the range</li>
<li>the symmetry</li>
<li>the peakedness</li>
</ol>

<p>of the regression curve.</p>
"
"0.110727306471653","0.134595475514541"," 68137","<p>I have a data set consisting of one continuous response variable and about 70 predictors. Using this data, I want to construct a linear regression model. However, I don't know what predictors are worth including in the model, so I'll need to utilize a variable selection method that will allow me to isolate specific response variables. Unfortunately, I've noticed that my data violates a number of assumptions associated with linear regression. Therefore, I'll need to utilize a different estimation than OLS, such as robust or least squares estimation.</p>

<p>When running a linear regression with a different estimation method than least squares, how does one utilize a variable selection method such as stepwise? How can this be implemented in R?</p>
"
"0.138409133089567","0.134595475514541"," 72339","<p>After some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameters seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)</p>

<p><a href=""http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&amp;context=usdeptcommercepub&amp;sei-redir=1&amp;referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22"" rel=""nofollow"">This very nice article</a> states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.</p>

<p>I take this latter equation as the definition of $\kappa$.</p>

<p><a href=""http://books.google.nl/books?id=Ohks0xwvyT4C&amp;pg=PA196&amp;lpg=PA196&amp;dq=kappa+parameter+negative+binomial+proc+glimmix&amp;source=bl&amp;ots=PYKpaGQ8VN&amp;sig=5sNEB-7H7ZocErTKhi35ORKd2lA&amp;hl=nl&amp;sa=X&amp;ei=lEBVUqCnNcTJ0QXppYGoAg&amp;ved=0CDYQ6AEwAA#v=onepage&amp;q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&amp;f=false"" rel=""nofollow"">Apparently</a> this kappa is implemented in SAS.</p>

<p>Now turning to R, the function <code>glm.nb</code> in the <code>MASS</code> package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for <code>glm.nb</code> only refers to it as an ""additional parameter"". The answers to <a href=""http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r"">this</a> and <a href=""http://stats.stackexchange.com/questions/10457/interpreting-negative-binomial-regression-output-in-r?rq=1"">this</a> stackexchange questions directly imply that $\theta = 1/\kappa$, but <a href=""http://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1"">this</a> question [EDIT: since removed] seems to suggest that $\theta = \kappa$. </p>

<p>The <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html"" rel=""nofollow"">help page for negative binomial in R</a> is nice and introduces a parameter called <code>size</code> that equals $1/\kappa$. Fitting <code>glm.nb</code> on random data generated by <code>rnbinom</code> for various choices of $\mu$ and <code>size</code> seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = <code>size</code>) but also that for large values of size the estimation is poor.</p>

<p>Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? </p>
"
"NaN","NaN"," 72968","<p>I am working on vector auto-regression (VARs) and impulse response function (IRFs) estimation based on panel data with 33 individuals over 77 quarters.  How should this type of situation be analyzed?  What algorithm's exist for this purpose?  I would prefer to conduct these analyses in R, so if anyone is familiar with R code or a package designed for this purpose that they could suggest, that would be especially helpful.  </p>
"
"0.0618984460590173","0.0601929265428846"," 77617","<p>I have done a cross-sectional regression of time-series average returns on estimated Betas (over the same time horizon) to determine average premiums. So far so good. But I was told that the standard t-statistics can be biased, due to the fact that betas are estimated.</p>

<p>There is a solution by:</p>

<blockquote>
  <p>Shanken (1992) - On the estimation of beta-pricing models [Review of Financial Studies].  </p>
</blockquote>

<p>It does some small adjustments to the formula of the estimated covariance matrix. However I don't understand how to implement this in R. The paper is also very mathematical, but the solutions are supposed to be easy if you look e.g. at Cochrane Asset Pricing, chap 12 or <a href=""http://www.uv.es/qf/06006.pdf"" rel=""nofollow"">http://www.uv.es/qf/06006.pdf</a>. I cannot find anything close to that in the original paper though. I think the notation is very different.</p>

<p>Does anyone know how to do it, or has done it already? I needed the adjusted formula in my context (in-sample regression), or even better the R-code.</p>
"
"0.107211253483779","0.104257207028537"," 78001","<p>I would like to perform something like a linear regression on my distribution of data, but I'm interested in a trendline that estimates the <strong>minimum</strong>, <em>not mean</em>, value for each time bin. I'd like to do this in R.</p>

<p>The image below shows a scatterplot of the minimum value for each time bin. The black line is a typical linear regression, which estimates the mean. What I'd like is something like what I painted in red - an estimation of the minimum.</p>

<p><img src=""http://i.stack.imgur.com/D0O7Q.gif"" alt=""enter image description here""></p>

<p>My data look like this:</p>

<p><img src=""http://i.stack.imgur.com/qGAiS.png"" alt=""enter image description here""></p>

<p>Those are just the first few lines but you get the idea.</p>

<p>Thank you!</p>
"
"0.232119172721315","0.240771706171538"," 78663","<p>Is any one here familiar with an R package called Zelig?</p>

<p>I have a data frame like this:</p>

<pre><code>IQ   AGE
80   50
100  18
90   25
</code></pre>

<p>etc.</p>

<p>What I need to do is build a model of IQ given AGE, I am running these commands:
<code>z.out &lt;- zelig(IQ~AGE,data=df,model=""ls"")</code>
this runs the what-if given age 110, what would be the IQ
<code>x.out &lt;- setx(z.out, AGE=110)</code>
This is a simulation model where given the age 110, after running 1 million runs of simulation, what would be the IQ with 95% confidence interval.
<code>s.out &lt;- sim(z.out,x.out, num=1000000, level=95)</code></p>

<p>I have a hard time understanding from what pool of data the <code>sim()</code> function draws the numbers. I read though the docs, but they are written for Ph.D. students, if not more advanced readers. I have asked the Zelig creators this question multiple times but they are directing me to the docs which I read multiple times, with no luck. However, one of the  person that works with Zelig sent me this email:</p>

<blockquote>
  <p>Suppose that you fit 
  $$\text{IQ} = a + \text{Age} * b + e$$
  Then you get a table of regression coefficients where a=50, b=2, and their standard errors are something like $\text{s.e.}(a)=\sqrt{10}$ and $\text{s.e.}(b)=1$. These are all hypothetical examples. 
  In maximum likelihood estimation, this regression output is another way of saying that $a$ and $b$ are distributed bivariate normal with means $[50,2]$ and there's a variance-covariance matrix that looks something like this (all numbers are made up):
  $$\begin{array}{cc}
10 &amp; cov(a,b) \\
cov(a,b) &amp; 1 \\
\end{array}$$
  So, the variance of $a$ is 10, the variance of $b$ is 1, and their covariance is $cov(a,b)$. It won't be shown in your regression table, but Zelig remembers it for you. Let's pretend it's 3.
  This variance-covariance matrix is the inverse of the Hessian I mentioned earlier. Don't worry about it. For this example, you need only remember that $\text{mean}(a,b) = [50,2]$ and $cov(a,b)=\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. For purposes of plain text email, I'm representing matrices with columns separated by commas and rows by semicolons.
  In addition, suppose that the error term $e$ is distributed with mean 0 and s.d.=1.
  Now, one way to predict what IQ you might get for somebody aged 88, based on this regression table, is exactly what you would expect: you simply calculate 50 + 88 * 2 = 226. This is your point estimate. The 95% confidence interval around this point estimate is a function of the standard errors of the coefficient estimates of $a=50$ and $b=2$, and the exact formula for that is in any econometrics textbook.
  Simulation makes it unnecessary to dig up that textbook. Instead, for 1000 rounds, <code>sim()</code> will come up with 1000 different pairs of $(a,b)$ estimates drawn from the bivariate normal with mean=[50,2] and cov=$\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. One such pair might be $(47,1.5)$; another might be $(52,3)$; yet another might be $(10,5)$. 
  Whatever they are, <code>sim()</code> plugs them into the formula and gives you 1000 different estimates for the IQ. Their average is your point estimate. If you stack them from lowest to highest, the ends of the 95% confidence interval are the top 25th value and the bottom 25th. That's it. That's all that <code>sim()</code> does.</p>
</blockquote>

<p>Given the above explanation, can anybody tell me in lay terms, what numbers <code>sim()</code> is picking? How are those numbers in pool generated? I would greatly appreciate if anyone brings some light into this.</p>
"
"0.151619608715781","0.147441956154897"," 84319","<p>I am working on an age estimation method using 4 types of biological measurements as age predictors. I am using RStudio. 
So far, I have good results when I use linear regression (<code>lm(age~predictor)</code>), but I am encountering heteroskedasticity, and therefore cannot build prediction intervals for my models.<br> 
I have tried transformations to normalize the predictors using ln, inverse, and square root, but to no avail.<br>
I have found a paper explaining the <code>wls</code> function, and I have used it in my models with the weight: $$\frac 1 {1+\frac{\text{predictor}^2} 2}$$ 
This has given me better age predictions, but does not solve the heteroskedasticity problem. </p>

<p>I have done some research, and apparently, one of my options is to create homoscedastic groups in my data by finding the data points where the residual variances change. 
For that, I have used the breakpoints function of strucchange, which gave me 5 breakpoints by default. 
I now want to give 6 different weights (weights are $\frac 1 {\text{var(age)}}$ of each interval) to my 6 intervals of data, but I cannot find a function to do that. I would greatly appreciate any help on the subject. 
Thanks.</p>
"
"0.167967753286756","0.181488502160157"," 85909","<p>The <code>plm</code> function of the <code>plm</code> library in R is giving me grief over having duplicate time-id couples, even when I'm running a model that I don't think should need a time variable at all (see reproducible example below).</p>

<p>I can think of three possibilities:</p>

<ol>
<li>My understanding of fixed effects regression is wrong, and they really do require unique time indices (or time indices at all!).</li>
<li>plm() is just being overly-finicky here and should relax this requirement.</li>
<li>The particular estimation technique that plm() uses--the within transformation--requires time indices, even though the order doesn't seem to matter and the less computationally-efficient version (including dummies in a straight-up OLS model) doesn't need them.</li>
</ol>

<p>Any thoughts?</p>

<pre><code>set.seed(1)
n &lt;- 1000
test &lt;- data.frame( grp = as.factor(rep( letters, (n/length(letters))+1 ))[seq(n)], x = runif(n), z = runif(n) )
test$y &lt;- with( test, 2*x + 3*z + rnorm(n) )
lm( y ~ x + z, data = test )
lm( y ~ x + z + grp, data = test )

require(plm)
# Model fails if I don't specify a time index, despite effect = ""individual""
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = ""grp"" ) 
# Create time variable and add it to the index but still specify individual FE not time FE also
library(plyr)
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = seq(nrow(dat)) ) )
# Now plm() works; note coefficients clearly include the fixed effects, as they match the lm() version above
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Scramble time variables and show they don't matter as long as they're unique within a cluster
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = sample(t) ) )
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Add a duplicate time entry and show that it causes plm() to fail
test[ 2, ""t"" ] &lt;- test[ 1, ""t"" ] 
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
</code></pre>

<p><strong>Why this matters</strong></p>

<p>I'm trying to bootstrap my model, and when I do the requirement that the index-time pairs be unique is causing headaches which seem unnecessary if (2) is true.</p>
"
"0.223178021137329","0.217028683070608"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"0.276818266179133","0.269190951029083"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"NaN","NaN"," 89597","<p>Which method would you suggest for estimation an univariate regression ($y = a + bx + e$) which comprise a lot of $0$ values in the dependent variable (actually kind of censored data). My experience hints at the use of the <strong>Tobit model</strong>. But There is a problem connected with the normal distribution and homoscedasticity of the error term. In my case I'm suspecting a <em>violation</em> of these basic assumptions for using Tobit. In particular the behaviour of latent $y$ under the zero is not anticipating. </p>

<p>Do you know some method (literature) for handling this kind of problems?</p>
"
"0.171675400874868","0.200334168988253"," 93352","<p>I'm trying to replicate the results of the first model of this article:</p>

<blockquote>
  <p>Hultman, Lisa, Jacob Kathman, and Megan Shannon. 2013. â€œUnited Nations Peacekeeping and Civilian Protection in Civil War.â€ <em>American Journal of Political Science</em> 57(4): 875â€“91.</p>
</blockquote>

<p>Replication material can be found here: <a href=""http://thedata.harvard.edu/dvn/dv/ajps/faces/study/StudyPage.xhtml?studyId=87987&amp;tab=files"" rel=""nofollow"">http://thedata.harvard.edu/dvn/dv/ajps/faces/study/StudyPage.xhtml?studyId=87987&amp;tab=files</a></p>

<p>The Stata code provided in the do file runs without problems. However, when I try to replicate the model in R. The error strongly resembles the one in <a href=""http://stats.stackexchange.com/questions/52527/unable-to-fit-negative-binomial-regression-in-r-attempting-to-replicate-publish"">this CV question</a>. Here's the R code I've written to replicate the results:</p>

<pre><code>library(MASS)
library(foreign)

pko &lt;- read.dta(""HKS_AJPS_2013.dta"")

pko_model1 &lt;- glm.nb(osvAll ~ troopLag + 
                       policeLag + 
                       militaryobserversLag + 
                       brv_AllLag + 
                       osvAllLagDum  + 
                       incomp + 
                       epduration + 
                       lntpop, data = pko, link = log)
</code></pre>

<p>This produces the following error:</p>

<pre><code>  Error in glm.fitter(x = X, y = Y, w = w, etastart = eta, offset = offset,  : 
  NA/NaN/Inf in 'x'
</code></pre>

<p>If I include the <code>control=glm.control(trace=10,maxit=100)</code> option in <code>glm.nb</code> it produces the following output:</p>

<pre><code>Deviance = 75787029 Iterations - 1
Deviance = 28247900 Iterations - 2
Deviance = 11043902 Iterations - 3
Deviance = 4952253 Iterations - 4
Deviance = 2896062 Iterations - 5
Deviance = 2286069 Iterations - 6
Deviance = 2152722 Iterations - 7
Deviance = 2135621 Iterations - 8
Deviance = 2134804 Iterations - 9
Deviance = 2134801 Iterations - 10
Deviance = 2134801 Iterations - 11
theta.ml: iter 0 'theta = 0.000609'
theta.ml: iter1 theta =0.00120256
theta.ml: iter2 theta =0.00234778
theta.ml: iter3 theta =0.00449211
theta.ml: iter4 theta =0.0082914
theta.ml: iter5 theta =0.0143798
theta.ml: iter6 theta =0.0225089
theta.ml: iter7 theta =0.0302781
theta.ml: iter8 theta =0.0342821
theta.ml: iter9 theta =0.0349533
theta.ml: iter10 theta =0.0349683
Initial value for 'theta': 0.034968
Deviance = 3634.951 Iterations - 1
Deviance = 1160161 Iterations - 2
Error in glm.fitter(x = X, y = Y, w = w, etastart = eta, offset = offset,  : 
  NA/NaN/Inf in 'x'
</code></pre>

<p>If I exclude the <code>epduration</code> variable or the <code>incomp</code> variable, the error disappears and I can roughly replicate the results from the article, but the parameter estimates of course vary because I don't include all variables in the model (and I don't use robust, clustered standard errors in R).</p>

<p>Two questions:</p>

<ol>
<li>Why does this run in Stata without any complaint, but not in R?</li>
<li>How can I make this work in R? The answers to this <a href=""http://stats.stackexchange.com/questions/52527/unable-to-fit-negative-binomial-regression-in-r-attempting-to-replicate-publish"">question</a> suggest a possible solution by first estimating a Poisson regression and then feeding the results as starting parameter values into an MLE estimation. Yet, I haven't been able to make this work in R. </li>
</ol>

<p>I realize that this might be a duplicate to the <a href=""http://stats.stackexchange.com/questions/52527/unable-to-fit-negative-binomial-regression-in-r-attempting-to-replicate-publish"">existing CrossValidated question</a> but since I'm having a similar problem with different data, this might be a more general problem.</p>
"
"0.107211253483779","0.104257207028537"," 94062","<p>I am trying to NOT use packages for the estimation of models in order to have a deeper understanding of how things work. Currently, I am trying to estimate a VAR(1) (vector autoregression of first order) and my question is about how to construct the matrix of dependent and independent variables.</p>

<p>Theoretically, my model is:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cbinom%7Bx_t%7D%7By_t%7D%20%3D%20%5Cbinom%7Bc_1%7D%7Bc_2%7D%20&plus;%20%5Cbegin%7Bpmatrix%7D%20a_%7B11%7D%20%26%20a_%7B12%7D%5C%5C%20a_%7B21%7D%20%26%20a_%7B22%7D%20%5Cend%7Bpmatrix%7D%5Cbinom%7Bx_%7Bt-1%7D%7D%7Bx_%7Bt-2%7D%7D%20&plus;%20%5Cbinom%7Bu_%7B1%2Ct%7D%7D%7Bu_%7B2%2Ct%7D%7D"" alt=""model""></p>

<p>As for the matrix of dependent variables, it should be easy enough. I just need to stack <code>xt</code> and <code>zt</code>. However, I don't know how to construct the matrix of independent variables. I read that VAR is very similar to SUR (Seemingly Unrelated Regressions) so I think that the matrix of independent variables should be a block diagonal matrix, but what should it look like? Especially if I want to add a constant and a time trend to the model?</p>

<p>Thank you.</p>
"
"0.241408116277306","0.262374883916401"," 95994","<p>I`d like to extract the parameters of a two-component mixture distribution of noncentral student t distributions which first has to be fitted to a one-dimensional sample.</p>

<p>My question is closely related to this thread, but as pointed out I want to use Student t components for the mixture:
<a href=""http://stats.stackexchange.com/questions/10062/which-r-package-to-use-to-calculate-component-parameters-for-a-mixture-model?newreg=fe1454a4702e4532a03bd2c705fe3b02"">Which R package to use to calculate component parameters for a mixture model</a></p>

<p>There are many packages for R that are capable of handling mixture distributions in one way or another. Some in the context of a Bayesian framework requiring kernels. Some in a regression framework. Some in a nonparametric framework. ...</p>

<p>In general the ""mixdist""-package seems to come closest to my wish. This package fits parametric mixture distributions to a sample of data. Unfortunately it doesn`t support the student t distribution.</p>

<p>I have also tried to manually set up a likelihood function as described here:
<a href=""http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions"">http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions</a>
But my result is far from perfect.</p>

<p>The ""gamlss.mx""-package might be helping, but originally it seems to be set up for another context, i.e. regression. I tried to regress my data on a constant and then extract the parameters for the estimated mixture error distribution. Is this a valid approach? </p>

<p>But with this approach the estimated parameters seem to be not directly accessable individually by some command (such as fit1$sigma). And more importantly there seem to be serious estimation problems even in pretty simple and nonambiguous cases.
E.g. in example 2 (see syntax below) I simulated a mixture which looks like this:</p>

<p><img src=""http://i.stack.imgur.com/MG7AA.jpg"" alt=""kernel density estimate of the mixture""></p>

<p>When trying to fit a two-component student t mixture to these data either I get this error message (the deeper meaning of which I don't understand):</p>

<p><img src=""http://i.stack.imgur.com/UPvg4.jpg"" alt=""enter image description here""></p>

<p>or I get wrong results (convergenve is reached only after approximately two hours as can be seen from the output):</p>

<p><img src=""http://i.stack.imgur.com/HjlfW.jpg"" alt=""enter image description here""></p>

<p>The means could be estimated well, but both the variance and the degrees of freedom are estimated badly. In the TF2 implementation of the student t, the sigma parameter denotes the standard deviation. Its estimate is NEGATIVE for the first component! And for the second component the degrees of freedom estimate is also NEGATIVE. Probably one should not use these results in practice :(</p>

<p>By the way: Is there a way to restrict these degree-of-freedom coefficient estimates to be natural numbers? </p>

<p>The following syntax is my gamlss.mx-setup so far:</p>

<pre><code>library(gamlss.dist)
library(gamlss.mx)
library(MASS)

# example 1 (real data):
data(geyser)
plot(density(geyser$waiting) )
fit1 &lt;- gamlssMX( waiting~1,data=geyser,family=""TF2"",K=2 )
fit1
# works fine

# example 2 (simulated data):
N &lt;- 100000
components &lt;- sample(1:2,prob=c(0.6,0.4),size=N,replace=TRUE)
mus &lt;- c(3,-6)    # denotes the mean of component 1 and 2, respectively
sds &lt;- c(1,9)     # ... the standard deviations
nus &lt;- c(25,3)    # ... the degrees of freedom
mixsim &lt;-data.frame(rTF2( N,mu=mus[components],sigma=sds[components],nu=nus[components] ))
colnames(mixsim) &lt;- ""MCsim""
plot(density(mixsim$MCsim) , xlim=c(-50,50))
fit2 &lt;- gamlssMX(MCsim~1,data=mixsim,family=""TF2"",K=2)
fit2
# error message or strange results (this also happens when using a sample of S&amp;P500 returns)
</code></pre>

<p>I would be very grateful for any advice!
I've read through many related manuals and vignettes so far but I`m still lost.</p>

<p>Thanks a lot in advance!!
Jo</p>
"
"0.0928476690885259","0.120385853085769"," 97437","<p>I am studying the factors influencing the annual salary for employees at a undisclosed bank. The regression model that I have decided to employ is as follows:</p>

<p>\begin{equation}
Y_{k}=\beta_{1}+\beta_{2}E_{k}+\beta_{3}D_{gk}+\beta_{4}D_{mk}+\beta_{5}D_{2k}+\beta_{6}D_{3k}+\varepsilon_{k}
\end{equation}
where $Y_{k}$ is the logarithm of annual salary, $E$ is the number of years of education, $D_{g}$ is a gender dummy, $D_{m}$Â is a minority dummy, and where </p>

<p>\begin{equation}
D_{2}=\begin{cases} 1 &amp;\text{Custodial job} \\ 0 &amp; \text{Otherwise} \end{cases}
\end{equation}<br>
and 
\begin{equation}
D_{3}=\begin{cases} 1 &amp;\text{Management job} \\ 0 &amp; \text{Otherwise} \end{cases}
\end{equation}
As you know, whenever one deals with GLS, $\Omega$ will almost surely be unknown and thus have to be estimated. In general there are $\frac{n(n+1)}{2}$ parameters to be estimated, which makes it pretty impossible to come up with a viable estimation out of $n$Â observations. This is usually counteracted by imposing some structure on $\Omega$.</p>

<p>In my case, I would like to make the assumption that the disturbance terms $\varepsilon_{k}$ in the above regression model have variance $\sigma_{i}^{2}$ for $i=1,2,3$, according to whether the $i$-th employee has a job in category 1,2, or 3 respectively. Now, we may introduce the transformations $\gamma_{1}=\log (\sigma_{1}^{2}),\gamma_{2}=\log(\sigma_{2}^{2}/\sigma_{1}^{2})$, and $\gamma_{3}=\log(\sigma_{3}^{2}/\sigma_{1}^{2})$ so as to enable us to formulate the following model for</p>

<p>\begin{equation}
\sigma_{k}^{2}= \exp \{ \gamma_{1}+\gamma_{2}D_{2k}+\gamma_{3}D_{3k} \}
\end{equation}
Since $\hat{\beta}_\rm{OLS}$ is a consistent estimate of $\beta$, even under the assumption of heteroscedasticity, we have that $\hat{\beta}_{\rm OLS} \xrightarrow[]{p}\beta$ as the number of observations increase. We may therefore argue that $e_{k}^{2} \approx \sigma_{k}^{2}$, and so we can regress upon information that we already possess. </p>

<p><strong>Summary of procedure</strong></p>

<p><strong>(1)</strong> Calculate the OLS estimate.</p>

<p><strong>(2)</strong> Calculate the OLS residual $\textbf{e}=\textbf{Y}-\textbf{X}\hat{\beta}$</p>

<p><strong>(3)</strong> Calculate the OLS estimate of $\gamma$ from $e_{k}^{2}=f_{\gamma}(Z_{k})+\overline{\varepsilon}_{k}$.</p>

<p><strong>(4)</strong> Calculate the FGLS estimate as the GLS estimate with $\hat{\Omega}=\Omega(\hat{\gamma})$ in place of $\Omega$. </p>

<blockquote>
  <p>What I would like to know is whether or not one can perform this estimation using a known function in R, say <code>gls</code>? If the answer is yes, then how exactly should I write to ensure that that my heteroscedasticity assumption is taken into account? Thanks for taking the time! Have a great day!</p>
</blockquote>
"
"0.126349673929817","0.147441956154897","103786","<p>I have a model where time is the response variable. I'd like to generate confidence intervals for the estimates. I have established that the error in the estimation is roughly normally distributed (it may be more cauchy). The Mean and Median are very different, with the median more accurately representing the middle of the data. Am I allowed to use the median for my confidence interface and if so is there a different method for doing so?</p>

<p>I have reviewed this question: <a href=""http://stats.stackexchange.com/questions/21103/confidence-interval-for-median"">Confidence interval for median</a> but it is not clear if they are trying to accomplish the same thing I am.</p>

<p>EDIT
The model is for an estimation of the amount of time a process takes to complete. I performed a linear regression and established that the model has a relatively good fit. I then took repeatedly (2000 times) took a random sample of 75% of the original sample and rebuilt the model. I then predicted the time for the remaining 25%, and stored the error in each case. This led to ~90000 results, which roughly follow a normal distribution (or possibly cauchy) I would like to find an estimate for the confidence interval of an individual result, e.g. for one specific process the actual time taken was 46 seconds, and the predicted time taken was 1 minute. I'd like to be able to say with 95% certainty that my estimate is accurate within +- 15 seconds (for example).</p>
"
"0.214422506967559","0.208514414057075","104180","<p>Suppose X is the design matrix of my experiment of which I want to model a linear regression model. Such a design matrix can be created by (in this example it is a full factorial design)</p>

<pre><code>library(BHH2)
Des2 &lt;- ffDesMatrix(5)
</code></pre>

<p>Now, a contrast matrix can be obtained by the following code, corresponding to ( t(X) %x% X)^-1 %x% t(X) (in which %x% represents matrix multiplication):</p>

<pre><code>solve(t(Des2) %*% Des2) %*% t(Des2)
</code></pre>

<p>This results in a matrix giving the Y observations that will be contrasted against each other in order to obtain a parameter estimate (e.g. first row will correspond to beta1).</p>

<p>Now, one can also make a design matrix using the model.matrix function</p>

<pre><code>Des &lt;- as.data.frame(Des2)
names(Des) &lt;- c(""X1"",""X2"",""X3"",""X4"",""X5"")
model.matrix(~X1*X2*X3*X4*X5,data=Des)
</code></pre>

<p>This is a model matrix that can be used to estimate all possible interactions between five factorial variables with two levels.</p>

<p>When comparing the columns of the model matrix obtained by the last piece of code with the rows of our contrast matrix, one can note that they are rather similar. The values might be different, but the signs representing the contrasts are identical for a row/column representing the same parameter to be estimated.</p>

<p>My question now refers to these columns and rows: is there any difference to be noted between these two? Or are they identical as in that they both represent contrasts of the Y observations that will be used for estimation of the parameter, and have I thus summed up two equivalent way in obtaining these contrasts?</p>
"
"0.110727306471653","0.134595475514541","105346","<p>I am interested in estimating an adjusted risk ratio, analogous to how one estimates an adjusted odds ratio using logistic regression. Some literature (e.g., <a href=""http://aje.oxfordjournals.org/content/159/7/702.abstract"">this</a>) indicates that using Poisson regression with Huber-White standard errors is a model-based way to do this</p>

<p>I have not found literature on how adjusting for continuous covariates affects this. The following simple simulation demonstrates that this issue is not so straightforward: </p>

<pre><code>arr &lt;- function(BLR,RR,p,n,nr,ce)
{
   B = rep(0,nr)
   for(i in 1:nr){
   b &lt;- runif(n)&lt;p 
   x &lt;- rnorm(n)
   pr &lt;- exp( log(BLR) + log(RR)*b + ce*x)
   y &lt;- runif(n)&lt;pr
   model &lt;- glm(y ~ b + x, family=poisson)
   B[i] &lt;- coef(model)[2]
   }
   return( mean( exp(B), na.rm=TRUE )  )
}

set.seed(1234)
arr(.3, 2, .5, 200, 100, 0)
[1] 1.992103
arr(.3, 2, .5, 200, 100, .1)
[1] 1.980366
arr(.3, 2, .5, 200, 100, 1)
[1] 1.566326 
</code></pre>

<p>In this case, the true risk ratio is 2, which is recovered reliably when the covariate effect is small. But, when the covariate effect is large, this gets distorted. I assume this arises because the covariate effect can push up against the upper bound (1) and this contaminates the estimation.</p>

<p>I have looked but have not found any literature on adjusting for continuous covariates in adjusted risk ratio estimation. I am aware of the following posts on this site: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes"">Poisson regression to estimate relative risk for binary outcomes</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38004/poisson-regression-for-binary-data"">Poisson regression for binary data</a></li>
</ul>

<p>but they do not answer my question. Are there any papers on this? Are there any known cautions that should be exercised? </p>
"
"0.138409133089567","0.134595475514541","108051","<p>I am trying to run a probit regression using panel data in R by first computing the log likelihood and then using the <code>optim</code> function to optimize. </p>

<ol>
<li><p>Scale of predictor variables: The predictor variables that I have vary in scale significantly. Some of the variables are in tens while some are in '000s. When I am trying to run the <code>optim</code> function, it frequently gives the error message: <code>vmmin is not finite</code>. So I have to be extremely careful in choosing the initial values. And some of these initial values are as small as 0.0001. Is this standard in probit? Or do we need to normalize the predictor variables before using probit?</p></li>
<li><p>I have a few variables that are computed within the estimation code, i.e., some of the parameters that are being estimated are used to calculate these inbuilt covariates. When I run the full model with all such inbuilt parameters, the <code>optim</code> function gives me a singular Hessian matrix. Again, I am not sure what the singularity of the hessian implies. When I the add the inbuilt covariates one at a time, the Hessian is not singular. </p></li>
</ol>

<p>Has anyone encountered such problems before? Please advise. </p>
"
"0.186630836985285","0.199637352376173","113450","<p>I am trying to replicate a Stata manual example (-mlogit-) of nested
logit estimation,</p>

<pre><code>webuse restaurant
nlogitgen type = restaurant(fast: Freebirds | MamasPizza, family: ///
 CafeEccell | LosNortenos | WingsNmore, fancy: Christophers | MadCows)

nlogittree restaurant type, choice(chosen)
nlogit chosen cost distance rating || type: income kids, base(family) ///
 || restaurant:, noconst case(family_id)
</code></pre>

<p>The output from this is as below:</p>

<pre><code>. nlogit chosen cost distance rating || type: income kids, base(family) ///
&gt;  || restaurant:, noconst case(family_id)

tree structure specified for the nested logit model

 type    N      restaurant    N   k 
-------------------------------------
 fast   600 --- Freebirds    300  12
             +- MamasPizza   300  15
 family 900 --- CafeEccell   300  78
             |- LosNortenos  300  75
             +- WingsNmore   300  69
 fancy  600 --- Christophers 300  27
             +- MadCows      300  24
-------------------------------------
                     total  2100 300

k = number of times alternative is chosen
N = number of observations at each level

Iteration 0:   log likelihood = -541.93581  
Iteration 1:   log likelihood = -517.95909  (backed up)
Iteration 2:   log likelihood = -511.99261  (backed up)
Iteration 3:   log likelihood =  -506.5559  (backed up)
Iteration 4:   log likelihood = -501.33127  (backed up)
Iteration 5:   log likelihood = -492.93121  (backed up)
Iteration 6:   log likelihood = -491.75884  
Iteration 7:   log likelihood = -490.81911  
Iteration 8:   log likelihood = -489.56195  
Iteration 9:   log likelihood = -488.58891  
Iteration 10:  log likelihood = -485.68122  
Iteration 11:  log likelihood = -485.59216  
Iteration 12:  log likelihood = -485.56202  
Iteration 13:  log likelihood = -485.51992  
Iteration 14:  log likelihood = -485.48062  
Iteration 15:  log likelihood = -485.47384  
Iteration 16:  log likelihood = -485.47333  
Iteration 17:  log likelihood = -485.47331  

RUM-consistent nested logit regression         Number of obs      =       2100
Case variable: family_id                       Number of cases    =        300

Alternative variable: restaurant               Alts per case: min =          7
                                                              avg =        7.0
                                                              max =          7

                                                  Wald chi2(7)    =      46.71
Log likelihood = -485.47331                       Prob &gt; chi2     =     0.0000

------------------------------------------------------------------------------
      chosen |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
restaurant   |
        cost |  -.1843847   .0933975    -1.97   0.048    -.3674404   -.0013289
    distance |  -.3797474   .1003828    -3.78   0.000    -.5764941   -.1830007
      rating |    .463694   .3264935     1.42   0.156    -.1762215     1.10361
------------------------------------------------------------------------------
type equations
------------------------------------------------------------------------------
fast         |
      income |  -.0266038   .0117306    -2.27   0.023    -.0495952   -.0036123
        kids |  -.0872584   .1385026    -0.63   0.529    -.3587184    .1842016
-------------+----------------------------------------------------------------
family       |
      income |          0  (base)
        kids |          0  (base)
-------------+----------------------------------------------------------------
fancy        |
      income |   .0461827   .0090936     5.08   0.000     .0283595    .0640059
        kids |  -.3959413   .1220356    -3.24   0.001    -.6351267   -.1567559
------------------------------------------------------------------------------
dissimilarity parameters
------------------------------------------------------------------------------
type         |
   /fast_tau |   1.712878    1.48685                     -1.201295    4.627051
 /family_tau |   2.505113   .9646351                       .614463    4.395763
  /fancy_tau |   4.099844   2.810123                     -1.407896    9.607583
------------------------------------------------------------------------------
LR test for IIA (tau = 1):           chi2(3) =     6.87   Prob &gt; chi2 = 0.0762
------------------------------------------------------------------------------
</code></pre>

<p>Here is the R code that I wrote to replicate that:</p>

<pre><code>library(mlogit)
library(foreign)
dfNL = read.dta('http://www.stata-press.com/data/r12/restaurant.dta')

nlRest = mlogit(chosen ~ - 1 + cost + distance + rating | 0 + income + kids,
       data = dfNL, un.nest.el = TRUE,
       nests = list(fast = c('Freebirds', 'MamasPizza'),
                     family = c('CafeEccell', 'LosNortenos', 'WingsNmore'),
                    fancy = c('Christophers', 'MadCows')),
       shape = 'long', choice = 'chosen', chid.var = 'family_id',
       alt.var = 'restaurant')
summary(nlRest)
</code></pre>

<p>How can I specify that the parameters on ""income"" and ""kids"" should
vary at the nest level rather than at the choice level? Should I
impose constraints or is there a more natural way to do this?</p>
"
"0.0618984460590173","0.0601929265428846","118396","<p>I'm trying to fit a quantile regression model for rigth censoring data and I'm using R with the package quantreg and its function  crq. I'm trying the Portnoy method that it's suposed to estimate the full range of tau (quantiles) , but the results only contains 85 taus, ending in tau=0.4 and giving me estimated values of that lasts taus that are very far away from the real 0.4 quantile and more around 0.95 quantile.</p>

<p>I read in the quantreg doc that Portnoy and Peng-Huang may be unable to estimate upper conditional quantiles if censoring is heavy in the upper in the upper tail, but this doesn't seem to be my case. In fact the general distribution summary of time is:</p>

<pre><code>Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.0   223.0   447.0   482.5   714.0  1251.0 
</code></pre>

<p>the quantiles vary obviously with levels of VAR1 and VAR2 but the shape of the distribution is almost the same.I can produce Kaplan-Mier estimation without problem. </p>

<p>Here is my syntax</p>

<pre><code>qreg1&lt;-crq(Surv(TIME,EVENT,type=""right"")~VAR1+VAR2,
       data=DATA_TRAIN,method = ""Portnoy"")
</code></pre>

<p>What am I doing wrong?</p>

<p>Thank you in advance,</p>
"
"0.311284094783699","0.324329062548308","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.123796892118035","0.120385853085769","133387","<p>I'm trying to create a prediction model for estimation of continuous variable based on about 35 Independent variables.My data set has circa 27k observartions.
Here is the summary of the the targeted continuous variable:</p>

<pre><code>              Frequency Percent
(0,5]              2706  10.053
(5,10]             5226  19.415
(10,25]            4397  16.335
(25,100]           7142  26.533
(100,1e+03]        6465  24.018
(1e+03,1e+05]       981   3.645
Total             26917 100.000
</code></pre>

<p>I tried (by using R) Random Forest (RandomForest package),Linear regression, Conditional Inference Trees (ctree function in party package) but all of them have results that have a significant overestimation.
Here are the results of the prediction where I counted number of observations by thier distance from the actual values:
Any idea how can i balance the results?</p>

<p><img src=""http://i.stack.imgur.com/y70OM.png"" alt=""enter image description here""></p>

<p>Here are some views on the data:
The target variable is LTV for a user, I would like to predict LTV value after 180 days  based on users behavior of the first 7 days.
Here Is a summary fot the target variavle:</p>

<pre><code>  vars     n   mean     sd median trimmed   mad  min      max    range skew kurtosis   se
1    1 26917 178.35 622.29  33.49   66.63 39.28 0.03 22103.73 22103.71 14.1   325.08 3.79
</code></pre>

<p>UPDATE:
Here are the distributions of the targeted variable (first)and the prediction (secound)results:
<img src=""http://i.stack.imgur.com/b3MBs.png"" alt=""targeted variable"">
<img src=""http://i.stack.imgur.com/3N7d1.png"" alt=""prediction results based on the linear regression model that was the best""></p>
"
"NaN","NaN","134012","<p>STATA has recently implemented effect size calculations for regression models in their postestimation procedures <a href=""http://blog.stata.com/2013/09/05/measures-of-effect-size-in-stata-13/"" rel=""nofollow"">measures of effect size in Stata 13</a>. It takes a regression or ANOVA object as input, and returns eta-squared or omega squared as well as their confidence intervals. </p>

<p>Is anyone aware of any such or similar implentation in R software, or how this could be achieved using the contents of lm or aov objects?</p>
"
"0.151619608715781","0.147441956154897","137673","<p>This is the data set I have:</p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
</code></pre>

<p>Doing an estimation of the coefficient with a linear regression in Java and R I got different results. This is my code in Java.</p>

<pre><code>List&lt;Double&gt; y= new ArrayList&lt;&gt;(Arrays.asList(
        -7.4599812, 13.2665113, 12.1012781, 2.3806622, 26.4239262
        ));
SimpleRegression ls = new SimpleRegression();
List&lt;Double&gt; yNew = new ArrayList&lt;Double&gt;();
for (int i = 0; i &lt; y.size(); i++) {
    ls.addData(i, y.get(i));
}

double alpha0 = ls.getIntercept();
double beta0 = ls.getSlope();
double coefficients[] = { alpha0, beta0 };
</code></pre>

<p>This is the output result:</p>

<pre><code>alpha0 = -2.0339138199999995
beta0  =  5.68819657
</code></pre>

<p>In R it seems to be that we should start for x with 1 instead of 0:</p>

<blockquote>
  <p>lsfit(x, y, wt = NULL, intercept = TRUE, tolerance = 1e-07, yname =
  NULL)</p>
</blockquote>

<p>Starting with 1 instead of 0 in R, I get:</p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
vecx &lt;- c(1,2,3,4,5)
lsfit(vecx, vector)
alpha = -7.72211 
beta = 5.688197
</code></pre>

<p>When I start with 0 in R, I get: </p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
vecx &lt;- c(0,1,2,3,4)
lsfit(vecx, vector)
alpha = -2.033915  
beta = 5.688197
</code></pre>

<p>which is the same value as my code in Java. On the internet I have found that the values of my Java code and with vecx &lt;- c(0,1,2,3,4) in R are the values which are correct but I am not sure. </p>

<p><strong>My question is: What is the truth? To start with 0 or with 1 to estimate the coefficients?</strong> </p>

<p>Thanks for your answer. </p>
"
"0.186630836985285","0.199637352376173","140761","<p>I have a question regarding the use of propensity score in a survival analysis with use of mutliple imputation to handle missing data. The question is of theoretical nature and may well apply to other situations.</p>

<p>I have a data set of <em>n</em> individuals. The aim is to estimate the effect of a treatment on a binary outcome (death). The analysis is based on propensity score; the propensity score is derived by means of logistic regression, which includes 30 predictors variables. Effect estimation is carried out by means of Cox regression (which uses the propensity score in various ways [stratification, covariate adjustments etc]). There are a large number of patients, and on average 2â€“7% missing for each variable (of which there are 30 included in the prop. score).</p>

<p>Thus, I have a large data set with a substantial amount of missing data (at least in terms of complete cases) which is why I use multiple imputation - 5 complete data sets are imputed. Now the question is what to do with the muliply imputed data sets; which one of the strategies below should I prefer?</p>

<p><strong>1.</strong> Calculate one average propensity score for each individual using the 5 separate data sets. That way, each individual will have one propensity score, which is the average from the n complete data sets. Then do the Cox regression..</p>

<p><strong>2.</strong> Analyze each separate multiply imputed data set (with Cox regression), and then pool the 5 hazard ratio estimates to one hazard ratio.</p>

<p>The second method appears to be used more often, but is it better/worse?</p>

<p>Any thoughts about this?</p>
"
"0.131306432859723","0.148969892882781","142693","<p><strong><em>Is the following a reasonable illustration of the OVB problem?</em></strong></p>

<p>We build up fictional data around the regression line:</p>

<p>$$y = 7.2 + 2.3 \, x_1 + 0.1 \, x_2 + 1.5 \, x_3 + 0.013 \, x_4 + eps$$</p>

<p>by using this function:</p>

<pre><code>correlatedValue = function(x, r){
  r2 = r**2
  ve = 1 - r2
  SD = sqrt(ve)
  e  = rnorm(length(x), mean = 0, sd = SD)
  y  = r * x + e
}
</code></pre>

<p>-thank you, @gung for this post:
<a href=""http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of"">How to generate correlated random numbers (given means, variances and degree of correlation)?</a></p>

<p>And the following function, which generates four variables (<strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong> and <strong><em>x4</em></strong>) as well as noise (<strong><em>eps</em></strong>). <strong><em>x1</em></strong> and <strong><em>x3</em></strong> are sample from normal distributions; <strong><em>x2</em></strong> is extracted from a uniform; and <strong><em>x4</em></strong> from a Poisson.</p>

<pre><code>variables &lt;- function(){
x &lt;- rnorm(1000)
x1 &lt;- 50 + 15 * x
x3 &lt;- 28 + 11 * correlatedValue(x = x, r = 0.6)
x2 &lt;- runif(1000, 0, 100)
x4 &lt;- rpois(1000,50)
eps &lt;- rnorm(1000,5, 7)
y = 7.2 + 2.3 * x1 + 0.001 * x2 + 1.5 * x3 + 0.013 * x4 + eps
dat &lt;- as.data.frame(cbind(y, x1, x2, x3, x4))
c &lt;- as.numeric(coef(lm(y ~ x2 + x3 + x4, dat))[3])
d &lt;- as.numeric(coef(lm(y ~ x1 + x2 + x3 + x4, dat))[4])
c(c,d)
}
</code></pre>

<p><strong><em>x1</em></strong> and <strong><em>x3</em></strong> are highly influential on <strong><em>y</em></strong> and are correlated with each other, setting the values up to observe <strong><em>OVB</em></strong>. <strong><em>x2</em></strong> and <strong><em>x4</em></strong> are less influential.</p>

<p>Here is the plotting of <strong><em>y</em></strong> against <strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong>  and <strong><em>x4</em></strong>, and <strong><em>x1</em></strong> over <strong><em>x3</em></strong> with added regression lines:</p>

<p><img src=""http://i.stack.imgur.com/I4u0S.png"" alt=""enter image description here""></p>

<p>And following is the variance-covariance matrix:</p>

<pre><code>             y           x1           x2         x3          x4
y   1.00000000  0.944410945  0.014421682 0.77571067 -0.01463981
x1  0.94441094  1.000000000 -0.001726526 0.56504020 -0.03562991
x2  0.01442168 -0.001726526  1.000000000 0.03537959  0.02253922
x3  0.77571067  0.565040198  0.035379590 1.00000000  0.02573827
x4 -0.01463981 -0.035629906  0.022539218 0.02573827  1.00000000
</code></pre>

<p>Predictably, the regression including all variables shows similar coefficients to the initial equation:</p>

<pre><code>coef(lm(y~.,dat))[2:5]
         x1          x2          x3          x4 
2.253353226 0.004899445 1.547915198 0.017710038 
</code></pre>

<p>Wrapping up, a quick simulation is carried out to obtain the mean of the <strong><em>x3</em></strong> coefficient in 1,000 simulations <em>WITHOUT</em> including <strong><em>x1</em></strong> (""coef_x3"") and then <em>WITH</em> <strong><em>x1</em></strong> (""coef_x3_full""):</p>

<pre><code>coef_x3 &lt;- NULL
coef_x3_full &lt;- NULL
for (i in 1:1000){
  coef_x3[i] = variables()[1]
  coef_x3_full[i] = variables()[2]
}
mean(coef_x3)
mean(coef_x3_full)
</code></pre>

<p>obtaining a coefficient for <strong><em>x3</em></strong> of <strong>3.383</strong> when <strong><em>x1</em></strong> is excluded versus a coefficient for <strong><em>x3</em></strong> of <strong>1.502</strong> when included. So when <strong><em>x1</em></strong> is included we have an unbiased estimation of the true <strong><em>x3</em></strong> coefficient (<strong><em>1.5</em></strong>), whereas the estimation is biased when we exclude <strong><em>x1</em></strong>.</p>
"
"0.138409133089567","0.134595475514541","143155","<p>I am trying to calculate multiple regression in R without intercept.</p>

<p>My data is as follow:</p>

<pre><code> y=c(60.323,61.122,60.171,61.187,63.221,63.639,64.989,63.761,66.019,67.857,68.169,66.513,68.655,69.564,69.331,70.551)
    x1=c(83,88.5,88.2,89.5,96.2,98.1,99,100,101.2,104.6,108.4,110.8,112.6,114.2,115.7,116.9)
    x2=c(107.608,108.632,109.773,110.929,112.075,113.27,115.094,116.219,117.388,118.734,120.445,121.95,123.366,125.368,127.852,130.081)
</code></pre>

<p>In this case, (I believe?) I am getting the coefficients WITH intercept:</p>

<pre><code>lm(formula = y ~ x1 + x2)
</code></pre>

<p>I would like to get the coefficients WITHOUT intercept. I tried this:</p>

<pre><code>lm(formula = y ~ x1 + x2 -1)
</code></pre>

<p>Is this correct? If so, my question would be: How can I calculate WITHOUT intercept without changing the x values (on the right side of the tilda), but by changing something on the y value (on the left side of the tilda). For instance:</p>

<pre><code>lm(formula = y -1 ~ x1 + x2)
</code></pre>

<p>Gets a different (and presumably incorrect coefficient estimation).</p>

<p>I know your question is... why do you have to only change the y values? The reason is because I am writing code in C to do this, and I do not want to change the dimensions of X by adding a -1 at the end because that would require dynamic array allocation, which is very meticulous for me.</p>
"
"0.214422506967559","0.208514414057075","143399","<p>I am trying to do some survival analysis in R and as a starting point, I want to make sure I can replicate a previous analysis. I notice differences and I will demonstrate them here. I feel like there is a daft explanation the user community can provide.</p>

<p>Let's start by using the ovarian dataset in R. We will fit a weibull distribution with residual disease and ECOG performance status as covariates. Then we will print the output using proportional hazards specification to match Stata's HR output.</p>

<pre><code>require(survival)
require(flexsurvreg)
require(dplyr)
attach(ovarian)
ovarian &lt;- ovarian %&gt;% mutate(resid.ds=resid.ds-1, ecog.ps=ecog.ps-1, futime=futime/365.25) # Make it 0,1
write.dta(ovarian %&gt;% mutate(resid.ds=resid.ds+1, ecog.ps=ecog.ps+1), ""data/ovarian.dta"") # Write dta
s.weib &lt;- flexsurvreg(Surv(futime, fustat) ~ age + resid.ds + ecog.ps, data=ovarian, dist=""weibull"") # Fit weibull

# Function to convert AFT to PH
flexsurvPHcoef &lt;- function(x) return(c(exp(x$coef[-(1:2)]*(-1)*exp(x$coef[""shape""])), exp(x$coef[""shape""]), exp(-x$coef[""scale""])))
flexsurvPHcoef(s.weib)
     age     resid.ds      ecog.ps        shape        scale 
1.150309872 2.702038142 1.060599568 1.752446996 0.002799146 
</code></pre>

<hr>

<p>Now let's compare to Stata.</p>

<pre><code>quietly stset futime, f(fustat)
streg age i.resid_ds i.ecog_ps, d(weib)
Weibull regression -- log relative-hazard form 

No. of subjects =           26                     Number of obs   =        26
No. of failures =           12
Time at risk    =  42.67761807
                                                   LR chi2(3)      =     17.88
Log likelihood  =   -20.828884                     Prob &gt; chi2     =    0.0005

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         age |    1.15031   .0515502     3.12   0.002     1.053583    1.255916
  2.resid_ds |   2.702038    2.00098     1.34   0.180     .6329054    11.53571
   2.ecog_ps |     1.0606   .6620784     0.09   0.925     .3120252    3.605066
       _cons |   .0000336   .0000906    -3.82   0.000     1.69e-07    .0066602
-------------+----------------------------------------------------------------
       /ln_p |   .5610131    .238929     2.35   0.019     .0927209    1.029305
-------------+----------------------------------------------------------------
           p |   1.752447   .4187103                      1.097156     2.79912
         1/p |   .5706307   .1363402                      .3572551    .9114478
------------------------------------------------------------------------------
</code></pre>

<p>We can see the resid.ds and ecog.ps are the same. As well, the shape. But the scale is off. </p>

<p><strong>So my question is, any thoughts on why only the scale parameter is different?</strong></p>

<hr>

<p>Let's move on to estimation. flexsurvreg has an interesting ability to predict at multiple time points. Let's assume a woman is 45, has residual disease and ECOG is 0.</p>

<pre><code>summary(s.weib, newdata=data.frame(age=45, resid.ds=1, ecog.ps=0), t=c(1:5))
age=45, resid.ds=1, ecog.ps=0 
  time       est         lcl       ucl
1    1 0.9517266 0.807744496 0.9930684
2    2 0.8464501 0.488174229 0.9681961
3    3 0.7122949 0.193976895 0.9285058
4    4 0.5702529 0.038982054 0.8762493
5    5 0.4358519 0.002925942 0.8097636
</code></pre>

<p>Not the most precise. Anyways, how does this compare to how weibull is parameterized (PH). From Stata's manual:
$$
S = exp(-exp(x{B}){t}^p)
$$</p>

<p>No we have to use the log scale coefficients. But let's go ahead and try and predict this manually.</p>

<pre><code>p.weib &lt;- function(cons, age, resid, t, p) return(exp(-exp(cons+age*45+resid)*t^p))
coef &lt;- flexsurvPHcoef(s.weib)
data.frame(time=1:5) %&gt;% mutate(S=p.weib(log(coef[""scale""]), log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.01617
2    2 0.00000
3    3 0.00000
4    4 0.00000
5    5 0.00000
</code></pre>

<p>That obviously didn't really work out. What happens if we substitute the scale from Stata (-10.30166)?</p>

<pre><code>data.frame(time=1:5) %&gt;% mutate(S=p.weib(-10.30166, log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.95173
2    2 0.84645
3    3 0.71230
4    4 0.57025
5    5 0.43585
</code></pre>

<p>It's just as flexsurvreg predicted. So now that I write this, maybe I've transformed the AFT scale incorrectly. Back to my original question, why are the scales different?</p>

<p>Finally, some aside questions. I couldn't reproduce my actual data problem. I can't really put up that much data here. Again, <code>summary.flexsurvreg</code> gives me predicted estimates that are not the same as the <code>p.weib</code>. But, when I substitute the scale from Stata for p.weib, I get estimates different to the original summary.flexsurvreg, albeit much closer than with the scale from my log(coef) of PH. Any thoughts?</p>
"
"0.151619608715781","0.147441956154897","143615","<p>I estimate a multilevel ordinal logistic regression models in Stata and R, and receive different estimators for the variance and the covariance of the latent variable of the higher level.
Among other data, I use this Stata datafile (from Stata help for meologit):
<a href=""http://www.stata-press.com/data/r13/tvsfpors.dta"" rel=""nofollow"">http://www.stata-press.com/data/r13/tvsfpors.dta</a>
I estimate (among others) this model:</p>

<pre><code># R formula syntax
library(ordinal)
library(readstata13)
mydata &lt;- read.dta13(file = ""http://www.stata-press.com/data/r13/tvsfpors.dta"",  
                     convert.factors = FALSE)
res &lt;- clmm(as.factor(thk) ~ prethk + cc + tv + (1 | school), mydata, nAGQ  =7)
summary(res)

* Stata equation
webuse tvsfpors
meologit thk prethk cc tv || school:
</code></pre>

<p>The columns in the variance-covariance-matrix for the variance of the latent higher-level variable seem to differ systematically. The covariances differ by an identical constant, and the variances (of the variance) themselves by the squared term of the constant. The constant differ dependent on the data and model structure.</p>

<p>Does anyone know the source of this difference?</p>

<p>This is the variance-covariance-matrix for the estimation described above for R:
Here is the var-cov-matrix for R</p>

<pre><code>&gt; vcov(m1)
                1|2          2|3          3|4        prethk            cc            tv           ST1
1|2     0.023567239  0.022089030  0.022112907  0.0030638974  0.0105995218  0.0111725311 -0.0043760134
2|3     0.022089030  0.023866162  0.023611332  0.0033154896  0.0111666464  0.0111977570 -0.0031374135
3|4     0.022112907  0.023611332  0.026259339  0.0036017005  0.0116806803  0.0112381854 -0.0018698361
prethk  0.003063897  0.003315490  0.003601701  0.0015113830  0.0003659838  0.0001657595 -0.0009479396
cc      0.010599522  0.011166646  0.011680680  0.0003659838  0.0216625428 -0.0002137013  0.0004195043
tv      0.011172531  0.011197757  0.011238185  0.0001657595 -0.0002137013  0.0214436765 -0.0012672853
ST1    -0.004376013 -0.003137414 -0.001869836 -0.0009479396  0.0004195043 -0.0012672853  0.0642318747
</code></pre>

<p>This is the analogue in Stata</p>

<pre><code>. matrix list e(V), nohalf

symmetric e(V)[7,7]
                                   thk:          thk:          thk:         cut1:         cut2:         cut3:  var(_cons~):
                                prethk            cc            tv         _cons         _cons         _cons         _cons
              thk:prethk     .00151138     .00036598     .00016576      .0030639     .00331549      .0036017    -.00015693
                  thk:cc     .00036598     .02166253    -.00021371     .01059951     .01116663     .01168067     .00006945
                  thk:tv     .00016576    -.00021371     .02144367     .01117254     .01119776     .01123818    -.00020983
              cut1:_cons      .0030639     .01059951     .01117254     .02356724     .02208902      .0221129    -.00072444
              cut2:_cons     .00331549     .01116663     .01119776     .02208902     .02386615     .02361132     -.0005194
              cut3:_cons      .0036017     .01168067     .01123818      .0221129     .02361132     .02625932    -.00030956
var(_cons[school]):_cons    -.00015693     .00006945    -.00020983    -.00072444     -.0005194    -.00030956     .00176027
</code></pre>

<p>Note that the column and row order is different. My question points at the diffference between the rows/columns of the variable var(_cons[school]) in the Stata matrix and ST1 in the R matrix.</p>
"
"0.107211253483779","0.104257207028537","144076","<p>I have a bunch of data that I fit a linear regression to, and now I need to find the variance of my slope. Is there an analytical way to get this?</p>

<p>If an example is necessary, consider this my data in R:</p>

<pre><code>x &lt;- c(1:6)
y &lt;- c(18, 14, 15, 12, 7, 6)
lm(y ~ x)$coefficients
</code></pre>

<p>So I have a slope estimate of <code>-2.4</code>, but I want to know the variance of that estimate.</p>

<p>After looking at previous questions, I've seen a few equations for estimating the slope parameter, but I'm a little confused about what the differences between equations are and what approach is valid for my problem.</p>

<p>For example, the answers in <a href=""http://stats.stackexchange.com/questions/122406/the-variance-of-linear-regression-estimator-beta-1"">this question</a> say that $\newcommand{\Var}{\rm Var}\newcommand{\slope}{\rm slope}\Var[\slope] = \frac{V[Y]}{\sum\left(\frac{x_i-\bar{x}}{\sum(x_i-\bar{x})^2}\right)}$.</p>

<p><a href=""http://stats.stackexchange.com/questions/12186/expected-value-and-variance-of-estimation-of-slope-parameter-beta-1-in-simple"">This question</a> says that $\Var[\slope] = \frac{V[Y]}{\sum(x_i-\bar{x})^2}$.</p>

<p>And if I look at the output in R (as a ""check"" mechanism), I'm given two other ways I could potentially calculate the slope variance (one using the standard error, another given the covariance matrix). I feel like I'm missing something key because all these estimates give me similar (but not the same) answer.</p>
"
"0.123796892118035","0.120385853085769","145053","<p>I have a linear regression model:</p>

<pre><code>model &lt;- lm(data=df, var1~var2+var3+var4+var5+var6+var7)
</code></pre>

<p>Hypothesis about absence of heteroscedasticity is rejected as Breusch-Pagan test gives small p-value:</p>

<pre><code>bptest(model)$p.value
#BP 
#1.014577e-06
</code></pre>

<p>But when I use robust estimations for parameters of the model:</p>

<pre><code>library(""sandwich"")
coeftest(model, vcov. = vcovHC(model))
</code></pre>

<p>... for all parameters the value <code>Pr(&gt;|t|)</code> is reducing. So the coefficients seem to become more significant although robust estimators usualy do vice versa.</p>

<p>Would you explain the matter of why this happens? Or robust estimators don't reduce statistical significance of parameters for some special cases? Would you mention these cases?</p>

<p>Thank you.</p>
"
"NaN","NaN","145402","<p>I have been trying logistic regression to fit the data and get an estimation of the response rate, but the power of the model is quite limited. The area under the ROC curve is always around 0.6. </p>

<p>I am just wondering whether there are other models to predict binary response variables that I can try? Thanks!</p>
"
"0.163767894813506","0.159255514317652","146665","<p>The answer to <a href=""http://stats.stackexchange.com/questions/41129/interpreting-coefficients-of-an-interaction-between-categorical-and-continuous-v/41292#41292"">Interpreting coefficients of an interaction between categorical and continuous variable</a> contains a phrase that seems to have some significant impact on how coefficients are interpreted in a multiple-regression when a factor is introduced: </p>

<blockquote>
  <p>If treatment contrasts for a categorical variable are present in a model, the estimation of further effects is based on the reference level of the categorical variable. [..]</p>
  
  <p>note that the estimation of the coefficients is based on the references categories of the factors (if treatment contrasts are employed). In this case the effects hold for $race = white$, $sex = male$, and $educa = 1$. They do not test an overall influence of the numeric variables irrespective of the levels of the factors.</p>
</blockquote>

<p><strong>Question:</strong> How does including a factor into a multiple regression affect the interpretation of the <em>other</em> coefficients (whether numeric predictors or interactions)?</p>

<p>Consider this example from <a href=""http://www.jstatsoft.org/v08/i15"" rel=""nofollow"">Fox 2003</a>: </p>

<pre><code>require(effects)
require(lmtest)
Arrests$year &lt;- as.factor(Arrests$year)
arrests.mod &lt;- glm(released ~ employed + citizen + checks
                         + colour*year + colour*age,
                         family=binomial, data=Arrests)
</code></pre>

<p>Which yields: </p>

<pre><code>&gt; coeftest(arrests.mod)

z test of coefficients:

                       Estimate Std. Error  z value  Pr(&gt;|z|)    
(Intercept)           0.3444334  0.3100749   1.1108 0.2666514    
employedYes           0.7350645  0.0847701   8.6713 &lt; 2.2e-16 ***
citizenYes            0.5859841  0.1137717   5.1505 2.598e-07 ***
checks               -0.3666425  0.0260322 -14.0842 &lt; 2.2e-16 ***
colourWhite           1.2125167  0.3497751   3.4666 0.0005272 ***
year1998             -0.4311794  0.2603589  -1.6561 0.0977023 .  
year1999             -0.0944343  0.2615447  -0.3611 0.7180519    
year2000             -0.0108975  0.2592073  -0.0420 0.9664655    
year2001              0.2430630  0.2630151   0.9241 0.3554129    
year2002              0.2129549  0.3532786   0.6028 0.5466444    
age                   0.0287279  0.0086191   3.3330 0.0008590 ***
colourWhite:year1998  0.6519565  0.3134898   2.0797 0.0375555 *  
colourWhite:year1999  0.1559504  0.3070430   0.5079 0.6115161    
colourWhite:year2000  0.2957537  0.3062034   0.9659 0.3341076    
colourWhite:year2001 -0.3805413  0.3040538  -1.2516 0.2107305    
colourWhite:year2002 -0.6173178  0.4192551  -1.4724 0.1409086    
colourWhite:age      -0.0373729  0.0102003  -3.6639 0.0002484 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Given that we have <code>employed={no,yes}</code> and <code>citizen={no,yes}</code> and the factor <code>year={1997,..,2002}</code> in the model... </p>

<p>Does this imply that the coefficient <code>colourWhite:age = -0.0373729</code> is <em>strictly</em> limited to describing <em>only</em> the interaction between colour and age for people who are unemployed, non-citizen and arrested in 1997? </p>
"
"0.14442970747104","0.180578779628654","154588","<p>I'm trying to build a covariance-based structural equation model (SEM) using both reflective and formative specifications of latent variables. I use the <code>sem</code> function in the <code>lavaan</code> package for estimation (R version 3.1.3, lavaan version 0.5-18). But estimates turn always out to be zero which is unreasonable.</p>

<p>The lavaan model syntax uses <code>=~</code> for reflective specification of latent variables, <code>&lt;~</code> for formative specification of latent variables, and <code>~</code> for regressions (<a href=""http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax"" rel=""nofollow"">http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax</a>). Here is a simple working example with only reflective specifications (it is a simplified version of the example provided at <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">http://lavaan.ugent.be/tutorial/sem.html</a> and by <code>example(sem)</code>)</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 =~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>Now assume that based on prior theory I would know that dem60 is a formative construct composed of y1 and y2. Thus I change the specification from <code>=~</code> to <code>&lt;~</code> and obtain the following code</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 &lt;~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>The estimates for both y1 and y2 turn out to be zero. Analogously, the regression effect of ind60 on dem60 turns out to be zero. What do I need to change to get a meaningful result?</p>

<p>Several websites and blogs suggested the following modifications:
(1) Fix one parameter in the formative construct, i.e. <code>dem60 &lt;~ 1*y1 + y2</code>.
(2) Allow for covariance of the manifest indicators, i.e. <code>y1 ~~ y2</code>. 
(3) Fix the variance of the formative construct, i.e. <code>dem60 ~~ 1</code>.
(4) Free the variance of the formative construct, i.e. <code>dem60 ~~ NA*dem60</code>. 
None of these are working. Again: What do I need to change to get a meaningful result?</p>
"
"0.0875376219064817","0.0851256530758749","156034","<p>I am dealing with a heteroscedastic censored dataset. I tried to use the survival analysis package in R to estimate a linear model for it. So before doing that, I conducted a simulation study, where I generate a sample for x:
$x  \sim Unif[-2,2]$
and y:
$y \sim 1+0.3x+0.6(1-0.3x)\epsilon $ where $\epsilon\sim N(0,1)$.
y is censored as $y=y$ if $y&lt;x$; $y=x$ otherwise.</p>

<p>Then I use the survreg function in R to estimate a linear model for it.</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6*(1-0.3*x)</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>The result is really poor, the estimate is around(intercept = 0.5, slope = 0.6) and it is very stable no matter what initial points I gave.</p>

<p>The result does not get significant improved even if I feed the true weight to it:</p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian', 'weights=1/(1-0.3x)^2')</code></p>

<p>But the estimation is great when no heteroscedasticity is presented:</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>I also tried quantile regression for censored data with tau=0.5. (basically LAD regression which is proposed by J.Powell 1984). The result is still poor under this parameter setting.</p>

<p>Is there any good way to get a consistent estimation for heteroscedastic censored data?</p>

<p>Any suggestion on package, software or papers are welcome.</p>

<p>Thanks a lot.</p>
"
"0.138409133089567","0.134595475514541","156275","<p>I have two paired samples following normal distributions N(0, $\sigma_1^2$) and N(0, $\sigma_2^2$). Samples represent estimation errors (residuals) of two linear regression models used to predict the same response variable using two different methods/independent variables. I have 30 pairs of residuals, so I would like to apply Wilcoxon signed-rank test to check whether means of absolute values or relative errors are different. Since absolute values do not follow normal distribution, I cannot use t-test or something similar.</p>

<p>I would like to find type II error and statistic power of Wilcoxon signed-rank test.
Is there some R function (or any other tool) that can be used? I have found a number of functions for testing the power of tests here <a href=""http://www.statmethods.net/stats/power.html"" rel=""nofollow"">http://www.statmethods.net/stats/power.html</a>  but Iâ€™m not sure could they be applied on Wilcoxon signed-rank test. If there is no built-in function is there some other tool or algorithm to manually calculate error?   </p>
"
"0.165062522824046","0.180578779628654","156619","<p>I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.</p>

<p><strong>What I have and what I do</strong></p>

<ul>
<li>my data consists out of real estates with following charcteristics: <code>price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.</code></li>
<li>I run a robust regression without intercept <code>lmRob(price ~ . -1)</code></li>
</ul>

<p><strong>What I want</strong></p>

<ul>
<li>a model with which I can predict the price of real estates, but which are not in the used data set</li>
<li>also it would be nice to have some constraints on the coefficients</li>
</ul>

<p><strong>Problems</strong></p>

<ul>
<li>very often I get bad values for the coefficients <code>ex: bathroomCount = -80000</code>. it's not possible that with a additive bathroom , the price of the house will sink with <code>80.000â‚¬</code></li>
<li><p>also I tried to use the function <code>pcls</code> in order to put some constraints on the coefficients, but this method gave very bad results. In the plot <code>Y = price</code> and <code>X = livingArea</code>. as you can see, the regression line isn't correct.
<img src=""http://i.stack.imgur.com/7PHp1.png"" alt=""enter image description here""></p>

<ul>
<li>another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do it</li>
<li>also I tried to use different regression methods <code>lm, lmrob, ltsReg, MARS</code>, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)</li>
<li>I think that the big number of dummy variables damages a little bit the regression</li>
</ul></li>
</ul>

<p>Is my approach false?</p>

<p>Does someone have some hints, tricks for me? (<em>I'm not a statistician</em>)</p>

<p><strong>[UPDATE]</strong></p>

<p><img src=""http://i.stack.imgur.com/a2kLe.png"" alt=""price ~ livingArea""></p>

<p>This is how the plotted data looks like. LivingArea is the only non-dummy variable.</p>

<p><strong>[UPDATE 2]</strong></p>

<pre><code>y = bX 

     means

y = b_0*X_0 + b_1*X_1 + ... + b_k*X_k

     which is an equation system like this:

y[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]
.
.
.
y[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]
</code></pre>

<p>Did I got it right? </p>

<p>If so, isn't possible to add some inequality constraints equation to it. example:</p>

<pre><code>b_0 &gt;= 2000
b_2 &lt;= b_0/2
</code></pre>

<p><strong>[UPDATE 3]</strong></p>

<p>I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0mÂ².
<img src=""http://i.stack.imgur.com/LYPB0.png"" alt=""enter image description here"">
but it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?</p>
"
"0.138409133089567","0.134595475514541","158492","<p>I have fitted a (Cragg's) truncated normal hurdle model over a dataset in which the dependent variable is either zero or positive. The model consists of two parts: a probit which estimates the probability of the value being zero and a truncated regression which is estimated over the subsample of positive values of the dependent variable.</p>

<p>The output of the estimation (using package <code>mhurdle</code> in R) consists of two columns: one gives the probability of y being 0 and the other gives the estimated value for an uncensored observation (let's call this y*).</p>

<p>Now I would like to use these results for prediction, but instead of probabilities and estimated values of the uncensored y I would like to have the estimated values of the dependent variable, including some zeros (or almost zeros). Should I multiply the probability of y NOT being zero by the value of y*? Or should I take all observations for which P(y=0) > 0.5 to be zero and all the others to be equal to y*?</p>

<p>Sorry if the question is trivial but I'm fairly new to statistics and I have not been able to find the answer so far.</p>
"
"0.195740073171568","0.19034674690672","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.107211253483779","0.104257207028537","164369","<p>I intend to use this: </p>

<pre><code>&gt;sys_ols=systemfit(A ~ B +C+D,data = inEUdata, method = ""OLS"")
&gt;sys_iv=systemfit( A ~ B+C+D, data = inEUdata, method = ""2SLS"",inst = ~ E)
&gt;hausman.systemfit(sys_iv, sys_ols)
</code></pre>

<p>Result[Error]: </p>

<pre><code>Error in crossprod(result$q, solve(result$qVar, result$q)) : error in evaluating the argument 'y' in selecting a method for function 'crossprod': Error in solve.default(result$qVar, result$q) : system is computationally singular: reciprocal condition number = 4.21018e-25
</code></pre>

<p>I am using the following code presently : </p>

<pre><code>&gt;sys_ols=systemfit(A ~ B, data = inEUdata, method = ""OLS"") 
&gt;sys_iv=systemfit(A ~ &gt;B, data = inEUdata, method = ""2SLS"",inst = ~ E)
&gt;hausman.systemfit(sys_iv, sys_ols)
</code></pre>

<p>Result:</p>

<pre><code>Hausman specification test for consistency of the 3SLS estimation 

data:  inEUdata
Hausman = 1.9046, df = 2, p-value = 0.3859
</code></pre>

<p>Please suggest me which code is most appropriate for the endogeneity test for multiple regression </p>
"
"0.205293920683813","0.199637352376173","176622","<p>In many papers in the social sciences, missing data are handled by <em>direct</em> or <em>full information</em> maximum likelihood estimation (FIML). Unfortunately this is almost always done with closed source software.  </p>

<p>To get an idea how FIML works, I tried to implement it myself in R. </p>

<p>In case of a multivariate normal distribution the loglikelihood for a single observation is given by (according to Enders, 2001, p. 134):</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log|\Sigma_i|-\frac{1}{2}(x_{i}-\mu_{i})^{'}\Sigma^{-1}(x_{i}-\mu_{i})
$$</p>

<p>Where $K_{i}$ is the numer of observed variables for obesrvation $i$. </p>

<p>In case of a simple linear regression model with $N$ observations and assumed homoskedascity the formula should reduce to:</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log(\sigma^{2})-\frac{1}{2\sigma^2}(y_{i}-X_{i}\beta_{i})^{2}
$$</p>



<pre><code>set.seed(42)
x &lt;- matrix(rnorm(1000), nrow=500) ; x &lt;- cbind(1,x)
y &lt;- x %*% c(2,1,3) + rnorm(500, mean = 0, sd = 1)
z.full &lt;- cbind(y,x)

# MCAR predictors
x[sample.int(n = 500, size = 50),2] &lt;- NA
x[sample.int(n = 500, size = 50),3] &lt;- NA
z.miss &lt;- cbind(y,x)

llog.single &lt;- function(z, beta, sigma){
    y &lt;- z[1]
    x &lt;- z[-1]
    idx &lt;- !is.na(x)  # = K_{i}
    return(sum(idx) + dnorm(y, mean = x[idx]%*%beta[idx], sd = sqrt(sigma), log = TRUE))
}

loglikelihood &lt;- function(y, x, theta) {
    p &lt;- ncol(x)
    beta  &lt;- theta[1:p]
    sigma &lt;- theta[p+1]
    return(-sum(apply(cbind(y,x), 1, llog.single, beta = beta, sigma = sigma)))
}

# Full information maximum likelihood 
optim(par = c(0,0,0,1), fn = loglikelihood, method = ""BFGS"", x=z.mis[,-1], y=z.miss[,1])$par
# OLS on full dataset
lm(V1 ~ V3 + V4, data = as.data.frame(z.full))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What should I do, if there is missing data in the outcome $Y_{i}$?</em></p>

<p>From my current point of view I would just ignore cases with missing data in $Y$. Is this correct or is there maybe a better way?</p>

<hr>

<p>Enders, C. K. (2001). A primer on maximum likelihood algorithms available for use with missing data. Structural Equation Modeling, 8(1), 128-141.</p>
"
"0.138409133089567","0.134595475514541","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.137018051220097","0.19034674690672","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.123796892118035","0.120385853085769","181489","<p>I have to estimate a number of regressions where a lot of autcorrelation is present. Now, for historical reasons, this autocorrelation is resolved using an iterative Prais-Winsten estimation (a modification of the Cochraneâ€“Orcutt estimation). I have found some R-code which performs this procedure:</p>

<pre><code>    prais.winsten.lm &lt;- function(mod){
    X &lt;- model.matrix(mod)
    y &lt;- model.response(model.frame(mod))
    e &lt;- residuals(mod)
 n &lt;- length(e)
 names &lt;- colnames(X)
 rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
 y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
 X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
 mod &lt;- lm(y ~ X - 1)
 result &lt;- list()
 result$coefficients &lt;- coef(mod)
     names(result$coefficients) &lt;- names
 summary &lt;- summary(mod, corr = F)
 result$cov &lt;- (summary$sigma^2) * summary$cov.unscaled
     dimnames(result$cov) &lt;- list(names, names)
 result$sigma &lt;- summary$sigma
 result$rho &lt;- rho
 class(result) &lt;- 'prais.winsten'
 result
 }
</code></pre>

<p>Now, this code works fine when all the other regressors are exogeneous. But, in my case, a part of X is endogeneous turning the standard ols regression performed in the above code not usable.</p>

<p>I was thinking about modifying the above code into the following:</p>

<pre><code>  prais.winsten.plm &lt;- function(mod){
  X &lt;- model.matrix(mod,component=""projected"")
  Z &lt;- model.matrix(mod,component=""instruments"")
  y &lt;- model.response(model.frame(mod))
  e &lt;- residuals(mod)
  n &lt;- length(e)
  names &lt;- colnames(X)
  rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
  y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
  X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
  Z &lt;- rbind(Z[1,] * (1 - rho^2)^0.5, Z[2:n,] - rho * Z[1:(n-1),])
  mod &lt;- ivreg(y ~ X -1|Z)
  result &lt;- list()
  result$coefficients &lt;- coef(mod)
      names(result$coefficients) &lt;- names
  summary &lt;- summary(mod, corr = F)
  cov &lt;- (summary$sigma^2) * summary$cov.unscaled
  result$se&lt;- sqrt(diag(cov))
      dimnames(cov) &lt;- list(names, names)
      result$sigma &lt;- summary$sigma
      result$rho &lt;- rho
  class(result) &lt;- 'prais.winsten'
  result
  }
</code></pre>

<p>I have however no idea if such an approach is correct, especially since I found no similar ways in dealing with this problem.</p>
"
"0.214422506967559","0.208514414057075","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0.0928476690885259","0.120385853085769","183206","<p>Simply put, I'd like to know how the plm package in R calculates the residuals of a random-effect regression.</p>

<p>I ask this because i'm getting some ""weird"" outputs. Let-me reproduce them here using the Grunfeld data for four firms, like Gujarati in his Basic Econometrics do:</p>

<pre><code>require(plm)
require(foreign)

Grunfeld&lt;-read.dta(""Data.dta"")
Grunfeld&lt;-pdata.frame(Grunfeld,index = c(""id"",""t""))

grun.re &lt;- plm(Y~X2+X3,data=Grunfeld,model=""random"",index=""id"")

#Means by id
X2M&lt;-tapply(Grunfeld$X2,Grunfeld$id,FUN = mean)
X3M&lt;-tapply(Grunfeld$X3,Grunfeld$id,FUN = mean)
YM&lt;-tapply(Grunfeld$Y,Grunfeld$id,FUN = mean)

#Random Effect: Fit the model and the calculate residuals ""by hand""
fit.re&lt;-grun.re$coefficients[1]+grun.re$coefficients[2]*Grunfeld$X2+grun.re$coefficients[3]*Grunfeld$X3
    calcResid.re&lt;-(Grunfeld$Y-fit.re)

#Random Effect:
head(cbind(grun.re$residuals,Grunfeld[,11:13],calcResid.re))

  grun.re$residuals   alphaRE       eRE        uRE calcResid.re
1         99.395803 -169.9282 116.23154  -53.69666    -53.69666
2         18.023715 -169.9282  34.85946 -135.06874   -135.06874
3        -39.256625 -169.9282 -22.42089 -192.34909   -192.34908
4         -2.857048 -169.9282  13.97869 -155.94951   -155.94951
5        -28.334107 -169.9282 -11.49837 -181.42656   -181.42656
6          6.475226 -169.9282  23.31096 -146.61723   -146.61723
</code></pre>

<p>In this table, uRE is the overall residual of the regression provided by Stata (which is identical to Gretl's) and calcResid.re is the manually calculated residuals from the fitted model. So, Stata, Gretl and I did the same. But what plm package do?</p>

<p>We can se that calcResid.re and uRE are equals. But the residuals provided by the plm estimation (grun.re$residuals) completely differs.</p>

<p>Here is a link to the dataset and results: <a href=""https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0</a></p>
"
"0.138409133089567","0.134595475514541","183461","<p>I'm working on a regression that predicts the number of births a female will have based on roughly ~15 variables. Therefore I have a discrete dependent variable. I was doing some research and it seems unfavorable to use a linear model for this estimation, so I then turned to a logit model. If I use the logit model I cannot predicts number of births, instead I can only get a probability of whether they will have a child or not. I was reading about multinomial regressions, but with the number of variables I have this could get extremely cumbersome very quickly, I tested it with only 2 variables, age and church attendance and my output was extremely long. I didn't see any significance levels either. I also ran into problems when I added all my variables,</p>

<pre><code>Error in nnet.default(X, Y, w, mask = mask, size = 0, skip = TRUE, softmax = TRUE,  : too many (1044) weights
</code></pre>

<p>I don't know how to adjust the weights in R, I tried but I kept getting errors.</p>

<p>So what do you think would be the best course of action? The simpler logit model or the multinomial model? My lack of knowledge with the multinom is making we lean toward the former option. </p>
"
"0.240201252859224","0.248181794051358","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.14442970747104","0.180578779628654","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"0.175075243812963","0.17025130615175","197001","<p>I am trying to follow the procedure offered by <a href=""http://www.jstor.org/stable/2082979?seq=1#page_scan_tab_contents"" rel=""nofollow"">Beck and Katz 1995</a> in a way that I also have a TSCS data with $T=100$ (time dimension) and $N=12$ (unit dimension). My data is not balanced, which means that for some time periods, not all units have observations. </p>

<p>I am using R, and I found a <code>pcse</code> package that does what I need. It calculates panel corrected standard errors which accounts for contemporaneous correlation of errors across units and unit level heteroskedasity of errors. However, the steps I have to take to calculate panel robust standard errors for this type of regression start with the need to correct for serial correlation of errors, if I understand it well. Particularly, that is what is recommended in <code>pcse</code> package documentation:</p>

<p><a href=""http://i.stack.imgur.com/GNowz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GNowz.png"" alt=""enter image description here""></a></p>

<p>So, I am lost trying to understand what I need to do. My options how I see them:</p>

<ol>
<li>Run simple OLS regression on my pooled panel data. </li>
<li><p>Test for serial correlation of error term using Durbinâ€“Watson test and examining ACF/PACF. In most cases, I will have AR(1) in errors. </p>

<ul>
<li>Either compute clustered standard errors - it should account for the fact that errors should be clustered on the unit variable. After this step, I would get robust standard errors, but I cannot use it in pcse estimation - I don't need the VCV of errors as an input for the <code>pcse</code> function, but the OLS <code>lm</code> object itself.</li>
<li>Or use Cochraneâ€“Orcutt transformation first, and then use transformed  model as an input for pcse estimation. I started doing it, but realized that after CO transformation, error term became serially independent, but had the kurtosis of 20 (normality assumption fails).</li>
</ul></li>
</ol>

<p>So, my options are not so suitable. How do you think I should approach this situation?</p>
"
"0.0618984460590173","0.0601929265428846","198219","<p>I ran the same Logistic regression with R and STATA. </p>

<p>The regressors include many dummy variables.</p>

<p>In R, the code I used is</p>

<pre><code>fit &lt;- glm(formula = y ~ ., family = ""binomial"", data = df)
</code></pre>

<p>which reports the warning message:</p>

<pre><code>glm.fit: algorithm did not converge 
</code></pre>

<p>In STATA, I simply ran</p>

<pre><code>Logit y x1 - x20
</code></pre>

<p>and the reported table looks OK and the estimation seems to be reasonable.</p>

<p>Actually if I have only a few regressors, say only $x_1, x_2, x_3$, they report the same result.</p>

<p>I'm wondering why there could be such difference? In R, how to fix the problem?</p>

<p>Thanks a lot!</p>
"
"0.138409133089567","0.134595475514541","200334","<p>I'm trying to model a mixed-effects model using lme4 or nlme in R where the <em>absolute</em> value of the random effect is predicted by a level two predictor, but I cannot seem to figure out how to get this done.</p>

<p>The basic equations are pretty simple, on level one I have no predictors so it's a simple $y_{ij} = \beta_{0j} + r_{ij}$. On level two it would then be $|\beta_{0j}| = \gamma_{00} + w_{j} + u_{0j}$.</p>

<p>So far, my only idea has been to do this with a two-step approach of estimating the empty model, extracting the random effects via <code>ranef(m)</code> and then taking their absolute value as the new outcome in a normal regression model. This, of course, has the downside of ignoring the estimation error associated with the random effects, so I would probably do this in a pseudo-latent-variable approach using the standard error as the residual variance. Are there any other approaches that allow for this in a single step? </p>
"
"0.215059722360368","0.225221308230725","202973","<p>Suppose I have a data set of <code>N</code> observations <code>(n = 1...N)</code> for out-of-sample estimation and values of ($y_n$). I have also <code>I</code> statistical models <code>(i= 1...I)</code> which every model has its own estimate on each data point ($\hat{y}^i_n$).</p>

<p>In addition I have a model selection method $\phi$ which would pick a model's estimate among the model set as its own according to its assessment on previous performance of the models ($\hat{y}^\phi_n = \min_i\{\hat{f_i}(y_n), i \in I\} $).</p>

<p>My claim should be ""model selection's performance is better than all models it picks estimates from"". I am trying to find a proper method to describe the statistical power of the model selection method, compared to individual models in the model set.</p>

<p>All individual models follow different assumptions, distributions and dependence structure. Some are iid, some have heteroskedasticity. Actually, there is no restriction on models except it should yield an estimate.</p>

<p>Some The models are employed on time series but what they do is asset pricing on different assets and contracts. But for a broaded audience I will make the following analogy.</p>

<p>Suppose you have a machine that predicts the scores on basketball matches. It does not only predict the final score, it also predicts a distribution of the scores throughout the time. It also predicts which player will score when.</p>

<p>Suppose you have many machines of this sort and all have different predictions. All of them had been right on some occasion (That is what statistics is after all right? No model is perfect.). </p>

<p>I am trying to figure out which machine is better at predicting what and when, using the previous performance of the machines. I can say stuff like 'oh machine A was good at predicting scores occured in the last 10 mins, but for the last 2 months model B became better'. </p>

<p>It turns out my estimates using the machines are better than any machine could do it alone in the long run. I checked for several error terms starting with MAPE and MSE. But I want to show that it is not a coincidence but a statistically significant fact. I have a fair sample size (~100k) over a good enough time period (5 years).</p>

<p>I fiddled with some thoughts about proportion of $\phi$ selecting the model with the lowest error and some logistic regression on that according to the criteria it uses to pick the models. But I lack the comprehensive knowledge on this domain of statistics.</p>

<p>ps. R package suggestions are also appreciated.</p>
"
"0.149304669588228","0.199637352376173","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.138409133089567","0.134595475514541","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.107211253483779","0.104257207028537","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.107211253483779","0.104257207028537","210339","<p>When using meta-regression with factor moderators, result differ a bit from using seperate estimation based on subgroup, even if the same model (mixed effects) is used for both. I understand the difference mostly stems from estimation of $\tau^2$.
I can't say from data, but it seem the subgroup-based method is more usually used for forest plots. </p>

<p>I'm using R and metafor, and it's easy to add subgroup polygons, but not with results from meta-regression. Am I wrong in assuming this is for a reason?
Wouldn't it be better to display results from meta-regresson, unlessof course differences in amount of heterogeneity are of interest?</p>
"
"0.0875376219064817","0.0851256530758749","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.165062522824046","0.180578779628654","212840","<p>I read Chen et al. <a href=""http://onlinelibrary.wiley.com/doi/10.1002/for.1134/abstract"" rel=""nofollow"">""Forecasting volatility with support vector machine-based GARCH model""</a> (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. 
The model is of the form </p>

<p>$y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ </p>

<p>$u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad  (2)$ </p>

<p>At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$.</p>

<hr>

<p><strong><em>Recurrent SVM Algorithm:</em></strong></p>

<p><strong>Step 1:</strong> Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $.</p>

<p><strong>Step 2:</strong> Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$</p>

<p><strong>Step 3:</strong> Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$.</p>

<p><strong>Step 4:</strong> Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2.</p>

<hr>

<p>The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. </p>

<p>As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. <strong>(Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept.</strong> Because I think I implemented it exactly as stated. My R code is the following</p>

<pre><code>rm(list = ls())

library(quantmod)
library(e1071)

#Get NYSE data and convert to log returns
id     &lt;- ""^NYA""
data   &lt;- getSymbols(id, source = ""yahoo"", auto.assign = FALSE, 
                     from = ""2004-01-08"", to = ""2007-12-31"")
series &lt;- data[,6]  #Get adjusted closing prices
series &lt;- na.omit(diff(log(series)))*100  #Compute log returns

#Lagged data for analysis
x      &lt;- na.omit(cbind(series, lag(series)))

#Set parameters as in paper
svm_eps   &lt;- 0.05
svm_cost  &lt;- 0.005
sigma     &lt;- 0.02
svm_gamma &lt;- 1/(2*sigma^2)


#SVM to get u_t
svm     &lt;- svm(x = x[,-1], y = x[,1], scale = FALSE,
               type = ""eps-regression"", kernel = ""radial"",
               gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

u    &lt;- svm$residuals  #Extract u_t
n    &lt;- 60  #Size of test set
u_tr &lt;- u[1:(nrow(u) - n)]  #Subset to training set
u_tr &lt;- na.omit(cbind(u_tr, lag(u_tr)))^2  #Final training set


#Recurrent SVM for vola estimation
i       &lt;- 1
p_count &lt;- 0

while(p_count &lt; 5){

  print(i)  #Print number of loops

  #Estimate SVM for u^2
  svmr     &lt;- svm(x = u_tr[,-1], y = u_tr[,1], scale = FALSE,
                  type = ""eps-regression"", kernel = ""radial"",
                  gamma = svm_gamma, cost = svm_cost, epsilon = svm_eps)

  #Test autocorrelation of residuals to lag 1
  test    &lt;- Box.test(svmr$residuals, lag = 1, type = ""Ljung-Box"")
  p_val   &lt;- test$p.value
  p_count &lt;- ifelse(p_val &gt; 0.1, p_count + 1, 0)

  #Extract residuals for next estimation step
  w        &lt;- svmr$residuals
  w        &lt;- c(0, w[-length(w)])  #lag 1

  u_tr &lt;- cbind(u_tr[,1:2], w)

  i &lt;- i + 1
}
</code></pre>
"
"0.0875376219064817","0.0851256530758749","213231","<p>I can perform a vector autoregression using the ""vars"" package in R.</p>

<pre><code>library(vars)
data(Canada)
VAR(Canada, p = 2, type = ""none"")
</code></pre>

<p>But as I understand it, vector autoregression will only work if the factors used are cointegrated.</p>

<p>Is there an R package for vector autoregression that only selects cointegrated factors, or penalises non-cointegrated factors before the VAR estimation?
I searched <a href=""http://rseek.org/"" rel=""nofollow"">http://rseek.org/</a> but did not find this.</p>
"
"0.107211253483779","0.104257207028537","215657","<p>Please can someone provide an accessible interpretation of the parameter estimates from a discrete weibull regression model, e.g in R: </p>

<pre><code>library(DWreg)
library(COMPoissonReg)

data(freight)

dw.reg(broken ~ transfers, data = freight,
       para.beta=FALSE,para.q1=FALSE,para.q2=TRUE)
</code></pre>

<p>produces:</p>

<pre><code>Maximum Likelihood estimation
Newton-Raphson maximisation, 11 iterations
Return code 1: gradient close to zero
Log-Likelihood: -18.82916 
3  free parameters
Estimates:
            Estimate Std. error t value  Pr(&gt; t)    
(Intercept) -26.7814     7.3943  -3.622 0.000292 ***
transfers    -2.5857     0.7579  -3.412 0.000646 ***
beta         10.8723     2.9356   3.704 0.000213 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 
</code></pre>

<p>I am trying to understand how one would communicate the results to a layman in an analogous way that one might interpret the parameter estimates from linear (unit increase in x is assoc with beta increase in y) or poisson (mulitplicative) etc.</p>
"
"0.123796892118035","0.120385853085769","216335","<p>I am studying parameters generated by the method of Ordinary Least Squares, in particular, a parameter's associated standard error. <a href=""https://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation"" rel=""nofollow"">Wikipedia</a> suggests calculating </p>

<p>$$s^2=\frac{S\left(\hat\beta\right)}{n-p}=\frac{\rvert\rvert y-X\hat\beta\rvert\rvert^2}{n-p}$$</p>

<p>describing this as </p>

<blockquote>
  <p>the standard error of the regression (SER), or standard error of the equation (SEE).</p>
</blockquote>

<p>This, however, is not sufficient to calculate the standard error for a given parameter. </p>

<p>I found <a href=""http://www.originlab.com/doc/origin-Help/PR-Algorithm#The_Parameter_Standard_Errors"" rel=""nofollow"">this</a>, which suggests</p>

<blockquote>
  <p>For each parameter, the standard error can be obtained by:</p>
</blockquote>

<p>$$s_{\hat \beta _j}=s \sqrt{C_{jj}}$$  </p>

<blockquote>
  <p>where $C_{jj}$ is the j$^{th}$ diagonal element of $(X'X)^{-1}$ (note that $(X'WX)^{-1}$ is used for weight fitting). </p>
</blockquote>

<p>Note, that using this technique yields precisely the same standard errors as those summarized by the <code>lm</code> method in R.</p>

<p>Where does this $C$ come from? I have been reading about this for days, and this is the first reference I have seen to this matrix. </p>
"
"0.138409133089567","0.134595475514541","217365","<p>If we have a dataset like x=(3,4,2,1,4,...,5), we have classic methods (method of moments, maximum likelihood method, etc) to fit a distribution.</p>

<p>However, in certain real life cases, we can have uncertain about our dataset: </p>

<ul>
<li>if they are mesurements, there can be incertainty due to diverse instruments, </li>
<li>if they are survey resutls, people can be very sure about one choice, and give an estimation, between x and y.</li>
<li>sometimes, the data comes from a retreatment: when estimating a fortune of a person who lived in 1900, we have to take into account the inflation. etc.</li>
</ul>

<p>So, let's then consider that along with the initial dataset, we can also give the standard deviation of estimation of each dataset: sdx=(1,0.4,0.3,1,0.7,...,0.8). How can we take this information when fitting a distribution? There are some well established methods (derivated from method of moments, or maximum likelihood method, etc.), any R packages ?</p>

<p>When doing a regression, I think that this problem can also be encountered. It seems that weighted regression could partillay deal with this.</p>
"
"0.107211253483779","0.104257207028537","218676","<p>I'm running a maximum likelihood of a logit regression, but the estimated parameters value and the loglikelihood value are depending on the value of the algorithm's start. For example, if my start is a vector of 0's I get one value, ifI change that I get another. Any way to avoid that?</p>

<p>I'm running the optimization with the maxLik package. (Sorry for the function is huge)</p>

<pre><code>    g = function(b){
    bb = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
             + b[7]*DENSIDADE - exp(b[8])*p.br - exp(b[9])*p.bs - exp(b[10])*p.bc - exp(b[11])*p.bi - exp(b[12])*p.bh)

      caixa = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                + b[7]*DENSIDADE - exp(b[13])*p.br- exp(b[14])*p.bs - exp(b[15])*p.bb - exp(b[16])*p.bi - exp(b[17])*p.bh)

      bradesco = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                   + b[7]*DENSIDADE - exp(b[18])*p.bb- exp(b[19])*p.bs - exp(b[20])*p.bc - exp(b[21])*p.bi - exp(b[22])*p.bh)

      hsbc = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
               + b[7]*DENSIDADE- exp(b[23])*p.br- exp(b[24])*p.bs - exp(b[25])*p.bc - exp(b[26])*p.bi - exp(b[27])*p.bb)

               itau = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
               + b[7]*DENSIDADE - exp(b[28])*p.br - exp(b[29])*p.bs - exp(b[30])*p.bc - exp(b[31])*p.bb - exp(b[32])*p.bh)

      santander = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                    + b[7]*DENSIDADE - exp(b[33])*p.br - exp(b[34])*p.bb - exp(b[35])*p.bc - exp(b[36])*p.bi - exp(b[37])*p.bh)

      l = sum(BB*(log(bb))
      +(1-BB)*(log(1 - bb))
      + CAIXA*(log(caixa))
      +(1-CAIXA)*(log(1 - caixa ))

      + BRADESCO*(log(bradesco))
      +(1-BRADESCO)*(log(1 - bradesco))

      + HSBC*(log(hsbc))
      +(1-HSBC)*(log(1 - hsbc))

      + ITAU*(log(itau))
      +(1-ITAU)*(log(1 - itau))

      + SANTANDER*(log(santander))
      +(1-SANTANDER)*(log(1 - santander))

      )
      return(l)

    }


    ll = maxLik(logLik = g, start = rep(0,46), method = ""BFGS"", control = list(tol = 1e-20, iterlim = 1000))
</code></pre>

<p>Output:
Maximum Likelihood estimation
BFGS maximization, 179 iterations
Return code 0: successful convergence 
Log-Likelihood: -11065.77 (46 free parameter(s))
Estimate(s): -4.278447 0.002979005 4.625648 8.724918e-05 -0.04196911 -0.01600133 -6.890406e-06 -1.14766 -0.6503709 -1.265141 -1.051562 -1.08185 -0.9048582 -0.5394084 -0.5875544 -0.30975 -0.1041046 -1.87203 -1.706692 -0.4717443 -0.8142002 -0.3034415 -2.413816 0.1269036 -1.353817 -1.455872 1.122101 -0.7207807 0.2147704 -0.1741227 -0.9657993 0.5873628 -1.392237 0.4828495 -0.3115696 -2.190175 -0.5272591 0.8638808 1.085701 0.4827111 0.6055928 1.302957 0.377728 -1.119531 -0.4588106 -0.6437521 </p>
"
"0.167967753286756","0.181488502160157","220317","<p>I'm running a fixed-effects Poisson regression and get different results in Stata and R. Unfortunately I cannot upload and share the data due to legal restrictions.</p>

<p>My code in R is (formula shortened for illustration):</p>

<pre><code>library(pglm)    
pdf &lt;- pdata.frame(data, index=c(""id"",""timevar""))    
model &lt;- pglm(y ~ x1 + x3 + x3_lag + x3_lag2 + season + x4 + x1*x3_lag + x1*x3_lag2+x1*x4,
              data = pdf,
              effect = ""individual"",
              model = ""within"",
              family = ""poisson"")
</code></pre>

<p>where <code>season</code> controls for seasonal effects, <code>Y</code> is a count variable and <code>x3</code> is a treatment. </p>

<p><code>summary(model)</code> yields this:</p>

<pre><code>Maximum Likelihood estimation
Newton-Raphson maximisation, 4 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -47369.66 
19  free parameters
Estimates:
                   Estimate Std. error t value Pr(&gt; t)
x1            -1.031e-04        Inf       0       1
x3            -1.196e-02        Inf       0       1
x3_lag        -4.783e-02        Inf       0       1
x3_lag2       -5.159e-02        Inf       0       1
season02      -7.038e-02        Inf       0       1
season03       9.323e-02        Inf       0       1
season04       1.257e-01        Inf       0       1
season05       1.427e-01        Inf       0       1
season06      -1.217e-01        Inf       0       1
season07      -1.566e+01        Inf       0       1
season08      -2.095e-01        Inf       0       1
season09      -1.886e-01        Inf       0       1
season10       4.488e-02        Inf       0       1
season11      -9.954e-02        Inf       0       1
season12       8.201e-02        Inf       0       1
x4             6.055e-01        Inf       0       1
x1:x3_lag      1.888e-05        Inf       0       1
x1:x3_lag2     3.529e-05        Inf       0       1
x1:x4          4.948e-04        Inf       0       1
</code></pre>

<p>In Stata14 I used:</p>

<pre><code>xtset id timevar
xtpoisson x1 x3 x3_lag x3_lag2 season x4 x1#x3_lag x1#x3_lag2 x1#x4, fe
</code></pre>

<p>The results in Stata (see below) yield standard errors and where the standard errors are significant the coefficients are very much the same as in R, so I assume in both cases the same model was calculated. The issue is: why do I get standard errors in Stata but ""Inf"" in R?</p>

<pre><code>Conditional fixed-effects Poisson regression    Number of obs     =    110,233
Group variable: id                              Number of groups  =     15,945

                                                Obs per group:
                                                              min =          2
                                                              avg =        6.9
                                                              max =         13

                                                Wald chi2(19)     =     816.49
Log likelihood  = -47369.663                    Prob &gt; chi2       =     0.0000

--------------------------------------------------------------------------------------
           y         |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
---------------------+----------------------------------------------------------------
                  x1 |  -.0001031   .0002119    -0.49   0.626    -.0005184    .0003121
                  x3 |  -.0119605   .0058094    -2.06   0.040    -.0233467   -.0005744
              x3_lag |  -.0478287   .0122489    -3.90   0.000    -.0718361   -.0238212
                     |
           x3_lag#x1 |   .0000189   .0000225     0.84   0.401    -.0000252     .000063
                     |
             x3_lag2 |  -.0515859   .0138096    -3.74   0.000    -.0786523   -.0245195
                     |
          x3_lag2#x1 |   .0000353   .0000245     1.44   0.149    -.0000127    .0000833
                     |
              season |
                 02  |  -.0703797   .0211975    -3.32   0.001    -.1119261   -.0288333
                 03  |    .093219   .0233858     3.99   0.000     .0473836    .1390544
                 04  |   .1256642   .0270927     4.64   0.000     .0725635    .1787649
                 05  |   .1426421   .0335767     4.25   0.000     .0768331    .2084512
                 06  |  -.1217462    .046838    -2.60   0.009     -.213547   -.0299454
                 07  |  -18.24382   741.5026    -0.02   0.980    -1471.562    1435.075
                 08  |  -.2095094   .0373193    -5.61   0.000    -.2826539   -.1363649
                 09  |  -.1886333   .0341701    -5.52   0.000    -.2556054   -.1216611
                 10  |    .044879   .0278778     1.61   0.107    -.0097604    .0995184
                 11  |  -.0995352   .0249665    -3.99   0.000    -.1484686   -.0506018
                 12  |   .0819983   .0214643     3.82   0.000      .039929    .1240675
                     |
                  x4 |   .6055111   .0754535     8.02   0.000      .457625    .7533972
                     |
               x4#x1 |   .0004948   .0001416     3.49   0.000     .0002173    .0007723
--------------------------------------------------------------------------------------
</code></pre>

<p>If you have any suggestions on how I could provide a replicable example (maybe with some sample panel data, any suggestions?), I will gladly do so.</p>

<p><strong>EDIT</strong></p>

<p>A smaller model produces identical results in R and Stata. It seems that with poor models R is more restrictive and does not show standard errors.</p>
"
"0.0875376219064817","0.0851256530758749","224929","<p>main equation</p>

<p>$y_1 = y_2 \beta + x_1  \gamma + u_i $</p>

<p>Instrumental equation</p>

<p>$y_2=x_1 \pi_1 + x_2 \pi_2$</p>

<p>I have a binary endogenous variable $y_2$ in my main estimation equation. 
My instrument $x_2$ is binary as well and recently my supervisor suggested a <a href=""http://blog.stata.com/2013/11/07/fitting-ordered-probit-models-with-endogenous-covariates-with-statas-gsem-command/"" rel=""nofollow"">structural equation approach</a> to me. Generally I ran </p>

<pre><code>gsem (y1 &lt;- y2 x1 L@a, oprobit) (y2 &lt;- x1 x2 L@a)
</code></pre>

<p>Where the latent variable $L$ is part of the parametrisation (cf. link before). The main issue is that this procedure is that I do not account for the binary nature of the instrument and the instrumented variable and consequently the log-likelihood function breaks as it is not continous. </p>

<p>Is there a way around this discontinous function problem? Or do I need to estimate the first part with probit/logit? If so is that then still IV-regression or something else? </p>
"
"0.215059722360368","0.225221308230725","226073","<p>I have panel data that is structured like the example below only with more variables. I am using R and my goal is pretty straight forward - 
estimate the effect of the independent variables on my dependent variable.</p>

<pre><code>        country   date        dependent     independent type1  independent type2
        Germany   01/01/2006  70            30                 0.754
        Germany   01/02/2006  72            36                 0.821
        ...  
        Germany   12/31/2016  70            16                 1.214
        Italy     01/01/2006  54            30                 0.213
        Italy     01/02/2006  59            36                 0.343
        ...
</code></pre>

<p>I assume that there are country specific effects that probably also vary across time in the long run which is why I wanted to include fixed effects and then split my panel into 4 time periods and run a regression that is simply like this:</p>

<pre><code>time period 1 lm(dependent ~ independent1+independent2+independent3 ....+country)
time period 2 lm(dependent ~ independent1+independent2+independent3 ....+country)
...
</code></pre>

<p>However when skimming trough some books  I became increasingly unsure (and confused) if this is an adequate approach. Thus my two questions would be:</p>

<ol>
<li><p>Is what I intended to do the (or a) suitable approach to achieve my objective?             </p>

<ol start=""2"">
<li>Can you recommend some other ways to estimate this? I am also very interested in trying some ""creative"" things as long as they are not way over my head.</li>
</ol></li>
</ol>

<p>One more remark.
I indicated type 1 and type 2 for the independent variables. While both vary across time the first one does not vary across countries. I am not sure if that is important on the other hand I feel like I am not sure about anything anymore after looking trough those statistics books.</p>

<p>Thank you.</p>

<p><strong>EDIT:</strong> </p>

<p>What I mean with ""fixed effects change over time"" 
There are (for me) unobservable variables. The effect of these variables is of different magnitude for every country however it will also change to some extend over time. I thought that fixed effects might be able to improve my estimation in that they would capture something like ""the average"" effect of these variables for every country. </p>

<p>It might be difficult to explain without the economic context. I left it out before, because I thought it might be clearer when I express it in general terms but maybe this helps. </p>

<p>I look at the credit default swap (CDS) bond basis which is a spread so simply the difference between the two ""yields"". Now some of this spread I can explain with variables that I am able to proxy for like counter party risk that is involved in CDS etc. However some other parts like ""embedded options in CDS contracts"" I can not observe or proxy for. The impact of these variables will likely be large and also be different for every country. So for example the option value is connected to the default risk of the country thus it will vary over time (as the default probability will vary over time) but especially it will be different for say Greece and Germany.</p>
"
"0.107211253483779","0.104257207028537","231974","<p>Many Gaussian process packages are available in R. For example there is $\textbf{BACCO}$ that offers some calibration techniques, $\textbf{mlegp}$ and $\textbf{tgp}$ focusing on treed models and parameter estimation and $\textbf{GPML}$ for Gaussian process classification and regression. </p>

<p>The problem with these packages is that the choice of correlation function is restricted. Only some choices are provided for building the correlation function (Gaussian, Matern, etc...).</p>

<p>Does anyone have specific experience on how to can insert my own correlation function and just use the optimization routines available in these packages which are specifically tailored for GP's. Or is there a package which allows me to do that ?</p>
"
"0.0714741689891863","0.104257207028537","232682","<p>I have a <em>balanced panel</em> dataset in which I have observations for N countries across T quarters. I use a <em>fixed effect model</em> to explore the relationship between my variables. Since all countries have the same number of observations, they are all given the same ""weight"" in the regression. I would like to apply a weigthing by GDP such that observations of big countries (in terms of wealth) have more impact on my estimation. My first idea was to compute weights for all countries and rescale the dependent variable. However, I think this method will only affect the (country-specific) intercepts and won't have much impact on the coefficients. I also don't think that controlling for GDP by including it in the regression is the solution. </p>

<p>I am working with the R plm package to estimate my models. I know that the lm function has a weight parameter but couldn't find an equivalent in the plm package. </p>

<p>Any idea how I should proceed?</p>
"
