"V1","V2","V3","V4"
"0.0778498944161523","0.0751646028002829","  4830","<p>Full Disclosure: This is homework. I've included a link to the dataset ( <a href=""http://www.bertelsen.ca/R/logistic-regression.sav"">http://www.bertelsen.ca/R/logistic-regression.sav</a> )</p>

<p>My goal is to maximize the prediction of loan defaulters in this data set.  </p>

<p>Every model that I have come up with so far, predicts >90% of non-defaulters, but &lt;40% of defaulters making the classification efficiency overall ~80%. So, I wonder if there are interaction effects between the variables? Within a logistic regression, other than testing each possible combination is there a way to identify potential interaction effects? Or alternatively a way to boost the efficiency of classification of defaulters. </p>

<p>I'm stuck, any recommendations would be helpful in your choice of words, R-code or SPSS syntax. </p>

<p>My primary variables are outlined in the following histogram and scatterplot (with the exception of the dichotomous variable)</p>

<p>A description of the primary variables: </p>

<pre><code>age: Age in years
employ: Years with current employer
address: Years at current address
income: Household income in thousands
debtinc: Debt to income ratio (x100)
creddebt: Credit card debt in thousands
othdebt: Other debt in thousands
default: Previously defaulted (dichotomous, yes/no, 0/1)
ed: Level of education (No HS, HS, Some College, College, Post-grad)
</code></pre>

<p>Additional variables are just transformations of the above. I also tried converting a few of the continuous variables into categorical variables and implementing them in the model, no luck there. </p>

<p>If you'd like to pop it into R, quickly, here it is: </p>

<pre><code>## R Code
df &lt;- read.spss(file=""http://www.bertelsen.ca/R/logistic-regression.sav"", use.value.labels=T, to.data.frame=T)
</code></pre>

<p><img src=""http://i.stack.imgur.com/aVqtZ.jpg"" alt=""alt text"">
<img src=""http://i.stack.imgur.com/VQJDg.jpg"" alt=""alt text""></p>
"
"0.149591518401353","0.144431628934824","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"0.0449466574975495","0.0433963036602746","  6734","<p>I have been reading the description of ridge regression in <em><a href=""http://rads.stackoverflow.com/amzn/click/007310874X"" rel=""nofollow"">Applied Linear Statistical Models</em>, 5th Ed</a> chapter 11. The ridge regression is done on body fat data available <a href=""http://www.cst.cmich.edu/users/lee1c/spss/V16_materials/DataSets_v16/BodyFat-TxtFormat.txt"" rel=""nofollow"">here</a>. </p>

<p>The textbook matches the output in SAS, where the back transformed coefficients are given in the fitted model as:<br>
$$
Y=-7.3978+0.5553X_1+0.3681X_2-0.1917X_3
$$</p>

<p>This is shown from SAS as:</p>

<pre><code>proc reg data = ch7tab1a outest = temp outstb noprint;
  model y = x1-x3 / ridge = 0.02;
run;
quit;
proc print data = temp;
  where _ridge_ = 0.02 and y = -1;
  var y intercept x1 x2 x3;
run;
Obs     Y    Intercept       X1         X2         X3

 2     -1     -7.40343    0.55535    0.36814    -0.19163
 3     -1      0.00000    0.54633    0.37740    -0.13687
</code></pre>

<p>But R gives very different coefficients:</p>

<pre><code>data &lt;- read.table(""http://www.cst.cmich.edu/users/lee1c/spss/V16_materials/DataSets_v16/BodyFat-TxtFormat.txt"", 
                   sep="" "", header=FALSE)
data &lt;- data[,c(1,3,5,7)]
colnames(data)&lt;-c(""x1"",""x2"",""x3"",""y"")
ridge&lt;-lm.ridge(y ~ ., data, lambda=0.02)   
ridge$coef
coef(ridge)

&gt;   ridge$coef
       x1        x2        x3 
10.126984 -4.682273 -3.527010 
&gt;   coef(ridge)
                   x1         x2         x3 
42.2181995  2.0683914 -0.9177207 -0.9921824 
&gt; 
</code></pre>

<p>Can anyone help me understand why?</p>
"
"0.155699788832305","0.150329205600566"," 12590","<p>I would like to do something in R that SAS can do using SAS's proc mixed (there is some way to do in STATA es well), namely fitting the so called Bivariate model from Reitsma et al (2005). This model is a special mixed model where the variance depends on the study (see below). Googling and talking to some people familiar with the model did not yield a straightforward approach that is fast at the same time (i.e. a nice high level model fitting function). I am nevertheless sure, there is something fast in R that one can built on.</p>

<p>In a nutshell one is faced with the following situation: Given pairs of proportions $(p_1,p_2)$  in $[0,1]^2$ one would like to fit a bivariate normal to the logit-transformed pairs. Since the proportions come from a 2x2 table (i.e. binomial data) each logit transformed observed proportion has a variance estimate that is to be included in the fitting process, say $(s_1, s_2)$. So one would like to fit a bivariate normal to the pairs, where the covariance matrix $\Sigma$ <em>depends</em> on the observation, i.e. </p>

<p>$(\text{logit}(p_1),\text{logit}(p_2)) \sim N((mu_1, mu_2), \Sigma + S)$</p>

<p>where S is the diagonal matrix with $(s_1, s_2)$ and depends entirely on the data but varies from observation to observation. mu and Sigma are the same for all observation though.</p>

<p>Right now I am using a call to <code>optim()</code> (using BFGS) to estimate the five parameters ($\mu_1$, $\mu_2$, and three parameters for $\Sigma$). Nevertheless this is painfully slow, and especially unsuitable for simulation. Also one of my aims is to introduce regression coefficients for mu later, increasing the number of parameters.</p>

<p>I tried speeding up fitting by supplying starting values and I also thought about computing gradients for the five parameters. Since the likelihood becomes quite complex due to the addition of $S$, I felt the risk of introducting errors this way was too big and did not attempt it yet, nor did I see a way to check my calculations.</p>

<p>Is the calculation of the gradients typically worthwhile? How do you check them?</p>

<p>I am aware of other optimizer besides <code>optim()</code>, i.e. <code>nlm()</code> and I also know about the CRAN Task view: Optimization. Which ones a are worth a try?</p>

<p>What kind of tricks are there to speed up <code>optim()</code> besides reducing accuracy?</p>

<p>I would be very grateful for any hints.</p>
"
"0.186355130673225","0.198866846404498"," 15160","<p>I have a large dataset with patients and I'm studying a rare outcome (~ 2%) and death is a competing risk (mean age ~69 years). I've used the R ""cmprsk"" package for my statistics and it seems that competing risks and the Cox regression are performing similarly although the competing risk analysis is more conservative giving hazard ratios closer to 1.</p>

<p>I've been suggested to do a Poisson regression on the data but the results don't make any sense and I would be really grateful to get some input on the benefits of doing this kind of analysis on survival data. I've created this simulation for creating a dataset with similar risk factors:</p>

<pre><code>library(""cmprsk"")
# The time for the study
accrual_time &lt;- 10
followup_time &lt;- 1

base_risk &lt;- list(""event"" = .015, ""cmprsk"" = .1)

risk_factors &lt;- list(list(""frequency""=.1, 
                ""event"" = base_risk$event*.5, 
                ""cmprsk"" = base_risk$cmprsk*2),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*1, 
                ""cmprsk"" = base_risk$cmprsk*1),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*-.5, 
                ""cmprsk"" = base_risk$cmprsk*0))

# Number of subjects
n &lt;- 5000

# Create base time, sequential inclusion
time_in_study &lt;- rep(c(1:n)/n*accrual_time + followup_time, 1)

set.seed(100)

# Create empty sets
x &lt;- matrix(0, ncol=length(risk_factors), nrow=n)
time_2_event &lt;- rep(0, n)
time_2_comprsk &lt;- rep(0, n)

# Create each studied observation and outcome
for(i in 1:n){
    # Set base risk
    event_risk &lt;- base_risk$event 
    comp_risk &lt;- base_risk$cmprsk

    for(j in 1:length(risk_factors)){
        x[i, j] &lt;- rbinom(1, 1, risk_factors[[j]]$frequency)[1]

        # If there is a risk factor defined
        if (x[i, j] &gt; 0){
            event_risk &lt;- event_risk +
                    risk_factors[[j]]$event
            comp_risk &lt;- comp_risk + 
                    risk_factors[[j]]$cmprsk
        }
    }

    # Time 2 event/risk is 1/rate meaning that higher number -&gt; shorter time
    time_2_event[i] &lt;- rexp(1, rate=event_risk)[1]
    time_2_comprsk[i] &lt;- rexp(1, rate=comp_risk)[1]
}

cn &lt;- c()
for(i in 1:length(risk_factors)){
    ev_rsk &lt;- risk_factors[[i]]$event/base_risk$event+1
    cmp_rsk &lt;- risk_factors[[i]]$cmprsk/base_risk$cmprsk+1
    name &lt;- paste(""Risk factor no: "", i, ""\n * ev="", ev_rsk, "" cr="", cmp_rsk, "" *"", sep="""")
    cn &lt;- c(cn, name)
}
colnames(x) &lt;- cn

# Select the event that happens first: study ends, evenent occurs, a competing event occurs
time &lt;- apply(cbind(time_in_study, time_2_event, time_2_comprsk), 1, min)

# Outcome identifiers
event &lt;- (time_2_event == time) + 0
comprsk &lt;- (time_2_comprsk == time) + 0
cens &lt;- event+2*(event==0 &amp; comprsk==1)

out.cox_ev &lt;- coxph(Surv(time, event)~x)
summary(out.cox_ev)

out.crr_ev &lt;- crr(time, cens, x, failcode=1)
summary(out.crr_ev)

out.cox_cmprsk &lt;- coxph(Surv(time, comprsk)~x)
summary(out.cox_cmprsk)

out.crr_cmprsk &lt;- crr(time, cens, x, failcode=2)
summary(out.crr_cmprsk)
</code></pre>

<p>The output makes sense but when I do a:</p>

<pre><code>out.glm_pr &lt;- glm(event ~ x, family=""poisson"")
summary(out.glm_pr)
</code></pre>

<p>It gives estimates of:</p>

<ul>
<li>RF 1 ~ .14 </li>
<li>RF 2 ~ .41 </li>
<li>RF 3 ~ -.23</li>
</ul>

<p>My questions: </p>

<ul>
<li>Is the glm() code correct or should I somehow transform my data?</li>
<li>Does the Poisson output make any sense and how should if so interpret it?</li>
<li>What are the benefits/pitfalls in using Poisson regression for survival data?</li>
</ul>

<p>Thanks!</p>

<hr>

<h2>UPDATE</h2>

<p>After adding exp(out.glm_pr$coefficients) the results are almost identical to the competing risk regression, here's a forest plot that compares the three:</p>

<p><img src=""http://i.stack.imgur.com/14Zt0.png"" alt=""A forestplot comparing the different methods - Poisson: 1.152  1.509  0.794, CRR: 1.151 1.524 0.812, Cox PH: 1.897 1.931 0.798""></p>

<p>The x-axis is perhaps not entirely valid (should be ""incident rate ratios"" for the Poisson regression) but why are the outcomes for CRR &amp; poisson almost identical?</p>

<p>As for testing over-dispersion I've found these two methods:</p>

<pre><code>&gt; library(qcc)
&gt; qcc.overdispersion.test(event)

Overdispersion test Obs.Var/Theor.Var Statistic p-value
       poisson data         0.9391878      4695 0.99902
&gt; 
&gt; library(pscl)
&gt; out.glm_nb &lt;- glm.nb(event ~ x)
Warning messages:
1: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
2: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
&gt; odTest(out.glm_nb)
Likelihood ratio test of H0: Poisson, as restricted NB model:
n.b., the distribution of the test-statistic under H0 is non-standard
e.g., see help(odTest) for details/references

Critical value of test statistic at the alpha= 0.05 level: 2.7055 
Chi-Square Test Statistic =  -0.0139 p-value = 0.5 
</code></pre>

<p>I conclude that there isn't any evidence of over-dispersion or are there other methods better suited for testing over-dispersion in this kind of survival data?</p>

<p>The quasipoisson analysis gives similar values:</p>

<pre><code>&gt; out.glm_quasi_pr &lt;- glm(event ~ x, family=quasipoisson(link=""log""))
&gt; round(exp(out.glm_quasi_pr$coefficients), 3)
(Intercept)       xRF 1       xRF 2       xRF 3 
      0.059       1.152       1.509       0.794 
</code></pre>
"
"0.155699788832305","0.150329205600566"," 17552","<p>I have a gene expression data-set with log2-transformed expression values (no NAs) for 495 genes for 59 samples for which values of a continuous response variable (r) are also known (no NAs). I want to use leave-one-out cross validation to test if r of the test sample can be predicted from the sample's gene expression.</p>

<p>For this, I intend to use the <a href=""http://cran.r-project.org/web/packages/samr/index.html"" rel=""nofollow"">samr</a> R package for Significance Analysis of Microarrays to identify significant genes associated with r in the training set of samples. Then, I want to generate a linear model using the significant genes as variables, which will then be used to predict r of the test sample. I have tried the following code to begin with, but when I generate the model and examine it, I see many NAs in the model summary, which makes me suspect that I am doing something wrong.</p>

<p>Can someone tell me what I might be doing wrong?</p>

<p>Secondly, I will appreciate any comment on the use of nperms (in SAM) with a value of 100. Is it too low for an expression data-set for 495 genes. </p>

<pre><code># rVals with the r values is read as a vector from a row of a table for phenotypic data read from a tab-delimited file with sample-names as column names and phenotype features as row-names
# geneVals is the log2-transformed gene expression data-set read as a matrix from a tab-delimited file with sample-names as column names and gene-names as row-names

# Perform SAM with FDR of 5% and obtain list of significant genes

sam &lt;- SAM(x=geneVals, y=rVals, resp.type=c(""Quantitative""),
testStatistic=c(""standard""), regression.method=c(""standard""), logged2=TRUE, 
fdr.output=0.05, eigengene.number=1, knn.neighbors=10, nperms=100, 
genenames=as.vector(rownames(geneVals)))

sigGenes &lt;- rbind(sam$siggenes.table$genes.up, sam$siggenes.table$genes.lo)

# Generate linear model
toModel &lt;- data.frame(t(rbind(rVals, geneVals)), check.names=FALSE)
myModel &lt;- lm(toModel[c('r', sigGenes[,c(""Gene ID"")])])

# Examine model
summary(myModel)

...output...

Call:
lm(formula = toModel[c(""rVals"", sigGenes[, c(""Gene ID"")])])

Residuals:
ALL 59 residuals are 0: no residual degrees of freedom!

Coefficients: (58 not defined because of singularities)
           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   -18.29363         NA      NA       NA
`let-7e`       -1.70545         NA      NA       NA
`miR-125a-5p`   2.43177         NA      NA       NA
`miR-151-5p`    2.67439         NA      NA       NA
...
</code></pre>
"
"0.149071198499986","0.143929256529458"," 18045","<p>The data simulated below has a maximum value of 4 and is interestingly skewed. The maximum of 4 is a limitation imposed by the instrument used and the data is semi-discrete, i.e., there are a reasonably large number of numbers it could be between -4 and 4. Because of the shape of the data, I thought about transforming it so it would approximate a gamma distribution:  </p>

<p><em>Edit to update for comments:</em><br>
It is limited to this range in this instance because it is a signal detection measure (d prime <a href=""http://en.wikipedia.org/wiki/D%27"" rel=""nofollow"">http://en.wikipedia.org/wiki/D%27</a>) and the accuracy we have for this particular measure limits us to +-4. It is skewed like this because one population does not very often get false positives and will generally get more hits while the other populations often do get false positives and less hits.</p>

<pre><code>set.seed(69)
g1&lt;-rnorm(700,0,1); g2&lt;-rnorm(100,-0.5,1.5); g3&lt;-rnorm(100,-1,2.5)
gt&lt;-data.frame(score=c(g1, g2, g3), fac1=factor(rep(c(""a"", ""b"", ""c""), c(700, 100, 100))), fac2=ordered(rep(c(0,1,2), c(3,13,4))))
gt$score&lt;-with(gt, ifelse(fac2 == 0, score, score-rnorm(1, 0.5, 2)))
gt$score&lt;-with(gt, ifelse(fac2 == 2, score-rnorm(1, 0.5, 2), score))
gt$score&lt;-round(with(gt, ifelse(score&gt;0, score*-1, score)), 1)+4
gt$score&lt;-with(gt, ifelse(score &lt; -4, -4, score))
gt$cov1&lt;-with(gt, score + rnorm(900, sd=40))/40
hist(gt$score)
gt$score2&lt;-with(gt, 4-score+0.0000001) #Gamma distribution can't have 0s (and is positive skewed???)
hist(gt$score2)

glm1&lt;-glm(score2~cov1+fac1*fac2, family=""Gamma"", data=gt)
</code></pre>

<p>This is quite new territory for me.<br>
1. Is this a reasonable thing to do?<br>
2. Are there other distributions I might try and compare (exponential perhaps)?</p>

<p><em>Update:</em><br>
After some comments below, I investigated beta regression using the <em>betareg</em> package in R. It gave me skewed residuals:  </p>

<pre><code>gt$scorer&lt;-with(gt, (score--4)/(4--4))
gt$scorer&lt;-with(gt, (scorer*(length(scorer)-1)+0.5)/length(scorer))
b1 &lt;- betareg(scorer ~ cov1 + fac1 * fac2, data=gt)
plot(density(resid(b1))) #Strange residuals, even straight lm looks better
</code></pre>

<p>So I had a look at a quasibinomial regression and it gave me smaller and better looking residuals:</p>

<pre><code>glm2 &lt;- glm(scorer~cov1 + fac1 * fac2, data=gt, family=""quasibinomial"")
plot(density(resid(g1))) #Better residuals
</code></pre>

<p>Are the residuals good enough to go on in this case?<br>
Or is the fact that d', while based upon T/F, is not a binary variable, a serious issue?  </p>

<p><em>Edit 3: d' clarification</em> 
The below is an example of my d' scores, with the rough distributional qualities and similar raw scores for hits and false positives.  </p>

<pre><code>hitrate&lt;-sample(0:16, 100, replace=T, prob=c(rep(0.02,11), 0.025, 0.05, 0.1, 0.2, 0.3, 0.2))/16
hitrate&lt;-ifelse(hitrate==1, 31/32,hitrate); hitrate&lt;-ifelse(hitrate==0, 1/32,hitrate)
farate&lt;-sample(0:32,100, replace=T, prob=c(0.7,0.1,0.05,0.05,0.05,0.02,rep(0.001, 27)))/32
farate&lt;-ifelse(farate==0, 1/64,farate); farate&lt;-ifelse(farate==1, 63/64,farate)

dprime&lt;-round(qnorm(hitrate) - qnorm(farate),1)
plot(density(dprime))
</code></pre>
"
"0.110096376512636","0.088582333908789"," 20672","<p>I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S.</p>

<p>If one calls <code>gls(Y ~ X, correlation = S)</code>, what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being <code>S^(-1)*Y = S^(-1) * X</code>.</p>

<p>How is this transformation actually performed? From the literature I've consulted, I've seen everything from:</p>

<pre><code>X.transformed &lt;- solve(chol(S)) %*% X 
#The inverse of the Choleski decomposition of S times the vertical vector X, 
#which in my case does nothing to the data
</code></pre>

<p>to</p>

<pre><code>X.transformed &lt;- chol(solve(S)) %*% X 
# which has negative values and gives meaningless values of X
</code></pre>

<p>Another method I've seen is transforming the dependent variable by </p>

<pre><code>chol(solve(S)) %*% Y 
</code></pre>

<p>and the independent variable by </p>

<pre><code>chol(solve(S)) %*% cbind(1,X) 
</code></pre>

<p>and doing the linear model using the transformed intercept terms in the first column of the X matrix: </p>

<pre><code>lm(Y ~ X - 1)
</code></pre>

<p>On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) I suppose it would follow from this that an $R^2$ statistic on transformed variables is also meaningless?</p>
"
"0.0603022689155527","0.097037084956597"," 22392","<p>I am learning logistic regression modeling using the book ""Applied Logistic Regression"" by Hosmer.</p>

<p>In chpaters, he suggested using Fractional Polynomials for fitting continuous variable which does not seems to be related to logit in linear fashion. I tried the <code>mfp</code> package and can give exactly the same verbose as the book. </p>

<p>But I don't know how to write the transformed variable based on the output of fractional polynomials. The book only shows example of the transformed variable when $J=2$ with $p_1=0$ and $p_2=-0.5$ (page 101) and when $J=2$ with $p_1=2$ and $p_2=2$ (page 101), But what about the others? Currently my case is $J=2$ with $p_1=-1$ and $p_2=-1$.</p>

<p>I know little about fractional polynomials and the book seems not giving sufficient hits on this part. Can anyone refer me to some place which I can know how to write the polynomial? Thanks.</p>
"
"0.168174993036504","0.162374100149152"," 23042","<p>Can someone explain my Cox model to me in plain English? </p>

<p>I fitted the following Cox regression model to <strong>all</strong> of my data using the <code>cph</code> function. My data are saved in an object called <code>Data</code>. The variables <code>w</code>, <code>x</code>, and <code>y</code> are continuous; <code>z</code> is a factor of two levels. Time is measured in months. Some of my patients are missing data for variable <code>z</code> (<em>NB</em>: I have duly noted Dr. Harrell's suggestion, below, that I impute these values so as to avoid biasing my model, and will do so in the future).</p>

<pre><code>&gt; fit &lt;- cph(formula = Surv(time, event) ~ w + x + y + z, data = Data, x = T, y = T, surv = T, time.inc = 12)

Cox Proportional Hazards Model
Frequencies of Missing Values Due to Each Variable
Surv(time, event)    w    x    y    z 
                0    0    0    0   14 

                Model Tests          Discrimination 
                                            Indexes        
Obs       152   LR chi2      8.33    R2       0.054    
Events     64   d.f.            4    g        0.437    
Center 0.7261   Pr(&gt; chi2) 0.0803    gr       1.548    
                Score chi2   8.07                      
                Pr(&gt; chi2) 0.0891                      

                   Coef    S.E.   Wald Z   Pr(&gt;|Z|)
         w      -0.0133  0.0503    -0.26     0.7914  
         x      -0.0388  0.0351    -1.11     0.2679  
         y      -0.0363  0.0491    -0.74     0.4600  
         z=1     0.3208  0.2540     1.26     0.2067
</code></pre>

<p>I also tried to test the assumption of proportional hazards by using the <code>cox.zph</code> command, below, but do not know how to interpret its results. Putting <code>plot()</code> around the command gives an error message.</p>

<pre><code> cox.zph(fit, transform=""km"", global=TRUE)
            rho chisq      p
 w      -0.1125 1.312 0.2520
 x       0.0402 0.179 0.6725
 y       0.2349 4.527 0.0334
 z=1     0.0906 0.512 0.4742
 GLOBAL      NA 5.558 0.2347
</code></pre>

<hr>

<h3>First Problem</h3>

<ul>
<li>Can someone explain the results of the above output to me in plain English? I have a medical background and no formal training in statistics.</li>
</ul>

<h3>Second Problem</h3>

<ul>
<li><p>As suggested by Dr. Harrell, I would like to internally validate my model by performing 100 iterations of 10-fold cross-validation using the <code>rms</code> package (from what I understand, this would entail building <code>100 * 10 = 1000</code> different models and then asking them to predict the survival times of patients that they had never seen).</p>

<p>I tried using the <code>validate</code> function, as shown.</p>

<pre><code>&gt; v1 &lt;- validate(fit, method=""crossvalidation"", B = 10, dxy=T)
&gt; v1
      index.orig training    test optimism index.corrected  n
Dxy      -0.2542  -0.2578 -0.1356  -0.1223         -0.1320 10
R2        0.0543   0.0565  0.1372  -0.0806          0.1350 10
Slope     1.0000   1.0000  0.9107   0.0893          0.9107 10
D         0.0122   0.0128  0.0404  -0.0276          0.0397 10
U        -0.0033  -0.0038  0.0873  -0.0911          0.0878 10
Q         0.0155   0.0166 -0.0470   0.0636         -0.0481 10
g         0.4369   0.4424  0.6754  -0.2331          0.6700 10
</code></pre>

<p>How do you perform the 100x resampling? I think my above code only performs the cross-validation once.</p></li>
<li><p>I then wanted to know how good my model was at prediction. I tried the following:</p>

<pre><code>&gt; c_index &lt;- abs(v1[1,5])/2 + 0.5
&gt; c_index
[1] 0.565984
</code></pre>

<p>Does this mean that my model is only very slightly better than flipping a coin?</p></li>
</ul>

<h3>Third Problem</h3>

<p>Dr. Harrell points out that I have assumed linearity for the covariate effects, and that the number of events in my sample is just barely large enough to fit a reliable model if all covariate effects happen to be linear.</p>

<ul>
<li>Does this mean that I should include some sort of interaction term in my model? If so, any advice as to what to put?</li>
</ul>
"
"0.162057478268133","0.144431628934824"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.100503781525921","0.097037084956597"," 25611","<p>I have a dataset with 9 continuous independent variables. I'm trying to select amongst these variables to fit a model to a single percentage (dependent) variable, <code>Score</code>. Unfortunately, I know there will be serious collinearity between several of the variables.</p>

<p>I've tried using the <code>stepAIC()</code> function in R for variable selection, but that method, oddly, seems sensitive to the order in which the variables are listed in the equation...</p>

<p>Here's my R code (because it's percentage data, I use a logit transformation for Score):</p>

<pre><code>library(MASS)
library(car)

data.tst = read.table(""data.txt"",header=T)
data.lm = lm(logit(Score) ~ Var1 + Var2 + Var3 + Var4 + Var5 + Var6 + Var7 +
             Var8 + Var9, data = data.tst)

step = stepAIC(data.lm, direction=""both"")
summary(step)
</code></pre>

<p>For some reason, I found that the variables listed at the beginning of the equation end up being selected by the <code>stepAIC()</code> function, and the outcome can be manipulated by listing, e.g., <code>Var9</code> first (following the tilde).</p>

<p>What is a more effective (and less controversial) way of fitting a model here? I'm not actually dead-set on using linear regression: the only thing I want is to be able to understand which of the 9 variables is truly driving the variation in the <code>Score</code> variable. Preferably, this would be some method that takes the strong potential for collinearity in these 9 variables into account.</p>
"
"0","0.0613716411932216"," 26762","<p>I'm reviewing a paper which has the following biological experiment. A device is used to expose cells to varying amounts of fluid shear stress. As greater shear stress is applied to the cells, more of them start to detach from the substrate. At each level of shear stress, they count the cells that remain attached, and since they know the total number of cells that were attached at the beginning, they can calculate a fractional attachment (or detachment).</p>

<p>If you plot the adherent fraction vs. shear stress, the result is a logistic curve. In theory, each individual cell is a single observation, but obviously there are thousands or tens of thousand of cells, so the data set would be gigantic, if it was set up in the usual way (with each row being an observation).</p>

<p>So, naturally, my question (as stated in the title) should make sense now. How do we do a logistic regression using the fractional outcome as the D.V.? Is there some automatic transform that can be done in glm?</p>

<p>Along the same lines, if there were potentially 3 or more (fractional) measurements, how would one do this for a multinomial logistic regression?</p>
"
"0.080403025220737","0.097037084956597"," 27400","<p>I'm reading A. Agresti (2007), <em><a href=""http://rads.stackoverflow.com/amzn/click/0471226181"">An Introduction to Categorical Data Analysis</a></em>, 2nd. edition, and am not sure if I understand this paragraph (p.106, 4.2.1) correctly (although it should be easy):</p>

<blockquote>
  <p>In Table 3.1 on snoring and heart disease in the previous chapter, 254
  subjects reported snoring every night, of whom 30 had heart disease.
  If the data file has grouped binary data, a line in the data file
  reports these data as 30 cases of heart disease out of a sample size
  of 254. If the data file has ungrouped binary data, each line in the
  data file refers to a separate subject, so 30 lines contain a 1 for
  heart disease and 224 lines contain a 0 for heart disease. The ML
  estimates and SE values are the same for either type of data file.</p>
</blockquote>

<p>Transforming a set of ungrouped data (1 dependent, 1 independent) would take more then ""a line"" to include all the information!? </p>

<p>In the following example a (unrealistic!) simple data set is created and a logistic regression model is build. </p>

<p>How would grouped data actually look like (variable tab?)? How could the same model be build using grouped data? </p>

<pre><code>&gt; dat = data.frame(y=c(0,1,0,1,0), x=c(1,1,0,0,0))
&gt; dat
  y x
1 0 1
2 1 1
3 0 0
4 1 0
5 0 0
&gt; tab=table(dat)
&gt; tab
   x
y   0 1
  0 2 1
  1 1 1
&gt; mod1=glm(y~x, data=dat, family=binomial())
</code></pre>
"
"0.201843356939833","0.203739367990215"," 27830","<p>In a previous post Iâ€™ve wondered how to <a href=""http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as"">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=""http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281"">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.
The formula is simple:</p>

<p>$logit(y)=log(\frac{y-y_{min}}{y_{max}-y})$</p>

<p>To avoid log(0) and division by 0 you extend the range by a small value, $\epsilon$. This gives an environment that respects the boundaries of the score. </p>

<p>The problem is that any $\beta$ will be in the logit scale and that makes doesnâ€™t make any sense unless transformed back into the regular scale but that means that the $\beta$ will be non-linear. For graphing purposes this doesnâ€™t matter but not with more $\beta$:s this will be very inconvenient. </p>

<p>My question:</p>

<p><strong>How do you suggest to report a logit $\beta$ without reporting the full span?</strong></p>

<hr>

<h2>Implementation example</h2>

<p>For testing the implementation Iâ€™ve written a simulation based on this basic function:</p>

<p>$outcome=\beta_0+\beta_1* xtest^3+\beta_2*sex$</p>

<p>Where $\beta_0 = 0$, $\beta_1 = 0.5$ and $\beta_2 = 1$. Since there is a ceiling in scores Iâ€™ve set any outcome value above 4 and any below -1 to the max value.</p>

<h3>Simulate the data</h3>

<pre><code>set.seed(10)
intercept &lt;- 0
beta1 &lt;- 0.5
beta2 &lt;- 1
n = 1000
xtest &lt;- rnorm(n,1,1)
gender &lt;- factor(rbinom(n, 1, .4), labels=c(""Male"", ""Female""))
random_noise  &lt;- runif(n, -1,1)

# Add a ceiling and a floor to simulate a bound score
fake_ceiling &lt;- 4
fake_floor &lt;- -1

# Just to give the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)

# Simulate the predictor
linpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == ""Female"") + random_noise
# Remove some extremes
linpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |
    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA
#limit the interval and give a ceiling and a floor effect similar to scores
linpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling
linpred[linpred &lt; fake_floor] &lt;- fake_floor
</code></pre>

<p>To plot the above:</p>

<pre><code>library(ggplot2)
# Just to give all the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)
qplot(y=linpred, x=xtest, col=gender, ylab=""Outcome"")
</code></pre>

<p>Gives this image:</p>

<p><img src=""http://i.stack.imgur.com/luZGu.png"" alt=""Scatterplot from simulation""></p>

<h3>The regressions</h3>

<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>

<pre><code>library(rms)

# Regular linear regression
fit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)
boot_fit_lm &lt;- bootcov(fit_lm, B=500)
p &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
lm_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# Quantile regression regular
fit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)
boot_rq &lt;- bootcov(fit_rq, B=500)
# A little disturbing warning:
# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique

p &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
rq_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# The logit transformations
logit_fn &lt;- function(y, y_min, y_max, epsilon)
    log((y-(y_min-epsilon))/(y_max+epsilon-y))


antilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)
    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/
        (1+exp(antiy))


epsilon &lt;- .0001
y_min &lt;- min(linpred, na.rm=T)
y_max &lt;- max(linpred, na.rm=T)
logit_linpred &lt;- logit_fn(linpred, 
                          y_min=y_min,
                          y_max=y_max,
                          epsilon=epsilon)

fit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)
boot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)


p &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))

# Change back to org. scale
transformed_p &lt;- p
transformed_p$yhat &lt;- antilogit_fn(p$yhat,
                                    y_min=y_min,
                                    y_max=y_max,
                                    epsilon=epsilon)
transformed_p$lower &lt;- antilogit_fn(p$lower, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)
transformed_p$upper &lt;- antilogit_fn(p$upper, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)

logit_rq_plot &lt;- plot.Predict(transformed_p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)
</code></pre>

<h3>The plots</h3>

<p>To compare with the base function Iâ€™ve added this code:</p>

<pre><code>library(lattice)
# Calculate the true lines
x &lt;- seq(min(xtest), max(xtest), by=.1)
y &lt;- beta1*x^3+intercept
y_female &lt;- y + beta2
y[y &gt; fake_ceiling] &lt;- fake_ceiling
y[y &lt; fake_floor] &lt;- fake_floor
y_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling
y_female[y_female &lt; fake_floor] &lt;- fake_floor

tr_df &lt;- data.frame(x=x, y=y, y_female=y_female)
true_line_plot &lt;- xyplot(y  + y_female ~ x, 
                         data=tr_df,
                         type=""l"", 
                         xlim=my_xlim, 
                         ylim=my_ylim, 
                         ylab=""Outcome"", 
                         auto.key = list(
                           text = c(""Male"","" Female""),
                           columns=2))


# Just for making pretty graphs with the comparison plot
compareplot &lt;- function(regr_plot, regr_title, true_plot){
  print(regr_plot, position=c(0,0.5,1,1), more=T)
  trellis.focus(""toplevel"")
  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)
  trellis.unfocus()
  print(true_plot, position=c(0,0,1,.5), more=F)
  trellis.focus(""toplevel"")
  panel.text(0.3, .65, ""True line"", cex = 1.2, font = 2)
  trellis.unfocus()
}

compareplot(lm_plot, ""Linear regression"", true_line_plot)
compareplot(rq_plot, ""Quantile regression"", true_line_plot)
compareplot(logit_rq_plot, ""Logit - Quantile regression"", true_line_plot)
</code></pre>

<p><img src=""http://i.stack.imgur.com/74Uid.png"" alt=""Linear regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/xHRtF.png"" alt=""Quantile regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/XfLy8.png"" alt=""Logistic quantile regression for bounded outcome""></p>

<h3>The contrast output</h3>

<p>Now I've tried to get the contrast and it's almost ""right"" but it varies along the span as expected:</p>

<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), 
+                              xtest=c(-1:1)), 
+          FUN=function(x)antilogit_fn(x, epsilon))
   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)
   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  
   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  
   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  
*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  
   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  
*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
</code></pre>
"
"0.174077655955698","0.15686828393899"," 28756","<p>I have two datasets a training and a test dataset. The dependent variable is a proportion and there are 54 predictors which are positive and negative real numbers and another 7 predictors that are text. </p>

<p>There are three response variables. Total the normalized total number of hits. Treatment the normalized total number during treatment and a percent which is a ratio of the other two responses.</p>

<p>At the moment using lm on the percent prediction data I have a corolation of .4. 85% of the varibles are within 20% of their target. For the treatment response variable using glm in poisson mode i have a correlation of .6 percent but the variables do not match the target data at all.</p>

<p>I have two main issues I need advice on: </p>

<p>(1) it rejected the text predictors because it said factor has new level(s)
I would like it to ignore the information for those that have new level but not disregard it for those that have the correct information how do i do that? </p>

<p>(2) To make my dependent variable a real number, rather than a proportion bounded between 0 and 1, I was advised to transform the response using, for example, the logit transform or the Normal quantile function (<code>qnorm</code> in R). The problem is that these transformations (and others like it) will map 0 and 1 to non-finite values. How can I model these data in a regression setting when the response is a proportion that can be 0 or 1? </p>

<p>Using linear regression with outlier removal I am able to get 2239 of 2583 testing data within 20% of their actual value I would like to have that many within 10%. </p>

<p>Using the posson distribution glm the amount of treatment correlates with 69%.</p>

<p>Ignoring this second issue for the moment, I transform the y~x1+x2 such that y=log(y/(1-y)) the correlation of my predictions to actual data drops from 6% to 2%
This is what the data looks like after the logit transform</p>

<p><img src=""http://i.stack.imgur.com/rqkaD.png"" alt=""log distribution""></p>

<p>This is what the data looks like before the log distribution
<img src=""http://i.stack.imgur.com/Mx8sh.png"" alt=""normal percentages""></p>
"
"0.0635641726163728","0.0613716411932216"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.0917469804271967","0.106298800690547"," 33265","<p>I have made 9 models using simple linear regression. I'm now checking that each of models meets the assumption of homogeneity of variance. Each of the models used either categorical or numeric (year as an integer) IVs. I carried out a Levenes test either at each level of a categorical IV or against the integer IV (year).</p>

<p>I have concluded that the IVs for which the p>0.05 show homogenous variance, while IVs for which the p&lt;0.05 show heterogenous variance. Three (out of 9) models contained IVs for which the p&lt;0.05. These were Model (1 out of 2 IVs showed p&lt;0.05), Model 2 (2 out of 2 IVs showed p&lt;0.05) and Model 3 (3 out of 3 IVs showed p&lt;0.05).</p>

<p>The following plots show these 3 models (IVs which have p&lt;0.05 only):</p>

<p><img src=""http://i.stack.imgur.com/sGWW8.jpg"" alt=""Model 1""></p>

<p><img src=""http://i.stack.imgur.com/VTSFX.jpg"" alt=""Model 2""></p>

<p><img src=""http://i.stack.imgur.com/rzVom.jpg"" alt=""Model 3""></p>

<p><strong>My questions are:</strong></p>

<p>â€¢ am I correct in concluding that the Levenes tests which gave a p&lt;0.05 indicates a violation of homogeneity of variance?</p>

<p>â€¢ in Model 1, in which only 1 out of 2 IVs gave a p&lt;0.05, need only the IV for which p&lt;0.05 be corrected (as opposed to also correcting the IV for which p>0.05)?</p>

<p>â€¢ looking at the plots, could anyone suggest possible solutions for the IVs which gave p>0.05: transformation of response variable, interaction IVs, quadratic IVs, Poisson generalised linear model, generalised additive modelling?????</p>
"
"NaN","NaN"," 33311","<p>I want to run series of simple lm in R, with a continuous/categorical outcome and binary group membership (patients - controls) and categorical predictors. 
However the categorical predictor is positively skewed and none of the transformations I have tried so far (log, ^2 etc.) has had an effect on the normality assumption. Some of the outcome variables are Error responses therefore causing the same problem.
Is it appropriate to use Nonparametric Regression (Lowess/Loess) in this case and which one would you recommend?</p>

<p>Your help would be much appreciated. </p>
"
"0.118917678002113","0.114815827304529"," 34445","<p>I'd be really grateful for recommendations of a robust package for fitting discrete choice models to a large amount ($n$ in the millions and $p$ in 2000 range) of data. I want a smoothed model that can deal with multi-colinear dependent variables and matrix inversion issues sensibly - like <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet</a>. I'm happy to bootstrap samples, which may be the only way to deal with big data in R.</p>

<p>I've tried using the <a href=""http://cran.r-project.org/web/packages/mlogit/index.html"" rel=""nofollow"">mlogit</a> package and it falls apart with more than a few hundred predictors, producing errors to do with matrix inversion.</p>

<p>My alternative is to use the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet</a> package for binary regression and then use transforms to approximate the discrete choice model using something called Begg and Gray's approximation.  </p>

<p>This data is not multinomial, in the traditional sense. It is discrete-choice, that is the classes themselves change from observation to observation -- possibly also the number of classes. Each of the classes has a set of predictors which are measured on the same scale and are class specific -- cf. <a href=""http://en.wikipedia.org/wiki/Discrete_choice"" rel=""nofollow"">Discrete Choice Models</a>. I wrote to the maintainer of <code>glmnet</code>, Trevor Hastie,  who says there is no mapping to discrete choice models in their package.</p>

<p>Another name for discrete-choice is conditional logit, with the correct parameterization. I found the package <a href=""http://cran.r-project.org/web/packages/pglm/index.html"" rel=""nofollow"">pglm</a>, but it is also lacking in robustness. There's reference, <a href=""http://stats.stackexchange.com/questions/10141/"">Discrete choice panel models in R</a> to <a href=""http://cran.r-project.org/web/packages/lme4/index.html"" rel=""nofollow"">lme4</a> also, but I have found no examples of the conditional logit with it.</p>
"
"0.134839972492648","0.130188910980824"," 35719","<p>I am just learning R. I have developed a regression model with six predictor variables. While developing it, I found the relationships are not very linear. So, maybe because of this the predictions of my model are not exact.</p>

<p>Here is Headers of my data set:</p>

<pre><code>1.bouncerate(To be predicted)
2.avgServerResponseTime
3.avgServerConnectionTime
4.avgRedirectionTime
5.avgPageDownloadTime
6.avgDomainLookupTime
7.avgPageLoadTime
</code></pre>

<p>Sample datasets:</p>

<pre><code>28.57142857,4.132,0.234,0,0.505,0,14.168
42.85714286,3.356777778,0.090777778,0.077333333,0.459,0.105444444,14.78644444
0,3.372,0.1105,0.0015,0.425,0.1305,34.3425
33.33333333,3.583,0.218,0,0.385,0.649,11.816
66.66666667,2.438,0.234,0,0.3405,0,8.645
100,2.805,0.179666667,3.203666667,0.000333333,0.11,13.47066667
66.66666667,0.977,0,0.003,0,0,12.847
0,2.776,0,7.888,0,0,14.393
100,2.59,0.261,0,0.517,0,6.216
</code></pre>

<p>Here is the summary of my model:</p>

<pre><code>Call:
lm(formula = y ~ x_1 + x_2 + x_3 + x_4 + x_5 + x_6)

Residuals:
     Min       1Q   Median       3Q      Max 
-125.302  -26.210    0.702   26.261  111.511 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 48.62944    0.27999 173.684  &lt; 2e-16 ***
x_1         -0.67831    0.08053  -8.423  &lt; 2e-16 ***
x_2          0.07476    0.49578   0.151 0.880143    
x_3         -0.22981    0.06489  -3.541 0.000399 ***
x_4          0.01845    0.09070   0.203 0.838814    
x_5          3.76952    0.67006   5.626 1.87e-08 ***
x_6          0.07698    0.01565   4.919 8.75e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 33.76 on 19710 degrees of freedom
Multiple R-squared: 0.006298,   Adjusted R-squared: 0.005995 
F-statistic: 20.82 on 6 and 19710 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>plot with all single variable are below:
<img src=""http://i.stack.imgur.com/jsW0j.png"" alt=""bouncerate vs avgServerConnectionTime"">
<img src=""http://i.stack.imgur.com/uhrya.png"" alt=""bouncerate vs ServerResponseTime"">
<img src=""http://i.stack.imgur.com/iuROe.png"" alt=""bouncerate vs avgRedirectionTime"">
<img src=""http://i.stack.imgur.com/wbfsP.png"" alt=""bouncerate vs avgDomainLookupTime"">
<img src=""http://i.stack.imgur.com/arTC6.png"" alt=""bouncerate vs avgPageLoadTime""></p>

<p>I have certain questions about this model:  </p>

<ol>
<li>Is there any way to improve the accuracy of this model?  </li>
<li>Which of the values is most useful: residual standard error, degrees of freedom, multiple R-squared, adjusted R-squared, F-statistics, or p-values for choosing best model?  </li>
<li>Is it appropriate to use polynomial transformations with these data?  </li>
<li>In case I do use polynomial terms in my model, which degree is most appropriate?  </li>
</ol>
"
"0.100503781525921","0.097037084956597"," 35990","<p>We asked 60 people to list as many restaurant franchises in Atlanta as they could.  The overall list included over 70 restaurants, but we eliminated those that were mentioned by fewer than 10% of the people, leaving us with 45.  For these 45, we calculated the proportion of informants who listed the franchise, and we're interested in modeling this proportion as a function of the franchises's (log-transformed) advertising budget and years since becoming a franchise.</p>

<p>So I wrote this code:</p>

<pre><code>model &lt;- glm ( cbind (listed, 55-listed) ~ log.budget + years, family = binomial, data = list.45)
</code></pre>

<p>As predicted, both variables exhibit strong, significant effects.</p>

<p>But even though I know that proportional data should never be modeled with OLS regression, I subsequently wrote this code:</p>

<pre><code>model.lm &lt;- lm ( proportion.55 ~ log.budget + years, data = list.45)
</code></pre>

<p>In this case, ""budget"" is still a significant predictor, but ""years"" is relatively weak and not significant.</p>

<p>It makes me worried that the confidence in the estimates is artificially inflated by the aggregation.  Doesn't the binomial glm essentially vectorize the data such that the model is based on 45 * 55 = 2,475 rows?  Is that appropriate given that there are really only 45 restaurants and 55 informants?  Would this call for mixed-effects modeling?</p>
"
"0.179786629990198","0.173585214641098"," 40453","<p>Perpendicular offset least square fitting has a lot of advantages compared to the native least square fitting scheme. The following figure illustrates the difference between there, and for a more detailed comparison of these two methods, we refer to <a href=""http://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html"" rel=""nofollow"">here</a>. </p>

<p><img src=""http://i.stack.imgur.com/Aue8i.gif"" alt=""enter image description here""></p>

<p>Perpendicular offset least square fitting, however, is not robust to outliers( points that are not supposed to be used for model estimation). Therefore, I am now considering to use a weighted perpendicular offset least square regression method. The method has two steps:</p>

<ol>
<li>Calculate the weighting factor for each points that are going to be used for line estimation;</li>
<li>Perform perpendicular offset in a weighted least square regression scheme. </li>
</ol>

<p>For the time being, my biggest problem comes from step 2. Suppose the weighting factors are given, how can I get the formula to estimate the parameters of the line? Many thanks!</p>

<p><strong>EDIT:</strong></p>

<p>Based on the kind suggestion of @MvG I have implemented the algorithm in MATLAB:</p>

<pre><code>function  line =  estimate_line_ver_weighted(pt_x, pt_y,w);
% pt_x  x coordinate
% pt_y  y coordinate
% w     weighting factor


pt_x = pt_x(:);
pt_y = pt_y(:);
w    = w(:);


% step 1: calculate n
n = sum(w(:));

% step 2: calculate weighted coordinates 
y_square = pt_y(:).*pt_y(:);
x_square = pt_x(:).*pt_x(:);
x_square_weighted = x_square.*w;  
y_square_weighted = y_square.*w;  
x_weighted        = pt_x.*w;
y_weighted        = pt_y.*w;

% step 3: calculate the formula
B_upleft = sum(y_square_weighted)-sum(y_weighted).^2/n;
B_upright = sum(x_square_weighted)-sum(x_weighted).^2/n;
B_down = sum(x_weighted(:))*sum(y_weighted(:))/n-sum(x_weighted.*pt_y);
B = 0.5*(B_upleft-B_upright)/B_down;

% step 4: calculate b
if B&lt;0
    b       = -B+sqrt(B.^2+1);
else
    b       = -B-sqrt(B.^2+1);
end

% Step 5: calculate a
a = (sum(y_weighted)-b*sum(x_weighted))/n;

% Step 6: the model is y = a + bx, and now we transform the model to 
% a*x + b*y + c = 0;
c_ = a;
a_ = b;
b_ = -1;

line = [a_ b_ c_];
</code></pre>

<p>The result is as good as we can expect, which is illustrated in the following script:</p>

<pre><code>%% Procedure 1: given the data
pt_x = [   692   692   693   692   693   693   750];
pt_y = [ 919         971        1022        1074        1126        1230        1289];

% Procedure 2: draw the point 
 close all; figure; plot(pt_x,pt_y,'b*');

% Procedure 3: estimate the line based on the weighted vertical offset
% least square method.
 weighting = ones(length(pt_x),1);
 weighting(end) = 0.01;  % we give the last point a low weighting because obvously it is an outlier
 myline =    estimate_line_ver_weighted(pt_x,pt_y,weighting); 
 a = myline(1); b = myline(2); c= myline(3);

 % Procedure 4: draw the line
 x_range = [min(pt_x):0.1:max(pt_x)];
 y_range = [min(pt_y):0.1:max(pt_y)];
 if length(x_range)&gt;length(y_range)
        x_range_corrspond = -(a*x_range+c)/b;
        hold on; plot(x_range,x_range_corrspond,'r');
 else
        y_range_correspond = -(b*y_range+c)/a;
        hold on; plot(y_range_correspond,y_range,'r');
 end
</code></pre>

<p>The following figure corresponds to the above script:
<img src=""http://i.stack.imgur.com/IIz2k.png"" alt=""enter image description here"">.</p>
"
"0.162057478268133","0.156467598012726"," 41390","<p>I am looking for a test similar to a 2-way ANOVA that would work on a binary response variable. My response variable is survival of plant seedlings (alive or dead).  My explanatory variables are Treatment (3 treatment groups) and Site (3 sites).  </p>

<p>First, I would like to know whether Treatment, Site and their interaction have a significant effect on survival.  Second, if either Treatment or Site is significant, I would like to test all pairs of treatment groups or sites to know which pairs of levels are significantly different, as I would normally do with an ANOVA.</p>

<p>I have considered several options:</p>

<ol>
<li><p>Transform the response variable, for example through an arcsin transformation, and then perform an ANOVA. This does not work on my data because at one of the sites I measure 100% survival.  Therefore there is 0 variability at this site and no transformation will change that.</p></li>
<li><p>Logistic regression with Treatment and Site recoded as dummy variables.  The results do not seem to give me a test of significance of Treatment, Site and interaction term -  Instead, I get the relative importance of each treatment group and each site separately.  Furthermore, it seems that I cannot test all the pairs of treatment groups or sites, I can only compare one ""baseline"" or ""default"" group to each of the two remaining groups.</p></li>
<li><p>Chi-square test on each explanatory variable separately.  This has the obvious drawback of not being able to test the interaction term.  Also I suspect that I am omitting important information if I am comparing survival across the 3 treatment groups without taking into account that this survival data is grouped in 3 different sites.  Does this bias the results?</p></li>
</ol>

<p>Can anyone recommend a different test or what the best approach would be in my case?</p>

<p><strong>UPDATE:</strong> Logistic regression can in fact give a test of significance of each independent variable.  In R, I discovered I can use glm to contruct a model and then the anova function to extract p-values for each IV:</p>

<pre><code>mymodel &lt;- glm(Survival ~ Treatment*Site, data=survivaldata, family=""binomial"")
anova(mymodel, test=""Chisq"")
</code></pre>
"
"0.118917678002113","0.114815827304529"," 43675","<p>I am working on a housing problem in which I use dichotomous and ratio data to predict
housing production (units constructed in a year-ratio) in a 17 year time period. At this time, I am using OLS and as I get better at stats, I shall attempt this problem using time-series analysis.  That said, I have used R to standardize all of my ratio predicting data and left the dichotomous data raw.  And I have also transformed the response variable to a Natural log to normalize the distribution (i.e. many, many zeros>>yes, I know Poisson or Zero-populated counts in the future).</p>

<p>I have read the post on ""interpret coefficients from a quantile regression on standardized data"" and also the ""convert my unstandardized independent variables to standardized."" Based on those, I think that can do the following interpretation based on the following output. The variable <code>region_id</code> is dichotomous, <code>supply</code> is standardized.</p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          2.687e+00  2.171e-01  12.379  &lt; 2e-16 ***

region_id            1.805e+00  1.383e-01  13.049  &lt; 2e-16 ***

supply              -2.205e+01  2.204e+00 -10.005  &lt; 2e-16 ***
</code></pre>

<p>Region Interpretation:<br>
For every on city that is located in the Houston region, you can expect that annual housing production will increase by 1.8%.  </p>

<p>Supply Interpretation:<br>
For every one-unit increase in the standard deviation of housing supply, you can expect that annual housing production will decrease by -22.05%.</p>

<p>Nota bene.<br>
I am not a stats or math person at all,
but I have been using R for the past three years
and I am quite familiar with OLS, but if you throw
up an equation it will look ""appropriately"" Greek to me. :)</p>
"
"0.14213381090374","0.13723116159877"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"NaN","NaN"," 47065","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros"">How should I transform non-negative data including zeros?</a>  </p>
</blockquote>



<p>I want to perform log transformation regression in R tool but the problem is that I don't know how to handle situations when data contains 0 since log 0 is infinite... Please help..</p>
"
"NaN","NaN"," 47296","<pre><code>Error in boxcox.default(y ~ x) : response variable must be positive
</code></pre>

<p>I am getting this error in <code>R</code> when I am performing a Box-Cox transformation on data.</p>

<p>Why is this error happening? Here is my <a href=""https://dl.dropbox.com/u/53624395/11.csv"" rel=""nofollow"">data</a>.  </p>

<p>This is a time series data and I have to perform logarithmic regression of the form:</p>

<p>$$y=a+b(\log x_1)+c(\log x_2)$$</p>

<p>I need to find a, b, c and then, check if any such type of relation exists or not.</p>
"
"0.330499974838372","0.324798188476895"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.0449466574975495","0.0433963036602746"," 48777","<p>I need to automate the transformation on some linear regression models. There is only one predictor in this case. Sometimes i get a good model with the original variables, sometimes i need to log the predictor, and in some cases log both sides.</p>

<p>I'm using R, so what kind of tests/packages can i use to automate this? I'm using Pearson correlation now, but i'm not sure if it makes sense.</p>

<p>thanks!</p>

<p>PS: This may look a duplicate question, but i couldn't find yet the methodology to apply.</p>
"
"0.110096376512636","0.106298800690547"," 49078","<p>I have a list of observations of females and their responses, categorized into 5 behaviors, to a potential threat. I'm wondering if, for each response type, whether the presence of an infant makes it more or less likely a female would perform that response. (E.g., we might hypothesize that females with infants are more likely to hide and less likely to go looking for food.) </p>

<p>The following was suggested:</p>

<ol>
<li>Bin females by social group (i.e., each bin would only contain
females who associated with each other).</li>
<li>Calculate the proportion of each response type observed in each
group (all these, for each group, would add to 1).</li>
<li>Arcsine-transform the proportion data.</li>
<li>Perform a t-test, for each behavior, to see whether the proportions
of females responding that way differs depending on the presence of
an infant.</li>
</ol>

<p>This sounded sketchy to me, and it <a href=""http://udel.edu/~mcdonald/stattransform.html"" rel=""nofollow"">seems</a> that <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613284/"" rel=""nofollow"">others</a> agree. The consensus seems to be that, in such cases, it's better to use multinomial regression (in this case, BEHAVIOR ~ INFANT_STATUS) to determine whether infant presence has an effect. However, I was wondering whether I could use those results determine whether (and in what direction) the presence of an infant affects the probability of each response behavior. Also, would that be possible if I were to include additional independent variables?</p>

<p>Any advice you can give, on analytical design or actual coding, is much appreciated. I've been doing this in R, but I'm more comfortable in MATLAB if that works as well. Thanks in advance.</p>
"
"0.0603022689155527","0.097037084956597"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.0518999296107682","0.0751646028002829"," 51786","<p>Does anyone know what exact data cleaning steps one need to undertake in order to clean data for a logit regression (not a logistic regression)?</p>

<p>I have only time variables, meaning year and month, as my independent variables, and I am using R.</p>

<p>A logit regression is simply a normal linear regression where the DV have been transformed with the following formula:</p>

<blockquote>
  <p><code>logit(y) = ln(y/(1-y)</code> for </p>
</blockquote>

<p>An example:</p>

<blockquote>
  <p>3 of 12 people gets cured from taking a pill in period 3 ->
  <code>ln(0.25/(1-0.25)</code></p>
  
  <p>5 of 25 people gets cured taking a pill in period 5 ->
  <code>ln(0.20/(1-0.20)</code></p>
</blockquote>

<p>One can use the logit transformation if you have ratios and in many papers and books it is closely related to the logistic regression.</p>
"
"0.100503781525921","0.097037084956597"," 52132","<p>I am currently working on a regression model where I have only categorical/factor variables as independent variables. My dependent variable is a logit transformed ratio.</p>

<p>It is fairly easy just to run a normal regression in R, as R automatically know how to code dummies as soon as they are of the type ""factor"". However this type of coding also implies that one category from each variable is used as a baseline, making it hard to interpret.</p>

<p>My professor have told me to just use effect coding instead (-1 or 1), as this implies the use of the grand mean for the intercept.</p>

<p>Does anyone know how to handle that?</p>

<p>Until now I have tried:</p>

<pre><code>gm &lt;- mean(tapply(ds$ln.crea, ds$month,  mean))
model &lt;- lm(ln.crea ~ month + month*month + year + year*year, data = ds, contrasts = list(gm = contr.sum))

Call:
lm(formula = ln.crea ~ month + month * month + year + year * 
    year, data = ds, contrasts = list(gm = contr.sum))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.89483 -0.19239 -0.03651  0.14955  0.89671 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.244493   0.204502 -15.865   &lt;2e-16 ***
monthFeb    -0.124035   0.144604  -0.858   0.3928    
monthMar    -0.365223   0.144604  -2.526   0.0129 *  
monthApr    -0.240314   0.144604  -1.662   0.0993 .  
monthMay    -0.109138   0.144604  -0.755   0.4520    
monthJun    -0.350185   0.144604  -2.422   0.0170 *  
monthJul     0.050518   0.144604   0.349   0.7275    
monthAug    -0.206436   0.144604  -1.428   0.1562    
monthSep    -0.134197   0.142327  -0.943   0.3478    
monthOct    -0.178182   0.142327  -1.252   0.2132    
monthNov    -0.119126   0.142327  -0.837   0.4044    
monthDec    -0.147681   0.142327  -1.038   0.3017    
year1999     0.482988   0.200196   2.413   0.0174 *  
year2000    -0.018540   0.200196  -0.093   0.9264    
year2001    -0.166511   0.200196  -0.832   0.4073    
year2002    -0.056698   0.200196  -0.283   0.7775    
year2003    -0.173219   0.200196  -0.865   0.3887    
year2004     0.013831   0.200196   0.069   0.9450    
year2005     0.007362   0.200196   0.037   0.9707    
year2006    -0.281472   0.200196  -1.406   0.1625    
year2007    -0.266659   0.200196  -1.332   0.1855    
year2008    -0.248883   0.200196  -1.243   0.2164    
year2009    -0.153083   0.200196  -0.765   0.4461    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3391 on 113 degrees of freedom
Multiple R-squared: 0.3626, Adjusted R-squared: 0.2385 
F-statistic: 2.922 on 22 and 113 DF,  p-value: 0.0001131 
</code></pre>
"
"0.0449466574975495","0.0650944554904119"," 52203","<p>I am doing a robust regression to predicted a left-skewed outcome.  Do I still need to do a transform on my data before doing robust regression, or will that issue be taken care of by the robust method? If so, what transform would you recommend? When I plot my residuals after doing robust regression, they still look left-skewed.  </p>

<p>I tried doing a transform ($\text{predicted.value}^4$) but even then my residuals are still left-skewed. I'm worried I'm not meeting certain assumptions of robust regression, but I'm not sure since I'm new to this method. </p>

<p>Any thoughts? Thanks!</p>
"
"0.0898933149950989","0.0867926073205492"," 52516","<p>I have a data set with the following:</p>

<p>N = 60;
x = developmental stage (range 25 to 44);
y = proportion of 10 minute trial performing a behavior (range 0 to 0.81; 30 zeros)</p>

<p>A scatterplot produces a quadratic looking curve where those in mid-development clearly performed the behavior for more time. Most of the zeros are in the youngest and oldest individuals. If I break up the data into 5 groups according to developmental stage, an ANOVA/Tukey strongly supports this pattern. However, I would like to analyze this data continuously without breaking it into groups.</p>

<p>I have considered arcsine square root transformed proportion data in a linear regression, but I am unsure if that can incorporate a quadratic term, and this analysis results in a very small R squared value (less than 0.1). I have also considered arcsine square root transformed proportion data in a GLM containing a quadratic term or a beta regression (zeros??), but am not sure where to go from here.</p>

<p>I am planning to say in the paper that the individuals in mid-development perform the behavior more than those in early or late development, but am struggling to interpret the data in a way that supports that statement.</p>

<p>I appreciate any suggestions, thank you!</p>
"
"0.0635641726163728","0.0613716411932216"," 54683","<p>I am struggling with a linear regression model of the shape $y = a + b_1\text{month} + b_2\text{year}$. I have 12 months for each year and 10 years. My dependent variable is a log transformed ratio. I have understood that much that when setting such a model up in R, R automatically picks a level for each variable to go into the intercept.  March and 2005 goes into the intercept, in order to provide a baseline for comparison for the other factors.</p>

<p>As stated my problem is that i cannot really figure out what the intercept represents. Is it simply an average of the ratio of March and 2005 or what is it?</p>

<pre><code>Residuals:                  
Min 1Q  Median  3Q  Max 
-0.90339    -0.16789    -0.00373    0.15472 0.88338 

Coefficients:                   
    Estimate    Std. Error  t   value   Pr(&gt;|t|)
(Intercept) -3.586154   0.131642    -27.242 &lt;2.00E-16   ***
MONTHJan    0.381735    0.140731    2.713   0.007875    **
MONTHFeb    0.256457    0.140731    1.822   0.071426    .
MONTHApr    0.072824    0.140731    0.517   0.605981    
MONTHMay    0.207984    0.140731    1.478   0.142613    
MONTHJun    -0.008194   0.140731    -0.058  0.953686    
MONTHJul    0.363693    0.140731    2.584   0.011217    *
MONTHAug    0.195791    0.140731    1.391   0.16727 
MONTHSep    0.212562    0.140731    1.51    0.134124    
MONTHOct    0.124234    0.140731    0.883   0.379495    
MONTHNov    0.204009    0.140731    1.45    0.15032 
MONTHDec    0.175348    0.140731    1.246   0.215711    
YEAR1999    0.477663    0.128469    3.718   0.000333    ***
YEAR2000    -0.027343   0.128469    -0.213  0.83189 
YEAR2001    -0.166637   0.128469    -1.297  0.197612    
YEAR2002    -0.060508   0.128469    -0.471  0.638684    
YEAR2003    -0.173492   0.128469    -1.35   0.179948    
YEAR2004    0.003592    0.128469    0.028   0.977753    
YEAR2006    -0.283261   0.128469    -2.205  0.029776    *
YEAR2007    -0.267752   0.128469    -2.084  0.03972 *
YEAR2008    -0.240654   0.128469    -1.873  0.063985    .
---                 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1                  

Residual standard error: 0.3147 on 99 degrees of freedom                    
Multiple R-squared: 0.4167,                 
Adjusted R-squared: 0.2988                  
F-statistic: 3.536 on 20 and 99 DF,  p-value: 1.491e-05 
</code></pre>
"
"0.0449466574975495","0.0433963036602746"," 55240","<p>I'm working on a data set modeling road kills (0 = random point, 1 = road kill) as a function of a number of habitat variables.  Following Hosmer and Lemeshow, I've examined each continuous predictor variable for linearity, and a couple appear nonlinear.  I'd like to try a fractional polynomial transformation for each, also following Hosmer and Lemeshow, and have looked at the R package mfp, but I'm having trouble coming up with (and understanding) the R code that will correctly transform the variable.  Can anyone suggest R code that would help me accomplish the concepts on p. 101 - 102 of Hosmer and Lemeshow's Applied Logistic Regression (2000).  Thanks!</p>
"
"0.135519271363624","0.130844778663143"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.220847116289638","0.229631654609057"," 56237","<p>I'm a beginner in R and Im wondering how to interprete my results.....
My question is about the results that I got after I did a regression on the Translog production function for panel data:
$ log(y)=log(A) + \alpha_{K} log(K) + \alpha_{L} log(L) + \beta_{KL} log(K)log(L) + \beta_{L^2} log^2(L) + \beta_{K^2} log^2(K)$</p>

<p>L stands for labour and K for Kapital.</p>

<p>The results I got for the Within, Random and first difference a the following:
Within:</p>

<pre><code>  #Within
    Coefficients :
  Estimate  Std. Error  t-value Pr(&gt;|t|)    
     K   1.0902e-05  1.0654e-06  10.2326   &lt;2e-16 ***
     L  -2.4009e-06  1.5086e-07 -15.9150   &lt;2e-16 ***
     LK  1.9788e-03  3.6069e-03   0.5486   0.5833    
     LL  3.0511e-02  1.3141e-03  23.2173   &lt;2e-16 ***
     KK  5.0333e-02  2.6650e-03  18.8868   &lt;2e-16 ***
     ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

    Total Sum of Squares:    6886.3
        Residual Sum of Squares: 1983.9
  R-Squared      :  0.71191 
  Adj. R-Squared :  0.69692 
    F-statistic: 10729.1 on 5 and 21709 DF, p-value: &lt; 2.22e-16


&gt; #regression random translog
&gt; tl.random&lt;-plm(Y ~ K + L + LK + LL + KK, data=panel, model=""random"")
 &gt; summary(tl.random)
 Oneway (individual) effect Random Effect Model 
(Swamy-Aroras transformation)

 Call:
 plm(formula = Y ~ K + L + LK + LL + KK, data = panel, model = ""random"")

  Balanced Panel: n=462, T=48, N=22176

  Effects:
               var std.dev share
  idiosyncratic 0.09139 0.30230 0.397
   individual    0.13856 0.37224 0.603
  theta:  0.8836  

  Residuals :
Min.  1st Qu.   Median  3rd Qu.     Max. 
 -3.16000 -0.14200  0.00724  0.15400  4.89000 

   Coefficients :
                 Estimate  Std. Error  t-value Pr(&gt;|t|)    
   (Intercept)  1.6266e+00  3.9030e-02  41.6763   &lt;2e-16 ***
    K            9.0932e-06  1.0552e-06   8.6178   &lt;2e-16 ***
    L           -2.5192e-06  1.5023e-07 -16.7684   &lt;2e-16 ***
   LK           2.7566e-03  3.6102e-03   0.7636   0.4451    
   LL           2.9491e-02  1.3138e-03  22.4474   &lt;2e-16 ***
   KK           4.8817e-02  2.6659e-03  18.3117   &lt;2e-16 ***
   ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

  Total Sum of Squares:    7183.6
  Residual Sum of Squares: 2070.2
  R-Squared      :  0.71181 
  Adj. R-Squared :  0.71162 
  F-statistic: 10951.9 on 5 and 22170 DF, p-value: &lt; 2.22e-16

  &gt; #regression first difference translog
   &gt; tl.fd&lt;-plm(Y ~ K + L + LK + LL + KK-1, data=panel, model=""fd"")
   &gt; summary(tl.fd)
    Oneway (individual) effect First-Difference Model


       #First difference regression
     Call:
      plm(formula = Y ~ K + L + LK + LL + KK - 1, data = panel, model = ""fd"")

      Balanced Panel: n=462, T=48, N=22176

      Residuals :
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    -1.4900 -0.0321  0.0199  0.0202  0.0715  0.9860 

          Coefficients :
          Estimate  Std. Error t-value  Pr(&gt;|t|)    
      K   2.3847e-07  2.8965e-06  0.0823 0.9343856    
      L  -8.0238e-07  2.3128e-07 -3.4693 0.0005229 ***
     LK -2.6986e-02  6.7755e-03 -3.9829 6.831e-05 ***
     LL  5.6920e-02  2.3933e-03 23.7830 &lt; 2.2e-16 ***
     KK  3.7811e-02  5.1254e-03  7.3773 1.674e-13 ***
    ---
       Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

        Total Sum of Squares:    426.54
       Residual Sum of Squares: 269.92
        R-Squared      :  0.38799 
          Adj. R-Squared :  0.3879 
</code></pre>

<p>My question are:</p>

<p>1) Is there a reason why the estimation for coefficient for LK is not significant in both within and random? but in first diff?</p>

<p>2) Why give within and random so similar results, and why first difference is different from them?</p>

<p>3)Can I interpret Standard error and R squared?
Is there anything else I can interpret? Which is the best model of the three?</p>

<p>Thank you so much for your help! </p>
"
"0.0778498944161523","0.0751646028002829"," 56935","<p><img src=""http://i.stack.imgur.com/dDglK.jpg"" alt=""Histogram of my data""></p>

<p>I have a set of data which in my opinion can be treated as coming from a Poisson distribution (they are all positive and represent days). I need to perform several regressions, with R, using these data as dependent variables. I tried linear models (manipulating the data using the Box-Cox transformation) and <code>glm</code> with all possible families but when I check if the residuals are normally distributed (using Shapiro's test) the answer is always negative. Any idea about any other possible distribution family? Thanks!</p>
"
"0.0635641726163728","0.0613716411932216"," 57688","<p>I have a data set from a repeated measures experimental design with different sets of stimuli. I want to know how strong the association between the continuous dependent variable and the continuous predictor is while accounting for the interindividual and interstimulus variation.</p>

<p>My <code>lmer</code> model description in <code>R</code> looks like this</p>

<pre><code>dv ~ pred + (1 |Â subject) + (1 | stimulus) 
</code></pre>

<p><em>Question 1</em>: I understand that it is non-trivial to calculate R squared for random intercept-slope models. Is the same true for random intercept models? Is there an R-implementation of any of the available methods?</p>

<p><em>Question 2</em>: If I z-transform my dependent variable and predictor, will the parameter estimate for the fixed effect reflect the strength of the association such as it would in an ordinary regression model? <strong>Update</strong>: I think I phrased this questions too vaguely. I was wondering if scaling variables would yield standardized regression estimates. I found this question has been answered before in <a href=""http://stats.stackexchange.com/questions/22346/standardized-beta-weights-for-a-multilevel-regression"">another question</a>.</p>

<p><em>Question 3</em>: Is there an entirely different/more appropriate way to quantify the association strength while controlling for the interindividual and interstimulus variation?</p>
"
"0.127920429813366","0.13723116159877"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.0917469804271967","0.106298800690547"," 60154","<p>I have proportion data (percentage viewership of TV programs) that i'd like to model as a function of various demographics (age, sex etc.) and time (year). After surveying options for appropriate multiple regression models, I'm debating between the following two strategies:</p>

<p>1) fit a beta regression model after dividing the percentage data by 100 and adjusting the range slightly so that values of zero and one do not occur.</p>

<p>2) fit an OLS model after logit transforming the percentage data (again, divided by 100 and adjusted slightly) so that the dependent variable is mapped to the Real line.</p>

<p>One key consideration is that i'd like to make the results as intuitive as possible to a non-statistical audience. So, interpretations such as ""for a one unit change in X we get a percent change in Y"", or something like that, would be most welcome.</p>

<p>Can anyone outline pros and cons of these two approaches in this regard? </p>

<p>It seems to me that using beta regression with a logit link, and then calculating odds ratios may lead to nice ""percent change"" explanations. The coefs from the OLS model would also be on the ln(odds) scale, so I assume I could also do the same for that model. My data are too large to share, but I ran both models in R and there are only minor differences in the coefs.</p>
"
"0.100503781525921","0.0776296679652776"," 61217","<p>I am trying to perform a multiple regression in <code>R</code>. However, my dependent variable has the following plot:</p>

<p><img src=""http://i.stack.imgur.com/AMXDm.jpg"" alt=""DV""></p>

<p>Here is a scatterplot matrix with all my variables (<code>WAR</code> is the dependent variable):</p>

<p><img src=""http://i.stack.imgur.com/qKsGL.jpg"" alt=""SPLOM""></p>

<p>I know that I need to perform a transformation on this variable (and possibly the independent variables?) but I am not sure of the exact transformation required. Can someone point me in the right direction? I am happy to provide any additional information about the relationship between the independent and dependent variables.</p>

<p>The diagnostic graphics from my regression look as follows:</p>

<p><img src=""http://i.stack.imgur.com/sduyK.jpg"" alt=""Diagnostic plots""></p>

<p><strong>EDIT</strong></p>

<p>After transforming the dependent and independent variables using Yeo-Johnson transformations, the diagnostic plots look like this:</p>

<p><img src=""http://i.stack.imgur.com/6WZTC.jpg"" alt=""After transforming""></p>

<p>If I use a GLM with a log-link, the diagnostic graphics are:</p>

<p><img src=""http://i.stack.imgur.com/SjfdK.jpg"" alt=""GLM with log-link""></p>
"
"0.130813988033546","0.168402394120179"," 61547","<p><strong>EDIT: Since making this post, I have followed up with an additional post <a href=""http://stats.stackexchange.com/questions/61711/fitting-a-zero-inflated-negative-binomial-regression-with-r"">here</a>.</strong></p>

<p><strong>Summary of the text below: I am working on a model and have tried linear regression, Box Cox transformations and GAM but have not made much progress</strong></p>

<p>Using <code>R</code>, I am currently working on a model to predict the success of minor league baseball players at the major league (MLB) level. The dependent variable, offensive career wins above replacement (oWAR), is a proxy for success at the MLB level and is measured as the sum of offensive contributions for every play the player is involved in over the course of his career (details here - <a href=""http://www.fangraphs.com/library/misc/war/"" rel=""nofollow"">http://www.fangraphs.com/library/misc/war/</a>). The independent variables are z-scored minor league offensive variables for statistics that are thought to be important predictors of success at the major league level including age (players with more success at a younger age tend to be better prospects), strike out rate [SOPct], walk rate [BBrate] and adjusted production (a global measure of offensive production). Additionally, since there are multiple levels of the minor leagues, I have included dummy variables for the minor league level of play (Double A, High A, Low A, Rookie and Short Season with Triple A [the highest level before the major leagues] as the reference variable]). Note: I have re-scaled WAR to to be a variable that goes from 0 to 1. </p>

<p>The variable scatterplot is as follows:</p>

<p><img src=""http://i.imgur.com/IJNowem.jpg"" alt=""scatterplot""></p>

<p>For reference, the dependent variable, oWAR, has the following plot:</p>

<p><img src=""http://i.stack.imgur.com/AMXDm.jpg"" alt=""dependentvariableplot""></p>

<p>I started with a linear regression <code>oWAR = B1zAge + B2zSOPct + B3zBBPct + B4zAdjProd + B5DoubleA + B6HighA + B7LowA + B8Rookie + B9ShortSeason</code> and obtained the following diagnostics plots: </p>

<p><img src=""http://i.imgur.com/U6ABt5a.jpg"" alt=""linearRegressionDiagnostics""></p>

<p>There are clear problems with a lack of unbiasedness of the residuals and a lack of random variation. Additionally, the residuals are not normal. The results of the regression are shown below:</p>

<p><img src=""http://i.imgur.com/3kW3ZAO.jpg"" alt=""linearRegressionResults""></p>

<p>Following the advice in a previous <a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">thread</a>, I tried a Box-Cox transformation with no success. Next, I tried a GAM with a log link and received these plots:</p>

<p><img src=""http://i.imgur.com/hWQAS2d.jpg"" alt=""splines""></p>

<p><strong>Original</strong>
<img src=""http://i.imgur.com/y6KVJtB.jpg"" alt=""diagnosticChecksGAM""></p>

<p><strong>New Diagnostic Plot</strong>
<img src=""http://i.imgur.com/UKD2ycB.jpg"" alt=""GAMDiag""></p>

<p>It looks like the splines helped fit the data but the diagnostic plots still show a poor fit. <strong>EDIT: I thought I was looking at the residuals vs fitted values originally but I was incorrect. The plot that was originally shown is marked as Original (above) and the plot I uploaded afterwards is marked as New Diagnostic Plot (also above)</strong></p>

<p><img src=""http://i.imgur.com/78BObfU.jpg"" alt=""GAMResults""></p>

<p>The $R^2$ of the model has increased</p>

<p>but the results produced by the command <code>gam.check(myregression, k.rep = 1000)</code> are not that promising. </p>

<p><img src=""http://i.imgur.com/XoubLbh.jpg"" alt=""GAMResults2""></p>

<p>Can anyone suggest a next step for this model? I am happy to provide any other information that you think might be useful to understand the progress I've made thus far. Thanks for any help you can provide. </p>
"
"0.305823268090656","0.312990913144388"," 62106","<p>I have been working on a baseball model to predict success at the major league level using minor league statistics. After posting multiple threads on this site (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>, <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">2</a>, <a href=""http://stats.stackexchange.com/questions/61711/fitting-a-zero-inflated-negative-binomial-regression-with-r"">3</a>) and receiving valuable feedback, I have settled on a zero-inflated negative binomial model as being the best fit for my data.</p>

<p>For those who do not want to go back through old threads,  I will recap some of the story here. Also, for those who have read the old threads, some of the details regarding the variables I am using have changed. </p>

<p>In my model, the dependent variable, offensive career wins above replacement (oWAR), is a proxy for success at the MLB level and is measured as the sum of offensive contributions for every play the player is involved in over the course of his career (details here - <a href=""http://www.fangraphs.com/library/misc/war/"" rel=""nofollow"">http://www.fangraphs.com/library/misc/war/</a>). The independent variables are z-scored minor league offensive variables for statistics that are thought to be important predictors of success at the major league level including age (players with more success at a younger age tend to be better prospects), strike out rate [SOPct], walk rate [BBPct] and adjusted production (a global measure of offensive production). Additionally, since position is an important determinant of whether a players makes the major leagues (those who play at easier positions will be required to perform at a higher offensive level in order to have the same value as a player at a more difficult position), I have included dummy variables to account for position. Note that I have not included the position dummy in the count portion of the model as the oWAR dependent variable has already been adjusted for the difficulty level of the position played by the player. </p>

<p><strong>EDIT: I have added the paragraphs below in response to the following comments in the answer below:</strong></p>

<p><em>""I see you do not include the same covariates in the Logit and the negative binomial process - why not? Ususally, each relevant predictor would be expected to influence both processes.""</em></p>

<p>I think it would help if I explained the data generating process. A player plays in the minor leagues. At some point, when they have demonstrated enough skill at the minor league level (this is a combination of statistical success and observed traits that scouts believe will allow them to be successful at the major league level), a player is promoted to the major leagues. At this point, they have an opportunity to accrue oWAR. At this point, the first data-generating process  (captured by the logit model) ends. Now, a different data generating process takes over, whereby players accumulate oWAR depending on how they play at the major league level. Some players will not perform well and accumulate zero oWAR, the same as a player who did not make the majors. That is one of the reasons I think this model is appropriate. It is not necessarily easy to separate a player who accumulates zero because they aren't good enough to make the major leagues from a player who makes the major leagues but does not succeed at that level (and still ends his career with zero oWAR). I have not included the positional dummy in the count part of the model because the oWAR measure is already adjusted for the position played by the player whereas the minor league statistics are not. When I tried testing them in the model, they were, not surprisingly, not significant. I omitted the BB Pct statistic from the logit part of the model as it was not significant (p = 0.22)</p>

<p><em>""Since your data appears to be a panel (you observe players/teams repeatedly), you can think about more sophisticated stuff like fixed or random effects. ""</em></p>

<p>In terms of how the data is sampled, a player can be in the dataset once (if they spend only a year at the level of the minor leagues) or multiple times if they take multiple years to advance. After reading up on fixed vs random effects model, I don't see how I can used a fixed model to predict out of sample players. However, I am sure that there are fixed effects (effects determined by the player that are not captured in the dependent variables) so I don't fully understand how to handle that issue.</p>

<p><strong>END EDIT</strong></p>

<p>After trying a linear, Box-Cox transformed and basic GLM model with generally poor results, I was directed to the zero-inflated negative binomial distribution set of models. After trying out different combination of variables and following the steps in this excellent <a href=""http://www.jstatsoft.org/v27/i08/paper"" rel=""nofollow"">step-by-step</a> guide for regression models for count data in R, I settled on the following model (shown below).</p>

<p><strong>Model</strong>
<img src=""http://i.imgur.com/neOP2RE.jpg"" alt=""model1""></p>

<p>Furthermore, when I re-estimate the standard errors using sandwich standard errors, the model still appears to have appropriate independent variables.</p>

<p><strong>Sandwich Standard Errors</strong>
<img src=""http://i.imgur.com/47JjOae.jpg"" alt=""sandwich""></p>

<p>At this point, I do not think I will find a better model type given the dataset I have. However, I am still left with some issues. The first issue may be a function of the dataset I am using. In most examples I have seen that discuss zero-inflated data, there are clearly more zeros than other values (hence, the name). However, the number of zeros still appears to be less than 50% of the total dependent variables and usually not even that high. In my dataset, approximately 87% of the dependent variables are zero i.e., it is hard to have success in major league baseball. I am guessing the model should technically be able to account for this scenario (albeit with less predictive value than a model with more non-zeros) but I am not sure how to check if that is the case. When I create a plot of fitted values and Pearson residuals, and a plot of fitted values and raw residuals, they appear as below:</p>

<p><strong>Fitted vs Pearson residuals</strong>
<img src=""http://i.imgur.com/tlx6ibn.jpg"" alt=""FvP""></p>

<p><strong>Fitted vs raw residuals</strong>
<img src=""http://i.imgur.com/vL8OIcV.jpg"" alt=""FvR""></p>

<p>Not knowing exactly what these plots look like in a good-fitting regression, I decided to take the sample data described <a href=""http://www.ats.ucla.edu/stat/r/dae/zipoisson.htm"" rel=""nofollow"">here</a> and examine the plots in an example where I know the fit has been deemed to be good. </p>

<p><strong>Fitted vs Pearson residuals - Sample problem</strong>
<img src=""http://i.imgur.com/TJh2zHF.jpg"" alt=""FvPEx""></p>

<p><strong>Fitted vs raw residuals - Sample problem</strong>
<img src=""http://i.imgur.com/YJMxJl9.jpg"" alt=""FvREx""></p>

<p>Clearly, these plots do not look that similar to mine. I am not sure how much has to do with a) model misspecification b) the fact that my dependent variable has 87% zeros and c) the fact that this is a simple sample problem designed to perfectly fit this model whereas my data is messy, real world data. Any thoughts on this issue would be appreciated. </p>

<p>My second issue, which I am not sure if I should be tackling after or simultaneously with the first issue, has to do with the functional form specification. I don't know if my independent variables are in the right form. It has been suggested to me by a friend that I could try a) multiple fractional polynomials with loops or b) informally play around with adding polynomials of covariates, interactions, etc. My issue at this point is that I do not know how to implement point a in R and and I am not sure which forms to try for point b besides randomly choosing some. Once again, help on this separate (but related?) issue would be greatly appreciated. </p>

<p>If anyone has any questions, I will do my best to answer them. In my first post (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>), I mentioned I could not provide the dataset but I have been given permission to do so if anyone wants to take a look. Thanks again. </p>
"
"0.162472478891985","0.15686828393899"," 63226","<p>I am trying to model some data regarding a predator prey interaction experiment (n=26). Predation rate is my response variable and I have 4 explanatory variables: predator density (1,2,3,4 5), predator size, prey density (5,10,15,20,25,30) and prey type (3 categories). I started with several linear models (GLM) and found (as expected) that prey and predator density were non-linearly related to predation rates. If I use a log transformation on these variables I get really nice curves and an adjusted $R^{2}$ of 0.82, but it is not really the right approach for modelling non-linear relationships.</p>

<pre><code>model &lt;-glm(rates ~ log(pred) + log (prey) + type)
</code></pre>

<p>Therefore I switched to non-linear least square regression (<code>nls</code>). I have several predator-prey models based on existing ecological literature e.g.:</p>

<pre><code> ### Holling's type II functional response

model1 &lt;- nls(rates ~ (a * prey)/(1 + b * prey),
start = list(a = 0.27,b = 0.13), trace = TRUE)

### Beddington-DeAngelis functional response

model2 &lt;- nls(rates ~ (a*prey)/(1+ (b * prey) + c * (pred -1 )),
start = list(a=0.22451, b=-0.18938, c=1.06941), trace=TRUE, subset=I1) 
</code></pre>

<p>These models work perfectly, but now I want to add prey type as well. In the linear models prey type was the most important variable so I don't want to leave it out. I understand that you can't add categorical variables in nls, so I thought I try a generalized additive model (GAM).</p>

<p>The problem with the gam models is that the smoothers (both spline and loess) don't work on both variables because there are only a very restricted number of values for prey density and predator density. I can manage to get a model with a single variable smoothed using loess. But for two variables it is simply not working. The spline function does not work at all because I have so few values (5) for my variables (see model 4).</p>

<pre><code>model3 &lt;- gam(rates~ lo(pred, span=0.9)+prey)
## this one is actually working but does not include a smoother for prey.

model4 &lt;- gam(rates~ s(pred)+prey)
## this one gives problems: 
A term has fewer unique covariate combinations than specified maximum degrees of freedom
</code></pre>

<p>My question is: are there any other possibilities to model data with 2 non-linear related variables in which I can also include a categorical variable. I would prefer to use <code>nls</code> (<code>model2</code>) with for example different intercepts for each category but I'm not sure how to get this sorted, if it is possible at all. The dataset is too small to split it up into the three categories, moreover, one of the categories only contains 5 data points.</p>

<p>Any help would be really appreciated.</p>
"
"0.201235851101624","0.203546706606235"," 63233","<h2>Background</h2>

<p>In a paper from Epstein (1991): <a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%281991%29004%3C0365%3AOODCVF%3E2.0.CO%3B2"" rel=""nofollow"">On obtaining daily climatological values from monthly means</a>, the formulation and an algorithm for calculating Fourier interpolation for periodical and even-spaced values are given.</p>

<p>In the paper, the goal is to <strong>obtain daily values from monthly means</strong> by interpolation.</p>

<p>In short, it is assumed that unknown daily values can be represented by the sum of harmonic components:
$$
y(t) = a_{0} + \sum_{j}\left[a_{j}\,\cos(2\pi jt/12)+b_{j}\,\sin(2\pi jt/12)\right]
$$
In the paper $t$ (time) is expressed in months.</p>

<p>After some derviation, it is shown that the terms can be calculated by:
$$
\begin{align}
a_{0} &amp;= \sum_{T}Y_{T}/12 \\
a_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\cos(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
b_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\sin(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
a_{6} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right]\times \sum_{T}\left[Y_{T}\cos(\pi T)/12\right] \\
b_{6} &amp;= 0
\end{align}
$$
Where $Y_{T}$ denote the monthly means and $T$ the month.</p>

<p><a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281995%29123%3C2251%3ATIODSU%3E2.0.CO%3B2"" rel=""nofollow"">Harzallah (1995)</a> summarizes this aproach as follows: ""The interpolation is carried out by adding zeros to the spectral coefficients of data and by performing an inverse Fourier transform to the resulting extended coefficients. The method is equivalent to applying a rectangular filter to Fourier coefficients.""</p>

<hr>

<h2>Questions</h2>

<p>My goal is to use the above methodology for interpolation of <strong>weekly means to obtain daily data</strong> (see <a href=""http://stats.stackexchange.com/questions/59418/interpolation-of-influenza-data-that-conserves-weekly-mean/63135#63135"">my previous question</a>). In summary, I have 835 weekly means of count data (see the example dataset at the bottom of the question). There are quite a few things that I don't understand before I can apply the approach outlined above:</p>

<ol>
<li>How would the formulas have to be changed for my situation (weekly instead of monthly values)?</li>
<li>How could the time $t$ be expressed? I assumed $t/835$ (or $t/n$ with $n$ data points in general), is that correct?</li>
<li>Why does the author calculate 7 terms (i.e. $0\leq j \leq 6$)? How many terms would I have to consider?</li>
<li>I understand that the question can probably be solved by using a <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> and using the predictions for interpolation (thanks to Nick). Still, some things are unclear to me: How many terms of harmonics should be included in the regression? And what period should I take? How can the regression be done to ensure that the weekly means are preserved (as I don't want an exact harmonic fit to the data)?</li>
</ol>

<p>Using the <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> (which is also explained in <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0116"" rel=""nofollow"">this paper</a>), I managed to get an exact harmonic fit to the data (the $j$ in my example would run through $1, \ldots, 417$, so I fitted 417 terms). <strong>How can this approach be modified -$~$if possible$~$- to achieve the conservation of the weekly means?</strong> Maybe by applying correction factors to each regression term?</p>

<p>The plot of the exact harmonic fit is:</p>

<p><img src=""http://i.stack.imgur.com/7XuxU.png"" alt=""Exact harmonic fit""></p>

<p><strong>EDIT</strong></p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/signal/"" rel=""nofollow"">signal package</a> and the <code>interp1</code> function, here's what I've managed to do using the example data set from below (many thanks to @noumenal). I use <code>q=7</code> as we have weekly data:</p>

<pre><code># Set up the time scale

daily.ts &lt;- seq(from=as.Date(""1995-01-01""), to=as.Date(""2010-12-31""), by=""day"")

# Set up data frame 

ts.frame &lt;- data.frame(daily.ts=daily.ts, wdayno=as.POSIXlt(daily.ts)$wday,
                       yearday = 1:5844,
                       no.influ.cases=NA)

# Add the data from the example dataset called ""my.dat""

ts.frame$no.influ.cases[ts.frame$wdayno==3] &lt;- my.dat$case

# Interpolation

case.interp1 &lt;- interp1(x=ts.frame$yearday[!is.na(ts.frame$no.influ.case)],y=(ts.frame$no.influ.cases[!is.na(ts.frame$no.influ.case)]),xi=ts.frame$yearday, method = c(""cubic""))

# Plot subset for better interpretation
par(bg=""white"", cex=1.2, las=1)
plot((ts.frame$no.influ.cases)~ts.frame$yearday, pch=20,
     col=grey(0.4),
     cex=1, las=1,xlim=c(0,400), xlab=""Day"", ylab=""Influenza cases"")
lines(case.interp1, col=""steelblue"", lwd=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R1FE8.png"" alt=""Cubicinterpo""></p>

<p>There are two issues here:</p>

<ol>
<li>The curve seem to fit ""too good"": it goes through every point </li>
<li>The weekly means are not conserved</li>
</ol>

<p><strong>Example dataset</strong></p>

<pre><code>structure(list(date = structure(c(9134, 9141, 9148, 9155, 9162, 
9169, 9176, 9183, 9190, 9197, 9204, 9211, 9218, 9225, 9232, 9239, 
9246, 9253, 9260, 9267, 9274, 9281, 9288, 9295, 9302, 9309, 9316, 
9323, 9330, 9337, 9344, 9351, 9358, 9365, 9372, 9379, 9386, 9393, 
9400, 9407, 9414, 9421, 9428, 9435, 9442, 9449, 9456, 9463, 9470, 
9477, 9484, 9491, 9498, 9505, 9512, 9519, 9526, 9533, 9540, 9547, 
9554, 9561, 9568, 9575, 9582, 9589, 9596, 9603, 9610, 9617, 9624, 
9631, 9638, 9645, 9652, 9659, 9666, 9673, 9680, 9687, 9694, 9701, 
9708, 9715, 9722, 9729, 9736, 9743, 9750, 9757, 9764, 9771, 9778, 
9785, 9792, 9799, 9806, 9813, 9820, 9827, 9834, 9841, 9848, 9855, 
9862, 9869, 9876, 9883, 9890, 9897, 9904, 9911, 9918, 9925, 9932, 
9939, 9946, 9953, 9960, 9967, 9974, 9981, 9988, 9995, 10002, 
10009, 10016, 10023, 10030, 10037, 10044, 10051, 10058, 10065, 
10072, 10079, 10086, 10093, 10100, 10107, 10114, 10121, 10128, 
10135, 10142, 10149, 10156, 10163, 10170, 10177, 10184, 10191, 
10198, 10205, 10212, 10219, 10226, 10233, 10240, 10247, 10254, 
10261, 10268, 10275, 10282, 10289, 10296, 10303, 10310, 10317, 
10324, 10331, 10338, 10345, 10352, 10359, 10366, 10373, 10380, 
10387, 10394, 10401, 10408, 10415, 10422, 10429, 10436, 10443, 
10450, 10457, 10464, 10471, 10478, 10485, 10492, 10499, 10506, 
10513, 10520, 10527, 10534, 10541, 10548, 10555, 10562, 10569, 
10576, 10583, 10590, 10597, 10604, 10611, 10618, 10625, 10632, 
10639, 10646, 10653, 10660, 10667, 10674, 10681, 10688, 10695, 
10702, 10709, 10716, 10723, 10730, 10737, 10744, 10751, 10758, 
10765, 10772, 10779, 10786, 10793, 10800, 10807, 10814, 10821, 
10828, 10835, 10842, 10849, 10856, 10863, 10870, 10877, 10884, 
10891, 10898, 10905, 10912, 10919, 10926, 10933, 10940, 10947, 
10954, 10961, 10968, 10975, 10982, 10989, 10996, 11003, 11010, 
11017, 11024, 11031, 11038, 11045, 11052, 11059, 11066, 11073, 
11080, 11087, 11094, 11101, 11108, 11115, 11122, 11129, 11136, 
11143, 11150, 11157, 11164, 11171, 11178, 11185, 11192, 11199, 
11206, 11213, 11220, 11227, 11234, 11241, 11248, 11255, 11262, 
11269, 11276, 11283, 11290, 11297, 11304, 11311, 11318, 11325, 
11332, 11339, 11346, 11353, 11360, 11367, 11374, 11381, 11388, 
11395, 11402, 11409, 11416, 11423, 11430, 11437, 11444, 11451, 
11458, 11465, 11472, 11479, 11486, 11493, 11500, 11507, 11514, 
11521, 11528, 11535, 11542, 11549, 11556, 11563, 11570, 11577, 
11584, 11591, 11598, 11605, 11612, 11619, 11626, 11633, 11640, 
11647, 11654, 11661, 11668, 11675, 11682, 11689, 11696, 11703, 
11710, 11717, 11724, 11731, 11738, 11745, 11752, 11759, 11766, 
11773, 11780, 11787, 11794, 11801, 11808, 11815, 11822, 11829, 
11836, 11843, 11850, 11857, 11864, 11871, 11878, 11885, 11892, 
11899, 11906, 11913, 11920, 11927, 11934, 11941, 11948, 11955, 
11962, 11969, 11976, 11983, 11990, 11997, 12004, 12011, 12018, 
12025, 12032, 12039, 12046, 12053, 12060, 12067, 12074, 12081, 
12088, 12095, 12102, 12109, 12116, 12123, 12130, 12137, 12144, 
12151, 12158, 12165, 12172, 12179, 12186, 12193, 12200, 12207, 
12214, 12221, 12228, 12235, 12242, 12249, 12256, 12263, 12270, 
12277, 12284, 12291, 12298, 12305, 12312, 12319, 12326, 12333, 
12340, 12347, 12354, 12361, 12368, 12375, 12382, 12389, 12396, 
12403, 12410, 12417, 12424, 12431, 12438, 12445, 12452, 12459, 
12466, 12473, 12480, 12487, 12494, 12501, 12508, 12515, 12522, 
12529, 12536, 12543, 12550, 12557, 12564, 12571, 12578, 12585, 
12592, 12599, 12606, 12613, 12620, 12627, 12634, 12641, 12648, 
12655, 12662, 12669, 12676, 12683, 12690, 12697, 12704, 12711, 
12718, 12725, 12732, 12739, 12746, 12753, 12760, 12767, 12774, 
12781, 12788, 12795, 12802, 12809, 12816, 12823, 12830, 12837, 
12844, 12851, 12858, 12865, 12872, 12879, 12886, 12893, 12900, 
12907, 12914, 12921, 12928, 12935, 12942, 12949, 12956, 12963, 
12970, 12977, 12984, 12991, 12998, 13005, 13012, 13019, 13026, 
13033, 13040, 13047, 13054, 13061, 13068, 13075, 13082, 13089, 
13096, 13103, 13110, 13117, 13124, 13131, 13138, 13145, 13152, 
13159, 13166, 13173, 13180, 13187, 13194, 13201, 13208, 13215, 
13222, 13229, 13236, 13243, 13250, 13257, 13264, 13271, 13278, 
13285, 13292, 13299, 13306, 13313, 13320, 13327, 13334, 13341, 
13348, 13355, 13362, 13369, 13376, 13383, 13390, 13397, 13404, 
13411, 13418, 13425, 13432, 13439, 13446, 13453, 13460, 13467, 
13474, 13481, 13488, 13495, 13502, 13509, 13516, 13523, 13530, 
13537, 13544, 13551, 13558, 13565, 13572, 13579, 13586, 13593, 
13600, 13607, 13614, 13621, 13628, 13635, 13642, 13649, 13656, 
13663, 13670, 13677, 13684, 13691, 13698, 13705, 13712, 13719, 
13726, 13733, 13740, 13747, 13754, 13761, 13768, 13775, 13782, 
13789, 13796, 13803, 13810, 13817, 13824, 13831, 13838, 13845, 
13852, 13859, 13866, 13873, 13880, 13887, 13894, 13901, 13908, 
13915, 13922, 13929, 13936, 13943, 13950, 13957, 13964, 13971, 
13978, 13985, 13992, 13999, 14006, 14013, 14020, 14027, 14034, 
14041, 14048, 14055, 14062, 14069, 14076, 14083, 14090, 14097, 
14104, 14111, 14118, 14125, 14132, 14139, 14146, 14153, 14160, 
14167, 14174, 14181, 14188, 14195, 14202, 14209, 14216, 14223, 
14230, 14237, 14244, 14251, 14258, 14265, 14272, 14279, 14286, 
14293, 14300, 14307, 14314, 14321, 14328, 14335, 14342, 14349, 
14356, 14363, 14370, 14377, 14384, 14391, 14398, 14405, 14412, 
14419, 14426, 14433, 14440, 14447, 14454, 14461, 14468, 14475, 
14482, 14489, 14496, 14503, 14510, 14517, 14524, 14531, 14538, 
14545, 14552, 14559, 14566, 14573, 14580, 14587, 14594, 14601, 
14608, 14615, 14622, 14629, 14636, 14643, 14650, 14657, 14664, 
14671, 14678, 14685, 14692, 14699, 14706, 14713, 14720, 14727, 
14734, 14741, 14748, 14755, 14762, 14769, 14776, 14783, 14790, 
14797, 14804, 14811, 14818, 14825, 14832, 14839, 14846, 14853, 
14860, 14867, 14874, 14881, 14888, 14895, 14902, 14909, 14916, 
14923, 14930, 14937, 14944, 14951, 14958, 14965, 14972), class = ""Date""), 
    cases = c(168L, 199L, 214L, 230L, 267L, 373L, 387L, 443L, 
    579L, 821L, 1229L, 1014L, 831L, 648L, 257L, 203L, 137L, 78L, 
    82L, 69L, 45L, 51L, 45L, 63L, 55L, 54L, 52L, 27L, 24L, 12L, 
    10L, 22L, 42L, 32L, 52L, 82L, 95L, 91L, 104L, 143L, 114L, 
    100L, 83L, 113L, 145L, 175L, 222L, 258L, 384L, 755L, 976L, 
    879L, 846L, 1004L, 801L, 799L, 680L, 530L, 410L, 302L, 288L, 
    234L, 269L, 245L, 240L, 176L, 188L, 128L, 96L, 59L, 63L, 
    44L, 52L, 39L, 50L, 36L, 40L, 48L, 32L, 39L, 28L, 29L, 16L, 
    20L, 25L, 25L, 48L, 57L, 76L, 117L, 107L, 91L, 90L, 83L, 
    76L, 86L, 104L, 101L, 116L, 120L, 185L, 290L, 537L, 485L, 
    561L, 1142L, 1213L, 1235L, 1085L, 1052L, 987L, 918L, 746L, 
    620L, 396L, 280L, 214L, 148L, 148L, 94L, 107L, 69L, 55L, 
    69L, 47L, 43L, 49L, 30L, 42L, 51L, 41L, 39L, 40L, 38L, 22L, 
    37L, 26L, 40L, 56L, 54L, 74L, 99L, 114L, 114L, 120L, 114L, 
    123L, 131L, 170L, 147L, 163L, 163L, 160L, 158L, 163L, 124L, 
    115L, 176L, 171L, 214L, 320L, 507L, 902L, 1190L, 1272L, 1282L, 
    1146L, 896L, 597L, 434L, 216L, 141L, 101L, 86L, 65L, 55L, 
    35L, 49L, 29L, 55L, 53L, 57L, 34L, 43L, 42L, 13L, 17L, 20L, 
    27L, 36L, 47L, 64L, 77L, 82L, 82L, 95L, 107L, 96L, 106L, 
    93L, 114L, 102L, 116L, 128L, 123L, 212L, 203L, 165L, 267L, 
    550L, 761L, 998L, 1308L, 1613L, 1704L, 1669L, 1296L, 975L, 
    600L, 337L, 259L, 145L, 91L, 70L, 79L, 63L, 58L, 51L, 53L, 
    39L, 49L, 33L, 47L, 56L, 32L, 43L, 47L, 19L, 32L, 18L, 34L, 
    39L, 63L, 57L, 55L, 69L, 76L, 103L, 99L, 108L, 131L, 113L, 
    106L, 122L, 138L, 136L, 175L, 207L, 324L, 499L, 985L, 1674L, 
    1753L, 1419L, 1105L, 821L, 466L, 274L, 180L, 143L, 82L, 101L, 
    72L, 55L, 71L, 50L, 33L, 26L, 25L, 27L, 21L, 24L, 24L, 20L, 
    18L, 18L, 25L, 23L, 13L, 10L, 16L, 9L, 12L, 16L, 25L, 31L, 
    36L, 40L, 36L, 47L, 32L, 46L, 75L, 63L, 49L, 90L, 83L, 101L, 
    78L, 79L, 98L, 131L, 83L, 122L, 179L, 334L, 544L, 656L, 718L, 
    570L, 323L, 220L, 194L, 125L, 95L, 77L, 46L, 42L, 29L, 35L, 
    21L, 29L, 16L, 14L, 19L, 15L, 19L, 18L, 21L, 10L, 14L, 7L, 
    7L, 5L, 9L, 14L, 11L, 18L, 22L, 39L, 36L, 46L, 44L, 37L, 
    30L, 39L, 37L, 45L, 71L, 59L, 57L, 80L, 68L, 88L, 72L, 74L, 
    208L, 357L, 621L, 839L, 964L, 835L, 735L, 651L, 400L, 292L, 
    198L, 85L, 64L, 41L, 40L, 23L, 18L, 14L, 22L, 9L, 19L, 8L, 
    14L, 12L, 15L, 14L, 4L, 6L, 7L, 7L, 8L, 13L, 10L, 19L, 17L, 
    20L, 22L, 40L, 37L, 45L, 34L, 26L, 35L, 67L, 49L, 77L, 82L, 
    80L, 104L, 88L, 49L, 73L, 113L, 142L, 152L, 206L, 293L, 513L, 
    657L, 919L, 930L, 793L, 603L, 323L, 202L, 112L, 55L, 31L, 
    27L, 15L, 15L, 6L, 13L, 21L, 10L, 11L, 9L, 8L, 11L, 7L, 5L, 
    1L, 4L, 7L, 2L, 6L, 12L, 14L, 21L, 29L, 32L, 26L, 22L, 44L, 
    39L, 47L, 44L, 93L, 145L, 289L, 456L, 685L, 548L, 687L, 773L, 
    575L, 355L, 248L, 179L, 129L, 122L, 103L, 72L, 72L, 36L, 
    26L, 31L, 12L, 14L, 14L, 14L, 7L, 8L, 2L, 7L, 8L, 9L, 26L, 
    10L, 13L, 13L, 5L, 5L, 3L, 6L, 1L, 10L, 6L, 7L, 17L, 12L, 
    21L, 32L, 29L, 18L, 22L, 24L, 38L, 52L, 53L, 73L, 49L, 52L, 
    70L, 77L, 95L, 135L, 163L, 303L, 473L, 823L, 1126L, 1052L, 
    794L, 459L, 314L, 252L, 111L, 55L, 35L, 14L, 30L, 21L, 16L, 
    9L, 11L, 6L, 6L, 8L, 9L, 9L, 10L, 15L, 15L, 11L, 6L, 3L, 
    8L, 4L, 7L, 7L, 13L, 10L, 23L, 24L, 36L, 25L, 34L, 37L, 46L, 
    39L, 37L, 55L, 65L, 54L, 60L, 82L, 55L, 53L, 61L, 52L, 75L, 
    92L, 121L, 170L, 199L, 231L, 259L, 331L, 357L, 262L, 154L, 
    77L, 34L, 41L, 21L, 17L, 16L, 7L, 15L, 11L, 7L, 5L, 6L, 13L, 
    7L, 6L, 8L, 7L, 1L, 11L, 9L, 3L, 9L, 9L, 8L, 15L, 19L, 16L, 
    10L, 12L, 26L, 35L, 35L, 41L, 34L, 30L, 36L, 43L, 23L, 55L, 
    107L, 141L, 217L, 381L, 736L, 782L, 663L, 398L, 182L, 137L, 
    79L, 28L, 26L, 16L, 14L, 8L, 4L, 4L, 6L, 6L, 11L, 4L, 5L, 
    7L, 7L, 6L, 8L, 2L, 3L, 3L, 1L, 1L, 3L, 3L, 2L, 8L, 8L, 11L, 
    10L, 11L, 8L, 24L, 25L, 25L, 33L, 36L, 51L, 61L, 74L, 92L, 
    89L, 123L, 402L, 602L, 524L, 494L, 406L, 344L, 329L, 225L, 
    136L, 136L, 84L, 55L, 55L, 42L, 19L, 28L, 8L, 7L, 2L, 7L, 
    6L, 4L, 3L, 5L, 3L, 3L, 0L, 1L, 2L, 3L, 2L, 1L, 2L, 2L, 9L, 
    4L, 9L, 10L, 18L, 15L, 13L, 12L, 10L, 19L, 15L, 22L, 23L, 
    34L, 43L, 53L, 47L, 57L, 328L, 552L, 787L, 736L, 578L, 374L, 
    228L, 161L, 121L, 96L, 58L, 50L, 37L, 14L, 9L, 6L, 15L, 12L, 
    9L, 1L, 6L, 4L, 7L, 7L, 3L, 6L, 9L, 15L, 22L, 28L, 34L, 62L, 
    54L, 75L, 65L, 58L, 57L, 60L, 37L, 47L, 60L, 89L, 90L, 193L, 
    364L, 553L, 543L, 676L, 550L, 403L, 252L, 140L, 125L, 99L, 
    63L, 63L, 76L, 85L, 68L, 67L, 38L, 25L, 24L, 11L, 9L, 9L, 
    4L, 8L, 4L, 6L, 5L, 2L, 6L, 4L, 4L, 1L, 5L, 4L, 1L, 2L, 2L, 
    2L, 2L, 3L, 4L, 4L, 7L, 5L, 2L, 10L, 11L, 17L, 11L, 16L, 
    15L, 11L, 12L, 21L, 20L, 25L, 46L, 51L, 90L, 123L)), .Names = c(""date"", 
""cases""), row.names = c(NA, -835L), class = ""data.frame"")
</code></pre>
"
"0.168174993036504","0.162374100149152"," 64535","<p>My ecological question is: ""What are the trends in percent coral cover by island and depth across the state of Hawaii from 1999 to 2012?""  </p>

<p>I am trying to analyze this hierarchical data set using R with 10 transects at each depth, 2 depths per site, and site nested in island.</p>

<p>Data structure:</p>

<pre><code>Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.
 WYear: 0-13. It was suggested that I use this factor as a covariate for years.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.
 Year: 1999 â€“ 2012 (14 years)

Dependent variable: PercentCover
</code></pre>

<p>Currently, I am using the <code>lmer</code> function in the <code>lmerTest</code> package and this is the model that I've constructed.</p>

<pre><code>fit1 &lt;- lmer(PercentCover ~ WYear*Island*DepthCat +
             (1+WYear|Island/Site/DepthCat/Transect) + (1|Year), data=Benthic)
</code></pre>

<p>Unfortunately, the data are spotty (i.e., missing data in multiple years for a number of sites) so the model returns <code>[1] ""Asymptotic covariance matrix A is not positive!""</code>, even using arcsin transformed data. I can still run the summary statistics to get results, but I don't feel comfortable with the error message. Perhaps I have not structured the model correctly in terms of organizing the nested factors, but the number of observations for each of the levels in the summary stats seems correct. I tried different and simpler iterations of the model such as:</p>

<pre><code> fit1 &lt;- lmer(PercentCover ~ WYear + Island + DepthCat + (1+WYear|Transect/Site) + 
              (1|Year), data=Benthic)
</code></pre>

<p>which works, but doesn't give me the interaction information and returns a larger AIC suggesting that the model does not fit the data as well.</p>

<p>To deal with all of the missing data, I tried another approach by using the regression slope of percent cover over time as the dependent variable for each site X depth combination.</p>

<pre><code>Data structure:

Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.

Dependent variable: Trend
</code></pre>

<p>I used the following model, but the summary results did not make much sense, even after transforming the data.</p>

<pre><code> fit1&lt;-lmer(Trend ~ Island*DepthCat + (1| Island/Site/DepthCat/Transect), data=Benthic)
</code></pre>

<p>Any suggestions on improving my analytical approach would be appreciated.</p>
"
"0.277416792858826","0.281240191291608"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.0518999296107682","0.0751646028002829"," 69000","<p>EDITED: I want to select the a regression model (geographically weighted regression vs. OLS) based on a pseudo-R2 comparing the predicted values and the actual ones. I am aiming to regress the predicted values onto the actual values. My models have a logged response variable. My questions are as follows:</p>

<p>1) Is it necessary to back transform the response before calculating the pseudo R2?</p>

<p>2) If it is the case, is it necessary to consider the variance only in the regression output or also in the logged input variable? </p>

<p>My R code looks like this: </p>

<pre><code>lm( exp( predict(regressionmodel)+var(predict(regressionmodel))*0.5 ) ~ exp(data$loggedinput), data=data )
</code></pre>

<p>I appreciate your response very much. Thanks in advance!</p>
"
"0.111237302078652","0.122743282386443"," 69524","<p>I am trying to fit a nonlinear regression model in R using <code>nls()</code>. I have a form of the equation I want to fit to:</p>

<p>$$y = (a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e)$$</p>

<p>where the coefficients to be found in regression are a,b,c,d, and e. My data is output from a simulation model where $x_{1}$, $x_{2}$, and $x_{3}$ are all integers from $0$ to $10$, with the condition that $x_{1} + x_{2} + x_{3} \le 10$. $y$ is also integer valued and ranges from $0$ to roughly $1000$. The objective is to fit these data to a rate function that will be used in a Markov Chain.</p>

<p>When I try to fit this regression model directly using <code>nls()</code>, my <code>nlsResiduals</code> plot looks like this:</p>

<p><img src=""http://i.stack.imgur.com/6scJ3.png"" alt=""nls residuals""></p>

<p>I know that autocorrelated residuals are problematic, and that non-normal residuals can also be problematic. How can I fix this problem? I was thinking of using transforms on the data like</p>

<p>$$\log(y) = \log((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))$$</p>

<p>or</p>

<p>$$y^{1/n} = ((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))^{1/n}$$  where $n &gt; 1$. I've noticed if $n$ increases, my autocorrelation graph and QQ-plot look ""better"" (i.e., more scattered and more normal, respectively). </p>

<p>Both of these seem to correct a lot (but not all) of the autocorrelated residuals, and help to make the residuals more normally distributed. Am I on the right track here, or am I committing some cardinal sin in statistics? Once I settle on a transformation, how can I tell which is best?</p>

<p>Any help, suggestions, or comments are very appreciated.</p>
"
"0.0778498944161523","0.0751646028002829"," 70598","<p>I am estimating an instrumental variables linear regression that has a large number of indicator (factor) variables.  I don't particularly care about the coefficient estimates on those indicator variables.  In Stata's ivreg2 package there is a ""partial"" option that applies the Frisch-Waugh-Lovell theorem to orthogonalize the dependent and exogenous variables to the indicator variables.  After this transformation the indicator variables are not estimated because they do not affect the coefficients on the variables I am interested in.</p>

<p>My question is, is there something like this in R?  It doesn't have to be part of an IV regression package but I am looking to orthogonalize one set of variables to another set of variables.  This seems like something that would have already been implemented.  Thanks.</p>
"
"0.100503781525921","0.097037084956597"," 71070","<p>I'm modeling the amount of organic content in bird bones (a percentage) in two different conditions and also over two time periods. The design is repeated measures - observations in both conditions and time periods come from the same bone (divided into pieces). I want to test the hypotheses: 1) there is no difference in E(Y) across conditions, 2) there is no difference in E(Y) across time, 3) there is no difference in difference of E(Y) (i.e., time*condition interaction). I've tried the following (here with dummy data):</p>

<pre><code>set.seed(6753)
dat &lt;- data.frame(
    id = rep(1:15, each = 4),
    pc.organic = rnorm(60, 0.11, 0.055),
    condition = factor(rep(c(""raw"", ""advanced""), times = 30)),
    year = factor(rep(c(1, 1, 2, 2), times = 15))
    )

library(lme4)
fm1 &lt;- lmer(pc.organic ~ condition * year + (1 | id), data = dat)
summary(fm1)
</code></pre>

<p>This, I think, is an appropriate model to account for the non-independence of observations in the repeated measures design. I'm unsure, however, whether this is ok given the nature of the response variable. The response varies between about .01 (1%) and .2 (20%). It is bounded at zero (obviously), but also at 40% (this is the maximum amount of organic content in any bone - 60% is inorganic). Another option would be to use 40% as the denominator when I define the percentage, thus, the previous values would be .025 and .5 respectively. However, this would still leave the response bounded between 0 and 1.</p>

<p>I've read about beta regression and also about using a logit transformation to linearize the data. If possible, I'd  like to avoid going down these paths, as other researchers in my field are not familiar with these methods. Any suggestions are most welcome. </p>
"
"0.207599718443073","0.225493808400849"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.127128345232746","0.122743282386443"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.0898933149950989","0.0867926073205492"," 74678","<p>I am comparing multiple published equation forms, refit with independent data.  I'm trying to be true to the original authors' methods as much as possible. Therefore, I have 3 linear equations (fit in R using lm()), two of which use transformed Y-variables, and one equation fit using nonlinear regression (fit in R using the gnls() function).</p>

<p>In all instances cases I'm weighting the residual variance structure using the inverse of one of the predictors to account for observed heteroskedasticity.</p>

<p>I have been evaluating the models using R2, and RMSE- using back-transformed data for the two models with transformations.</p>

<p>I've calculated RMSE ""by hand"" using the following equation:</p>

<pre><code> RMSE&lt;-sqrt(sum(residuals(Equation)^2)/length(residuals(Equation))-2))
</code></pre>

<p>Should I use similar code to calculate RMSE for the linear and nonlinear regression models?  Is the metric still a valid statistic for comparison, or am I missing some important assumption?  </p>

<p>Edited: I initially stated that I was also comparing models using AIC; I later recalled that AIC would not be appropriate if the Y-variables were transformed because the models would be estimating different things.</p>
"
"0.162472478891985","0.15686828393899"," 78455","<p>Disclaimer: Statistics is not my strong side, so if my question is nonsense I apologize. I'm a beginner, but really wanting to understand this.</p>

<p>My question is: why do I get so widely different parameter estimates when using different transformations on my data in a non-linear regression ?</p>

<p>I'm trying to do a nonlinear regression and to estimate the uncertainty of the fit (confidence interval) using linear approximation. From my understanding the more linear-like the shape of the nonlinear function, the more accurate will the confidence interval calculation by linear approximation be.  I therefore want to transform the data to make it as linear as possible. The errors in $y$ can be assumed to be log-normal. My data is monotonic and assumed to follow a power function in most cases.</p>

<p>$$ y = a*(x-x_0)^b $$</p>

<p>where $y$ is river discharge, $x$ is an arbitrary water level in the river and $x_0$ is the water level where where discharge $y$ is 0. This can be rewritten as log transformed, and nice and linear
$$ log(y) = a + b \times log(x-x_0) $$.</p>

<p>I need to estimate the parameters $a$, $b$ and $x_0$, so to do so simultaneously I use nonlinear regression. I also have some data that follows quadratic functions, so I would like to set up (and understand) a non-linear method.</p>

<p>I use r and <code>nlsLM()</code> from <code>minpack.lm</code> to carry out the non-linear regression.
Here is some example code:</p>

<pre><code>library(minpack.lm)

xdata &lt;- c(19,  21,  24,    25, 29, 34, 35, 40, 40, 46, 48, 48, 52, 56, 57, 65, 65, 68)
ydata &lt;- c(10,  11, 14, 20, 24, 50, 42, 96, 89, 134,    135,    161,    171,    218,    261,    371,    347,    393)
df&lt;-data.frame(x=xdata, y=ydata)

#weights applied in the case of no transformation (relative error assumed to be the same for all y data)
W&lt;-1/ydata

# NLS regression with weights, no transformation
nlsmodel1&lt;-nlsLM(y ~ a*(x-x0)^b,data=df,start=list(a=0.1, b=2.5,x0=0))

# log transformed
nlsmodel2&lt;-nlsLM(log(y) ~ a+b*(log(x-x0)),data=df,start=list(a=0.1, b=2.5,x0=0))
&gt; coef(nlsmodel1)
          a           b          x0 
0.005158377 2.719693093 4.896772931 
&gt; coef(nlsmodel2)
        a         b        x0 
-8.683758  3.445699 -4.139127 

&gt; exp(-8.683758)
[1] 0.0001693136
</code></pre>

<p>I understand that the weights are very important, and can have a say in the differences here, but not by this much? My judgement of the two parameter sets is that <code>nlsmodel1</code> performs ""better"", and that the <code>b</code> coefficient is too high in the fit from <code>nlsmodel2</code>. <code>nlsmodel2</code> does a poor job in the upper end of the data, with large residuals there. But why are they so different? I feel like I'm doing something very silly here, and is unable to see the error. I have tried some other transformations, for example only transforming LHS as <code>log(y)</code>, but the problem remains.</p>

<p>I appreciate any tips that can help me improve, and not the least understand, the transformed fit.</p>

<p>Cheers</p>

<p>Related <a href=""http://stats.stackexchange.com/questions/58928/nonlinear-regression-confidence-intervals-on-transformed-or-untransformed-param"">post #1</a> and <a href=""http://stats.stackexchange.com/questions/69524/on-nonlinear-regression-fits-and-transformations"">post #2</a></p>
"
"0.0898933149950989","0.0867926073205492"," 78633","<p>I'm currently playing around with linear regression in R, and I've come up with a regression that fits data quite well. I'm just having some problems with interpreting the coefficients of my model. I know how to interpret log-log models in a simpler form, but when I have interactions I'm not quite sure how to interpret them.</p>

<p>Here's my output from R:</p>

<pre><code>Call:
lm(formula = log(y) ~ log(x1) + x2 * log(x1) + x3 * log(x1) + 
I(x3^2), data = Data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.56943 -0.12082  0.00012  0.11123  0.54579 

Coefficients:
                Estimate   Std. Error t value          Pr(&gt;|t|)    
(Intercept) -2.393889950  0.545879641  -4.385 0.000025149470154 ***
log(x1)      0.497477722  0.056113496   8.866 0.000000000000009 ***
x2          -0.000264760  0.000055476  -4.773 0.000005220020368 ***
x3           0.041126987  0.017930934   2.294           0.02357 *  
I(x3^2)     -0.000688879  0.000231778  -2.972           0.00358 ** 
log(x1):x2   0.000031580  0.000006691   4.720 0.000006494076511 ***
log(x1):x3   0.003145219  0.001277909   2.461           0.01528 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1932 on 119 degrees of freedom
Multiple R-squared: 0.9865,     Adjusted R-squared: 0.9859 
F-statistic:  1454 on 6 and 119 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>I've been Googling for the past hour, but I can only find answers to some simpler models like the answer given here: <a href=""http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor"">Interpretation of log transformed predictor</a> or <a href=""http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm</a></p>

<p>I hope someone out there can help me with interpreting the interaction terms and the polynomial term in my model. </p>
"
"0.0635641726163728","0.0613716411932216"," 79746","<pre><code>&gt; ncvTest(alm)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 121.2316    Df = 1     p = 3.400245e-28 

&gt; spreadLevelPlot(alm)
Suggested power transformation:  4.428269
</code></pre>

<p>I am having issues adjusting my regression formula based on what the results of the non-constant variable test shows. How do I implement the suggested power transformation here? Obviously with such a low p value this is heteroscedastic. I have run a robust standard error linear regression already and it does not change the BP test p value. I have also performed a variance inflation factor test to see if multicolinearity is an issue here, which it does not seem to be. </p>

<p>Any and all help is appreciated!. </p>
"
"0.111237302078652","0.122743282386443"," 84054","<p>I encountered a real-world problem where I want to model the effectiveness of various advertising media of a brand (measured in terms of sales). Basically, the Y in this case is weekly sales, and the X's are media investments in newspaper, magazine, display boards, tv, radio and online, as well as incentive, which is a percentage (like 10% off the original).</p>

<p>There are a few problems with the modelling work:</p>

<ul>
<li><p>all variables should have positive coefficients. Typically, more advertising or incentive is at least as good as less advertising/incentive (maybe this is not true if you buy all of the advertising spots in the world, as then your consumer will start to hate your brand, but this is not going to happen here). However, when I fit a typical regression (e.g. lm, glm, gls etc), some coefficients turn out to be negative (as data may be a bit noiser than expected, hence causing this problem?). I wonder if this can be controlled (I know in nonlinear regressions you can set constraints for parameters)</p></li>
<li><p>there should be some sort of diminishing marginal return of advertising spendings, but I am not exactly sure how to model that. Some ideas include using a log or square root transformation, another idea may be to use a nonlinear regression and estimator something like a*newspaper^b, where a is some coefficient, and b is an exponent between 0 and 1.</p></li>
<li><p>this is serial correlation, but this may not be exactly important here as the goal is only to estimate the parameters (if I use a regression I think I still get the unbiased estimators right? Autocorrelation only screws up the p-values, which is ignored here). Also, how to deal with seasonalities? I don't have much data (2 years) so maybe there is nothing we can do about it, but I have seen adding cos(0.0172*time) + sin(0.0172*time) to the regression equation to adjust for seasonal changes.</p></li>
</ul>

<p>Thanks.</p>
"
"0.110096376512636","0.106298800690547"," 84319","<p>I am working on an age estimation method using 4 types of biological measurements as age predictors. I am using RStudio. 
So far, I have good results when I use linear regression (<code>lm(age~predictor)</code>), but I am encountering heteroskedasticity, and therefore cannot build prediction intervals for my models.<br> 
I have tried transformations to normalize the predictors using ln, inverse, and square root, but to no avail.<br>
I have found a paper explaining the <code>wls</code> function, and I have used it in my models with the weight: $$\frac 1 {1+\frac{\text{predictor}^2} 2}$$ 
This has given me better age predictions, but does not solve the heteroskedasticity problem. </p>

<p>I have done some research, and apparently, one of my options is to create homoscedastic groups in my data by finding the data points where the residual variances change. 
For that, I have used the breakpoints function of strucchange, which gave me 5 breakpoints by default. 
I now want to give 6 different weights (weights are $\frac 1 {\text{var(age)}}$ of each interval) to my 6 intervals of data, but I cannot find a function to do that. I would greatly appreciate any help on the subject. 
Thanks.</p>
"
"0.0778498944161523","0.0751646028002829"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.149071198499986","0.143929256529458"," 85909","<p>The <code>plm</code> function of the <code>plm</code> library in R is giving me grief over having duplicate time-id couples, even when I'm running a model that I don't think should need a time variable at all (see reproducible example below).</p>

<p>I can think of three possibilities:</p>

<ol>
<li>My understanding of fixed effects regression is wrong, and they really do require unique time indices (or time indices at all!).</li>
<li>plm() is just being overly-finicky here and should relax this requirement.</li>
<li>The particular estimation technique that plm() uses--the within transformation--requires time indices, even though the order doesn't seem to matter and the less computationally-efficient version (including dummies in a straight-up OLS model) doesn't need them.</li>
</ol>

<p>Any thoughts?</p>

<pre><code>set.seed(1)
n &lt;- 1000
test &lt;- data.frame( grp = as.factor(rep( letters, (n/length(letters))+1 ))[seq(n)], x = runif(n), z = runif(n) )
test$y &lt;- with( test, 2*x + 3*z + rnorm(n) )
lm( y ~ x + z, data = test )
lm( y ~ x + z + grp, data = test )

require(plm)
# Model fails if I don't specify a time index, despite effect = ""individual""
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = ""grp"" ) 
# Create time variable and add it to the index but still specify individual FE not time FE also
library(plyr)
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = seq(nrow(dat)) ) )
# Now plm() works; note coefficients clearly include the fixed effects, as they match the lm() version above
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Scramble time variables and show they don't matter as long as they're unique within a cluster
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = sample(t) ) )
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Add a duplicate time entry and show that it causes plm() to fail
test[ 2, ""t"" ] &lt;- test[ 1, ""t"" ] 
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
</code></pre>

<p><strong>Why this matters</strong></p>

<p>I'm trying to bootstrap my model, and when I do the requirement that the index-time pairs be unique is causing headaches which seem unnecessary if (2) is true.</p>
"
"0.179786629990198","0.173585214641098"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"0.0778498944161523","0.0501097352001886"," 86888","<p>I implemented AVAS on my data in R. </p>

<p>$y = \text{weight}$, 
$x =$ matrix with several predictors, e.g. age, height, gender. </p>

<p>From what I understand, AVAS estimates transformations of $x$ and $y$ such that the regression of $y$ on $x$ is approximately linear with constant variance.</p>

<p>I followed <a href=""http://rgm3.lab.nig.ac.jp/RGM/R_rdfile?f=acepack/man/avas.Rd&amp;d=R_CC"" rel=""nofollow"">the help file</a> and did a plot for the following: </p>

<p><code>plot(a$y,a$ty)</code> â€“ this looks like a cubic curve which is not on any of the graphs on the help file. 
<code>plot(a$x,a$tx)</code> â€“ this looks like a big blob of black.</p>

<p>My question is â€“ how do I interpret the results, and how do I know what transformation was used to transform the data? E.g if I have a linear model: $\text{weight = age + height + gender}$ then how do I transform this using AVAS in R? </p>

<p>Also â€“ if AVAS transforms the data â€“ can I then perform variable selection on this data to 'eliminate' variables? Or does it also eliminate variables in the process?</p>
"
"0.185606467466924","0.189160102178341"," 87278","<p>I don't know if a similar problem has been asked before so if it has been, please provide me a link to the related/duplicate questions. I am sorry if I seem to be asking too much. But I really like to learn this stuff and this seems to be a good place to start asking.</p>

<p>I have been teaching myself statistics through self-study and I found Logan's <a href=""http://as.wiley.com/WileyCDA/WileyTitle/productCd-1405190086.html"" rel=""nofollow""><strong>Biostatistical Design and Analysis Using R</strong></a> very helpful in that it shows how the actual computations are done (in R) and how the results are interpreted. I particularly like the part about multiple regression (Chapter 9). I use R since it is the most accessible (and free) software that I can get my hands into. </p>

<p>Right now, I am trying to learn multivariate multiple regression. But unfortunately, I can't find a good resource. My specific problem is finding the best linear model for each response variable for the following morphometric data of a plant species (that some of my high school biology students are investigating), where <code>Leaves</code>, <code>CorL</code>, <code>CorD</code>, <code>FilL</code>, <code>AntL</code>, <code>AntW</code>, <code>StaL</code>, <code>StiW</code>, and <code>HeiP</code> are the response variables and <code>pH</code>, <code>OM</code>, <code>P</code>, <code>K</code> (nutrient variables), <code>Elev</code>, <code>SoilTemp</code>, and <code>AirTemp</code> (environment variables) are the independent variables.</p>

<p>I don't know if it is okay to proceed as in the case of only one dependent variable, but I went through the steps of Example 9B of Logan anyhow.</p>

<h3>lily.csv</h3>

<pre><code>Leaves,CorL,CorD,FilL,AntL,AntW,StaL,StiW,HeiP,Elev,pH,OM,P,K,SoilTemp,AirTemp
55,213.4,114.6,170.3,10.6,2.35,210,6.7,0.93,1431,6.37,1,3,170,29,26
44,192.15,95.25,160.6,7.1,2.25,176.4,6.55,0.79,1471,6.02,1,0,180,25,23
38,156.75,95.5,155.2,5.65,1.8,170.9,4.4,0.78,1471,6.02,1,0,180,25,23
29,191.8,88.35,155.2,10,2.5,178.25,5.9,0.75,1464,5.99,1,3,150,25,22
36,200.85,99.4,161.9,6.5,1.55,187.4,6.15,0.8,1464,5.99,1,3,150,25,22
43,210.2,74,147,7,1,170,5,0.8,1464,5.99,1,3,150,25,22
34,183.2,97.3,149.5,6.9,1.85,168.8,5.45,0.71,1464,5.99,1,3,150,25,22
52,233.3,107.7,179.6,9.2,3.05,210,6.45,0.82,1464,5.99,1,3,150,25,22
43,205.7,108.8,164.6,9.4,2,190.9,5.15,0.66,1464,5.99,1,3,150,25,22
28,203.15,119.35,160.6,8.9,2.3,180,6.85,0.77,1503,5.98,3,2,240,29.5,25.5
45,188.85,100.5,150.6,6.4,2.3,174.85,7.7,0.84,1503,5.98,3,2,240,29.5,25.5
49,205.2,126.85,150.8,10.1,2.8,177.5,9,0.84,1487,6.09,4,4,180,26,25
35,187.7,102.35,142.1,5.55,1.85,175.35,5.75,0.56,1485,6.17,3.5,1,220,24,23
23,181.05,94.6,136.6,6.9,1.8,169.3,5.8,0.59,1485,6.17,3.5,1,220,24,23
31,172.5,63.7,113.6,5.2,1.5,151.2,4.7,0.57,1482,6.29,5,2,280,24,23
34,190.5,93.1,151.9,5.65,1.85,172.5,5.25,0.68,1482,6.29,5,2,280,24,23
41,185.85,85.2,148.6,5.9,1.05,169.6,5.9,0.62,1472,6.48,0.5,3,170,25.22,22.89
29,195,159.2,159.3,15,4,185,6.3,0.59,1472,6.48,0.5,3,170,25.22,22.89
31,115.6,108.6,165.8,8.5,3,200.5,7.5,0.83,1454,5.53,5,14,350,25.22,22.89
27,176.65,93.1,128.65,6.65,2.85,180.5,6.65,0.53,1454,5.53,5,14,350,25.22,22.89
33,210,119,148,7,3,193,6,0.62,1454,5.53,5,14,350,25.22,22.89
42,200,93,166,18.3,4.55,177,8,1.12,1454,5.53,5,14,350,25.22,22.89
42,205,101.4,166.8,9,2.5,190,8.2,1.12,1454,5.53,5,14,350,25.22,22.89
25,192.9,94.15,147.8,6.45,2.3,167.65,7.15,0.61,1445,5.59,4,7,260,25.22,22.89
36,187.95,65.05,150.2,6.55,2.7,177.5,6.55,0.52,1445,5.59,4,7,260,25.22,22.89
32,110.4,11.6,168.15,7.6,2,197.95,7.85,0.73,1481,6.29,1.5,1,80,25.22,22.89
29,185,80,143,9,2,179,7.5,0.69,1481,6.29,1.5,1,80,25.22,22.89
29,179.8,70.6,134.8,11.15,3.2,165.65,5.3,0.6,1481,6.29,1.5,1,80,25.22,22.89
</code></pre>

<p>Firstly, I tried to investigate for possible collinearity among the variables.</p>

<pre><code>library(car)
lily = read.csv(""lily.csv"",header=T)
scatterplotMatrix(lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ytsnd.png"" alt=""enter image description here""></p>

<p><code>FilL</code>, <code>AntL</code>, <code>AntW</code>, and <code>HeiP</code> seem to be non-normal so I made <code>log10</code> transformations. This <em>seems</em> to work fine. (And it is fine for you to educate me at this point if I am doing it wrong. I'd appreciate it very much.)</p>

<pre><code>scatterplotMatrix(~Elev + pH + OM + P + K + SoilTemp + AirTemp +
AirTemp + Leaves + CorL + CorD + log10(FilL) + log10(log10(log10(AntL)+0.1)+0.1) + 
log10(AntW) + StaL + StiW + log10(HeiP),data=lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/t9nmI.png"" alt=""enter image description here""></p>

<p>I check for multicolinearity among the independent variables.</p>

<pre><code>&gt; cor(lily[,10:16])
                Elev          pH          OM           P           K
Elev      1.00000000  0.48252995 -0.06601928 -0.56726786 -0.28159580
pH        0.48252995  1.00000000 -0.58587694 -0.81673123 -0.70434283
OM       -0.06601928 -0.58587694  1.00000000  0.65931857  0.86478172
P        -0.56726786 -0.81673123  0.65931857  1.00000000  0.79782480
K        -0.28159580 -0.70434283  0.86478172  0.79782480  1.00000000
SoilTemp  0.14558365  0.01543524 -0.10436250 -0.05023853 -0.01041523
AirTemp   0.26450883  0.15711849  0.16862694 -0.09735977  0.11655030
            SoilTemp     AirTemp
Elev      0.14558365  0.26450883
pH        0.01543524  0.15711849
OM       -0.10436250  0.16862694
P        -0.05023853 -0.09735977
K        -0.01041523  0.11655030
SoilTemp  1.00000000  0.83202496
AirTemp   0.83202496  1.00000000
</code></pre>

<p>Among the independent variables, pairs <code>P</code> and <code>pH</code>, <code>K</code> and <code>pH</code>, <code>P</code> and <code>K</code>, <code>OM</code> and <code>K</code>, and <code>SoilTemp</code> and <code>AirTemp</code> have strong collinearity. </p>

<p>I also checked for collinearity among the dependent variables although I don't have an idea if this is a alright.</p>

<pre><code>&gt; cor(lily[,1:9])
            Leaves        CorL       CorD      FilL      AntL        AntW
Leaves 1.000000000  0.44495257 0.17903019 0.5222644 0.1495016 0.004680606
CorL   0.444952572  1.00000000 0.51084625 0.1319070 0.2101801 0.097530007
CorD   0.179030187  0.51084625 1.00000000 0.2368117 0.3297344 0.376806953
FilL   0.522264352  0.13190704 0.23681171 1.0000000 0.3932006 0.284738542
AntL   0.149501570  0.21018008 0.32973443 0.3932006 1.0000000 0.796401542
AntW   0.004680606  0.09753001 0.37680695 0.2847385 0.7964015 1.000000000
StaL   0.416083096  0.06574503 0.23272070 0.7762797 0.2701401 0.318744025
StiW   0.194927129 -0.05594094 0.08322138 0.3752195 0.3755628 0.445964273
HeiP   0.577737137  0.17603412 0.13911530 0.6348948 0.4583508 0.254173681
             StaL        StiW      HeiP
Leaves 0.41608310  0.19492713 0.5777371
CorL   0.06574503 -0.05594094 0.1760341
CorD   0.23272070  0.08322138 0.1391153
FilL   0.77627970  0.37521953 0.6348948
AntL   0.27014013  0.37556279 0.4583508
AntW   0.31874403  0.44596427 0.2541737
StaL   1.00000000  0.38306631 0.3794643
StiW   0.38306631  1.00000000 0.5039679
HeiP   0.37946433  0.50396793 1.0000000
</code></pre>

<p>From here, I can check for variance inflation and their inverses and possibly investigate interactions but I am really not sure now how to proceed or if it is alright at all to do these things in the multivariate case. And it seems to be a long way still to assessing the best multivariate model. In the case of the one dependent variable case, I can use the <code>MuMIn</code> package to automate the determination of the best fit but it doesn't work in the multiple response  case.</p>

<p>How do I proceed from this point? I will also appreciate it very much if you can point me to a good book or online material (preferably with applications in R).</p>
"
"0.14213381090374","0.13723116159877"," 87608","<p>I have run a few tests/methods on my data and am getting contradictory results.</p>

<p>I have a linear model saying:
reg1 = lm(weight = height + age + gender (categorical) + several other variables). </p>

<p>If I model each term linearly i.e. no squared or interaction term, and run vif(reg1), 4 variables are >15. If I delete the variable with the highest vif number and re-run it the gifs change and now only 2 variables are >15. I repeat this until I'm left with 20 variables (out of 30) below 10. If I use stepwise directly on reg1 then it does not delete the 'highest vic' factor. <strong>I don't understand how it tells me 'what' is linearly dependant on 'what variable' and how (and I cannot seem to find this information despite googling for ages).</strong> </p>

<p>Furthermore, when I look at the residual plots, most appear horizontal except a few which are upside down u curved (none of these have high vifs). Does this means a transformation is needed? (I removed outliers, leverage points etc - but now there seem to be more!)</p>

<p>reg2 = lm(weight = (height + age + gender (categorical) + several other variables)^2). </p>

<p>If I run vif on this all of the terms are >500! </p>

<p>What else I have tried (without cutting any variables): 
(1) The errors seem correlated when i run diagnostics and check with Durbin Waston statistics indicating the model is not linear... however...
(2) Box Cox gives lambda = 1 so no transformation is needed.
(3) LASSO gives the lowest mallows cp on the full 30 variable model (i.e. least squares)
(4) Ridge regression gives lambda = 0 which did surprise me. </p>

<p>I'm getting really confused about this data. <strong>To determine a suitable model for weight should I be looking just at linear terms or linear and interaction terms (remember there are 25 variables so there are 30^2 interaction terms)?</strong> </p>

<p>When I check which ones are significant in reg2 only 12 predictors and 6 interaction terms seem significant (AIC is lowest with this combination after I run step). <strong>Should I just use this 'new model with deleted variables/interaction terms' and do all my tests e.g. stepwise method, LASSO etc or do I do it on the entire model?</strong> </p>

<p>I'm getting quite lost in terms of making sense of steps to find a suitable model for weight using the variables. </p>

<p><strong>My final question is once I have the model - how do i test/prove its the best/a decent model?</strong> </p>

<p>Any help would really be appreciated. </p>
"
"0.129749824026921","0.137801771800519"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0917469804271967","0.106298800690547"," 88212","<p>I have 30 variables and am trying to select the best model. I have run the following methods on a 'large' data set (having removed a smaller test set): </p>

<ul>
<li>OLS, </li>
<li>best subset selection, </li>
<li>stepwise selection, </li>
<li>ridge regression, </li>
<li>LASSO, </li>
<li>PCR and </li>
<li>PLS. </li>
</ul>

<p>All outliers were removed from both data sets. None of the variables/response have been transformed in any way prior to running the above methods and there is little/no collinearity between variables. </p>

<p>I ran each model (for OLS I ran the entire 30 variable model) and computed the MSE and variance for each. They differ only by 0.001 in MSE (best = PLS, worst = stepwise) and the variance (between the best â€“ stepwise and the worst â€“ ridge). </p>

<p><strong>How do I now choose the best model?</strong> I'm pretty stuck! </p>

<p>One idea I had is to cross validate the MSE and var on the test set, but I'm unsure about how to write this code in R. </p>

<p>I'm using code similar to this <a href=""http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/linearregression/linearregression.R"" rel=""nofollow"">website</a>'s. I'm not sure that will solve the problem though. I'm using <code>summary(model - y.test)^2</code> at the moment. </p>
"
"0.135519271363624","0.130844778663143"," 89930","<p>I am attempting to construct a contrast matrix that I can run in R, using the limma bioconductor package, but I am not sure that I have coded the contrast matrix correctly. A previous <a href=""https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r?newreg=add2674ca9d04b7eb85fad255b45b7f5"">post</a> and the limma guide were helpful, but my two factorial design is more complicated than what is illustrated there.</p>

<p>The first factor is the treatment, with two levels (control=c and stress=s), and the second factor is the genotype, with five levels (g1, g2, g3, g4, g5). Each genotype/treatment consists of 3-biological replicates (30xsamples total). My dataset has already been normalized and log2 transformed. It consists of 1208 proteins (based upon spectral counting for those that care) that measures protein abundance differences in the five genotypes and two treatments. The dataset is complete, meaning each sample/condition has a datapoint.</p>

<h2>Subset of the data:</h2>

<pre><code>proteinID   g1.s1   g1.s2   g1.s3   g1.c1   g1.c2   g1.c3   g2.s1   g2.s2   g2.s3   g2.c1   g2.c2   g2.c3   g3.s1   g3.s2   g3.s3   g3.c1   g3.c2   g3.c3   g4.s1   g4.s2   g4.s3   g4.c1   g4.c2   g4.c3   g5.s1   g5.s2   g5.s3   g5.c1   g5.c2   g5.c3
prot1   -9.70583694 -9.940059478    -9.764489183    -9.691937821    -9.547306096    -9.668928704    -9.821333234    -10.00376839    -9.843380585    -10.0789111 -9.958506961    -9.791583706    -10.04996359    -10.10279896    -10.0689715 -9.989303332    -10.05414639    -10.00619809    -9.907032795    -10.09700113    -10.00902876    -10.05603575    -10.26218387    -10.15527373    -9.88009858 -9.748974338    -9.730010667    -9.899956956    -9.773955101    -9.957684691
prot2   -9.810354967    -9.844319231    -9.896748977    -9.777040294    -9.821308434    -9.906798728    -9.832236541    -9.876359355    -9.935535795    -10.05991278    -9.831098077    -9.789738587    -10.08470861    -10.18515166    -10.10371621    -10.01971224    -9.977142493    -10.09055782    -9.739831978    -9.586647999    -9.949407778    -9.800183583    -9.83900565 -9.943521592    -9.99229056 -9.744850134    -9.794814509    -9.98542989 -9.766324886    -9.95430439
prot3   -11.70842601    -11.72521838    -11.90389475    -11.98273998    -11.915401  -11.88620205    -11.91603643    -11.96029519    -12.14926486    -12.23846499    -12.26650985    -11.84300821    -12.64562082    -12.41471031    -12.66462278    -12.577619  -12.90001898    -12.31577711    -11.66323243    -11.50283992    -11.4844068 -11.60402491    -11.95270942    -11.68245512    -12.32380181    -12.24294758    -12.23990879    -12.21563403    -12.33730369    -12.437377
prot4   -10.88942769    -11.16906693    -11.13942576    -11.31332257    -11.04718433    -11.11811122    -11.17687812    -11.12503828    -10.9724186 -11.16837945    -11.19642214    -10.96468249    -11.3975887 -11.28808753    -11.32778647    -11.34124725    -11.30972182    -11.29564372    -10.74370929    -10.92223539    -10.97733154    -11.40528844    -11.1238659 -11.15938598    -11.24937805    -10.8691392 -11.12478375    -10.75566728    -10.99485703    -11.09493115
prot5   -10.0102959 -9.936796529    -9.964629149    -9.842835973    -9.791578592    -9.773380518    -9.72290866 -9.715837804    -9.79028651 -9.951486129    -9.636225505    -9.820715987    -10.41899204    -10.25269382    -10.26949484    -10.02644184    -10.13120897    -10.20756299    -9.752087376    -9.687001368    -10.07111473    -9.815279198    -9.995624174    -9.993526894    -9.722360141    -9.551502595    -9.551929198    -9.724500546    -9.502769792    -9.65324573
prot6   -10.34051005    -10.27571947    -10.14968761    -10.17419023    -10.47812301    -10.11019796    -10.40447672    -10.15885481    -10.22900798    -10.26612428    -10.21920493    -10.17186677    -10.66125689    -10.95438025    -10.63751536    -10.65825783    -10.60857688    -10.78516027    -10.33890785    -10.49726978    -10.47100414    -10.64742463    -10.78932619    -10.5318634 -10.26494688    -9.975182247    -10.24870036    -10.2356165 -10.26689552    -10.13061368
prot7   -10.24930429    -10.37307132    -10.03573128    -10.29985129    -9.991216794    -10.05854902    -10.1958704 -10.30549818    -10.2078462 -10.28795766    -10.23314344    -10.23897922    -9.997472306    -10.27461285    -10.20805608    -10.06261332    -10.24876706    -10.12643737    -9.906088449    -10.07316322    -10.23545822    -10.30970717    -10.40745591    -10.36432166    -10.22423532    -10.25703553    -10.44925268    -9.902554721    -9.891163766    -10.0695915
prot8   -10.98782595    -10.84184533    -10.76496107    -10.68290092    -10.55763113    -10.91736394    -10.87505278    -10.76474268    -10.58319007    -10.87547281    -10.71948079    -10.95011831    -10.99753277    -11.061728  -10.8852958 -10.86371208    -10.96638746    -11.24112703    -10.46809937    -10.78446288    -10.71240489    -10.80931259    -10.6598091 -10.54801115    -10.70612733    -10.7339808 -10.8184854 -10.53370359    -10.47323989    -10.62675183
prot9   -8.83857166 -8.736344638    -8.743339515    -8.8152675  -8.743086044    -8.719612156    -8.898093257    -8.902781886    -9.071574958    -8.945970659    -8.862394746    -8.825061244    -8.82313363 -9.161452294    -8.905846232    -8.940119002    -9.024995852    -8.943721201    -8.768488159    -8.802155458    -8.721187011    -8.84850416 -8.931513624    -8.86743278 -8.856904592    -8.675257846    -8.900833162    -8.676117406    -8.758661701    -8.925717389
prot10  -10.65297508    -10.74532307    -10.65940071    -10.36671791    -10.50431649    -10.54915637    -11.07154003    -10.79884265    -10.97164196    -11.1201714 -11.14821342    -10.9254445 -10.92875918    -10.90806369    -10.77581175    -11.2324716 -11.31360896    -11.01070959    -11.04450945    -10.89694291    -10.76865867    -10.92983387    -11.07365287    -11.43888216    -11.14948441    -10.69611194    -10.85827316    -10.64470128    -10.79046792    -10.86048168
</code></pre>

<h2>Code that I am attempting to utilize:</h2>

<pre><code>proteins.mat &lt;- as.matrix(proteins.df)
treat = c(""g1.s"",""g1.c"",""g2.s"",""g2.c"",""g3.s"",""g3.c"",""g4.s"",""g4.c"",""g5.s"",""g5.c"")
factors = gl(10,3,labels=treat)
design &lt;- model.matrix(~0+factors)
colnames(design) &lt;- treat
</code></pre>

<h2>Here is the design for my model:</h2>

<pre><code>&gt; design
   g1.s g1.c g2.s g2.c g3.s g3.c g4.s g4.c g5.s g5.c
1     1    0    0    0    0    0    0    0    0    0
2     1    0    0    0    0    0    0    0    0    0
3     1    0    0    0    0    0    0    0    0    0
4     0    1    0    0    0    0    0    0    0    0
5     0    1    0    0    0    0    0    0    0    0
6     0    1    0    0    0    0    0    0    0    0
7     0    0    1    0    0    0    0    0    0    0
8     0    0    1    0    0    0    0    0    0    0
9     0    0    1    0    0    0    0    0    0    0
10    0    0    0    1    0    0    0    0    0    0
11    0    0    0    1    0    0    0    0    0    0
12    0    0    0    1    0    0    0    0    0    0
13    0    0    0    0    1    0    0    0    0    0
14    0    0    0    0    1    0    0    0    0    0
15    0    0    0    0    1    0    0    0    0    0
16    0    0    0    0    0    1    0    0    0    0
17    0    0    0    0    0    1    0    0    0    0
18    0    0    0    0    0    1    0    0    0    0
19    0    0    0    0    0    0    1    0    0    0
20    0    0    0    0    0    0    1    0    0    0
21    0    0    0    0    0    0    1    0    0    0
22    0    0    0    0    0    0    0    1    0    0
23    0    0    0    0    0    0    0    1    0    0
24    0    0    0    0    0    0    0    1    0    0
25    0    0    0    0    0    0    0    0    1    0
26    0    0    0    0    0    0    0    0    1    0
27    0    0    0    0    0    0    0    0    1    0
28    0    0    0    0    0    0    0    0    0    1
29    0    0    0    0    0    0    0    0    0    1
30    0    0    0    0    0    0    0    0    0    1
attr(,""assign"")
[1] 1 1 1 1 1 1 1 1 1 1
attr(,""contrasts"")
attr(,""contrasts"")$factors
[1] ""contr.treatment""
</code></pre>

<h2>My contrast model. I want to test for interaction, differences between genotypes, and to see if specific genotypes respond differently to the treatment from one another:</h2>

<pre><code>cmtx &lt;- makeContrasts(
  GenotypevsTreatment=(g1.s-g1.c)-(g2.s-g2.c)-(g3.s-g3.c)-(g4.s-g4.c)-(g5.s-g5.c),
  genotype=(g1.s+g1.c)-(g2.s+g2.c)-(g3.s+g3.c)-(g4.s+g4.c)-(g5.s+g5.c),
  Treatment=(g1.s+g2.s+g3.s+g4.s+g5.s)-(g1.c+g2.c+g3.c+g4.c+g5.c),
  levels=design)
</code></pre>

<h2>What my contrast model looks like, but I don't think this is correct:</h2>

<pre><code>&gt; cmtx
      Contrasts
Levels GenotypevsTreatment Genotype Treatment
  g1.s                   1        1         1
  g1.c                  -1        1        -1
  g2.s                  -1       -1         1
  g2.c                   1       -1        -1
  g3.s                  -1       -1         1
  g3.c                   1       -1        -1
  g4.s                  -1       -1         1
  g4.c                   1       -1        -1
  g5.s                  -1       -1         1
  g5.c                   1       -1        -1
</code></pre>

<h2>Fitting the linear model by empirical bayes statistics for differential expression:</h2>

<pre><code>fit &lt;- eBayes(contrasts.fit(lmFit(proteins.mat, design), cmtx))
topTable(fit, adjust.method=""BH"")
</code></pre>

<h2>The below topTable proteins are the same as the subset of data from above:</h2>

<pre><code>&gt; topTable(fit, adjust.method=""BH"")
       GenotypevsTreatment Genotype    Treatment    AveExpr        F      P.Value    adj.P.Val
prot1        -0.40786338 60.30918  0.073054723  -9.918822 17308.55 1.124646e-39 1.232079e-36
prot2        -0.09255219 59.60864  0.061701713  -9.897968 15801.43 3.304533e-39 1.232079e-36
prot3        -0.23880357 73.48557  0.536672827 -12.090016 15650.65 3.701463e-39 1.232079e-36
prot4        -0.11834000 66.76931  0.305471823 -11.122034 15522.46 4.079731e-39 1.232079e-36
prot5        -0.15210172 59.21509 -0.183849274  -9.876144 14734.51 7.556112e-39 1.423908e-36
prot6        -0.15761118 62.87467  0.155340561 -10.389362 14565.87 8.658504e-39 1.423908e-36
prot7        -0.03886438 61.15652 -0.166795475 -10.182834 14551.88 8.757515e-39 1.423908e-36
prot8        -0.10425341 64.63523 -0.186904167 -10.780359 14461.18 9.429854e-39 1.423908e-36
prot9        -0.03426380 53.48057  0.007403722  -8.854471 13713.49 1.767090e-38 2.021378e-36
prot10       -0.75250251 66.62646  0.327497120 -10.894506 13480.51 2.164184e-38 2.021378e-36
</code></pre>

<p>Aside from thinking that I didnâ€™t do this correctly, the result for Genotype looks incorrect to me. Any input would be much appreciated.</p>
"
"0.0953462589245592","0.122743282386443"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.168174993036504","0.162374100149152"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.111237302078652","0.107400372088138"," 96362","<p>I'm trying to compare some data and see if there is a significant P<sub>interaction</sub> value between them. The data is highly skewed and thus I would like to use a transformation; a log transformation results in highly non-normal residuals, thus I am looking for a more appropriate transformation, if it exists.  I came upon the Box-Cox transformation, and I'm trying to see if it will work.  However, for every dataset I have a unique lambda, and thus a different equation of the form </p>

<p><a href=""http://upload.wikimedia.org/math/0/2/2/02212644563e50f011700bb2e24c5ea4.png"" rel=""nofollow"">Box-Cox</a></p>

<p>using the former, because my lambda value was found to be not zero on all occasions.  </p>

<p>My question, therefore, is if I can statistically compare two data sets transformed with different lambda values, or if there is a way to find a lambda value which is the maximum likelyhood for both data sets.  Or if I've made a horrible mistake.  Thank you!</p>

<p>~~~~~</p>

<p>This is how I found my lambda value, just to make sure I did not make a mistake.</p>

<p>Assume data sets Data1 and Data2, where Data1 is the response variable. </p>

<pre><code>library('MASS')

    #Initial regression to get regression object
LM &lt;- lm(Data1 ~ Data2)
LM.b &lt;- boxcox(LM)
    #x = lambda values, y = likelihood values
lam &lt;- LM.b$x
    lik &lt;- LM.b$y
lam.lik &lt;- cbind(lam,lik)
    #Sort by likelihood to get maximum likelihood lambda 
lam.lik.sort &lt;- lam.lik[order(-lik),]
LAM &lt;- lam.lik.sort[1,1]

    #Perform regression on transformed values
Data1.trans &lt;- ((Data1^LAM) - 1)/LAM
LM.trans &lt;- lm(Data1.trans ~ Data2)
shapiro.test(LM.trans$residuals)
</code></pre>

<p>Thank you for your time! </p>
"
"0.080403025220737","0.097037084956597"," 97437","<p>I am studying the factors influencing the annual salary for employees at a undisclosed bank. The regression model that I have decided to employ is as follows:</p>

<p>\begin{equation}
Y_{k}=\beta_{1}+\beta_{2}E_{k}+\beta_{3}D_{gk}+\beta_{4}D_{mk}+\beta_{5}D_{2k}+\beta_{6}D_{3k}+\varepsilon_{k}
\end{equation}
where $Y_{k}$ is the logarithm of annual salary, $E$ is the number of years of education, $D_{g}$ is a gender dummy, $D_{m}$Â is a minority dummy, and where </p>

<p>\begin{equation}
D_{2}=\begin{cases} 1 &amp;\text{Custodial job} \\ 0 &amp; \text{Otherwise} \end{cases}
\end{equation}<br>
and 
\begin{equation}
D_{3}=\begin{cases} 1 &amp;\text{Management job} \\ 0 &amp; \text{Otherwise} \end{cases}
\end{equation}
As you know, whenever one deals with GLS, $\Omega$ will almost surely be unknown and thus have to be estimated. In general there are $\frac{n(n+1)}{2}$ parameters to be estimated, which makes it pretty impossible to come up with a viable estimation out of $n$Â observations. This is usually counteracted by imposing some structure on $\Omega$.</p>

<p>In my case, I would like to make the assumption that the disturbance terms $\varepsilon_{k}$ in the above regression model have variance $\sigma_{i}^{2}$ for $i=1,2,3$, according to whether the $i$-th employee has a job in category 1,2, or 3 respectively. Now, we may introduce the transformations $\gamma_{1}=\log (\sigma_{1}^{2}),\gamma_{2}=\log(\sigma_{2}^{2}/\sigma_{1}^{2})$, and $\gamma_{3}=\log(\sigma_{3}^{2}/\sigma_{1}^{2})$ so as to enable us to formulate the following model for</p>

<p>\begin{equation}
\sigma_{k}^{2}= \exp \{ \gamma_{1}+\gamma_{2}D_{2k}+\gamma_{3}D_{3k} \}
\end{equation}
Since $\hat{\beta}_\rm{OLS}$ is a consistent estimate of $\beta$, even under the assumption of heteroscedasticity, we have that $\hat{\beta}_{\rm OLS} \xrightarrow[]{p}\beta$ as the number of observations increase. We may therefore argue that $e_{k}^{2} \approx \sigma_{k}^{2}$, and so we can regress upon information that we already possess. </p>

<p><strong>Summary of procedure</strong></p>

<p><strong>(1)</strong> Calculate the OLS estimate.</p>

<p><strong>(2)</strong> Calculate the OLS residual $\textbf{e}=\textbf{Y}-\textbf{X}\hat{\beta}$</p>

<p><strong>(3)</strong> Calculate the OLS estimate of $\gamma$ from $e_{k}^{2}=f_{\gamma}(Z_{k})+\overline{\varepsilon}_{k}$.</p>

<p><strong>(4)</strong> Calculate the FGLS estimate as the GLS estimate with $\hat{\Omega}=\Omega(\hat{\gamma})$ in place of $\Omega$. </p>

<blockquote>
  <p>What I would like to know is whether or not one can perform this estimation using a known function in R, say <code>gls</code>? If the answer is yes, then how exactly should I write to ensure that that my heteroscedasticity assumption is taken into account? Thanks for taking the time! Have a great day!</p>
</blockquote>
"
"0.127128345232746","0.107400372088138","102998","<p>I have roughly 15 variables / attributes characterizing 6k customers in my data set. As they are categorical I have transformed them into 1 attribute for each possible value (1-out-of-K coding). An example could be Region with values ""A"", ""B"" and ""C"", which is transformed into 3 variables: <code>Region_A</code>, <code>Region_B</code> and <code>Region_C</code>. The same goes for other variables such as the <code>Sales Channel</code>. After this transformation I now have around 70 attributes. </p>

<p>I would like to examine if there are any significant 2-way interactions between the different variables with regards to a response variable (concerning <code>customer quality</code>) using logistic regression. For instance, it is interesting to see if there is an interaction between <code>Region_A</code> and <code>Sales Channel 1</code>. However, there are very many possible interactions and therefore I would like to start by removing some variables, which have very few observations connected to them. An example could be that only 3 customers come from <code>Region_A</code>.</p>

<p>More specifically, I would start by removing all attributes that have 5 observations or less connected to them (out of 6k observations). However, I cannot find out how to do that. Thus I have the following questions:</p>

<ol>
<li><p>Does my thinking make sense? Or should I approach the issue in another way?</p></li>
<li><p>How do I remove all attributes in a dataset which has fewer than 5 observations connected to them? The values of the variables are always 0 or 1 as the customer is either from <code>Region A</code> (=1) or not from <code>Region A</code> (=0).</p></li>
<li><p>After removing these variables there should be fewer interactions. However, it would still be quite a large amount. I would therefore also like to only examine interactions with 5 observations or more. I am thinking this could be done using a formula in the logistic regression, but can you help me how I would find the right variables for the formula?</p></li>
</ol>
"
"0.0898933149950989","0.0867926073205492","104485","<p>I have more of a programming background, and I am fairly new to statistics. I am currently trying to solve some sample exercises to get more familiar with data science / modelling. </p>

<h3>Problem Background</h3>

<p>A user posts a request on a forum. Considering number of responses / number of up votes / users' reputation / etc., can you predict if the request will be fulfilled? There are about 4000 rows of test data available.</p>

<h3>Current Attempts</h3>

<p>I have created basic graphs to investigate correlation, and applied a few transformations to the data. 
I have decided to go with logistic regression, and based on the graphs, have chosen 2 factors for the initial model. </p>

<h3>Problem</h3>

<p>I have created the basic model using R, and I can see some summary stats, but I am a bit lost when I try to understand (a) How well the model fits the data and (b) whether the model is accurate. I tried Googling around, but most of the articles were too technical for me. Is there a cheat sheet/quick test of some sort that I can apply? Or can somebody suggest the simplest ""complicated"" article?</p>
"
"0.2341070818533","0.233826186772478","109464","<p>I am new to regression and having problem in solving Heteroscedasticity in OLS. Have done lots of homework and test before seeking your advice. Sharing the background and what I have done to solve the problem. Hope you can share your thoughts if my approach was correct.</p>

<p><strong>Objectives:</strong></p>

<ol>
<li>To find the relationship (model) between an explanatory variable (x) and an explained variable (y) using OLS regression.</li>
<li>if a model (relationship) is found, its usefulness and accuracy of prediction will be studied.</li>
</ol>

<p><strong>Dataset (Cross-sectional):</strong></p>

<ol>
<li>Have 4 datasets, with each 350 sample size.</li>
<li>Each dataset obtained using different intensity of experiment and this is already captured by the explanatory variable in x.</li>
<li>Due to the heterogenity of data, not possible to lump all into a single dataset.</li>
</ol>

<p><strong>Requirement:</strong></p>

<p>One common and statistically acceptable model for all the 4 datasets using OLS</p>

<p><strong>Steps Followed:</strong></p>

<ol>
<li><p>Explanatory Analysis: Found Non-linear relationship </p></li>
<li><p>As intending to use OLS, did 3 transformations of variables in attempt to have linearity:
a) ln(x) ~ ln(y);
b) ln(x) ~ y;
c) x ~ ln(y).
<strong>Note:</strong> Kept d) x ~ y as benchmark</p></li>
<li><p>Did heteroscedasticity test using Breusch-Pagan (BP) test in R for 2(a)-(d) for all the datasets in attempt to find valid model(s).
On the best case i.e 2b), only 2 out of 4 datasets passed the BP test (p-value>0.05)</p></li>
<li><p>As the aim is to have one common model for all the 4 datasets, another variable transformation is done using Tukey's Ladder of Transformation in attempt to have homoscedasticity:
a) ? ? {-2,-1,-0.5, 0.5, 1, 2} is used for x/y/x and y for each of the models in 2(a)-(d). Have total of 64 models (16 x 4) to consider. X and Y refer to the transformed x and y;
b) Now have 2 models passed BP test for 3 out of 4 datasets in the best case;
c) The one that failed has p-value &lt;2.20E-16.</p></li>
<li><p>[deadlock unable to find one valid model that passes all the 4 datasets]</p></li>
<li><p>Proceeded to take the two valid models in Step 4 and done inference Test:
a) the p-values for t-test and F-test are below 0.05 for all the 4 datasets;
b) R-square are above 0.9402 for all the 4 datasets.</p></li>
<li><p>Did cross validation and selected the best model using the smallest mean square error against the two ""valid"" models. Did back transformation on the original scale first before the selection is done so that its apple to apple data comparison. The mean average percentage error for the best model is below 10%</p></li>
<li><p>Now tried to use the best model for prediction:
a) Selected 20 random x values which were not part of the dataset;
b) Predicted y and compared it against Measured y;
c) the  mean average percentage error is below 8% and within the model's mean average percentage error i.e below 10%.</p></li>
</ol>

<p><strong>The problem:</strong></p>

<p>With the steps above I am unable to get a model that passes the heteroscedasticity test all the 4 datasets. Have I done anything incorrectly or is there anything more can be done in Step 4? </p>

<p>Believe mis-specification issue has duly been attended. Not intending to use GLS as I need to use .OLS</p>

<p>I have used heteroscedasticity robust standard errors as a remedy of heteroscedasticity on the one dataset that failed BP test per the Youtube below.
Refer - <a href=""https://www.youtube.com/watch?v=hFoDDwTF4KY"" rel=""nofollow"">https://www.youtube.com/watch?v=hFoDDwTF4KY</a></p>

<p>The standard error increased and t-value decreased for Y for the HC3 corrected dataset. 
But the Y= a  + b X model remain the same.</p>

<p>Is it sufficient to show the p-value for t-test and F-test for the corrected dataset are still below 0.05 hence its ok to use the same Y= a+bX though it failed the BP test earlier?</p>

<p>Hope you can share your thoughts as I am new to regression. </p>

<p>Using many reference books to learn such as </p>

<ol>
<li>Introduction to Econometrics by Wooldridge</li>
<li>Basic Econometrics by Gujerati</li>
<li>Regression Analysis by Example by Chatterjee</li>
</ol>

<p><strong>Original:</strong></p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.612116   0.009006  -68.76   &lt;2e-16 ***
Y                     5.955984   0.039653  145.65   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic: 2.092e+04 on 1 and 348 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Heteroskedasticity Robust Standard Errors corrected using HC3:</strong></p>

<pre><code>Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.61212    0.01767  -33.77   &lt;2e-16 ***
Y                     5.95598    0.08432   69.12   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic:  4640 on 1 and 348 DF,  p-value: &lt; 2.2e-16

Note: Heteroscedasticity-consistent standard errors using adjustment hc3 
</code></pre>

<p>Thanks</p>
"
"0.080403025220737","0.097037084956597","110469","<p>So the background is that the I collected yield data for past 5-6 decades and location from where I collected yield data had high yielding varieties introduced over time. I am looking at the relationship between yield and rainfall but this introduction of HYV might affect the true impact of monsoon on yield and therefore I am detrending the data to remove the effect of HYV.</p>

<p>I did a linear regression of yield against time in R:</p>

<pre><code>mdl1 &lt;- lm(yield ~ time, data=data)
</code></pre>

<p>and then removed the linear trend by taking the residuals of the above regression: </p>

<pre><code>yield.res &lt;- resid(mdl1)
</code></pre>

<p>Now I am using these residuals for my subsequent analysis. For example, the relationship between yield and rainfall is: </p>

<pre><code> mdl2 &lt;- lm(yield.res ~ rain, data=data)
</code></pre>

<p>In this case, do my <code>yield.res</code> have to be normally distributed before I do this regression? If yes, what sort of transformation do I need to use? Since <code>yield.res</code> consists of both negative and positive numbers, I am slightly confused how to go about it.</p>
"
"0.0898933149950989","0.0650944554904119","112541","<p>I'm having trouble interpreting the results from the Spread-Level Plot function in R (car package). The documentation says:</p>

<blockquote>
  <p>PowerTransformation<br>
  spread-stabilizing power transformation, calculated as 1 - slope of the line fit to the plot.</p>
</blockquote>

<p>This is not explicit enough for me. Should this transformation be applied to every variable in the regression?</p>

<p>For example, assume I have an lm object given by:</p>

<pre><code>myFit &lt;- lm(y ~ x1 + x2)
</code></pre>

<p>Then I use Spread-Level Plot:</p>

<pre><code>slp(myFit)
</code></pre>

<p>If the 'suggested power transformation' is 0.5, then does that imply a homoscedastic model could be fit using one of the following?</p>

<pre><code>refitA &lt;- lm(sqrt(y) ~ sqrt(x1) + sqrt(x2))
refitB &lt;- lm(sqrt(y) ~ x1 + x2)
refitC &lt;- lm(sqrt(y) ~ sqrt(x1 + x2))
</code></pre>

<p>If I understand correct, refitA would be the suggested model to approximate homoscedasticity. On the other hand, if I <em>only</em> want to transform the LHS, I would use the <code>powerTransform</code> function (also from car package). i.e., an ""estimated transform parameter"" of 0.5 from the powerTransform function would imply that refitB is homoscedastic.</p>

<p>Is this correct?</p>

<p>Thanks!</p>
"
"0.0778498944161523","0.0751646028002829","120443","<p>I have run a linear regression with the following equation (in r):</p>

<pre><code>lm(formula = logTotal ~ Continent + logArea + Method + Servs)
</code></pre>

<p>where <code>Total</code> is $/ha/year (numeric), <code>Area</code> is hectare (numeric), <code>Continent</code> and <code>Method</code> are factors and <code>Servs</code> is numeric.  It returns the output:</p>

<pre><code>Call:
lm(formula = logTotal ~ Continent + logArea + Method + Servs)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.99416 -0.26931 -0.00622  0.28885  1.19875 

Coefficients:
                       Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)            -1.82886    0.71446  -2.560 0.016903 *  
ContinentAsia           3.82452    0.60471   6.325 1.28e-06 ***
ContinentAustralasia    4.52516    0.96517   4.688 8.35e-05 ***
ContinentEurope         2.18022    0.48260   4.518 0.000130 ***
ContinentGlobal         2.44750    0.74092   3.303 0.002881 ** 
ContinentNorth America  2.35244    0.55281   4.255 0.000256 ***
ContinentSouth America  3.67853    0.61454   5.986 2.99e-06 ***
logArea                 0.03643    0.03583   1.017 0.318911    
MethodCVM              -0.18171    0.43296  -0.420 0.678300    
MethodOther hedonic    -1.53284    0.79781  -1.921 0.066165 .  
MethodValue Transfer    0.98101    0.29773   3.295 0.002941 ** 
Servs                   0.10723    0.04273   2.509 0.018948 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.552 on 25 degrees of freedom
  (65 observations deleted due to missingness)
Multiple R-squared:  0.7582,    Adjusted R-squared:  0.6518 
F-statistic: 7.127 on 11 and 25 DF,  p-value: 2.501e-05  
</code></pre>

<p>I wish to predict <code>Total</code> based on various inputs, however I'm a bit lost on fully understanding the output. If I wished to predict ""Total"" on the basis of:</p>

<pre><code>Continent:Global, Area:1 hectare, Method:CVM, Servs:11
</code></pre>

<p>is the following equation correct?</p>

<pre><code>exp(Total) = 2.44750 + exp(1*0.03643) - 0.18171 + (11*0.10723)
</code></pre>

<p>I have read UCLA's statistics help site's <a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/log_transformed_regression.htm"" rel=""nofollow"">FAQ on log transformed regression</a>. I feel like I've oversimplified it but I just keep reading that link over and over and still not fully understanding.  Also read <a href=""http://stats.stackexchange.com/questions/20397/how-to-interpret-logarithmically-transformed-coefficients-in-linear-regression"">How to interpret logarithmically transformed coefficients in linear regression?</a>.</p>
"
"0.0674199862463242","0.0867926073205492","120749","<p>I'm working on a biological question, with species data derived from an external database, which has multiple response and predictor variables. As a result, I want to do multivariate regression across a phylogeny to empirically test if my response variables are significantly different in respect to my predictors.</p>

<p>Please refer to source [2] and it's citations for your own investigation of this process.</p>

<p>I know how to do multiple regression via pGLS, but the R package [1] only mentions predictors and response. Furthermore, another source [2] discusses how multivariate regression though pGLS in R can be done, but requires one to transform the data under a Brownian motion model. (Edit: It seems that [2] is a solution...so I'm looking the process).</p>

<p>Sources:</p>

<ol>
<li><p>The vignette for the pGLS package (<a href=""http://cran.r-project.org/web/packages/pGLS/pGLS.pdf"" rel=""nofollow"">pdf</a>)</p></li>
<li><p>D.C. Adams. 2014. A Method for Assessing Phylogenetic Least Squares Models for Shape and Other High-Dimensional Multivariate Data. Evolution. 68:9 2675-2688. doi: 10.1111/evo.12463</p></li>
<li><p>Revell, L. J. (2010), Phylogenetic signal and linear regression on species data. Methods in Ecology and Evolution, 1: 319â€“329. doi: 10.1111/j.2041-210X.2010.00044.x</p></li>
</ol>
"
"0.201235851101624","0.203546706606235","121255","<p>I am working on a dataset with a continuous response (which could be dichotomized), one continuous covariate, and multiple categorical variables. The continuous covariate (weight) is directly correlated to the response, and must be accounted for so that we can determine which of the categorical variables are most influential to the response. Here is <a href=""http://pastebin.com/891mheRf"" rel=""nofollow"">example data</a>.</p>

<p>Each row is an individual subject, with the continuous response, the covariate of underlying primary importance (weight), then 10 categorical variables that are to be tested (individuals can score yes = 1 to multiple categories). </p>

<p>My first thought in working with this data was a linear model, with stepwise elimination of categorical variables.</p>

<pre><code> lm(Response~Weight+var1+var2...+var11)
</code></pre>

<p>However, I believe there is extensive collinearity, since some variables may be eliminated early, but then are significant if you add them back into the model at the end. I'm curious if there is a better way to approach this data in R, that may help sort through which of the variables are of most importance to influencing the response. My two thoughts are</p>

<p>1) Building a single model with the continuous covariate and 5 categorical variables that were selected to be of most interest before the study, and refrain from any stepwise reduction of this model</p>

<p>2) Some sort of princicpal component regression, which I know little about at this point and thus wanted to ask advice before proceeding down that path</p>

<p>To help visualize the data, and the effect of Weight on the Response, I've constructed the follow plots. In the second plot, I attempt to control for the natural Response~Weight relationship.</p>

<pre><code> #GRAPH
 library(ggplot2)
 library(reshape2)

 Data &lt;- read.table(""Fake Data.txt"",header=TRUE)
 #Creating long format for ggplot2
 Data2&lt;-melt(Data, id.vars = c(""Subject"",""Response"",""Weight""), measure.vars = c(""var1"",""var2"",""var3"",""var4"",""var5"",""var6"",""var7"",""var8"",""var10"",""var11""))

 #Adding in weight to the varibles to be plotted
 Data2&lt;-rbind(Data2,Data2[1:31,])
 levels(Data2$variable)&lt;-c(levels(Data2$variable),""Weight"")
 Data2[311:341,4]&lt;-""Weight""
 Data2[311:341,5]&lt;-1

 #Removing rows where the categorical variable is 0=No
 for(i in 1:length(Data2[,1])){
 if(Data2[i,5]==0)Data2[i,]&lt;-NA
 }
 Data3&lt;-na.omit(Data2)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables
  scatter &lt;- ggplot(Data3, aes(Weight, Response, colour = variable))
 scatter + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/AgQog.jpg"" alt=""enter image description here""></p>

<pre><code> #Zeroing the Response~Weight relationship to remove its influence. Correction coefficients from linear model fit to Response~Weight
 Data4&lt;-Data3
 Data4$Response&lt;-Data4$Response-(0.01494*(Data4$Weight)+ 84.67715)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables for zeroed Response~Weight relationship (as seen in bottom right facet)
 scatter2 &lt;- ggplot(Data4, aes(Weight, Response, colour = variable))
 scatter2 + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/JfCrn.jpg"" alt=""enter image description here""></p>

<p>This second plot helps to show how, when the Response~Weight relationship is controlled for, variables like 'var10' have no influence on the response, while variables like 'var11' have all individuals below that zero-centered mean. Thus, from a visual test, I could identify var11 as a categorical variable of interest that negatively influences our response.</p>

<p>Additionally, this plot shows some of the confounding in this dataset, as you can see certain categorical variables 'clump'/are only documented in certain weight ranges. This is due to the underlying biology.</p>

<p>As a final note, I wonder if it is appropriate to use the corrected response in the second plot as the 'Response' for a linear model, thus eliminating the need for a 'Weight' covariate, or if it is incorrect to use such a transformation</p>

<p>Any thoughts are much appreciated</p>
"
"0.0635641726163728","0.0613716411932216","123059","<p>As the title says, does a linear regression model make assumptions about distributions of depended and independent variables and what should these distributions be for the model to work as expected ?  I have a log normal distribution (or rather a very positively skewed one) for both  dependent and independent variables and I always log transformed them before doing any fitting but came across an article saying that is not necessary. Also just to add to this, I would like to use the model to predict values of individual data points. </p>
"
"0.211554354139178","0.221278599181538","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.101929438287525","0.114815827304529","126356","<p>For linear and parametric regression there are multiple tests where variables and residuals are used by means of performing a linear regression function to test serial correlation of regression errors and homocedasticity of regression errors. </p>

<p>My question is about non linear and non parametric regression for prediction or classification such us SVM, NeuralNets, knn, Recursive Partitioning, Adaptive Regression Spline, etc. </p>

<p>In this regard my questions are:</p>

<ol>
<li><p>As is not linear regression what is the equivalent of OLS assumptions for non linear non parametric regression. Are the consequences of OLS violation in the context of nonlinear and non-parametric regression still valid? </p></li>
<li><p>How could I test or what tests exist for serial correlation of errors for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes in mind like testing for significant acf or pacf on the residual errors - Unsure if this is OK).</p></li>
<li><p>How could I test or what tests exist for homocedasticity for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes too mind like homogenity of distances between the residual errors across time).</p></li>
<li><p>Would it be better to transfor the data into linear by seeking some adequate transformation as to avoid all the non linearity issues mentioned above? </p></li>
</ol>

<p>Thank you</p>
"
"0.127920429813366","0.13723116159877","126990","<p>I would like to conduct a meta-analysis in the context where I have studies available that measure a continuos variable at multiple time points (0, 1, 2, 3, 4, 5). Time 0 represents the baseline where values are at 100%. Right afterwards there is an intervention and the effect of the intervention is measured over time (114% represents a 14% change relative to baseline). Also I have given two different groups that received different interventions.</p>

<p>Please consider the following dummy data set:</p>

<pre><code>library(ggplot2)
library(metafor)
library(dplyr)
n &lt;- 10
a &lt;- c(rnorm(n,100,0), rnorm(n, 110,2), rnorm(n,130,2), rnorm(n,135,2), rnorm(n,130,2), rnorm(n,125,2))
b &lt;- c(rnorm(n,100,0), rnorm(n,107,2), rnorm(n,122,2), rnorm(n,128,2), rnorm(n,122,2), rnorm(n,125,2))
sd &lt;- rnorm(n,10,1)
my_dat &lt;- data.frame(mean=c(a, b), sd=rep(sd,12), time=rep(c(rep(0,n), rep(1,n), rep(2,n), rep(3,n), rep(4,n), rep(5,n)),2), group=c(rep(""A"", 60), rep(""B"",60)), n=rep(n,120))
my_dat$study &lt;- 1:10
p &lt;- ggplot(aes(y=mean, x=time, colour=group), data=my_dat)
p + geom_jitter() + geom_smooth() + ylab(""% relative to baseline"") + xlab(""time"") 
</code></pre>

<p><img src=""http://i.stack.imgur.com/Lx6TY.png"" alt=""raw data example""></p>

<p>I would like to :</p>

<p>1) investigate the main effect of time (as well as post-hoc tests) for each group individually using the metafor package.</p>

<p>2) investigate the main effect of group (as well as post-hoc tests) for each point in time using the metafor package.</p>

<p>3) investigate group-time interactions.</p>

<p>Thus I rearrange the data and calculate hegdes g relative to baseline t0:</p>

<pre><code>t0_dat &lt;- summarise(group_by(my_dat[my_dat$time==0,], study, group), t0_mean=mean(mean), t0_sd=mean(sd))
my_dat &lt;- merge(my_dat, t0_dat, by=c(""study"", ""group""), all.x=T)
my_dat &lt;- escalc(m1i=mean, m2i=t0_mean, sd1i=sd, sd2i=t0_sd, n1i=n, n2i=n, measure=""SMD"", data=my_dat, append=T)
p &lt;- ggplot(aes(y=yi, x=time, xmin=yi-vi, xmax=yi+vi, colour=group), data=my_dat)
p + geom_point() + geom_smooth() + ylab(""hedges g"") + xlab(""time"") + xlim(c(0,5)) + ylim(c(0,5))
</code></pre>

<p><img src=""http://i.stack.imgur.com/Gf11j.png"" alt=""enter image description here""></p>

<p>Finally I can run the meta-analysis:</p>

<pre><code>m1 &lt;- rma(yi,vi, data=my_dat, mods=~time*group)
summary(m1)
</code></pre>

<p>This indicates a sig. effect of time, a sig. effect of group but no interaction:
Model Results:</p>

<pre><code>         estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt        1.4500  0.2159   6.7158  &lt;.0001   1.0269   1.8732  ***
time           0.3294  0.0667   4.9363  &lt;.0001   0.1986   0.4602  ***
groupB        -0.6808  0.2994  -2.2738  0.0230  -1.2676  -0.0940    *
time:groupB    0.0802  0.0932   0.8609  0.3893  -0.1024   0.2628     

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Is this an valid approach?
Would it be appropriate to instead of converting to effect size (hedges g) to use the percentage values (as extracted from the papers) and log-transform them as suggested <a href=""http://stats.stackexchange.com/questions/34057/estimating-percentages-as-the-dependent-variable-in-regression"">in this question</a>, <a href=""http://stackoverflow.com/questions/9958722/r-variable-selection-for-multiple-regression-w-percentage-dependent-variable"">in this question</a> or in the comments below?
Hints to papers that conducted comparable analysis are more then welcome!</p>
"
"0.185319816380856","0.168402394120179","129337","<h3>The out-of-context short version</h3>

<p>Let $y$ be a random variable with CDF
$$
F(\cdot) \equiv \cases{\theta &amp; y = 0 \\ \theta + (1-\theta) \times \text{CDF}_{\text{log-normal}}(\cdot; \mu, \sigma) &amp; y &gt; 0}
$$</p>

<p>Let's say I wanted to simulate draws of $y$ using the inverse CDF method. Is that possible? This function doesn't exactly have an inverse. Then again there's <a href=""http://stats.stackexchange.com/q/73028/36"">Inverse transformation sampling for mixture distribution of two normal distributions</a> which suggests that there is a known way to apply inverse transformation sampling here.</p>

<p>I'm aware of the two-step method, but I don't know how to apply it to my situation (see below).</p>

<hr>

<h3>The long version with background</h3>

<p>I fitted the following model for a vector-valued response, $y^i = \left( y_1 , \dots , y_K \right)^i$, using MCMC (specifically, Stan):</p>

<p>$$
\theta_k^i \equiv \operatorname{logit}^{-1}\left( \alpha_k x^i \right), \quad \mu_k^i \equiv \beta_k x^i - \frac{ \sigma^2_k }{ 2 } \\
F(\cdot) \equiv \cases{\theta &amp; y = 0 \\ \theta + (1-\theta) \times \text{CDF}_{\text{log-normal}}(\cdot; \mu, \sigma) &amp; y &gt; 0} \\
u_k \equiv F(y_k), \quad z_k \equiv\Phi^{-1}{\left( u_k \right)} \\
z \sim \mathcal{N}(\mathbf{0}, R) \times \prod_k f(y_k) \\
\left( \alpha, \beta, \sigma, R \right) \sim \text{priors}
$$</p>

<p>where $i$ indexes $N$ observations, $R$ is a correlation matrix, and $x$ is a vector of predictors/regressors/features.</p>

<p>That is, my model is a regression model in which the conditional distribution of the response is assumed to be a Gaussian copula with zero-inflated log-normal marginals. I've posted about this model before; it turns out that Song, Li, and Yuan (2009, <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2008.01058.x/abstract"" rel=""nofollow"">gated</a>) have developed it and they call it a vector GLM, or VGLM. The following is their specification as close to verbatim as I could get it:
$$
f(\mathbf{y}; \mathbf{\mu}, \mathbf{\varphi}, \Gamma) = c\{ G_1(y_1), \dots, G_m(y_m) | \Gamma \} \prod_{i=1}^m g(y_i; \mu_i, \varphi_i) \\
c(\mathbf{u} | \Gamma) = \left| \Gamma \right|^{-1/2}\exp\left( \frac{1}{2} \mathbf{q}^T \left( I_m - \Gamma^{-1} \right) \mathbf{q} \right) \\
\mathbf{q} = \left( q_1, \dots, q_m \right)^T, \quad q_i = \Phi^{-1}(u_i)
$$
My $F_K$ corresponds to their $G_m$, my $z$ corresponds to their $\mathbf{q}$, and my $R$ corresponds to their $\Gamma$; the details are on page 62 (page 3 of the PDF file) but they're otherwise identical to what I wrote here.</p>

<p>The zero-inflated part roughly follows the specification of Liu and Chan (2010, <a href=""http://www.jstatsoft.org/v35/i11/paper"" rel=""nofollow"">ungated</a>).</p>

<p>Now I would like to simulate data from the estimated parameters, but I'm a little confused as to how to go about it. First I thought I could just simulate $y$ directly (in R code):</p>

<pre><code>for (i in 1:N) {
    for (k in 1:K) {
        Y_hat &lt;- rbinom(1, 1, 1 - theta[i, k])
        if (Y_hat == 1)
            Y_hat &lt;- rlnorm(1, mu[i, k], sigma[k])
    }
}
</code></pre>

<p>which doesn't use $R$ at all. I'd like to try to actually use the correlation matrix I estimated.</p>

<p>My next idea was to take draws of $z$ and then convert them back to $y$. This also seems to coincide with the answers in <a href=""http://stats.stackexchange.com/q/78894/36229"">Generating samples from Copula in R</a> and <a href=""http://stats.stackexchange.com/q/123698/36229"">Bivariate sampling for distribution expressed in Sklar&#39;s copula theorem?</a>. But what the heck is my $F^{-1}$ here? <a href=""http://stats.stackexchange.com/q/73028/36229"">Inverse transformation sampling for mixture distribution of two normal distributions</a> makes it sound like this is possible, but I have no idea how to do it.</p>
"
"0.194325082689389","0.243213949547965","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.100503781525921","0.097037084956597","130643","<p>I tried a regression in the form ${\rm logit}(Y) = {\rm coefficient}\times X + 0 + e$, where $Y$ is a binomial variable and $X$ is a factor variable with $n$ levels. I noticed that removing the intercept yields higher $p$ values. I'm wondering how to interpret it though.</p>

<p>Since removing the intercept makes it equal to $0$, I believe that the coefficients returned are relative to a $0$ probability of the event $Y$ and that all $X$ factors are in the $0$ state. But I think this is impossible isn't it?</p>

<p>$X$ are mutually exclusive factors, therefore it's impossible to have a case where no factor is $1$, at least in the presented observations. And it cannot be interpreted like the coefficient is relative to hypothetical cases in which really no one of the factors is present, because we have no data like that.</p>

<p>Regarding $Y$ having a $0$ intercept, wouldn't it mean forcing the probability of the event to $0$ when none of the factors is present? Again, this is an impossible case.</p>

<p>Nonetheless this kind of regression would allow me to retrieve pure probability range of the event Y given a factor by transforming the coefficients in the confidence intervals given as $\exp({\rm coefficient})/(1 + \exp({\rm coefficient}))$, and the $p$ values would test whether this probability is not $50\%$. This could also be a valuable result, since it would give independent probabilities for each factor.</p>

<p>Am I wrong?</p>
"
"0.265908011739155","0.256735994745803","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.135519271363624","0.143929256529458","134141","<p>Recently I am reading a paper where the authors use the GAM to make predictions. In brief, the data looks like following:</p>

<pre><code>  y    i    j     x    weekend
5.6    1    1   4.6    Mon.
6.5    1    2   5.6    Mon.
...
4.6    2    1   6.7    Sta.
2.4    2    2   1.2    Sta.
...
</code></pre>

<p>where <code>y</code>, <code>x1</code>, <code>x2</code> are continuous numbers, <code>weekend</code> is the day of the week. In the paper, the authors use the following formula:  </p>

<p>$$y_{ij} = \beta_0 + b_{0i} + \beta_1{\rm weekend}_i + f_1(x_{ij}, {\rm weekend}_i) + \varepsilon_{ij}$$</p>

<p>In the formula, $\beta_0$ is the overall mean, $b_{0i}$ is the random intercept, ${\rm weekend}_i$ determines whether it is weekday or weekend. Ans so I transform ${\rm weekend}$ from {Mon., Thu., .., Sun.} into {0, 1}. And $f_1$ is cubic regression function with 17 spline knots, and in fact will generate two smooth functions one for weekday, another for weekend.</p>

<p>I want to use following code:  </p>

<pre><code>gam(y~ s(i,bs=""re"") + weekend + s(x, by=weekend, bs=""cr"", k=17))
</code></pre>

<p>But I'm not sure whether it fits the formula or not. My questions are:</p>

<ol>
<li><code>gam</code> will automatically generate the mean of the model, so there is no need to specify a $\beta_0$ in the code?  </li>
<li>Is it right that by using <code>s(i,bs=""re"")</code>, the <code>gam</code> will calculate different random effect with distribution $N(0, \delta_i)$ for every $i$ specifically?</li>
<li>Is it good to transform weekend into 0-1 value? and in the code <code>s(x, by=weekend, bs=""cr"", k=17)</code>, does the <code>by</code> keyword mean that it will generate different smooth functions of <code>x</code> for different <code>weekend</code> value?</li>
<li>The last question is that without specifying <code>knots=list()</code>, as in the above code, the default behaviour of the model is to put knot points evenly of the range of value?</li>
</ol>
"
"0.149071198499986","0.143929256529458","134803","<p>Suppose we have a system that essentially evolves as follows:</p>

<pre><code>stock_t+1  = stock_t + inflows_t - outflows_t 
inflows_t  = a1*predictor11_t + a2*predictor12_t+.... error1_t
outflows_t = b1*predictor21_t + b2*predictor22_t+.... error2_t
</code></pre>

<p>I have observations for each of these variables, i.e. I can observe the stock, the inflows, outflows, and the sum of the flows (they add up correctly), as well as each of the predictors. All variables are time series, and simple time series analysis goes a long way. That said, while the stock is not overly volatile and relatively easy to predict, the flow variables are much more volatile and more challenging to model.</p>

<p>Using regression analysis, in the beginning I've only attempted to model the evolution of the stock using a combination of the flow predictors. I found that the best predictors of the stock are close to the best predictors of the flows (though not identical, i.e. some additional transformation is required). Recently, I have also attempted to model the flows. </p>

<p>The trouble is - not unexpectedly, I should say - that using the best regression models for each of the series, the system does not follow the add-up constraint; or put differently: if I calculate the evolution of the stock using the first equation, based on a starting value and equations 2-3, then my prediction of the stock is quite a bit different than my direct forecast of the stock. At the same time, since the stock model is the model that I have the most faith in, I'd rather not move away from these predictions.</p>

<p>So I was thinking that there surely must be a way to model the entire system directly, rather than estimate each equation separately. Right now a state-space approach comes to mind; before I go off into that direction, though, I am wondering whether I am missing something and whether anyone has a different suggestion.</p>

<p>PS I'm using R</p>
"
"0.100503781525921","0.097037084956597","134837","<p>My data has a binary outcome (attack or not attack), day (20 day in repeated measured design) and some covariates (nestlingâ€™s movement).
The objectives of my experiment are testing the effect of time and other factors and selecting useful variables affecting outcomes.</p>

<p>My data look like below</p>

<pre><code>subject outcome Day nestling.move
   1        A    1      N 
   2        A    1      Y 
   3        A    1      Y 
   4        N    1      Y 
   5        N    1      Y 
   6        N    1      Y 
   7        N    1      Y 
   8        N    1      N 
   9        N    1      N 
   .        .    .      . 
   .        .    .      . 
   1        A    20     N 
   2        A    20     N   
</code></pre>

<p>First of all, I simply transformed outcomes to ratios(attack rate for each day) and test if there is a correlation between attack rates and days by using Spearmanâ€™s rank correlation. But I think it is not a good way to test the effect of time on outcome.</p>

<p>I checked other <a href=""http://stats.stackexchange.com/questions/81246/unable-to-fit-repeated-measures-in-r"">post</a>. and I think I should used an AR1 model with logistic regression since it could be a time-varying processes. However, I don't know how to do this with R or SPSS. </p>

<p>Is this the correct syntax to use in R?</p>

<pre><code>model&lt;- glmmPQL(outcome ~  nestling.move + Day, data=mydata, family=binomial,  random = ~ 1 | subject, correlation = corAR1(form=~Day|subject)) 
</code></pre>
"
"0.212352996432344","0.213229393565553","135043","<p>I have three questions concerning accelerated failure time models (AFT), one statistical, one regarding how to implement these models in R, and one related to finding out information about what R is doing. In short my questions are;</p>

<p>1) What is the relationship between the Gumbel and Weibull distributions?</p>

<p>2) How can I use (1) to simulate a AFT model using Gumbel errors and fit this model in R?</p>

<p>3) Where can I find formulae regarding exactly what distribution specification R is using when fitting a Weibull distribution, and exactly what model is being fitted?</p>

<p>I am having difficulties implementing 2), which may be due to my mis-understanding of 1), but which I can't seem to resolve due to 3). Question (3) is self-explanatory but (2) and (3) require more detail;</p>

<p>1) It seems a standard result that if $U\sim Gumbel(\alpha,\beta)$ then $V:=\exp(U)\sim Weibull(\lambda,\sigma)$ where $\alpha=\log(\sigma)$ and $\beta=1/\lambda$. However using the definition of the Gumbel and Weibull distributions commonly used (for example Wikipedia), when I do the derivation I can only get the transformation $V':=1/\exp(U)$ to give this result but where $\alpha=-\log(\sigma)=\log(1/\sigma)$. Thus can anyone confirm or not any knowledge of this relationship, or perhaps suggest where I have gone wrong (for brevity in the first instance I do not supply the detail)?</p>

<p>2) My approach is to use</p>

<p>$Y_{i}:=\log\left(\frac{1}{T_{i}}\right)=\beta_{0} + \beta_{1}x_{i} + e_{i},\hspace{20pt}i=1,...,N$,</p>

<p>as a data-generating mechanism for the logarithm of the time to event where $e_{i}\sim Gumbel(\alpha,\beta)$, where $i$ indexes subjects, $x_{i}$ is a scalar covariate, and the $e_{i}$ are all independent. I choose $\alpha=-\beta*c$ where $c$ is Euler's constant in order to ensure $E[e_{i}]=\alpha+c\beta=0$. This gives</p>

<p>$Y_{i}\sim Gumbel(\beta_{0} + \beta_{1}x_{i}+\alpha,\beta)$,</p>

<p>and using (1)</p>

<p>$T_{i}\sim Weibull(1/\beta,\exp[-(\beta_{0} + \beta_{1}x_{i}+\alpha)])$</p>

<p>The code at the end of this post is a minimal working example of this approach, where I censor subjects if $T_{i}$ is greater than the median of the $N$ theoretical medians of $\{T_{1},...,T_{N}\}$, and create an event if not. This gives $50-60\%$ of subjects being censored, the balance having events, and I interpret this to be right-censoring (say the end of a study).</p>

<p>I then use the survreg package in R to try to fit an AFT to $Y_{i}$ using the ""dist=weibull"" option. Using $\beta_{0}=-10$ and $\beta_{1}=0$ gives the following output</p>

<p><img src=""http://i.stack.imgur.com/4sQlb.png"" alt=""enter image description here""></p>

<p>which gives the intercept being positive when it should be negative. Things get worse when using $\beta_{0}=-10$ and $\beta_{1}=2$ which gives the following output</p>

<p><img src=""http://i.stack.imgur.com/i7o3f.png"" alt=""enter image description here""></p>

<p>which is obviously wrong. Thus I would like to know what model I am actually fitting when using the survreg package.</p>

<p>The code below is a minimal working example (apart from some code to produce plots which can be helpful).</p>

<pre><code># minimal working example
set.seed(123)
require(survival)
#params of the gumbel(alpha_gum,beta_gum) distribution so that E[X]=0
beta_gum = 1/5 #
alpha_gum = -(beta_gum*(-digamma(1)))

#calc the mean of the errors using Eulers constant as the negative of the diagamma function
mu_e = alpha_gum + (beta_gum*(-digamma(1)))#should be 0   

# regression parameters
intercept = -10;
beta1 =0;
#beta1 =2;

#number of subjects
N=1000;

# vector of uniform random numbers
U = runif(N)

#vector for gumbel distributed errors
e = matrix(,nrow=N,ncol=1)


# log of time to event, time to event, mean LTTE
logTTE = matrix(,nrow=N,ncol=1)
Xbeta_LTTE= matrix(,nrow=N,ncol=1)
TTE = matrix(,nrow=N,ncol=1)
TTE2 = matrix(,nrow=N,ncol=1)

#censoring variable
censor = matrix(,nrow=N,ncol=1)

#simulate covariate from a normal distribution
covariate1 = rnorm(N,6,4)

for (i in 1:N)
{
  # calculate the Gumbel RV from the inverse CDF of the Gumbel
  e[i,1] = alpha_gum + (-beta_gum*log(-log(U[i])))

  #generate the mean log TTE  
  Xbeta_LTTE[i,1] = intercept + (beta1*covariate1[i])

  #add the errors
  logTTE[i,1] = Xbeta_LTTE[i,1] + e[i,1]  

  #transform to raw time variable - this is a Weibull dist
  #TTE_i ~ Weibull[1/beta_gum , exp(-[logTTE_i+alpha_gum])
  TTE[i,1] = 1/exp(logTTE[i,1])      
}

#calc the median the TTE given TTE ~ Weibull[1/beta_gum , exp(-[X_i^t*beta+alpha_gum])
lambda_array = exp(-(Xbeta_LTTE + alpha_gum + (beta_gum*(-digamma(1)))))
kappa = 1/beta_gum
median_TTE_array = (lambda_array)*(log(2)^(1/kappa))
median_TTE = median(median_TTE_array)

# calculate the censoring variable
for (i in 1:N)
{
  #censoring: subjects with a TTE &gt;median_TTE will be right-censored
  #i.e. study ends at T=median_TTE say
  if (TTE[i,1]&gt;median_TTE)
  {
    censor[i,1]=1 
    TTE2[i,1]=median_TTE
  }
  else
  {
    censor[i,1]=0    
    TTE2[i,1]=TTE[i,1]
  }  
}

#calculate the percentage of censored subjects and do a plot
pc_censored = sum(censor)/N

#fit AFT model
datframe_surv = data.frame(covariate1)
attach(datframe_surv)

m.surv = Surv(TTE2,censor,type=""right"")
m.surv.fit = survreg(m.surv~covariate1,dist=""weibull"",scale=1)
sum = summary(m.surv.fit)
print(sum)



###################  plots ########################


#histogram of the errors - gumbel dist
h1 = hist(e, breaks=50, plot=FALSE) 

#histogram of the mean log TTE - gumbel dist
h2 = hist(logTTE, breaks=50, plot=FALSE) 

#histogram of the fixed means
h3 = hist(Xbeta_LTTE, breaks=50, plot=FALSE) 

#histogram of the TTE - weibul dist
h4 = hist(TTE, breaks=50, plot=FALSE) 

#calc the mean of the log TTE given logTTE ~ Gumbel(X_i^t*beta+alpha_gum,beta_gum)
median_logTTE_array = Xbeta_LTTE + alpha_gum - (beta_gum*(log(log(2))))
median_logTTE = median(median_logTTE_array)



#calc the means
ylim_h1 = c(min(h1$density),max(h1$density) )
xlim_h1 = c(mu_e,mu_e )

ylim_h2 = c(min(h3$density),max(h3$density) )
xlim_h2 = c(median_logTTE,median_logTTE )

ylim_h3 = c(min(h3$density),max(h3$density) )
xlim_h3 = c(mean(Xbeta_LTTE),mean(Xbeta_LTTE) )


ylim_h4 = c(min(h4$density),max(h4$density) )
xlim_h4 = c(median_TTE,median_TTE )


#dev.off()
par(mfrow=c(2,2))

plot(h1$mids,h1$density,col='red',main=""errors - gumbel dist"",xlab=""errors (log time)"")
lines(xlim_h1,ylim_h1)

plot(h3$mids,h3$density,col='red',main=""mean log TTE (X*beta) - fixed"",xlab=""mean log TTE (log time)"")
lines(xlim_h3,ylim_h3)

plot(h2$mids,h2$density,col='red',main=""log TTE - gumbel dist"",xlab=""log TTE (log time)"")
lines(xlim_h2,ylim_h2)


plot(h4$mids,h4$density,col='red',main=""TTE - Weibull dist"",xlab=""TTE (time)"")
lines(xlim_h4,ylim_h4)
</code></pre>
"
"0.0449466574975495","0.0433963036602746","136138","<p>I need some help getting pointed in the right direction for creating a regression model in R with data that looks like this.</p>

<p>This is my first foray into this. So using Excel's trend line equation as my reference, I was able to create a logarithmic trend line for another set of data which matched between the two applications. </p>

<p>However, with this specific example, I'm not sure how to formulate the model or even if I should be using non-linear vs linear regression with transformation. Below is an example of the data in the plot.</p>

<pre><code>x = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)
y = c(0.008,0.004,0.0025,0.0024,0.0023,0.0022,0.0021,0.002,0.0018,0.0005,0.012,0.006,
     0.00375,0.0036,0.00345,0.0033,0.00315,0.003,0.0027,0.00075)
z = c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)

df = data.frame(x, y, z)

plot(df$y ~ df$x, type=""p"", pch=20, col=df$z)
</code></pre>

<p><img src=""http://i.stack.imgur.com/467Co.png"" alt=""Sample Data""></p>
"
"0.201007563051842","0.194074169913194","138691","<p>I have been unsuccessfully trying to model a relationship between two measured variables described by two different power functions, on either side of a threshold. My question is how to best estimate this relationship with a model. The aims of this model is to find the threshold $x_0$ and interpolate as exactly as possible the values of $y$ close to $x_0$. Estimating the relationship further from $x_0$ is less important. Based on theory I derived the following function: \begin{equation} f(x) = \begin{cases}
    a(x_0-x)^b+c;&amp;  x\leq x_0\\
    \frac{c-y_0}{d(x-x_0)+1}+y_0;              &amp; x &gt; x_0.
\end{cases} \end{equation} where $a&gt;0, b,c&gt;0,d&gt;0, \text{and } x_0&gt;0$ are unknown parameters. This function behaves like a power function below $x_0$, and decreases roughly as $1/x$ to the asymptotic value of $y_0$ in the region above $x_0$. The function looks roughly like this:</p>

<p><img src=""http://i.stack.imgur.com/jUXoP.png"" alt=""enter image description here""></p>

<p>So far I have tried using non-linear least squares to estimate this model, but I am getting the ""singular gradient matrix at initial parameter estimates"" error. My data and code in <strong>R</strong> is as follows:</p>

<pre><code>x &lt;- c( 0.33, 0.35, 0.39, 0.44, 0.48, 0.53, 0.57, 0.63, 0.74, 0.99, 1.12, 1.23, 1.37)
y &lt;- c(72354.00, 23578.20, 1863.40, 743.80, 113.00, 9.80, 7.38, 5.30, 5.22, 5.03, 4.74, 4.53, 4.32)
</code></pre>

<p>and the code for the model I tried fitting is:</p>

<pre><code>starting.values &lt;- c(a = 8, b = 17, c = 8, d = 1,y_0 = 3, x_0 = .55)

model2 &lt;- nls(y~ifelse(px &lt; x_0,a*(x_0-px)^b+c,(c-y_0)/(d*(px-x_0)+1)+y_0),data = data.frame(x,y), 
              start = starting.values)
</code></pre>

<p>I have been trying a variety of likely starting parameters without success, is the problem too few data points, or is the model impossible to evaluate this way?</p>

<p>I have been successful, however, in modeling the relationship with a much simpler function: \begin{equation} g(x) = \begin{cases}
    ax^b;&amp;  x\leq x_0\\
    dx^c;              &amp; x &gt; x_0.
\end{cases} \end{equation}</p>

<p>where $d = ax_0^{b-c}$ to ensure continuity at the threshold. I did this by first fitting the data after applying the log-log transform, and than using the resulting values as inputs to the final model evaluated with nls. The code is as follows:</p>

<pre><code># transforming the data
x.log &lt;- log(x)
y.log &lt;- log(y)

# fitting a broken regression line to the log-log data
starting.values.log &lt;- c(a = -5, b = 10, c = -.1, x_0 = .55)
model1 &lt;- nls(y.log ~ifelse(x.log &lt; log(x_0),b*x.log + a, c*x.log+b+log(x_0)*(a-c)),
              data = data.frame(x.log,y.log), 
              start = starting.values.log)
</code></pre>

<p>below is the plot of the resulting model on the log-log plot:</p>

<p><img src=""http://i.stack.imgur.com/zDRvT.png"" alt=""The model vs. the measurements on a log-log scale""></p>

<p>and now I use the obtained parameters to fit the function:</p>

<pre><code># fitting the actual model using the parameters found previously
starting.values &lt;- c(a = exp(-8.6238),b = -17.7984, c = -.4418, x_0 = 0.555)
model2 &lt;- nls(y~ifelse(x &lt; x_0,a*x^b,a*x_0^((b-c))*x^c),data = data.frame(x,y), 
              start = starting.values)
</code></pre>

<p>The parameters are:</p>

<pre><code>Parameters:
      Estimate Std. Error t value Pr(&gt;|t|)    
a    3.653e-05  1.130e-05   3.233   0.0103 *  
b   -1.931e+01  2.804e-01 -68.857 1.45e-13 ***
c   -4.816e-01  8.832e+01  -0.005   0.9958    
x_0  5.341e-01  1.335e+00   0.400   0.6984    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 388.5 on 9 degrees of freedom

Number of iterations to convergence: 18 
Achieved convergence tolerance: 3.328e-07
</code></pre>

<p>The main problems with this method is that a) I am unable to determine the exponent of the power law when $x \rightarrow x_0$ from below and b) it does not allow for an asymptote other than 0 in $x \rightarrow \infty$. The first problem is much more important to me.</p>

<p>My question is how to best model the relationship between x and y, bering in mind that the theory supports the first function? Is the problem with trying to evaluate the first function too few data points or is it impossible to evaluate and I should try to use the second method instead? If so, than is there any way to obtain the exponent of the power law describing y when $x \rightarrow x_0$ from below?</p>
"
"0.150867301828271","0.168073161363204","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.179786629990198","0.173585214641098","142317","<p>I want to do <strong>multivariate</strong> (with more than 1 response variables) <strong>multiple</strong> (with more than 1 predictor variables) <strong>nonlinear regression</strong> in <strong>R</strong>.</p>

<p>The data I am concerned with are 3D-coordinates, thus they interact with each other, i.e. the x,y,z-coordinates are not independent. So I cannot just call the <em>nls</em> separately for each response variable (which I tried at first). </p>

<p>A subset of the data-frame with 3D-coordinates where x,y,z are the predictive variables and a,b,c the response variables:</p>

<pre><code>              x           y         z           a            b         c
1  -2.26470e-03 -0.05081670 0.0811701 -0.00671079 -0.045721600 0.0705679
2  -9.13106e-05 -0.00670734 0.0724838 -0.00676299 -0.001638430 0.0588486
3   3.81399e-04  0.03556000 0.0782059 -0.00783726  0.038503800 0.0641364
4   1.42293e-03  0.06133920 0.0708688 -0.00820760  0.062697100 0.0572740
5  -5.06043e-02  0.04759040 0.0418189 -0.05949350  0.040427800 0.0266159
6   5.92963e-02  0.04183450 0.0431029  0.05124780  0.038396500 0.0327903
7  -4.44213e-02 -0.00909717 0.0459059 -0.05021130 -0.005634520 0.0329833
8  -3.75400e-02 -0.00625770 0.0567296 -0.04255200 -0.000666089 0.0436465
9  -2.37768e-02 -0.00707318 0.0581552 -0.03048950 -0.001260670 0.0457355
10 -1.56645e-02 -0.01326670 0.0540247 -0.02101350 -0.009021990 0.0413755
</code></pre>

<p><strong>My question:</strong> Is it possible to call the <em>nls</em> function with more than 1 (in my case 3) response variables? In other words is it possible to substitute <em>y</em> in <code>nls(y ~ f(x,y,z, parameters), data)</code> with something like <em>c(a,b,c)</em> or <em>cbind(a,b,c)</em>, such that <code>nls(cbind(a,b,c) ~ f(x,y,z, parameters), data)</code> ?</p>

<p>In the post <a href=""http://stackoverflow.com/questions/12161659/how-to-write-r-formula-for-multivariate-response"">How to write R formula for multivariate response?</a> it is shown that one can combine several response variables with <em>cbind</em> in the case of linear modeling with the <em>lm</em> function.
This doesn't seem to work for nonlinear modeling with <em>nls</em> .., because the <em>nls</em> call in the code sample at the bottom of my question throws the following error:</p>

<p><code>Error in parse(text = x) : &lt;text&gt;:2:0: unexpected end of input
1: ~ 
   ^</code></p>

<p>which I could not find a solution for online concerning my case of a multivariate regression..</p>

<hr>

<p>My web-searches to my main question only gave me results concerning <em>multivariate <strong>linear</strong> regression</em>, which for example included <a href=""http://stats.stackexchange.com/questions/11127/multivariate-multiple-regression-in-r/11132#11132"">solutions with the manova function</a>..</p>

<p>Therefore, <strong>my question asked in a more general way:</strong> How do you in general solve such a non-linear multivariate multiple regression problem in R which takes into account interactions/dependencies between variables?</p>

<p>Here is my code where </p>

<ul>
<li>function <em>f</em> computes the rotations of coordinates about three axes
in the order x-axis, y-axis, and then z-axis (unfortunately I cannot
include the pic of the equation I wrote in LaTeX here since I haven't
got 10 reputation points yet);</li>
<li><em>rot_data_all</em> is structured as the data-subset above, just with more rows;</li>
<li>alpha1, alpha2 and so on are the parameters which nonlinear
regression should approximate:</li>
</ul>

<p>The code:</p>

<pre><code>f &lt;-function(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s) { 
      a &lt;- alpha1 + s*(cos(theta)*cos(phi)*x - cos(theta)*sin(phi)*y + sin(theta)*z)
      b &lt;- alpha2 + s*((sin(gamma)*sin(theta)*cos(phi) + cos(gamma)*sin(phi))*x 
                          + (-sin(gamma)*sin(theta)*sin(phi) + cos(gamma)*cos(phi))*y
                          - sin(gamma)*cos(phi)*z)
      c &lt;- alpha3 + s*((cos(gamma)*sin(theta)*cos(phi) + sin(gamma)*sin(phi))*x 
                          + (cos(gamma)*sin(theta)*sin(phi) + sin(gamma)*cos(phi))*y
                          + cos(gamma)*cos(phi)*z)
      return(c(a,b,c))
    }

    rot.nls &lt;- nls(cbind(a, b, c) ~ f(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s), 
                   data = rot_data_all, 
                   start = c(alpha1 = 0, alpha2 = 0, alpha3 = 0, gamma = 0.1, theta = 0.1, phi = 0.1, s = 0.1), trace = TRUE)
</code></pre>

<hr>

<p>I hope to find a solution which is general enough to also solve other transformations which cannot be easily linearized like the set of equations for <strong>projective transformation</strong>, i.e. something like the following function:</p>

<pre><code>f.proj &lt;-function(x, y, z, betas) {
  a &lt;- (betas[1,1]*x + betas[1,2]*y + betas[1,3]*z + betas[1,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  b &lt;- (betas[2,1]*x + betas[2,2]*y + betas[2,3]*z + betas[2,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  c &lt;- (betas[3,1]*x + betas[3,2]*y + betas[3,3]*z + betas[3,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  return(c(a,b,c))
}
</code></pre>

<hr>

<p>I am happy to provide more information if needed! Thank you so much!</p>
"
"0.0635641726163728","0.0613716411932216","143110","<p>I saw this sentence:</p>

<p>""I use log(income) partly because of skewness in this variable but also because income is better considered on a multiplicative rather than additive scale. </p>

<p>In other words, \$1,000 is worth a lot more to a poor person than a millionaire because \$1,000 is a much greater fraction of the poor personâ€™s wealth""</p>

<p>on page 143 on this link <a href=""http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch12.pdf"" rel=""nofollow"">http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch12.pdf</a>.</p>

<p>But when I check their skewness using <code>library(e1071)</code> in R (as seen below), I found out that the skewness of income is not that high or low. My question is how do I determine if I need to used log transformation in a regression model?</p>

<p>PS the chicago data is in <code>library(faraway)</code></p>

<pre><code>&gt; skewness(chicago$race)
  [1] 0.5570103
&gt; skewness(chicago$race)
  [1] 0.5570103
&gt; skewness(chicago$fire)
  [1] 1.271188
&gt; skewness(chicago$theft)
  [1] 2.955751
&gt; skewness(chicago$age)
  [1] -0.9210877
&gt; skewness(chicago$income)
  [1] 1.155
&gt; skewness(chicago$involact)
  [1] 0.8079598
</code></pre>
"
"0.144149994031289","0.162374100149152","143399","<p>I am trying to do some survival analysis in R and as a starting point, I want to make sure I can replicate a previous analysis. I notice differences and I will demonstrate them here. I feel like there is a daft explanation the user community can provide.</p>

<p>Let's start by using the ovarian dataset in R. We will fit a weibull distribution with residual disease and ECOG performance status as covariates. Then we will print the output using proportional hazards specification to match Stata's HR output.</p>

<pre><code>require(survival)
require(flexsurvreg)
require(dplyr)
attach(ovarian)
ovarian &lt;- ovarian %&gt;% mutate(resid.ds=resid.ds-1, ecog.ps=ecog.ps-1, futime=futime/365.25) # Make it 0,1
write.dta(ovarian %&gt;% mutate(resid.ds=resid.ds+1, ecog.ps=ecog.ps+1), ""data/ovarian.dta"") # Write dta
s.weib &lt;- flexsurvreg(Surv(futime, fustat) ~ age + resid.ds + ecog.ps, data=ovarian, dist=""weibull"") # Fit weibull

# Function to convert AFT to PH
flexsurvPHcoef &lt;- function(x) return(c(exp(x$coef[-(1:2)]*(-1)*exp(x$coef[""shape""])), exp(x$coef[""shape""]), exp(-x$coef[""scale""])))
flexsurvPHcoef(s.weib)
     age     resid.ds      ecog.ps        shape        scale 
1.150309872 2.702038142 1.060599568 1.752446996 0.002799146 
</code></pre>

<hr>

<p>Now let's compare to Stata.</p>

<pre><code>quietly stset futime, f(fustat)
streg age i.resid_ds i.ecog_ps, d(weib)
Weibull regression -- log relative-hazard form 

No. of subjects =           26                     Number of obs   =        26
No. of failures =           12
Time at risk    =  42.67761807
                                                   LR chi2(3)      =     17.88
Log likelihood  =   -20.828884                     Prob &gt; chi2     =    0.0005

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         age |    1.15031   .0515502     3.12   0.002     1.053583    1.255916
  2.resid_ds |   2.702038    2.00098     1.34   0.180     .6329054    11.53571
   2.ecog_ps |     1.0606   .6620784     0.09   0.925     .3120252    3.605066
       _cons |   .0000336   .0000906    -3.82   0.000     1.69e-07    .0066602
-------------+----------------------------------------------------------------
       /ln_p |   .5610131    .238929     2.35   0.019     .0927209    1.029305
-------------+----------------------------------------------------------------
           p |   1.752447   .4187103                      1.097156     2.79912
         1/p |   .5706307   .1363402                      .3572551    .9114478
------------------------------------------------------------------------------
</code></pre>

<p>We can see the resid.ds and ecog.ps are the same. As well, the shape. But the scale is off. </p>

<p><strong>So my question is, any thoughts on why only the scale parameter is different?</strong></p>

<hr>

<p>Let's move on to estimation. flexsurvreg has an interesting ability to predict at multiple time points. Let's assume a woman is 45, has residual disease and ECOG is 0.</p>

<pre><code>summary(s.weib, newdata=data.frame(age=45, resid.ds=1, ecog.ps=0), t=c(1:5))
age=45, resid.ds=1, ecog.ps=0 
  time       est         lcl       ucl
1    1 0.9517266 0.807744496 0.9930684
2    2 0.8464501 0.488174229 0.9681961
3    3 0.7122949 0.193976895 0.9285058
4    4 0.5702529 0.038982054 0.8762493
5    5 0.4358519 0.002925942 0.8097636
</code></pre>

<p>Not the most precise. Anyways, how does this compare to how weibull is parameterized (PH). From Stata's manual:
$$
S = exp(-exp(x{B}){t}^p)
$$</p>

<p>No we have to use the log scale coefficients. But let's go ahead and try and predict this manually.</p>

<pre><code>p.weib &lt;- function(cons, age, resid, t, p) return(exp(-exp(cons+age*45+resid)*t^p))
coef &lt;- flexsurvPHcoef(s.weib)
data.frame(time=1:5) %&gt;% mutate(S=p.weib(log(coef[""scale""]), log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.01617
2    2 0.00000
3    3 0.00000
4    4 0.00000
5    5 0.00000
</code></pre>

<p>That obviously didn't really work out. What happens if we substitute the scale from Stata (-10.30166)?</p>

<pre><code>data.frame(time=1:5) %&gt;% mutate(S=p.weib(-10.30166, log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.95173
2    2 0.84645
3    3 0.71230
4    4 0.57025
5    5 0.43585
</code></pre>

<p>It's just as flexsurvreg predicted. So now that I write this, maybe I've transformed the AFT scale incorrectly. Back to my original question, why are the scales different?</p>

<p>Finally, some aside questions. I couldn't reproduce my actual data problem. I can't really put up that much data here. Again, <code>summary.flexsurvreg</code> gives me predicted estimates that are not the same as the <code>p.weib</code>. But, when I substitute the scale from Stata for p.weib, I get estimates different to the original summary.flexsurvreg, albeit much closer than with the scale from my log(coef) of PH. Any thoughts?</p>
"
"0.156162493533897","0.162374100149152","144247","<p>This is my first time attempting to build a linear regression model and I am not sure what to do next given the results I have.</p>

<p>I have a data set with 24 predictors and 1 response and there are 999 rows in the data set.  The data can be found <a href=""http://pastebin.com/Whv6tgiv"" rel=""nofollow"">here</a>.  I am trying to build a linear regression model with the end goal of being able to predict the response variable.</p>

<p>I decided to log transform the response variable as this resulted in the histogram looking more Normally distributed:
<img src=""http://i.imgur.com/vK1hF7Q.png"" alt=""Untransformed response"">
<img src=""http://i.imgur.com/eVERAw8.png"" alt=""Log transformed response""></p>

<p>My first linear regression model was the following:</p>

<pre><code>Call:
lm(formula = log(y) ~ ., data = the_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.70154 -0.13329  0.01642  0.14626  1.10267 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.112026   0.061692  82.863  &lt; 2e-16 ***
v1           0.136381   0.024563   5.552 3.64e-08 ***
v2           0.069991   0.024519   2.855  0.00440 ** 
v3           0.034584   0.024504   1.411  0.15845    
v4           0.031069   0.024562   1.265  0.20620    
v5          -0.078188   0.024556  -3.184  0.00150 ** 
v6          -0.007898   0.024579  -0.321  0.74803    
v7          -0.062695   0.024613  -2.547  0.01101 *  
v8          -0.007664   0.024553  -0.312  0.75501    
v9          -0.019956   0.024558  -0.813  0.41664    
v10          0.025143   0.024561   1.024  0.30623    
v11          0.003946   0.024583   0.161  0.87250    
v12          0.024605   0.024551   1.002  0.31650    
v13         -0.005139   0.024558  -0.209  0.83429    
v14         -0.071931   0.024534  -2.932  0.00345 ** 
v15          0.002112   0.024552   0.086  0.93145    
v16         -0.050584   0.024510  -2.064  0.03930 *  
v17          0.012561   0.024491   0.513  0.60815    
v18         -0.029778   0.024515  -1.215  0.22478    
v19          0.030362   0.024559   1.236  0.21666    
v20         -0.022925   0.024519  -0.935  0.35002    
v21         -0.003210   0.024532  -0.131  0.89591    
v22          0.027668   0.024617   1.124  0.26132    
v23         -0.009467   0.024557  -0.386  0.69993    
v24          0.003332   0.024567   0.136  0.89214    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.223 on 974 degrees of freedom
Multiple R-squared:  0.07552,   Adjusted R-squared:  0.05274 
F-statistic: 3.315 on 24 and 974 DF,  p-value: 1.603e-07
</code></pre>

<p>The R-squared value (0.076) tells me that this model is not very good, correct?</p>

<p>Looking at the Residuals vs Fitted and Scale-Location plots I think that there is some pattern (upside down parabola).<br>
<img src=""http://i.imgur.com/C7i2kgA.png"" alt=""First model residuals""></p>

<p>So I decided to fit the following model which has first order interactions</p>

<pre><code>Call:
lm(formula = log(y) ~ .^2, data = the_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.60878 -0.11444  0.00161  0.11522  0.71236 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.510425   0.372884  12.096  &lt; 2e-16 ***
v2           0.417113   0.211401   1.973 0.048879 *  
v3           0.554384   0.218996   2.531 0.011577 *  
...   
v12          0.644474   0.219123   2.941 0.003378 ** 
...
v1:v2       -0.501372   0.088464  -5.668 2.12e-08 ***
... 
v1:v5        0.226257   0.091674   2.468 0.013823 *  
...
v1:v12      -0.265495   0.089155  -2.978 0.003003 ** 
...
v1:v16      -0.226053   0.090448  -2.499 0.012674 *  
...
v2:v9        0.175206   0.088957   1.970 0.049285 *  
...
v2:v12      -0.558579   0.087183  -6.407 2.73e-10 ***
...
v2:v16       0.230151   0.088761   2.593 0.009716 ** 
v2:v17      -0.235761   0.087570  -2.692 0.007267 ** 
...
v2:v22       0.219280   0.085991   2.550 0.010984 *  
...
v3:v5       -0.305964   0.087337  -3.503 0.000489 ***
...
v3:v9       -0.303287   0.086619  -3.501 0.000492 ***
...
v4:v5       -0.382494   0.091162  -4.196 3.07e-05 ***
...
v5:v13       0.222259   0.089072   2.495 0.012816 *  
...
v6:v9        0.185673   0.090535   2.051 0.040656 *  
v6:v10       0.216236   0.090258   2.396 0.016849 *  
...
v6:v17       0.255650   0.087794   2.912 0.003707 ** 
...
v6:v22      -0.198706   0.089566  -2.219 0.026838 *  
...
v7:v15      -0.192992   0.089102  -2.166 0.030652 *  
...
v8:v16       0.230660   0.090440   2.550 0.010971 *  
...
v9:v13      -0.190571   0.087136  -2.187 0.029070 *  
...
v9:v23       0.322755   0.088111   3.663 0.000268 ***
...
v12:v17      0.182697   0.090321   2.023 0.043480 *  
...
v12:v22     -0.229433   0.089649  -2.559 0.010700 *  
...
v13:v14      0.232871   0.086159   2.703 0.007043 ** 
...
v14:v15      0.206125   0.089665   2.299 0.021810 *  
...
v15:v16     -0.203924   0.091111  -2.238 0.025523 *  
...   
v22:v23     -0.184572   0.090622  -2.037 0.042055 *  
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2055 on 698 degrees of freedom
Multiple R-squared:  0.4369,    Adjusted R-squared:  0.1949 
F-statistic: 1.805 on 300 and 698 DF,  p-value: 1.863e-10
</code></pre>

<p>I have omitted multiple lines that are not significant at the 0.05 level or below in the output (denoted by ...) for brevity.</p>

<p>The residual plots for this model are shown below:
<img src=""http://i.imgur.com/VFk6ClY.png"" alt=""Second interaction model residuals""></p>

<p>The R-squared value is greatly improved (0.437 versus 0.076) but still nowhere close to what I think I need for a good model (R-squared > 0.8).</p>

<p>I am unsure of how to proceed - what to try next in order to get a better model.</p>
"
"0.080403025220737","0.097037084956597","145455","<p>So I want to compute a regression using R. The problem is, that I want to compute the regression with log transformed variables. Here is what I am trying to do:</p>

<pre><code>reg1 &lt;- lm(y~x+z+u+log(p))
</code></pre>

<p>Now since you cant take the log of 0, the following message pops up: </p>

<pre><code>Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  NA/NaN/Inf in 'x' 
</code></pre>

<p>How can I compute a regression even though there are NaNs (in my case they are produced only for the first few observations of the variable p)? Many thanks</p>

<pre><code>Date        y        x      z       u      p
25.06.2009  0.582   1.145   0.603   26.36   0
26.06.2009  0.604   1.12    0.61    25.93   0
29.06.2009  0.647   1.108   0.647   25.35   0
30.06.2009  0.597   1.099   0.669   26.35   0
01.07.2009  0.604   1.085   0.633   26.22   0
02.07.2009  0.54    1.072   0.63    27.95   0
06.07.2009  0.543   1.048   0.57    29      0
07.07.2009  0.512   1.044   0.567   30.85   0
08.07.2009  0.496   1.029   0.533   31.3    0
09.07.2009  0.487   1.018   0.515   29.78   23
10.07.2009  0.482   1.007   0.504   29.02   66
13.07.2009  0.473   0.996   0.503   26.31   162
14.07.2009  0.471   0.985   0.503   25.02   235
15.07.2009  0.472   0.979   0.492   25.89   585
16.07.2009  0.441   0.969   0.486   25.42   668
17.07.2009  0.431   0.954   0.461   24.34   1080
20.07.2009  0.438   0.944   0.451   24.4    1883
21.07.2009  0.435   0.937   0.451   23.87   2398
</code></pre>

<p><strong>EDIT</strong>:I think I formulated my question unclear. So if the vector p has a zero, it means that on this date nothing happend. So I think it is no problem to exclude the obeservations from p which contain a zero from the regression. But still, how can I tell R it should not include those observations? (I think I could also replace the 0 with NA since nothing happend in the variable p till 09.07.2009) I tried <code>nan.action=nan.exclude</code> but this doesnt work...</p>
"
"0.119857753326799","0.130188910980824","146853","<p><strong>What I am doing so far:</strong></p>

<p>I am doing a constraint linear regression with R's <code>quadprog</code> package, function <code>solve.QP()</code>. The regression does not have an intercept $\alpha$, therefore the objective function can be stated by $$min_{b} (Y-Xb)^\top(Y-Xb)$$ which is the squared residuals. </p>

<p>Quadprog optimizes the function $$min_{b}\Big(\frac{1}{2}b^\top Db-d^\top b\Big)$$ Therefore I have to transform the first function into the second one. The end result is $$\frac{1}{2}b^\top X^\top Xb-Y^\top Xb$$ where $X^\top X =:D$ and $Y^\top X=:d$.</p>

<p>There are two risk factors in this example (hence Y is a vector of the dependent variable and X is a 2-dim matrix of the independent variables), whereas the first one is restricted to be greater or equal to -10 and the second one greater or equal to zero. The code for this is the following:</p>

<pre><code>require(""quadprog"")

Dmat = t(X) %*% X
Amat = t(diag(2))
bvec = c(-10,0)
dvec = t(Y) %*% X

solve.QP(Dmat = Dmat, dvec = dvec, Amat = Amat, bvec = bvec, meq = 0, factorized = F)
</code></pre>

<hr>

<p><strong>What I want to add:</strong></p>

<p>I want to add penalties to the regression in order to replicate a Lasso regression. Therefore, the initial objective function has to be expanded by the penalty term $\lambda |b|$ $$min_{b} (Y-Xb)^\top(Y-Xb)+\lambda |b|$$ 
In order to bring it into the form usable by the algorithm, I form it into $$\frac{1}{2}b^\top X^\top Xb-(Y^\top X-\frac{1}{2}\lambda)|b|$$
The penalty term $\lambda$ is a vector with two entries $\lambda=(80.56,5.65)$. However, when I now run the algorithm, the objective function gets negative and $b_1$ will be $b_1 = -10$, which is the most negative piossible value allowed by the constraints. $b_2$ will be $b_2=0$. </p>

<p>These results are not equal to results I get with the <code>glmnet</code> package which allows me to perform a Lasso regression with the same penalties. Those results have been checked and are correct. Hence, I do not know why the quadprog algorithm delivers different results. Any hints? Is the objective function wrong? Did I specify any input parameter for <code>quadprog</code> incorrectly? </p>
"
"0.101929438287525","0.114815827304529","147923","<p>I have a data set with continuous variable and a binary target variable (0 and 1). </p>

<p>I need to discretize the continuous variables (for logistic regression) with respect to the target variable and with the constrained that the frequency of observation in each interval should be balanced. I tried machine learning algorithms like Chi Merge, decision trees. Chi merge gave me intervals with very unbalanced numbers in each interval (an interval with 3 observations and another one with 1000). The decision trees were hard to interpret.</p>

<p>I came to the conclusion that an optimal discretization should maximise the $\chi^2$ statistic between the discretized variable and the target variable and should have intervals containing roughly the same amount of observations. </p>

<p>Is there an algorithm for solving this?</p>

<p>This how it could look like in R (def is the target variable and x the variable to be discretized). I calculated Tschuprow's $T$ to evaluate the ""correlation"" between the transformed and the target variable because $\chi^2$ statistics tends to increase with the number of intervals. I'm not certain if this is the right way.</p>

<p>Is there another way of evaluating if my discretization is optimal other than Tschuprow's $T$ (increases when number of classes decreases)? </p>

<pre><code>chitest &lt;- function(x){
  interv &lt;- cut(x, c(0, 1.6,1.9, 2.3, 2.9, max(x)), include.lowest = TRUE)
  X2 &lt;- chisq.test(df.train$def,as.numeric(interv))$statistic
  #Tschuprow
  Tschup &lt;- sqrt((X2)/(nrow(df.train)*sqrt((6-1)*(2-1))))
  print(list(Chi2=X2,freq=table(interv),def=sum.def,Tschuprow=Tschup))
}
</code></pre>
"
"0.162472478891985","0.168073161363204","148913","<p>I am new in R and itâ€™s my first time using it so Iâ€™ll appreciate the help. I am estimating income elasticity for electricity consumption using budget shares. I have data for 8 regions categorized into 5 classes depending on the household size (from A to E). I am regressing budget shares (wi) on log of income(lxi), classes dummy of household size (D) and the interaction between log of income and household class size (lxi*D):</p>

<blockquote>
  <p>Wi = C + lxi + D + lxi*D + u</p>
</blockquote>

<p>I have an unbalanced panel data for 2067 observations saved in .csv format. I attached my data, transformed the Date from factor, made a new data frame including the new date, and finally set the data as a panel data as the code below:</p>

<blockquote>
  <p>mydata&lt;-read.csv(""C:/Users/Fadhila/Desktop/Remeasuring 2015/DataClass-unbalanced.csv"", header=T)</p>
  
  <p>attach(mydata) </p>
  
  <p>date&lt;- as.Date(factor(Date),format= ""%m/%d/%Y"")</p>
  
  <p>ndata=data.frame(Class,date,lxi,wi)</p>
  
  <p>ndata&lt;-plm.data(ndata, index=c(""Class"", ""date""))</p>
</blockquote>

<p>I have regressed my model before using Pooled OLS, Fixed (twoways), and Random and both Hausman test and F-test suggested using Random which I doubt so I thought to test for heteroskedasticity and outliers. So I plotted the below model , plotted the leverage versus the residual and compared them with cooks distance:</p>

<blockquote>
  <p>r &lt;- lm (wi~lxi + Class:lxi, data=ndata)</p>
  
  <p>summary(r)</p>
  
  <p>par(mfrow=c(2,2))</p>
  
  <p>plot (r)</p>
  
  <p>windows()</p>
  
  <p>with(ndata, plot(lxi, cooks.distance(r)))</p>
  
  <p>identify(ndata$lxi, cooks.distance(r)) </p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/1imMP.jpg"" alt=""residuals plot ""></p>

<p><img src=""http://i.stack.imgur.com/PdTBk.jpg"" alt=""residual versus cooks distance""></p>

<p>Than to estimate how many points are far from the leverage points, but it seems that am doing something wrong as I got all the points to be twice greater than the leverage.</p>

<blockquote>
  <p>lev = hatvalues(r)</p>
  
  <p>lev</p>
  
  <p>4/2067</p>
  
  <p>lev[lev>2*4/2067]</p>
</blockquote>

<p>However, I plotted the below to see </p>

<blockquote>
  <p>plot(ndata$lxi, rstandard(r))</p>
  
  <p>identify(ndata$lxi,rstandard(r))</p>
  
  <p>plot(ndata$lxi, lev)</p>
  
  <p>identify(ndata$lxi, lev)</p>
  
  <p>windows()</p>
  
  <p>plot(ndata$lxi, ndata$Class)</p>
  
  <p>identify(ndata$lxi, ndata$Class)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/gdERl.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/JPyuU.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/ULWNr.jpg"" alt=""enter image description here""></p>

<p>To double check and see which points influence the results I made the below:</p>

<blockquote>
  <p>outs &lt;- influencePlot(r)</p>
  
  <p>n &lt;-2</p>
  
  <p>Cooksdist &lt;- as.numeric(tail(row.names(outs[order(outs$CookD), ]), n))</p>
  
  <p>Levr &lt;- as.numeric(tail(row.names(outs[order(outs$Hat), ]), n))</p>
  
  <p>StdRes &lt;- as.numeric(tail(row.names(outs[order(outs$StudRes), ]), n))</p>
  
  <p>plot(ndata$lxi, ndata$wi)</p>
  
  <p>abline(r, col = ""blue"")</p>
  
  <p>Warning message:
  In abline(r, col = ""blue"") :
    only using the first two of 41 regression coefficients*</p>
  
  <p>points(ndata$lxi[Cooksdist], ndata$wi[Cooksdist], col = ""red"", pch = 0, lwd = 15)</p>
  
  <p>points(ndata$lxi[Levr], ndata$wi[Levr], col = ""blue"", pch = 25, lwd = 8)</p>
  
  <p>points(ndata$lxi[StdRes], ndata$wi[StdRes], col = ""green"", pch = 20, lwd = 5)</p>
  
  <p>text(ndata$lxi[as.numeric(row.names(outs))], 
         ndata$wi[as.numeric(row.names(outs))], 
       labels = round(ndata$wi[as.numeric(row.names(outs))], 3),
       pos = 1)</p>
  
  <p>identify(ndata$lxi, ndata$wi)</p>
</blockquote>

<p>and got the below, but not sure how I identify the points:
<img src=""http://i.stack.imgur.com/QJmoI.jpg"" alt=""enter image description here""></p>

<p>my questions are:</p>

<ol>
<li><p>How to identify to deal with the outliers! After correcting for the leverage point error (all points far from leverage point)</p></li>
<li><p>Do I need to use â€œplmâ€ or itâ€™s ok since I identified my data as panel?</p></li>
<li><p>How to do two way tests for Random?</p></li>
<li><p>Testing for heteroskedasticity by:</p></li>
</ol>

<blockquote>
  <p>p&lt;-plm(wi~lxi + Class+Class:lxi, data=ndata1)</p>
  
  <p>summary(p)</p>
  
  <p>bptest(p)</p>
</blockquote>

<p>the null was rejected(Null: homoskedastic). Does this solve the heteroskedasticity problem:</p>

<blockquote>
  <p>vcovHC(p, omega = Null, type=""HC4"")</p>
  
  <p>coeftest(p, df=Inf, vcov=vcovHC(p, type=""HC4""))</p>
</blockquote>

<p>or I need to remove the insignificant points!</p>
"
"0.0898933149950989","0.0867926073205492","149322","<p>I am trying to use multiple regression for a time series dataset. I have values corresponding to a variable measured by 24 hrs for 4 months. Since there was a pattern which repeated every 24 hours I used 23 dummy variables for the hourly variations in values.</p>

<p>I used log transformation of the dependent variable before performing multiple regression. The fitted coefficients were highly significant and the R-squared was around 0.99.
However, when I look at the Residuals vs fitted plot, it seems sort of weird. According to the plots <a href=""http://www.r-bloggers.com/model-validation-interpreting-residual-plots/"" rel=""nofollow"">here</a>, my plot is neither biased nor heteroskedastic, but it also doesn't look like random noise. Can someone help me find the issue here? 
<img src=""http://i.stack.imgur.com/sCGPx.png"" alt=""enter image description here"">
Also please find below a plot of the observed and fitted model for first 500 hrs<img src=""http://i.stack.imgur.com/jiTts.png"" alt=""Observed Values VS Time in hours overlaid by fitted model in red""></p>
"
"0.0449466574975495","0.0433963036602746","149413","<p>I created a spreadlevel plot on my simple linear regression model in R. Here is my code,</p>

<pre><code>spreadLevelPlot(ols_reg)
</code></pre>

<p>where <code>ols_reg</code> is my regression model, <code>ols_reg &lt;- lm(y~0+.,dat)</code>. I first encountered spread-level plots from this <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">link</a>, but I don't fully understand how to read the plot and what to do with the transformation this function provides. Suggested power transformation: 0.718 </p>

<p>This is how my plot looks,</p>

<p><img src=""http://i.stack.imgur.com/DEp1O.jpg"" alt=""enter image description here""></p>

<p>How do I read it?</p>

<p>These are the libraries I included in my script.</p>

<pre><code>library(car)
library(MASS)
library(lmtest)
</code></pre>
"
"0.0449466574975495","0.0433963036602746","149908","<p>The function <code>powerTranform</code> from the ""car"" package in R mentions the following code for Box-Cox transformation for multiple regression: </p>

<pre><code>summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))
# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>Is it sensible to apply Box-Cox method to just the dependent variable (and not the whole formula) and proceed with the regression: </p>

<pre><code>library(fifer)
cycles = boxcoxR(cycles)
summary(m1 &lt;- lm(cycles ~ len + amp + load, Wool))
</code></pre>

<p>I suspect this method is not right but I am not sure.</p>
"
"0.0449466574975495","0.0433963036602746","151600","<p>Has anyone written a package for R that can do a logistic regression over categorical variables (like <code>glm</code>) but with the constraint, and I do realize this is weird, that <em>all the residuals must be nonnegative?</em>  (In response space, not link space.  In other words, the predicted probability in each cell must come out less than or equal to the observed probability in that cell.) Alternatively, is there a straightforward way to transform a <code>glm</code> problem so that it will come out with nonnegative residuals?</p>

<p>I know I can probably persuade <code>optim</code> to do what I want but if a shortcut exists that sure would be nice.</p>
"
"0.258198889747161","0.241738458186313","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.110096376512636","0.106298800690547","151915","<p>I've performed a logistic regression with L-BFGS on R and noticed that if I changed the initialization, the model retuned was different.</p>

<p>Here is my dataset (390 obs. of 14 variables, Y is the target variable) :</p>

<pre><code>GEST    DILATE    EFFACE    CONSIS    CONTR    MEMBRAN    AGE    STRAT    GRAVID    PARIT    DIAB    TRANSF    GEMEL    Y
31           3       100         3        1         2     26         3         1        0       2         2       1     1
28           8         0         3        1         2     25         3         1        0       2         1       2     1
31           3       100         3        2         2     28         3         2        0       2         1       1     1
...
</code></pre>

<p>This dataset is found here: <a href=""http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html"" rel=""nofollow"">http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html</a> in ""DonnÃ©es : prematures.xls"". Y is a column I created with the column ""PREMATURE"", Y=IF(PREMATURE=""positif"";1;0)</p>

<p>I've used the optimx package like here <a href=""http://stats.stackexchange.com/questions/17436/logistic-regression-with-lbfgs-solver"">Logistic regression with LBFGS solver</a>, here is the code: </p>

<pre><code>install.packages(""optimx"")
  library(optimx)

vY = as.matrix(premature['PREMATURE'])
# Recoding the response variable
vY = ifelse(vY == ""positif"", 1, 0)

mX = as.matrix(premature[c('GEST', 'DILATE', 'EFFACE', 'CONSIS', 'CONTR', 
                           'MEMBRAN', 'AGE', 'STRAT', 'GRAVID', 'PARIT', 
                           'DIAB', 'TRANSF', 'GEMEL')])

#add an intercept to the predictor variables
mX = cbind(rep(1, nrow(mX)), mX)

#the number of variables and observations
iK = ncol(mX)
iN = nrow(mX)

#define the logistic transformation
logit = function(mX, vBeta) {
  return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )
}

# stable parametrisation of the log-likelihood function
logLikelihoodLogitStable = function(vBeta, mX, vY) {
  return(-sum(
    vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))
    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))
  )  # sum
  )  # return 
}

# score function
likelihoodScore = function(vBeta, mX, vY) {
  return(t(mX) %*% (logit(mX, vBeta) - vY) )
}

# initial set of parameters (arbitrary starting parameters)
vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)

optimLogitLBFGS = optimx(vBeta0, logLikelihoodLogitStable,
                         method = 'L-BFGS-B', gr = likelihoodScore, 
                         mX = mX, vY = vY, hessian=TRUE)
</code></pre>

<p>I get this :</p>

<pre><code> optimLogitLBFGS
                p1         p2       p3         p4         p5         p6
L-BFGS-B 9.720242 -0.1652943 0.525449 0.01681583 0.02781123 -0.3921004
                 p7          p8         p9       p10        p11        p12
L-BFGS-B -1.694412 -0.03461208 0.02759248 0.1993573 -0.6718275 0.02537887
                 p13      p14   value fevals gevals niter convcode  kkt1  kkt2
L-BFGS-B -0.8374338 0.625044 187.581    121    121    NA        1 FALSE FALSE
          xtimes
L-BFGS-B  0.044
</code></pre>

<p>But if I change </p>

<pre><code>vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)
</code></pre>

<p>in</p>

<pre><code>vBeta0 = rep(0.1, iK)
</code></pre>

<p>I get a different result :</p>

<pre><code>optimLogitLBFGS
                 p1             p2             p3              p4               p5
L-BFGS-B 0.372672689046 0.206785276091 0.398104550108 0.0175008380158 -0.0460042719084
                 p6             p7               p8            p9            p10
L-BFGS-B 0.139760396213 -1.43192069477 -0.0207666651106 -1.1396642657 0.212186387416
                 p11             p12             p13            p14         value
L-BFGS-B -0.583698421298 0.0576485672766 -0.802789658686 0.993103617257 185.472518798
         fevals gevals niter convcode  kkt1  kkt2 xtimes
L-BFGS-B    121    121    NA        1 FALSE FALSE   0.05
</code></pre>

<p>How can I choose the initial parameters to get the best model?</p>
"
"0.134839972492648","0.115723476427399","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.168174993036504","0.162374100149152","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.0898933149950989","0.0650944554904119","154043","<p>In polynomial regression, it is recommended to center predictor input variables to break multi colinear relationships of x to x^2.</p>

<p>From Wikipedia: The underlying monomials can be highly correlated ""For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). ""</p>

<p>When a variable x is between -1 and 1, x^2 makes the magnitude smaller while when x is outside of that range, x^2 makes x's magnitude larger.</p>

<p>Making the variable into an integer variable could change the behavior.</p>

<p>E.g.</p>

<pre><code>df$x=round((df$x - mean(df$))*100)
</code></pre>

<p>Any opinions on the scale especially in regards to interval [-1,1] vs [-100,100]</p>

<p>It is common to normalize predictors subtracting the mean and dividing by the standard deviation when doing inference analysis but this question pertains to regression prediction.</p>

<p>Asking a similar question in regards to natural log, a variable that has a range (0,1] has a dramatically different transformed value than [1,100].</p>

<pre><code>log(seq(0.1,1,.1)) #mostly negative
log(seq(0.1,1,.1)*100) #rather positive
</code></pre>

<p>If the predictor variable in the case of log happened to be sometimes less than 1 and others greater than 1, that could make the transformation act a little ""wild"". Would it be best to transform the variable to be within (0,1] or [1,] but not both?</p>
"
"0.111237302078652","0.122743282386443","154621","<p>I using the regression method called <code>MARS</code>, in <code>R</code> is it called <code>earth</code> and is located in the package <code>earth</code>, in order to find the best regression model for my datat.</p>

<p>I know that this method is suitable for large data-sets, can handle <code>NA</code> and also decides which variables will be used and which not into the regression.</p>

<p><strong>What I'm doing</strong></p>

<p>After the regression is estimated, I detect the <code>outliers</code> using <code>boxplot</code>  and then I eliminate from the data the observations which are <code>extreme values</code> and compute the model again.</p>

<p>I do this until maximum of <code>grsq</code> and <code>rsq</code> are found.</p>

<p><strong>CODE</strong></p>

<pre><code>model &lt;- earth(log(price) ~ ., data = data, weights = weights)
max_grsq &lt;- round(model$grsq, digits = 4)
    max_rsq &lt;- round(model$rsq, digits = 4)
min_diff &lt;- abs(max_grsq - max_rsq)

while(!done) {
  residuals_abs &lt;- abs(model$residuals)
      boxplot &lt;- boxplot(residuals_abs, plot=F)
      indexes_to_remove &lt;- c(which((residuals_abs &gt; boxplot$stats[4]) == T), which((residuals_abs &lt; boxplot$stats[2]) == T))

  if (length(indexes_to_remove) &gt; 0) {
    data &lt;- data[-indexes_to_remove, ]
    distances &lt;- distances[-indexes_to_remove]
    weights &lt;- (1/distances)/(sum(1/distances))
  }

  tempModel &lt;- earth(log(price) ~ ., data = data, weights = weights)
  temp_grsq &lt;- round(tempModel$grsq, digits = 4)
      temp_rsq &lt;- round(tempModel$rsq, digits = 4)
  temp_diff &lt;- abs(temp_grsq - temp_rsq)

  if ((temp_grsq &gt; max_grsq &amp;&amp; temp_rsq &gt;= max_rsq) || (temp_grsq &gt;= max_grsq &amp;&amp; temp_rsq &gt; max_rsq)) {
    model &lt;- tempModel
    max_grsq &lt;- temp_grsq
    max_rsq &lt;- temp_rsq
    min_diff &lt;- temp_diff
  } else {
    done = T
  }
 }
</code></pre>

<p><strong>QUESTION</strong></p>

<p>I'm not a statistician so I don't know any better way for removing the outliers. </p>

<ul>
<li>is my approach correct?</li>
<li>should I use another approach?</li>
<li>I know that there are bad outliers and good outliers (leverage points), how can I remove only the bad outliers?</li>
<li>I'm using the <code>semi-log form</code> of the regression. because of the use of <code>dummy variables</code> I can't use the <code>log-log form</code>. Is there any other approach for data transformation? or should I standardize the data? <code>x &lt;- (x - x_min)/(x_max - x_min)</code></li>
</ul>

<p>Does anyone has some hints?</p>
"
"0.0449466574975495","0.0433963036602746","156058","<p>I want to perform a simple linear regression in R. However, the plot of fitted and residual values has outliers. Transformations (ie log, square root) did not solve this problem. Removing these outliers created new outliers. So, what can I do? Is it wrong to adjust a Poisson distribution for this case of a simple linear regression and perform a generalized model? Are there other options?</p>
"
"0.104875534160949","0.130188910980824","156619","<p>I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.</p>

<p><strong>What I have and what I do</strong></p>

<ul>
<li>my data consists out of real estates with following charcteristics: <code>price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.</code></li>
<li>I run a robust regression without intercept <code>lmRob(price ~ . -1)</code></li>
</ul>

<p><strong>What I want</strong></p>

<ul>
<li>a model with which I can predict the price of real estates, but which are not in the used data set</li>
<li>also it would be nice to have some constraints on the coefficients</li>
</ul>

<p><strong>Problems</strong></p>

<ul>
<li>very often I get bad values for the coefficients <code>ex: bathroomCount = -80000</code>. it's not possible that with a additive bathroom , the price of the house will sink with <code>80.000â‚¬</code></li>
<li><p>also I tried to use the function <code>pcls</code> in order to put some constraints on the coefficients, but this method gave very bad results. In the plot <code>Y = price</code> and <code>X = livingArea</code>. as you can see, the regression line isn't correct.
<img src=""http://i.stack.imgur.com/7PHp1.png"" alt=""enter image description here""></p>

<ul>
<li>another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do it</li>
<li>also I tried to use different regression methods <code>lm, lmrob, ltsReg, MARS</code>, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)</li>
<li>I think that the big number of dummy variables damages a little bit the regression</li>
</ul></li>
</ul>

<p>Is my approach false?</p>

<p>Does someone have some hints, tricks for me? (<em>I'm not a statistician</em>)</p>

<p><strong>[UPDATE]</strong></p>

<p><img src=""http://i.stack.imgur.com/a2kLe.png"" alt=""price ~ livingArea""></p>

<p>This is how the plotted data looks like. LivingArea is the only non-dummy variable.</p>

<p><strong>[UPDATE 2]</strong></p>

<pre><code>y = bX 

     means

y = b_0*X_0 + b_1*X_1 + ... + b_k*X_k

     which is an equation system like this:

y[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]
.
.
.
y[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]
</code></pre>

<p>Did I got it right? </p>

<p>If so, isn't possible to add some inequality constraints equation to it. example:</p>

<pre><code>b_0 &gt;= 2000
b_2 &lt;= b_0/2
</code></pre>

<p><strong>[UPDATE 3]</strong></p>

<p>I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0mÂ².
<img src=""http://i.stack.imgur.com/LYPB0.png"" alt=""enter image description here"">
but it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?</p>
"
"0.110096376512636","0.106298800690547","156654","<p>I get a fan-shaped scatter plot of the relation between two different quantitative variables:</p>

<p><img src=""http://i.stack.imgur.com/L7rS3.png"" alt=""enter image description here""></p>

<p>I am trying to fit a linear model for this relation. I think I should apply some kind of transformation to the variables in order to unify the ascent variance in the relation before fitting a linear regression model, but I can't find the way to do it. Or maybe, there is a better model to use in these cases, I can't either find it.</p>

<p>I have tried <code>rlm</code>, but the residuals still have heteroscedasticity. I have also tried to apply a SD ratio calculated from all the y of each x and other similar erratic approaches.</p>

<p>My questions:</p>

<ul>
<li>Is there any typical way of fitting a model for a fan-shaped relation or a typical model to use in these cases?</li>
<li>Is there any typical transformation that could be applied to the variables in order to reduce its variance?</li>
</ul>

<p>Thanks!</p>
"
"0.134839972492648","0.115723476427399","156661","<p><img src=""http://i.stack.imgur.com/6gO09.gif"" alt=""enter image description here""></p>

<p>I get a similar scatter plot (as above) showing the relation between two different quantitative variables. It is also fan-shaped.</p>

<p>I am trying to fit a linear model for this relation. I think I should apply some kind of transformation to the variables in order to unify the ascent variance in the relation before fitting a linear regression model, but I can't find the way to do it. Or maybe, there is a better model to use in these cases, I can't either find it.</p>

<p>I have tried <code>rlm</code>, but when plotting the residuals vs the predictor they are very skewed. I have tried to use the weights retrieved by the model to transform the output:</p>

<pre><code>fit &lt;- rlm(y ~ x, data=df)
plot(fit$resid ~ df$x) # Heteroskedastic &amp; Skewed

df$y &lt;- df$y * fit$w 
    plot(df$y ~ df$x) # Wrong
</code></pre>

<p>But it is obviously wrong.</p>

<p>I have also tried to apply a SD ratio calculated from all the y of each x and other similar erratic approaches.</p>

<p>Is there any typical way of fitting a model for this kind of relation (fan-shaped) or a typical model (and R package) to use in this cases?</p>

<p>Is there any typical transformation that could be applied to the variables in order to reduce the variance?</p>

<p><strong>Sorry, I duplicated the post: Follow <a href=""http://stats.stackexchange.com/questions/156654/fit-regression-model-from-a-fan-shaped-relation-in-r"">Fit regression model from a fan-shaped relation, in R</a>, instead.</strong></p>
"
"0","0.0613716411932216","157735","<p>Polynomial regression is a common way of doing curvilinear regression.  It is common to also use the inverse transform x^-1 (<a href=""http://pareonline.net/getvn.asp?v=8&amp;n=6"" rel=""nofollow"">http://pareonline.net/getvn.asp?v=8&amp;n=6</a>).</p>

<p>One can extend the concept of the inverse transform by thinking about Laurent polynomials.</p>

<p>My question is do some practitioners use Laurent polynomial regression? What are the general ""rules"" around them?  With regular polynomials one tends to get a hill or a valley for each additional power.</p>

<p>I read this post which got me thinking about it:
<a href=""http://math.stackexchange.com/questions/231357/the-degree-of-a-polynomial-which-also-has-negative-exponents"">http://math.stackexchange.com/questions/231357/the-degree-of-a-polynomial-which-also-has-negative-exponents</a></p>

<p><a href=""https://en.wikipedia.org/wiki/Laurent_polynomial"" rel=""nofollow"">https://en.wikipedia.org/wiki/Laurent_polynomial</a></p>

<p>Example R code:</p>

<pre><code>x=runif(100,2,10)/2
y=jitter(1+abs(cos(x)))
df=data.frame(x=x,y=y)
df=df[order(df$x),]
    plot(df$x,df$y)
    lines(df$x,predict(lm(y~x+I(x^2)+I(x^3),df)))
title(""3rd degree poly"");

plot(df$x,df$y)
lines(df$x,predict(lm(y~x+I(x^-1)+I(x^-2),df)))
title(""Laurent poly"");
</code></pre>

<p>I wonder if some regular polynomial regression could benefit by having negative powers in addition to the positive?</p>

<p>Thoughts?</p>
"
"0.0449466574975495","0.0433963036602746","159257","<p>I am working on a marketing data which is a time series data with marketing spend done through different channels and revenue generated.</p>

<p>The data looks like this :
<img src=""http://i.stack.imgur.com/k3yCE.png"" alt=""DataSet Sample""></p>

<p>My data contains too many zeros (no spend at that particular time) and my spend vs channel1 looks like this :</p>

<p><img src=""http://i.stack.imgur.com/NnrNo.png"" alt=""Spend Vs Channel1(Catalog)""></p>

<p>I want to develop a marketing mix model on this data set to understand the impact of various marketing spend on Sales and to predict future Sales and to optimize spend on different channels. How can I proceed analyzing the same. Can I transform variables and do a regression?</p>
"
"0.192118809359305","0.228297867985437","159355","<p>I performed regression with robust variances (after Stata 12.1 lnskew transformation). A question of overfitting has been raised.</p>

<p>To summarise what I did: </p>

<ol>
<li>Comparison of disease B (disgrp=2) versus disease C (disgrp=3)
patients with 45-54 dependent observations (FibrosisP, continuous variable) in each disease group. Each patient had observations taken from Regions P, Q and R and Walls X, Y and Z (i.e. 9 observations per patient). </li>
<li>lnskew0 transformation (natural log transformation with zero skew of
resulting distribution) of FibrosisP to give lfibr. </li>
<li>Simple and then multiple regression with clustered robust variances/standard errors (clustered by patient and using independent categorical variables Disease, Wall and Region).</li>
</ol>

<p>Note that Disease A is excluded from this analysis (and is not provided in data set below).</p>

<p>$$ multipleregression: lfibr \sim Disease + Wall + Region $$</p>

<p>I have copied the original Stata v12.1 log file below, which will hopefully tell the whole story. </p>

<pre><code>. gen disgrp=.
(153 missing values generated)

. replace disgrp=1 if disease==""A""
(54 real changes made)

. replace disgrp=2 if disease==""B""
(54 real changes made)

. replace disgrp=3 if disease==""C""
(45 real changes made)

. lnskew0 lfibr= fibrosisp

       Transform |         k     [95% Conf. Interval]       Skewness
-----------------+--------------------------------------------------
  ln(fibrosis-k) |   .0116473      (not calculated)        -2.77e-08

    . gen region1=.
(153 missing values generated)

. replace region1=1 if region==""P""
(51 real changes made)

. replace region1=2 if region==""Q""
(51 real changes made)

. replace region1=3 if region==""R""
(51 real changes made)

. gen wall1=.
(153 missing values generated)

. replace wall1=1 if wall==""X""
(51 real changes made)

. replace wall1=2 if wall==""Y""
(51 real changes made)

. replace wall1=3 if wall==""Z""
(51 real changes made)

. **Comparing C with B**

. xi:regress lfibr disgrp if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =   20.51
                                                       Prob &gt; F      =  0.0011
                                                       R-squared     =  0.3884
                                                       Root MSE      =   .5833

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372    .204061     4.53   0.001     .4694609    1.378813
       _cons |  -4.667246   .4602243   -10.14   0.000    -5.692689   -3.641802
------------------------------------------------------------------------------

. xi:regress lfibr region1 if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =    3.17
                                                       Prob &gt; F      =  0.1055
                                                       R-squared     =  0.0068
                                                       Root MSE      =  .74333

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     region1 |   .0748154   .0420368     1.78   0.105    -.0188483    .1684792
       _cons |   -2.54854    .177876   -14.33   0.000    -2.944872   -2.152207
------------------------------------------------------------------------------

. xi:regress lfibr i.region1 if disgrp &gt; 1  , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    5.59
                                                       Prob &gt; F      =  0.0235
                                                       R-squared     =  0.0612
                                                       Root MSE      =  .72645

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
 _Iregion1_2 |   .4400513   .1398977     3.15   0.010     .1283397    .7517628
 _Iregion1_3 |   .1496308   .0845103     1.77   0.107    -.0386698    .3379314
       _cons |   -2.59547   .1905071   -13.62   0.000    -3.019946   -2.170993
------------------------------------------------------------------------------

. xi:regress lfibr i.wall1 if disgrp &gt; 1  , cluster(pat)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    6.17
                                                       Prob &gt; F      =  0.0180
                                                       R-squared     =  0.0630
                                                       Root MSE      =  .72575

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   _Iwall1_2 |   .3285724   .1654396     1.99   0.075    -.0400499    .6971948
   _Iwall1_3 |   .4356131   .1305289     3.34   0.008     .1447766    .7264496
       _cons |  -2.653637   .1780801   -14.90   0.000    -3.050425    -2.25685
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp*i.region1*i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  8,    10) =       .
                                                       Prob &gt; F      =       .
                                                       R-squared     =  0.5401
                                                       Root MSE      =  .55355

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .5419458   .4154947     1.30   0.221    -.3838342    1.467726
 _Iregion1_2 |  -.2963738   1.409317    -0.21   0.838    -3.436529    2.843781
 _Iregion1_3 |  -.2791626   1.335259    -0.21   0.839    -3.254304    2.695979
   _Iwall1_2 |  -1.039268     .97762    -1.06   0.313    -3.217541    1.139005
   _Iwall1_3 |  -.9227228   1.131622    -0.82   0.434    -3.444133    1.598687
    _IdiXre2 |    .305921   .5859783     0.52   0.613    -.9997201    1.611562
    _IdiXre3 |   .2146185   .5228838     0.41   0.690    -.9504393    1.379676
    _IdiXwa2 |   .5887627   .4158743     1.42   0.187    -.3378629    1.515388
    _IdiXwa3 |   .5677226   .5322211     1.07   0.311      -.61814    1.753585
   _Ire2Xwa2 |   .9560212   1.372943     0.70   0.502    -2.103087    4.015129
   _Ire2Xwa3 |   1.876106   1.632401     1.15   0.277    -1.761111    5.513323
   _Ire3Xwa2 |   .1403149   1.711091     0.08   0.936    -3.672233    3.952863
   _Ire3Xwa3 |   .5961959   1.627029     0.37   0.722     -3.02905    4.221442
_IdiXre2Xwa2 |  -.4387073   .5346165    -0.82   0.431    -1.629907    .7524925
_IdiXre2Xwa3 |  -.7328102   .7126107    -1.03   0.328    -2.320606    .8549855
_IdiXre3Xwa2 |  -.1024311   .6268405    -0.16   0.873    -1.499119    1.294257
_IdiXre3Xwa3 |  -.3174033   .6961228    -0.46   0.658    -1.868461    1.233655
       _cons |  -4.217918   .9792797    -4.31   0.002     -6.39989   -2.035947
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp i.region1 i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  5,    10) =   10.60
                                                       Prob &gt; F      =  0.0010
                                                       R-squared     =  0.5127
                                                       Root MSE      =  .53177

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372   .2084032     4.43   0.001     .4597858    1.388488
 _Iregion1_2 |   .4400513   .1421362     3.10   0.011      .123352    .7567505
 _Iregion1_3 |   .1496308   .0858625     1.74   0.112    -.0416828    .3409444
   _Iwall1_2 |   .3285724   .1680868     1.95   0.079    -.0459482    .7030931
   _Iwall1_3 |   .4356131   .1326175     3.28   0.008     .1401229    .7311033
       _cons |  -5.118535   .4532473   -11.29   0.000    -6.128433   -4.108637
------------------------------------------------------------------------------
</code></pre>

<p>csv data:</p>

<pre><code>""row"",""PatientID"",""Disease"",""Wall"",""Region"",""FibrosisP""
""1"",1,""C"",""X"",""P"",0.11574464021797
""2"",1,""C"",""X"",""Q"",0.06409239204845
""3"",1,""C"",""X"",""R"",0.05589004594181
""4"",2,""C"",""X"",""P"",0.08452786770152
""5"",2,""C"",""X"",""Q"",0.19765474370344
""6"",2,""C"",""X"",""R"",0.29491566808792
""7"",3,""C"",""X"",""P"",0.13849556170319
""8"",3,""C"",""X"",""Q"",0.21529108879539
""9"",3,""C"",""X"",""R"",0.23260346696877
""10"",4,""C"",""X"",""P"",0.03242538798989
""11"",4,""C"",""X"",""Q"",0.18213249953927
""12"",4,""C"",""X"",""R"",0.0464009382069
""13"",17,""C"",""X"",""P"",0.12925196186539
""14"",17,""C"",""X"",""Q"",0.16685146683109
""15"",17,""C"",""X"",""R"",0.16298253982187
""16"",5,""B"",""X"",""P"",0.06082167946576
""17"",5,""B"",""X"",""Q"",0.06179248715729
""18"",5,""B"",""X"",""R"",0.04635879285168
""19"",6,""B"",""X"",""P"",0.0512284261286
""20"",6,""B"",""X"",""Q"",0.05560175796177
""21"",6,""B"",""X"",""R"",0.05038057719884
""22"",7,""B"",""X"",""P"",0.03485909775192
""23"",7,""B"",""X"",""Q"",0.07526805988175
""24"",7,""B"",""X"",""R"",0.03989544438546
""25"",8,""B"",""X"",""P"",0.05069990522336
""26"",8,""B"",""X"",""Q"",0.11638788902232
""27"",8,""B"",""X"",""R"",0.23086071670409
""28"",9,""B"",""X"",""P"",0.12712370092246
""29"",9,""B"",""X"",""Q"",0.05070659692429
""30"",9,""B"",""X"",""R"",0.06183074530974
""31"",10,""B"",""X"",""P"",0.04509566111129
""32"",10,""B"",""X"",""Q"",0.09050081347533
""33"",10,""B"",""X"",""R"",0.05178363738579
""52"",1,""C"",""Y"",""P"",0.14421181658066
""53"",1,""C"",""Y"",""Q"",0.1299066509205
""54"",1,""C"",""Y"",""R"",0.14904819595697
""55"",2,""C"",""Y"",""P"",0.08801608368174
""56"",2,""C"",""Y"",""Q"",0.24864891863453
""57"",2,""C"",""Y"",""R"",0.15962998919524
""58"",3,""C"",""Y"",""P"",0.4272296674396
""59"",3,""C"",""Y"",""Q"",0.2593375589095
""60"",3,""C"",""Y"",""R"",0.26700346966879
""61"",4,""C"",""Y"",""P"",0.14002780500134
""62"",4,""C"",""Y"",""Q"",0.28346720806288
""63"",4,""C"",""Y"",""R"",0.19312813953225
""64"",17,""C"",""Y"",""P"",0.17668051188556
""65"",17,""C"",""Y"",""Q"",0.18609876357474
""66"",17,""C"",""Y"",""R"",0.26587590290484
""67"",5,""B"",""Y"",""P"",0.05356234036154
""68"",5,""B"",""Y"",""Q"",0.04731210983269
""69"",5,""B"",""Y"",""R"",0.04877515848359
""70"",6,""B"",""Y"",""P"",0.06240572241178
""71"",6,""B"",""Y"",""Q"",0.13301297541279
""72"",6,""B"",""Y"",""R"",0.17973855854636
""73"",7,""B"",""Y"",""P"",0.06463245380331
""74"",7,""B"",""Y"",""Q"",0.10244742460486
""75"",7,""B"",""Y"",""R"",0.0599854720435
""76"",8,""B"",""Y"",""P"",0.05824947941558
""77"",8,""B"",""Y"",""Q"",0.11926213239492
""78"",8,""B"",""Y"",""R"",0.04685947691071
""79"",9,""B"",""Y"",""P"",0.06752011460398
""80"",9,""B"",""Y"",""Q"",0.09542812038592
""81"",9,""B"",""Y"",""R"",0.08668150350578
""82"",10,""B"",""Y"",""P"",0.06486814661182
""83"",10,""B"",""Y"",""Q"",0.05854476138367
""84"",10,""B"",""Y"",""R"",0.04438863783229
""103"",1,""C"",""Z"",""P"",0.05133333746688
""104"",1,""C"",""Z"",""Q"",0.14821006659988
""105"",1,""C"",""Z"",""R"",0.08174176027544
""106"",2,""C"",""Z"",""P"",0.23884995419341
""107"",2,""C"",""Z"",""Q"",0.2099355433643
""108"",2,""C"",""Z"",""R"",0.13176723596276
""109"",3,""C"",""Z"",""P"",0.46479557484677
""110"",3,""C"",""Z"",""Q"",0.33304596595977
""111"",3,""C"",""Z"",""R"",0.29770388592371
""112"",4,""C"",""Z"",""P"",0.15308213537672
""113"",4,""C"",""Z"",""Q"",0.28081619128875
""114"",4,""C"",""Z"",""R"",0.24592983188039
""115"",17,""C"",""Z"",""P"",0.21312809357862
""116"",17,""C"",""Z"",""Q"",0.23336174725733
""117"",17,""C"",""Z"",""R"",0.22714195157817
""118"",5,""B"",""Z"",""P"",0.06818263568709
""119"",5,""B"",""Z"",""Q"",0.07257093444773
""120"",5,""B"",""Z"",""R"",0.08201262934886
""121"",6,""B"",""Z"",""P"",0.0644884733419
""122"",6,""B"",""Z"",""Q"",0.11937946452025
""123"",6,""B"",""Z"",""R"",0.07081608918845
""124"",7,""B"",""Z"",""P"",0.06720225949377
""125"",7,""B"",""Z"",""Q"",0.12509595330262
""126"",7,""B"",""Z"",""R"",0.06657357031905
""127"",8,""B"",""Z"",""P"",0.05878644062606
""128"",8,""B"",""Z"",""Q"",0.26638352132337
""129"",8,""B"",""Z"",""R"",0.06789933388591
""130"",9,""B"",""Z"",""P"",0.0908078338911
""131"",9,""B"",""Z"",""Q"",0.17670466924957
""132"",9,""B"",""Z"",""R"",0.10642489420997
""133"",10,""B"",""Z"",""P"",0.05107976253608
""134"",10,""B"",""Z"",""Q"",0.07242867177979
""135"",10,""B"",""Z"",""R"",0.05074329491013
</code></pre>

<p>My interpretation is that we have 99 observations for 3 categorical variables (2-3 categories each) in the regression analysis; Root MSE = 0.53177.</p>

<p>Is overfitting a valid concern here? If so, is there any way to address it?</p>

<p>A general answer would be helpful. Moreover, I'm trying to move to R (rather than Stata), so advice on replicating the analysis and/or addressing overfitting with R would be gratefully received.</p>

<p>ADDENDUM1:
I've partially worked out how to replicate analysis in R. Haven't yet replicated lnskew0, though <a href=""https://rpubs.com/chrisbrunsdon/skewness"" rel=""nofollow"">https://rpubs.com/chrisbrunsdon/skewness</a> looks similar.</p>

<pre><code>p2.df &lt;- read.table(""data_above.csv"", header=TRUE, sep="","")

library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)

options(digits = 8)  # for more exact comparison with Stata's output

#create ln FibrosisP (need to replicate lnskew0 from Stata - manual k entered here from Stata calculation)

p2.df$FibrosisPln &lt;- log(p2.df$FibrosisP-0.0116473)

p1.df &lt;- p2.df[c(""PatientID"", ""DiseaseG"", ""WallG"", ""RegionG"", ""FibrosisPln"")]

p1.df$DWR &lt;- paste(p1.df$DiseaseG, p1.df$WallG, p1.df$RegionG)

p.df &lt;- pdata.frame(p1.df, index = c(""PatientID"", ""DWR""), drop.index = F, row.names = T)

# tools_reg.R from http://www.existencia.org/pro/?p=134
source(""tools_reg.R"")
mod &lt;- lm(FibrosisPln~factor(DiseaseG)+factor(WallG)+factor(RegionG),data=p.df)
get.coef.clust(mod, p.df$PatientID) # identical to Stata output
</code></pre>

<p>ADDENDUM2:
I have performed 10-fold cross-validation with CVlm from R package DAAG. However, I am uncertain how to interpret the results. Does the presence of overlapping lines in the plot for all 10 folds suggest no overfitting is present?</p>

<pre><code>library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)
CVlm(df=p.df, form.lm=mod, m=10, plotit = c(""Observed"",""Residual""), main=""Small symbols show cross-validation predicted values"", legend.pos=""topleft"", printit=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/HqTzi.png"" alt=""CVlm plot""></p>

<pre><code>t test of coefficients:

                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -3.2703     0.1441  -22.70  &lt; 2e-16 ***
factor(DiseaseG)C   0.9241     0.2084    4.43  2.5e-05 ***
factor(WallG)Y      0.3286     0.1681    1.95   0.0536 .  
factor(WallG)Z      0.4356     0.1326    3.28   0.0014 ** 
factor(RegionG)Q    0.4401     0.1421    3.10   0.0026 ** 
factor(RegionG)R    0.1496     0.0859    1.74   0.0847 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>ADDENDUM3:
After a lot of searching, the only explanation I could find online (which I presume is accurate) is given at <a href=""http://rstatistics.net/regression-modelling/"" rel=""nofollow"">http://rstatistics.net/regression-modelling/</a></p>

<blockquote>
  <p>""The fitted lines of different colors are parallel and on-top of each other. Indicating a stable model direction and less influence of outliers.""</p>
</blockquote>
"
"0.149591518401353","0.156467598012726","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.100503781525921","0.0776296679652776","160021","<p><a href=""http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor"">Interpretation of log transformed predictor</a> neatly explains how to interpret a <code>log</code> transformed predictor in OLS. Does the interpretation change if there are 0s in the data and the transformation becomes <code>log(1 + x)</code> instead? </p>

<p>Some authors (e.g. Fox and Weisberg 2011) recommend adding a <code>start</code> (i.e. a positive constant) if a <code>log</code> transformation is necessary to correct skewness and improve symmetry, but the data contains zeros. </p>

<p>Consider a variation of the <code>Ornstein</code> example in CAR (p. 303): </p>

<pre><code>require(car)
data(Ornstein)
boxplot(Ornstein$interlocks, horizontal = T) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/j8XSa.png"" alt=""enter image description here""></p>

<p>The data is clearly right skewed, and contains 0s. </p>

<pre><code>summary(powerTransform(1 + Ornstein$interlocks))
    ## bcPower Transformation to Normality 
    ## 
    ##                         Est.Power Std.Err. Wald Lower Bound Wald Upper Bound
    ## 1 + Ornstein$interlocks    0.1248    0.053           0.0209           0.2287
## 
## Likelihood ratio tests about transformation parameters
##                              LRT df      pval
## LR test, lambda = (0)   5.502335  1 0.0189911
## LR test, lambda = (1) 262.431991  1 0.0000000
</code></pre>

<p>The <code>powerTransform()</code> function suggests that a <code>log(1 + x)</code> transformation here could be useful. </p>

<pre><code>boxplot(log(1 + Ornstein$interlocks), horizontal = T)
</code></pre>

<p><img src=""http://i.stack.imgur.com/W3YlX.png"" alt=""enter image description here""></p>

<p>As you can see, symmetry is indeed improved. </p>

<p><strong>Question:</strong> If this transformed variable were to be included in an OLS regression as an IV, would the coefficient estimates still have the usual interpretation of <code>log</code> transformed variables? </p>
"
"0.0635641726163728","0.0306858205966108","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.127128345232746","0.122743282386443","163604","<p>I'm using a plate reader to measure optical density of different bacterial
strains so I can compare their responses (growth rates and changes in them over
time) to stress conditions. The growth curves often don't follow any standard
shape so I'm fitting them empirically with the <code>loess</code> or <code>locfit</code> functions in
R, breaking the fits into intervals, and taking the derivatives to get growth
rates. My plots look like this:</p>

<p><a href=""http://i.stack.imgur.com/fiiLH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fiiLH.png"" alt=""locfit fitted data points""></a>
<a href=""http://i.stack.imgur.com/4dui4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4dui4.png"" alt=""simplified fitted curves""></a>
<a href=""http://i.stack.imgur.com/4t7K4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4t7K4.png"" alt=""derivatives of simplified curves""></a></p>

<p>As you can see the fitted curves have confidence intervals, but I'm not sure
how to transform them into a meaningful form (95% confidence or standard
deviation for example). And assuming that's doable, how do I go on to calculate
uncertainty in the rates?</p>

<p>I suppose I could just use the worst-case difference in slopes like this:</p>

<p><a href=""http://i.stack.imgur.com/fcEaY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fcEaY.png"" alt=""bad idea""></a></p>

<p>But that seems like a bad idea.</p>

<p>I could fit each well separately or split them into groups--there are a few
replicates for each strain and I could add more if needed--and just use the
standard deviation of the final calculated rates. Is that the best way? If so,
how do I decide the optimal group size to balance accurate fits with a good
number of replicates? I would also be open to using a different type of fit of course.</p>

<p>I've found a couple related questions, but neither one quite answers it:</p>

<ul>
<li><p><a href=""http://stats.stackexchange.com/questions/70629/calculate-uncertainty-of-linear-regression-slope-based-on-data-uncertainty"">This one</a> seems to rely on the true relationship being linear, which my curves violate</p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/18391/how-to-calculate-the-difference-of-two-slopes"">This one</a> may well be correct but my stats knowledge is too basic to understand the answer</p></li>
</ul>

<p>EDIT: I'm using <code>deg=1</code> for both types of fits because I expect growth during log-phase to be linear on a log-transformed scale, but maybe higher-degree polynomials would be more accurate?</p>

<p>EDIT: <a href=""http://stats.stackexchange.com/questions/147106/determining-if-two-growth-curves-are-significantly-different"">This answer</a> looks very promising and I'm off to read the suggested paper.
EDIT: Nope, also depends on having a known underlying physical model.</p>
"
"0.0674199862463242","0.0867926073205492","164017","<p>I have a set of noisy data that can be described by a functional form.</p>

<p>For each observation f(x), where x is an index that runs from 0-100, I know that f(x)=g(x+1)/g(x)-g(x+1). I would like to find a way of fitting f(x). I also know that f(x) must be smooth. How could I do this?  </p>

<p>My idea is to try and fit this data using penalized splines. I choose a spline basis and a smoothing factor, and then find the coefficients of a regression on the spline basis that produce a curve f(x). I then optimize the coefficients to produce a curve such that when it is transformed it fits my data. A minimal reproducible example in R is below. </p>

<pre><code>require(dplyr)
require(gam)

target = c(0.132167681875765,0.804942648636132,0.60485585022111,1.02164234486286,0.58437549344597,0.88268397325963)

to_optim = function(par,target,knots,smooth,range) {

spline_reg = function(range,knots,par) bs(range,knots) %*% par

distance = function(fitted,target,smooth) sum((fitted-target)^2) +t(as.matrix(diff(fitted))) %*% 
  (as.matrix(diff(fitted))) * (smooth)

fitted = spline_reg(range,knots,par)

crs = fitted/lag(fitted)-fitted
crs=crs[3:length(crs)]
target=target[3:length(target)]

to_ret = distance(crs,target,smooth)

return(to_ret)

}



my_range = seq(1,6)
mypars = 4
smooth=.8

fit = optim(c(runif(mypars)),to_optim,lower=c(rep(-10,mypars)),
            upper=c(rep(10,mypars)),smooth=smooth,knots=mypars,target=target,range=my_range,
            method=""L-BFGS-B"")


par(mfrow=c(1,2))
bs(my_range,mypars) %*% fit$par %&gt;% plot

test = bs(my_range,mypars) %*% fit$par
plot(test/lag(test)-test~target)
abline(0,1)
</code></pre>
"
"0.127128345232746","0.122743282386443","164120","<p>I am attempting to conduct a logistic regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. My response variable (service points) is binary in the sense that it can have only two outcomes for each observation - a success (service point win) or a failure (service point loss). </p>

<p>I have an issue with my data: For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>250 matches, each with about 70 dependent variable observations (service points). So for each match I currently have the two variables: Total_Service_Points_Played <strong>and</strong> Total_Service_Points_Won. </p>

<p>Eg - Match 1: Total_Service_Points_Played: 70 ; Total_Service_Points_Won: 47</p>

<p>So my data isn't in 1's and 0's. Is there a way I can implement a logistic regression with my dependent variable observations in their current form? Is there any simple transformation that comes to mind?</p>

<p>What springs to mind for me is to flesh out my match data into 1's and 0's. So following on from Match 1 above I would have: 47 1's followed by 26 0's . My data doesn't provide information as to what sequence these 1's and 0's arrived in, but since the depdendent variable observations are i.i.d this won't cause an issue? Correct me if I'm wrong please. Another issue posed by this technique would be the massive increase in my data - from 250 observations as a ratio (service point wins/service points played) to 250*70=17500 observations or more.</p>

<p>As a side note, the last thing I'm wondering is about the dispersion of my dependent variable data. Specifically, in the ratio of serve wins to total serve points as above, there exists no values &lt; 0.2 or 20% .... In addition, there exists no value > 0.9 ..... Does this fit the bill for the (link=logit) argument? I know this relates to an S shape curve which is undefined at 0 and 1, but approaches both values.... I might be going off track here but is this something to be concerned about? </p>
"
"0.0898933149950989","0.0867926073205492","164648","<p>I have created a Logistic Regression using the following code:</p>

<pre><code>full.model.f = lm(Ft_45 ~ ., LOG_D)
base.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg)
step(base.model.f, scope=list(upper=full.model.f, lower=~1),
     direction=""forward"", trace=FALSE)
</code></pre>

<p>I have then used the output to create a final model:</p>

<pre><code>final.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg + IP_util_E2_m02_flg + 
                           AE_NumVisit1_flg + OP_NumVisit1_m01_flg + IP_TotLoS_m02 + 
                           Ft1_45 + IP_util_E1_m05_flg + IP_TotPrNonElecLoS_m02 + 
                           IP_util_E2pl_m03_flg + LTC_coding + OP_NumVisit0105_m03_flg +
                           OP_NumVisit11pl_m03_flg + AE_ArrAmb_m02_flg)
</code></pre>

<p>Then I have predicted the outcomes for a different set of data using the predict function:</p>

<pre><code>log.pred.f.v &lt;- predict(final.model.f, newdata=LOG_V)
</code></pre>

<p>I have been able to use establish a pleasing ROC curve and created a table to establish the sensitivity and specificity which gives me responses I would expect. </p>

<p>However What I am trying to do is establish for each row of data what the probability is of Ft_45 being 1. If I look at the output of log.pred.f.v I get, for example,:</p>

<pre><code>1 -0.171739593    
2 -0.049905948    
3 0.141146419    
4 0.11615669    
5 0.07342591    
6 0.093054334    
7 0.957164383    
8 0.098415639    
.
.
.
104 0.196368229    
105 1.045208447    
106 1.05499112
</code></pre>

<p>As I only have a tentative grasp on what I am doing I am struggling to understand how to interpret the negative and higher that 1 values as I would expect a probability to be between 0 and 1.</p>

<p>So my question is am I just missing a step where I need to transform the output or have I gone completely wrong.
Thank you in advance for any help you are able to offer.</p>
"
"0.0778498944161523","0.0751646028002829","167324","<p>I'm trying to obtain the variance-covariance matrix of a logistic regression:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
mylogit &lt;- glm(admit ~ gre + gpa, data = mydata, family = ""binomial"")
</code></pre>

<p>through matrix computation. I have been following the example published <a href=""http://www.ats.ucla.edu/stat/r/library/matrix_alg.htm"" rel=""nofollow"">here</a> for the basic linear regression</p>

<pre><code>X &lt;- as.matrix(cbind(1, mydata[,c('gre','gpa')]))
beta.hat &lt;- as.matrix(coef(mylogit))
Y &lt;- as.matrix(mydata$admit)
y.hat &lt;- X %*% beta.hat

n &lt;- nrow(X)
p &lt;- ncol(X)

sigma2 &lt;- sum((Y - y.hat)^2)/(n - p)        
v &lt;- solve(t(X) %*% X) * sigma2
</code></pre>

<p>But then my var/cov matrix doesn't not equals the matrix computed with <code>vcov()</code></p>

<pre><code>v == vcov(mylogit)

1   gre   gpa
1   FALSE FALSE FALSE
gre FALSE FALSE FALSE
gpa FALSE FALSE FALSE
</code></pre>

<p>Did I miss some log transformation?</p>
"
"0.156162493533897","0.139177800127844","167825","<p>I am trying to use ""Cursor"" , ""PostCursor"" and ""CTLE"" to predict ""left"", and I added interactions and quandratic in the model.</p>

<pre><code>  &gt;left_int3&lt;-lm(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + I
               (PostCursor^2), data = QPI)
  &gt;summary(left_int3)

  Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept)     -412.58163   71.34574  -5.783 8.16e-09 ***
  Cursor            21.46885    2.85689   7.515 7.63e-14 ***
  PostCursor         2.96808    0.38768   7.656 2.62e-14 ***
  CTLE              -0.20459    0.01884 -10.858  &lt; 2e-16 ***
  I(Cursor^2)       -0.22646    0.02837  -7.982 2.09e-15 ***
  I(PostCursor^2)    0.24471    0.04070   6.013 2.06e-09 ***
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
  Residual standard error: 2.171 on 2794 degrees of freedom
  Multiple R-squared:  0.4174,  Adjusted R-squared:  0.4164 
  F-statistic: 400.4 on 5 and 2794 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Then I inspected the 4 assumptions of regression and found that normality, linearity and constant variance are violated so need to transform:</p>

<pre><code> **HOMOSCEDASTICITY**

 &gt; ncvTest(left_int3)
 Non-constant Variance Score Test 
 Variance formula: ~ fitted.values 
 Chisquare = 3.505792    Df = 1     p = 0.06115458 
 &gt; spreadLevelPlot(left_int3)
 Suggested power transformation:  1.12032

 **Linearity**

 &gt; boxTidwell(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + I
             (PostCursor^2), data = QPI)  #

                 Score Statistic   p-value MLE of lambda
 Cursor                 7.162587 0.0000000      7.123073
 PostCursor            -3.534346 0.0004088     16.129858
 CTLE                  -1.921833 0.0546268      3.891245
 I(Cursor^2)           -7.641956 0.0000000      4.145477
 I(PostCursor^2)        4.937534 0.0000008      8.687134

 **Normality**

&gt; summary(powerTransform(QPI$Left))
bcPower Transformation to Normality 

           Est.Power Std.Err. Wald Lower Bound Wald Upper Bound
QPI$Left    3.7107   0.4409           2.8466           4.5749

Likelihood ratio tests about transformation parameters
                         LRT df         pval
LR test, lambda = (0) 72.13642  1 0.000000e+00
LR test, lambda = (1) 38.30386  1 6.054269e-10

**Independence** 

boxTidwell(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + 
         I(PostCursor^2), data = QPI)  #
                Score Statistic   p-value MLE of lambda
Cursor                 7.162587 0.0000000      7.123073
PostCursor            -3.534346 0.0004088     16.129858
CTLE                  -1.921833 0.0546268      3.891245
I(Cursor^2)           -7.641956 0.0000000      4.145477
I(PostCursor^2)        4.937534 0.0000008      8.687134
</code></pre>

<p>Then I performed the transformations and fit again, but the R^2 is still low, so I am wondering my transformations are correct or not.</p>

<pre><code> &gt;QPI$Left&lt;-QPI$Left^3.7107  
 &gt;QPI$Cursor&lt;-QPI$Cursor^7.123
 &gt;QPI$PostCursor&lt;-QPI$PostCursor^16.129
 &gt;QPI$CTLE&lt;-QPI$CTLE^3.891245
 &gt;left_int3&lt;-lm(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + 
             I(PostCursor^2), data = QPI)
 &gt;summary(left_int3)
 &gt;Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept)      1.455e+07  6.651e+05  21.880  &lt; 2e-16 ***
 Cursor           2.299e-06  1.302e-06   1.766  0.07754 .  
 PostCursor       1.150e-06  2.147e-06   0.536  0.59231    
 CTLE            -4.772e+00  4.548e-01 -10.493  &lt; 2e-16 ***
 I(Cursor^2)     -2.162e-18  6.854e-19  -3.154  0.00163 ** 
 I(PostCursor^2) -2.775e-19  5.977e-19  -0.464  0.64253    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 1175000 on 2794 degrees of freedom
 Multiple R-squared:  0.3942,    Adjusted R-squared:  0.3932 
 F-statistic: 363.7 on 5 and 2794 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.195917937881753","0.17920430732685","168068","<p>I am using R for this analysis, and so examples and graphics will be produced in this language. I am willing to provide equivalent examples in similar languages if it will help someone, and am willing to accept answers in terms of other languages.</p>

<p>In this question, I intend to display graphs produced in order to verify assumptions, and ask for help in getting a better model. I understand that this may be considered too specific. However, it is my opinion that it would be helpful to have more examples of bad models and how to correct them on this site. If a moderator finds this not to be the case, I will happily delete this post.</p>

<p>I have conducted an initial linear model (lm) in R. It is multiple categorical regression with approx 100,000 cases, two categorical regressors and a continuous regressand. The goal of this regression is prediction: specifically, I would like to estimate prediction intervals. Find below some diagnostics of the initial model:</p>

<p>Residuals histogram (full) below. It may be difficult (impossible) to see, but there exist (sparse) values between 300 and 2000, as well as -50 and -500. Between -50 and 300, values are very dense. This indicates, to my understanding, heavy tails.</p>

<p><a href=""http://i.stack.imgur.com/FoGN7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FoGN7.png"" alt=""Residuals Historgram""></a></p>

<p>Residuals histogram (partial) below. Same image as above, but zoomed to the dense area.</p>

<p><a href=""http://i.stack.imgur.com/Q9bBl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q9bBl.png"" alt=""enter image description here""></a></p>

<p>A normal Quantile Quantile (normal QQ plot) is found below. Again, according to the <a href=""http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot"">holy grail of qqplots</a>, (super) heavy tails are indicated.</p>

<p><a href=""http://i.stack.imgur.com/TrATp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TrATp.png"" alt=""Initial QQPlot""></a></p>

<p>Below is predicted vs residuals. Clearly, funky stuff is going on, suggesting heteroscedasticity:</p>

<p><a href=""http://i.stack.imgur.com/oOMRU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oOMRU.png"" alt=""Resid Vs Predicted""></a></p>

<p>I first tried some transformations. BoxCox yields a value very close to zero. So I will try to take the log of the regressand (in accordance with <a href=""https://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation"" rel=""nofollow"">the Wikipedia page</a>). </p>

<p><a href=""http://i.stack.imgur.com/3IbMv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3IbMv.png"" alt=""boxcox""></a></p>

<p><strong>Log Transform:</strong></p>

<p>Log transformed histogram, looks a lot better, but we still have some skew:</p>

<p><a href=""http://i.stack.imgur.com/exSgd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/exSgd.png"" alt=""Histogram of Log transform""></a></p>

<p>And the NormalQQ Plot. Still seems that the residuals are not normally distributed.</p>

<p><a href=""http://i.stack.imgur.com/JosvR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JosvR.png"" alt=""log QQPlot""></a></p>

<p>Logarithm transformed Residual vs Predicted. Seems we have some decreasing variance now, but I would be willing to accept this assumption.</p>

<p><a href=""http://i.stack.imgur.com/Sg4B9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Sg4B9.png"" alt=""Log Resid Vs Predicted""></a></p>

<p>Other transformations I tried: raising regressand to powers 1/2, 1/3 and -1. None of these had satisfactory results; I choose not to include information about these transformations in order to save space, but will happily provide such information should it be requested.</p>

<p><strong>Here lie my questions:</strong></p>

<p>1) Is the solution to this problem simply to keep trying increasingly wacky transformations (ex: $1/log(x^{\pi/3})$)?</p>

<p>2) I have been looking (intermittently over a period of weeks) at Generalized Linear Models, which seem to allow a non-normal distribution of residuals. Unfortunately, I have not been able to understand them, and non of my (undergraduate statistics) peers have knowledge of them. If GLM's present a solution to this issue, I would be grateful if someone could explain them in this context. (Even if they are not a solution, I would be grateful for a simple explanation, or a reference to one).</p>

<p>2i) If GLM's are a good fit, I believe I would still need a distribution to model error by. What ways are there of detecting which (family) of distribution is the best fit for the residuals, after which I assume I can perform MLE to get the parameters? I've been having issues trying to evaluate heavy tailed distributions with respect to skew, because they tend not to have any moments, and so have $\infty$ or indeterminate skew.</p>

<p>3) Is there another class of models not aforementioned I should look into?</p>

<p>4) Is my current model sufficient for prediction intervals, despite the non-normality of residuals?</p>

<p>Some more information about the model: I am predicting a cost, thus the log transform is appealing in that my predicted values are positive reals.</p>

<p>I will be hanging around my computer all day, and have R gui open on my other monitor, so should be able to fulfill most requests for additional information.</p>
"
"0.0778498944161523","0.0751646028002829","168482","<p>I am running a probit regression with a random effect:</p>

<pre><code>m1&lt;-glmer(Binary~Explan+(1|Random),family=binomial(link=""probit""))
</code></pre>

<p>where Explan is a three-level categorical variable. </p>

<p>I want to calculate the mean predicted probabilities for each level of Explan. I tried doing so using this code:</p>

<pre><code>newdata=data.frame(Explan=""First"")
predict(m1,newdata,type=""response"")
</code></pre>

<p>where First is a level of the categorical Explan variable.</p>

<p>However I get the following error message:</p>

<pre><code>Error: (p &lt;- ncol(X)) == ncol(Y) is not TRUE
</code></pre>

<p>Were this a logit model, I would simply strip the model of the intercept and then back-transform the model summary coefficients to get the predicted values that I'm after, but I am unsure of how I would go about this with a mixed-effects probit model. </p>

<p>Any help in extracting the predicted probabilities would be greatly appreciated.</p>
"
"0.0518999296107682","0.0751646028002829","169334","<p>I'm trying to use the <code>circular</code> package in R to perform regression of a circular response variable and linear predictor, and I do not understand the coefficient value I'm getting. I've spent considerable time searching in vain for an explanation that I can understand, so I'm hoping somebody here may be able to help.</p>

<p>Here's an example:</p>

<pre><code>library(circular)

# simulate data
x &lt;- 1:100
set.seed(123)
y &lt;- circular(seq(0, pi, pi/99) + rnorm(100, 0, .1))

# fit model
m &lt;- lm.circular(y, x, type=""c-l"", init=0)

&gt; coef(m)
[1] 0.02234385
</code></pre>

<p>I don't understand this coefficient of 0.02 -- I would expect the slope of the regression line to be very close to pi/100, as it is in garden variety linear regression:</p>

<pre><code>&gt; coef(lm(y~x))[2]
         x
0.03198437
</code></pre>

<p>Does the circular regression coefficient not represent the change in response angle per unit change in the predictor variable? Perhaps the coefficient needs to be transformed via some link function to be interpretable in radians? Or am I thinking about this all wrong? Thanks for any help you can offer.</p>
"
"0.0778498944161523","0.0751646028002829","171763","<p>I am working on a paper about sexual coherence in women. Sexual coherence is defined as the relationship between subjective (SA) and genital sexual (GA) arousal. There is research that shows that this coherence can be higher or lower, depending on other factors, like age or arousability...</p>

<p>Both measures (SA and GA) have been measured continuously over a period of 5 minutes. I divided these 5-minutes into 15-second intervalls and calculated the mean for both arousal measures for each section.</p>

<p>Additionally, I have 2 questionnaire scores (P1, P2) that might influence SA, GA or (most importantly) the relationship between SA and GA</p>

<p>I use the package nlme in R and my data is transformed into long format.</p>

<p>My first question: Is it, in your opinion, possible to assess sexual coherence between GA and SA with a regression analysis, in which SA is the outcome and GA is the predictor?</p>

<p>My second question: If I want to investigate the impact of P1 and P2 on sexual concordance (the association between GA and SA), is it feasible to add the questionnaires to the above mentioned regression?
The model would look something like this: Coherence.model &lt;-nlme ( SA ~ GA + P1 + P2 + (GAP1) + (GAP2) + (P1*P2)) My idea is that you can assess the direct influence of GA, P1 and P2 on SA and (if the interaction terms (GA*P1) is significant) you can say that, e.g., P1 is a moderator of the relationship between GA and SA.</p>

<p>What do you think? Or do you have another idea, who to work with an ""coherence measure"" as outcome variable?</p>

<p>Best, Julia</p>

<p>Please excuse that I did not get into detail regarding syntax or programming. But I hope that this is not necessary at this moment.</p>
"
"0.0674199862463242","0.0867926073205492","172003","<p>I'd like to fit integer coefficients, e.g. summing to 10, to a regression equation. The absolute values of the coefficients (i.e. predicted y) aren't important, I just want to retain the appropriate relative values. The use case is for an easily interpretable scoring system.</p>

<p>For example, this regression yields the following coefficients (ignoring the intercept):</p>

<pre><code>set.seed(0)
y &lt;- rnorm(100)
x &lt;- matrix(rnorm(300), ncol=3)
m &lt;- lm(y ~ x)
(coef &lt;- m$coefficients[-1])
#          x1          x2          x3 
#  0.12100965  0.05506511  0.14708549 
</code></pre>

<p>Rounding with the below code yields a rounding error (sums to 11):</p>

<pre><code>round(10 * coef / sum(coef))
# x1 x2 x3 
#  4  2  5 
</code></pre>

<p>A method like this also doesn't guarantee maximally similar weights to the regression equation. </p>

<p>This was asked <a href=""http://www.researchgate.net/post/How_do_I_transform_beta_coefficients_into_integer_values"" rel=""nofollow"">here</a> without satisfactory answers, and might be addressed in <a href=""http://www.jstor.org/stable/2347432"" rel=""nofollow"">this paywalled research paper</a>.</p>

<p><strong>Edit:</strong> looks like <a href=""http://stackoverflow.com/questions/792460/how-to-round-floats-to-integers-while-preserving-their-sum"">How to round floats to integers while preserving their sum?</a> may be able to help minimize the roundoff error. If my question is further specified as minimizing the error of a predicted (scaled) y, I'm unsure whether this is an equivalent optimization.</p>
"
"0.0449466574975495","0.0433963036602746","173568","<p>In helping us understand how to fit a logistic regression in <code>R</code>, we are told to first replace 0 and 1 in the response variable by 0.05 and 0.95, respectively and second to take the logit transform of the resulting response variable. Last we fit these data using iterative re-weighted least squares method. </p>

<p>Then we are asked to use 0.005 and 0.995 instead of 0.05 and 0.95. Then the resulting coefficients are quite <strong>different</strong>.</p>

<p>My question is in <code>glm</code> function, how are 0 and 1 dealt with? Are they replaced by some numbers as above? What numbers are used by default and why are they used? How sensitive is the choice of these numbers?</p>
"
"0.0898933149950989","0.0867926073205492","173588","<p>I am not able to interpret properly the Tukey Test results from a linear regression model i have built. The Tukey Test for the model i believe is to test is the model is linear. Please correct me if i am wrong. We get this Tukey result from residualPlots command in R. </p>

<p>From the Tukey Test, all the variable have a p-value that are non-significant (p>0.05) but the Tukey Test for the entire model is significant (p=000). How can i make the Tukey Test non- significant for the entire model.</p>

<p>Please let me know the approach i need to use to make the tukey test non-significant for the entire model. And also the R codes that will help me here. Thanks.</p>

<p>The data is ordinal survey data. Most of the questions are scaled from 1 to 5. However, i am treating them to be continuous to build an overall model. There are a few variables that are binary 1/0. In total there are 18 independent variables, and three additional transformed variables (i.e. squared).  The sample size is ~2000 observations.</p>
"
"0.0635641726163728","0.0613716411932216","175111","<p>I've understood that relative importance of predictors is a tricky question. Suggested methods range from very complex models to very simple variable transformations. I've understood that the brightest still debate which way to go on this matter. I'm looking for an easy but still appealing method to approach this in survival analysis (Cox regression).</p>

<p>My aim is to answer the question: which predictor is the most important one (in terms of predicting the outcome). The reason is simple: clinicians want to know which risk factor to adress first. I understand that ""important"" in clinical setting is not equal to ""important"" in the regression-world, but there is a link.</p>

<p>Should I compute the proportion of explainable log-likelihood that is explained by each variable (see Frank Harrell <a href=""http://stats.stackexchange.com/questions/155246/which-variable-relative-importance-method-to-use"">post</a>), by using:</p>

<pre><code>library(survival); library(rms)
data(lung)
S &lt;- Surv(lung$time, lung$status)
f &lt;- cph(S ~ rcs(age,4) + sex, x=TRUE, y=TRUE, data=lung)
plot(anova(f), what='proportion chisq')
</code></pre>

<p>As I understand it, its only possible to use the 'proportion chisq' for Cox models and this should suffice to convey some sense of each variables relative importance. Or should I perhaps use the default plot(anova()), which displays Wald Ï‡2 statistic minus its degrees of freedom for assessing the partial effect of each variable?</p>

<p>I would appreciate some guidance if anyone has any experience on this matter.</p>
"
"0.100503781525921","0.097037084956597","175305","<p>I don't understand when I should transform a qualitative ordinal independent variable into an unordered factor and when I should transform it into an ordered factor instead, when I'm performing a regression in <code>R</code>.
For example, let's consider these explanatory variables:</p>

<ul>
<li>day of the week (Monday,..., Sunday): this is a qualitative variable. It is also ordinal because: Monday &lt; Tuesday &lt; ... &lt; Sunday.</li>
<li>month: this is a qualitative variable. It is also ordinal because: January &lt; February &lt; ... &lt; December.</li>
<li>year: this is a qualitative variable. But it is also ordinal because there's a natural ordering between years.</li>
<li>education (with levels ""Low"",""Medium"",""High""): this is a qualitative variable. It is also ordinal because: Low &lt; Medium &lt; ... &lt; High.</li>
</ul>

<p>From a statistical point of view, these variables are all qualitative ordinal, however sometimes they are treated as unordered factors in <code>R</code>. Why? Is there a general rule which lets me know when I should treat a qualitative ordinal explanatory variable as an ordered factor and when I shouldn't in a regression problem?
Thank you in advance.</p>
"
"0.0917469804271967","0.106298800690547","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"NaN","NaN","176788","<p>I'm running a LASSO regression following this <a href=""https://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">guide</a>. I pre - processed my dependent variable using a simple power transformation to obtain a standard normal distribution. Unfortunately, this means I have NA's in my dependent variable, so I can't run LASSO using glmnet (returns: <code>Error in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian,  : 
  NA/NaN/Inf in foreign function call (arg 6)</code>. </p>

<p>Is there anyway to overcome this? </p>
"
"0.0317820863081864","0.0613716411932216","177219","<p>When I run a <code>glm</code> with binomial-family (logistic regression), R output gives me the logit-estimates, which can be transformed into probabilities using <code>plogis(logit)</code>. So using something like <code>plogis(predict(glm_fit, type = ""terms""))</code> would give me the adjusted probabilities of success for each predictor.</p>

<p>But what would be the equivalent for Poisson regression? How can I ""predict"" the adjusted incidents rates for each predictor?</p>

<p>Given this example:</p>

<pre><code>set.seed(123)
dat &lt;- data.frame(y = rpois(100, 1.5),
                  x1 = round(runif(n = 100, 30, 70)),
                  x2 = rbinom(100, size = 1, prob = .8),
                  x3 = round(abs(rnorm(n = 100, 10, 5))))

fit &lt;- glm(y ~ x1 + x2 + x3, family = poisson(), data = dat)
</code></pre>

<p>and using <code>predict.glm(fit, type = ""terms"")</code></p>

<p>I get:</p>

<pre><code>         x1          x2          x3
1 -0.023487964  0.04701003  0.02563723
2  0.052058119 -0.20041119  0.02563723
3  0.003983339  0.04701003  0.01255701
4 -0.119637524  0.04701003 -0.03322376
5  0.010851165  0.04701003 -0.00706332
6 -0.105901873 -0.20041119 -0.00706332
...
attr(,""constant"")
[1] 0.3786072
</code></pre>

<p>So, how many ""incidents"" (y-value) would I expect for each value of <code>x1</code>, holding <code>x2</code> and <code>x3</code> constant (what <code>predict</code> does, afaik)?</p>

<p><em>I'm not sure whether this question fits better into Stackoverflow or Cross Validated - please excuse if posting here was wrong!</em></p>
"
"0.118917678002113","0.0984135662610246","178160","<p>I have a run a linear mixed effects model in R to model clinical data. However, this model is heteroscedastic (as there excess zeros in the response variable).</p>

<p>I have tried transforming the data (log transform) and (sqrt). Still, neither transformation resolves the issue (see residual versus fitted value plot). I have not used Cox proportional hazards model as the data is not time-to-event data, the data measures force and there are a large number of observations have a reading of zero. I cannot exclude these readings as they are valid.</p>

<p>I have found an R package that runs Tobit regression (AER). Nevertheless, this will not accommodate the random effects in the model.
I cannot find any R packages that run Weibull mixed effects models (or gamma mixed effects models)... </p>

<p>Does anyone know if there is a package to run these type of models? (or can they suggest any alternative approach). </p>

<p>Many thanks</p>

<p>Etn</p>
"
"0.0449466574975495","0.0433963036602746","179435","<p>I have a dataset of accelerometer readings and I'm using <code>fft</code> to transform my data into frequency domain. Then, I would like to apply <code>glm</code> to find a model.</p>

<p>The problem is that <code>glm</code> does not allow the use of complex variables, and I can't just give up of the imaginary parts.</p>

<p>I'm trying to use Logistic Regression (that's why I'm using <code>glm</code>). Is there a way to do it with the complex variables?</p>
"
"0.0778498944161523","0.0751646028002829","179891","<p>I have a data frame that is consisted of 20 observations and 35 variables.</p>

<p>I want to prepare the data for partial least square regression PLS in R.</p>

<p>Many authors suggest:</p>

<p>1)Check whether the variables are normally distributed or not </p>

<p>2)log-transform variables that are not normally distributed</p>

<p>3)center data </p>

<p>4)scale data (standardize data)</p>

<p>I checked the normal distribution of the variables using Shapiro-Wilks test and then I log transformed the variables that are not normally distributed.</p>

<p>My questions are: 1) should I standardize log transformed data or the original dataset?
2) Is there any R package that pre-process data for pls?</p>
"
"0.135519271363624","0.143929256529458","181065","<p>In some previous asked questions, I was told to not delete the outliers, because they contain valuable information.</p>

<p>After testing different regression, I came to the conclusion that until now, the <code>MARS</code> regression delivers the ""best responses"".</p>

<p>I know that <code>MARS</code> is very <em>robust</em> and there is no <em>a priori</em> knowledge about the data distribution needed.</p>

<p>But there are some question which I have about the parameters.</p>

<p>I'm using the <code>earth</code> function implemented in <code>R</code></p>

<p><strong>data set:</strong>
   <a href=""http://www.filedropper.com/data_8"" rel=""nofollow"">file</a></p>

<p>So I've got 5 variables, <em>price, livingArea, area, discrete, dummy</em> and I'm trying to explain <code>price</code> using the other ones.</p>

<p><a href=""http://i.stack.imgur.com/gGf4G.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gGf4G.png"" alt=""enter image description here""></a></p>

<p>as you can see, there are some outliers and a <code>log</code> doesn't really solve the problem.
Due to the fact that <code>area</code> can be <code>null</code>, a <code>log</code> won't be a good transformation idea. </p>

<p><strong>what I do:</strong></p>

<p>Because the answers from other questions suggested to use the raw data, 
I'm running now the regression through my data without doing any changes to it.</p>

<p>so my regression formula looks like this:</p>

<pre><code>earth(price ~ ., data = data[,-1], weights = weights, penalty = -1)
</code></pre>

<p>I'm setting <code>penalty = -1</code> because I saw that doing this, the method defines more knots and also the results look better.</p>

<p>Also I tried to define the variables <code>discrete</code> and <code>dummy</code> as <code>factors</code> and use them as follows in the regression:</p>

<ol>
<li>independent</li>
<li>livingArea * discrete or livingArea : discrete and <code>dummy</code> as independent</li>
<li>the same as at <code>3.</code> but changing <code>discrete</code> with <code>dummy</code></li>
<li>livingArea * discrete * dummy </li>
</ol>

<p>I must say that I didn't expect, that a regression with this variables as factors, will return such ""bad"" results.</p>

<p><strong>what I want:</strong></p>

<p>I want to use the model in order to predict the value of new data.</p>

<pre><code>    livingArea area discrete dummy
1         87    0        7    0.5
</code></pre>

<p>The prediction of this observation should be <code>~ 330000</code>, but with what I'm doing now, I ain't coming not even close to this value.</p>

<p>I think that, having more knots increases the precision of the result.</p>

<p><strong>questions:</strong></p>

<ul>
<li>I don't really understand the parameters</li>
<li>I created my model with different values for the <code>pmethod</code>, but the result was always the same. what's the point in choosing a method when the result will be the same?</li>
<li>how could I determine if I have to set/change the values of different parameters like <code>thresh</code>, <code>minspan</code>, <code>nk</code>, etc.</li>
</ul>
"
"0.110096376512636","0.106298800690547","181237","<p>I am using the coxph function to model a Cox regression.
By using stepwise BIC selection I obtained an model with 6 variables.
One of the variables I had to transform using the logarithm to make it fulfill the proportional hazard criterion. All Variables are marked as high significant with very low p-values- What I am now confused about is the fact, that the confidence intervals of two variables are very high. Therefore I wonder how ""useful"" those variables are! Since they are selected by the algorithm I have no doubt that they are useful in a mathematical way, but what is the meaning of a variable whose 95% confidence interval is spanning over a wide range.</p>

<p><a href=""http://i.stack.imgur.com/y7jVq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y7jVq.jpg"" alt=""Cox Regression Result""></a></p>

<p>As you can see the variables log(F) and especially E have a very high confidence interval. How can E be so important for the model (there have been more than 70 variables to choose from) and still be so ""uncertainly"" determined.
I hope I was able to formulate my problem in an understandable way.</p>

<p>Thanks in advance!
Mark</p>
"
"0.150867301828271","0.168073161363204","182905","<p>I am trying to create a regression comparing resampled data from 2 sampling periods using $n=6$ samples. 
I have tried a few different models (e.g., linear, log-transform, and exponential), and have received mixed results. </p>

<p>For example, the linear models look best graphically, but because I have so few data points, each point has a strong effect on the trend and therefore points with greater distance from the mean are resulting in the model having a negative adjusted $R^2$ value. </p>

<p>An exponential model has a much improved, and positive, adjusted $R^2$ value, but graphically doesn't make as much sense as the linear model. 
Essentially, the exponential model is reacting to the dispersion of points around the linear regression line and trying to make up for the residual error of more outlier-ish points. 
However, these points would almost certainly be within reasonable distance from the mean if more points were added.</p>

<p>So I have 2 questions:</p>

<ol>
<li>Do I go with what looks more sensible graphically, or just blindly believe the adjusted $R^2$?</li>
<li>Obviously, more data points would be beneficial, but what are my options if I can't supplement with more data?</li>
</ol>

<p><strong>Examples:</strong></p>

<p>3 variables demonstrating this issue. Solid color lines are model fits and dashed lines are 95% CI. Black = linear, red = log-trasnformed, green = exponential. x-axis = 1977, y-axis = 2015. Created in R using <code>lm()</code>.</p>

<p><a href=""http://i.stack.imgur.com/Evpzp.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Evpzp.jpg"" alt=""Example1""></a> 
<a href=""http://i.stack.imgur.com/7yJrx.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7yJrx.jpg"" alt=""Example2""></a>
<a href=""http://i.stack.imgur.com/fssgU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fssgU.jpg"" alt=""Example3""></a></p>

<p>Update: </p>

<p>Background info:
I have 21 soil variables I resampled in 1977 &amp; 2015. Each of these variables had 6 sample plots shared b/w each sampling period. However, I have a number of 1977 samples &amp; 2015 samples that were only sampled once (in 1 year &amp; not the other). The soil processing was different b/w years, so the 2 years aren't directly comparable. I'm trying to 'correct' the non-resampled 1977 samples to match 2015 data (not sampled in 1977) by regressing my 6 resampled samples for each variable. I'm assuming soil has not changed in 40 years, so I'm not trying to compare soil change, but rather trying to lump all extant samples (resampled and not) for common analyses. </p>

<p><strong>My example data:</strong></p>

<pre><code>dput(soil.dat)
structure(list(plot.2015 = c(5L, 10L, 24L, 25L, 44L, 51L), Silt.2015 = c(28.88, 21.84, 22.89, 25.34, 16.96, 22.36), Sand.2015 = c(63.92, 73.67, 60.38, 69.03, 81.6, 70.18), BS.2015 = c(70, 60.4, 43.2, 41, 84.4, 54), Silt.1977 = c(44L, 38L, 40L, 42L, 50L, 36L), Sand.1977 = c(37L, 49L, 37L, 36L, 39L, 42L), BS.1977 = c(36.6, 26.48, 44.08, 36.6, 53.32, 44.08)), .Names = c(""plot.2015"", ""Silt.2015"", ""Sand.2015"", ""BS.2015"", ""Silt.1977"", ""Sand.1977"", ""BS.1977""), class = ""data.frame"", row.names = c(1L, 2L, 15L, 16L, 18L, 19L))
</code></pre>
"
"0","0.0433963036602746","183245","<p>I'm new to regression and I am trying to perform regression analysis on two time series <code>Price A</code> (x-variable) and <code>Price B</code> (y-variable). When doing LASSO regression, the R-squared score is extremely low at 0.01. Log transformations made it worse.</p>

<p>Plotting out the scatter plot, we can see 2 different clusters. How should we handle such a data set? And how do we interpret such a scatter plot?</p>

<pre><code>plt.scatter(df['priceA'], df['priceB'])
plt.xlabel('Price A')
plt.ylabel('Price B')
</code></pre>

<p><a href=""http://i.stack.imgur.com/MZM1Z.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MZM1Z.png"" alt=""enter image description here""></a></p>
"
"0.0449466574975495","0.0433963036602746","187053","<p>I am regressing actual counts of traffic against predictions using ridge regression (<code>cv.glmnet</code> in R).  The data (both predicted and actual) has a roughly exponential distribution, i.e. a few large values (which are important to predict) and many small ones.  Residuals in the model are usually proportional to the size of the target variable.</p>

<p>What is the best approach to fit such a model correctly?</p>

<p>Transform both predicted and target data beforehand (cube root, log, Box-Cox)?</p>

<p>Or is there something I can do with the estimating process that negates the need to do this - by treating errors in large values as less bad than errors in small ones?</p>
"
"NaN","NaN","188289","<p>(This is a follow up to this great answer: <a href=""http://stats.stackexchange.com/a/136597/99091"">How to perform orthogonal regression (total least squares) via PCA?</a>.)</p>

<p>I have run this:</p>

<pre><code>v    &lt;- prcomp(cbind(X,y), center=TRUE, scale=TRUE)$rotation
beta &lt;- -v[-ncol(v),ncol(v)] / v[ncol(v),ncol(v)]
</code></pre>

<p>Now I would like to know how to transform my beta back to the scale and center of my original variables. I've tried multiplying by the sd and adding the mean but I've had no luck and I'm really confused. </p>
"
"0.101929438287525","0.114815827304529","189202","<p>I would be very grateful if you could please advise on sample size....</p>

<p>I have estimated a sample size using R - library(pwr) using command
""pwr.f2.test(u = 6, v = NULL, f2 =0.02 , sig.level =0.05 , power =0.8 )""</p>

<p>This estimates a sample size of 97.</p>

<p>The above sample size is estimated for a linear regression model.
My data will be cost data, thus would be all positive data (with no zeros). To account for an all positive data set I can log transform my data to satisfy model assumptions, or alternatively I can run a gamma regression model</p>

<p>My query: if I decide to run a gamma regression model, is the above sample size sufficient? (or is there any way of calculating a sample size for a gamma regression in R?). </p>

<p>Any advice is greatly appreciated</p>

<p>Kind regards</p>

<p>Etn</p>
"
"0.0778498944161523","0.0751646028002829","189515","<p>I would like to fit the following model by ridge regression (the xs correlate strongly with one another)</p>

<p>$y = \beta_1 {x_1}^{\lambda_1} + \beta_2 {x_2}^{\lambda_2} + \beta_3 {x_3}^{\lambda_3} + \cdots$</p>

<p>My current approach is to try Box-Tidwell transformation on the $x$s followed by linear ridge regression, but <code>cv.glmnet</code> in R throws an error (<code>lm.fit: NA/NaN/Inf in x</code>.  There are definitely none of these in my data, nor any zeros; all $x\gt0$ and $y\gt0$).  If you think these errors can be solved both for my current and future data then perhaps the real question is how to do that; example data pasted below if needed.</p>

<p>Assuming Box-Tidwell isn't robust enough for the job (in the everyday sense of the word, not statistical) then what are my alternatives?  Preferably with <code>R</code> libraries?</p>

<p><em>Data that causes the error, if relevant</em></p>

<p>Y</p>

<pre><code>2.121973109561620 
1.356081828171345 
3.386240338106292 
4.191699045929254 
3.335211073713898 
3.361101880939723 
1.356081828171345 
4.861907349730742 
3.894757399692251 
1.886732223373362 
1.079580255790821 
1.079580255790821 
2.653488025839402 
2.011958073636261 
3.677721001141955 
0.685538468679168 
3.361101880939723 
2.901321785495637 
0.685538468679168 
3.894757399692251 
2.011958073636261 
0.685538468679168 
4.410650146393066 
1.079580255790821 
2.761096650525593 
0.685538468679168 
0.685538468679168 
3.997709960290063 
0.685538468679168 
1.079580255790821 
1.886732223373362 
3.361101880939723 
3.128459524780916 
1.079580255790821 
3.695194367478646 
4.410650146393066 
4.534314613554609 
2.983752390192320 
1.356081828171345 
1.886732223373362 
3.623052422197949 
4.333074550938076 
</code></pre>

<p>XS (columns are variables)</p>

<pre><code>478.666646867709005 3334.999755859999823 8875.997070310000709 18324.000000000000000 73794.968750000000000 84945.000000000000000 86130.000000000000000 97553.000000000000000 95064.997070309997071 216030.000000000000000 259379.000000000000000 
10099.663411979709053 11680.000000000000000 12200.000000000000000 7958.000000000000000 8131.000000000000000 18328.009765599999810 20007.000000000000000 15786.959960899999714 21986.997070310000709 33306.000000000000000 33565.000000000000000 
33075.661458879709244 85765.968750000000000 164516.000000000000000 236134.000000000000000 257607.000000000000000 251330.703125000000000 253966.000000000000000 267988.000000000000000 313167.997070309997071 674297.000000000000000 386713.000000000000000 
18147.663411979709053 43723.000000000000000 112686.968750000000000 232787.000000000000000 359299.000000000000000 416794.687500000000000 448409.000000000000000 456474.000000000000000 580426.997070310055278 1177783.000000000000000 806379.000000000000000 
1751.666341689709043 742.000000000000000 1373.000000000000000 3774.000000000000000 3417.997070309999799 7048.000000000000000 8601.000000000000000 12807.000000000000000 12036.997070310000709 58174.968750000000000 88478.000000000000000 
8614.666341689709952 15974.997070299999905 35111.000000000000000 31349.009765599999810 45828.960937500000000 60135.000000000000000 94882.000000000000000 126677.000000000000000 162313.997070309997071 536617.687500000000000 588591.000000000000000 
1497.666341689709043 1377.000000000000000 1679.000000000000000 1488.000000000000000 2739.000000000000000 3895.997070309999799 4282.000000000000000 6349.000000000000000 7341.997070309999799 17356.000000000000000 15809.000000000000000 
9709.666341689709952 24608.996093800000381 49844.011718800000381 83619.960937500000000 108208.000000000000000 122971.000000000000000 120834.000000000000000 129527.000000000000000 176157.997070309997071 770005.687500000000000 1558291.000000000000000 
3656.666341689708588 10260.997070299999905 9192.000000000000000 19816.000000000000000 40678.011718800000381 66365.960937500000000 83218.000000000000000 96234.000000000000000 90788.997070309997071 122521.000000000000000 68211.000000000000000 
1435.666341689709043 1039.000000000000000 1120.000000000000000 1548.000000000000000 2795.000000000000000 2819.997070309999799 1381.000000000000000 2814.000000000000000 2597.997070309999799 7300.000000000000000 4795.000000000000000 
326.666646867709005 222.000000000000000 233.000000000000000 590.999694824000017 646.000000000000000 1294.000000000000000 902.000000000000000 1876.000000000000000 1755.997070310000026 4264.000000000000000 2435.000000000000000 
15090.663411979709053 30045.000000000000000 51073.011718800000381 56378.960937500000000 74637.000000000000000 113832.000000000000000 99594.000000000000000 107876.000000000000000 92927.997070309997071 124484.000000000000000 90528.000000000000000 
6778.666341689709043 15995.997070299999905 28689.000000000000000 35889.011718800000381 39506.960937500000000 38009.000000000000000 42170.000000000000000 57875.000000000000000 51799.997070309997071 106503.000000000000000 115013.000000000000000 
728.666646867709005 4308.999511719999646 5236.997070309999799 7361.000000000000000 7402.000000000000000 7643.000000000000000 10397.000000000000000 8805.000000000000000 8702.997070310000709 16156.009765599999810 25867.000000000000000 
11485.663411979709053 29994.000000000000000 55584.011718800000381 110733.960938000003807 196418.000000000000000 224935.000000000000000 301638.000000000000000 415136.687500000000000 486633.997070309997071 1537518.000000000000000 2612288.000000000000000 
14.999997049609000 113.000003814999999 569.999938964999956 2424.999755859999823 1697.999694820000059 4809.000000000000000 4335.000000000000000 3392.996948240000165 2.000000000000000 9806.997070310000709 9400.997070310000709 
12241.996419779708958 16586.000000000000000 43257.997070299999905 51420.968750000000000 64047.000000000000000 46403.000000000000000 45279.000000000000000 56147.000000000000000 44679.006835909996880 149701.960938000003807 272353.000000000000000 
600.999944597709032 5093.999511719999646 25560.996093800000381 73356.007812500000000 132986.958007999986876 184185.000000000000000 253398.000000000000000 249111.000000000000000 259542.700195309997071 468466.000000000000000 598152.000000000000000 
2.000000894069000 2.000000000000000 3.999999046330000 261.999938965000013 664.000000000000000 1598.999389650000012 1488.000000000000000 4466.000000000000000 6187.997070309999799 8815.997070310000709 4447.000000000000000 
27280.663411979709053 60442.011718800000381 154128.953125000000000 303640.000000000000000 601715.687500000000000 953291.000000000000000 1463397.000000000000000 1965766.000000000000000 2591598.997070310171694 6003257.000000000000000 6577179.000000000000000 
12.666664034109001 73.000007629400002 223.999954223999993 757.999694824000017 1546.000000000000000 2144.000000000000000 2766.000000000000000 6687.997070309999799 11251.997070310000709 56485.011718800000381 124376.960938000003807 
24.999994188509000 291.999938965000013 1250.000000000000000 1965.999389650000012 6617.000000000000000 868.000000000000000 4410.000000000000000 132.000000000000000 3736.997070309999799 8040.994140630000402 8349.000000000000000 
18833.996419779708958 50967.997070299999905 85909.968750000000000 101534.000000000000000 93684.009765599999810 98219.960937500000000 105931.000000000000000 143515.000000000000000 144080.997070309997071 231816.000000000000000 253980.703125000000000 
181.000020891708999 698.999938964999956 648.000000000000000 557.999389648000033 803.000000000000000 1047.000000000000000 423.000000000000000 1839.000000000000000 2772.997070309999799 3955.000000000000000 2865.000000000000000 
220.666631608708997 479.000000000000000 999.999694824000017 1255.000000000000000 4805.000000000000000 1357.000000000000000 7213.997070309999799 2235.000000000000000 2047.997070310000026 2.000000000000000 2991.000000000000000 
2.000000000000000 18.999996900599999 218.999977112000010 933.999969481999983 1800.999450680000109 7763.000000000000000 9354.997070310000709 2147.000000000000000 4617.997070309999799 9389.997070310000709 2.000000000000000 
2.000000059604000 4.999999761580000 55.999992370599998 161.999980926999996 431.999954224000021 1728.999694820000059 2003.999694820000059 3507.000000000000000 4759.997070309999799 16910.994140599999810 22492.000000000000000 
475.999944597709032 2699.999450679999882 9437.000000000000000 35624.996093800000381 110005.965819999997620 230986.000000000000000 389725.000000000000000 489768.687500000000000 540855.997070310055278 689805.000000000000000 557554.000000000000000 
208.999975114708974 487.999969481999983 865.000000000000000 1298.999389650000012 1367.000000000000000 2181.000000000000000 2760.000000000000000 4338.000000000000000 4262.997070309999799 7572.994140630000402 5933.000000000000000 
13.999999910609001 258.999938965000013 700.000000000000000 4038.999450679999882 1804.000000000000000 2.000000000000000 3711.000000000000000 2.000000000000000 2893.997070309999799 9079.997070310000709 15039.000000000000000 
145.999961763708995 443.999969481999983 1364.999694820000059 3855.999755859999823 8110.000000000000000 18502.994140599999810 27661.000000000000000 57368.011718800000381 89620.958007809997071 275004.000000000000000 570516.009765999973752 
9875.999349509709646 30551.996093800000381 62530.008789100000286 125178.960938000003807 210236.000000000000000 341185.011719000001904 598850.648438000003807 965927.000000000000000 1534750.997070309938863 4654071.000000000000000 5840271.000000000000000 
7055.666341689709043 16131.997070299999905 15552.000000000000000 21378.000000000000000 51034.968750000000000 85692.000000000000000 134986.000000000000000 233489.000000000000000 280857.997070309997071 517137.687500000000000 554130.000000000000000 
184.000020891708999 1451.999877929999911 12273.999755900000309 28868.993164100000286 1043.000000000000000 10414.000000000000000 10303.000000000000000 3733.000000000000000 18091.997070310000709 30986.009765599999810 59903.011718800000381 
3310.999349509708736 5260.000000000000000 8476.997070310000709 12254.997070299999905 25941.000000000000000 63078.000000000000000 113956.937500000000000 171371.000000000000000 230371.997070309997071 522103.000000000000000 647156.687500000000000 
9667.666341689709952 20902.996093800000381 42888.011718800000381 76499.960937500000000 110618.000000000000000 175940.000000000000000 213728.000000000000000 193852.000000000000000 154408.997070309997071 322866.687500000000000 361995.000000000000000 
3845.999349509708736 22311.996093800000381 60476.008789100000286 114873.960938000003807 231065.000000000000000 433093.009765999973752 637188.687500000000000 736014.959961000015028 754746.997070310055278 1528714.000000000000000 1540701.000000000000000 
4525.999349509709646 16227.997070299999905 43726.997070299999905 75031.968750000000000 105352.000000000000000 129998.000000000000000 166115.000000000000000 219251.009766000002855 267542.997070309997071 643765.648438000003807 947828.000000000000000 
3696.999654679708783 10310.996826200000214 17780.000000000000000 20852.000000000000000 28198.009765599999810 50840.958007799999905 50278.000000000000000 87680.000000000000000 129214.997070309997071 398212.000000000000000 670668.687500000000000 
54.999992281209003 259.999938965000013 808.000000000000000 2680.999450679999882 8467.000000000000000 21471.997070299999905 32668.997070299999905 59477.968750000000000 90554.997070309997071 267148.000000000000000 571955.009765999973752 
1435.999959859708952 5265.999206540000159 15566.997070299999905 26677.000000000000000 38977.008789100000286 45958.960937500000000 53466.000000000000000 61751.000000000000000 40377.997070309997071 52698.000000000000000 63474.000000000000000 
1067.666341689709043 6814.000000000000000 30005.996093800000381 52336.011718800000381 113843.960938000003807 168087.000000000000000 227640.000000000000000 306229.000000000000000 340356.684570309997071 650659.000000000000000 878088.000000000000000 
</code></pre>
"
"0.113707048722992","0.123508045438893","190080","<p>I've created a linear regression model in R that contains the following interaction terms.</p>

<pre><code>lm.data &lt;- lm(sharer_prob ~ sympathy + trust + fear + greed, na.action=NULL, data=data)
</code></pre>

<p>Greed, Sympathy, Trust and fear are independent variables with allowable values of 0, 1, 2, or 3. The response variable is sharer_prob, which has values from 0 to 1. The model contains the following interaction terms. </p>

<pre><code>IX_greed    &lt;- data$greed * data$sharer_prob
IX_sympathy &lt;- data$sympathy * data$sharer_prob
IX_fear     &lt;- data$fear * data$sharer_prob
IX_trust    &lt;- data$trust * data$sharer_prob
</code></pre>

<p>That makes it possible for me to regress pairs of the independent variables like so:</p>

<pre><code>lmFGData = lm( data$sharer_prob ~ IX_fear * IX_greed )
lmFSData = lm( data$sharer_prob ~ IX_fear * IX_sympathy )
lmFTData = lm( data$sharer_prob ~ IX_fear * IX_trust )
lmGSData = lm( data$sharer_prob ~ IX_greed * IX_sympathy )
lmGTData = lm( data$sharer_prob ~ IX_greed * IX_trust )
lmTSData = lm( data$sharer_prob ~ IX_trust * IX_sympathy ) 
</code></pre>

<p>Unfortunately, the resulting models fail three of the four assumptions for linear regression. So I created a new model that regresses the logit of sharer_prob against the independent variables like so:</p>

<pre><code>lm.Logitdata = lm(logit(sharer_prob, , ) ~ sympathy + trust + fear + greed, 
                  na.action=NULL, data=data)
</code></pre>

<p>How do I create expressions that regress the interacting pairs of variables? </p>

<ul>
<li><p>Option A: Use the same expressions, but change the name of the<br>
objects that represent each new model?</p></li>
<li><p>Option B: Create a new dataframe containing the independent
variables and the transformed response variable, and use that in each
expression?</p></li>
<li><p>Option C: Do something else?</p></li>
</ul>
"
"0.0778498944161523","0.0751646028002829","191381","<p>I have a quasi-poisson regression model for analysing the correlation between academic prestige and bulic visibility.
I have a set of independent variables- continouse and dummies in this model.
I would like to create scatter plots which describes this model and maby indicate of an interaction effects.
I understand that poisson models are log-linear models, so I tried to plot scatter plot in which the dependent variable is in log transformation.
However,  I don't know which line to choose- should it be a linear regression line or  loess line (I'm using ggplot2 plots).
for example, in the model, the economics department seem to be more visible then the sociology department. However, in the the plots it seem to be the oposite.
<a href=""http://i.stack.imgur.com/T6XMt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T6XMt.png"" alt=""this is the plot with lm line""></a>
<a href=""http://i.stack.imgur.com/ZowTL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZowTL.png"" alt=""this is the plot with non-method line""></a></p>

<p>How can I make a reliable plot? </p>
"
"0.155699788832305","0.150329205600566","192436","<p>I'm not a stats major and I'd appreciate any help I can get.
I've got data with each point defined by a score (between 0-1) and a frequency (0-100%). At 100%, the score is most reliable, at 1% the score is VERY UNRELIABLE. A score of 1 represents a very interesting case, 0.5 is not interesting. I realistically only care about scores between 0.5-1 (0 would technically be ""interesting"", but I'm keeping things one-tailed). This score is normally distributed, with mean centred around 0.5 (at least it should be). Ideally, a score of 1 and frequency of 100% would be the most interesting case. I'm trying to rank data from most interesting to least interesting. Standard score makes sense to me in order to rank this, but I'm having some trouble computing the variance.</p>

<p>Currently, I'm assuming the mean is consistent across all frequencies (0.5), and I know the observed score, so I only need to determine variance (which I'm assuming is different at any given frequency). The issue that is complicating this is that the frequencies are NOT normally distributed. The majority of the data (>99% of the data) falls below 5% frequency (and score at lower frequencies becomes increasingly quantized. At 0.006% frequency (the lowest frequency), scores are either 0 or 1. At 0.012% frequency, scores are either 0, 0.5 or 1. etc.. At most points above 15% frequency, only one data point exists, so I'm not sure at all how to calculate the variance given a single point and only an estimate of the mean based on normal distribution.</p>

<p>Is there a way to create a ranked list from most interesting to least? Is standard score even appropriate?</p>

<p>What I've tried so far on R (data represents the entire data set, col1 is a row.name, V2 (column2) is frequency, V3 (column3) is the score): </p>

<pre><code>    &gt; head(data)
                          V2  V3
    CREB3L1      0.013793103 1.0
    MMP2         0.006896552 0.0
    PCDHB15      0.020689655 1.0
    FEZF1        0.006896552 1.0
    TRAF3IP2-AS1 0.013793103 0.5
    PP12613      0.013793103 0.5
data$half &lt;- abs(data$v3-.5);
datalp &lt;- locpoly(data$V2, data$half, bandwidth=dpill(data$V2,data$half));
Error in if (!missing(bandwidth) &amp;&amp; bandwidth &lt;= 0) stop(""'bandwidth' must be strictly positive"") : 
  missing value where TRUE/FALSE needed
dpill(data$V2, data$half)
[1] NaN 
&gt; summary(data)
       V2                 V3        
 Min.   :0.006897   Min.   :0.0000  
 1st Qu.:0.006897   1st Qu.:0.0000  
 Median :0.006897   Median :0.5000  
 Mean   :0.010443   Mean   :0.5105  
 3rd Qu.:0.013793   3rd Qu.:1.0000  
 Max.   :0.868965   Max.   :1.0000  
plot(data)
</code></pre>

<p><a href=""http://i.stack.imgur.com/uiGr4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uiGr4.jpg"" alt=""enter image description here""></a></p>

<p>Above clearly does not work as I can't generate a regression estimate using dpill. I'm assuming it is because the data is so quantized and discrete at lower frequencies? Again, I am not limited to ranking based on standard score, if anyone has any better idea or a method to transform the data to a linear scale, the goal is just to rank using a single score the most to least interesting cases (where frequency determines reliability of score).</p>
"
"0.137125558534574","0.156467598012726","195293","<p>I thought I understood this issue, but now I'm not as sure and I'd like to check with others before I proceed.</p>

<p>I have two variables, <code>X</code> and <code>Y</code>. <code>Y</code> is a ratio, and it is not bounded by 0 and 1 and is generally normally distributed. <code>X</code> is a proportion, and it is bounded by 0 and 1 (it runs from 0.0 to 0.6). When I run a linear regression of <code>Y ~ X</code> and I find out that <code>X</code> and <code>Y</code> are significantly linearly related. So far, so good.</p>

<p>But then I investigate further and I start to think that maybe <code>X</code> and <code>Y</code>'s relationship might be more curvilinear than linear. To me, it looks like the relationship of <code>X</code> and <code>Y</code> might be closer to <code>Y ~ log(X)</code>, <code>Y ~ sqrt(X)</code>, or <code>Y ~ X + X^2</code>, or something like that. I have empirical reasons to assume the relationship might be curvilinear, but not reasons to assume that any one non-linear relationship might be better than any other. </p>

<p>I have a couple of related questions from here. First, my <code>X</code> variable takes four values: 0, 0.2, 0.4, and 0.6. When I log- or square-root-transform these data, the spacing between these values distorts so that the 0 values are much further away from all the others. For lack of a better way of asking, is this what I want? I assume it isn't, because I get very different results depending on the level of distortion I accept. If this isn't what I want, how should I avoid it?</p>

<p>Second, to log-transform these data, I have to add some amount to each <code>X</code> value because you can't take the log of 0. When I add a very small amount, say 0.001, I get very substantial distortion. When I add a larger amount, say 1, I get very little distortion. Is there a ""correct"" amount to add to an <code>X</code> variable? Or is it inappropriate to add <em>anything</em> to an <code>X</code> variable in lieu of choosing an alternative transformation (e.g. cube-root) or model (e.g. logistic regression)? </p>

<p>What little I've been able to find out there on this issue leaves me feeling like I should tread carefully. For fellow R users, this code would create some data with a sort of similar structure as mine.</p>

<pre><code>X = rep(c(0, 0.2,0.4,0.6), each = 20)
Y1 = runif(20, 6, 10)
Y2 = runif(20, 6, 9.5)
Y3 = runif(20, 6, 9)
Y4 = runif(20, 6, 8.5)
Y = c(Y4, Y3, Y2, Y1)
plot(Y~X)
</code></pre>
"
"0.111237302078652","0.107400372088138","197001","<p>I am trying to follow the procedure offered by <a href=""http://www.jstor.org/stable/2082979?seq=1#page_scan_tab_contents"" rel=""nofollow"">Beck and Katz 1995</a> in a way that I also have a TSCS data with $T=100$ (time dimension) and $N=12$ (unit dimension). My data is not balanced, which means that for some time periods, not all units have observations. </p>

<p>I am using R, and I found a <code>pcse</code> package that does what I need. It calculates panel corrected standard errors which accounts for contemporaneous correlation of errors across units and unit level heteroskedasity of errors. However, the steps I have to take to calculate panel robust standard errors for this type of regression start with the need to correct for serial correlation of errors, if I understand it well. Particularly, that is what is recommended in <code>pcse</code> package documentation:</p>

<p><a href=""http://i.stack.imgur.com/GNowz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GNowz.png"" alt=""enter image description here""></a></p>

<p>So, I am lost trying to understand what I need to do. My options how I see them:</p>

<ol>
<li>Run simple OLS regression on my pooled panel data. </li>
<li><p>Test for serial correlation of error term using Durbinâ€“Watson test and examining ACF/PACF. In most cases, I will have AR(1) in errors. </p>

<ul>
<li>Either compute clustered standard errors - it should account for the fact that errors should be clustered on the unit variable. After this step, I would get robust standard errors, but I cannot use it in pcse estimation - I don't need the VCV of errors as an input for the <code>pcse</code> function, but the OLS <code>lm</code> object itself.</li>
<li>Or use Cochraneâ€“Orcutt transformation first, and then use transformed  model as an input for pcse estimation. I started doing it, but realized that after CO transformation, error term became serially independent, but had the kurtosis of 20 (normality assumption fails).</li>
</ul></li>
</ol>

<p>So, my options are not so suitable. How do you think I should approach this situation?</p>
"
"0.149071198499986","0.143929256529458","198372","<p>I have a series of single-armed trials where the outcome is a binary response. Imagine a trial where you have no control arm; you merely give 100 patients a procedure (which can be done in many different ways) and see how many are 'well' (more later) at the end of the year. There are hundreds of these trials for me to look at.</p>

<p>I believe I can meta-analyse these as big group as follows, assuming x is the number well, n is n, and they're in df.</p>

<pre><code>model &lt;- rma(measure=""PLO"", xi=x, ni=n, data=df) #PLO = logit transformed proportion (log odds)
print(res, digits=3) #This will print the log odds
predict(model, transf=transf.ilogit, digits=3) #This will back-transform with the inverse logit transformation
</code></pre>

<p>I can plot this quite nicely with:</p>

<pre><code>forest(model,transf=transf.ilogit)
</code></pre>

<p>The thing is, as alluded to, there are lots of different ways to do the procedure and lots of different classifications of whether the patient is 'well'.</p>

<p>I want to do meta-regression/MV analysis on these trials (I may have over 100) to see if the characteristics of the trial predict the outcomes significantly.</p>

<p>I've done a lot of reading e.g <a href=""http://www.metafor-project.org/doku.php/tips:regression_with_rma"" rel=""nofollow"">http://www.metafor-project.org/doku.php/tips:regression_with_rma</a> but my problem is all the examples of meta-regression seem to treat each 'row' equally, when of course they should be weighted by n.</p>

<p>I was wondering if it would be valid to supply my predictors in question merely via the mods argument and otherwise performing the analysis as I did for the meta-analysis, e.g.:</p>

<pre><code>model_2 &lt;- rma(measure=""PLO"", xi=x, ni=n, data=df, mods=~predictor1 + predictor2 + predictor3)
</code></pre>

<p>If I do I end up with something like:</p>

<pre><code>Mixed-Effects Model (k = 60; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.3651 (SE = 0.0908)
tau (square root of estimated tau^2 value):             0.6042
I^2 (residual heterogeneity / unaccounted variability): 81.40%
H^2 (unaccounted variability / sampling variability):   5.38
R^2 (amount of heterogeneity accounted for):            0.00%

Test for Residual Heterogeneity: 
QE(df = 57) = 311.1484, p-val &lt; .0001

Test of Moderators (coefficient(s) 2,3): 
QM(df = 2) = 0.2739, p-val = 0.8720

Model Results:

                      estimate      se     zval    pval    ci.lb   ci.ub     
intrcpt                  1.1155  0.2997   3.7220  0.0002   0.5281  1.7030  ***
predictor1               0.0974  0.2763   0.3525  0.7244  -0.4441  0.6390     
predictor2              -0.0818  0.2085  -0.3923  0.6949  -0.4905  0.3269  
</code></pre>

<p>1) Is this the appropriate way of doing this?</p>

<p>2) Also, when I used to do patient-level multivariate regression, my practice was to include variables in the multivariate analysis if they were significant on univariate analysis; is this standard practice for my example, too? As in should I supply them individually as single <code>mods=~predictor</code> and look for significance before including them in a model?</p>

<p>Thank you</p>
"
"0.100503781525921","0.097037084956597","203359","<p>Consider the following heteroscedastic model:
$$y_i = f(x_i, \beta) + g(x_i, \theta)\varepsilon_i, i = 1, \ldots, n, \tag{1}$$
where $f(\cdot, \beta)$ is the regression function and $g(\cdot, \theta)$
is the variance function. For simplicity, assume the errors $\{\varepsilon_i\}$ are i.i.d. with mean $0$ and variance $\sigma^2$.</p>

<p>Regarding model $(1)$, I understand (but I am not quite sure) that the <code>gls</code> function in <code>nlme</code> package can be used (at least when $f$ is linear) to implement the iteratively reweighted least squares algorithm (Carroll, Ruppert, <em>Transformation and Weighting in Regression</em>, pp. 69). When I read the manual of <code>nlme</code>, it looks to me that <code>gls</code> function restricts the forms of $g$ to a very small class of functional forms. For example, given observations $\{(y_i, x_{i1}, x_{i2}): i = 1, \ldots, n\}$, is it
possible to use <code>gls</code> to fit the following special case of $(1)$:
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \sqrt{\theta_0 + \theta_1 x_{i1}^2 + \theta_2 x_{i2}^2}\varepsilon_i, i = 1, \ldots, n$$
, where $\theta_0 &gt; 0, \theta_1 \geq 0, \theta_2 \geq 0$? If yes, how should I specify my own square-root variance functional form in <code>gls</code>? If
no, are there any other available R packages to implement IRLS algorithm?</p>
"
"0.0635641726163728","0.0613716411932216","205918","<p>My data follows a sigmoidal function of the form<br>
$$y=asym/(1+e^{(xmid-x)/scale)})$$<br>
I have taken the function from the SSLogis function in R.<br>
My supervisor and I think that there is a second variable that influences the asymptote and scale of the function.  </p>

<p>So I'm stuck with the questions:<br>
1. Is it at all possible to do a multiple regression based on this function?<br>
If yes, 2. Is it possible to transform the sigmoidal to a linear function and use it in a multiple linear regression?<br>
Or 3. Is there a way to do a multiple non-linear regression?</p>

<p>Any help is really welcome.</p>
"
"0.100503781525921","0.097037084956597","207999","<p>I am working on a project to predict a range for patient length of stay.  My data consists of 215,000 rows of the following variables (30 total):</p>

<ul>
<li><code>LOS</code> (length of stay in days)</li>
<li><code>AGE</code> (in years)</li>
<li><code>GENDER</code> </li>
<li><code>MARITAL</code></li>
<li><code>DIAGNOSIS 1</code></li>
<li><code>DIAGNOSIS 2</code></li>
<li><code>DIAGNOSIS 3</code></li>
<li>... and so on</li>
</ul>

<p>With the exception of <code>AGE</code> and <code>LOS</code>, all the variables are binary.  The distribution for <code>LOS</code> is heavily skewed - almost all values are between 1-30, with extreme outliers from 50-370 that account for only 0.02% of the data.</p>

<p>My approach to modeling the relationship between <code>LOS</code> and the rest of the variables is as follows.  First, remove the 0.02% outliers for the dependent variable.  Second, do a simple log transform of the dependent variable.  After taking these two steps, the <code>LOS</code> data is normally distributed.  </p>

<p>My question is - is there any reason why I should not simply use plain old multivariate linear regression on this normalized <code>LOS</code> data?  </p>

<p>When I do this, I get highly significant p-values and an R-squared of 0.207.  Which, as I understand it, isn't horrible for complex health care data (please correct me if I am wrong).  This approach also results in nicely distributed residuals.</p>

<p>However, I was looking up different data distributions to see if I should be modeling in a different way.  Other length of stay models on the internet treat the data as a Poisson distribution, which led me here to inquire and hopefully acquire a greater understanding of how to treat this data!  </p>

<p>So, is my methodology sound in this case?  Any and all feedback is greatly appreciated!</p>
"
"NaN","NaN","208367","<p>I'm intended to run a linear regression model (Rain~dBZ) for my data set.</p>

<p>I would like to know how to transform non-normal set of ""Rain"" column in to a normal distribution.</p>

<p>I would really appreciate it to have your kindly assistant.</p>

<p>I attach the data-set in the following link:</p>

<p><a href=""https://drive.google.com/file/d/0B7aMnS118Vltd19UdThrNVVVcDg/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B7aMnS118Vltd19UdThrNVVVcDg/view?usp=sharing</a></p>
"
"0.100503781525921","0.097037084956597","209421","<p>If I have theoretical reasons to suppose the data might be fit with an unusual equation such as the following:</p>

<p>$$Y_i = (\beta_0 + \beta_1x_{1i} +  \beta_2x_{2i} + \epsilon_i)^{\beta_3}$$</p>

<p>Can I use Ordinary Least Squares Multiple Linear Regression after a transformation to estimate parameters $\beta{_0,_1,_2,_3}$? If yes, what transformation?</p>

<p>If not, is there some specialized package in R (and brief reading) that might help me compare the fit and residuals from this model against a more typical MLR model?</p>

<p>Thanks.</p>

<p>Example Code:</p>

<pre><code>## while I can run ""nls,"" I cannot get $\epsilon$ inside parentheses nor
## can I have four BETAs

var1 &lt;- rnorm(50, 100, 1)
var2 &lt;- rnorm(50, 120, 2)
var3 &lt;- rnorm(50, 500, 5)

## make a model without $\beta_1$ and $\beta_2$ and with $\epsilon_i$ on outside
nls(var3 ~ (a + var1 + var2)^b, start = list(a = 0.12345, b = 0.54321))

Nonlinear regression model
  model: var3 ~ (a + var1 + var2)^b
  data: parent.frame()
   a        b 
 475.5234   0.9497 
 residual sum-of-squares: 1365

Number of iterations to convergence: 6 
Achieved convergence tolerance: 8.332e-08

## FAILS with exponent on left-hand side and $\epsilon$ inside parentheses
nls(var3^(1/b) ~ (a + var1 + var2), start = list(a = 0.12345, b = 0.54321))
Error in eval(expr, envir, enclos) : object 'b' not found

## FAILS with all BETAs
nls(var3 ~ (a + b*var1 + c*var2)^d, start = list(a = 4, b = 1, c = 1, d = 1))
Error in numericDeriv(form[[3L]], names(ind), env) : 
Missing value or an infinity produced when evaluating the model
</code></pre>
"
"0.100503781525921","0.097037084956597","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.0778498944161523","0.0501097352001886","213011","<p>In the <code>car</code> package, we have the function <code>powerTransform</code> which transforms variables in a regression equation to make the residuals in the transformed equation as normal as possible. I am confused about what this transformation is and further in the following example:</p>

<pre><code># Box Cox Method, univariate
summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))

# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>What I am confused about is what exactly the model <code>p1</code> is. Is it simply the linear model without a transformation, then it finds the optimal parameter, we then use that to specify <code>m1</code>? So what is the regression equation for <code>p1</code>, <code>m1</code>??</p>
"
"0.0449466574975495","0.0433963036602746","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"0.101929438287525","0.0984135662610246","218483","<p><strong>I have the TPM (transcripts per million) values generated for my RNAseq data. My overall goal is to identify genes that show allele specific expression differences.</strong></p>

<p>In the table below the TPM values (gene.erc.M and gene.erc.S) are for two different haplotypes (flagged as M and S) and the total TPM for that locus (gene or id) is T, so reads for M+S = reads for T. <strong>See the data table and description below for more details.</strong></p>

<p><strong>Here is my data structure:</strong></p>

<pre><code>gene_id_locus   gene.erc.M  gene.erc.S  gene.erc.T
Al_scaffold_0001_1044   713 314 1027
Al_scaffold_0001_1048   774 483 1257
Al_scaffold_0001_1062   193 199 392
Al_scaffold_0001_1084   8   50  58
Al_scaffold_0001_1095   7   9   16
Al_scaffold_0001_1106   392 8   400
Al_scaffold_0001_1120   253 62  315
Al_scaffold_0001_1122   38  38  76
Al_scaffold_0001_1125   311 135 446
Al_scaffold_0001_1133   164 71  235
Al_scaffold_0001_1146   364 302 666
Al_scaffold_0001_1151   265 51  316
Al_scaffold_0001_1171   34  37  71
Al_scaffold_0001_1186   26  39  65
Al_scaffold_0001_1189   170 208 378
Al_scaffold_0001_1195   551 158 709
Al_scaffold_0001_1209   91  56  147
Al_scaffold_0001_1233   81  46  127
Al_scaffold_0001_1245   121 19  140
Al_scaffold_0001_125    378 153 531
Al_scaffold_0001_1271   138 95  233
Al_scaffold_0001_1275   35  150 185
Al_scaffold_0001_128    73  2115    2188
Al_scaffold_0001_1280   67  149 216
Al_scaffold_0001_1283   28  67  95
Al_scaffold_0001_1304   12  12  25
Al_scaffold_0001_1307   48  16  64
Al_scaffold_0001_1326   248 73  321
#contd.......
</code></pre>

<p>There are about 16000 gene_id (after filtering), thats why there is going to be a some good amount of over dispersion.</p>

<p><strong>Description:</strong>  </p>

<ul>
<li><code>gene_locus_id</code> - is the gene name</li>
<li><code>gene.erc.T</code> - represent the total number of transcripts from the  - <code>corresponding gene_locus_id.</code> This values is different than RPKM, FPKM, so I don't have to worry about normalization any more.</li>
<li><code>gene.erc.M</code> - represent the total number of transcripts from the corresponding <code>gene_locus_id</code> but with haplotype M.</li>
<li><code>gene.erc.S</code> - same as above but for haplotype S.</li>
</ul>

<p><strong>This is what I want to do specifially:</strong>  </p>

<ol>
<li><p>Find the appropriate distribution of my data (for M, S and T); poisson vs. negative binomial? Generally data from RNAseq have over-dispersion so negative binomial regression might be appropriate, but I want to check for dispersion and distribution for the data either way?</p></li>
<li><p>If any transformation is needed? - I have tried some log transformation but I am not getting anything useful.  </p>

<p><a href=""http://i.stack.imgur.com/beU6N.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/beU6N.png"" alt=""log of (erc.gene.M/erc.gene.S)""></a>  </p>

<p><a href=""http://i.stack.imgur.com/EFCDs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EFCDs.png"" alt=""gene.erc.T vs. (log(erc.M/S) * sqrt(gene.erc.T))""></a></p></li>
<li><p>My main goal is to see if there is significant expression differences between two haplotypes (M vs. S) for the same gene ID. - I tried looking for similar examples around but not finding anythig useful. DeSeq, edgeR mainly focus on variation between samples and condition. I tried to apply those but looks like I am having brain freeze in here.</p></li>
</ol>
"
"0.179786629990198","0.173585214641098","220868","<p>The goal of this regression is to determine whether the amount of leaf disk that an insect consumed varied by what tree the leaf material came from. I'll acknowledge upfront that my coding is rarely pretty/efficient, but hopefully it works (usually).</p>

<ul>
<li>Variables:

<ul>
<li>Response: pctrans; the percent of a 7 mm diameter leaf disk that was consumed.  Values have been transformed to fit (0,1).</li>
<li>Explanatory: tree; a categorical (factor) variable of six tree types.</li>
</ul></li>
</ul>

<p>When I use betareg(), which as I understand it, is best suited to data of this sort, I get no significance:</p>

<pre><code>model.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
modelnull.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
lrtest(model1.beta, modelnull.beta)
</code></pre>

<p>Results:</p>

<pre><code>Call:
betareg(formula = pctrans ~ tree, data = BT.data, link = ""logit"")

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.7716 -0.5800  0.0472  0.5351  3.5109 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.111504   0.069191 -16.064  &lt; 2e-16 ***
treeBC3F3   -0.050940   0.095889  -0.531  0.59525    
treeD54     -0.279927   0.096470  -2.902  0.00371 ** 
treeD58     -0.034000   0.095716  -0.355  0.72242    
treeEllis1  -0.006764   0.095175  -0.071  0.94334    
treeQing     0.785992   0.094003   8.361  &lt; 2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi)   3.5549     0.1352   26.29   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 529.8 on 7 Df
Pseudo R-squared: 0.1105
Number of iterations: 20 (BFGS) + 2 (Fisher scoring) 

Likelihood ratio test

Model 1: pctrans ~ tree
Model 2: pctrans ~ 1
  #Df LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 529.82                         
2   2 460.70 -5 138.25  &lt; 2.2e-16 ***
</code></pre>

<p>As I've been told, since the model is significantly worse than the null, no comparisons can be made between treatment means.</p>

<p>HOWEVER...
If I run the same model using glm the model is significantly better than the null.</p>

<pre><code>beta.glm &lt;- glm(pctrans ~ tree, data=BT.data, family=quasibinomial)
</code></pre>

<p>Results:</p>

<pre><code>Call:
glm(formula = pctrans ~ tree, family = quasibinomial, data = BT.data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.94474  -0.38492  -0.08785   0.22725   1.80291  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.22601    0.07643 -16.042  &lt; 2e-16 ***
treeBC3F3    0.06826    0.10660   0.640  0.52205    
treeD54     -0.33864    0.11312  -2.994  0.00281 ** 
treeD58     -0.19878    0.11062  -1.797  0.07260 .  
treeEllis1  -0.07763    0.10808  -0.718  0.47276    
treeQing     0.88596    0.09978   8.879  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2069603)

    Null deviance: 307.54  on 1240  degrees of freedom
Residual deviance: 267.59  on 1235  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4

Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: pctrans
Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                  1240     307.54              
tree  5   39.951      1235     267.59 &lt; 2.2e-16 ***
</code></pre>

<p>Where do I go from here?</p>
"
"NaN","NaN","223236","<p>I am using logistic regression to predict binary outcomes with 5 features. When putting 20x weight on the 0.001% outliers the peformance gets a lot better.</p>

<p>It seems that some really high/low values in the features are predictive.</p>

<p>What are some suggested methods here to further improve? (e.g. transformation, finding the best weight, filtering the data with robust PCA to remove noise)</p>
"
"0.100503781525921","0.097037084956597","224310","<p>This is a two part question concerning linear regression in R. Here is my code and what my residual plot looks like before transformation:</p>

<pre><code>linear_model &lt;- lm(dependent ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p>[I'll add image, as soon as I've got enough reputation to post three links.]</p>

<p>Obviously there is a pattern, although I find it hard to describe. In any case I would think there is evidence for a non-linear relationship. The only function I can think of is $sine$. So I applied the $sine$ function on the dependent variable, played with it a bit and ended up with a shift of $1/2*\pi$ (so the same as cosinus). This what my code and residual plot look like after transformation:</p>

<pre><code>linear_model &lt;- lm(sin((1/2)*pi+dependent) ~ ., data=independent)
residuals &lt;- resid(linear_model)
plot(residuals)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pJ91J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pJ91J.png"" alt=""enter image description here""></a></p>

<p><strong>Question 1</strong>
Is the application of $sine$ on the dependent variable legitimate?</p>

<p><strong>Question 2</strong>
Although there still is a pattern - maybe even more distinct than before - the residuals are much lower. Does that indicate that the transformation was the right thing to do? And should I try to get rid of the still existing pattern?</p>

<p>Finally, the model's adjusted RÂ² value increased from .13 to .3.</p>

<p><strong>Edit:</strong> As a reaction to the comments, some more information on the dataset. The data has been recorded over a time span of <s>two</s> four weeks, 18 hours each day with intervals between roughly 2-10 minutes. Here is the autocorrelation function estimate of the dependent variable:</p>

<pre><code>acf(dependent)
</code></pre>

<p><a href=""http://i.stack.imgur.com/A0S1D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/A0S1D.png"" alt=""enter image description here""></a></p>

<p>Apparently, the answer to question 1 is <em>No</em>.</p>
"
"0.134839972492648","0.115723476427399","224377","<p>I am runnning a Random Coefficient Mixed Model in <code>R</code> using <code>lme</code> in <code>{lme4}</code>. I had to transform my dependent variable by square-root because of problems of uniqual variance of the errors. However, with this formulation of the DV, the interpretation of my coefficients' predictors becames quite tricky.</p>

<p>My sample counts 20,000 observations.</p>

<p>Originally I have thought of switching to Non-Linear Mixed models, but in <em>stackoverflow.com</em> someone suggested that ""fitting a variance structure with the weights parameter"" could be a valid alternative to the use of Non-Linear Mixed Model.</p>

<p>I have thus tried fitting a regression of the kind below with <code>lme</code> <code>{lme4}</code> in <code>R</code>,</p>

<p><strong>note that</strong> part of the heteroskedasticity takes place between groups, the random coefficient model improves the structure of the errors when taking into account for the province and district levels, however the non-normality of the DV causes the errors' distribution to be non-normal too.
The square root transformation makes the DV approximate a normal almost perfectly- see at the end of the post.</p>

<p><code>Model2 &lt;-
  lme(
       fcs ~ hh_size + head_sex + head_age + head_edu + residence_code + head_marital_status + ...,
       random = list(
       dist_code_unique = ~ 1 + some vars,
       prov_code = ~ 1
       ),
       weights =~ fcs_sqrt,
       data = data
  )</code>
where fcs is my original dependent variable, and fcs_sqrt is the square root transformation of it.</p>

<p>The result using and not using weights in terms of standardize residuals is shown in the two graphs below.</p>

<p><strong>The question is</strong>: Am I allowed to give the weights in this manner? Are there any implications for the interpretation of the results?</p>

<p><a href=""http://i.stack.imgur.com/svIQo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/svIQo.png"" alt=""No weights used""></a>
<a href=""http://i.stack.imgur.com/h4iZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/h4iZh.png"" alt=""Weights = fcs_sqrt""></a></p>

<p><a href=""http://i.stack.imgur.com/xv6Ve.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xv6Ve.png"" alt=""enter image description here""></a></p>
"
"NaN","NaN","228184","<p>Let me preface this by saying I'm new to statistics. </p>

<p>I'm working with regression models, attempting to understand transformations a bit more. I'm modeling (Y~X) and I get an $R^2$ of 0.4. I see that the residuals of this plot are left skewed so I take (Y^2~X) assuming that would correct the issue but now my $R^2$ is 0.3. Just out of curiosity, I did (Log(Y)~X and got an $R^2$ of 0.5.</p>

<p>I'm really not sure what is going on and not sure what transformation I should use going forward. </p>
"
"0.134839972492648","0.130188910980824","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0898933149950989","0.0867926073205492","230257","<p>I am teaching myself regression using Regression Modeling Strategies by Harell and the author goes at quite the length to showcase the importance of modeling interactions and transformations of the initial variables. I can't help but wonder how to approach this in a more structured/automated way when dealing with a lot of potential variables. </p>

<p>Can we use recursive partitioning, for example, to somehow to do the work for us and then use the output as variables, shrink the estimates with LASSO to deal with colinearity and do a final step where we use some sort of filtering for feature importance. </p>

<p>In my mind this will leave us with a well specified model which can be manually inspected and improved if need be, but is this reasonable? Are there other ways to approach this? Are there some resources that deal with problems like this? </p>
"
"0.0778498944161523","0.0751646028002829","230303","<p>I am teaching myself regression using Regression Modeling Strategies by Harell and the author goes at quite the length to showcase the importance of modeling interactions and transformations of the initial variables. I can't help but wonder how to approach this in a more structured/automated way when dealing with a lot of potential variables.</p>

<p>Can we use recursive partitioning, for example, to somehow to do the work for us and then use the output as variables, shrink the estimates with LASSO to deal with colinearity and do a final step where we use some sort of filtering for feature importance.</p>

<p>In my mind this will leave us with a well specified model which can be manually inspected and improved if need be, but is this reasonable? Are there other ways to approach this? Are there some resources that deal with problems like this?</p>
"
"0.0603022689155527","0.097037084956597","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.127128345232746","0.122743282386443","235272","<p>Given a multinomial logistic regression model with 4 independent variables, 4 relevant interactions and a dependent variable with 3 categorical outcomes, I wanted to test for linearity of the logit.</p>

<p>R told me, it is always a good idea to scale the independent variables to the range [0,1], so I did.</p>

<p>So when I wanted to test for linearity of the logit by including the interactions between each predictor and its natural log in the model, I found that two of them were significant, so I had to reject the hypothesis of linearity of the logit.</p>

<p>However, when I ran the same model without scaling my predictors to [0,1] (the original range is [0,1500]) p-values for the log interactions were > 0.8 suggesting that I don't have to reject the linearity of the logit assumption.</p>

<p>When I looked at the transforms in the different ranges, it made sense why the outcome would be different:</p>

<p><a href=""http://i.stack.imgur.com/WFEbW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WFEbW.png"" alt=""log(x)*x[0,1]""></a></p>

<p><a href=""http://i.stack.imgur.com/PiPOE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PiPOE.png"" alt=""log(x)*x[0,1000]""></a></p>

<p>So my question is, does the Box Tidwell test for linearity of the logit require predictors to be in the range [0,1]? If so, why is it so hard to find any mention of this on the internet? If not, what is a valid range for the test? Because the test-results obviously depend on the range.</p>

<p>Thank you very much for your help.</p>
"
