"V1","V2","V3","V4"
"NaN","NaN","  2234","<p>I would like as many algorithms that perform the same task as logistic regression.  That is  algorithms/models that can give a prediction to a binary response (Y) with some explanatory variable (X).</p>

<p>I would be glad if after you name the algorithm, if you would also show how to implement it in R.  Here is a code that can be updated with other models:</p>

<pre><code>set.seed(55)
n &lt;- 100
x &lt;- c(rnorm(n), 1+rnorm(n))
y &lt;- c(rep(0,n), rep(1,n))
r &lt;- glm(y~x, family=binomial)
plot(y~x)
abline(lm(y~x),col='red',lty=2)
xx &lt;- seq(min(x), max(x), length=100)
yy &lt;- predict(r, data.frame(x=xx), type='response')
lines(xx,yy, col='blue', lwd=5, lty=2)
title(main='Logistic regression with the ""glm"" function')
</code></pre>
"
"NaN","NaN","  2335","<p>I'm trying to interpret the following type of logistic model:</p>

<pre><code>mdl &lt;- glm(c(suc,fail) ~ fac1 + fac2, data=df, family=binomial)
</code></pre>

<p>Is the output of <code>predict(mdl)</code> the expected odds of success for each data point? Is there a simple way to tabulate the odds for each factor level of the model, rather than all the data points?</p>
"
"0.112815214963553","0.107476300250388","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.0953462589245592","0.090834052439095","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.104446593573419","0.0995037190209989","  6505","<p>Suppose I am going to do a univariate logistic regression on several independent variables, like this:</p>

<pre><code>mod.a &lt;- glm(x ~ a, data=z, family=binominal(""logistic""))
mod.b &lt;- glm(x ~ b, data=z, family=binominal(""logistic""))
</code></pre>

<p>I did a model comparison (likelihood ratio test) to see if the model is better than the null model by this command</p>

<pre><code>1-pchisq(mod.a$null.deviance-mod.a$deviance, mod.a$df.null-mod.a$df.residual)
</code></pre>

<p>Then I built another model with all variables in it</p>

<pre><code>mod.c &lt;- glm(x ~ a+b, data=z, family=binomial(""logistic""))
</code></pre>

<p>In order to see if the variable is statistically significant in the multivariate model, I used the <code>lrtest</code> command from <code>epicalc</code></p>

<pre><code>lrtest(mod.c,mod.a) ### see if variable b is statistically significant after adjustment of a
lrtest(mod.c,mod.b) ### see if variable a is statistically significant after adjustment of b
</code></pre>

<p>I wonder if the <code>pchisq</code> method and the <code>lrtest</code> method are equivalent for doing loglikelihood test? As I dunno how to use <code>lrtest</code> for univate logistic model.</p>
"
"0.0738548945875996","0.0703597544730292","  6562","<p>I have read an <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">article</a> from Christopher Manning, and saw an interesting code for collapsing categorical variable in an logistic regression model:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class == 1), family=binomial(""logit""))
</code></pre>

<p>Does the <code>I(class == 1)</code> means that the <code>class</code> variable has been recoded into either it is <code>1</code> or it is not <code>1</code>?</p>

<p>After that, I am thinking of modifying it a bit:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class %in% C(1,2)), family=binomial(""logit""))
</code></pre>

<p>I am planning to merge the variable <code>class</code> from <code>c(1,2,3,4)</code> into two groups, one group contains <code>c(1,2)</code>, another group contains <code>c(3,4)</code>, can the code above give me the result I want?</p>

<p>Thanks.</p>
"
"NaN","NaN","  8303","<p>I am fitting a binomial family glm in R, and I have a whole troupe of explanatory variables, and I need to find the best (R-squared as a measure is fine). Short of writing a script to loop through random different combinations of the explanatory variables and then recording which performs the best, I really dont know what to do. And the <code>leaps</code> function from package <strong><a href=""http://cran.r-project.org/web/packages/leaps/index.html"">leaps</a></strong> does not seem to do logistic regression.</p>

<p>Any help or suggestions would be greatly appreciated
Leendert   </p>
"
"0.120604537831105","0.10053487318375","  8511","<p>Christopher Manning's <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">writeup on logistic regression in R</a> shows a logistic regression in R as follows:</p>

<pre><code>ced.logr &lt;- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)
</code></pre>

<p>Some output:</p>

<pre><code>&gt; summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -1.31827    0.12221 -10.787 &lt; 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 &lt; 2e-16
followsV       0.53408    0.05660   9.436 &lt; 2e-16
factor(class)2 1.27045    0.10320  12.310 &lt; 2e-16
factor(class)3 1.04805    0.10355  10.122 &lt; 2e-16
factor(class)4 1.37425    0.10155  13.532 &lt; 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4
</code></pre>

<p>He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.</p>

<p>However, how much variance does the model account for?  A <a href=""http://www.ats.ucla.edu/stat/stata/output/old/lognoframe.htm"" rel=""nofollow"">Stata page on logistic regression</a> says:</p>

<blockquote>
  <p>Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the ""constant-only"" model and L1 is the log likelihood for the full model with constant and predictors. </p>
</blockquote>

<p>I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.</p>

<p>Can someone verify how one actually computes the pseudo-R^2 in R using this example?</p>
"
"0.0738548945875996","0.0469065029820195","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.113960576459638","0.108567458176987"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0572077553547355","0.054500431463457"," 11178","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio"">Logistic Regression in R (Odds Ratio)</a>  </p>
</blockquote>



<p>I need to do a logistic regression in R. My response variable is <code>surv=0</code>; <code>surv=1</code> and I have about 18 predictor variables.</p>

<p>After reading my model, I got the table of Coefficients below and I need to go through some steps, which I am not familiar with, until I get to the odds ratios.</p>

<p>This is my first time to do a logistic regression in R and your help would be appreciated.</p>

<pre><code>Call:
glm(formula = surv ~ as.factor(tdate) + as.factor(line) + as.factor(wt) + 
    as.factor(crump) + as.factor(pind) + as.factor(pcscore) + 
    as.factor(ptem) + as.factor(pshiv) + as.factor(pincis) + 
    as.factor(presp) + as.factor(pmtone) + as.factor(pscolor) + 
    as.factor(ppscore) + as.factor(pmstain) + as.factor(pbse) + 
    as.factor(psex) + as.factor(pgf), family = binomial(link = ""logit""), 
    data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9772  -0.5896  -0.4419  -0.3154   2.8264  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.59796    0.27024  -2.213 0.026918 *  
as.factor(tdate)2009-09-08  0.43918    0.19876   2.210 0.027130 *  
as.factor(tdate)2009-09-11  0.27613    0.20289   1.361 0.173514    
as.factor(tdate)2009-09-15  0.58733    0.19232   3.054 0.002259 ** 
as.factor(tdate)2009-09-18  0.52823    0.20605   2.564 0.010360 *  
as.factor(tdate)2009-09-22  0.45661    0.19929   2.291 0.021954 *  
as.factor(tdate)2009-09-25 -0.09189    0.21740  -0.423 0.672526    
as.factor(tdate)2009-09-29 -0.15696    0.28369  -0.553 0.580076    
as.factor(tdate)2010-01-26  1.39260    0.21049   6.616 3.69e-11 ***
as.factor(tdate)2010-01-29  1.67827    0.21099   7.954 1.80e-15 ***
as.factor(tdate)2010-02-02  1.35442    0.21292   6.361 2.00e-10 ***
as.factor(tdate)2010-02-05  1.36856    0.21439   6.383 1.73e-10 ***
as.factor(tdate)2010-02-09  1.18159    0.21951   5.383 7.33e-08 ***
as.factor(tdate)2010-02-12  1.40457    0.22001   6.384 1.73e-10 ***
as.factor(tdate)2010-02-16  1.01063    0.21783   4.639 3.49e-06 ***
as.factor(tdate)2010-02-19  1.54992    0.21535   7.197 6.14e-13 ***
as.factor(tdate)2010-02-23  0.85695    0.33968   2.523 0.011641 *  
as.factor(line)2           -0.26311    0.07257  -3.625 0.000288 ***
as.factor(line)5            0.06766    0.11162   0.606 0.544387    
as.factor(line)6           -0.30409    0.12130  -2.507 0.012176 *  
as.factor(wt)2             -0.33904    0.10708  -3.166 0.001544 ** 
as.factor(wt)3             -0.28976    0.13217  -2.192 0.028359 *  
as.factor(wt)4             -0.50470    0.16264  -3.103 0.001915 ** 
as.factor(wt)5             -0.74870    0.20067  -3.731 0.000191 ***
as.factor(crump)2           0.07537    0.10751   0.701 0.483280    
as.factor(crump)3          -0.14050    0.13217  -1.063 0.287768    
as.factor(crump)4          -0.20131    0.16689  -1.206 0.227724    
as.factor(crump)5          -0.23963    0.20778  -1.153 0.248803    
as.factor(pind)2           -0.29893    0.10752  -2.780 0.005434 ** 
as.factor(pind)3           -0.40828    0.12436  -3.283 0.001027 ** 
as.factor(pind)4           -0.73021    0.14947  -4.885 1.03e-06 ***
as.factor(pind)5           -0.68878    0.17650  -3.902 9.52e-05 ***
as.factor(pcscore)2        -0.52667    0.13606  -3.871 0.000108 ***
as.factor(ptem)2           -0.72600    0.08964  -8.099 5.52e-16 ***
as.factor(ptem)3           -0.79145    0.10503  -7.536 4.86e-14 ***
as.factor(ptem)4           -0.89956    0.10331  -8.707  &lt; 2e-16 ***
as.factor(ptem)5           -0.90181    0.10721  -8.412  &lt; 2e-16 ***
as.factor(pshiv)2           0.25236    0.07713   3.272 0.001068 ** 
as.factor(pincis)2          0.02327    0.07216   0.323 0.747041    
as.factor(presp)2           0.43746    0.11598   3.772 0.000162 ***
as.factor(pmtone)2          0.34515    0.11178   3.088 0.002016 ** 
as.factor(pscolor)2         0.53469    0.26851   1.991 0.046443 *  
as.factor(ppscore)2         0.25664    0.08751   2.933 0.003361 ** 
as.factor(pmstain)2        -0.48619    0.84408  -0.576 0.564611    
as.factor(pbse)2           -0.28248    0.07335  -3.851 0.000117 ***
as.factor(psex)2           -0.18240    0.06385  -2.857 0.004280 ** 
as.factor(pgf)12            0.10329    0.14314   0.722 0.470554    
as.factor(pgf)21           -0.06481    0.10772  -0.602 0.547388    
as.factor(pgf)22            0.39584    0.12740   3.107 0.001890 ** 
as.factor(pgf)31            0.18820    0.10082   1.867 0.061936 .  
as.factor(pgf)32            0.39662    0.13963   2.841 0.004504 ** 
as.factor(pgf)41            0.09178    0.10413   0.881 0.378106    
as.factor(pgf)42            0.21056    0.14906   1.413 0.157787    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7812.9  on 8714  degrees of freedom
Residual deviance: 6797.4  on 8662  degrees of freedom
  (418 observations deleted due to missingness)
AIC: 6903.4

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.104446593573419","0.0829197658508324"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.0426401432711221","0.0406222231851194"," 12219","<p>My logistic regression model seems to identify successes very well (about 85% - 94%), but fails to identify the failures (only identifying 18% - 32% correctly). </p>

<p>I have thought of weighting the success and failures differently, but I am not sure of how to do this R. I have tried:</p>

<pre><code>Fr &lt;- ifelse (y==0, 2, 1)
model &lt;- glm(y~factor(x1) + factor(x2), weights=Fr, 
                                        family = ""binomial"", 
                                        data = data)
</code></pre>

<p>but did not get any improvement this way.</p>
"
"0.112815214963553","0.107476300250388"," 12319","<p>Background: Iâ€™m analyzing data with mixed-models (lmer in lme4) from an experiment that had RTs and Error Rates as dependent variables. This is a repeated-measures design with approximately 300 measurements for each of the 190 human subjects. The fixed-effects are 1 between-subjects experimental manipulation (dichotomous), 2 within-subjects experimental manipulations (both dichotomous), and 1 subject variable (continuous, centered). My uncorrelated random effects are the participants, and 2 stimulus characteristics.  For the mixed-models, Iâ€™ve coded the experimental manipulations as a -.5/+.5 contrasts so that the parameters are estimates of the experimental effects and the intercept should be the grand mean.</p>

<p>The grand mean produced by the RT model (740 ms) does not match the mean I get if I average all of the individual trials (730 ms). Why does this happen?</p>

<p>A related question: the GLMM (binomial distribution, logistic link function) for error rates produces a parameter estimate  with an associated Z-score that has an absolute value over 2, but when I look at the means (determined the same way as above) to examine this difference they are tiny and almost identical (0.01353835 vs. 0.01354846). What are the values that I can provide that support the reliable parameter estimate? </p>

<p>I have a feeling the discrepancy between my calculated means and the model estimates has something to do with the random factors (perhaps the grouping by subjects), but Iâ€™m not sure exactly what.</p>

<p>If I want to display descriptive statistics along with the table of mixed model estimates, how should these descriptive be determined? Any points to references, examples, etc. will be greatly appreciated.</p>

<p>If this is all just a brain fart on my part, please let me know that too.</p>

<p>Edit: It is probably also important to mention that the amount of trials and types of trials contributed are not the same for every person. The between-subject manipulation changes the proportions of the different trial types presented, and for RTs only correct trials were analyzed. There were, however, very few errors made.</p>
"
"0.0603022689155527","0.0574484989621426"," 14546","<p>I have run the following logistic regression:</p>

<pre><code>glm(formula = DecisionasReceiver ~ L1 + L2 + L3, 
  family = binomial(""logit""), data = lue)
</code></pre>

<p>where L1 L2 and L3 code for differences in condition of no.GREEN.
L1: 1,-1,0,0 : is there a difference in DecisionasReceiver as no.GREEN changes from 1 to 2?</p>

<p>L2: 0,1,-1,0: is there a difference in DecisionasReceiver as no.GREEN changes from 2 to 3?</p>

<p>L2: 0,0,1,-1: is there a difference in DecisionasReceiver as no.GREEN changes from 3 to 4?</p>

<p>And I'm running this regression both for the cases where MessageReceived is BLUE and for MessageReceived RED. </p>

<p>I have the following output:</p>

<pre><code>   Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9535     0.3659  -2.606  0.00916 ** 
L1            2.2753     0.5406   4.209 2.56e-05 ***
L2            3.1234     0.7318   4.268 1.97e-05 ***
L3            1.9369     0.8134   2.381  0.01726 *  
</code></pre>

<p>Looking at my graph it seems strange that the coefficients are positive and that the intercept is -0.953. How exactly should I interpret these results in the light of the graph? </p>

<p><img src=""http://dl.dropbox.com/u/22681355/graph.png"" alt=""""></p>
"
"0.127920429813366","0.121866669555358"," 15623","<p>I am completely new to R, just downloaded and installed it today. I am familiar with SAS and Stata; I am using R because I have found out that in survey regression analysis, R is capable of using data that have stratum with one PSU. However, I cannot figure out how to write the code at all.</p>

<p>Here is what I have done so far: read a Stata dataset and save the .RData file. I have also put in the MASS, pscl, and survey (for svyglm) packages.</p>

<p>Here's what I need to do:
1) I am using survey data, so I have a ""weight"" variable, a ""strata"" variable, and a ""PSU"" variable. I need to incorporate those; I know how to use svyset in Stata, but no idea in R.
2) I have stratum with singleton PSUs. I need to use an option called survey.lonely.psu I believe, and I have no idea where to even begin with that. This is the reason why I am using R instead of Stata as I do not want to collapse stratum or delete observations.
3) The types of regression models I have to run: survey negative binomial, survey zero-inflated negative binomial (need to also determine the predictors of zeros), survey logistic, and survey OLS regression.
4)I also really can't make much sense in R of how to write the model in R code. In Stata, I can simply write the model as:</p>

<p>svy: nbreg dependent_var independent_var1 independent_var2 independent_var3</p>

<p>I can't figure out how to do that at all in R.</p>

<p>Any and all help will be greatly appreciated.</p>
"
"0.0738548945875996","0.0469065029820195"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.104446593573419","0.0995037190209989"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.0762770071396474","0.090834052439095"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"NaN","NaN"," 20854","<p>I have read from <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">here</a> and understand how to calculate the estimated logit from a fitted logistic regression model, but how to work on the confidence interval? As it involved a variance-covariance matrix and I think it is better to have a program to do the calculation, rather then doing it by myself.</p>

<p>Thanks.</p>

<h3>Edit 01</h3>

<p>I have added a script here:</p>

<pre><code>chdage.dummy &lt;- data.frame(chd=c(rep(1,50),rep(0,50)),
                           race=c(rep(""white"",5),rep(""black"",20),rep(""hispanic"",15),rep(""other"",10),
                                  rep(""white"",20),rep(""black"",10),rep(""hispanic"",10),rep(""other"",10)),
                           stringsAsFactors=FALSE)
chdage.dummy[,""race""] &lt;- factor(chdage.dummy[,""race""],levels=c(""white"",""black"",""hispanic"",""other""))
chdage.lr.02 &lt;- glm(chd~race,data=chdage.dummy,family=""binomial"")
predict(chdage.lr.02,newdata=data.frame(race=""white""))
</code></pre>

<p><code>predict</code> function can give me an estimate, but I can't use <code>confint</code> outside <code>predict</code>, so what can I do?</p>
"
"0.0762770071396474","0.090834052439095"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0852802865422442","0.0812444463702388"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.0603022689155527","0.0574484989621426"," 24975","<p>I am working on an assignment involving a logistic regression model, where I need to plot the pearson standardized residuals against one of the predictors. Here's the basic setup:</p>

<pre><code>model &lt;- glm(outcome ~ predictor1 + predictor2, family=binomial(logit))
res &lt;- residuals(model, ""pearson"")
</code></pre>

<p>When looking at the residuals' distribution, I see something totally different than my colleagues who use Stata (using predict and rstandard). Their residuals are more or less normal, whereas in mine there is a gap in the values (not a singe residual is between -0.05 and 1.15). That does make sense in the context of logistic regression, especially that the maximum predicted probability is not so high (38%). </p>

<p>I'd like to understand what's happening here... What is Stata doing that R isn't, with those residuals? </p>
"
"0.0762770071396474","0.072667241951276"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.0852802865422442","0.0812444463702388"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.112815214963553","0.107476300250388"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.114415510709471","0.109000862926914"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.0762770071396474","0.090834052439095"," 27400","<p>I'm reading A. Agresti (2007), <em><a href=""http://rads.stackoverflow.com/amzn/click/0471226181"">An Introduction to Categorical Data Analysis</a></em>, 2nd. edition, and am not sure if I understand this paragraph (p.106, 4.2.1) correctly (although it should be easy):</p>

<blockquote>
  <p>In Table 3.1 on snoring and heart disease in the previous chapter, 254
  subjects reported snoring every night, of whom 30 had heart disease.
  If the data file has grouped binary data, a line in the data file
  reports these data as 30 cases of heart disease out of a sample size
  of 254. If the data file has ungrouped binary data, each line in the
  data file refers to a separate subject, so 30 lines contain a 1 for
  heart disease and 224 lines contain a 0 for heart disease. The ML
  estimates and SE values are the same for either type of data file.</p>
</blockquote>

<p>Transforming a set of ungrouped data (1 dependent, 1 independent) would take more then ""a line"" to include all the information!? </p>

<p>In the following example a (unrealistic!) simple data set is created and a logistic regression model is build. </p>

<p>How would grouped data actually look like (variable tab?)? How could the same model be build using grouped data? </p>

<pre><code>&gt; dat = data.frame(y=c(0,1,0,1,0), x=c(1,1,0,0,0))
&gt; dat
  y x
1 0 1
2 1 1
3 0 0
4 1 0
5 0 0
&gt; tab=table(dat)
&gt; tab
   x
y   0 1
  0 2 1
  1 1 1
&gt; mod1=glm(y~x, data=dat, family=binomial())
</code></pre>
"
"0.0426401432711221","0.0406222231851194"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.134839972492648","0.128458748884677"," 29653","<p>The likelihood ratio (a.k.a. deviance) $G^2$ statistic and lack-of-fit (or goodness-of-fit) test is fairly straightforward to obtain for a logistic regression model (fit using the <code>glm(..., family = binomial)</code> function) in R. However, it can be easy to have some cell counts end up low enough that the test is unreliable. One way to verify the reliability of the likelihood ratio test for lack of fit is to compare its test statistic and <em>P</em>-value to those of Pearson's chi square (or $\chi^2$) lack-of-fit test.</p>

<p>Neither the <code>glm</code> object nor its <code>summary()</code> method report the test statistic for Pearson's chi square test for lack of fit. In my search, the only thing I came up with is the <code>chisq.test()</code> function (in the <code>stats</code> package): its documentation says ""<code>chisq.test</code> performs chi-squared contingency table tests and goodness-of-fit tests."" However, the documentation is sparse on how to perform such tests:</p>

<blockquote>
  <p>If <code>x</code> is a matrix with one row or column, or if <code>x</code> is a vector and <code>y</code> is not given, then a <em>goodness-of-fit</em> test is performed (<code>x</code> is treated as a one-dimensional contingency table). The entries of <code>x</code> must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in <code>p</code>, or are all equal if <code>p</code> is not given.</p>
</blockquote>

<p>I'd imagine that you could use the <code>y</code> component of the <code>glm</code> object for the <code>x</code> argument of <code>chisq.test</code>. However, you can't use the <code>fitted.values</code> component of the <code>glm</code> object for the <code>p</code> argument of <code>chisq.test</code>, because you'll get an error: ""<code>probabilities must sum to 1.</code>""</p>

<p>How can I (in R) at least calculate the Pearson $\chi^2$ test statistic for lack of fit without having to run through the steps manually?</p>
"
"0.150755672288882","0.172345496886428"," 31592","<p>I am a bit puzzled about the behavior of uncorrelated predictors in logistic regression. 
As in OLS, I thought that if two predictors (<code>rv1</code> and <code>rv2</code>) are uncorrelated, then the regression weights of <code>rv1</code> will not change from a regression that only includes <code>rv1</code> to one that includes <code>rv1</code> and <code>rv2</code>. 
However, it seems to be the case that this is not true in logistic regression and coefficients change between the two regression models, even if the predictors are uncorrelated.</p>

<p>I have pasted some R syntax below that demonstrates this behavior.</p>

<p>Why is this the case and how do the regression weights from the two regressions (the one with only rv1 and the other one with rv1 and rv2) relate to each other? Is there a way to know what the regression weight of rv1 will be if one knows the regression weight of rv1 in the regression that includes both predictors?</p>

<p>Thanks!
P.S. This post is crossposted at another unrelated stat answer site.</p>

<pre><code>library(MASS)

#generate lots of data (a little bit weird data handling, I know)
n &lt;- 10000
rdta &lt;- as.data.frame(mvrnorm(n=n,c(0,0),matrix(c(1,0,0,1),2,2),empirical=TRUE))
names(rdta) &lt;- c(""rv1"",""rv2"")

#confirm that preds are uncorrelated
cov(rdta$rv1,rdta$rv2)

rv1 &lt;- rdta$rv1
    rv2 &lt;- rdta$rv2

rv1ry &lt;- 1
rv2ry &lt;- 1

#generate binary data from known regression coefficients
ylinp &lt;- (1 / (1+exp(-(-1 + rv1*rv1ry + rv2*rv2ry))))
y &lt;- rbinom(n,1,ylinp) 
glm(y~rv1+rv2,family=binomial(link='logit'))
glm(y~rv1,family=binomial(link='logit'))
glm(y~rv2,family=binomial(link='logit'))

#confirm that OLS regression works as expected (regression weights do not change)
rv1y &lt;- .222
rv2y &lt;- .333
y &lt;- rv1y * rv1 + rv2y * rv2 + rnorm(n,0,.5)
lm(y~rv1+rv2)
lm(y~rv1)
lm(y~rv2)   
</code></pre>

<p>I am not sure if it is expected to also paste relevant output here, but here goes:
OLS results</p>

<blockquote>
  <p>lm(y~rv1+rv2)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1 + rv2)

Coefficients:
(Intercept)          rv1          rv2  
 0.001096     0.220051     0.333072  
</code></pre>

<blockquote>
  <p>lm(y~rv1)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1)

Coefficients:
(Intercept)          rv1  
 0.001096     0.220051  
</code></pre>

<blockquote>
  <p>lm(y~rv2)  </p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv2)

Coefficients:
(Intercept)          rv2  
 0.001096     0.333072  
</code></pre>

<p>Logistic regression results</p>

<blockquote>
  <p>glm(ry~rv1+rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1 + rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1          rv2  
     -1.001        1.916        2.469  
</code></pre>

<blockquote>
  <p>glm(ry~rv1,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1  
    -0.5495       1.0535  
</code></pre>

<blockquote>
  <p>glm(ry~rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv2  
-0.6538       1.6140  
</code></pre>
"
"0.0738548945875996","0.0703597544730292"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.0852802865422442","0.0812444463702388"," 32070","<p>I'm trying out the boot() function for internal validation of a logistic glm model using the AUC (aka c-statistic) as my performance measure. My problem is that depending on the dataset I use, sometimes the function gets ""50 or more warnings,"" all of which are of this type:</p>

<pre><code>Warning messages:
1: In wilcox.test.default(pred[obs == 1], pred[obs == 0],  ... :
  cannot compute exact p-value with ties
</code></pre>

<p>Even though the function produces these warnings, it does also produce the relevant bootstrap statistics.</p>

<p>I'm not sure what to make of this situation since I don't even know in what context boot() is calling wilcox.test. The documentation ?boot doesn't mention the use of the Wilcoxon test. </p>

<p>Here is an example that generates these warnings:</p>

<pre><code>data(nuclear)
library(verification)

AUC = function(data, i) {
    d = data[i,]
    rs1 = glm(ne ~ cost, family=binomial(link=""logit""), data=d)
    return(roc.area(d$ne, predict(rs1))$A)
}

library(boot)
boot(data=nuclear, statistic=AUC, R=600)
</code></pre>

<p>And an example that doesn't:</p>

<pre><code>data(melanoma)

AUC = function(data, i) {
    d = data[i,]
    rs1 = glm(ulcer ~ thickness, family=binomial(link=""logit""), data=d)
    return(roc.area(d$ulcer, predict(rs1))$A)
}

boot(data=melanoma, statistic=AUC, R=600)
</code></pre>

<p>(I apologize if this question belongs in Stack Overflow instead of here -- not sure how statistical vs. programmatic the issue is.)</p>
"
"0.0738548945875996","0.0703597544730292"," 33151","<p>I have a dataset (<code>data.mrsa</code>) about the MRSA prevalence of elderly in long term care facilities (LTCF) with the following information:</p>

<ul>
<li>mrsa_result: MRSA result of recruited elderly (positive VS negative)</li>
<li>age: residents' age</li>
<li>ltcf: UID for each LTCF (we sampled 30 out of 1000 LTCFs)</li>
<li>ltcf_type: type of LTCF (private VS non-private)</li>
</ul>

<p>I have a multi-level logistic regression model like the one below:</p>

<pre><code>fit2 &lt;- glmer(mrsa_result ~ age + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>I know I am trying to find out the effect of <code>age</code> on the log-odds of <code>mrsa_result</code>, having the <code>ltcf</code> on the 2nd level gives me a wider CI on the lod-odds.</p>

<p>Now I want to add the <code>ltcf_type</code> into the model, I think this should be a fixed effect, as there can only be private and non-private, but <code>itcf_type</code> should be considered as 2nd level data, right? As this describe the type of LTCF, not the type of elderly.</p>

<p>I am puzzled on where should I put the term <code>ltcf_type</code> into my model, I wonder which of the following lines is correct:</p>

<pre><code>fit2a &lt;- glmer(mrsa_result ~ age + ltcf_type + (1|ltcf), family=binomial(""logit""),data=data.mrsa)
fit2b &lt;- glmer(mrsa_result ~ age + (1|ltcf + ltcf_type), family=binomial(""logit""),data=data.mrsa)
</code></pre>

<p>Thanks.</p>
"
"0.104446593573419","0.0995037190209989"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.127920429813366","0.121866669555358"," 34319","<p>Let's say I have the following logistic regression models:</p>

<pre><code> df=data.frame(income=c(5,5,3,3,6,5),
                  won=c(0,0,1,1,1,0),
                  age=c(18,18,23,50,19,39),
                  home=c(0,0,1,0,0,1))

&gt; md1 = glm(factor(won) ~ income + age + home, 
+           data=df, family=binomial(link=""logit""))
&gt; md2 = glm(factor(won) ~ factor(income) + factor(age) + factor(home), 
+           data=df, family=binomial(link=""logit""))
&gt; summary(md1)

Call:
glm(formula = factor(won) ~ income + age + home, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
      1        2        3        4        5        6  
-1.0845  -1.0845   0.8017   0.4901   1.7298  -0.8017  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.784832   6.326264   0.756    0.449
income      -1.027049   1.056031  -0.973    0.331
age          0.007102   0.097759   0.073    0.942
home        -0.896802   2.252894  -0.398    0.691

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178  on 5  degrees of freedom
Residual deviance: 6.8700  on 2  degrees of freedom
AIC: 14.87

Number of Fisher Scoring iterations: 4

&gt; summary(md2)

Call:
glm(formula = factor(won) ~ factor(income) + factor(age) + factor(home), 
    family = binomial(link = ""logit""), data = df)

Deviance Residuals: 
         1           2           3           4           5           6  
-6.547e-06  -6.547e-06   6.547e-06   6.547e-06   6.547e-06  -6.547e-06  

Coefficients: (3 not defined because of singularities)
                  Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      2.457e+01  1.310e+05       0        1
factor(income)5 -4.913e+01  1.605e+05       0        1
factor(income)6 -2.573e-30  1.853e+05       0        1
factor(age)19           NA         NA      NA       NA
factor(age)23   -1.383e-30  1.853e+05       0        1
factor(age)39   -3.479e-14  1.605e+05       0        1
factor(age)50           NA         NA      NA       NA
factor(home)1           NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178e+00  on 5  degrees of freedom
Residual deviance: 2.5720e-10  on 1  degrees of freedom
AIC: 10
</code></pre>

<p>So depending on the mode of the predictors, R produced different outputs. For factors, R splits out the coefficients into separate categories for the levels, but not for the model with numeric predictors. I'm wondering about a couple things.</p>

<ol>
<li><p>Is it ever useful to have the response categories expressed as individual rows?</p></li>
<li><p>To express the general regression equation, how does one go from a model with the categories expressed in an individual equation to an equation with a single B_i. So, for example, if gender has two coefficients, 3.5 for Male and 2.3 for Female, how does one use that in an equation such that (besides converting them into numeric values):</p></li>
</ol>

<p>Y = B0 + B1 (Gender)</p>
"
"0.113707048722992","0.0947851874319452"," 34930","<p>I have a large set of data for 37 different clinical units (all oncology) in their respective 37 hospitals. There are two specific outcome variables that I need to analyse:</p>

<p>First, drug usage for specific drugs types and classes (aggregated drugs) that are expressed as a rate â€“  DDD (Defined Daily Doses) per 100 patient days. There are individual patient drug use figures for this set.</p>

<p>Question1: Which regression approach should I take? From what I can gather I can use a Poisson regression model. IF there is overdispersion in the outcome I could resort to a negative binomial model.</p>

<p>Second: I have antibiotic resistance data that is expressed as proportion in the range 0 â€“ 1.These are not available as individual patient data points but aggregated to each of the 37 hospitals.</p>

<p>Question 2: Again, which approach? From what I have read I can use a logistic regression model. I have been advised by another statistician to initiall use a logit model and then use a probit model and compare goodness of fit for each model.</p>

<p>Does this sound like a reasonable approach? Is there a specific text that you could direct me to in order to upgrade my basic regression modelling skills. I will be using R for the analysis.</p>

<p>Thanks in advance.</p>
"
"0.0738548945875996","0.0703597544730292"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"0.0870388279778489","0.0995037190209989"," 38500","<p>I am currently working with a logistic semi-parametric model in R using the mgcv package.  The output from the model gives the standard log-odds coefficients; however, reviewers have requested marginal effects (like the ones in Stata using the margins command).  I would like to do average marginal effects (though, marginal effects at the means--modes for categorical covariates--would at least be a start).</p>

<p>I was wondering if anyone has an implementation for this in R for models that use the gam() function.  I have a rather large data set (1.2 million observations, a large number of discrete and continuous covariates, and fixed effects for 2,000 individuals).  Are these estimates even tractable, given the non-parametric treatment of a couple of the continuous covariates?  Any information would be helpful.</p>

<p>Here is what I am working with, though I get errors when attempting to do the bootstrapped SEs for the effects (comes from this helpful site probitlogit-marginal-effects-in-r-2/).  I am not sure how this treats the non-parametrically estimated smooths (but maybe these don't matter, since it is using the ""predict"" function?):</p>

<pre><code>mfxboot &lt;- function(modform,dist,data,boot=1000,digits=3){ #dist is the distribution choice of logit or probit
      require(mgcv)

x &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",data)
      # get marginal effects

pdf &lt;- ifelse(dist==""probit"",               
 mean(dnorm(predict(x, type = ""link"")))            
 mean(dlogis(predict(x, type = ""link"")))
 marginal.effects &lt;- pdf*coef(x)


bootvals &lt;- matrix(rep(NA,boot*length(coef(x))), nrow=boot)  
set.seed(1111)  
for(i in 1:boot){    
samp1 &lt;- data[sample(1:dim(data)[1],replace=T,dim(data)[1]),]    
x1 &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",samp1)    
pdf1 &lt;- ifelse(dist==""probit"",                   
mean(dnorm(predict(x1, type = ""link""))),                   
mean(dlogis(predict(x1, type = ""link""))))       
bootvals[i,] &lt;- pdf1*coef(x1)     
}

res &lt;- cbind(marginal.effects,apply(bootvals,2,sd),marginal.effects/apply(bootvals,2,sd))     
if(names(x$coefficients[1])==""(Intercept)""){        
res1 &lt;- res[2:nrow(res),]    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep=""""),res1)),nrow=dim(res1)[1])         
rownames(res2) &lt;- rownames(res1)        
} else {    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep="""")),nrow=dim(res)[1]))       
rownames(res2) &lt;- rownames(res)    
}     
colnames(res2) &lt;- c(""marginal.effect"",""standard.error"",""z.ratio"") 
      return(res2)
}
</code></pre>
"
"0.153741222957161","0.14646550861729"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.147709789175199","0.140719508946058"," 41390","<p>I am looking for a test similar to a 2-way ANOVA that would work on a binary response variable. My response variable is survival of plant seedlings (alive or dead).  My explanatory variables are Treatment (3 treatment groups) and Site (3 sites).  </p>

<p>First, I would like to know whether Treatment, Site and their interaction have a significant effect on survival.  Second, if either Treatment or Site is significant, I would like to test all pairs of treatment groups or sites to know which pairs of levels are significantly different, as I would normally do with an ANOVA.</p>

<p>I have considered several options:</p>

<ol>
<li><p>Transform the response variable, for example through an arcsin transformation, and then perform an ANOVA. This does not work on my data because at one of the sites I measure 100% survival.  Therefore there is 0 variability at this site and no transformation will change that.</p></li>
<li><p>Logistic regression with Treatment and Site recoded as dummy variables.  The results do not seem to give me a test of significance of Treatment, Site and interaction term -  Instead, I get the relative importance of each treatment group and each site separately.  Furthermore, it seems that I cannot test all the pairs of treatment groups or sites, I can only compare one ""baseline"" or ""default"" group to each of the two remaining groups.</p></li>
<li><p>Chi-square test on each explanatory variable separately.  This has the obvious drawback of not being able to test the interaction term.  Also I suspect that I am omitting important information if I am comparing survival across the 3 treatment groups without taking into account that this survival data is grouped in 3 different sites.  Does this bias the results?</p></li>
</ol>

<p>Can anyone recommend a different test or what the best approach would be in my case?</p>

<p><strong>UPDATE:</strong> Logistic regression can in fact give a test of significance of each independent variable.  In R, I discovered I can use glm to contruct a model and then the anova function to extract p-values for each IV:</p>

<pre><code>mymodel &lt;- glm(Survival ~ Treatment*Site, data=survivaldata, family=""binomial"")
anova(mymodel, test=""Chisq"")
</code></pre>
"
"0.104446593573419","0.0995037190209989"," 41561","<p>I am fitting a logistic model to data using the glm function in R.   I have attempted to specify interaction terms in two ways:</p>

<pre><code>fit1 &lt;- glm(y ~ x*z, family = ""binomial"", data = myData) 
fit2 &lt;- glm(y ~ x/z, family = ""binomial"", data = myData) 
</code></pre>

<p>I have 3 questions:</p>

<p>1) What is the difference between specifying my interaction terms as x*z compared to x/d?</p>

<p>When I call summary(fit1) the report includes results for the intercept, x, z, and x:z while summary(fit2) only includes results for intercept, x, and x:z.</p>

<p>I did look at Section 11.1 in ""An Introduction to R"" but couldn't understand the meaning. </p>

<p>2) How do I interpret the fit equation mathematically?  Specifically, how do I interpret the interaction terms formulaically?</p>

<p>Moving to math instead of R, do I interpret the equation as:
logit(y) = (intercept) + (coeff_x)*x + (coeff_z)*x + (coeff_xz)*x*z
?</p>

<p>This interpration may differ in the two specifications fit1 and fit2.  What is the interpretation of each?</p>

<p>3) Assuming the above interpretation is correct, how to I fit the model of x*(1/z) in R?  Do I need to just create another column with these values?</p>
"
"0.0603022689155527","0.0574484989621426"," 41660","<p>I'm modeling a set of outcome data the depends on two parameters:</p>

<ol>
<li>time, T</li>
<li>-100 &lt; A &lt; 100</li>
</ol>

<p>I've done logistic regression using R with the command:</p>

<pre><code>model &lt;- glm(Outcome ~ A + T, family = ""binomial"", data = myData)
</code></pre>

<p>My expectation (the only thing that makes sense) is that when A &lt; 0, the fit probability should be an increasing function of time approaching 0.5, while when A > 0 it should be a decreasing function of time approaching 0.5.</p>

<p>However, the fit I get is that A &lt; 0, A > 0, and A = 0 all are increasing functions of time.  They in fact appear to be the same curve just shifted (ie same ""shape"").</p>

<p>What am I doing incorrectly?  Any suggestions?</p>
"
"0.113707048722992","0.0947851874319452"," 41670","<p>I have a dataset in which each row belongs to one of 8 categories. I'm running a logistic regrssion on it using R. I created dummy variables for each of these categories. In my logistic regression model I know one of these dummies need to be excluded to not fall for the dummy variable trap. However the model keeps kicking out the same dummy-variable, even if I exclude another. I thought, maybe this dummy is kicked out due to other variables (other than the other dummy variables), but after using only the category-dummies to estimate the model, that particular variable still results in NA. </p>

<p>Why is this? Am I missing something?</p>

<p>Example: (I've removed all other variables and just included the first 100 rows of the dependent variable + the category dummies)</p>

<pre><code> structure(list(buy = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), cat1 = c(0L, 1L, 0L, 
1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 
0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L), cat2 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), cat3 = c(1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
cat4 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L), cat5 = c(0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), cat6 = c(0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L), cat7 = c(0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L), cat8 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L)), .Names = c(""buy"", ""cat1"", ""cat2"", ""cat3"", ""cat4"", 
""cat5"", ""cat6"", ""cat7"", ""cat8""), row.names = c(NA, 100L), class = ""data.frame"")
</code></pre>

<p>For the logistic regression I use:</p>

<pre><code>mylogit &lt;- glm(formula= buy ~ cat1 + cat2 + cat4 + cat5 + cat6 + cat7 + cat8, family = binomial, data=df)
</code></pre>

<p>As shown in the output of this model, cat7 is NA</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -2.6626     0.5972  -4.459 8.24e-06 ***
cat1          -17.9035  2769.0186  -0.006    0.995    
cat2          -17.9035 17730.3699  -0.001    0.999    
cat4            1.0531     1.2476   0.844    0.399    
cat5          -17.9035 17730.3699  -0.001    0.999    
cat6          -17.9035  8865.1850  -0.002    0.998    
cat7                NA         NA      NA       NA    
cat8          -17.9035 17730.3699  -0.001    0.999
</code></pre>

<p>However, 1 of the dummies should be excluded. If I exclude for instance cat 6 by leaving it out of the equation or by using '- cat6' the results for cat7 are still NA. </p>
"
"0.0953462589245592","0.090834052439095"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"NaN","NaN"," 43785","<p>If I have a set of continuous predictors $X$ and a binary outcome $Y$ and I wanted to build a predictive model of $P(Y|X)$, I would start with a logistic regression model.</p>

<p>However, in my particular case, my $Y$ isn't binary, it's continuous between 0 and 1.  Is there a similar Generalized Linear Model that can be applied in this case?  My optimistic/naive attempt in R reveals that </p>

<pre><code>set.seed(123)
df &lt;- data.frame(y=runif(8), x1=rnorm(8), x2=rnorm(8))
mod &lt;- glm(y ~ ., data=df, family=binomial('logit'))

# Warning message:
# In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!

rbind(yhat=predict(mod, newdata=df), y=df$y)
#              1         2          3         4         5          6         7           8
# yhat 0.7461449 0.4869853 -0.1092115 1.9854276 0.8328304 -1.3708688 1.0150934 -0.03496334
# y    0.2875775 0.7883051  0.4089769 0.8830174 0.9404673  0.0455565 0.5281055  0.89241904
</code></pre>

<p>Note that some of the predictions are outside of $(0,1)$.  Any suggestions?</p>
"
"0.112815214963553","0.0921225430717611"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.0603022689155527","0.0574484989621426"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"0.110782341881399","0.140719508946058"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.0426401432711221","0.0406222231851194"," 47020","<p>Hello I have the following logistic model with a categorical variable interaction which I wish to plot in R but I am struggling to find any solutions -</p>

<pre><code>M &lt;-glm(disorder~placement*ethnic, family=binomial)
</code></pre>

<p>The ethnic variable has three categories (White, Black &amp; Other)
The 'other' category interacts with the variable placement to produce a significant result.</p>

<p>I've tried the following but it isn't displaying a line:</p>

<pre><code>disorder_1 &lt;- cbind(disorder) - 1
any_ic_dat &lt;- as.data.frame(cbind(ethnic,disorder_1,placement))
g &lt;- glm(disorder_1~placement+ethnic, family=binomial,any_ic_dat)
plot(placement,disorder_1)


x &lt;- seq(0,19, length.out=1500)
mydata &lt;- data.frame(ethnic='Other', placement=x)
y.ethnic&lt;-predict(g,newdata=mydata)
lines(x,y.ethnic, col='red')
</code></pre>

<p>How would I plot this on a graph?</p>

<p>Thank you in advance!</p>

<p>Sarah</p>
"
"0.0603022689155527","0.0574484989621426"," 47306","<p>I have a dataset with one binary target variable called â€œtargetâ€ and many many factors â€œF1â€, F2â€â€¦ â€œF200â€. Iâ€™m trying to come up with code to fit 200 single factor logistic regression models and return the model summaries. Here is what I have so far</p>

<pre><code>single &lt;- function(df, factor){
    s &lt;- summary(glm(as.formula(paste('target ~ ', factor, sep='')), family=binomial, data=df));
return(s);
}
single(data, c(""F1"", ""F2"", ""F2""));
</code></pre>

<p>but this gives me only a summary of the first model. Am I missing something obvious here?</p>
"
"0.0738548945875996","0.0703597544730292"," 47348","<p>I am trying to run a logistic regression in R on my data where my independent variables are 13 continuous variables and my dependent variable is binary.  I want to segment my data so that I train on the first 80% and test on the last 20%.  I have a total of 3750 rows of data so I utilize the first 3000 for training.  I have written the following:  </p>

<pre><code>mydata&lt;-totaldata[1:3000,2:15]
mylogit&lt;-glm(mydata$TARGET ~ mydata$VAR1+mydata$VAR2+mydata$VAR3+mydata$VAR4+ #$
                             mydata$VAR5+mydata$VAR6+mydata$VAR7+mydata$VAR8+
                             mydata$VAR9+mydata$VAR10+mydata$VAR11+mydata$VAR12+
                             mydata$VAR13, family=""binomial"")

predictdata=totaldata[3001:3751,3:15]
in_frame&lt;-data.frame(predictdata)
predictions=predict(mylogit,in_frame,type=""response"")
</code></pre>

<p>However I get the following warning message: 
Warning message:
'newdata' had 751 rows but variable(s) found have 3000 rows </p>

<p>Then when I look at predictions there are 3000 predictions not the 751 that I wanted.  What can I do to fix this?</p>
"
"0.0492365963917331","0.0703597544730292"," 47795","<p>I am currently carrying out an investigation to find if certain factors such as playing home or away or position of a footballer affects overall pass completion using logistic regression. I am using R to compute my data. In my current section in which I am trying to analyse uses the data of every player to convey a general conclusion to whether or not the position of a player affects the successfulness of pass completion. </p>

<p>so far I have computed:</p>

<pre><code>test.logit &lt;- glm( cbind(Total.Successful.Passes.All,Total.Unsuccessful.Passes.All) ~
                   as.factor(Position.Id), data=passes.data, family = ""binomial"")

summary(test.logit)
</code></pre>

<p>and my output was:</p>

<pre><code>Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    

(Intercept)              0.28482    0.01256   22.67   &lt;2e-16 

as.factor(Position.Id)2  0.99768    0.01438   69.38   &lt;2e-16 

as.factor(Position.Id)4  1.06679    0.01398   76.29   &lt;2e-16 

as.factor(Position.Id)6  0.68090    0.01652   41.23   &lt;2e-16 

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 32638  on 10269  degrees of freedom
Residual deviance: 26499  on 10266  degrees of freedom

AIC: 60422

Number of Fisher Scoring iterations: 4
</code></pre>

<p>the intercept is goalkeepers,position.Id 2 is for a defender, 4 = midfielder and 6 = striker</p>

<p>Is this a good set of results to come to a conclusion? and with the large deviances?</p>
"
"0.165468060798406","0.147785044358525"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.147709789175199","0.140719508946058"," 48410","<p>I'm modeling the effect of a categorical predictor on a binary dependent variable using logistic regression. I'm comparing models with/without the predictor using a likelihood-ratio test.</p>

<p>Two categories of the predictor are associated with values of 1 only (no 0s) for the dependent variable. Regression coefficients for these categories (expressed as changes in log(odds) compared to a reference category) are very large and highly suspicious, as this reference category is always associated with response values of 1 (but for one case), and I would thus expect regression coefficients close to 0 for these two categories. Comparisons between the reference category and other categories having more balanced distribution of 1 and 0s matches what I'm expecting from visual inspection of the data. 
Removing cases associated with these two 'problematic' categories does not change the logLikelihood of the models, but because it changes the number of parameters it affects the results of the likelihood ratio test.</p>

<p>Models are fitted using the glm function with binomial family and logit link in R.</p>

<p>My question therefore is: what model (or procedure) should I use to: </p>

<p>(1) test the global significance of the effect of the predictor on the dependent variable? Should I keep data from the 'problematic' categories in the model or not before conducted the likelihood ratio test?</p>

<p>(2) compare these two 'problematic' categories with others?</p>

<p>Any hint appreciated,</p>
"
"0.135400640077266","0.128992883200554"," 48766","<p>My logistic model <a href=""http://stats.stackexchange.com/q/48739/5509"">has been suspicious</a> due to enormous coefficients, so I tried to do a crossvalidation, and also do a crossvalidation of simplified model, to confirm the fact that the original model is overspecified, as <a href=""http://stats.stackexchange.com/a/48741/5509"">James suggested</a>. However, I don't know how to interpret the result (this is the model from the linked question):</p>

<pre><code>&gt; summary(m5)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj + resid_usili2 + 
    rok:obdobi + rok:kraj + obdobi:kraj + kraj:resid_usili2 + 
    rok:obdobi:kraj, family = ""quasibinomial"")
[... see http://stats.stackexchange.com/q/48739/5509 for complete summary output ]

&gt; cv.glm(na.omit(data.frame(orel, resid_usili2)), m5, K = 10)
$call
cv.glm(data = na.omit(data.frame(orel, resid_usili2)), glmfit = m5, 
    K = 10)

$K
[1] 10

$delta
[1] 0.2415355 0.2151626

$seed
  [1]         403         271  1234892862 -1124595763  -489713400  1566924080   147612843
  [8]  1879282918  -694084381  1171051622  2063023839 -1307030905  -477709428  1248673977
 [15]  -746898494   420363755  -890078828   460552896  -758793089  -913500073  -882355605
[....]
Warning message:
glm.fit: algorithm did not converge
</code></pre>

<p>I guess the delta is the mean fitting error, but how to interpret it? Is it a good or bad fit? BTW, the algorithm did not converge, maybe due to the enormous coefficients (?)</p>

<p>I tried a simplified model:</p>

<pre><code>&gt; summary(m)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj, family = ""quasibinomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7335  -1.2324  -0.1666   1.0866   3.1788  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -107.60761   48.06535  -2.239 0.025335 *  
rok            0.05381    0.02393   2.249 0.024683 *  
obdobinehn    -0.26962    0.10372  -2.599 0.009441 ** 
krajJHC        0.68869    0.27617   2.494 0.012761 *  
krajJHM       -0.26607    0.28647  -0.929 0.353169    
krajLBK       -1.11305    0.55165  -2.018 0.043828 *  
krajMSK       -0.61390    0.37252  -1.648 0.099593 .  
krajOLK       -0.49704    0.32935  -1.509 0.131501    
krajPAK       -1.18444    0.35090  -3.375 0.000758 ***
krajPLK       -1.28668    0.44238  -2.909 0.003691 ** 
krajSTC        0.01872    0.27806   0.067 0.946322    
krajULKV      -0.41950    0.61647  -0.680 0.496315    
krajVYS       -1.17290    0.39733  -2.952 0.003213 ** 
krajZLK       -0.38170    0.36487  -1.046 0.295698    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for quasibinomial family taken to be 1.304775)

    Null deviance: 2396.8  on 1343  degrees of freedom
Residual deviance: 2198.6  on 1330  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and it's crossvalidation:</p>

<pre><code>&gt; cv.glm(orel, m, K = 10)
$call
cv.glm(data = orel, glmfit = m, K = 10)

$K
[1] 10

$delta
[1] 0.2156313 0.2154078

$seed
  [1]         403         526   300751243  -244464717  1066448079  1971573706 -1154513152
  [8]   634841816 -1521293072 -1040655077   505710009  -323431793 -1218609191  1060964279
 [15]  1349082996   -32847357 -1387496845   821178952  -971482876  1295018851  1380491861
</code></pre>

<p>Now it converged. But the delta seems more or less the same, despite of the fact that this model looks much more sane! I'm confused by the crossvalidation now... please give me a hint on how interpret it.</p>
"
"0.0852802865422442","0.0609333347776791"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.0639602149066831","0.0812444463702388"," 50726","<p>I'm using the <code>lme4</code> package in R to do some logistic mixed-effects modeling.<br>
My understanding was that sum of each random effects should be zero.</p>

<p>When I make toy linear mixed-models using <code>lmer</code>, the random effects are usually &lt; $10^{-10}$ confirming my belief that the <code>colSums(ranef(model)$groups) ~ 0</code>
But in toy binomial models (and in models of my real binomial data) some of the random effect sum to ~0.9. </p>

<p>Should I be concerned?  How do I interpret this?  </p>

<p>Here is a linear toy example
<code><pre>
toylin&lt;-function(n=30,gn=10,doplot=FALSE){
 require(lme4)
 x=runif(n,0,1000)
 y1=matrix(0,gn,n)
 y2=y1
 for (gx in 1:gn)
 {
   y1[gx,]=2*x*(1+(gx-5.5)/10) + gx-5.5  + rnorm(n,sd=10)
   y2[gx,]=3*x*(1+(gx-5.5)/10) * runif(1,1,10)  + rnorm(n,sd=20)
 }
 c1=y1*0;
 c2=y2*0+1;
 y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
 g=rep(1:gn,each=n,times=2)
 x=rep(x,times=gn*2)
 c=c(c1,c2)
 df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
 (m=lmer(y~x*c + (x*c|g),data=df))
 if (doplot==TRUE)
  {require(lattice)
   df$fit=fitted(m)
   plot1=xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1)
   plot2=xyplot(y ~ x|g,data=df,group=c)
   print(plot1+plot2)
  }
 print(colMeans(ranef(m)$g))
 m
}
</code></pre></p>

<p>In this case the colMeans always come out $&lt;10^{-6}$ </p>

<p>Here is a binomial toy example (I would share my actual data, but it is being submitted for publication and I am not sure what the journal policy is on posting beforehand):</p>

<p><pre><code>
toybin&lt;-function(n=100,gn=4,doplot=FALSE){
  require(lme4)<br>
  x=runif(n,-16,16)
  y1=matrix(0,gn,n)
  y2=y1
  for (gx in 1:gn)
  { com=runif(1,1,5)
    ucom=runif(1,1,5)
    y1[gx,]=tanh(x/(com+ucom) + rnorm(1)) > runif(x,-1,1)
    y2[gx,]=tanh(2*(x+2)/com + rnorm(1)) > runif(x,-1,1)
  }
  c1=y1*0;
  c2=y2*0+1;
  y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
  g=rep(1:gn,each=n,times=2)
  x=rep(x,times=gn*2)
  c=c(c1,c2)
  df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
  (m=lmer(y~x*c + (x*c|g),data=df,family=binomial))
  if (doplot==TRUE)
   {require(lattice)
    df$fit=fitted(m)
    print(xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1))
   }
  print(colMeans(ranef(m)$g))
  m
}
</pre></code></p>

<p>Now the colMeans sometimes come out above 0.3, and definitely higher, on average than the linear example.</p>
"
"0.14191497503738","0.14646550861729"," 52206","<p>I have a data set which consists of binomial proportions, let's say the success rate of converting a customer depending on the advertisement, the customer age, and various other factors.</p>

<p>For some common combinations of covariates, I have a lot of data, and therefore the binomial proportion of successes has low variance. For rare combinations of covariates, however, I have very little data, and therefore the variance of the proportion is high.</p>

<p>The magnitude of differences is very large, for example I might have 1 million trials for some combinations of covariates, and only 50 for others. However, I want to include ALL data in my model and weight it appropriately to get the best model fit.</p>

<p>I've tried to use R to do binomial (logistic) regression using a generalized linear model:</p>

<pre><code>lrfit &lt;- glm ( cbind(converted,not_converted) ~ advertisement + age, family = binomial)
</code></pre>

<p>This is a good start because it automatically weights the observations by the number of trials.</p>

<p>However, it's not good enough. Here's why: Let's say you have some observations with 100,000 trials and others with 1,000,000 trials. If you weight by number of trials the latter group is going to receive 10 times the weight. This seems nonsensical, however, because both observations are easily precise enough to receive equal treatment in the model. Clearly you want to penalize groups with only 10 or 100 trials, but as the number of trials gets larger, the weight should stop increasing.</p>

<p>Since in weighted least squares the reciprocal of the error variance is used as the weight, my idea would be to use calculate the posterior variance of the proportion (using Jeffrey's prior), then add some constant term to it (this will make sure the variance stops increasing at a certain number of trials) and then use the reciprocal of the sum as the weight.</p>

<p>Is this approach reasonable? Am I missing something? Can someone give me more information about this method?</p>
"
"0.0953462589245592","0.090834052439095"," 52475","<p>I have been running some binomial logistic regressions in R on a data set and I realised that the p-values of the estimated coefficients are not computed based upon a Normal distribution. For e.g. I have the following result from the glm() function:</p>

<pre><code>    Coefficients:

                                   Estimate Std. Error z value Pr(&gt;|z|) 
    (Intercept)                    -1.6127     0.1124 -14.347  &lt; 2e-16 ***
    relevel(Sex, ""M"")F             -0.3126     0.1410  -2.216   0.0267 *  
    relevel(Feed, ""Bottle"")Both    -0.1725     0.2056  -0.839   0.4013    
    relevel(Feed, ""Bottle"")Breast  -0.6693     0.1530  -4.374 1.22e-05 ***
</code></pre>

<p>I previously thought that the beta hats (estimated coefficients) are asymptotically normal and the p-value should be calculated using the normal distribution. But it seems from the p-values here that a t-distribution is used to compute them (I think). </p>

<p>I know that the asymptotic condition does not hold when sample size is small but if so, what is the process that causes the estimator to be distributed with a Students distribution? And how do I find out the degrees of freedom for such a distribution?</p>
"
"0.104446593573419","0.0995037190209989"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0738548945875996","0.0703597544730292"," 56590","<p>I'm reading the technical manual for a <a href=""http://www.nwea.org/sites/www.nwea.org/files/resources/NJ_2011_LinkingStudy.pdf"" rel=""nofollow"">linking study</a> between two assessments.  It's pretty clear that the table is model output from a fitted logistic regression equation.  Here's what pass odds look like on test 2 as a function of score on test 1 (RIT score):  </p>

<p><img src=""http://i.stack.imgur.com/8lwbx.png"" alt=""enter image description here""></p>

<p>It seems silly to use a lookup table that rounds to 5 when the model that made that table could give a better estimate.  But how do I recreate that equation from this output? </p>

<p>I have a good sense of how I would fit this model if I had the raw data, but I'm not sure what to do here.  Not <code>glm(family=binomial)</code> because the data I have is are odds ratios, not pass / no pass (i.e., 1s and 0s), right?</p>

<p>Here's the data:</p>

<pre><code>PASS &lt;- c(0, 0, 0, 0.01, 0.01, 0.01, 0.02, 0.04, 0.06, 0.1, 0.15, 0.23, 
0.33, 0.45, 0.57, 0.69, 0.79, 0.86, 0.91, 0.94, 0.96, 0.98, 0.99, 
0.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)

RIT &lt;- c(120L, 125L, 130L, 135L, 140L, 145L, 150L, 155L, 160L, 165L, 
170L, 175L, 180L, 185L, 190L, 195L, 200L, 205L, 210L, 215L, 220L, 
225L, 230L, 235L, 240L, 245L, 250L, 255L, 260L, 265L, 270L, 275L, 
280L, 285L, 290L, 295L, 300L)
</code></pre>
"
"0.120604537831105","0.114896997924285"," 58315","<p>I want to find the most important predictors for a binomial dependent variable out of a set of more than 43,000 independent variables (These form the columns of my input dataset). The number of observations is more than 45,000 (these form the rows of my input dataset). Most of the independent variables are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. Here is some code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"", type.measure = ""class"")
betacoeff = as.matrix(fit$glmnet.fit$beta[,ncol(fit$glmnet.fit$beta)])
</code></pre>

<p>betacoeff returns the betas for all the independent variables. I am thinking of showing the predictors corresponding to the top 50 betas as the most important predictors. 
My questions are:</p>

<ol>
<li><p>glmnet picks one good predictor out of a bunch of highly correlated good predictors. So I am not sure how much I can rely on the betas returned by the above model run.</p></li>
<li><p>Should I manually sample the data (say 10 times) and each time run the above model, get the list of predictors with the top betas and then find those which are present in all 10 repetitions? Is there any standard way of doing this? What is the standard way of sampling in this case?</p></li>
<li><p>My other question is about cvm (cross validation error) returned by the above model. Since I use type.measure = ""class"", cvm gives the misclassification error for different values of lambda. How do I report the misclassification error for the entire model? Is it the cvm corresponding to lambda.min?</p></li>
</ol>
"
"0.0603022689155527","0.0574484989621426"," 60027","<p>I'm doing biomedical research and I need to set a GAM Logistic Model which get the maximum AUC score as possible. I have 4 disease markers; $Y_1, Y_2, Y_3, Y_4$ with different data in each one, and the model must have the following specification:</p>

<p>$\text{Model} = \log(\text{Odds Ratio} (\text{Illness}~|~Y_j,Y_i)) = \alpha + \sum\limits_{i,j}^n f_k(Y_k) $</p>

<p>The problem is that I need to code in R all the possible combinations between this 4 markers without repetitions, in order to do multiple comparisons, such as:</p>

<p>$\text{Model}_1 = \alpha + f_1(Y_1) + f_3(Y_3)$ or $\text{Model}_2 = \alpha + f_1(Y_1) + f_2(Y_2) + f_3(Y_3)$ etc. </p>

<p>When I code each model manually I use sentences like these. This is an example for a model with only one marker:</p>

<pre><code>model=gam(group~s(y1),family='binomial')
pred=predict(model,type='response')
r=ROCEmpiric(group,1-pred)
</code></pre>

<p>Where <code>group</code> is always in, as indicator of disease, and <code>ROCEmpiric</code> is a function which calculates the ROC value.</p>

<p>Does anyone know the way to automate the calculations? It's seems ridiculous to write each function after doing the summation of permutations.</p>
"
"0.0603022689155527","0.0574484989621426"," 60109","<p>I would like to understand what the following code is doing. The person who wrote the code no longer works here and it is almost completely undocumented. I was asked to investigate it by someone who thinks ""<em>it's a bayesian logistic regression model</em>""</p>

<pre><code>bglm &lt;- function(Y,X) {
    # Y is a vector of binary responses
    # X is a design matrix

    fit &lt;- glm.fit(X,Y, family = binomial(link = logit))
    beta &lt;- coef(fit)
    fs &lt;- summary.glm(fit)
    M &lt;- t(chol(fs$cov.unscaled))
    betastar &lt;- beta + M %*% rnorm(ncol(M))
    p &lt;- 1/(1 + exp(-(X %*% betastar)))
    return(runif(length(p)) &lt;= p)
}
</code></pre>

<p>I can see that it fits a logistic model, takes the transpose of the Cholseky factorisation of the estimated covariance matrix, post-multiplies this by a vector of draws from $N(0,1)$ and is then added to the model estimates. This is then premultiplied by the design matrix, the inverse logit of this is taken, compared with a vector of draws from $U(0,1)$ and the resulting binary vector returned. But what does all this <strong><em>mean</em></strong> statistically ?</p>
"
"0.127920429813366","0.121866669555358"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.0953462589245592","0.090834052439095"," 61344","<p>In a paper by <a href=""http://www.ncbi.nlm.nih.gov/pubmed/23628224"" rel=""nofollow"">Faraklas et al</a>, the researchers create a Necrotizing Soft-Tissue Infection Mortality Risk Calculator. They use logistic regression to create a model with mortality from necrotizing soft-tissue infection as the main outcome and then calculate the area under the curve (AUC). They use the bootstrap method to find the ""bootstrap optimism-corrected ROC area.""</p>

<p>If I were to do this in <code>R</code>, how would it look like? The code I have been toying with looks something like below:</p>

<pre><code>library(boot)
library(ROCR)

auc_calc &lt;- function(data, indices, outcomes) {
  d &lt;- data[indices,]
  # Using glm for logistic regression
  # Do I recreate the glm model for each dataset?
  fit &lt;- glm(outcomes[indices,] ~ X1 + X2 + X3, data=d, family=binomial)
  fit.predict &lt;- predict(fit, type=""response"")

  # Using ROCR to calculate AUC
  pred &lt;- prediction(fit.predict, outcomes[indices,])
  perf &lt;- performance(pred, ""auc"")

  # Returning the AUC
  return(perf@y.values[[1]])
}

boot.results &lt;- boot(data=my.data, statistic=auc_calc, R=10000, outcomes=my.outcomes)
</code></pre>

<p>Is this correct? Or am I doing something wrong - namely should I be passing in a glm model rather than recalculating it each time? As always thanks for the help.</p>
"
"0.159900537266708","0.152333336944198"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0762770071396474","0.090834052439095"," 63436","<p>I sometimes have to vectorise the Huber weights from a robust regression and use them in a lm.
Recently I've had to do something similar for a logistic model but I'm slightly worried because I don't get very similar results</p>

<pre><code>library(robustbase)
data(vaso)
ROB &lt;- glmrob(Y ~Volume+Rate, family=binomial(""logit""), data=vaso)
ROB
glm(Y ~Volume+Rate,data=vaso,family=binomial(""logit""))
glm(Y ~Volume+Rate,data=vaso,weights=ROB$w.r,family=binomial(""logit""))
</code></pre>

<p>The coefficients from the weighted glm are more similar to the robust regression than the unweighted glm, but is there a way to make them the same? I can get the same results with a robust (rlm) and weighted lm but this doesn't seem to be the case with glm. I haven't looked at the glm robust regression in detail so what I'm asking may be impossible...</p>

<p>Thanks for your help</p>
"
"0.0603022689155527","0.0574484989621426"," 63494","<p>In <code>R</code> I have a categorical variable that I performed logistic regression on and got the following result:</p>

<pre><code>glm(formula = mortality ~ SMOKE, family = binomial, data = c.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2155  -0.2155  -0.2155  -0.1860   2.8515  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.0483     0.3189 -12.694   &lt;2e-16 ***
SMOKEN        0.2968     0.3559   0.834    0.404    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 492.45  on 2369  degrees of freedom
Residual deviance: 491.72  on 2368  degrees of freedom
AIC: 495.72

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Is the value for the intercept the same as <code>SMOKEY</code> (has a history of smoking)?</p>
"
"0.112815214963553","0.107476300250388"," 63566","<p>I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.</p>

<p>My data-frame in short: A subject has one property, <code>is_frisian</code> which is either 0 or 1 depending on the subject. And it is tested for two conditions, <code>person</code> and <code>condition</code>. The measurement variable is <code>error</code>, which is either 0 or 1.</p>

<p>My mixed linear model in R is:</p>

<pre><code>&gt; model &lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)
</code></pre>

<p>However, the residuals plot of this model gives an unexpected (?) result.</p>

<p><img src=""http://i.stack.imgur.com/nz2KY.png"" alt=""Residuals lmer model""></p>

<p>I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?</p>

<p>The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable <code>error</code> is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?</p>

<p><strong>UPDATE:</strong></p>

<pre><code>&gt; model &lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')
&gt; binnedplot(fitted(model),resid(model))
</code></pre>

<p>Gives the following result:</p>

<p><img src=""http://i.stack.imgur.com/XMXFx.png"" alt=""binned residual plot""></p>

<p><strong>FINAL EDIT:</strong></p>

<p>The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), <a href=""https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/"" rel=""nofollow"">see here at statisticssolutions.com</a></p>
"
"0.105528970602217","0.114896997924285"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.112815214963553","0.107476300250388"," 64788","<p>I performed multivariate logistic regression with the dependent variable <code>Y</code> being death at a nursing home within a certain period of entry and got the following results (note if the variables starts in <code>A</code> it is a continuous value while those starting in <code>B</code> are categorical):</p>

<pre><code>Call:
glm(Y ~ A1 + B2 + B3 + B4 + B5 + A6 + A7 + A8 + A9, data=mydata, family=binomial)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0728  -0.2167  -0.1588  -0.1193   3.7788  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  20.048631  6.036637   3.321 0.000896 ***
A1           0.051167   0.016942   3.020 0.002527 ** 
B2          -0.664940   0.304299  -2.185 0.028878 *  
B3          -2.825281   0.633072  -4.463 8.09e-06 ***
B4          -2.547931   0.957784  -2.660 0.007809 ** 
B5          -2.862460   1.385118  -2.067 0.038774 *  
A6          -0.129808   0.041286  -3.144 0.001666 ** 
A7           0.020016   0.009456   2.117 0.034276 *  
A8          -0.707924   0.253396  -2.794 0.005210 ** 
A9           0.003453   0.001549   2.229 0.025837 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 485.10  on 2206  degrees of freedom
Residual deviance: 417.28  on 2197  degrees of freedom
AIC: 437.28

Number of Fisher Scoring iterations: 7

 (Intercept)           A1           B2           B3           B4           B5           A6           A7           A8           A9 
5.093426e+08 1.052499e+00 5.143045e-01 5.929197e-02 7.824340e-02 5.712806e-02 8.782641e-01 1.020218e+00 4.926657e-01 1.003459e+00 

                   2.5 %       97.5 %
(Intercept) 3.703525e+03 7.004944e+13
A1          1.018123e+00 1.088035e+00
B2          2.832698e-01 9.337710e-01
B3          1.714448e-02 2.050537e-01
B4          1.197238e-02 5.113460e-01
B5          3.782990e-03 8.627079e-01
A6          8.099945e-01 9.522876e-01
A7          1.001484e+00 1.039302e+00
A8          2.998207e-01 8.095488e-01
A9          1.000416e+00 1.006510e+00
</code></pre>

<p>As you can see, all of the variables are ""significant"" in that their p values are below the usual threshold of 0.05. However looking at the coefficients, I'm not quite sure what to make of these results. It seems that although these variables contribute to the model, looking at the odds ratios, they don't seem to really seem to have much predictive power. Of note, when I calculated the AUC, I got approximately 0.8. </p>

<p>Can I say that this model is better at predicting against mortality (e.g. predicting that seniors will live past the prescribed period) compared to predicting for mortality?</p>
"
"0.0738548945875996","0.0703597544730292"," 65095","<p>In documentation to <code>glm</code> I read: ""<em>For binomial and quasibinomial families the response can also be specified as a factor (when the first level denotes failure and all others success)</em>"" Does it mean that probability of failure or success is being modeled? </p>

<p>I'm trying to apply simple logistic model to ""german credit scoring"" dataset where there are levels ""good"" and ""bad"". To get correct results (higher probability means higher likelihood of being good) I have to assume that <code>Failure=Good</code> and <code>Success=Bad</code>. This works, but it is really counterintuitive. I interpret this as - this will model probability of Failure (failed to be bad).</p>

<pre><code>require(ggplot2)

german_data &lt;- read.csv(file=""http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"",
              sep="" "", header=FALSE)

names(german_data) &lt;- c('ca_status','mob','credit_history','purpose','credit_amount','savings',
'present_employment_since','status_sex','installment_rate_income','other_debtors',
'present_residence_since','property','age','other_installment','housing','existing_credits',
'job','liable_maintenance_people','telephone','foreign_worker','gb')

str(german_data)

german_data$gb &lt;- factor(german_data$gb,levels=c(2,1),labels=c(""bad"",""good""))

levels(german_data$gb)[1] 

table(german_data$gb)

model &lt;- glm(data=german_data,formula=gb~.,family=binomial(link=""logit""))

german_data$prob &lt;- predict(model,newdata=german_data, type=""response"")

ggplot(data=german_data) + geom_boxplot(aes(y=prob,x=gb))  + coord_flip()
</code></pre>

<p><img src=""http://i.stack.imgur.com/kcbao.png"" alt=""enter image description here""></p>
"
"0.0953462589245592","0.090834052439095"," 65244","<p>I'm curious about how to understand the accuracy of my model which I computed with <code>glm( family = binomial(logit) )</code>.</p>

<p>In some articles it is mentioned that we should perform chisq test with residual deviance with it's DoF. 
When I call summary() of my glm module.
<strong>""Residual deviance: 9109.9 on 99993 degrees of freedom""</strong> 
Therefore when I perform pchisq test with these inputs: <strong>1-pchisq(9110, 99993)</strong> it returns 1.</p>

<p>Hence it is much more greater than our significance level. So we are curious about why does it return 1, is it a perfect model ?</p>

<p>In addition to these, here's the output of my Logistic Regression Model</p>

<pre><code>Logistic Regression Model

lrm(formula = bool.revenue.all.time ~ level + building.count + 
    gold.spent + npc + friends + post.count, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
level           0.0756 0.0193   3.92 &lt;0.0001 
building.count  0.0698 0.0091   7.64 &lt;0.0001 
gold.spent      0.0020 0.0002  11.05 &lt;0.0001 
npc             0.0172 0.0057   3.03 0.0024  
friends         0.0304 0.0045   6.82 &lt;0.0001 
post.count     -0.0132 0.0042  -3.17 0.0015 
</code></pre>

<p>This is validation with bootstrap's output</p>

<pre><code>  index.orig training   test optimism index.corrected    n
Dxy           0.5511   0.5500 0.5506  -0.0006          0.5518 1000
R2            0.1469   0.1469 0.1465   0.0005          0.1465 1000
Intercept     0.0000   0.0000 0.0002  -0.0002          0.0002 1000
Slope         1.0000   1.0000 0.9997   0.0003          0.9997 1000
Emax          0.0000   0.0000 0.0001   0.0001          0.0001 1000
D             0.0149   0.0149 0.0148   0.0000          0.0148 1000
U             0.0000   0.0000 0.0000   0.0000          0.0000 1000
Q             0.0149   0.0149 0.0148   0.0001          0.0148 1000
B             0.0086   0.0086 0.0086   0.0000          0.0086 1000
g             1.1410   1.1381 1.1365   0.0016          1.1394 1000
gp            0.0111   0.0111 0.0111   0.0000          0.0111 1000
</code></pre>

<p>And this is the output of my calibration curve:</p>

<pre><code>n=100000   Mean absolute error=0.002   Mean squared error=5e-05
0.9 Quantile of absolute error=0.002
</code></pre>

<p><img src=""http://i.stack.imgur.com/2AUEX.png"" alt=""Calibration Curve""></p>

<p>Thanks.</p>
"
"0.0603022689155527","0.0574484989621426"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.080582296402538","0.107476300250388"," 65463","<p>This is a question regarding using logistic regression, and relating it to gaussian distribution or a binomial distribution.  </p>

<pre><code>model&lt;-glm(target~ x1, data=data, type='response', family='binomial')
model&lt;-glm(target~ x1, data=data, type='response')  #defaults to gaussian
</code></pre>

<p>My understanding of binomial is that it is </p>

<pre><code>theta=chance of success
z=trails ending in success
k=trials ending in failure
(theta^z)*(1-theta)^k
</code></pre>

<p>And something Gaussian is </p>

<pre><code>theta = standard deviation
x = success
u = mean
Y = [ 1/Ïƒ * sqrt(2Ï€) ] * e -(x - Î¼)2/2Ïƒ2 
</code></pre>

<p>So I understand how to do GLM with R, I kind of understand what binomial and gaussian means, but I have no understanding of how you relate binomial or gaussian to logistic regression, and how binomial and gaussian are different in this context.</p>

<p>Question 1- Can someone explain the intuition behind how ""family='binomial'"" is used when building a model with GLM?</p>

<p>Question 2- Given that the shapes of a binomial distribution and a gaussian distribution look very much the same (they both peak in the middle and gradually go down towards the ends), how does choosing either binomial or guassian lead to different models built from GLM?</p>

<p>thanks!!</p>
"
"0.0852802865422442","0.0812444463702388"," 65656","<p>My design is as follows.</p>

<ul>
<li>$y$ is Bernoulli response </li>
<li>$x_1$ is a continuous variable </li>
<li>$x_2$ is a categorical (factor) variable with two levels</li>
</ul>

<p>The experiment is completely within subjects. That is, each subject receives each combination of $x_1$ and $x_2$.</p>

<p>This is a repeated measures logistic regression set-up. The experiment will give two ogives for $p(y=1)$ vs $x_1$, one for level1 and one for level2 of $x_2$. The effect of $x_2$ should be that for level2 compared to level1, the ogive should have a shallower slope and increased intercept.</p>

<p>I am struggling with finding the model using <code>lme4</code>. For example,</p>

<pre><code>glmer(y ~ x1*x2 + (1|subject), family=binomial)
</code></pre>

<p>So far as I understand it, the <code>1|subject</code> part says that <code>subject</code> is a random  effect. But I do not see how to specify that $x_1$ and $x_2$ are repeated measures variables. In the end, I want a model that includes a random effect for subjects, and gives estimated slopes and intercepts for level1 and level2.</p>
"
"0.127920429813366","0.121866669555358"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.0953462589245592","0.090834052439095"," 65730","<p>I want to check the probability distribution for my data using R.In some site I found <code>fitdistr</code> function in package <code>MASS</code> can do that. To check that I have generated 105 random Poisson numbers &amp; run the <code>fitdistr</code> function to check whether it is able to identify or not. I used following code</p>

<pre><code>library(MASS)
zpois=rpois(105,0.1)

fitdistr(zpois, 't')$loglik
fitdistr(zpois, 'normal')$loglik
fitdistr(zpois, 'logistic')$loglik
fitdistr(zpois, 'weibull')$loglik
fitdistr(zpois, 'gamma')$loglik
fitdistr(zpois, 'lognormal')$loglik
fitdistr(zpois, 'exponential')$loglik
fitdistr(zpois, 'Poisson')$loglik
fitdistr(zpois, 'negative binomial')$loglik
</code></pre>

<p>I found that it is giving lowest value for normal distribution. I know that the large sample approximation of Poisson distribution is normal distribution but I don't want the large sample approximation. I want to see the exact distribution.</p>

<p>Can you help me to guide the suitable function in R using which I can get the exact distribution fit?</p>
"
"0.0852802865422442","0.0812444463702388"," 65740","<p>I have binomial data infected/not-infected on individuals of a sample from different years for two different infections.
I have used <code>prop.trend.test</code> to test for trend for each infection.</p>

<p>I want to test if there is a trend in the ratio of infected/not-infected between the two infections. Since a ratio could vary between 0 and &infin;, what test is appropriate? </p>

<p>For testing trends in proportins, some have suggested <code>glm</code> with family <code>binomial</code>. Can i use <code>glm</code> with a different family?</p>

<p><a href=""http://stats.stackexchange.com/questions/29502/what-test-can-i-use-to-prove-there-is-an-upward-trend-in-time-series-of-ratios-a"">What test can I use to prove there is an upward trend in time series of ratios assesment in R?</a> suggests using the <code>Cochran-Armitage test</code> (which I think is the same as <code>prop.trend.test</code>) or logistic regression. Not sure if it is suitable in the case of ratios, though.</p>
"
"0.0426401432711221","0.0406222231851194"," 66279","<p>Has anyone written a package in <code>R</code> to calculate diagnostic plots after <code>clogit</code>, conditional logistic regression? e.g. leverage. Or a related question, how do you stratify using <code>glm</code> (perhaps I can stratify using <code>glm</code> and <code>family =binomial</code>, and then use diagnostic packages for <code>glm</code>?)</p>
"
"0.154134927117691","0.146840580956429"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.112815214963553","0.107476300250388"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.149240501448927","0.162488892740478"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.174077655955698","0.157547555116582"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.0426401432711221","0.0406222231851194"," 72060","<p>In the following code I perform a logistic regression on grouped data using glm and ""by hand"" using mle2. Why does the logLik function in R give me a log likelihood logLik(fit.glm)=-2.336 that is different than the one logLik(fit.ml)=-5.514 I get by hand? </p>

<pre><code>library(bbmle)

#successes in first column, failures in second
Y &lt;- matrix(c(1,2,4,3,2,0),3,2)

#predictor
X &lt;- c(0,1,2)

#use glm
fit.glm &lt;- glm(Y ~ X,family=binomial (link=logit))
summary(fit.glm)

#use mle2
invlogit &lt;- function(x) { exp(x) / (1+exp(x))}
nloglike &lt;- function(a,b) {
  L &lt;- 0
  for (i in 1:n){
     L &lt;- L + sum(y[i,1]*log(invlogit(a+b*x[i])) + 
               y[i,2]*log(1-invlogit(a+b*x[i])))
  }
 return(-L) 
}  

fit.ml &lt;- mle2(nloglike,
           start=list(
             a=-1.5,
             b=2),
           data=list(
             x=X,
             y=Y,
             n=length(X)),
           method=""Nelder-Mead"",
           skip.hessian=FALSE)
summary(fit.ml)

#log likelihoods
logLik(fit.glm)
logLik(fit.ml)


y &lt;- Y
x &lt;- X
n &lt;- length(x)
nloglike(coef(fit.glm)[1],coef(fit.glm)[2])
nloglike(coef(fit.ml)[1],coef(fit.ml)[2])
</code></pre>
"
"0.121355975243384","0.115612873996209"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.112815214963553","0.107476300250388"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0738548945875996","0.0703597544730292"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.0904534033733291","0.10053487318375"," 76935","<p>I have data from a psychology experiment in which participants were assigned to one of 3 training conditions and then gave responses for 16 trials for each combination of two within-subjects factors, section and format.  Each response is classified as correct or incorrect.  I originally calculated a % correct for each participant for each combination of section and format, then used a mixed ANOVA to analyze these percentage scores with condition as a between-subjects factor and section &amp; format as within-subjects factors.  However, a reviewer of my manuscript commented that ANOVA is inappropriate for this accuracy data because it's not a genuine continuous variable and has various properties making it inappropriate to use ANOVA.  I think logistic regression WOULD be applicable to this data because it is, after all, binary choice data originally.  Assuming that's right (please tell me if not), how can I do this in R?</p>

<p>Specifically, I need to do logistic regression with correctness/incorrectness of responses as my DV, condition as a b-s factor, and section &amp; format as w-s factors.  (I do not have any continuous predictors.)  Here is what my data looks like:</p>

<pre><code>nsubj       = 150
nsection    = 3
nformat     = 3
ntrials     = 16
subjid      = rep( 1:nsubj, each=nformat*nsection )
condition   = c( replicate( nsubj, rep( sample( c( 'condition 1', 'condition 2', 'condition 3' ), 1 ), nformat*nsection ) ) )
section     = rep( rep( c( 'a', 'b', 'c' ), each=nformat ), nsubj )
format      = rep( rep( c( 'x', 'y', 'z' ), nsection ), nsubj )
nCorrect    = sample( 1:ntrials, nsubj*nsection*nformat, replace=TRUE )
nIncorrect  = ntrials - nCorrect
D           = data.frame( subjid=factor(subjid), condition=factor(condition), section=factor(section), format=factor(format), nCorrect=nCorrect, nIncorrect=nIncorrect )
D$accuracy  = D$nCorrect / (D$nCorrect+D$nIncorrect)
</code></pre>

<p>I tried this, but I know it is wrong because it does not account for the repeated measures, and may inflate significance in other ways (I keep getting significant results from randomly generated data, which seems wrong):    </p>

<pre><code>fit &lt;- glm( cbind( D$nCorrect, D$nIncorrect ) ~ section*format*condition, family=binomial(""logit""), data=D )
anova( fit, test=""Chisq"" )
</code></pre>
"
"0.0953462589245592","0.090834052439095"," 77094","<p>I have a logistic model fitted with the following R function:</p>

<pre><code>glmfit&lt;-glm(formula, data, family=binomial)
</code></pre>

<p>A  reasonable cutoff value in order to get a good data classification (or confusion matrix) with the fitted model is 0.2 instead of the mostly used 0.5.</p>

<p>And I want to use the <code>cv.glm</code> function with the fitted model:</p>

<pre><code>cv.glm(data, glmfit, cost, K)
</code></pre>

<p>Since the response in the fitted model is a binary variable an appropriate cost function is (obtained from ""Examples"" section of ?cv.glm):</p>

<pre><code>cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)
</code></pre>

<p>As I have a cutoff value of 0.2, can I apply this standard cost function or should I define a different one and how?</p>

<p>Thank you very much in advance.</p>
"
"0.0603022689155527","0.0574484989621426"," 77540","<p>I am try to evaluate a logistic regression with new data using R. I have created the logistic regression using the old data and evaluated that using chi^2 etc. i.e.</p>

<pre><code>model &lt;- glm(outcome ~ input + 0, data = training.data, family = binomial())
</code></pre>

<p>(note I do need the intercept = 0). And then the predicted probabilities:</p>

<pre><code>predicted.probabilities &lt;- predict(model, test.data, type = ""response"")
</code></pre>

<p>How can I now perform the same sort of tests with these new probabilities? and what are the best tests to do here?</p>
"
"0.0426401432711221","0.0406222231851194"," 80463","<p>I have the following set of model-averaged fixed effects from a set of binomial GLMMs: </p>

<p><img src=""http://i.stack.imgur.com/yN4wR.png"" alt=""model parameters image""></p>

<p>I would like to plot the predicted effect of ""NBT"", along with confidence bands, while holding all the other variables at their baseline levels. My attempt to do this in ggplot:</p>

<pre><code>Xvars &lt;- seq(from=0, to=100, by=0.1)  #NBT range is 0-100
  binomIntercept &lt;- 1.317
  binomSlope &lt;- -0.0076     
  binomSE &lt;- 0.009    
Means &lt;- logistic(binomIntercept + binomSlope*Xvars)              
loCI &lt;- logistic(binomIntercept + (binomSlope - 1.96*binomSE)*Xvars)
upCI &lt;- logistic(binomIntercept + (binomSlope + 1.96*binomSE)*Xvars)
df &lt;- data.frame(Xvars,Means,loCI,upCI)
p &lt;- ggplot(data=df, aes(x = Xvars, y = Means)) + 
geom_line() +          
geom_line(data=df, aes(x = Xvars, y = upCI),col='grey') +
geom_line(data=df, aes(x = Xvars, y = loCI), col='grey')
p                                            
</code></pre>

<p><img src=""http://i.imgur.com/eMJBxQQ.png"" alt=""graph image""></p>

<p>I'm assuming that the confidence bands are cone shaped because I'm not accounting for uncertainty in the estimate for the intercept. Maybe this is okay (?), but it does look different from every regression line I've ever seen with confidence intervals plotted.</p>

<p>Can someone please tell me how I should be writing my equations to get the correct confidence intervals, given the intercept, slope, and standard errors from my model output?</p>

<p>(I know I can use the predict function to do this in R, but would like to know how to do it by hand.)  </p>
"
"0.0762770071396474","0.090834052439095"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.0603022689155527","0.0574484989621426"," 83945","<p>How do you solve the following problem?</p>

<blockquote>
  <p>A Simulation Study (Probit Regression).</p>
  
  <p>Assume $y|x\sim {\rm Binary}(p)$, where $p= E(y|x)$, and $Î¦^{-1}(\pi)=-1+5.1x_{1i}-0.3x_{2i}$
  Generate data with $x_{1i}\sim{\rm Unif}(0,1)$, $x_{2i}=1$ for $i$ odd and $x_{2i}=0$ for $i$ even, and sample size $n=500$. Try generalized linear model (GLM) with logistic and probit links.</p>
</blockquote>

<p>Here is what I did, I know there is a problem, but I don't know what:</p>

<pre><code>n         &lt;- 500
beta0     &lt;- -1
beta1     &lt;- 5.1
beta2     &lt;- -0.3
x1        &lt;- runif(n=n, min=0, max=1)
x2        &lt;- (1:n)%%2
y         &lt;- pnorm(beta0 + beta1*x1 + beta2*x2)
prob.glm  &lt;- glm(y~x1+x2, family=binomial(link=probit))
logit.glm &lt;- glm(y~x1+x2, family=binomial)
</code></pre>

<p>I know <code>y</code> is a probability here, but how do you simulate a binary variable from the probability? </p>
"
"0.0738548945875996","0.0469065029820195"," 84166","<p>I recently read a paper made a logistic regression and used a table like this to summarise the model:</p>

<pre><code>data.frame(predictors = c(""drat"", ""mpg""), ""chi squared statistic"" = c(""x"", ""x""), ""p-value"" = c(""x"", ""x""))

  predictors chi.squared.statistic p.value
1       drat                     x       x
2        mpg                     x       x
</code></pre>

<p>The data.frame presented the chi-squared and p value (the x's) for each predictor in the logistic regression model, and thus allowed to see at a glance which predictors were most important.</p>

<p>I have made this logistic regression model:</p>

<pre><code>mtcars_log_reg &lt;- glm(vs ~ drat + mpg, mtcars, family = ""binomial"")
</code></pre>

<p>How can I fill in the x's for the above table using R code?</p>
"
"0.0953462589245592","0.090834052439095"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.190692517849118","0.18166810487819"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.135400640077266","0.128992883200554"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0966987556830456","0.107476300250388"," 87872","<p>When I do a (logistic) regression in R, I run something like this:</p>

<pre><code>mydata &lt;- read.csv(""data.csv"")
mylogit &lt;- glm(a ~ c+d, data = mydata, family=""binomial"")
summary(mylogit)
</code></pre>

<p>As of a few months ago, the output for the coefficients might look like this:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6476     0.1898  -8.680  &lt; 2e-16 ***
c             2.4558     0.3414   7.194 6.29e-13 ***
d             2.3783     0.4466   5.326 1.01e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Trying it today (with a newer version of R), the output looks like the following:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6709     0.1924  -8.683  &lt; 2e-16 ***
c1            2.4961     0.3476   7.181 6.94e-13 ***
cc           18.2370   979.6100   0.019    0.985    
d1            2.4524     0.4630   5.296 1.18e-07 ***
dd                NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the ""c1"", ""cc"", etc fields mean?  I can't seem to find this any documentation, but perhaps I am looking in the wrong places?</p>
"
"0.0852802865422442","0.0812444463702388"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.0738548945875996","0.0703597544730292"," 88036","<p>I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).</p>

<ol>
<li><p>I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  </p>

<p><code>output &lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)</code><br>
RE is a factor with several levels.</p>

<p>This works. But I'm wondering why it's necessary to use the <code>(1 | RE)</code> syntax instead of just <code>DV~IV1+IV2 | RE</code>.</p></li>
<li><p>I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using <code>ranef()</code>. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?</p></li>
</ol>
"
"0.0953462589245592","0.090834052439095"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.0426401432711221","0.0406222231851194"," 89130","<p>Consider this logistic regression:</p>

<pre><code>mtcars$vs &lt;- as.factor(mtcars$vs)
log_reg_mtcars &lt;- glm(am ~ vs*wt +vs*mpg, family = ""binomial"", mtcars)
</code></pre>

<p>I tried using the effects package to extract some coefficients from the model:</p>

<pre><code>as.data.frame(effect(""vs"", log_reg_mtcars)) 

NOTE: vs is not a high-order term in the model
  vs       fit       se       lower     upper
1  0 0.3957869 2.916877 0.002150238 0.9950031
2  1 0.0336822 2.181009 0.000484832 0.7146704
</code></pre>

<p>Could anyone explain why the coefficients/standard error for <code>vs</code> given by this code are different. Or in other words, what is the effects package doing?</p>

<pre><code>as.data.frame(summary(log_reg_mtcars)$coef)[2,1:2]

     Estimate Std. Error
vs1 -40.70047   40.53197
</code></pre>
"
"0.134839972492648","0.128458748884677"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.125356634105602","0.151994441447782"," 90906","<p>I'm new to R and logistic regression and have to admit that I don't really know how to interpret the result. I'm trying to compute a pretty simple model with 2 predictors (A and B). When I first try to compute models with the predictors one by one they are both significant. When I put them together and add an interaction term they lose their significance (but the interaction term is weakly significant). I interpret this as A and B are overlapping and no longer significant when the oter parameter is hold constant. Right?</p>

<p>But now to the part I don't know how to interpret. I make predictions from my models (see code below) and then run t-tests for the predictions vs. the depending variable. I think this should give a hint on how good the model is (is there a better way?). When I do it this way I get a much lower p-value for the model with both A and B. I think this is contradictory. The first part tells me that A doesn't provide any significant information to the model when combined with B, but on the other hand I get much better predictions. I guess something is really wrong, but I can't figure out what. Can you help me?</p>

<pre><code>model1=glm(f~A, , family=binomial(link=""logit""))
model2=glm(f~B,   family=binomial(link=""logit""))
model3=glm(f~A*B, family=binomial(link=""logit""))
summary(model1)
summary(model2)
summary(model3)
p1=predict(model1, newdata=data, type=""response"", na.rm=TRUE)
p2=predict(model2, newdata=data, type=""response"", na.rm=TRUE)
p3=predict(model3, newdata=data, type=""response"", na.rm=TRUE)
t.test(p1~f)
t.test(p2~f)
t.test(p3~f)
</code></pre>

<p>Part of the output:  </p>

<pre><code>&gt; summary(model1)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.9756     0.3499  -5.647 1.64e-08 ***
A            -0.5898     0.2119  -2.784  0.00537 ** 

&gt; summary(model2)
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  8.354e-01  1.309e+00   0.638   0.5234  
B           -1.028e-04  5.122e-05  -2.007   0.0447 *

&gt; summary(model3)
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  1.254e+00  1.705e+00   0.735    0.462  
A            1.589e+00  9.743e-01   1.631    0.103  
B           -1.324e-04  7.333e-05  -1.805    0.071 .
A:B         -9.418e-05  4.632e-05  -2.033    0.042 *

&gt; t.test(p1~f)
t = -2.614, df = 11.83, p-value = 0.02286

&gt; t.test(p2~f)
t = -1.8702, df = 15.679, p-value = 0.08024

&gt; t.test(p3~f)
t = -4.9777, df = 17.344, p-value = 0.0001084
</code></pre>
"
"0.0852802865422442","0.0609333347776791"," 90953","<p><em>R in Action</em> (Kabacoff, 2011) suggests the following routine to test for overdispersion in a logistic regression:</p>

<p>Fit logistic regression using binomial distribution:</p>

<pre><code>model_binom &lt;- glm(Species==""versicolor"" ~ Sepal.Width,
                   family=binomial(), data=iris)
</code></pre>

<p>Fit logistic regression using quasibinomial distribution:</p>

<pre><code>model_overdispersed &lt;- glm(Species==""versicolor"" ~ Sepal.Width, 
                           family=quasibinomial(), data=iris)
</code></pre>

<p>Use chi-squared to test for overdispersion:</p>

<pre><code>pchisq(summary(model_overdispersed)$dispersion * model_binom$df.residual, 
       model_binom$df.residual, lower = F)
# [1] 0.7949171
</code></pre>

<p>Could somebody explain how and why the chi-squared distribution is being used to test for overdispersion here? The p-value is 0.79 - how does this show that overdispersion is not a problem in the binomial distribution model?</p>
"
"NaN","NaN"," 91184","<p>Suppose I fit a generalized mixed logistic model such like that:</p>

<pre><code>set.seed(2014)
require(lme4)
df&lt;-data.frame(id=rep(1:5, c(8,10,12,14,15)), 
               out=c(rbinom(8,1,0.1), rbinom(10,1,0.3),rbinom(12,1,0.1),rbinom(14,1,0.05),rbinom(15,1,0.1)),
               age=rnorm(59,50,10),
               gender=rbinom(59,1,0.5))
fit&lt;-glmer(out~age+gender+(1|id),data=df,binomial)
df$predicted&lt;-predict(fit,type=""response"")
df$pred.binary&lt;-with(df,ifelse(predicted&gt;=0.5,1,0))
apply(df[,c(2,6)],2,sum)
        out pred.binary 
          9           0 
</code></pre>

<p>What's the best way to predict a binary outcomes by accounting the random effects and with an optimized threshold based on the original data set?</p>
"
"0.0603022689155527","0.0574484989621426"," 91682","<p>Suppose I fitted a logistic model and get the estimates as well as their <code>vcov</code> matrix. I would realize this: draw length($\beta_s$) independent $\mathcal N(0,1)$ values to create a random vector $z$, then $\beta^*$=$\hat{\beta}+Az$ where $A$ is the upper triangular matrix of the Cholesky decomposition matrix ($\hat{V}=A'A$). How can I draw the $\beta^*$ using the <code>vcov</code> matrix? Here is an example:</p>

<pre><code>set.seed(123)
df &lt;- data.frame(y=rbinom(100,1,0.5),
                 x1=rnorm(100,10,2),
                 x2=rbinom(100,20,0.6))

fit &lt;- glm(y~x1+x2, data=df, family=""binomial"")
</code></pre>

<p>$\hat{\beta}$:</p>

<pre><code>coef(summary(fit))
            Estimate Std. Error  z value Pr(&gt;|z|)
(Intercept)   0.1482    1.57451  0.09413   0.9250
x1           -0.1710    0.10962 -1.55984   0.1188
x2            0.1181    0.09047  1.30567   0.1917
</code></pre>

<p>vcov matrix:</p>

<pre><code>vcov(fit)
(Intercept)      2.4791 -0.1225802 -0.1020612
x1              -0.1226  0.0120164  0.0003505
x2              -0.1021  0.0003505  0.0081844
</code></pre>

<p>Would somebody know how to draw new $\beta^*$ using the <code>coef</code> and <code>vcov</code>?</p>
"
"0.0301511344577764","0.0574484989621426"," 91747","<p>I have a logistic regression with data that are kind of like this:</p>

<pre><code>y &lt;- rep(c(""A"", ""A"", ""B""), each = 30)
x &lt;- c(     rep(1, 12), rep(2, 18), rep(3, 16), rep(4, 12), rep(5, 2),
            rep(1, 3), rep(2, 5), rep(3, 8), rep(4, 10), rep(5, 4)  )

da &lt;- data.frame(y = y, x = x)
table(da)
   x
y    1  2  3  4  5
  A 12 18 16 12  2
  B  3  5  8 10  4
</code></pre>

<p>I'd like to show that there are more A's than B's in <code>y</code> (instead of a Bernoulli with <code>p</code> = 0.5), even after controlling for <code>x</code>, so I fitted two logistic regression models and used an ANOVA to compare them.</p>

<pre><code>mlogis.x              &lt;- glm(y ~ x,     family = binomial, da)
mlogis.x.no_intercept &lt;- glm(y ~ x + 0, family = binomial, da)

summary(mlogis.x.no_intercept)
summary(mlogis.x)

anova(mlogis.x.no_intercept, mlogis.x, test = ""Chisq"")
</code></pre>

<p>I have a few questions:</p>

<ul>
<li>Does what I did make sense overall?</li>
<li>Is it okay to not have an intercept in the more basic model and then add it to the full(er) model?</li>
<li>The coefficient for <code>x</code> changes sign between the two models, how should I interpret this?</li>
</ul>
"
"NaN","NaN"," 91847","<p>Suppose I have a logistic regression model such like this:</p>

<pre><code>set.seed(123)
df&lt;-data.frame(
y=rbinom(100,1,0.5),
x1=rnorm(100,10,2),
x2=rbinom(100,20,0.6))

fit&lt;-glm(y~x1*x2,data=df,family=""binomial"")
coef(summary(fit))
               Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)  5.08314564 6.43692399  0.7896855 0.4297115
x1          -0.66691041 0.64071095 -1.0408912 0.2979260
x2          -0.28338654 0.51254819 -0.5528974 0.5803337
x1:x2        0.04037126 0.05100223  0.7915588 0.4286180
</code></pre>

<p>Does somebody know how to get the prediction matrix in a format like this:</p>

<pre><code>    intercept x1       x2  x1:x2
1   1        10.506637 10  105.06637
2   1        9.942906  17  169.02941
3   1        9.914259  10  99.14259
4   1        12.737205 11  140.10925
</code></pre>
"
"0.120604537831105","0.10053487318375"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.0852802865422442","0.0812444463702388"," 92575","<p>I'm pretty new to stats, so this may be dumb.
I've been running a bunch of models on randomly generated data to try and develop my understanding of type 1 error.</p>

<p>I've noticed that using <code>glm(family=binomial)</code> I get more type 1 errors than I should when giving a binomial input (a two-column matrix of success and failure). In the code below, the first loop generates a thousand logistic regression models for random data, but I get type 1 errors (p &lt; .05) about 15% of the time!</p>

<p>The second loop runs the a similar thing as a Bernoulli test (just single vector of zeros and ones in the y). Here I get what I want to see, about 50 type 1 errors per 1000 models.</p>

<p>Can anyone explain this to me? I see that if I change the possible values for the random numbers in my success/failure y-matrix (the variable I call range here) I can lower the type 1 errors, but I don't understand why. </p>

<p>This gives me about triple the number of type 1 errors that I expect.</p>

<pre><code>#Binomial glm
fit.p=c()
for(i in 1:1000){
    range=0:10
    y=matrix(sample(range,2000,replace=T),ncol=2,nrow=1000)
    x=rnorm(1000,100,50)      
    fit=glm(y~x,family=binomial(link='logit'))               
    fit.p[i]=anova(fit,test='Chisq')[2,5]                            
}
print(length(which(fit.p&lt;.05)))
</code></pre>

<p>This works fine. About 50 errors per 1000</p>

<pre><code>#Bernoulli glm
fit2.p=c()
for(i in 1:1000){
    y=sample(0:1,1000,replace=T)
    x=rnorm(1000,100,50)      
    fit2=glm(y~x,family=binomial(link='logit'))               
    fit2.p[i]=anova(fit2,test='Chisq')[2,5]                            
}
print(length(which(fit2.p&lt;.05)))
</code></pre>
"
"0.160806050441474","0.172345496886428"," 92737","<p>In my data, I have two treatment conditions with repeated measures for each subject. I would like to run a mixed logistic regression separately for each of my two conditions where my binary outcome DV (dependent variable) is regressed on my IV (independent variable) and also have a random slope and intercept fitted for each subject.</p>

<p>So, I run the following:</p>

<pre><code>modelT0 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D0, family = binomial)
modelT1 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D1, family = binomial)
</code></pre>

<p>In the above, D0 and D1 are data sets restricted to treatment conditions 0 and 1, respectively. What I would like to do is compare the estimated fixed effects coefficient on IV across conditions to see if it significantly changes. To do this, I pool D0 and D1 into a single data set, D, and create a treatment indicator that takes value 0 in D0 and 1 in D1. I then run:</p>

<pre><code>model &lt;- glmer(DV ~ IV + treatment + treatment:IV + (1 + treatment|subject:treatment) 
               + (0 + IV + treatment:IV|subject:treatment), data = D, family = binomial)
</code></pre>

<p>I should be able to look at the fixed effects coefficient on treatment:IV to get my answer, but the issue is that for whatever combination of random effects I seem to specify, the coefficients from the pooled regression are slightly different from the regressions specified by treatment. So for instance, the fixed effect coefficient on treatment:IV plus the one on IV in model is not equal to the coefficient on IV in model1.</p>

<p>Any idea about what I might be doing wrong or how to answer the question I have? Thanks!</p>

<p>EDIT:</p>

<p>As per Henrik's suggestion, I'm copying the random effects output of the models below:</p>

<p>summary(modelT0):</p>

<pre><code>    Random effects:
    Groups    Name        Variance  Std.Dev. 
    subject   (Intercept) 1.412e-07 0.0003758
    subject.1 IV          1.650e+00 1.2844341
</code></pre>

<p>summary(modelT1):</p>

<pre><code>    Random effects:
    Groups    Name        Variance Std.Dev.
    subject   (Intercept) 0.00378  0.06148 
    subject.1 IV          0.26398  0.51379 
</code></pre>

<p>summary(model):</p>

<pre><code>    Random effects:
    Groups              Name         Variance  Std.Dev. Corr 
    subject.treatment   (Intercept)  0.0005554 0.02357       
                        treatment    0.0066042 0.08127  -0.88
    subject.treatment.1 IV           1.6500112 1.28453       
                        IV:treatment 1.0278663 1.01384  -0.93
</code></pre>
"
"0.0492365963917331","0.0469065029820195"," 93390","<p>This question is about understanding the logistic regression output using R.  Here is my sample data frame:</p>

<pre><code>    Drugpairs             AdverseEvent  Y    N
1   Rebetol + Pegintron       Nausea   29 1006
2   Rebetol + Pegintron      Anaemia   21 1014
3   Rebetol + Pegintron     Vomiting   14 1021
4   Ribavirin + Pegasys       Nausea    5  238
5   Ribavirin + Pegasys      Anaemia   12  231
6   Ribavirin + Pegasys     Vomiting    1  242
7 Ribavirin + Pegintron       Nausea   15  479
8 Ribavirin + Pegintron      Anaemia    7  487
9 Ribavirin + Pegintron     Vomiting    9  485
</code></pre>

<p>This basically describes the number of times a particular drug pair has caused a medically adverse event. (<code>Y=yes, N=no</code>). I ran a logistic regression on this dataset in R using the following commands:</p>

<pre><code>mod.form    = ""cbind(Y,N) ~ Drugpairs * AdverseEvent""
glmhepa.out = glm(mod.form, family=binomial(logit), data=hepatitis.df)
</code></pre>

<p>The summary output was as follows (only showing the coefficients table):  </p>

<pre><code>                                                      Estimate Std. Error z value
(Intercept)                                          -3.8771     0.2205 -17.586
DrugpairsRibavirin + Pegasys                          0.9196     0.3691   2.491
DrugpairsRibavirin + Pegintron                       -0.3652     0.4399  -0.830
AdverseEventNausea                                    0.3307     0.2900   1.140
AdverseEventVomiting                                 -0.4123     0.3479  -1.185
DrugpairsRibavirin + Pegasys:AdverseEventNausea      -1.2360     0.6131  -2.016
DrugpairsRibavirin + Pegintron:AdverseEventNausea     0.4480     0.5457   0.821
DrugpairsRibavirin + Pegasys:AdverseEventVomiting    -2.1191     1.1013  -1.924
DrugpairsRibavirin + Pegintron:AdverseEventVomiting   0.6678     0.6157   1.085
</code></pre>

<p>I understand that the coefficients give probabilistic odds. I am curious however, as to why there are no coefficients for the <code>AdverseEventAnaemea</code> and also why is there no coefficient for any combination of the drugs and the adverse event anaemea? (The last 4 rows are the combination effects of drugs and adverse events)</p>
"
"0.0603022689155527","0.0574484989621426"," 94619","<p>Stata allows for fixed effects and random effects specification of the logistic regression through the <code>xtlogit fe</code> and <code>xtlogit re</code> commands accordingly. I was wondering what are the equivalent commands for these specifications in R.</p>

<p>The only similar specification I am aware of is the mixed effects logistic regression </p>

<pre><code>&gt; mymixedlogit &lt;- glmer(y ~ x1 + x2 +  x3 + (1 | x4), data = d, family = binomial)
</code></pre>

<p>but I am not sure whether this maps to any of the aforementioned commands.</p>
"
"0.180906806746658","0.172345496886428"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.14142135623731","0.134728672455117"," 95386","<p>What I have is a medical data set with several variables, all 0-1 variables. I want to make inference about them with logistic regression. I have a few problems:</p>

<ol>
<li><p>I have location variables for the disease. I was advised by my statistic advisor to put them in bins as follows: If it was solely in the right part of the organ then I would mark 1 in the column for right and similarily for left. However if it were in both places I marked in neither of the left and right column but marked one in column both. Using this approach I get error in R, numeric 0 1 error when I use glm in R and I think it is due to how these variables are constructed. Shouldn't I rather have just left and right variables and when we have the disease in both sides I should mark in left and right column and skip the both column and maybe introduce interaction term between left and right (that I would at least do in a linear model).</p></li>
<li><p>Using glm (family binomial for logistic regression) in R I was thinking how to find the best model describing some variable. I started with one usual approach with finding univarietly which variables had p-value less than $0.1$ in Fischer exact test. Then I included those variables in my model and started to delete them after which had the highest p-value. In most medical reasearches I have read when applying multivariate regression I see the usage of p-value $0.05$ but I have a feeling that it might be because of lack of understanding of the subject. When I ranked the model according to AIC and explored the best model I usually got variable with p-value around $0.1$. Which approach is preferably, is it justifyable to just cut of at p-value $0.05$ or should use AIC as an estimator of the best multivariate model? AIC does punish for extra variables and so it shouldnt give one too many variables.</p></li>
</ol>
"
"0.104446593573419","0.0995037190209989"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.134839972492648","0.128458748884677"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"0.0426401432711221","0.0406222231851194"," 96006","<p>Let's say I have 10 positives out of 1000 observations. I'd like to run <code>glm</code> on the 10 positives and a sample of 10 non-positives (so a total of 20 records in the dataset going into the analysis). To maintain ratios, the weighing scheme is set to 1 for positives and 99 for non-positives, and <code>weights = c(rep(1,10),rep(99,10))</code> if the 10 positives come first in the data, followed by the 10 sampled non-positives:</p>

<pre><code> glm( is_positive ~ . , data = D_sampled , family = binomial , weights = c(rep(1,10),rep(99,10)) )
</code></pre>

<p>Is this the right way to run an oversampled logistic with 'glm' (keeping in mind that the intercept will have to be corrected)? </p>
"
"0.112815214963553","0.0921225430717611"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.0953462589245592","0.090834052439095"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0852802865422442","0.0812444463702388"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"0.0852802865422442","0.0812444463702388"," 97834","<p>I ran a mixed model using lme4::glmer for a logistic regression and consistently got these warning messages. I noticed there are still regular results even so, but are they accurate estimates?</p>

<pre><code>    &gt; glmm.ms1&lt;-glmer(as.formula(paste(paste(y[1], x, sep=""~""), mix[1], sep=""+"")),
    +             data=rtf2,control=glmerControl(optimizer=""bobyqa"",
    +             optCtrl=list(maxfun=100000),family=binomial)
Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.8766 (tol = 0.001)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; coef(summary(glmm.ms1))
                       Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)           1.810e+00  6.558e-01   2.760464 5.772e-03
lepidays             -3.340e+00  2.770e-01 -12.059620 1.726e-33
cldaysbirth          -1.555e+00  5.224e-01  -2.975934 2.921e-03
rotaarm              -2.057e-01  3.209e-01  -0.641102 5.215e-01
cldaysbirth2         -3.072e-01  2.955e-01  -1.039510 2.986e-01
bfh2                 -1.043e+01  1.160e+03  -0.008996 9.928e-01
bfh3                  4.653e-01  4.806e-01   0.968103 3.330e-01
bfh4                  2.547e-01  4.994e-01   0.509966 6.101e-01
bfh5                  3.744e-01  9.926e-01   0.377213 7.060e-01
ruuska               -1.020e-01  5.928e-02  -1.720396 8.536e-02
genderMale           -4.008e-01  2.645e-01  -1.515453 1.297e-01
epiexlbf              6.078e-04  2.796e-03   0.217391 8.279e-01
haz.epi              -7.211e-02  1.373e-01  -0.525039 5.996e-01
cldaysbirth:rotaarm   6.928e-01  4.771e-01   1.452148 1.465e-01
rotaarm:cldaysbirth2  5.181e-01  3.352e-01   1.545527 1.222e-01
Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
</code></pre>

<p>Due to data's sensitivity, I can't post the whole process for generating same messages, but I would like to know how to handle these warnings. I don't think it's suitable to keep my eye blind here.</p>
"
"0.0953462589245592","0.090834052439095","102695","<p>I'm using R to run some logistic regression. My variables were continuous, but I used cut to bucket the data. Some particular buckets for these variables always result in dependent variable being equal to 1. As expcted, the coefficient estimate for this bucket is very high, but the p-value is also high. There are about ~90 observations in either these buckets, and around 800 total observations, so I don't think it's a problem of sample size. Also, this variable should not be related to other variables, which would naturally reduce their p-values.</p>

<p>Are there any other plausible explanations for the high p-value?</p>

<p>Example:</p>

<pre><code>myData &lt;- read.csv(""application.csv"", header = TRUE)
myData$FICO &lt;- cut(myData$FICO, c(0, 660, 680, 700, 720, 740, 780, Inf), right = FALSE)
myData$CLTV &lt;- cut(myData$CLTV, c(0, 70, 80, 90, 95, 100, 125, Inf), right = FALSE)
fit &lt;- glm(Denied ~ CLTV + FICO, data = myData, family=binomial())
</code></pre>

<p>Results are something like this:</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.53831  -0.77944  -0.62487   0.00027   2.09771  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -1.33630    0.23250  -5.747 9.06e-09 ***
CLTV(70,80]     -0.54961    0.34864  -1.576 0.114930    
CLTV(80,90]     -0.51413    0.31230  -1.646 0.099715 .  
CLTV(90,95]     -0.74648    0.37221  -2.006 0.044904 *  
CLTV(95,100]     0.38370    0.37709   1.018 0.308906    
CLTV(100,125]   -0.01554    0.25187  -0.062 0.950792    
CLTV(125,Inf]   18.49557  443.55550   0.042 0.966739    
FICO[0,660)     19.64884 3956.18034   0.005 0.996037    
FICO[660,680)    1.77008    0.47653   3.715 0.000204 ***
FICO[680,700)    0.98575    0.30859   3.194 0.001402 ** 
FICO[700,720)    1.31767    0.27166   4.850 1.23e-06 ***
FICO[720,740)    0.62720    0.29819   2.103 0.035434 *  
FICO[740,780)    0.31605    0.23369   1.352 0.176236    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1037.43  on 810  degrees of freedom
Residual deviance:  803.88  on 798  degrees of freedom
AIC: 829.88

Number of Fisher Scoring iterations: 16
</code></pre>

<p>FICO in the range [0, 660) and CLTV in the range (125, Inf] indeed always results in Denial = 1, so their coefficients are very large, but why are they also ""insignificant""?</p>
"
"0.121355975243384","0.102766999107742","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.113707048722992","0.108325928493652","102973","<p>I am using logistic regression to benchmark the performance of some students in different years. I created a scenario as below:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
benchmark.data &lt;- mydata[1:300,] # students form year 1990-1995 as benchmark
compare.data &lt;- mydata[301:400,] # students from year 1996

# logistic regression model created using benchmark student result
temp.glm &lt;- glm(admit~gre+gpa+rank,data=benchmark.data,family=""binomial"")

# using the regression model to predict how students in 1996 perform
compare.data[,""predict""] &lt;- predict(temp.glm,newdata=compare.data,type=""response"")

# making a threshold that if the predicted chance of admit &gt; 0.5, then it is asssumed that the student will get admitted
compare.data[,""predict_admit""] &lt;- ifelse(compare.data[,""predict""]&gt;0.5,1,0)
table(compare.data[,c(""admit"",""predict_admit"")])

#      predict_admit
# admit  0  1
#     0 59  6
#     1 26  9
</code></pre>

<p>From the table, it is seen that 15 students predicted to get admitted and actual number of students get admitted is 35, so the observed/expected ratio is <code>35/15=2.33</code>, as it is larger than <code>1</code>, so I will say that students in year 1996 is performing better than benchmark.</p>

<p>Can I draw my conclusion using the method mentioned above?</p>

<p>Besides, how should I set the threshold? Or should I <code>sum(compare.data[,""predict""])</code> and treat it as expected value?</p>

<h3>Update 1</h3>

<p>I tried and used ROC curve to determine the threshold:</p>

<pre><code>library(ROCR)
benchmark.data[,""predict""] &lt;- predict(temp.glm,newdata=benchmark.data,type=""response"")
preds &lt;- prediction(benchmark.data[,""predict""],as.numeric(benchmark.data[,""admit""]))
plot(performance(preds,""tpr"",""fpr""),print.cutoffs.at=seq(0,1,by=0.05))
</code></pre>

<p>And the charts suggests that threshold at 0.35 seems to give maximized sensitivity and specificity.</p>
"
"0.135400640077266","0.117266257455049","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.0852802865422442","0.0812444463702388","105633","<p>There are times when one might want to estimate a prevalence ratio or relative risk, in preference to an odds ratio, for data with binary outcomes - say, if the outcome in question isn't rare, so the RR ~ OR relationship doesn't hold.</p>

<p>I've implemented a model in R to do that, as follows:</p>

<pre><code>uni.out &lt;- glm(Death ~ onset, family= binomial(link=log), data=data)
</code></pre>

<p>But I'm continually getting convergence issues, even when providing starting values (such as the coefficient estimates pulled from a logistic regression), or turning up the number of allowed iterations. I've also tried <code>glm2</code> without any success.</p>

<p>The two ideas I have from here are to either fit a poisson model to the same data using a sandwich estimator for the variance, or fitting the model using MCMC and taking the standard error of the posterior (this is being used alongside multiple imputation, so I can't just report the posterior). The problem is, I have no idea how to implement either one of these in <code>R</code>, nor if they're the best solution.</p>

<p>Additionally, while using a model like:</p>

<pre><code>glm(Death ~ age, family= binomial(link=log),start=c(-3.15,0.03),data=data)
</code></pre>

<p>I'm regularly get an error message ""Error: cannot find valid starting values: please specify some"", but not always. What is generating this message?</p>
"
"0.121355975243384","0.128458748884677","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"0.105528970602217","0.114896997924285","108750","<p>I'm working with a data set like the following:</p>

<p><em>X</em> =</p>

<pre><code>c1 c2 c3 c4 c5 y
a  c  f  h  j  0
a  d  f  i  k  0
a  c  g  h  j  1
a  c  f  h  k  0
b  d  g  h  k  0
b  e  f  h  j  0
</code></pre>

<p>I'm trying to create a logistic regression model that is able to predict <code>y</code> (0 or 1) based on the features of <em>X</em>. </p>

<p>Info on <em>X</em>:<br>
The values of <code>c1</code>â€“<code>c5</code> are all factors.<br>
The features have a varying number of levels (e.g. <code>c1</code>: 2 levels, <code>c2</code>: 4 levels, <code>c3</code>: 3 levels, etc.)<br>
Approx 10% of <code>y</code> is 1.  </p>

<p>I've tried using SVM and GLM without any good result.</p>

<pre><code>model &lt;- svm(y ~ ., data = X)  
pred &lt;- predict(model, X)   
table(pred,X$y)

        y
pred    FALSE TRUE
FALSE   1332   113
TRUE       0     0
</code></pre>

<p>and</p>

<pre><code>model &lt;- glm(y ~ ., family=binomial(""logit""), data=X)  
pred &lt;- predict(model, X, type=""response"")  
table(pred,X$y)  

pred                   FALSE TRUE
4.2260288377431e-08        2    0
4.24333100181876e-08       1    0
...
0.706714407238236          1    1
0.736650629322038          0    1
</code></pre>

<p>I'm used to working with features with continuous values when creating a predictive model, but I don't really know how to tackle a problem with factors.<br>
What would you guys recommend for this type of problem? Changing from logistic regression to something else or using other functions besides GLM &amp; SVM?</p>
"
"0.11570838237598","0.122480611322833","109222","<p>I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.</p>

<p>D = 1 if disease B present, 0 otherwise</p>

<p>E = 1 if disease A present, 0 otherwise</p>

<p>I am also including in the model a measure of healthcare utilization. </p>

<p>F is a positive integer proportional to an individual's utilization of healthcare.</p>

<p>I am running the logistic regression model as such in R:</p>

<pre><code>glm(D ~ E + F, family = ""binomial"") 
</code></pre>

<p>Now, this works fine. </p>

<p>However, when I try to run conditional logistic regression, it gives me an error:</p>

<pre><code>library(survival)
clogit(D ~ E + F, strata(matched.pairs))
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge
</code></pre>

<p>I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)</p>

<p>This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. </p>

<p>I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). </p>

<p>I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?</p>

<p>Thank you all in advance.</p>
"
"0.0870388279778489","0.0995037190209989","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.0603022689155527","0.0574484989621426","110308","<p>I am working on fitting a GEE model to a multinomial logistic outcome using the R package <code>geepack</code>. My understanding is that the package uses <code>glm</code> to fit the formula. However, from what I can see, <code>glm</code> doesn't have a family for a multinomial outcome, just binomial. How do you go about fitting a multinomial logistic equation using <code>gee</code> in R? 
Thanks.</p>
"
"0.143125289466427","0.146840580956429","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.217422922601844","0.191200161882872","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.112815214963553","0.107476300250388","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.14191497503738","0.14646550861729","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.160806050441474","0.162770747059404","112670","<p>I'm working on a dataset with the following variables:</p>

<pre><code>Y:  a boolean telling whether the subject has experienced a seizure
ID:  the id of the subject
Sess:  the subject's session number (a subject has been observed multiple times)
X3:  numeric measurements of the subject's behavior during the session 
X4:  ""
X6:  ""
</code></pre>

<p>The idea is to see if the behavior measurements(X3,X4,X6) can be used predict the Seizure status (Y) in the population.  If there were just one session per subject I'd model the data with a logistic regression, but since each subject has multiple sessions the observations cannot be assumed independent.</p>

<p>It seems a GEE logistic model makes the most sense for this dataset, but I'm having trouble understanding the results when compared to a regular glm.  When introducing correlation structure to the GEE equation the signs of the coefficients are reversed, and the predicted values are opposite of what they would be with an independent correlation structure.</p>

<pre><code>library(""geepack"")
#regular logistic model
mod0 = glm(Y~X3+X4+X6, family=binomial(""logit""), data=mice)
#gee logistic model, independent correlation structure
mod1 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""indep"", scale.fix=T, waves=Sess, data=mice)
#gee logistic model, exchangeable correlation structure
mod2 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""exchangeable"", scale.fix=T, waves=Sess, data=mice)
</code></pre>

<p>As expected, the parameter estimates of the glm model(mod0) and the independent correlation gee model(mod1) are the same.  But when an exchangeable correlation structure(mod2) is introduced, the estimates are completely different and change sign.</p>

<pre><code>&gt; 
&gt; summary(mod1)                    

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""indep"", scale.fix = T)

 Coefficients:
            Estimate Std.err  Wald Pr(&gt;|W|)    
(Intercept)   -1.494   0.459 10.59  0.00114 ** 
X3            -2.377   0.626 14.42  0.00015 ***
X4             1.090   0.398  7.50  0.00619 ** 
X6             1.233   0.403  9.36  0.00222 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = independenceNumber of clusters:   28   Maximum cluster size: 4 
&gt; summary(mod2)

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""exchangeable"", 
    scale.fix = T)

 Coefficients:
            Estimate Std.err Wald Pr(&gt;|W|)   
(Intercept)  -0.8928  0.4255 4.40   0.0359 * 
X3            0.1059  0.0383 7.64   0.0057 **
X4           -0.0427  0.0299 2.04   0.1535   
X6           -0.0528  0.0213 6.16   0.0131 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
Number of clusters:   28   Maximum cluster size: 4 
</code></pre>

<p>My biggest concern is that the predicted Y's of the two GEE models show opposite trends, i.e. behaviors that would predict a positive seizure status in mod1, predict a negative seizure status in mod2.    </p>

<pre><code>plot(mod1$fitted, mod2$fitted)
</code></pre>

<p><img src=""http://i.stack.imgur.com/nAogH.png"" alt=""scatter plot of fitted values from mod1, mod2""></p>

<p>Also, what's with the estimated correlation parameter of alpha = 1.09 in mod2?  Unless I'm interpreting this wrong, shouldn't this always fall between -1 and 1?</p>

<pre><code>Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
</code></pre>

<p>It seems odd to me that the results are completely flipped depending on whether or not the observations are assumed to have dependence structure.  Can anyone else offer insight on this behavior?</p>

<p>Here is the data:</p>

<pre><code>&gt; dput(mice)
structure(list(Y = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), ID = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 
5L, 5L, 6L, 6L, 6L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 
11L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 
19L, 19L, 19L, 20L, 20L, 20L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 
23L, 23L, 23L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L, 26L, 
26L, 26L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L), Sess = c(2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 
4L, 3L, 4L, 3L, 4L, 3L, 4L, 3L, 4L, 1L, 3L, 4L, 1L, 3L, 4L, 1L, 
2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 1L, 2L, 3L, 1L, 2L, 
3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 
4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L), X3 = c(0.511015060794264, 
0.898356533693696, 0.798280430157052, 1.31144372617517, 0.829923189452201, 
0.289089506643144, -0.763028944257538, -0.944459588217789, -1.16474609928919, 
-0.182524267014845, -0.338967193889711, -0.896037509887988, 0.00426073081308205, 
0.0576592603165749, -1.4984737260339, 1.34752684212464, 0.106461438449047, 
-0.108424579472268, -2.85991432039569, -0.230115838261355, -1.54479536845993, 
-1.23693649938367, -1.53704616612456, -1.04825100254239, 0.142768659484482, 
0.28358135516745, -0.236302896321009, 0.708743856986942, -0.507503006972081, 
0.401550711842527, -0.16928449007327, 0.867816722958898, 0.487459858572122, 
1.35172112260613, 0.14742652989871, 0.742155288774287, 0.348552056119878, 
-0.82489952485408, 0.0366834636917457, -0.731010479377091, 0.979544093857171, 
1.4161996712129, 0.661035838980077, 0.600235250313596, -1.10872641912335, 
-0.212101744145196, -0.919575135240643, -0.813993077336991, -0.547068540188791, 
-0.0260198210967738, -0.0962240349391501, -0.251025721625606, 
0.894913664382802, -0.21993004239326, 0.0628839847717805, 1.77763503559622, 
0.718459471596243, 0.984412886705251, 0.77603470471174, 0.486187732642953, 
1.78012655684609, 1.31622243756713, 1.29635178661133, 0.427995111986702, 
0.993748401511881, 0.387623239882247, 0.42006794384777, -0.815889182132972, 
-0.897540332229183, -1.041943103505, 0.379425827374942, -1.00707718576756, 
-0.889182530787803, 0.148432805676879, -0.287928359114935, -0.747152636892815, 
-1.41003790431546, -0.611571256991109, -1.02569548477235, -1.02700056733181, 
-1.45808867127733, -1.47973458605138, 2.23643966561508, 2.69397876103083, 
0.81841473415516, 2.12167589051282, 0.267133799544379, -0.326215175076418, 
-1.08788244901967, -1.18733017947214), X4 = c(0.050598970482242, 
-0.0279694583060402, 0.999225143631274, 0.199872317584803, 0.779316284168575, 
-0.3552692229881, -0.232161792808608, -0.333479851296274, -0.748169603107953, 
-0.57785843363913, -0.480747933235349, -0.740466500603612, -0.618559437949564, 
-0.591541699294345, -0.538855647639331, 0.431376763414175, -0.327931008191724, 
-0.469416282917978, -0.659224551441466, -0.55285236403596, -0.637082867133913, 
-0.780321541069982, -0.40539035027884, -0.54024676972473, -0.185562290173831, 
0.054439450703482, 0.624097793456316, 0.24018937319873, -0.264194638773171, 
-0.389590537012038, -0.42771343162755, -0.738790918078674, -0.122411831542788, 
0.600119921164627, 0.0442597161778152, -0.0955011351192086, -0.521259643827527, 
-0.550050365103255, -0.504566887441653, -0.506571005423286, 0.523650149759566, 
0.341920916685254, -0.396343801985993, -0.366532239883921, -0.739276449002057, 
-0.56054127343218, -0.587601788901296, -0.56798329186843, -0.454937006653748, 
-0.672730639942183, -0.564864467446687, -0.678853515419629, 0.573072971483937, 
0.596973680548765, 0.0403978228634349, 1.93617633381248, 2.54301964691615, 
0.363075891004736, 0.0205658396444095, 0.560923287570261, 1.24212005971229, 
2.32518793880728, 2.69979166871713, 0.626868716830008, 0.219463581391793, 
0.236477261174534, -0.115429539698909, -0.49754151674106, -0.40827433350644, 
-0.0433283703798658, -0.578451015506926, -0.714208713291922, 
-0.802387726290423, -0.836794085697031, -0.471800405613954, -0.668030208971065, 
-0.610945789491312, -0.780838257914176, -0.411360572155088, -0.494388869332376, 
-0.63231547268951, -0.743853022088574, 4.90627675753856, 3.38455016460328, 
0.859445571488139, 2.42212262705776, -0.324759764820016, -0.541581784452693, 
-0.485324968098865, -0.770539730529603), X6 = c(0.0150287583709043, 
-0.151984283645294, -0.347950002037732, 0.379891135882966, 0.129107019894704, 
-0.314047917638528, -0.516381047940779, -0.751192211830495, -0.884460389494645, 
-0.462363867892961, -0.397583161539858, -0.559528880497725, -0.842987555132397, 
-0.922797893301111, -1.01175722882932, 0.32346425626624, -0.610909601293237, 
-0.605155952259822, -1.29840867980623, 0.0793710626694382, -0.806959976634144, 
-0.674523251142452, -0.960113466801064, -0.783836535852452, -0.0665321645536412, 
0.482235339656537, -0.319499220427413, -0.115345965733089, -0.30806448545927, 
0.251747727063608, -0.305013811851957, -0.931916656036151, 0.415032839884745, 
0.337184728843034, 0.0584335852357015, -0.0712185313438638, 0.78632612201797, 
0.490831043388539, 0.8902425262631, 0.160088439571744, 0.90343086944952, 
0.928495373121098, -0.389259569427933, -0.304578433259833, -0.593364448723133, 
-0.411333868741105, -0.882691663964141, -0.91208274239495, -0.708633954450382, 
-0.339396626779965, -0.420927315080057, -0.421383857909298, 0.407474183771483, 
0.629710767351175, -0.438726438495567, 2.40977730548689, 2.47250810430208, 
0.783562677342961, 0.781304150896319, 0.563221804716475, 1.85514126067038, 
1.30723846671955, 1.94869625911545, 0.876751836832149, 0.626629859119409, 
0.067113945916172, 3.54280776301513, 0.0082773667305384, -0.311414481848668, 
-0.732779325538588, -0.594477082903005, -1.0385239418576, -1.04141739541776, 
-0.99472304141247, -0.599659297534257, -0.804801224448196, -1.13096932958525, 
-0.641957537144073, -0.722959119516237, -0.671146043591047, -0.714432955420477, 
-0.766750949574034, 1.20993739830475, 2.79011376379402, 2.64532317075082, 
2.54251033029822, -0.539516582572252, -0.6419726544563, -0.663768795224503, 
-0.644829826467113)), .Names = c(""Y"", ""ID"", ""Sess"", ""X3"", ""X4"", 
""X6""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>
"
"NaN","NaN","114034","<p>I fit the logistic regression model for gender and drink for the data ihd using the following command </p>

<pre><code>model&lt;-glm(ihd~as.factor(gender)*as.factor(drink),Family='binomial',data=ihd)
</code></pre>

<p>My question is how can I get the estimated log odd-ratio for:  </p>

<ol>
<li>gender in non-drinker stratum, and</li>
<li>gender in drinker stratum</li>
</ol>
"
"0.112815214963553","0.0921225430717611","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.0426401432711221","0.0406222231851194","114583","<p>I want to simulate a binary response variable which depends on two normally distributed continuous variables, and I want to have more 1s than 0s in the response variable. I wonder how this can be done such that a logistic regression will not identify a significant interaction term.</p>

<p>My current approach in R looks like this:</p>

<pre><code>n = 1e5
x1 = rnorm(n)
x2 = rnorm(n)
y = x1+x2+rnorm(n)
y = ifelse(y &gt; 2, 1, 0)
df=data.frame(x1=x1, x2=x2, y=y)
summary(glm(y ~ x1*x2, df, family=binomial(logit)))$coefficients
</code></pre>

<p>This usually results in a highly significant interaction term, even though the y is just the sum of x1 and x2.
So how can I simulate a y which depends on both x1 and x2, but not on their interaction?</p>
"
"0.128564869306645","0.134728672455117","115188","<p>I am trying to look at whether 2 variables (one dichotomous categorical and one continuous) predict the occurrence of a dichotomous categorical dependent variable.</p>

<pre><code>dependent variable is LENIpos - 0 = no event, 1 = event
predictor variables are Hip.Prox.Femur - 0 = no hip fracture, 1 = hip fracture
                and     age (continuous)
</code></pre>

<p>Both predictor variables have significant p values in separate chi square test and Mann Whitney U test respectively.</p>

<p>When I run a logistic regression <code>glm(LENIpos ~ age + Hip.Prox.Femur, family = ""binomial)</code>, the variables come out as not significant. (1)</p>

<p>However, when I run the logistic regression with interactions <code>glm(LENIpos ~ age * Hip.Prox.Femur...)</code> (2), they are no both significant.  How is this to be interpreted?</p>

<p>Example R outputs:</p>

<p>(1)</p>

<pre><code>Call: glm(formula = LENIpos ~ age + Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -0.9346  -0.7826  -0.4952  -0.3374   2.1897  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)              -3.46888    1.00693  -3.445 0.000571 ***
age                       0.02122    0.01519   1.397 0.162535  
Hip.Prox.Femhip fracture  0.72410    0.57790   1.253 0.210212    

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 135.48  on 149  degrees of freedom
AIC: 141.48

Number of Fisher Scoring iterations: 5
</code></pre>

<p>(2)</p>

<pre><code>glm(formula = LENIpos ~ age * Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
        Min       1Q   Median       3Q      Max  
    -1.0364  -0.7815  -0.5373  -0.1761   2.3443  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)                  -5.89984    1.98289  -2.975  0.00293 **
age                           0.05851    0.02818   2.076  0.03788 * 
Hip.Prox.Femhip fracture      5.04990    2.46269   2.051  0.04031 * 
age:Hip.Prox.Femhip fracture -0.06058    0.03339  -1.814  0.06965 . 


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 131.82  on 148  degrees of freedom
AIC: 139.82

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.128564869306645","0.122480611322833","115219","<p><strong><em>Imagine the situation:</em></strong> Mythical Seafolk use holes in the seabed as their burrows. Each hole has two parameters - diameter and depth. <strong>Majority of holes are unoccupied</strong> due to their surplus (n = 235). Occupied holes (n = 15) are (generally) expected to be much deeper and with larger diameter than random.</p>

<pre><code># generate data
set.seed(1234)
x &lt;- runif(250, min=0, max=10)
y &lt;- runif(250, min=0, max=10)
rbPal   &lt;- colorRampPalette(c(""deepskyblue"",""darkblue""))
my.data &lt;- data.frame(x_coor = x, y_coor = y,
                      diameter = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      depth = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      usage = rep(c(1,0), times = c(15, 235)))
my.data$col &lt;- rbPal(10)[as.numeric(cut(my.data$depth,breaks = 10))]

# look at the situation
# occupied holes are marked by red circles
plot(my.data$x, my.data$y, cex = my.data$diameter, col = my.data$col, pch = 20)
grid(5, 5, lwd = 0.75, lty = 2, col = ""grey"") 
points(my.data$x[1:25], my.data$y[1:25], pch = 1, cex = 2,
       col = ""red"", lwd = 1.75)
</code></pre>

<p><img src=""http://i.stack.imgur.com/8O1Xg.png"" alt=""enter image description here""></p>

<p>Although this analysis seem to be rather straightforward, <strong>I have some doubts about comparing unequal samples (15 vs 235)</strong>. Such problem do not occur in other <em>Seafolk burrow selection studies</em> because <strong>mapping of seabed is very expensive and time demanding</strong> and when researchers find 10 used holes, they continue to map the seabed only till they get find additonal 10 nonused holes (which are most probably located in close vicinity of used holes). Due to this
data collection approach the sample sizes of compared groups are usually almost equal.
<strong>However, we were able to find ALL holes in seabed - which can be now considered as a ""handicap"".</strong></p>

<p><strong><em>We do not want to do random sampling of 15 holes from 235 to obtain equal sizes (15 vs 15). Why to throw away such large amount of data!? Is there any solution/approach to this situation which is able to fully embrace such unique dataset?</em></strong></p>

<p><em>I have in mind two possible ways (please comment or add others):</em></p>

<p><strong>1.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, each of these reduced groups will be compared with sample of occupied holes by binomial logistic regression. By this I will obtain 50 logistic curves which will
be ""averaged"" into just one curve - final model.</p>

<p><strong>2.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, from each of these reduced groups (and from sample of used holes also) we gain (mean, sd, sample size). Thus, we will have six numbers for each comparison and we calculate effect size - standardized mean difference (SMD). Effect sizes will be inserted into meta-analytic model and tested if there is a heterogeneity present among effect sizes (<em>to see if the ""randomnes"" of creating unused holes sample has the significant impact on test result</em>).</p>
"
"0.147709789175199","0.128992883200554","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.0870388279778489","0.0829197658508324","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.0852802865422442","0.0812444463702388","117192","<p>I would like to get a covariance matrix of fitted probabilities for a logistic regression model in R. I would like to do this because I want to find the variance of the difference between the two fitted probabilities ($\hat{p}_1 - \hat{p}_2$).</p>

<p>Here is my attempt:</p>

<pre><code>x&lt;-rnorm(10,10,10)
y&lt;-x+rnorm(10,0,15)
z&lt;-round(runif(10,0,1))

m1&lt;-glm(z~x+y,family=binomial(link = ""logit""))

predict(m1,newdata=data.frame(x=c(1,0),y=c(1,1)),se.fit=TRUE,vcov=TRUE,type=""response"")
</code></pre>

<p>This gives me the standard errors of the fitted probabilities but not the covariance. </p>

<p>I am aware of the delta method to find the distribution of a function of a normal distribution but I would really like to avoid using the delta method if possible because my actual code needs to be extremely flexible. It'll be difficult to implement the delta method properly in my actual situation.</p>
"
"0.0904534033733291","0.0861727484432139","117450","<p>I have a confusing situation where I have strongly conflicting results from two ways of analyzing my simple data. I measure two binary variables from each participant, AestheticOnly and ChoiceVA. I want to know if AestheticOnly depends on ChoiceVA and whether this relation is different in two different experiments. Here is my participant count data:</p>

<pre><code>Experiment 1
                 AestheticOnly
                 0   1  All
ChoiceVA A      35   6   41
         V      20  13   33
         All    55  19   74

Experiment 2
                 AestheticOnly
                 0   1  All
ChoiceVA A      12  10   22
         V      31  11   42
         All    43  21   64
</code></pre>

<p>I run a logistic regression where AestheticOnly is modelled by ChoiceVA, Experiment, and the interaction:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ ChoiceVA*Experiment, data = d, family=binomial)
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ ChoiceVA * Experiment, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1010  -0.7793  -0.5625   1.2557   1.9605  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -3.3449     0.9820  -3.406 0.000659 ***
ChoiceVAV              3.5194     1.2630   2.787 0.005327 ** 
Experiment             1.5813     0.6153   2.570 0.010170 *  
ChoiceVAV:Experiment  -2.1866     0.7929  -2.758 0.005820 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apparently all factors are significant. But, this just doesn't make sense to me. For example, looking at the main effect of experiment should be equivalent to performing a Fisher's Exact test comparing 55 and 19 with 43 and 21 (bottom lines of each table). This is obviously not significant (p=.452). So why does the regression model give such a different result? Any help much appreciated.</p>
"
"0.135400640077266","0.128992883200554","117593","<p>This is a really simple problem I am having, yet for the life of me I can't find a solution searching around. In theory I can simply recode the data, but that is an extreme solution I would rather not use if I don't have to. </p>

<p>I am simply trying to do a logistic regression with an ordered factor as my predictor. For a toy data set, consider:</p>

<pre><code>  radiation leukemia other total
1         0       13   378   391
2       1-9        5   200   205
3     10-49        5   151   156
4     50-99        3    47    50
5   100-199        4    31    35
6       200       18    33    51
</code></pre>

<p>I want to execute the following:</p>

<pre><code>glm(cbind(leukemia,other)~radiation,data=leuk,family=binomial(""logit""))
</code></pre>

<p>That is, leukemia are the ""successes"" and other are the ""failures"". Basically, trying to predict dose-response relationship between radiation and the proportional mortality rates for leukemia. However, this model is oversaturated:</p>

<pre><code>Call:  glm(formula = cbind(leukemia, other) ~ radiation, family = binomial(""logit""), 
    data = leuk)

Coefficients:
     (Intercept)      radiation1-9    radiation10-49  radiation100-199  
         -3.3699           -0.3189           -0.0379            1.3223  
    radiation200    radiation50-99  
          2.7638            0.6184  

Degrees of Freedom: 5 Total (i.e. Null);  0 Residual
Null Deviance:      54.35 
Residual Deviance: -3.331e-15   AIC: 33.67
</code></pre>

<p>I don't want each level of radiation as a factor to be its own predictor variable; that makes no sense, especially when you only have a small number of data points (note, this isn't actually the real data I am using, this is just a toy example that is similar). In any case, how do I force R to simply consider the factor radiation as a single variable with multiple levels? For example, if I do the following:</p>

<pre><code>x&lt;-c(0,1,2,3,4,5)
glm(cbind(leukemia,other)~x,data=leuk,family=binomial(""logit""))

Call:  glm(formula = cbind(leukemia, other) ~ x, family = binomial(""logit""), 
    data = leuk)

Coefficients:
(Intercept)            x  
    -3.9116       0.5731  

Degrees of Freedom: 5 Total (i.e. Null);  4 Residual
Null Deviance:      54.35 
Residual Deviance: 10.18        AIC: 35.84
</code></pre>

<p>This is more in line with what I want. But I am nervous about using that x variable in the regression for fear of changing the interpretation of the results. Similarly, I'd prefer to avoid an irritating system of dummy variables. </p>

<p>How do I go about doing this? Or is there a better workaround altogether for studying this type of relationship that I am not considering?</p>
"
"0.130088727117598","0.123932353445399","117631","<p>I measure two binary responses from each participant (ChoiceVA = V or A, AestheticOnly = 0 or 1). There are two experiments (between-participant). I want to test the following hypotheses:</p>

<p>AestheticOnly depends on Experiment (main effect)
AestheticOnly depends on ChoiceVA (main effect)
The way AestheticOnly depends on Experiment depends on ChoiceVA (interaction)</p>

<p>Here is my data. The first number in each cell is the proportion of participants scoring 1 for AestheticOnly, and the second number is the n for participants in that cell.</p>

<pre><code>                         ChoiceVA               
                        A       V     All

Experiment  1      0.1463  0.3939  0.2568
                       41      33      74

            2      0.4545  0.2619  0.3281
                       22      42      64

            All    0.2540  0.3200  0.2899
                       63      75     138
</code></pre>

<p>Just from looking at the data it is pretty obvious that neither main effect is significant (e.g. for ChoiceVA, bottom row, .25 of 63 participants is not significantly different from .32 of 75 participants). In my naivity I thought perhaps I could test these hypotheses with a straightforward binary logistic regression:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ Experiment+ChoiceVA+Experiment*ChoiceVA, data = d, family=binomial )
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ Experiment + ChoiceVA + Experiment *
    ChoiceVA, family = binomial, data = d)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.1010  -0.7793  -0.5625   1.2557   1.9605 

Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)            -1.7636     0.4419  -3.991 6.57e-05 ***
Experiment2             1.5813     0.6153   2.570  0.01017 * 
ChoiceVAV               1.3328     0.5676   2.348  0.01887 * 
Experiment2:ChoiceVAV  -2.1866     0.7929  -2.758  0.00582 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Clearly, the main effects are not being tested here in the way I hoped. I believe that this model, in testing main effects, rather than testing e.g. ChoiceVA=A against ChoiceVA=V across both levels of Experiment, is confining itself to that comparison only when Experiment=1. Can a model be constructed that instead tests the main effects in the way I would like?</p>

<p>This is related to a previous question (<a href=""http://stats.stackexchange.com/questions/117450/logistic-regression-gives-very-different-result-to-fishers-exact-test-why"">Logistic regression gives very different result to Fisher&#39;s exact test - why?</a>), but when I asked it I understand this even worse than I do now and consequently the question was so unclear that I need to start again.</p>
"
"0.165468060798406","0.167489716939662","118141","<p>I'm trying to fit a logistic curve to cumulative data, derived from satellite imagery. Previously, I have point observation data which were either 0s or 1s. Os being 'forest' and 1s being 'non-forest'. These point observations existed for multiple images/dates. So I had one csv file with 'observation date' in once column and 'state' in another. Weights were also included, but these aren't relevant here. </p>

<p>This was the code I used:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Owner\\Desktop\\BV\\Deforestation_Analysis\\Ambaro_Ambanja\\Analysis\\CDM_Table_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%m/%d/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,weights=Weight,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>Now, rather than having multiple 0s and 1s for each date, I have simply number of non-forest pixels (and no weight). Here is the data I have:</p>

<p><img src=""http://i.stack.imgur.com/oFFIe.jpg"" alt=""enter image description here""></p>

<p>The 'State' field is literally the portion of non-forest pixels (18.6% of pixels were non-forest in 2014)</p>

<p>I tried runnning the following adaption of the original script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>I fully expected it to fail, because the data is no longer binomial. But while it did throw up a warning ('In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!'), the coefficients it spat out formed a curve which looked perfect. But I wasn't overly confident in this hash.</p>

<p>I've read that you can still use the binomial glm family if you feed R with a table containing successes (non-forest) and failures. So I came up with this adapted data:</p>

<p><img src=""http://i.stack.imgur.com/CMSd3.jpg"" alt=""enter image description here""></p>

<p>and the following adapted script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=cbind(Deforested, Total-Deforested)~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>It ran fine with no errors, but the trend it generated doesn't fit the data anywhere near as well as the hashed version:</p>

<p><img src=""http://i.stack.imgur.com/FXL6I.jpg"" alt=""Blue line is hased version; Grey line is adapted script; Red points are the data I&#39;m using to fit the model""></p>

<p>The blue line is hased version; grey line is adapted script; red points are the data I'm using to fit the model.</p>

<p>Why does the adapted version fit the point worse than the hashed version? Is R so clever that it just uses my fractional values how I want it to in the glm(family=binomial)?</p>

<p>Any advice greatly appreciated! Am not happy with how I got the blue trend and this work is very important to our study.</p>

<p>THANK YOU!!</p>
"
"NaN","NaN","120254","<p>I'm familiar with (some) approaches to evaluating the fit (or accuracy) of a binary (logistic) model (e.g. AUC). Are there methods/approaches that are particularly well-suited for a binomial (success vs. failure) model? </p>

<p>If the suggestion is to use a variant of a (pseudo-) R-square, what are recommended approaches?  I can think of a few, using logit vs. response-scale predictions and with and without weighting by # of subjects, but am unsure of their appropriateness: </p>

<p>[<code>s</code> = # of successes, <code>f</code> = # of failures]</p>

<ol>
<li><p><code>summary(lm(predict(model)~I(s/(s+f))))$r.square</code></p></li>
<li><p><code>summary(lm(predict(model,type='response')~I(s/(s+f))))$r.square</code></p></li>
<li><p><code>summary(lm(predict(model)~I(s/(s+f)),weights=s+f))$r.square</code></p></li>
<li><p><code>summary(lm(predict(model,type='response')~I(s/(s+f)),weights=s+f))$r.square</code></p></li>
<li><p><code>1-var(residuals(model))/(var(s/(s+f)))</code></p></li>
</ol>
"
"0.192335662301631","0.199166835294659","121120","<p>I am trying to create a logistic regression model with mgcv::gam
with what I think is a simple decision boundary, but the model
I build performs very poorly.  A local regression model built
using locfit::locfit on the same data finds the boundary very easily.
I want to add additional parametric regressors to my real-life model, so
I do not want to switch to a purely local regression.</p>

<p>I want to understand why GAM is having trouble fitting the data,
and whether there was ways of specifying the smooths that
can perform better.</p>

<p>Here's a simplified, reproducible example:</p>

<p>Ground truth is 1 = point lies within the unit circle, 0 if outside</p>

<p>e.g. z = 1 if sqrt(x^2 + y^2) &lt;= 1, 0 otherwise</p>

<p>The observed data is noisy, with both false positives and false negatives</p>

<p>Construct a logistic regression to predict whether a point
is inside the circle or not, based on the point's Cartesian
coordinates.</p>

<p>Local regression can find the boundary well (50% probability contour
is very close to the unit circle), but a logistic GAM consistently 
overestimates the size of the circle for the same probability band.</p>

<pre><code>library(ggplot2)
library(locfit)
library(mgcv)
library(plotrix)

set.seed(0)
radius &lt;- 1 # actual boundary
n &lt;- 10000 # data points
jit &lt;- 0.5 # noise factor

# Simulate random data, add polar coordinates
df &lt;- data.frame(x=runif(n,-3,3), y=runif(n,-3,3))
df$r &lt;- with(df, sqrt(x^2+y^2))
    df$theta &lt;- with(df, atan(y/x))

# Noisy indicator for inside the boundary
df$inside &lt;- with(df, ifelse(r &lt; radius + runif(nrow(df),-jit,jit),1,0))

# Plot data, shows ragged edge
(ggplot(df, aes(x=x, y=y, color=inside)) + geom_point() + coord_fixed() + xlim(-4,4) + ylim(-4,4))
</code></pre>

<p><img src=""http://i.stack.imgur.com/BfzkT.png"" alt=""enter image description here"">    </p>

<pre><code>### Model boundary condition using x,y coordinates

### local regression finds the boundary pretty accurately
m.locfit &lt;- locfit(inside ~ lp(x,y, nn=0.3), data=df, family=""binomial"")
plot(m.locfit, asp=1, xlim=c(-2,-2,2,2))
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/fy6z3.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM fits very poorly, also tried with fx=TRUE but didn't help
m.gam &lt;- gam(inside ~ s(x,y), data=df, family=binomial)
plot(m.gam, trans=plogis, se=FALSE, rug=FALSE)
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/ObeIm.png"" alt=""enter image description here"">  </p>

<pre><code>### gam.check doesn't indicate a problem with the model itself
gam.check(m.gam)

Method: UBRE   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [5.41668e-10,5.41668e-10]
(score -0.815746 &amp; scale 1).
Hessian positive definite, eigenvalue range [0.0002169789,0.0002169789].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

           k'    edf k-index p-value
s(x,y) 29.000 13.795   0.973    0.08

#### Try using polar coordinates

### Again, locfit works well
m.locfit2 &lt;- locfit(inside ~ lp(r, nn=0.3), data=df, family=""binomial"")
plot(m.locfit2)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9zr73.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM misses again
m.gam2 &lt;- gam(inside ~ s(r, k=50), data=df, family=binomial)
plot(m.gam2, se=FALSE, rug=FALSE, trans=plogis)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/55XiQ.png"" alt=""enter image description here"">    </p>

<pre><code>### Can also plot gam on link scale for alternate view
plot(m.gam2, se=FALSE, rug=FALSE)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/voMP4.png"" alt=""enter image description here"">    </p>

<pre><code>gam.check(m.gam2)

Method: UBRE   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-3.29203e-08,-3.29203e-08]
(score -0.8240065 &amp; scale 1).
Hessian positive definite, eigenvalue range [7.290233e-05,7.290233e-05].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

         k'    edf k-index p-value
s(r) 49.000 10.537   0.979    0.06
</code></pre>
"
"0.120604537831105","0.114896997924285","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"NaN","NaN","122508","<p>I am trying to replicate a multilevel logistic analysis which uses a dyadic time series data set and R. As for my part, I am using the same data set but Stata.</p>

<p>The original syntax has the following specifications:</p>

<p><code>lmer(depvar ~ indepvar_n (1|country_a)+(1|country_b)+(1|year), data=d, family='binomial'))</code></p>

<p>My Stata syntax currently looks like this:</p>

<p><code>xtmelogit depvar indepvars || country_a:country_b</code></p>

<p>Obviously, somehow I still have to add the <strong>year</strong> variable, but I have troubles in doing so. When excluding years from both equations I produce identical results. However, this is pointless since it is a different analysis.</p>

<p>In which way do I have to modify the Stata syntax to produce the same results which are computed by the R command? I cannot simply add :year after country_b since this is not possible. What am I missing?</p>
"
"0.0953462589245592","0.090834052439095","122593","<p>I wanted to check whether the level of satisfaction relates to the level of support to the value of democracy.
Dependent variable (support) is binary variable (Good/Bad) and independent variable is ordinary variable (level of satisfaction).
After doing binary logistic regression, I got this â€œstrangeâ€ figure. Is this result correct? Or how can I fix it?</p>

<pre><code>satisfaction &lt;- data_american$V23c
    support &lt;- data_american$V130b
dat=as.data.frame(cbind(satisfaction,suport)) 
library(ggplot2)
ggplot(dat, aes(x=satisfaction, y=support)) + geom_point() + 
  stat_smooth(method=""glm"", family=""binomial"", se=FALSE)
</code></pre>

<p>Here is a part of data:</p>

<pre><code>   satisfaction support
1             7       1
2             8       1
3             8       1
4             8       1
5            10       1
6             6       1
7             7       1
8             7       1
9             7       1
10            8       0
11            8       1
12            7       1
13            7       0
14            1       1
15            7       1
16            8       1
17            6       1
18            7       1
19            8       1
20            8       1
</code></pre>

<p><img src=""http://i.stack.imgur.com/CgjzY.png"" alt=""results I got""></p>

<p>[Added 1]
Based on the first answer, I got this results:</p>

<pre><code>&gt; satisfaction_j &lt;- jitter(satisfaction)
&gt; chisq.test(table(satisfaction_j,support))

    Pearson's Chi-squared test

data:  table(satisfaction_j, support)
X-squared = 2158, df = 2157, p-value = 0.4899

&gt; t.test(satisfaction_j~support)

    Welch Two Sample t-test

data:  satisfaction_j by support
t = -2.7775, df = 459.931, p-value = 0.005703
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.57390716 -0.09829989
sample estimates:
mean in group 0 mean in group 1 
       7.164214        7.500317 
</code></pre>
"
"0.0852802865422442","0.108325928493652","123210","<p>I am doing a logistic regression in R, where I am modeling how potholes and weather correlate to accidents. When I run a logistic regression, I get the message ""Algorithm does not converge""</p>

<p>The problem I think I have is that I have 24,000 accidents with only 350 potholes related to these accidents. Is this to small of a sample size?</p>

<p>The other possible issue I thought of, is that when I look at sample logistic regressions, the outcome is either zero or one, but the only outcome I have is the outcome of one, or accident in my case. I do not have any non accident data in my set, could this be what is causing the problem? </p>

<p>I will attach my current code and its output.</p>

<pre><code>require(ggplot2)
require(sandwich)
require(msm)
mydata &lt;- read.csv(""C:\\Users\\myname\\downloads\\logreg1.csv"")
## view the first few rows of the data
head(mydata)
summary(mydata)
mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")

&gt; head(mydata)
      Date Pothole Rain Snow Accident
1 1/1/2012       0    0    0        1
2 1/1/2012       0    0    0        1
3 1/1/2012       0    0    0        1
4 1/1/2012       0    0    0        1
5 1/1/2012       0    0    0        1
6 1/1/2012       0    0    0        1
&gt; summary(mydata)
         Date          Pothole            Rain              Snow            Accident
 1/8/2014  :   87   Min.   :0.0000   Min.   :0.00000   Min.   : 0.0000   Min.   :1  
 1/30/2013 :   82   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:1  
 3/21/2013 :   77   Median :0.0000   Median :0.00000   Median : 0.0000   Median :1  
 12/21/2012:   76   Mean   :0.0173   Mean   :0.08077   Mean   : 0.1129   Mean   :1  
 3/10/2013 :   66   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:1  
 12/13/2013:   59   Max.   :8.0000   Max.   :3.32000   Max.   :11.1000   Max.   :1  
 (Other)   :23606                                                                   
&gt; mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")
Warning message:
glm.fit: algorithm did not converge
</code></pre>
"
"0.14142135623731","0.134728672455117","123498","<p>I would like to generate a confidence interval for predicted vs actual rates.</p>

<p>I am auditing my group of anaesthetists (aka anesthesiologists) to see how we compare on a number of potentially preventable complications (eg post-operative nausea, severe pain, hypothermia).</p>

<p>I have 20000 surgical operation records and I can make a GLM to make a ""case-mix adjusted risk"" (using age, gender, type of surgery, duration of surgery as risk factors) and thus <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">generate a risk</a> for each patient.</p>

<p>I can then aggregate the risk and actual per clinician I can generate an actual and predicted rate for each complication. I can make a confidence interval for my actual - but it seems a bit simplistic to just test to see if the confidence interval on the actual rate includes the rate generated from summing the glm-predicted risks.</p>

<p><a href=""http://stats.stackexchange.com/questions/7344/how-to-graphically-compare-predicted-and-actual-values-from-multivariate-regress"">This question</a> has some pointers to a package but I am hoping for some more specific suggestions.</p>

<p>To clarify what I have already (using ""requirement for pain protocol"" as an example):</p>

<pre><code># make model (dependent variable has values 1/0)
model.pp = glm(
pain_protocol1 ~
age + log_age + age2 + inv_age
+ op_time + log_op_time + op_time2
+ gender
+ category
+ thimble,
family = ""binomial"",
data=d4)
# calculate predicted PACU time and then difference between predicted and actual:
d4$pred_pp = predict(model.pp, newdata=d4, type=""response"", na.action=""na.pass"")

d4$extra_pp = d4$pain_protocol1 - d4$pred_pp
# aggregate deviation from predicted rate
ppr_pa &lt;- aggregate(extra_pp ~ adult_anaesthetist, data=d4, FUN=mean)
barplot(ppr_pa$extra_pp, name=ppr_pa$adult_anaesthetist,las=2) 
</code></pre>

<p>So I can make this plot for my colleagues, showing the variation we have in how much pain our patients experience in the ""post anaesthesia care unit"". These variations are great enough so that they definitely represent a material difference in patient experience, and most of the difference will also be unlikely to be variation due to chance (ie ""statistically significant""). However, as I examine smaller subgroups and other complications that are less frequent it would be good to be able to calculate confidence intervals.</p>

<p><img src=""http://i.stack.imgur.com/FFe8W.png"" alt=""bar plot illustrating difference between predicted and actual rates of &quot;needing pain protocol&quot;""></p>

<p>Note that each clinician has a different number of cases, and each clinician is given a bird code-name for anonymity.</p>
"
"0.112815214963553","0.107476300250388","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.134839972492648","0.128458748884677","125130","<p>I realize that a similar question to this has been asked, but it was not ultimately resolved. I have tried the suggestions posted to that question <a href=""https://stackoverflow.com/questions/23347467/is-there-any-way-to-fit-a-glm-so-that-all-levels-are-included-i-e-no-refer"">here</a>, but have had no success. I am using the following code:       </p>

<pre><code>allinfa4.exp = glm(survive ~ year + julianvisit + class + sitedist + roaddist
+ ngwdist, family = binomial(logexp(alldata$expos)), data=alldata)
summary(allinfa4.exp)

 Call:
glm(formula = survive ~ year + julianvisit + class + sitedist + 
roaddist + ngwdist, family = binomial(logexp(alldata$expos)), 
data = alldata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.6435   0.3477   0.4164   0.4960   0.9488  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  4.458e+00  7.117e-01   6.265 3.74e-10 ***
year2013     3.680e-01  1.862e-01   1.976  0.04819 *  
year2014     2.136e-02  1.802e-01   0.119  0.90564    
julianvisit -5.714e-03  3.890e-03  -1.469  0.14192    
classb       2.863e-02  2.194e-01   0.131  0.89615    
classc      -2.394e-01  2.277e-01  -1.051  0.29304    
classd      -1.868e-01  2.479e-01  -0.754  0.45109    
classe      -4.500e-01  2.076e-01  -2.167  0.03021 *  
classf      -5.728e-01  2.005e-01  -2.858  0.00427 ** 
classg      -8.495e-01  3.554e-01  -2.390  0.01684 *  
classh      -1.858e-01  2.224e-01  -0.835  0.40351    
classi      -3.196e-01  4.417e-01  -0.724  0.46932    
sitedist    -2.607e-04  5.043e-04  -0.517  0.60520    
roaddist     6.768e-05  4.311e-04   0.157  0.87525    
ngwdist     -5.751e-05  9.456e-05  -0.608  0.54306
</code></pre>

<p>The main thing to note here is that I have two categorical variables, <code>year</code> and <code>class</code>, and R has combined the first level of each (2012 and class a) into a reference level intercept term. Not only do I need to know the intercept term for these levels individually, but I also need to know the base intercept terms itself (beta0), just as SAS produces. </p>

<p>I have tried changing the contrasts and deviation coding to accomplish this, but although doing so allows me to extract different levels, it changes the way they are calculated and still does not produce beta0. I've also tried adding +0 and -1, but this also does not provide what I need. Is what I'm trying to do simply impossible in R? It may seem like a strange request, but beta0 is necessary to convert the results of logistic exposure (special kind of logistic regression for nest-survival data) to daily survival rates. Any help would be hugely appreciated. Thanks!</p>

<p>Here is an example of SAS output I want to emulate (taken from a similar analysis done by my lab mate) :</p>

<pre><code>Parameter Estimates
Parameter   Estimate    Standard Error  DF  t Value Pr &gt; |t|              
beta0       7.8404      2.8479          19  2.75    0.0127  
NT         -3.8786      1.8831          19  -2.06   0.0534  
bgdensity  -0.1127      0.1614          19  -0.70   0.4935  
nwh         1.3466      1.4625          19  0.92    0.3687      
NRD        -2.6981      1.9496          19  -1.38   0.1824      
NAGW       -0.4898      2.2518          19  -0.22   0.8301      
</code></pre>
"
"0.0426401432711221","0.0406222231851194","126150","<p>I need some help answering a homework question, </p>

<p><a href=""https://www.dropbox.com/s/xs1gflfl063m4lo/IMG_20141201_123018~2.jpg?dl=0"" rel=""nofollow"">Question</a></p>

<p>I have entered the data into R using assignments    </p>

<pre><code>data &lt;- data.frame(category=c(1:8),
                   obese =c(597,380,665,524,1014,365,942,552),
                   number = c(2346,1659,2576,1732,1499,639,1491,769), 
                   male=c(1,0,1,0,1,0,1,0),
                   white =c(1,1,0,0,1,1,0,0),
                   younger =c(1,1,1,1,0,0,0,0))
</code></pre>

<p>I then used  </p>

<pre><code>output = glm(obese ~ male + white + younger, family = binomial)
</code></pre>

<p>to model the data. But this doesn't seem to be working. I've not really used much R before for GLMs or logistic regression and I don't understand how to get GLM from categorial, binary predictor variables.
If someone could explain the theory behind because searching online I have only found answers where the response variable is binary and this seems straight forward. </p>
"
"NaN","NaN","127226","<p>I'm trying to simulate a logistic regression. My goal is showing that if <code>Y=1</code> is rare, than the intercept is biased. In my R script I define the logistic regression model through the latent variable's approach (see for example pp. 140 <a href=""http://gking.harvard.edu/files/abs/0s-abs.shtml"" rel=""nofollow"">http://gking.harvard.edu/files/abs/0s-abs.shtml</a>):</p>

<pre><code>x   &lt;- rnorm(10000)

b0h &lt;- numeric(1000)
b1h &lt;- numeric(1000)

for(i in 1:1000){
  eps &lt;- rlogis(10000)
  eta &lt;- 1+2*x+eps
  y   &lt;-numeric(10000)
  y   &lt;- ifelse (eta&gt;0,1,0)

  m      &lt;- glm(y~x,family=binomial)
  b0h[i] &lt;- coef(m)[1]
  b1h[i] &lt;- coef(m)[2]
}

mean(b0h)
mean(b1h)
hist(b0h)
hist(b1h)
</code></pre>

<p>The problem here is that I don't know how to force the observations y to be balanced before (50:50), then unbalanced (90:10). As we can see with the function table(), in my script the proportion of ones is random.</p>

<pre><code>table(y)
</code></pre>

<p>How to solve this problem?</p>
"
"NaN","NaN","127415","<p>I'm building a logistic regression model and one of the variables I have is postcode, I might be over thinking this but is it fine for me to leave postcode as is and regress it as:</p>

<pre><code>fitlogit &lt;- glm(target~postcode+..., family=""binomial"", data = dat)
</code></pre>

<p>I was thinking it might be better to combine postcodes into 4 distinct areas and model it as dummy variables once I figure out rough ranges for the groups. </p>
"
"0.241319899961953","0.236866229254101","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.112815214963553","0.0921225430717611","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.176791999975709","0.168425420962895","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.0603022689155527","0.0574484989621426","131349","<p>Following Dormann et al 2007 Ecography, I have employed a GLMM approach in R to account for spatial autocorrelation in a binomial regression model (logistic regression) that does not have random terms. Using data from mtcars (just so we all have the same numbers), my code looks as follows:</p>

<pre><code>library(MASS)
data &lt;- mtcars
data$group &lt;- factor(rep(""A"", nrow(data)))
mod1 &lt;- glmmPQL(vs ~ mpg, random=~1|group, correlation=corExp(form=~disp+qsec),
        data=data, family=binomial)
</code></pre>

<p>Question 1: Is this a reasonable way to account for spatial autocorrelation in a binomial model, as Dormann et al suggest?</p>

<p>Question 2: Should/can I demonstrate that there is no longer spatial autocorrelation (SAC) in the error for this model? Can a vario/correlagram be used? How? I have had trouble finding information about how to look for SAC in GLMMs, and the variogram I've managed to create looks very funky -- hard to interpret.</p>
"
"0.159900537266708","0.162488892740478","131456","<p>I'm exploring the effects of removing the intercept in a logistic regression model.</p>

<p>Assume a model:</p>

<p>$$logit(Y = 1) = \beta_1 x + \beta_2z + 0$$</p>

<p>with $x$ and $z$ being categorical variables with 2 levels each and no intercept.</p>

<p>I understood that having no intercept with categorical predictors produce coefficients that compare the $P(Y = 1)$ in each level of the two predictor against a null case where $P(Y=1) = 0.5$ or $logit(Y=1) = 0$.</p>

<p>I noticed a phenomenon that can understand. Using glm() function in R if you change the order of the variable in the right hand part of the formula, the coefficients change too. But even more oddly, the coefficient of the first variable is always the same.</p>

<p>Here's an <code>R</code> demo:</p>

<pre><code>y &lt;- as.factor(sample(rep(1:2), 30, T))
x &lt;- as.factor(sample(rep(1:2), 30, T))
z &lt;- as.factor(sample(rep(1:2), 30, T))

coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ z + x - 1, binomial))
#        z1         z2         x2 
#-0.1764783 -0.3099976  0.5025523 
</code></pre>

<p>As you can see the first predictors have the same coefficient while the other are different in the two models.</p>

<p>Here is what I expected and instead behave differently than what I though:</p>

<ol>
<li>Since every level of the two predictors is compared to the same null case, I expected to have the same coefficients in the two models, independently from the order in which I use them.</li>
<li>I expected to see the coefficients of every level of every predictor, instead the coefficient for the 1 level of the second predictor is not shown.</li>
<li>I therefore assume that only the first variable is compared against the null case, while the second is compared against a reference level; but what is this level? Is it $P(Y = 1 | X = 1 \cap Z = 1)$? Reproducing one of the models WITH the intercept we get:</li>
</ol>

<p>`(for some reason stackexchange don't understand the following is code without the tick)   </p>

<pre><code>coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ x + z, binomial))
#(Intercept)         x2          z2 
#-0.1764783   0.5025523  -0.1335192
</code></pre>

<p>As expected x1 become the intercept, and x2 is likely relative to x1. z1 is missing also in this case and z2 is the same as in the model without intercept.</p>

<p>Thus should I assume that the comparison against the null case $P(Y = 1) = 0.5$ is made only for the first variable in a formula, while the other are compared against the usual intercept?
Is this behavior normal?
What about the fact that the first coefficient has the same value whichever the order of the predictors in the formula?
What if I want to compare all level of each predictor against the null case and have a coefficient for all levels?
Or it's theoretically impossible for some reason I don't get?</p>
"
"0.0492365963917331","0.0469065029820195","132787","<p>How do you calculate marginal effects of parameters of logit model in R uging package {glm}?</p>

<p>Are following codes correct?</p>

<pre><code>#### preparation ####
# dependent variable
yseed &lt;- rnorm(100)
y &lt;- ifelse(yseed &gt; 0, 1, 0)

# independent variables
x1 &lt;- rnorm(100, mean=100, sd=20)
x2 &lt;- rnorm(100, mean=50, sd=20)
X &lt;- cbind(1, x1, x2)ã€€
Xmean &lt;- apply(X, 2, mean)

#### analysis ####
# logit model
res &lt;- glm(y ~ x1 + x2, family=binomial(link=""logit""))
summary(res)

# Marginal effects (ME) calculation
LAMBDA &lt;- function(x) { 1 / (1 + exp(-x))}ã€€# cdf of standard logistic distribution

# ME of (intercept)
ME_1  &lt;- coef[1] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res)))
# ME of x1   
ME_x1 &lt;- coef[2] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
# ME of x2
ME_x2 &lt;- coef[3] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
</code></pre>
"
"0.135400640077266","0.128992883200554","132971","<p>There is something I'm not quite understanding conceptually about the output from generalized linear mixed models. I have read that the target of inference in GLMMs is subject-specific. For example, the accepted answer to <a href=""http://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm"">this</a> question states that in a logistic GLMM the odds-ratios are conditioned on both the fixed and random effects. So, in a GLMM of pupils within classrooms, with random intercepts for classroom (i.e., the ""subject"" in this case), the odds-ratios will differ for each classroom as there will be many random intercepts. So far, this makes sense to me.</p>

<p>What I am confused about is that the typical output from the fixed effects part of such a model reports just one odds-ratio. For example, in the R example I provide below, the odds-ratio for the fixed effect of <code>sex</code> is .662. I have three questions:</p>

<ol>
<li><p><strong>How do I interpret this single fixed effect odds-ratio?</strong><br>
(Is it an odds-ratio ignoring the random effects? Is it an odds-ratio of the average random effect - in which case, isn't it a population average? Is it calculated assuming the random effect variance is zero?) </p></li>
<li><p><strong>Is it possible to calculate a population average odds-ratio using the output from a GLMM?</strong><br>
I know this can be done using a GEE, but what about a GLMM?</p></li>
<li><p><strong>How would I go about calculating the odds-ratio for a particular random effect (a particular classroom, lets say class 7 in the example below)?</strong><br>
Presumably this involves combining the fixed and random effect estimates somehow.</p></li>
</ol>

<p><strong>EDIT 1:</strong>
It seems after doing more reading (for example, this <a href=""http://stats.stackexchange.com/questions/32419/difference-between-generalized-linear-models-generalized-linear-mixed-models-i?lq=1"">post</a>), that since the fixed effect for <code>sex</code> in this example does not have its own random effect (e.g., a random slope), there will be no subject-level interpretation of this parameter. Does this mean that only the intercept term in the model below is subject-specific, while the <code>sex</code> term is a population average?</p>

<pre><code># dummy data:
set.seed(1)
dat &lt;- data.frame(Y         = factor(sample(rep(c(0, 1), 100))),
                  sex       = factor(sample(rep(c(""M"", ""F""), 100))),
                  classroom = factor(sample(rep(paste(""class"", 1:10), 20)))
) 

# model:
library(lme4)
fit &lt;- glmer(Y ~ sex + (1 | classroom), family=binomial, data=dat)

# summary(fit)
exp(fixef(fit))
# (Intercept)   sexM 
#  1.229       0.662 
</code></pre>
"
"0.0426401432711221","0.0406222231851194","133320","<p>I am using logistic regression to solve the classification problem.</p>

<pre><code>g = glm(target ~ ., data=trainData, family = binomial(""logit""))
</code></pre>

<p>There are two classes (target): 0 and 1 </p>

<p>When I run the prediction function, it returns probabilities.</p>

<pre><code>p = predict(g, testData, type = ""response"")
</code></pre>

<p>However, it is not clear to me how to understand which class has been assigned?</p>

<pre><code>Real  p 

1   0.17568578
1   0.41698474
1   0.19151927
1   0.25587242
1   0.25604452
0   0.39976069
0   0.39910282
0   0.16879320
</code></pre>

<p>I appreciate if someone can explain me how this works based on the above example. Thanks</p>
"
"0.0953462589245592","0.090834052439095","134837","<p>My data has a binary outcome (attack or not attack), day (20 day in repeated measured design) and some covariates (nestlingâ€™s movement).
The objectives of my experiment are testing the effect of time and other factors and selecting useful variables affecting outcomes.</p>

<p>My data look like below</p>

<pre><code>subject outcome Day nestling.move
   1        A    1      N 
   2        A    1      Y 
   3        A    1      Y 
   4        N    1      Y 
   5        N    1      Y 
   6        N    1      Y 
   7        N    1      Y 
   8        N    1      N 
   9        N    1      N 
   .        .    .      . 
   .        .    .      . 
   1        A    20     N 
   2        A    20     N   
</code></pre>

<p>First of all, I simply transformed outcomes to ratios(attack rate for each day) and test if there is a correlation between attack rates and days by using Spearmanâ€™s rank correlation. But I think it is not a good way to test the effect of time on outcome.</p>

<p>I checked other <a href=""http://stats.stackexchange.com/questions/81246/unable-to-fit-repeated-measures-in-r"">post</a>. and I think I should used an AR1 model with logistic regression since it could be a time-varying processes. However, I don't know how to do this with R or SPSS. </p>

<p>Is this the correct syntax to use in R?</p>

<pre><code>model&lt;- glmmPQL(outcome ~  nestling.move + Day, data=mydata, family=binomial,  random = ~ 1 | subject, correlation = corAR1(form=~Day|subject)) 
</code></pre>
"
"NaN","NaN","135424","<p>I have a 2 class classification problem that I'm trying to optimize the log-loss for (not ROC/AUC).  I'm using the <code>glmnet</code> package in <code>R</code> but am unsure how to set the code up.  </p>

<p>I'm assuming that the <code>family</code> argument should be <code>'binomial'</code> but am uncertain if I should use the <code>link = 'logistic'</code> or <code>'log'</code> and whether I need to adjust the <code>type.measure</code> argument.</p>

<p>Any suggestions?</p>
"
"0.127920429813366","0.108325928493652","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.0644658371220304","0.0921225430717611","136094","<p>I was trying to get an intuition for the interpretation of the coefficients in a logistic regression that was intended to reproduce to some extent that presented in a youtube video (<a href=""http://youtu.be/vq-_4kWmzTo"" rel=""nofollow"">http://youtu.be/vq-_4kWmzTo</a>). So I created a fictitious data set reflecting the chances of getting accepted (Accepted: int: 0 0 ... 1 0) into a college as related to SAT scores (int 1136 1347 1504) and family/ethnic background (categories = ""red"" vs ""blue""). </p>

<pre><code>fit &lt;- glm(Accepted ~ Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded:</p>

<pre><code>                      OR     2.5 %    97.5 %
Backgroundblue 0.7088608 0.5553459 0.9017961
Backgroundred  1.7352941 1.3632702 2.2206569
</code></pre>

<p>The interpretation seemed easy: Red applicants have 1.7 times more chances of getting in over the rest; blue applicants were at a disadvantage, and had  7 over 10 odds of getting in.</p>

<p>However, the more complete model,</p>

<pre><code>fit &lt;- glm(Accepted ~ SAT.scores + Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded coefficients for background that are difficult to reconcile or interpret:</p>

<pre><code>                         OR        2.5 %       97.5 %
SAT.scores     1.008558e+00 1.006940e+00 1.010297e+00
Backgroundblue 8.730056e-06 8.459031e-07 7.634723e-05
Backgroundred  2.329513e-05 2.426748e-06 1.929259e-04
</code></pre>

<p>Can you help point out what I am missing? Thank you.</p>

<p>Thanks to the enlightening answer from Maarten below, I was able to make some progress, and obtain the correct Odds Ratios without and with the SAT confounder:</p>

<p>Here is just regressing to Background (""Red"" versus ""Blue""):</p>

<pre><code>fit &lt;- glm(Accepted ~ Background, data = dat, family = ""binomial"")
exp(cbind(Odds_Ratio_RedvBlue = coef(fit), confint(fit)))

                        Odds_Ratio_RedvBlue             2.5 %       97.5 %
(Intercept)             0.7088608                     0.5553459   0.9017961
Backgroundred           2.4480042                     1.7397640   3.4595454
</code></pre>

<p>Which brought up a couple of additional questions (probably very basic): 1. Why is the Odds Ratio of the (Intercept) - 0.7088608 - the same for this model as for the <em>Odds</em> - as opposed to <em>Odds Ratio</em> - of the Background Blue in the model without the intercept above? And 2. Shouldn't the OddsRatio of Blue and Red be the reciprocal of each other $OddsRatioBlue = 1 / OddsRatioRed$?</p>
"
"0.0870388279778489","0.0995037190209989","137424","<p>I am fitting a binomial logistic regression in R using glm. By chance, I have found out that if I change the order of my predictor variables, glm fails to estimate the model. The message I get is  <em>unexpected result from lpSolveAPI for primal test</em>. </p>

<p>I am using the safeBinaryRegression package, so I am confident there are no separation issues between my outcome and predictor variables. However, I am not so confident that there are no quasi-separation issues among my predictor variables. Am I correct that if this is the case, then I might be running into multicolinearity, and this is the source of glm not being able to fit the model? </p>

<p>If so, my question is for advice on how to approach the issue. Should I look for the predictor variables highly correlated and omit one of them? Is there any convenient way of doing so for 11 categorical predictors? </p>

<p>What I see right now: </p>

<pre><code>lModel &lt;- glm(mob_change ~ education + gender + start_age + income + dist_change + lu_change + dou_change + marriage + student2work + wh_change,
              data = regression_data, 
              family = binomial())
# Fine, and I can inspect the model. No predictor has std. error &gt; 1.05

# Now if I move the last variable (or any of the last three, for what I've tested) to
# be the first in predictor... 
lModel.3 &lt;- glm(mob_change ~ wh_change + gender + education + start_age + income + dist_change + lu_change + dou_change + marriage + student2work,
            data = regression_data, 
            family = binomial())

Error in separator(X, Y, purpose = ""find"") : 
  unexpected result from lpSolveAPI for primal test
</code></pre>
"
"0.0603022689155527","0.0574484989621426","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"NaN","NaN","138332","<p>Iâ€™m trying to do logistic regression,</p>

<p>I utilize the following command:</p>

<pre><code>mylogit &lt;- glm(Var0 ~Var1, data = mydata, family = ""binomial"")
</code></pre>

<p>And I obtain a p-value of <code>0.003</code></p>

<p>After that I want to know the effect of <code>Var2</code> and <code>Var3</code> and I use the following command:</p>

<pre><code>mylogit &lt;- glm(Var0 ~Var1+Var2+Var3, data = mydata, family = ""binomial"")
</code></pre>

<p>obtaining a p-value of <code>0.993</code></p>

<p>My problem is that <code>Var1</code> and <code>Var2</code> are dependent and for that reason I obtain such p value.
Is there any method to indicate that <code>Var1</code> and <code>Var2</code> are dependent or I have to remove <code>Var2</code>?</p>
"
"0.143125289466427","0.146840580956429","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.112815214963553","0.107476300250388","139988","<p>For example, if you have a logistic regression on certain dataset:</p>

<pre><code>fit &lt;- glm(y ~ x, data = test, family = ""binomial"")
</code></pre>

<p>If you do <code>predict(fit, newdata, type = ""link"", se = TRUE)</code>, you will get a column named <code>se.fit</code>, which is the standard error for each predicted y value.</p>

<p>My questions are:  </p>

<ol>
<li><p>How is the MSE value for the link function is computed here?  </p>

<p>The variance of the fitting coefficients are basically the MSE times the variance-covariance matrix, there should be a way to compute the MSE value first. But for response variables that have 0 and 1 values, the link function corresponds to 0 and infinity. In this case, how does the model compute this value? Is there any way I can get the MSE value for the <code>glm</code> fitting in R?</p></li>
<li><p>Is <code>se.fit</code> the standard error for the link function value of the fitted line at point <code>x0</code>, or the standard error for the predicted link function value of <code>y</code> at point <code>x0</code>?</p></li>
</ol>
"
"0.0426401432711221","0.0406222231851194","140509","<p>I used logistic regression and found that my model fits well: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6434  -1.4623   0.8704   0.9013   1.0066  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.41595    0.02115   19.67   &lt;2e-16 ***
init_att_cnt  0.02115    0.00146   14.48   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 154956  on 122239  degrees of freedom
Residual deviance: 154746  on 122238  degrees of freedom
AIC: 154750
</code></pre>

<p>The chi-squared test is hightly statisticaly significant: <code>p = 9.642755e-48</code>. I decided to check the Nagelkerke $R^2$ statistic, </p>

<pre><code>R2 &lt;- R2/(1-exp((-mylogit$null.deviance)/n))
</code></pre>

<p>but it was $R^2 = 0.001350927$. This is unbelievable, why is $R^2$ so small, if my model fits well?</p>
"
"0.172727272727273","0.173213741660499","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.0738548945875996","0.0703597544730292","141203","<p>My question regards the use of the <code>gamlss</code> package. I am using <code>gamlss</code> package to fit a dataset to a logistic function. There is only one predictor, let me denote it with <code>x</code> and because the overall dependence is not exactly sigmoid, a better model is achieved by wrapping the predictor <code>x</code> in a smoother function. I chose the cubic spline smoother (<code>cs</code>). The response is binomial. I'll denote it with <code>y</code>:  </p>

<pre><code>y = cbind(number of successful events, total number - number of successful events). 
</code></pre>

<p>The R code is  the following:</p>

<pre><code>cs15 &lt;- gamlss(y~cs(x, df=15), sigma.fo=~1, family=BI,  data=mydata, trace=FALSE) 
</code></pre>

<p>I want to estimate predicted response for a set of predictors not contained in the original data set. I know this can be achieved with the function: </p>

<pre><code>cs15fit = predict(cs25, newdata=data.frame(x=xnew), type=""response"")
</code></pre>

<p>However, my problem is that I also want to estimate the standard errors, which should be done by adding <code>se.fit=T</code>:</p>

<pre><code>cs15fit=predict(cs25, newdata=data.frame(x=xnew), type=""response"", se.fit=T)
</code></pre>

<p>But the addition of <code>se.fit = T</code> produces the following error:</p>

<blockquote>
  <p>se.fit = TRUE is not supported for new data values at the moment</p>
</blockquote>

<p>Anyone know how I can still find standard errors for the new values? </p>
"
"0.112815214963553","0.107476300250388","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"NaN","NaN","143328","<p>I am developing a logistic regression model where perfect variable separation occurs. I want to calculate a cutoff from this data. Interestingly, the length of the slot <code>cutoffs</code> of <code>pred.obj</code> is only 5, as well as the slots <code>fp</code>, <code>tp</code>, <code>tn</code>, <code>fn</code>, <code>n.pos.pred</code> and <code>n.neg.pred</code>. I expect it to have the same length as the observations. </p>

<p>Has anybody an explanation for this? (And knows how to solve it?) </p>

<p>MWE:</p>

<pre><code> library(ROCR) # package for prediction/performance functions
 y &lt;- c(0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0)
 x &lt;- c(-5, 5, 3, -2, 4, 3, -8, 2, 5, 3, -5, -3, -2)
 model &lt;- glm(as.factor(y) ~ x, family = ""binomial"")
 preds &lt;- predict(model, type = ""response"")
 (pred.obj &lt;- prediction(preds, y))
 perf &lt;- performance(pred.obj, ""acc"")
 (cutoff &lt;- perf@x.values[[1]][which.max(perf@y.values[[1]])])
</code></pre>
"
"0.0426401432711221","0.0406222231851194","144603","<p>I have built a logistic regression where the outcome variable is being cured after receiving treatment (<code>Cure</code> vs. <code>No Cure</code>). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. </p>

<p>In R my logistic regression output looks as follows: </p>

<pre><code>Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.2735     0.1306   9.749   &lt;2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75
</code></pre>

<p>However, the confidence interval for the odds ratio <strong>includes 1</strong>:</p>

<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167
</code></pre>

<p>When I do a chi-squared test on these data I get the following:</p>

<pre><code>data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365
</code></pre>

<p>If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:</p>

<pre><code>Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)
</code></pre>

<p>My question is: Why don't the p-values and the confidence interval including 1 agree? </p>
"
"0.0572077553547355","0.054500431463457","145315","<p>I have age as a covariate in my material. A continuous variable. The age varies between 18-70 years.</p>

<p>I'm into a logistic regression and do not really know how to treat the variable. As a linear effect or as a polynomial?</p>

<pre><code>   gender       passinggrade age    prog
1    man          FALSE      69     FRIST
2    man             NA      70     FRIST
3 woman             NA       65     FRIST
4 woman           TRUE       68      FRIST
5 woman             NA       65     NMFIK
6    man          FALSE      70     FRIST
</code></pre>

<p>my model;</p>

<pre><code>mod.fit&lt;-glm(passinggrade ~prog+gender+age,family=binomial,data=both)
</code></pre>

<p>summary(mod.fit)</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.42653    0.28096   8.636  &lt; 2e-16 ***
progLARAA    0.44931    0.25643   1.752 0.079746 .  
progNASTK   -0.15524    0.26472  -0.586 0.557597    
progNBFFK    0.12091    0.65460   0.185 0.853462    
progNBIBK   -0.18850    0.37656  -0.501 0.616659    
progNDATK   -2.84617    0.73077  -3.895 9.83e-05 ***
progNFYSK    0.64391    0.19634   3.280 0.001040 ** 
progNMATK    0.18424    0.16451   1.120 0.262733    
progNMETK    0.22433    0.29086   0.771 0.440554    
progNMFIK    0.38877    0.42152   0.922 0.356373    
progNSFYY    0.97205    0.29320   3.315 0.000915 ***
progSMEKK   -0.58043    0.18185  -3.192 0.001414 ** 
genderman   -0.05623    0.10477  -0.537 0.591496        
age         -0.11780    0.01028 -11.462  &lt; 2e-16 ***
</code></pre>

<p>how would you treat the variable age?
and how should I interpret the results for age?</p>
"
"0.0426401432711221","0.0406222231851194","146434","<p>Trying to run a panel logistic model.  In the parameters a default NULL is specified for the ""start"" parameter.  </p>

<p>My model is:</p>

<pre><code>res&lt;-pglm(DFLT_DEBT_01.3~NGDPRPC+NGDP_RPCH+BCA_NGDPD+LC_PER_USD+OFF_RSVA+M2 ,
index=c(""Country"",""Date""),start=NULL,data=dat,family=""binomial"", model=""within"")
</code></pre>

<p>However, even if I explicitly set <code>start=NULL</code> I receive the message:</p>

<pre><code>Error in prepare Fixed(start = start, activePar = activePar, fixed = fixed) : 
  argument ""start"" is missing, with no default
</code></pre>

<p>I'm not sure how I can specify starting values as I have not seen anywhere the precise order of specification of the parameters.</p>
"
"0.0870388279778489","0.0995037190209989","147060","<p>I am studying an experiment of the kind: 
Let $n_{ij}$ be the number of fetuses, $X_{ij}$ the number of responses i.e. the number of fetuses with a malformation in the jth litter of the ith dose level for j=1,...,25 and i=1,...,5 . 
Then, $p_{ij}$ is the probability of response of in the jth litter of the ith dose level and hence we have:
$$
P(X_{ij} = x_{ij}|p_{ij}) \sim Bin(n_{ij},p_{ij})
$$
But the probability of response $p_{ij}$ follow a beta distribution hence
$$
P(p_{ij})=B^{-1}(\alpha_i , \beta_i )p_{ij}^{x_{ij}}(1-p_{ij})^{n_{ij}-x_{ij}}
$$
and hence, at the end, $X_{ij}$ follow a beta-binomial distribution. </p>

<p>My problem is that I have to generate the number of responses $X_{ij}$ but I'm having some troubles. </p>

<p>The data that I have are all the $n_{ij}$ and $p_1, p_2, p_3, p_4, p_5$ (and this probabilities follow a logistic model) i.e. the probability response for each dose group, hence I don't have $p_ij$. </p>

<p>What should I do? I think that I should first generate $p_{ij}$ using the fact that they follow a beta distribution. But in which way should I do? How to estimate the parameter $\alpha_i$ and $\beta_i$? </p>

<p>Maybe someone has some ideas.. 
Thank you in advance!</p>
"
"0.0603022689155527","0.0574484989621426","147693","<p>I have a dataset of 1931 observations and I intend to predict a binary outcome out of that. There is a list of 128 predictors (both binary and continuous). First I ran logistic regression modeling using all predictors and got a highly significant model (AUC = 0.84). Assuming that the high value of AUC was due to overfitting the model by using too many predictors, I did stepwise modeling to find the effective predictors: </p>

<pre><code>mylogit &lt;- glm(outcome ~ . , data = temp,family=""binomial"")
step &lt;- step(mylogit, direction=""both"")
</code></pre>

<p>Now, I am not sure whether should I have done cross validation before or after stepwise modeling. </p>
"
"0.14191497503738","0.14646550861729","148699","<p>For a current piece of work Iâ€™m trying to model the probability of tree death for beech trees in a woodland in the UK. I have records of whether trees were alive or dead for 3 different census periods along with data on their diameter and growth rate. Each tree has an ID number so it can be identified at each time interval. However, the census intervals vary so that for the time between one survey and another is either 4, 12 or 18 years. Obviously the longer the census period the greater the probability a tree will have died by the time it is next surveyed. <strong>I had problems making a realistic reproducible example so you can find the <a href=""https://github.com/PhilAMartin/Denny_mortality/blob/master/Data/Stack_dead.csv"" rel=""nofollow"">data here</a>.</strong></p>

<p>The variables in the dataset are:</p>

<ol>
<li>ID - Unique ID for tree</li>
<li>Block - the ID for the 20x20m plot in which the tree was located</li>
<li>Dead - Status of tree, either dead (1) or alive (0)</li>
<li>GR - Annual growth rate from previous survey</li>
<li>DBH - diameter of tree at breast height</li>
<li>SL - Length of time between censuses in years</li>
</ol>

<p>Once a tree is recorded as dead it disappears from subsequent surveys.</p>

<p>Ideally I would like to be able to estimate the annual probability of mortality of a tree using information on diameter and growth rate. Having searched around for quite a while I have seen that logistic exposure models appear able to account for differences in census periods by using an altered version of logit link for binomial models as detailed by Ben Bolker <a href=""https://rpubs.com/bbolker/logregexp"" rel=""nofollow"">here</a>. This was originally used by Shaffer to determine the daily probability of bird nest survival where the age (and therefore exposure) of the nest differed. I've not seen it used outside of the context of models of nest survival but it seems like I should be able to use it to model survival/mortality where the exposure differs.</p>

<pre><code>require(MASS)
logexp &lt;- function(exposure = 1)
{
  linkfun &lt;- function(mu) qlogis(mu^(1/exposure))
  ## FIXME: is there some trick we can play here to allow
  ##   evaluation in the context of the 'data' argument?
  linkinv &lt;- function(eta)  plogis(eta)^exposure
  logit_mu_eta &lt;- function(eta) {
    ifelse(abs(eta)&gt;30,.Machine$double.eps,
           exp(eta)/(1+exp(eta))^2)
    ## OR .Call(stats:::C_logit_mu_eta, eta, PACKAGE = ""stats"")
  }
  mu.eta &lt;- function(eta) {       
    exposure * plogis(eta)^(exposure-1) *
      logit_mu_eta(eta)
  }
  valideta &lt;- function(eta) TRUE
  link &lt;- paste(""logexp("", deparse(substitute(exposure)), "")"",
                sep="""")
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, 
                 name = link),
            class = ""link-glm"")
}
</code></pre>

<p>At the moment my model looks like this, but I will incorporate more variables as I go along:</p>

<pre><code>require(lme4)
Dead&lt;-read.csv(""Stack_dead.csv"",)


M1&lt;-glmer(Dead~DBH+(1|ID),data=Dead,family=binomial(logexp(Dead$SL))) 
#I use (1|ID) here to account for the repeated measurements of the same individuals
    summary(M1)

plot(Dead$DBH,plogis(predict(M1,re.form=NA)))
</code></pre>

<p><strong>Primarily I want to know</strong>:</p>

<ol>
<li><strong>Does the statistical technique I am using to control for the difference in time between census seem sensible? If it isn't, can you think of a better way to deal with the problem?</strong></li>
<li><strong>If the answer to the first question is yes, is using the inverse logit (plogis) the correct way to get predictions expressed as probabilities?</strong></li>
</ol>

<p>Thanks in advance for any help!</p>
"
"0.127920429813366","0.121866669555358","151568","<p>I'm working with a large 3-way contingency table (roughly $180 \times 40 \times 2$) &mdash; both independent variables are categorical and the response is binary.  One independent variable (X) represents a classification of experimental test objects into types, and the other (Y) a set of test conditions.  The experiment is only balanced for Y - that is, each test object was tested in all Y conditions, but each test object is only one X type, and the number of objects assigned to each type varies by three orders of magnitude (40Â­&ndash;69600).  Response to the test is either ""normal"" or ""anomalous"", and, critically, I know <em>a priori</em> that only interactions between X and Y are interesting.  I know it's necessary to model the main effects of X and Y as well, but (in the context of the larger experiment) they are noise factors.  So the thing that I care about is identifying all pairs of levels of X and Y such that the interaction is significant (given some threshold).</p>

<p>In R-ese, the natural model is</p>

<pre><code>glm(cbind(anom, total-anom) ~ X * Y, family=""binomial"", data=apt.d)
</code></pre>

<p>where <code>apt.d</code> (a $10\times10\times2$ downsample) is at the end of this question.  Now, I have two problems:</p>

<ol>
<li><p><code>summary(fit object)</code> tells me that this model leaves me with zero residual degrees of freedom and a residual deviance of $4 \times 10^{-10}$, which makes me worry about overfitting, but I don't see how I could structure the model any other way.</p></li>
<li><p>Actually running the fit on the full data frame is painfully slow -- nearly an hour just to generate the fit object.  And then it says that the fit didn't converge and ""fitted probabilities numerically 0 or 1 occurred"".  Is there an alternative implementation of logistic regression in R that would be more appropriate for this size of data set and/or this sort of unbalanced data with widely varying levels of anomalous response?</p></li>
</ol>

<p>Downsampled data:</p>

<pre><code>apt.d &lt;- structure(list(X = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L), .Label = c(""A"", 
""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""I"", ""J""), class = ""factor""), 
    Y = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
    1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
    9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 
    3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 
    7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
    1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 
    5L, 6L, 7L, 8L, 9L, 10L), .Label = c(""A"", ""B"", ""C"", ""D"", 
    ""E"", ""F"", ""G"", ""H"", ""I"", ""J""), class = ""factor""), total = c(765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L, 765L, 
    1530L, 894L, 49L, 77L, 97L, 779L, 1111L, 445L, 45L), anom = c(148L, 
    235L, 150L, 3L, 0L, 12L, 113L, 148L, 74L, 1L, 121L, 290L, 
    116L, 1L, 3L, 15L, 276L, 220L, 44L, 2L, 47L, 66L, 23L, 1L, 
    0L, 3L, 50L, 28L, 20L, 4L, 51L, 42L, 64L, 13L, 0L, 3L, 33L, 
    67L, 42L, 0L, 91L, 148L, 128L, 0L, 0L, 10L, 69L, 106L, 53L, 
    4L, 242L, 546L, 307L, 13L, 14L, 25L, 320L, 431L, 106L, 22L, 
    45L, 37L, 61L, 11L, 0L, 3L, 20L, 28L, 28L, 0L, 143L, 251L, 
    150L, 16L, 12L, 26L, 118L, 193L, 61L, 3L, 34L, 58L, 29L, 
    7L, 0L, 3L, 40L, 36L, 33L, 0L, 43L, 32L, 73L, 1L, 1L, 3L, 
    24L, 30L, 24L, 0L), p.anom = c(0.193464052287582, 0.15359477124183, 
    0.167785234899329, 0.0612244897959184, 0, 0.123711340206186, 
    0.145057766367137, 0.133213321332133, 0.166292134831461, 
    0.0222222222222222, 0.158169934640523, 0.189542483660131, 
    0.129753914988814, 0.0204081632653061, 0.038961038961039, 
    0.154639175257732, 0.354300385109114, 0.198019801980198, 
    0.098876404494382, 0.0444444444444444, 0.061437908496732, 
    0.0431372549019608, 0.0257270693512304, 0.0204081632653061, 
    0, 0.0309278350515464, 0.0641848523748395, 0.0252025202520252, 
    0.0449438202247191, 0.0888888888888889, 0.0666666666666667, 
    0.0274509803921569, 0.0715883668903803, 0.26530612244898, 
    0, 0.0309278350515464, 0.0423620025673941, 0.0603060306030603, 
    0.0943820224719101, 0, 0.118954248366013, 0.0967320261437909, 
    0.143176733780761, 0, 0, 0.103092783505155, 0.0885750962772786, 
    0.0954095409540954, 0.119101123595506, 0.0888888888888889, 
    0.316339869281046, 0.356862745098039, 0.343400447427293, 
    0.26530612244898, 0.181818181818182, 0.257731958762887, 0.410783055198973, 
    0.387938793879388, 0.238202247191011, 0.488888888888889, 
    0.0588235294117647, 0.0241830065359477, 0.0682326621923937, 
    0.224489795918367, 0, 0.0309278350515464, 0.0256739409499358, 
    0.0252025202520252, 0.0629213483146067, 0, 0.186928104575163, 
    0.164052287581699, 0.167785234899329, 0.326530612244898, 
    0.155844155844156, 0.268041237113402, 0.151476251604621, 
    0.173717371737174, 0.137078651685393, 0.0666666666666667, 
    0.0444444444444444, 0.0379084967320261, 0.0324384787472036, 
    0.142857142857143, 0, 0.0309278350515464, 0.0513478818998716, 
    0.0324032403240324, 0.0741573033707865, 0, 0.0562091503267974, 
    0.0209150326797386, 0.08165548098434, 0.0204081632653061, 
    0.012987012987013, 0.0309278350515464, 0.030808729139923, 
    0.027002700270027, 0.0539325842696629, 0)), .Names = c(""X"", 
""Y"", ""total"", ""anom"", ""p.anom""), row.names = c(NA, -100L), class = ""data.frame"")
</code></pre>
"
"0.14191497503738","0.14646550861729","151816","<p>I am using propensity scores for IPW in a logistic GLM in R. Two of the propensities are quite small and thus the resulting weights are quite large - much larger than all the others. I expected these two observations to have a large impact on the modelling results. This is indeed the case when using a binomial model with just the exposure as covariate. However, when including all the covariates used for creating the propensities, there is virtually no influence of these two highly-weighted observations. When removing the two observations, the fitted model stays almost precisely the same.</p>

<p>Investigating this further, it turns out that the working weights of the glm model for these observations are very small when including additional covariates, but not when using just the exposure. I am aware that glm adjusts the weights in some way to arrive at the working weights, but I am not sure how. Can you give me insights on why the weights become very small when including additional covariates?</p>

<p>The dataset is quite large (8500 rows), but I am happy to share it if this gives additional insights. I assume this is mostly a theoretical question, so I am not including the dataset for now. The code I used is simply</p>

<pre><code>glm(outcome ~ exposure, weights = 1/propensity, family = ""binomial"")
</code></pre>

<p>(or the same line with additional covariates added).</p>
"
"0.0426401432711221","0.0406222231851194","152091","<p>I am fairly new with logistic regression. I have a binary response. And did this plot. The binary response is:</p>

<p>Y = 0: The student fails</p>

<p>Y = 1: The student succeed</p>

<pre><code>library(ggplot2)
ggplot(data = both, aes(x = age, y = succeed)) + 
  stat_smooth(method = 'glm', family = 'binomial') +
  theme_bw()+xlab(""X"")+ylab(""The student succeed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xM9wS.png"" alt=""enter image description here""></p>

<p><strong>What is the y-axis? estimated probability that the student succeed? Or estimated log odds of a student succeed?
Feeling a little confused. Could someone explain what they are I have on the y-axis?</strong></p>
"
"0.147709789175199","0.0938130059640389","152841","<p>I have a dependent variable distinguishing between patients that should go to treatment A or treatment B. </p>

<p>I want to develop a questionnaire containing binary variables that should decide if the patients have to go to treatment A or treatment B. We have found six variables which we think of will predict that a patient goes to treatment A or treatment B (we have used a methodology to come up with these variables). </p>

<p>I have draw a ROC curve using a logistic regression in R using the following syntax.</p>

<pre><code> mod &lt;- glm(df$treatmentAorB~df$varA+df$varB+df$varC+df$varD+df$varE+df$varF, 
family=""binomial"")

df$predpr &lt;- predict(mod,newdata=data.frame(df$treatmentAorB,
                                            df$varA,df$varB,df$varC,
                                            df$varD,df$varE,df$varF),
                     type=c(""response""))
</code></pre>

<p>So the roc curve can be drawn:</p>

<pre><code>roccurve &lt;- roc(df$treatmentAorB ~ df$predpr)
plot(roccurve)
</code></pre>

<p>I calculated the cut-off point (or threshold) (specificity and sensitivity are equal important) by using the Youden index. </p>

<p>As I need the cut-off score based on the total score of the questionnaire I used the following (-4.9048 is the B0 of the logistic regression).</p>

<pre><code>rawcutoffscore = log(treshold[1]/(1-treshold[1]))- -4.9048 
</code></pre>

<p>I got a cutoff score of 4.5 and in the questionnaire I tell the researchers to use the weights associated with the variables I put in the logistic regression. If the total score is above 4.5 they have to sent the patient to treatment A and if it is below to treatment B.</p>

<p>So the weights of the logistic regression are the weights I use in the questionnaire. </p>

<ul>
<li><p>My first question is if this is a sound methodology. And if not, what
is a better way to address this issue?</p></li>
<li><p>My second question is, if I can use the ROC curve with my independent
(binary variables)?</p></li>
</ul>
"
"0.0953462589245592","0.090834052439095","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.175809814598306","0.157637380649093","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.0953462589245592","0.090834052439095","154986","<p>We want to do the logistic regression analysis to consider the effect of Age, CD4 on drug resistance mutations. The code that we wrote is:</p>

<pre><code>logist.summary(glm(DRM ~ Age, data = Database, family = binomial),""wald"")
</code></pre>

<p>The results are: </p>

<pre><code>            log.OR OR lower.CI upper.CI p.value
(Intercept)  -0.31 0.74     0.05     9.95  0.8169
Age          -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>However, we want to do the test like, we will consider whether, 20 years old differences between the subjects, what the results is? Is it relative to DRMs? We wrote:</p>

<pre><code>logist.summary(glm(DRM ~ I(Age+20), data = Database, family = binomial),""wald"")
</code></pre>

<p>Results:</p>

<pre><code>            log.OR   OR lower.CI upper.CI p.value
(Intercept)   1.17 3.22     0.05   190.62  0.5742
I(Age + 20)  -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>I want to ask:</p>

<ul>
<li>Is the code we wrote correct?</li>
<li>Can you help me explain what is meaning of these table?</li>
<li>Why it is the same results for the Age and Age+20? But differences in the Intercept? What does intercept meaning in this case?</li>
</ul>
"
"0.0994936676326182","0.108325928493652","155668","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has: 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>doing this below doesn't specify the individual age group, or can this be worked out from the summary stats? </p>

<pre><code>Model&lt;-glm(Maturity~Lgnth+age, family=binomial(logit)) 
</code></pre>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this. Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.0904534033733291","0.10053487318375","155762","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has:
 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this.
Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.190692517849118","0.18166810487819","156465","<p>The following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , the group-level residuals $u_{0j}$ and $u_{1j}$ are assumed to have a multivariate normal distribution with expectation zero . The variance of the residual errors  $u_{0j}$ is specified as $\sigma^2_0$ , and the variance of the residual errors  $u_{1j}$ is specified as $\sigma^2_1$ .</p>

<p>I want to estimate the parameter of the model and I like to use  <code>R</code> command <code>glmmPQL</code> . </p>

<p>Substituting  equation (2) and (3) in equation (1) yields ,</p>

<p>$$\text{logit}(p_{ij})=\gamma_{00}+\gamma_{10}x_{ij}+\gamma_{01}z_j+\gamma_{11}x_{ij}z_j+u_{0j}+u_{1j}x_{ij}\ldots (4)$$</p>

<p>There are 30 groups$(j=1,...,30)$ and 5 individual in each group .</p>

<p>R code  :</p>

<pre><code>   #Simulating data from multilevel logistic distribution 
   library(mvtnorm)
   set.seed(1234)

   J &lt;- 30             ## number of groups
   n_j &lt;- rep(5,J)     ## number of individuals in jth group
   N &lt;- sum(n_j)

   g_00 &lt;- -1
   g_01 &lt;- 0.3
   g_10 &lt;- 0.3
   g_11 &lt;- 0.3

   s2_0 &lt;- 0.13  ##variance corresponding to specific ICC
   s2_1 &lt;- 1     ##variance standardized to 1
   s01  &lt;- 0     ##covariance assumed zero

   z &lt;- rnorm(J)
   x &lt;- rnorm(N)

   #Generate (u_0j,u_1j) from a bivariate normal .
   mu &lt;- c(0,0)
  sig &lt;- matrix(c(s2_0,s01,s01,s2_1),ncol=2)
  u &lt;- rmvnorm(J,mean=mu,sigma=sig,method=""chol"")

  pi_0 &lt;- g_00 +g_01*z + as.vector(u[,1])
  pi_1 &lt;- g_10 + g_11*z + as.vector(u[,2])
  eta &lt;- rep(pi_0,n_j)+rep(pi_1,n_j)*x
  p &lt;- exp(eta)/(1+exp(eta))

  y &lt;- rbinom(N,1,p)
</code></pre>

<p>Now the parameter estimation .</p>

<pre><code>  #### estimating parameters 
  library(MASS)
  library(nlme)

  sim_data_mat &lt;- matrix(c(y,x,rep(z,n_j),rep(1:30,n_j)),ncol=4)
  sim_data &lt;- data.frame(sim_data_mat)
  colnames(sim_data) &lt;- c(""Y"",""X"",""Z"",""cluster"")
  summary(glmmPQL(Y~X*Z,random=~1|cluster,family=binomial,data=sim_data,,niter=200))
</code></pre>

<h3>OUTPUT :</h3>

<pre><code>      iteration 1
      Linear mixed-effects model fit by maximum likelihood
      Data: sim_data 

      Random effects:
      Formula: ~1 | cluster
              (Intercept)  Residual
      StdDev: 0.0001541031 0.9982503

      Variance function:
      Structure: fixed weights
      Formula: ~invwt 
      Fixed effects: Y ~ X * Z 
                      Value Std.Error  DF   t-value p-value
      (Intercept) -0.8968692 0.2018882 118 -4.442404  0.0000
      X            0.5803201 0.2216070 118  2.618691  0.0100
      Z            0.2535626 0.2258860  28  1.122525  0.2712
      X:Z          0.3375088 0.2691334 118  1.254057  0.2123
      Correlation: 
           (Intr) X      Z     
      X   -0.072              
      Z    0.315  0.157       
      X:Z  0.095  0.489  0.269

      Number of Observations: 150
      Number of Groups: 30 
</code></pre>

<ul>
<li><p>Why does it take only $1$ iteration while I mentioned to take $200$ iterations inside the function <code>glmmPQL</code> by the argument <code>niter=200</code> ?</p></li>
<li><p>Also p-value of group-level variable $(Z)$ and cross-level interaction $(X:Z)$ shows they are not significant . Still why in this <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955447/"" rel=""nofollow"">article</a>, they keep the group-level variable $(Z)$ and cross-level interaction $(X:Z)$ for further analysis ?</p></li>
<li><p>Also How are the degrees of freedom <code>DF</code> being calculated ?</p></li>
<li><p>It doesn't match with the relative bias of the various estimates of <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955447/table/T1/"" rel=""nofollow"">the table</a> .  I tried to calculate the relative bias as :</p>

<pre><code> #Estimated Fixed Effect parameters :

 hat_g_00 &lt;- -0.8968692 #overall intercept
 hat_g_10 &lt;- 0.5803201  # X
 hat_g_01 &lt;-0.2535626   # Z
 hat_g_11 &lt;-0.3375088   #X*Z

fixed &lt;-c(g_00,g_10,g_01,g_11)
hat_fixed &lt;-c(hat_g_00,hat_g_10,hat_g_01,hat_g_11)


#Estimated Random Effect parameters :

hat_s_0 &lt;-0.0001541031  ##Estimated Standard deviation of random intercept 
hat_s_1 &lt;-  0.9982503 

std  &lt;- c(sqrt(0.13),1) 
hat_std  &lt;- c(0.0001541031,0.9982503) 

##Relative bias of Fixed Effect :
rel_bias_fixed &lt;- ((hat_fixed-fixed)/fixed)*100
[1] -10.31308  93.44003 -15.47913  12.50293

##Relative bias of Random Effect :
rel_bias_Random &lt;- ((hat_std-std)/std)*100
[1] -99.95726  -0.17497
</code></pre></li>
<li>Why doesn't the relative bias match with the table ?</li>
</ul>
"
"0.0492365963917331","0.0469065029820195","156766","<p>I have been having real issues trying to calculate the length at which certain probabilities of maturing are reached. This is NOT the same as the proportion of individuals that are mature as <code>dose.p</code> would calculate.</p>

<p>I know I have to run a logistic regression with binomial errors, something similar to the below code</p>

<pre><code>mylogit &lt;- glm(Maturity ~ Lngth, data = data, family = ""binomial"")
</code></pre>

<p>But from this how can I work out the length at which the probability of maturing equals 50%.
My data frame consists of Maturity (0,1) data and length data. (I have other data but believe this is all I need at this stage (I could be wrong!)
Any help would be greatly appreciated! </p>
"
"0.0852802865422442","0.0609333347776791","159316","<p>I have been running logistic regression in R, and have been having an issue where as I include more predictors the z-scores and respective p-values approach 0 and 1 respectively.  For example if have few predictors:</p>

<pre><code>&gt; model1
b17 ~ i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -6.9461     1.8953  -3.665 0.000247 ***
i74           0.6842     0.9543   0.717 0.473384    
i73           1.7691     4.8008   0.368 0.712502    
i72           0.5134     2.0142   0.255 0.798812    
i71          -0.6753     4.9173  -0.137 0.890771    
</code></pre>

<p>The results appear to be fairly reasonable; however, if I have more predictors:</p>

<pre><code> &gt; model1
b17 ~ i90 + i89 + i88 + i87 + i86 + i85 + i84 + i83 + i82 + i81 + 
i80 + i79 + i78 + i77 + i76 + i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.887e+02  3.503e+05  -0.001    0.999
i90          1.431e-01  1.009e+04   0.000    1.000
i89          8.062e+01  1.027e+05   0.001    0.999
i88          9.738e+01  7.398e+04   0.001    0.999
i87         -1.980e+01  9.469e+03  -0.002    0.998
i86          9.829e+00  1.098e+05   0.000    1.000
i85          5.917e+01  3.074e+04   0.002    0.998
i84         -2.373e+01  1.378e+05   0.000    1.000
i83          7.257e+00  2.173e+05   0.000    1.000
i82         -1.397e+01  1.894e+05   0.000    1.000
i81          6.503e+01  1.373e+05   0.000    1.000
i80          3.728e+01  4.904e+04   0.001    0.999
i79          1.010e+02  5.556e+04   0.002    0.999
i78         -2.628e+01  1.546e+05   0.000    1.000
i77          4.725e+01  3.027e+05   0.000    1.000
i76         -6.517e+01  1.509e+05   0.000    1.000
i74          1.267e+01  1.175e+05   0.000    1.000
i73          2.796e+02  5.280e+05   0.001    1.000
i72         -2.533e+02  4.412e+05  -0.001    1.000
i71         -1.240e+02  4.387e+05   0.000    1.000
</code></pre>

<p>I know it is hard to say exactly what is going on without seeing the data, but the predictors are all 5-point Likert Scale items.  However, are there any thoughts to what is occurring here?  I don't have much experience with logistic regression, so I apologize if the question seems naive, but is there a certain threshold of predictors where logistic regression falls apart due to having such a large amount of predictors what is ultimately a very small amount of variance?  Is the potentially a multi-co-linearity issue?  Finally, when I run OLS regression on the data I get results that make more sense (or at least appear to), is it okay/what are the consequences of running OLS regression on a binary outcome?  Thank you!</p>
"
"0.0738548945875996","0.0703597544730292","159735","<p>I have two factors that are fully crossed, the levels of the factor are each coded 0 and 1. I am running a regression testing for one main effect and one interaction. The following is my logistic regression formula:</p>

<pre><code>m1=glmer(y~1+A+A:B+(1|Participants)+(1|Word),data=data, family = ""binomial"")
</code></pre>

<p>I am wondering if this is acceptable (only testing for one main effect and an interaction), and also why I am getting two interaction terms in my output:</p>

<pre><code>Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.18740    0.21600  -0.868  0.38561   
A1           0.74546    0.28399   2.625  0.00867 **
A0:B1        0.01537    0.28244   0.054  0.95662   
A1:B1        0.15884    0.28650   0.554  0.57929   
</code></pre>
"
"0.127920429813366","0.121866669555358","160109","<p>I'm trying to fit a (logistic) regression model to predict the successful funding of crowdfunding ventures (0/1) based on a series of IV with different level of measurement. One of these IVs is a categorial variable that indicates the nature of the venture (like technology, media, etc.). From visually inspecting the IVs I can see that the other IVs take different values relative to the venture category.</p>

<p><strong>My question is, how do I properly incorporate the categorial variable into a regression model (using R)?</strong></p>

<p>I do realize I could simply add the categorial variable with all the other IVs like this:</p>

<pre><code>glm(success ~ IV1 + IV2 + IV3 + ... + ventureCategory +, data=data, family=""binomial"")
</code></pre>

<p>From what I understand this would only return the overall influence of the venture categories on successful funding, but not the potential interaction between venture category and IV1 to IVn.</p>

<p>If I was to include interactions, how would I know which interactions to add?
And if I assume the venture category to be influential on all the IVs do I include separate interactions for every IV with the ventureCategory like this</p>

<pre><code>glm(success ~ IV1 + Iv1:ventureCategory + IV2 + Iv2:ventureCategory + ... + ventureCategory, data=data, family=""binomial"")
</code></pre>

<p>It seems that model quickly becomes quiet messy and I got something mixed up here.</p>

<p>Finally, I read two studies, who include this exact variable differently.
Here is a link to one of them: <a href=""https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44"" rel=""nofollow"">https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44</a></p>

<p>Their model simply states they controled for categories (see table p.12) without providing any coefficients or p values. I read this in other papers too, but do not understand how the control variable actually contributes to the model in this case.</p>

<p>I also read through some of the threads here, but couldn't find anything that really helped me understand the underlying rationale.</p>

<p>Update: I have 5 IV in total, excluding the categorial one.</p>

<p>Best</p>
"
"0.120604537831105","0.114896997924285","160425","<p>I am somewhat new to R and trying to polish my logistic regression. I am testing if my risk factors(cruise, age, sex, and year) have a significant effect on my dependent variable, MPS infection (named MPS_BINARY). I have a total of four cruises (5, 7, 9, 11), three years, thirteen ages, and two sexes (1 or 0). </p>

<p>I am able to run the full model with the following command:    </p>

<pre><code>&gt;mylogit&lt;-glm(MPS_BINARY~ AGE * SEX * CRUISE * YEAR, data=mps,family=""binomial"")
</code></pre>

<p>From this I have my p-values, I can now identify significant risk factors and interactions. My data shows a significant p-value for the interaction between cruise 7 and year. Following backwards selection, I now need to run the model again with my original risk factors and significant terms. I am having increasing difficulty isolating cruise 7 from my cruise data to run as an interaction with year. I have tried using the command:</p>

<pre><code>&gt;mylogit&lt;-glm(MPS_BINARY~AGE+SEX+YEAR+CRUISE+mps$CRUISE7*YEAR,data=mps,family=""binomial"")
</code></pre>

<p>But this, of course, does not recognize cruise 7 and I receive the error message: Error in model.frame.default(formula = MPS_BINARY ~ AGE + SEX + CRUISE +  : 
  invalid type (NULL) for variable 'mps$CRUISE7'.</p>

<p>My question is how can I run my logistic regression with all of my risk factors, and the significant interaction between year and cruise 7? I cannot figure out how to isolate only cruise 7 for the interaction with year. Please let me know if you need more information, thank you! </p>
"
"0.0738548945875996","0.0703597544730292","160598","<p>I have an imbalanced dataset with 4995:5 ratio as well as other datasets with less imbalanced ratios. I split this 4995:5 ratio into training and testing for about 2/3 training and 1/3 testing. I also decided to downSample using caret for the 4995:5 ratio dataset - this dataset now becomes 5:5.</p>

<p>Repeated cross validation works fine for the other datasets since there are more of the minority class, but for the training set of the 4995:5 ratio, I get the binomial class has less than 8 observations for either random forest or logistic regression.</p>

<p>Would I have to resort to bootstrap or LOOCV? This dataset seems to be problematic because of the terrible ratio.</p>
"
"0.148148749397529","0.141137695630083","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.127920429813366","0.121866669555358","161113","<p>I am working on example 7.3.1 from the Second Edition of the book <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=an+introduction+to+generalized+linear+models+second+edition+pdf"" rel=""nofollow"">An Introduction to Generalized Linear Models</a> in section <em>7.3 Dose response models</em>. This example fits a simple logistic regression model on the following data: </p>

<p><img src=""http://i.stack.imgur.com/YkHCG.png"" alt=""enter image description here""></p>

<p>This seems easy enough. However, I am having an issue with the Deviance Statistic calculated for this example. The following is my R code that will reproduce a Deviance Statistic $D=11.23$ just like this example in the book has. </p>

<pre><code>#original data
#copied in by row
( df &lt;-  data.frame( 
  Trial = 1:8,
  Dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839),
  Yes = c(6, 13, 18, 28, 52, 53, 61, 60),
  No = c(59, 60, 62, 56, 63, 59, 62, 60)- c(6, 13, 18, 28, 52, 53, 61, 60),
  Total = c(59, 60, 62, 56, 63, 59, 62, 60)
) )

#Logistic Regression Model
mle_beet &lt;- glm(cbind(Yes, No)~Dose, family=binomial(logit), data=df)
mle_beet$deviance
##
</code></pre>

<p>Section 5.6.1 of this same book derives the <em>Deviance Statistic</em> for the Binomial Model to be: </p>

<p>$D = 2\sum^{N}_{i=1}y_{i}[ log_{e}(\frac{y_i}{\hat{y_i}})+(n_i - y_i)log_{e}(\frac{n_i - y_i}{n_i - \hat{y_i}}) ]$</p>

<p>However, looking closely at the given data, it can be seen that for the last row, the number of beetles killed is the same as the total number of beetles ( $n_{8}=y_{8}$ ). This means that the very last part in the sum for <code>D</code> is: </p>

<p>$ y_{8}log_{e}(\frac{y_8}{\hat{y_8}})+(n_8 - y_8)log_{e}(\frac{n_8 - y_8}{n_8 - \hat{y_8}}) = 60log_{e}(\frac{60}{\hat{y_8}})+(0)log_{e}(\frac{0}{n_8 - \hat{y_8}})$</p>

<p>In particular, this value contains: </p>

<p>$0log_{e}(0)=0(-\infty)=$ <strong><em>undefined</em></strong></p>

<p>Here is the R code that agrees with this: </p>

<pre><code>sum( 2*(df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) + (df$Total-df$Yes)*
log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) )
</code></pre>

<p>My question is: What is the mathematical reasoning for computing the Deviance Statistic when $n_i=y_i$? What do the book and R do in the background to obtain $D=11.23$?</p>

<p>(Note that the book likely didn't use R to get this value, but the two agree)</p>

<p>Thank you!</p>

<p>EDIT: See the accepted answer and its comments for a great explanation.</p>

<p>If you happen to be computing the Deviance through the formula in R (you likely shouldn't since <code>mle_beet$deviance</code> shows this for you), you can replace <code>-Inf</code> or <code>Nan</code> in each vector that results from an individual operation. The following works for this example: </p>

<pre><code>x &lt;- df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) 
x[is.na(x) | x==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 
y &lt;- (df$Total-df$Yes)*
    log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) 
    y[is.na(y) | y==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 

sum(x+y)*2 #the deviance
</code></pre>
"
"0.0852802865422442","0.0609333347776791","161338","<p>I have data that can be fit, more or less, by logistic growth functions. Hence I used <a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html"" rel=""nofollow"">this tutorial</a> to do this.</p>

<p>Now I want to get an x value for a specific y value from the model. Maybe this is too trivial, but I could not find anything on the forums...or perhaps I was looking in the wrong way. For the below example, I would want to get the age at which Menarche is 0.5. In Excel I'd get the formula of the fit, solve it for x and put in y=0.5 ... but in R with logistic fit?</p>

<pre><code>library(""MASS"")
data(menarche)
str(menarche)

summary(menarche)

plot(Menarche/Total ~ Age, data=menarche)

glm.out = glm(cbind(Menarche, Total-Menarche)~Age, family=binomial(logit), data=menarche)

plot(Menarche/Total ~ Age, data=menarche)
lines(menarche$Age, glm.out$fitted, type=""l"", col=""red"")
title(main=""Menarche Data with Fitted Logistic Regression Line"")
</code></pre>
"
"0.0738548945875996","0.0703597544730292","161581","<p>I want to extract standard deviation of residual from <code>glmer()</code> function in R .</p>

<p>So I wrote :</p>

<pre><code>lmer_obj = glmer(Y ~ X1 + X2 + (1|Subj), data=D, family=binomial)
sigma(lmer_obj)
</code></pre>

<p>I noticed that the last command <code>sigma(lmer_obj)</code> returns always <code>1</code> irrespective of the data That is, whether I used the <code>cbpp</code> data or my own simulated data from multilevel logistic distribution, the residual standard error is always <code>1</code>.</p>

<p>How can I get the residual standard deviation from <code>glmer()</code> function?</p>
"
"0.0492365963917331","0.0703597544730292","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.0852802865422442","0.0812444463702388","162599","<p>This question is in response to an answer given by @gung in regards to this <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">question</a></p>

<p>I am also wanting to use simulation to conduct a power analysis on a multiple logistic regression. To keep it simple <strong>I want to do a post-hoc power analysis to determine the power associated with my regression</strong>. Lets take an <code>alpha=0.05</code></p>

<p>Lets say we have 1000 samples of data. Our dataset can be assumed as:</p>

<pre><code>set.seed(123)
N &lt;- 1000
var1 &lt;- runif(N, min=0, max=0.5)
var2 &lt;- runif(N, min=0.3, max=0.7)
var3 &lt;- rbinom(n=N, size=1, prob = 0.15)
output &lt;- rbinom(n=N, size=1, prob = 0.05)

df &lt;- data.frame(var1, var2, var3, output)
</code></pre>

<p>And a simple logistic model we are using is</p>

<pre><code>model &lt;- glm(output~var1+var2+var3, 
             data=df,
             family = binomial()
</code></pre>

<p>Now where this question differs from the example, we have our binary output (<code>output</code>) and not a continuous rate. </p>

<p>From what I have read, when unsure of the effect size, select a medium rate.</p>
"
"0.0738548945875996","0.0703597544730292","162867","<p>I'm currently building zero-inflated Poisson &amp; negative binomial predictive models using the zeroinfl() function from the pscl package in R.</p>

<p>Incorporating penalized regressions into my model to account for shrinkage and variable selection is a priority. In addition I'd like to use penalization to avoid convergence issues due to perfect/quasi separation in my data (better than manually removing variables).</p>

<p><strong>Question</strong>: Realizing that zero-inflated models $\neq$ hurdle models, for purposes of variable selection will my models be <strong>seriously biased</strong> if I first run separate run lasso (or elastic net) Poisson and logistic regressions with glmnet to select variables for the zeroinfl()?</p>
"
"0.159544807043493","0.141137695630083","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.0738548945875996","0.0703597544730292","164158","<p>In R and in binomial logistic regression to be specific, the modelling is based on which class amongst 0 and 1? And if it builds model based on 1 by default, is there a parameter or something in which we can pass 0 so that the model is built basing on class 0?</p>

<p>At least this is what I percieve, please correct me if I'm wrong.</p>

<p>I've heard that in SAS, modelling is done based on class 1 rather than class 0(Is it true?). So I was curious to know how it works in R.</p>
"
"0.0953462589245592","0.090834052439095","164186","<p>I have continuous variable with missing values. Missing values are of different types (indicated by special values such as 991, 992). How do I best encode my data for logistic regression? I can create separate variables for 991 and 992, but what I should use for the column with the rest of data? If I use NA, then R fails (using <code>na.pass</code>). If I use <code>na.exclude</code> then 991, 992 variables do not make any sense.</p>

<p>In this case, 991 represents missing value (not collected), and 992 represent not provided value (value was attempted to be collected, but response was not provided). I do not want to exclude rows with 991, 992 as these are valid inputs and I need to model response even for these rows. Also, in real-life scenario I have many such columns and removing all rows with special values would exclude vast majority of the rows.</p>

<pre><code>df &lt;- read.table(header= TRUE, text = '
x y
1 0
1 0
1 1
2 1
2 1
2 0
3 1
3 1
3 0
991 1
992 0
')

glm(y ~ x, df, family = binomial(), na.action = na.pass)
</code></pre>
"
"0.0966987556830456","0.107476300250388","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.127920429813366","0.108325928493652","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.0572077553547355","0.054500431463457","167324","<p>I'm trying to obtain the variance-covariance matrix of a logistic regression:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
mylogit &lt;- glm(admit ~ gre + gpa, data = mydata, family = ""binomial"")
</code></pre>

<p>through matrix computation. I have been following the example published <a href=""http://www.ats.ucla.edu/stat/r/library/matrix_alg.htm"" rel=""nofollow"">here</a> for the basic linear regression</p>

<pre><code>X &lt;- as.matrix(cbind(1, mydata[,c('gre','gpa')]))
beta.hat &lt;- as.matrix(coef(mylogit))
Y &lt;- as.matrix(mydata$admit)
y.hat &lt;- X %*% beta.hat

n &lt;- nrow(X)
p &lt;- ncol(X)

sigma2 &lt;- sum((Y - y.hat)^2)/(n - p)        
v &lt;- solve(t(X) %*% X) * sigma2
</code></pre>

<p>But then my var/cov matrix doesn't not equals the matrix computed with <code>vcov()</code></p>

<pre><code>v == vcov(mylogit)

1   gre   gpa
1   FALSE FALSE FALSE
gre FALSE FALSE FALSE
gpa FALSE FALSE FALSE
</code></pre>

<p>Did I miss some log transformation?</p>
"
"0.0426401432711221","0.0406222231851194","167794","<p>I wrote a script that create a logistic model, for Email opening probability, for each user name.</p>

<pre><code>form&lt;-formula(OpenOrNor~as.factor(TimeSend)
              +OpenWithSmartphoneind)
models&lt;- dlply(Data, ""User_id"", 
               function(df) {
                 model&lt;-glm(formula = form,family = binomial(""logit""),data = df,control = glm.control(epsilon = 1e-9, maxit = 500))
                 return(model)})
</code></pre>

<p>for some users it return me </p>

<pre><code>Call:  glm(formula = form, family = binomial(""logit""), data = df, weights = HistoryWeights, 
    control = glm.control(epsilon = 0.000000001, maxit = 500))

Coefficients:
              (Intercept)            as.factor(TimeSend)2      as.factor(TimeSend)3   as.factor(TimeSend)4  
                -22.61106                   20.21853                    0.07738                   20.54737  
          as.factor(TimeSend)5       as.factor(TimeSend)6      as.factor(TimeSend)7   as.factor(TimeSend)9  
                  0.19292                   -0.03624                    0.22013                    0.11837  
          OpenWithSmartphoneind  
                       NA  

Degrees of Freedom: 83 Total (i.e. Null);  76 Residual
Null Deviance:      190.6 
Residual Deviance: 166.8    AIC: 182.8
</code></pre>

<p>We can see that for OpenWithSmartphoneind their  is NA. and this is because their are no Opening With Smartphone at all In this user history.</p>

<p>My question is how it will impact on predict?<br>
And doe's it make different if OpenWithSmartphoneind in the formula will be a factor type or not?</p>
"
"0.175809814598306","0.157637380649093","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.0953462589245592","0.090834052439095","169291","<p>I have a logistic regression model below, predicting a dichotomous variable <em>type</em> from a single continuous predictor <em>fatigue</em>. Using the coefficients below I can obtain the increase in the odds of a positive <em>type</em> from a 1 unit increase in fatigue.</p>

<p>Also I believe by forming the model expression</p>

<pre><code>logit(type) = 0.3134 - 91.1171 * fatigue 
</code></pre>

<p>I can obtain the odds of a positive <em>type</em> for a given value of <em>fatigue</em> by plugging it in, say for a value <em>fatigue</em> = 1.</p>

<p><strong>However</strong>, what I want to do is to obtain the odds of a positive <em>type</em> for a range of <em>fatigue</em> values, i.e. <strong>&lt;= 0</strong>. Is this possible?</p>

<pre><code>## Call:
## glm(formula = type ~ fatigue, family = binomial(), data = myData)

## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6703 -1.3104 0.8369 1.0049 1.4695
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.3134 0.1496 2.095 0.0362 *
## fatigue -91.1171 36.3785 -2.505 0.0123 *
## ---
## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 282.84 on 210 degrees of freedom
## Residual deviance: 276.03 on 209 degrees of freedom
## AIC: 280.03
##
## Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.112815214963553","0.107476300250388","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.159544807043493","0.141137695630083","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.0738548945875996","0.0703597544730292","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.143125289466427","0.146840580956429","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.120604537831105","0.114896997924285","173076","<p>I was following the procedure in a statistics textbook to run a multinomial logistic regresion using <code>mlogit</code>. However, the Odds Ratios calculated seemed too high for some of the variables (>1000). Can someone take a look at this and check wether I am doing everything correctly? The data can be downloaded from <a href=""https://dl.dropboxusercontent.com/u/14303378/LogisticRegressionSample.csv"" rel=""nofollow"">here</a>. I prepared the data with the following commands:</p>

<pre><code>#read in the data
test&lt;-read.csv(file=""LogisticRegressionSample.csv"",sep="","")
#trasnform data into the correct form for mlogit
mlogitData&lt;-mlogit.data(test,choice=""Outcome"",shape=""wide"")
#build model
MLogitFit&lt;-mlogit(Outcome~1|V1+V2+V3+V4+V5+V6+V7+V8,reflevel=3,data=mlogitData)
#summary of the model
summary(MLogitFit)
#OddsRatios
data.frame(exp(MLogitFit$coefficients))
# confidence Interval of the odds Ratios
exp(confint(MLogitFit))
</code></pre>

<p>The summary of mlogit gives me:</p>

<pre><code>    Call:
mlogit(formula = Outcome ~ 1 | V1 + V2 + V3 + V4 + V5 + V6 + 
    V7 + V8, data = mlogitData, reflevel = 3, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
      Z       A       B 
0.43333 0.25556 0.31111 

nr method
7 iterations, 0h:0m:0s 
g'(-H)^-1g = 1.56E-06 
successive function values within tolerance limits 

Coefficients :
               Estimate Std. Error t-value  Pr(&gt;|t|)    
A:(intercept)  -6.74640    5.97451 -1.1292 0.2588147    
B:(intercept)  -7.12401    4.50350 -1.5819 0.1136759    
A:V1            3.65979    3.90808  0.9365 0.3490331    
B:V1            4.24363    3.25687  1.3030 0.1925822    
A:V2          -15.11554    6.92901 -2.1815 0.0291475 *  
B:V2           -4.88778    3.65249 -1.3382 0.1808302    
A:V3            1.71465    6.57907  0.2606 0.7943839    
B:V3            2.94335    3.96557  0.7422 0.4579497    
A:V4           -1.70660    1.58849 -1.0744 0.2826633    
B:V4           -1.67210    1.17575 -1.4222 0.1549820    
A:V5            1.18494    1.60760  0.7371 0.4610682    
B:V5            1.03084    1.25573  0.8209 0.4116971    
A:V6            8.28902    2.51631  3.2941 0.0009873 ***
B:V6            3.44578    1.91844  1.7961 0.0724727 .  
A:V7           -1.34395    2.67943 -0.5016 0.6159612    
B:V7            1.04803    1.95147  0.5370 0.5912343    
A:V8           -7.46263    4.12978 -1.8070 0.0707577 .  
B:V8            0.21861    2.13596  0.1023 0.9184810    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -64.636
McFadden R^2:  0.33149 
Likelihood ratio test : chisq = 64.1 (p.value = 1.0515e-07)
</code></pre>

<p>Running <code>data.frame(exp(MLogitFit$coefficients))</code> to calculate the odds ratios gives:</p>

<pre><code>              exp.MLogitFit.coefficients.
A:(intercept)                1.175103e-03
B:(intercept)                8.055280e-04
A:V1                         3.885310e+01
B:V1                         6.966040e+01
A:V2                         2.725226e-07
B:V2                         7.538147e-03
A:V3                         5.554743e+00
B:V3                         1.897938e+01
A:V4                         1.814819e-01
B:V4                         1.878524e-01
A:V5                         3.270504e+00
B:V5                         2.803423e+00
A:V6                         3.979917e+03
B:V6                         3.136764e+01
A:V7                         2.608125e-01
B:V7                         2.852036e+00
A:V8                         5.741439e-04
B:V8                         1.244345e+00
</code></pre>

<p>I obtained the confidence interavls with: <code>exp(confint(MLogitFit))</code>:</p>

<pre><code>                     2.5 %       97.5 %
A:(intercept) 9.650816e-09 1.430830e+02
B:(intercept) 1.182216e-07 5.488637e+00
A:V1          1.831725e-02 8.241213e+04
B:V1          1.176881e-01 4.123248e+04
A:V2          3.446800e-13 2.154711e-01
B:V2          5.864847e-06 9.688857e+00
A:V3          1.394913e-05 2.211978e+06
B:V3          7.994348e-03 4.505896e+04
A:V4          8.066986e-03 4.082774e+00
B:V4          1.875058e-02 1.881996e+00
A:V5          1.400307e-01 7.638467e+01
B:V5          2.392271e-01 3.285238e+01
A:V6          2.870699e+01 5.517731e+05
B:V6          7.303065e-01 1.347282e+03
A:V7          1.366460e-03 4.978060e+01
B:V7          6.223884e-02 1.306918e+02
A:V8          1.752860e-07 1.880591e+00
B:V8          1.891518e-02 8.185990e+01
</code></pre>

<p>The predicted Probabilities are as following:</p>

<pre><code>fitted(MLogitFit, outcome=FALSE)
                 Z            A          B
 [1,] 0.2790108926 3.880184e-01 0.33297074
 [2,] 0.5191458618 2.900625e-01 0.19079169
 [3,] 0.7263001933 1.633014e-02 0.25736966
 [4,] 0.8386056883 3.700203e-03 0.15769411
 [5,] 0.8050365007 7.487290e-03 0.18747621
 [6,] 0.7855655154 3.860347e-02 0.17583101
 [7,] 0.7878404896 7.992930e-03 0.20416658
 [8,] 0.8386056883 3.700203e-03 0.15769411
 [9,] 0.7878404896 7.992930e-03 0.20416658
[10,] 0.4363708036 2.827104e-01 0.28091885
[11,] 0.6126060746 3.320075e-02 0.35419317
[12,] 0.0274357267 8.418204e-01 0.13074390
[13,] 0.1438998597 5.869087e-01 0.26919146
[14,] 0.1850027820 2.105586e-01 0.60443858
[15,] 0.8427092407 5.933393e-03 0.15135737
[16,] 0.1537160539 4.929905e-01 0.35329341
[17,] 0.0434283140 6.358897e-01 0.32068201
[18,] 0.1868202029 1.141679e-01 0.69901186
[19,] 0.3064594418 1.156597e-01 0.57788084
[20,] 0.5737141160 6.734724e-02 0.35893865
[21,] 0.5841338911 1.374758e-01 0.27839031
[22,] 0.0866451414 4.019366e-01 0.51141821
[23,] 0.2794060013 9.964607e-02 0.62094793
[24,] 0.0252343516 7.343045e-01 0.24046118
[25,] 0.1314775919 4.602643e-01 0.40825811
[26,] 0.0274357267 8.418204e-01 0.13074390
[27,] 0.1303195991 6.649645e-01 0.20471586
[28,] 0.2818251202 4.896734e-01 0.22850146
[29,] 0.0063990341 8.874618e-01 0.10613917
[30,] 0.0002408527 9.742025e-01 0.02555668
[31,] 0.0523052465 7.073015e-01 0.24039322
[32,] 0.3287956423 2.756959e-01 0.39550841
[33,] 0.0419093705 7.521689e-01 0.20592173
[34,] 0.0523052465 7.073015e-01 0.24039322
[35,] 0.3287956423 2.756959e-01 0.39550841
[36,] 0.0100998700 7.475180e-01 0.24238212
[37,] 0.1609808596 2.268570e-01 0.61216212
[38,] 0.0119603037 8.065964e-01 0.18144331
[39,] 0.0697132279 4.549378e-01 0.47534896
[40,] 0.5756435353 6.315652e-02 0.36119994
[41,] 0.4689676672 6.796615e-02 0.46306619
[42,] 0.2652679745 6.358962e-02 0.67114240
[43,] 0.7870195702 2.038999e-03 0.21094143
[44,] 0.6438437943 9.222002e-03 0.34693420
[45,] 0.7462282258 5.881047e-04 0.25318367
[46,] 0.3532662528 2.193975e-01 0.42733620
[47,] 0.9563852795 4.133754e-05 0.04357338
[48,] 0.9079031419 2.786314e-03 0.08931054
[49,] 0.0220230156 8.017508e-01 0.17622619
[50,] 0.2268852285 1.745210e-01 0.59859376
[51,] 0.2268852285 1.745210e-01 0.59859376
[52,] 0.0751929214 6.261548e-01 0.29865225
[53,] 0.9426667411 4.520877e-06 0.05732874
[54,] 0.0212631471 6.729961e-01 0.30574075
[55,] 0.0212631471 6.729961e-01 0.30574075
[56,] 0.9218535421 1.166953e-02 0.06647693
[57,] 0.6374868816 3.856300e-02 0.32395012
[58,] 0.2920703240 2.410709e-01 0.46685876
[59,] 0.7047942848 1.728601e-02 0.27791970
[60,] 0.1850395244 5.297673e-01 0.28519316
[61,] 0.4402296785 8.870861e-03 0.55089946
[62,] 0.6781988218 3.852569e-04 0.32141592
[63,] 0.9889453179 4.036588e-05 0.01101432
[64,] 0.1618635354 8.011851e-02 0.75801796
[65,] 0.3008372801 9.835522e-02 0.60080750
[66,] 0.0740319347 4.284039e-01 0.49756417
[67,] 0.5529727485 1.768537e-01 0.27017351
[68,] 0.7824740564 5.001713e-03 0.21252423
[69,] 0.5343045050 5.865850e-02 0.40703700
[70,] 0.4564647083 1.733995e-01 0.37013579
[71,] 0.4711837972 8.449081e-03 0.52036712
[72,] 0.9154349308 2.364316e-02 0.06092191
[73,] 0.1858643216 2.217595e-01 0.59237621
[74,] 0.3770813535 9.943397e-02 0.52348468
[75,] 0.8124141650 3.243679e-04 0.18726147
[76,] 0.3195206223 2.932236e-01 0.38725578
[77,] 0.8615871019 5.063299e-04 0.13790657
[78,] 0.8615871019 5.063299e-04 0.13790657
[79,] 0.8254986241 2.059378e-03 0.17244200
[80,] 0.1208591778 4.615235e-01 0.41761730
[81,] 0.0035765650 9.093754e-01 0.08704806
[82,] 0.7583239965 3.544345e-02 0.20623255
[83,] 0.8141948591 5.016280e-03 0.18078886
[84,] 0.1204323818 2.545405e-01 0.62502710
[85,] 0.9594950290 3.694056e-05 0.04046803
[86,] 0.6858228916 1.691396e-01 0.14503752
[87,] 0.8254986241 2.059378e-03 0.17244200
[88,] 0.8254986241 2.059378e-03 0.17244200
[89,] 0.2463233530 2.793410e-01 0.47433568
[90,] 0.5674338104 1.448538e-02 0.41808081
</code></pre>

<p>To assess multicolinearity I calculated the VIF statistic but using the a glm model of the same dataset.</p>

<pre><code>fullmod&lt;-glm(as.factor(Outcome)~.,data=test,family=binomial())
vif(fullmod)
      V1       V2       V3       V4       V5       V6       V7       V8 
1.789116 1.822252 2.216444 1.320244 1.821820 1.439183 1.512865 1.121805 
</code></pre>
"
"0.14142135623731","0.134728672455117","173625","<p>I am new to bootstrapping. Let me explain my case, I ran logistic model (binomial) with 9 parameters. I divided complete dataset into train(70%) and test(30%). I checked stats for the model validity and then did bootstrapping over complete dataset. Now I have 3 sets of coefficients one from modeled train set, one from bootstrap original set (that ran on complete dataset) and last one bootstrapped coefficient. Now I am not sure which set of coefficients should I use while predicting in future? Below are all coefficient results where <code>prob_model$coefficient</code> is list fo coefficients from model trained on training set. In boot model result original is list of coefficient on complete data set and original + bias would give me list of bootstrapped coefficients. Now I am not sure which one should I use for future prediction. Please suggest.</p>

<pre><code>Bootstrap Statistics :
          original        bias     std. error
t1*   0.9974316479  3.619136e-03 3.772206e-03
t2*  -0.0001680308  1.082013e-06 1.595247e-05
t3*  -0.0122041526 -2.240127e-05 6.932449e-05
t4*  -0.2103373656 -1.739921e-03 1.102777e-02
t5*  -0.0065004644 -2.184167e-05 1.525726e-04
t6*   0.2189768454 -1.910363e-03 1.564095e-02
t7*   0.0794606315 -5.810673e-04 1.066958e-03
t8*   0.0015241943  3.244818e-06 5.442563e-05
t9*   0.1165591650  7.599514e-04 5.028385e-03
t10*  0.0010037383  2.001973e-05 1.115370e-04



prob_model$coefficients
           (Intercept)                    a b       c                   d 
          0.9903145633          -0.0001745036          -0.0122231856          -0.2017792147          -0.0064298957 
            e       f            g       h       i 
          0.2032220963           0.0795136175           0.0015688575           0.1149821030           0.0011356770 
</code></pre>
"
"NaN","NaN","175216","<p>I have a binary dependent variable (<code>R</code>) and four numeric independent variables (<code>Q, M, S, T</code>) and want to examine coefficients for them.</p>

<p>Here is my glm code in R:</p>

<pre><code>fit = glm(R ~ Q + M + S + T, data=data, family=binomial())
</code></pre>

<p>When I run <code>predict(fit)</code>, I get a lot of predicted values greater than one (but none below 0 so far as I can tell). I have tried bayesglm and glmnet per suggestions to similar questions but both are a little over my head and the output I did get didn't seem to fix my problem.</p>

<p>I want to know:
A) Is this typical of logistic regression?
B) If not, how do I fix it?</p>
"
"0.0301511344577764","0.0574484989621426","175325","<p>I am trying to determine which of several independent variables {A,B,C} best predicts a subject's response R, which can be one of 3 choices, {high, medium, low}.</p>

<p>As the dependent variable is multinomial i.e. not binomial, a binomial logistic regression is not suitable. However, using the lme4 package to fit a glm with <code>family = 'binomial'</code> to the data runs without error or warning, and correctly identifies that independent variable B is the best predictor.</p>

<p>Is this a lucky coincidence, or is the call to <code>glm()</code> automatically fitting multiple binomial regressions to the data (one for each binary combination), or is it something else entirely? Any comments are welcome.</p>
"
"0.104446593573419","0.0829197658508324","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.153741222957161","0.135198931031345","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0738548945875996","0.0234532514910097","175782","<p>I want to predict the impact of oil price over a Colombian oil company's stock price. I plan to use a logistic regression for this with a categorical variable (Up or Down given the direction of the stock price). Here is part of my dataset:</p>

<pre><code>Minute  ecopet  profit  sum_profit   direccion  cl1_chg   sum_cl1    direccion_cl1
571     2160     0       10           Up         -0.03     0.00      Down
572     2160     0        0           Neutral     0.07    -0.03      Down
573     2160     0       -5           Down       -0.08     0.04      Up
574     2160     0       -5           Down       -0.07    -0.04      Down
575     2160     5       -5           Down       -0.08    -0.11      Down
576     2165     0       -25          Down        0.00    -0.19      Down
577     2165     0       -25          Down       -0.05    -0.19      Down
578     2165     0       -15          Down       -0.17    -0.24      Down
579     2165     5       -15          Down       -0.06    -0.41      Down
580     2170     0       -20          Down        0.03    -0.47      Down
581     2170    -10       0           Neutral     0.04    -0.44      Down
</code></pre>

<p>My dependent variable is 'direccion'. But as you can see it has 3 response classes. I know that to implement a binary logistic regression in R the code is:</p>

<pre><code>glm.fit=glm(direccion~direccion_cl1, data=datos, family=binomial)
</code></pre>

<p>I am working with intraday information and plan to predict what happens when the oil moves up/ down (in the previous 10 minutes) and how it impacts the stock price in the next 10 minutes.</p>

<p>Could anyone tell me how could I perform this? I don't really know how to perform the logistic regression with 3 response classes. Thanks!</p>
"
"0.155126306998506","0.157637380649093","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0603022689155527","0.0574484989621426","177219","<p>When I run a <code>glm</code> with binomial-family (logistic regression), R output gives me the logit-estimates, which can be transformed into probabilities using <code>plogis(logit)</code>. So using something like <code>plogis(predict(glm_fit, type = ""terms""))</code> would give me the adjusted probabilities of success for each predictor.</p>

<p>But what would be the equivalent for Poisson regression? How can I ""predict"" the adjusted incidents rates for each predictor?</p>

<p>Given this example:</p>

<pre><code>set.seed(123)
dat &lt;- data.frame(y = rpois(100, 1.5),
                  x1 = round(runif(n = 100, 30, 70)),
                  x2 = rbinom(100, size = 1, prob = .8),
                  x3 = round(abs(rnorm(n = 100, 10, 5))))

fit &lt;- glm(y ~ x1 + x2 + x3, family = poisson(), data = dat)
</code></pre>

<p>and using <code>predict.glm(fit, type = ""terms"")</code></p>

<p>I get:</p>

<pre><code>         x1          x2          x3
1 -0.023487964  0.04701003  0.02563723
2  0.052058119 -0.20041119  0.02563723
3  0.003983339  0.04701003  0.01255701
4 -0.119637524  0.04701003 -0.03322376
5  0.010851165  0.04701003 -0.00706332
6 -0.105901873 -0.20041119 -0.00706332
...
attr(,""constant"")
[1] 0.3786072
</code></pre>

<p>So, how many ""incidents"" (y-value) would I expect for each value of <code>x1</code>, holding <code>x2</code> and <code>x3</code> constant (what <code>predict</code> does, afaik)?</p>

<p><em>I'm not sure whether this question fits better into Stackoverflow or Cross Validated - please excuse if posting here was wrong!</em></p>
"
"0.0852802865422442","0.0812444463702388","177650","<p>I have a binary response variable and a categorical predictor variable. If I test for associations between the 2 variables using chi-square test , it turns out to be significant. However, if I do a logistic regression with the same set of variables, the predictor is not significant. Why does this happen?</p>

<pre><code>  table(Data1$pred,Data1$target)

                            0    1
  Level1                    1    0
  Level2                    4    0
  Level3                   98    1
  Level4                 2056   22
  Level5                    1    0
  Level6                    2    0
  Level7                  311    0
  Level8                    6    1
  Level9                  131    7
  Level10                  49    2

  chisq.test(table(Data1$pred,Data1$target))

  Pearson's Chi-squared test

  data:  tabletable(Data1$pred,Data1$target)
  X-squared = 34.2614, df = 9, p-value = 8.037e-05
</code></pre>

<p>Logistic Regression on the same</p>

<pre><code>  logit.glm &lt;- glm(as.factor(target) ~ pred,                  
               data=Data1, family=binomial(link=""logit"")
  summary(logit.glm)
  Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5553  -0.1459  -0.1459  -0.1459   3.0315  

  Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)
  (Intercept)   -2.057e+01  1.773e+04  -0.001    0.999
  Data1Level2   -6.313e-06  1.982e+04   0.000    1.000
  Data1Level3    1.598e+01  1.773e+04   0.001    0.999
  Data1Level4    1.603e+01  1.773e+04   0.001    0.999
  Data1Level5   -6.312e-06  2.507e+04   0.000    1.000
  Data1Level6   -6.312e-06  2.172e+04   0.000    1.000
  Data1Level7   -6.312e-06  1.776e+04   0.000    1.000
  Data1Level8    1.877e+01  1.773e+04   0.001    0.999
  Data1Level9    1.764e+01  1.773e+04   0.001    0.999
  Data1Level10   1.737e+01  1.773e+04   0.001    0.999

  (Dispersion parameter for binomial family taken to be 1)

   Null deviance: 356.09  on 2691  degrees of freedom
   Residual deviance: 333.06  on 2682  degrees of freedom
   AIC: 353.06

   Number of Fisher Scoring iterations: 19
</code></pre>
"
"0.0852802865422442","0.0609333347776791","178153","<p>I am running into difficulties when using <code>randomForest</code> (in R) for a classification problem. My R code, an image, and data are here: <a href=""http://www.psy.plymouth.ac.uk/research/Wsimpson/data.zip"" rel=""nofollow"">http://www.psy.plymouth.ac.uk/research/Wsimpson/data.zip</a> The observer is presented with either a faint image (contrast=<code>con</code>) buried in noise or just noise on each trial. He rates his confidence (<code>rating</code>) that the face is present. I have categorised rating to be a yes/no judgement (<code>y</code>). The face is either inverted (<code>invert=1</code>) or not in each block of 100 trials (one file). I use the contrast (1st column of predictor matrix <code>x</code>) and the pixels (the rest of the columns) to predict <code>y</code>.</p>

<p>It is critical to my application that I have an ""importance image"" at the end which shows how much each pixel contributes to the decision <code>y</code>. I have 1000 trials (length of <code>y</code>) and 4248 pixels+contrast=4249 predictors (ncols of <code>x</code>). Using <code>glmnet</code> (logistic ridge regression) on this problem works fine</p>

<p><code>fit&lt;-cv.glmnet(x,y,family=""binomial"",alpha=0)</code></p>

<p>However <code>randomForest</code> does not work at all,</p>

<p><code>fit &lt;- randomForest(x=x, y=y, ntree=100)</code></p>

<p>and it gets worse as the number of trees increases. For <code>invert=1</code>, the classification error for <code>randomForest</code> is 34.3%, and for <code>glmnet</code> it is 8.9%. I realise that these are not directly comparable, because <code>randomForest</code> is giving OOB error rate.</p>

<p>Please let me know what I am doing wrong with <code>randomForest</code>, and how to fix it.</p>
"
"0.0852802865422442","0.0812444463702388","178420","<p>I have data where number of observation <code>n</code> is smaller than number of variables <code>p</code>. The answer variable is binary. For example:</p>

<pre><code>n &lt;- 10
p &lt;- 100
x &lt;- matrix(rnorm(n*p), ncol = p)
y &lt;- rbinom(n, size = 1, prob = 0.5)
</code></pre>

<p>I would like to fit logistic model for this data. So </p>

<pre><code>model &lt;- glmnet(x, y, family = ""binomial"", intercept = FALSE)
</code></pre>

<p>The function returns 100 model for different lambda values (panalization parameter in lasso regression). I choose the biggest model which has <code>n - 1</code> parameters or less (so less than number of observations). Let's say chosen model is for <code>lambda_opt</code>. </p>

<pre><code>model_one &lt;- glmnet(x, y, family = ""binomial"", intercept = FALSE, lambda = lambda_opt)
</code></pre>

<p>Now I would like to do the second step - use <code>step</code> function to my model to choose the submodel which will be the best in term of BIC. Unfortunately <code>step</code> function doesn't work.</p>

<pre><code>step(model_one, direction = ""backward"", k = log(n))
</code></pre>

<p>How can I make it work? Is there some other function for this specific kind of model to do what I want?</p>
"
"0.147709789175199","0.117266257455049","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.112815214963553","0.107476300250388","180135","<p>I have fit a generalized additive model (GAM) using the mgcv package in R. My model has a dichotomous response variable and so i've used the binomial family link function. After creating the model I would like to do a little post-estimation inference above and beyond the plot.gam graphs. </p>

<p>I would like to take two x-values, for example, and calculate the risk ratio and 95% confidence intervals for that ratio. Obtaining the risk ratio seems fairly straightforward. I could transform the predictions into probabilities and simply divide the two probabilities corresponding to the x-values of interest in order to get the risk ratio. I am less certain how to get the confidence intervals.</p>

<p>In this link here: <a href=""http://grokbase.com/t/r/r-help/125qbnw21a/r-mgcv-how-to-calculate-a-confidence-interval-of-a-ratio"" rel=""nofollow"">http://grokbase.com/t/r/r-help/125qbnw21a/r-mgcv-how-to-calculate-a-confidence-interval-of-a-ratio</a> Simon Wood, the author of the mgcv package explained how to get the CIs for a log ratio using a poisson model. I'm uncertain how I would need to change the code to get the risk ratios and 95% CIs from my logistic model. </p>

<p>Here is a reproducible example provided by Simon Wood in the link above:</p>

<pre><code>    library(mgcv)

    ## simulate some data
    dat &lt;- gamSim(1, n=1000, dist=""poisson"", scale=.25)

    ## fit log-linear model...
    b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3), family=poisson,
    data=dat, method=""REML"")

    ## data at which predictions to be compared...
    pd &lt;- data.frame(x0=c(.2,.3),x1=c(.5,.5),x2=c(.5,.5),
    x3=c(.5,.5))

    ## log(E(y_1)/E(y_2)) = s(x_1) - s(x_2)
    Xp &lt;- predict(b,newdata=pd,type=""lpmatrix"")

    ## ... Xp%*%coef(b) gives log(E(y_1)) and log(E(y_2)),
    ## so the required difference is computed as...
    diff &lt;- (Xp[1,]-Xp[2,])
    dly &lt;- t(diff)%*%coef(b) ## required log ratio (diff of logs)
    se.dly &lt;- sqrt(t(diff)%*%vcov(b)%*%diff) ## corresponding s.e.
    dly + c(-2,2)*se.dly ## 95%CI
</code></pre>

<p>Any help is greatly appreciated.</p>
"
"0.14142135623731","0.134728672455117","180302","<p>IÂ´m fairly new to the tools of statistics and I need some help.
IÂ´m working in R.
I have a list of students, their age, sex, test results(continuous variable) and education (a categorical variable. 0 if student has no education, 1 if student has education). This is stored in the data frame df.</p>

<p>I have assigned each student an age group depending on their age. The groups are 5-10,11-15, 16-20, 21-25. So now I have a new categorical variable with 4 levels.</p>

<p>I have done logistic regression for each age group. I did the test in R like this:</p>

<pre><code>model1 = glm(education ~ test results + sex + age, data = df, family = binomial())
</code></pre>

<p>From the logistic regression result I compute the odds ratio (exp(coefficient)) for the test results variable. For each age group I got an odds ratio and confidence interval. There are some differences in the odds ratio between age groups. The odd ratio gets closer to 1 with higher age group. I interpret this in the following way; as the age of students is higher there is less effect from education on test result.
My question is how can I test if the difference in odds ratios is significant, i.e. what test can I use in R ?</p>

<p>Here is a subset of the data:</p>

<pre><code>student sex age testScore education ageGroup

1        1   10   0.12       1        1

2        1    8   0.08       1        1

3        2   16   0.85       0        3

4        2   20   0.12       0        3

5        1   22   1.02       0        4

6        2   18   0.98       1        3

7        1   19   0.46       1        3

8        1   16   0.83       0        3

9        2   12   0.26       1        2

10       2   14   0.46       0        2
</code></pre>

<p>I have been searching books and the web without success, canÂ´t seem to find an example that I can relate to.
Any suggestions would be appreciated.</p>
"
"0.143125289466427","0.157329193881888","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.0639602149066831","0.0812444463702388","182162","<p>I have the following data,</p>

<pre><code>Cond.1 &lt;- c(2.9, 3.0, 3.1, 3.1, 3.1, 3.3, 3.3, 3.4, 3.4, 3.4, 3.5, 3.5, 3.6, 3.7, 3.7,
        3.8, 3.8, 3.8, 3.8, 3.9, 4.0, 4.0, 4.1, 4.1, 4.2, 4.4, 4.5, 4.5, 4.5, 4.6,
        4.6, 4.6, 4.7, 4.8, 4.9, 4.9, 5.5, 5.5, 5.7)
Cond.2 &lt;- c(2.3, 2.4, 2.6, 3.1, 3.7, 3.7, 3.8, 4.0, 4.2, 4.8, 4.9, 5.5, 5.5, 5.5, 5.7,
        5.8, 5.9, 5.9, 6.0, 6.0, 6.1, 6.1, 6.3, 6.5, 6.7, 6.8, 6.9, 7.1, 7.1, 7.1,
        7.2, 7.2, 7.4, 7.5, 7.6, 7.6, 10, 10.1, 12.5)
</code></pre>

<p>and I want to apply Logistical Regression,  </p>

<pre><code>library(caret)
dat  = stack(list(cond1=Cond.1, cond2=Cond.2))
lr.model = glm(ind~values+I(values^2), dat, family=""binomial"")
lr.preds = predict(lr.model, type=""response"")
</code></pre>

<p>After using this code, I get value of lr.preds to be</p>

<pre><code>0.4195376 0.3559950 0.3022795 0.3022795 0.3022795 0.2226140 0.2226140 0.1945281 0.1945281 0.1945281 0.1726781 0.1726781 0.1560078 0.1436538 0.1436538 0.1349528 0.1349528 0.1349528 0.1349528 0.1294300 0.1267819 0.1267819 0.1268630 0.1268630 .....
</code></pre>

<p>but if change variable names from cond1 to cond1 and cond2 to con2 and run the same code i get different result of lr.preds.</p>

<pre><code>dat  = stack(list(uc=Cond.1, c=Cond.2))
lr.model = glm(ind~values+I(values^2), dat, family=""binomial"")
lr.preds = predict(lr.model, type=""response"")
</code></pre>

<p>In this case values of lr.preds are</p>

<pre><code>5.804624e-01 6.440050e-01 6.977205e-01 6.977205e-01 6.977205e-01 7.773860e-01 7.773860e-01 8.054719e-01 8.054719e-01 8.054719e-01 8.273219e-01 8.273219e-01 8.439922e-01 8.563462e-01 8.563462e-01 8.650472e-01 8.650472e-01 8.650472e-01 
</code></pre>

<p>Why are the values changing just by changing the variable?</p>
"
"NaN","NaN","183150","<p>Can I use binary variables in R's glm function with a binomial outcome (logistic regression)?</p>
"
"0.170856428594066","0.172345496886428","183320","<p>I have the following dataframe on which I did logistic regression with response as outcome. There are some good predictors in these variables so I expected significant variables.</p>

<pre><code>structure(list(response = c(0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 
    0L, 0L, 1L, 0L, 1L, 0L), HIST1H3F_rna = c(1.09861228866811, 0.693147180559945, 
    2.07944154167984, 1.09861228866811, 1.79175946922805, 0, 0, 0, 
    2.39789527279837, 1.38629436111989, 1.6094379124341, 1.6094379124341, 
    0.693147180559945, 1.79175946922805, 0), NCF1_rna = c(2.77258872223978, 
    3.09104245335832, 2.63905732961526, 2.19722457733622, 2.30258509299405, 
    2.56494935746154, 3.09104245335832, 3.98898404656427, 2.56494935746154, 
    4.06044301054642, 3.87120101090789, 2.07944154167984, 3.49650756146648, 
    3.17805383034795, 3.95124371858143), WDR66_rna = c(5.06890420222023, 
    4.49980967033027, 5.11799381241676, 3.40119738166216, 3.25809653802148, 
    4.02535169073515, 5.8348107370626, 5.89440283426485, 3.87120101090789, 
    5.67675380226828, 5.35185813347607, 4.15888308335967, 6.23441072571837, 
    5.91889385427315, 3.68887945411394), PTH2R_rna = c(0.693147180559945, 
    5.08759633523238, 0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 
    6.56526497003536, 5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 
    1.09861228866811, 3.49650756146648, 1.38629436111989, 5.93753620508243
    ), HAVCR2_rna = c(4.48863636973214, 3.40119738166216, 3.09104245335832, 
    2.94443897916644, 3.2188758248682, 3.76120011569356, 3.95124371858143, 
    2.83321334405622, 2.07944154167984, 4.36944785246702, 3.58351893845611, 
    1.94591014905531, 4.23410650459726, 3.43398720448515, 2.56494935746154
    ), CD200R1_rna = c(2.484906649788, 2.94443897916644, 0.693147180559945, 
    1.94591014905531, 0.693147180559945, 2.89037175789616, 2.56494935746154, 
    1.6094379124341, 1.6094379124341, 1.94591014905531, 2.19722457733622, 
    0.693147180559945, 4.26267987704132, 1.6094379124341, 0.693147180559945
    )), .Names = c(""response"", ""HIST1H3F_rna"", ""NCF1_rna"", ""WDR66_rna"", 
    ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna""), row.names = c(NA, 
    -15L), class = ""data.frame"")
</code></pre>

<p>However, running the following lines and getting a summary of the model I find that all variables have a p-value of 1 and the standard errors seem so high. What's going on here?</p>

<pre><code>fullmod &lt;- glm(response ~ ., data=final_model,family='binomial')
summary(fullmod)
Call:
glm(formula = response ~ ., family = ""binomial"", data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-6.515e-06  -2.404e-06  -2.110e-08   2.110e-08   7.470e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   1.460e+02  5.598e+05       0        1
HIST1H3F_rna  2.135e+01  5.145e+05       0        1
NCF1_rna     -4.133e+01  3.388e+05       0        1
WDR66_rna     1.296e+01  6.739e+05       0        1
PTH2R_rna     1.975e+00  3.775e+05       0        1
HAVCR2_rna   -2.477e+01  1.191e+06       0        1
CD200R1_rna  -1.420e+01  1.315e+06       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0190e+01  on 14  degrees of freedom
Residual deviance: 2.2042e-10  on  8  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25
</code></pre>

<hr>

<p>In response to your comments I'll show the feature selection step (and the complete dataframe I'm working with below that).  </p>

<pre><code># forward  feature selection 
library('boot')
z = c()
nullmod &lt;- glm(response ~ 1, data=final_model, family='binomial') ## â€˜emptyâ€™ 
fullmod &lt;- glm(response ~ ., data=final_model, family='binomial') ## Full model
first = T
for(x in 1:ncol(final_model)){
  stepmod &lt;- step(nullmod, scope=list(lower=formula(nullmod), upper=formula(fullmod)),
                  direction=""forward"", data=final_model, steps=x, trace=F)
  cv.err  &lt;- cv.glm(data=final_model, glmfit=stepmod, K=nrow(final_model))$delta[1]
  if (first == T){
    first=F
    final_features &lt;- stepmod
  }else{
    if (cv.err &lt; min(z)){ final_features &lt;- stepmod }
  }
  z[x] &lt;- cv.err
  print(paste(x,cv.err))
  print(colnames(final_features$model))
}

plot(z, main='Forward Feature Selection GLM Final Model', 
     xlab='Number of Steps', ylab='LOOCV-error', col='red', type='l')
points(z)
colnames(final_features$model)
summary(final_features)

structure(list(response = c(0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 0L, 1L, 1L, 1L), HIST1H3F_rna = c(1.09861228866811, 2.07944154167984, 
1.09861228866811, 1.79175946922805, 0, 0, 0, 2.39789527279837, 
1.38629436111989, 1.6094379124341, 1.6094379124341, 0.693147180559945, 
2.19722457733622, 2.39789527279837, 2.89037175789616), NCF1_rna = c(2.77258872223978, 
2.63905732961526, 2.19722457733622, 2.30258509299405, 2.56494935746154, 
3.09104245335832, 3.98898404656427, 2.56494935746154, 4.06044301054642, 
3.87120101090789, 2.07944154167984, 3.49650756146648, 2.07944154167984, 
2.07944154167984, 1.09861228866811), WDR66_rna = c(5.06890420222023, 
5.11799381241676, 3.40119738166216, 3.25809653802148, 4.02535169073515, 
5.8348107370626, 5.89440283426485, 3.87120101090789, 5.67675380226828, 
5.35185813347607, 4.15888308335967, 6.23441072571837, 4.0943445622221, 
4.21950770517611, 3.95124371858143), PTH2R_rna = c(0.693147180559945, 
0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 6.56526497003536, 
5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 1.09861228866811, 
3.49650756146648, 0, 0.693147180559945, 1.38629436111989), 
HAVCR2_rna = c(4.48863636973214, 
3.09104245335832, 2.94443897916644, 3.2188758248682, 3.76120011569356, 
3.95124371858143, 2.83321334405622, 2.07944154167984, 4.36944785246702, 
3.58351893845611, 1.94591014905531, 4.23410650459726, 1.38629436111989, 
1.09861228866811, 1.38629436111989), CD200R1_rna = c(2.484906649788, 
0.693147180559945, 1.94591014905531, 0.693147180559945, 2.89037175789616, 
2.56494935746154, 1.6094379124341, 1.6094379124341, 1.94591014905531, 
2.19722457733622, 0.693147180559945, 4.26267987704132, 1.94591014905531, 
0, 0.693147180559945), GDF7 = c(0.2232, -0.7281, 0.0655, -0.7919, 
0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 0.7371, 
-0.4978, -0.5096, -0.0827), HS1BP3 = c(0.2232, -0.7281, 0.0655, 
-0.7919, 0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 
0.7371, -0.4978, -0.5096, -0.0827), NKAIN3 = c(0.4072, 0.3216, 
-0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 0.6331, 
-0.135, 1.3532, -0.503, -0.1241, 0.2061), UG0898H09 = c(0.4072, 
0.3216, -0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 
0.6331, -0.135, 1.3532, -0.503, -0.1241, 0.2061), C15orf41 = c(0.122, 
-0.7519, -1.1267, -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, 
-0.5944, 0.0714, -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), 
    FAM98B = c(-0.1871, -0.7519, -1.1267, -0.7882, -0.1117, -0.5105, 
    -0.3905, -0.6834, -0.5944, 0.0714, -0.8134, -0.0115, -1.1112, 
    -1.1488, -0.4878), SPRED1 = c(-0.1871, -0.7519, -1.1267, 
    -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, -0.5944, 0.0714, 
    -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), MPDZ_ex = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0), TPR_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BUB1B_ex = c(0, 
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), APC_ex = c(0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ATM_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0), DYNC1LI1_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), TTK_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), PSMG2_ex = c(1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), NegRegMitosis = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0), brca1ness = c(0.037719, 
    0.900878, 0.013261, 0.900878, 0.659963, 0.005629, 9.8e-05, 
    0.996336, 0.910072, 0.850776, 0.000613, 0.104428, 0.978114, 
    0.938767, 0.041696), Methylation = c(0L, 0L, 0L, 1L, 1L, 
    1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L), LinoleicAcid_Metab = structure(c(2L, 
    2L, 2L, 2L, 1L, 3L, 2L, 2L, 1L, 5L, 2L, 5L, 1L, 2L, 2L), .Label = c(""CYP2E1_high"", 
    ""CYP2E1_med"", ""high"", ""low"", ""PLA2G2A_high""), class = ""factor""), 
    Neuro_lr = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 3L, 1L, 3L, 
    1L, 1L, 3L, 3L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor""), 
    NOX_signalling = structure(c(2L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L), .Label = c(""high"", ""low""
    ), class = ""factor"")), .Names = c(""response"", ""HIST1H3F_rna"", 
""NCF1_rna"", ""WDR66_rna"", ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna"", 
""GDF7"", ""HS1BP3"", ""NKAIN3"", ""UG0898H09"", ""C15orf41"", ""FAM98B"", 
""SPRED1"", ""MPDZ_ex"", ""TPR_ex"", ""BUB1B_ex"", ""APC_ex"", ""ATM_ex"", 
""DYNC1LI1_ex"", ""TTK_ex"", ""PSMG2_ex"", ""NegRegMitosis"", ""brca1ness"", 
""Methylation"", ""LinoleicAcid_Metab"", ""Neuro_lr"", ""NOX_signalling""
), row.names = c(NA, -15L), class = ""data.frame"")
</code></pre>

<p>Summary now gives the following:</p>

<pre><code>Call:
glm(formula = response ~ NegRegMitosis, family = ""binomial"", 
    data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-3.971e-06  -3.971e-06   3.971e-06   3.971e-06   3.971e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       25.57   76367.61       0        1
NegRegMitosis    -51.13  111790.71       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0728e+01  on 14  degrees of freedom
Residual deviance: 2.3655e-10  on 13  degrees of freedom
AIC: 4

Number of Fisher Scoring iterations: 24
</code></pre>

<p>Again even in a single predictor model, my p-value is 1. The predictor in this case is equal to the response, so it should predict perfectly. Then why is my pvalue 1?</p>
"
"0.120604537831105","0.114896997924285","183528","<p>I have three questions about the assumptions of the logistic regression:</p>

<ol>
<li><p>I read that the percentages of zeros and ones should be equal. If there's a data set where one of them is abundand, i.e. there are 80% zeros and 20% ones can I somehow put different weights in my glm? There's also the weight-function, but I don't understand what it's exactly for... </p></li>
<li><p>I didn't really get what the pseudo-coefficients of determination tell me - Do this, i.e. the Nagelkerke's index tell me something about the assumptions of my model, how much they are fullfilled or just how much my predicted model differ from the data points I've observed. </p></li>
<li><p>I also red for the assumptions that there should be at least 25 data points / group, what exactly is my group? When I i.e. have the mtcars-dataset in R </p>

<p>data(""mtcars"") </p></li>
</ol>

<p>and I want to look</p>

<pre><code>glm(vs ~ carp + disp, family = binomial) 
</code></pre>

<p>what are my groups? (maybe this is also a false point of view, but I'm really irritated...)</p>

<p>Best wishes </p>

<p>Marry</p>
"
"0.148148749397529","0.141137695630083","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.0603022689155527","0.0574484989621426","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"0.134839972492648","0.128458748884677","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.0426401432711221","0.0406222231851194","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.112815214963553","0.107476300250388","185757","<p>I'm doing a meta-analysis which involves some multiple logistic regression.  As it's a meta-analysis, I'm compiling data from various studies, which therefore differ slightly in their methodology.  One particular aspect of methodology is likely to affect the response variable, and I therefore decided to include it as a random effect in my models. </p>

<p>However, now I'm being asked to provide more information about the likely effect of this predictor.  Is it significant?  How strong is the effect?  The only way I can think to do so is to treat it as a fixed effect and see if adding/removing it improves the model or not, using F tests to compare models (they are quasi-binomial models).  However, I had a feeling that treating variables as both random and fixed variables was wrong - it should be one of the other.  I've never quite got a grip on the difference between the two, and would appreciate any advice.
Thanks</p>

<p>edit: Here's a bit more info about my study.  In my meta-analysis I'm comparing the results of other studies to my own study.  The response in the model is the similarity in the data.  Regarding this particular aspect of methodology, I used ""method A"". If all the other studies used ""method A"" it would be fine, but some used ""method A"", some ""method B"", some ""method C"" and some ""method D"". Each different method will introduce bias, but may do so to a different degree.  Hope that makes sense.</p>
"
"0.135400640077266","0.140719508946058","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"NaN","NaN","187443","<p>I wanna run sample selection on my data. My response is binary and also, I have some continuous and some binary variables. I was wondering how can I use ""sampleSelection"" package? 
It seems that it does not support logistic regression as well as factor variables.
Any thought?</p>

<p>PS. My goal is modeling my data using binomial logistic regression.</p>
"
"0.127920429813366","0.108325928493652","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"0.105528970602217","0.114896997924285","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.0852802865422442","0.0812444463702388","189627","<p>I am using the <code>survey</code> package and my model is:  </p>

<pre><code>modelsvy &lt;- svydesign(id =~ 1, data=temp12, weights=~WGT) 
model12s &lt;- svyglm(DEPVAR ~ var1 + var2 +... ,  modelsvy, family= quasibinomial)  
</code></pre>

<p>This has been going well for me. </p>

<p>I have a sample of the US population (N=4,343), with fractional weights (<code>WGT</code>) on each observation. The completed model is then scored as a probability: </p>

<pre><code>mdlg2012$DEPVARSCR &lt;- predict(model12s,mdlg2012)  
mdlg2012$DEPVARSCRP &lt;- (1 / (1 + exp(-1 * mdlg2012$DEPVARSCR)))  
</code></pre>

<p>The probabilities are applied to census block-group data to estimate the number of households interested in buying things like Life Insurance.  </p>

<p>In a variation, I built a couple models that did not use all N. For example, I subset to N=2,339, with DEPVAR = 1 (n=178) and DEPVAR = 0 (n=2161). The entire population was not included. The proportion of N was 178 / 2,339 = 0.076. For N overall, it would have been 178 / 4343 = 0.041 (stated on an unweighted basis, I do need to weight them).  </p>

<p>My question is, would you happen to know how I might calibrate the resulting new model probabilities back to the full US population? I down sampled, removing n = 2,004, which inflated the DEPVAR incidence. The logistic transform in this case overstates the probabilities. Is the calibrate(design,...) command useful here?</p>
"
"0","0.0406222231851194","191434","<p>I want to do a logistic regression simulation using R </p>

<p>I use this code</p>

<pre><code>set.seed(666)
age = rnorm(60)         
blood_pressure = rnorm(60)
race = sample(c(rep(1,30),rep(0,30)))
inactivity = sample(c(rep(1,30),rep(0,30)))
weight = rnorm(60)

z=1+1*age+blood_pressure*2+3*weight+3*inactivity+0*race
pr=exp(z)/(1+exp(z))
y=rbinom(60,1,pr)

df = data.frame(y=y,age,blood_pressure,inactivity,weight,race)
glm(y~age+blood_pressure+inactivity+weight+race,data=df,family=binomial(link='logit'),control = list(maxit = 50))
</code></pre>

<p>I got very strange result from it.</p>

<pre><code>Coefficients:
   (Intercept)             age  blood_pressure      inactivity          weight            race  
        -39.75           46.64          106.65          143.52          229.75          100.87  
</code></pre>

<p>And it says the model doesn't converge.</p>

<p>Does someone know what's wrong and how to fix it?</p>
"
"0.0603022689155527","0.0574484989621426","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.246352704820576","0.293367757169825","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.153741222957161","0.14646550861729","191891","<p>I'm hoping someone can help clarify a few things for me.</p>

<p>I ran some relatively simple logistic regressions in r and am having trouble with interpretation.  I'm interested in the effects of elevation and a species diversity index on the presence/absence of a disease in individual animals.</p>

<p>I ran a simple model of: <code>Result~Elevation+Diversity</code> which gave this result</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation + Simpsons_Diversity, family = binomial, 
    data = XXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8141  -0.6984  -0.5317  -0.4143   2.3337  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.118e+00  1.594e-01 -13.289  &lt; 2e-16 
Elevation           1.316e-04  2.247e-05   5.855 4.76e-09 
Simpsons_Diversity -9.907e-01  2.725e-01  -3.635 0.000278 

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2923.6  on 3297  degrees of freedom
AIC: 2929.6
</code></pre>

<p>I have a strong suspicion that diversity decreases with increasing elevation which I have confirmed although the relationship isn't quite as strong as I thought. When I run a model with an interaction term <code>elevation*diversity</code> I get:</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation_1000 + Simpsons_Diversity_100 + 
    Elevation_1000 * Simpsons_Diversity_100, family = binomial, 
    data = XXXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7908  -0.6959  -0.5437  -0.3963   2.4215  

Coefficients:
                                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                           -2.014422   0.179507 -11.222  &lt; 2e-16 
Elevation_1000                         0.112466   0.027433   4.100 4.14e-05 
Simpsons_Diversity_100                -0.015851   0.005780  -2.743   0.0061  
Elevation_1000:Simpsons_Diversity_100  0.001408   0.001200   1.173   0.2406   

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2922.2  on 3296  degrees of freedom
AIC: 2930.2

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Showing that adding the interaction term doesn't really help the fit of the model (AIC = 2930) and the interaction term itself is not significant (p-value=0.24).</p>

<p>Am I on the right track so far?</p>

<p>If I am, I understand how to convert coefficients to odds ratios and interpret those.  My main question is can I plot the predicted probabilities for a combination of elevation and diversity where each variable is allowed to vary? Or is this essentially plotting the interaction?  </p>

<p>I was able to create a dataframe where I varied elevation and diversity and I used my simple non-interaction model to obtain predicted probabilities using the PREDICT fuction) for those combinations, but I want to make sure that I am doing things correctly.  I've attached the plot of predicted probs for different levels of diversity. </p>

<p><a href=""http://i.stack.imgur.com/NINBE.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NINBE.gif"" alt=""Elevation vs. Predicted Probabilities for various levels of diversity)""></a></p>
"
"0.153741222957161","0.135198931031345","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.0426401432711221","0.0406222231851194","193166","<p>I have limited statistic knowledge but I am trying to conduct logistic regression by using a data with 300+ predictors. So I decided to use glmnet and LASSO. Below please see my code:</p>

<pre><code>fit.lasso = glmnet(x, y,family=""binomial"",alpha = 1)
    plot(fit.lasso, xvar = ""lambda"", label = TRUE)
    cv.lasso = cv.glmnet(x,y,family=""binomial"",alpha = 1)
    plot(cv.lasso)
    coef(cv.lasso)
    cv.lasso$lambda.min
        bestlam = cv.lasso$lambda.min
    lasso.pred=predict(fit.lasso,s=bestlam,newx = x,type = ""response"")
</code></pre>

<p>I have two questions and appreciate any helps.</p>

<ol>
<li>(removed since it was more related to purely programming question)</li>
<li>I have used CV to select lambda but I didn't partition the data into training and testing. Is it necessary since I have already used CV? I will need the <code>lasso.pred</code> to compare with actual to calculate the prediction accuracy.</li>
</ol>

<p>Thank you in advance!</p>
"
"0.104446593573419","0.0995037190209989","194867","<p>At several time points, I sample different individuals from a population (say 60 ind/time point). I assign to each of them one category (either low &lt; middle &lt; high). I then want to model this ordinal dependent variables using several covariates including time (to evaluates temporal changes in proportions/probabilities). </p>

<p>If I had only one time point, I would use ordered logistic regression in R with polyR, vglm... But these time series, I fear to have residual autocorrelation. Can I easily check residual autocorrelation with these models? Should I include the lagged dependent variable as a covariate to remove it? What are the different alternatives to analyse this kind of data in R?</p>

<p>For the moment, I analysed separately each of the categorical variable level with binomial regression and the lagged dependent variable as one of the covariates but hope to find a better solution.</p>
"
"0.0603022689155527","0.0574484989621426","198219","<p>I ran the same Logistic regression with R and STATA. </p>

<p>The regressors include many dummy variables.</p>

<p>In R, the code I used is</p>

<pre><code>fit &lt;- glm(formula = y ~ ., family = ""binomial"", data = df)
</code></pre>

<p>which reports the warning message:</p>

<pre><code>glm.fit: algorithm did not converge 
</code></pre>

<p>In STATA, I simply ran</p>

<pre><code>Logit y x1 - x20
</code></pre>

<p>and the reported table looks OK and the estimation seems to be reasonable.</p>

<p>Actually if I have only a few regressors, say only $x_1, x_2, x_3$, they report the same result.</p>

<p>I'm wondering why there could be such difference? In R, how to fix the problem?</p>

<p>Thanks a lot!</p>
"
"0.105528970602217","0.114896997924285","198737","<p>I have a dataset with the following variables:</p>

<ul>
<li>proportion of species present, between 0 and 1 (called speciesProp)</li>
<li>a binomial (0,1) presence/absence of the same species (called PA in the model)</li>
<li>year</li>
</ul>

<p>The dataset has many 0s in the proportion and binomial columns. These are actual 0 values collected in the field.</p>

<p>I want to know if the proportion of species is increasing over the year (controlling for random effects)</p>

<p>I logit transformed my proportional data, and then originally I thought of running a linear mixed effects model in lme4 as follows:</p>

<pre><code>m01 &lt;- lmer(speciesPropLOGIT ~ year + (1|referenceID), data = speciesAll)
</code></pre>

<p>But then wondered if the following was more appropriate:
1) a logistic model of the binomial presence / absence data as follows:</p>

<pre><code>model &lt;- glm(PA ~ year , family = binomial(link = ""cloglog""), data = speciesAll)
</code></pre>

<p>followed by the following linear mixed effects model, where the proportion is the response variable and excluding the 0s:</p>

<pre><code>m01 &lt;- lmer(speciesPropLOGIT ~ year + (1|referenceID), data = speciesAll)
</code></pre>

<p>Someone suggested that I think about model multiplication, of the two outputs, the first estimating the proportion of species over year with the 1s and 0s, and the second looking at the positive data over time. I then would like to plot one line of model fit.</p>

<p>When i run the models separately, year comes out as significant in all of them.</p>

<p>I have spent a long time looking for advice on how to do it, but can't seem to find any. </p>

<p>Also - do I need to have family = binomial somewhere if I have logit transformed the proportional data?</p>

<p>Hope you can help?</p>
"
"0.0644658371220304","0.0614150287145074","198925","<p>Although there has been some detailed discussions about power analysis on this website (for example <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">here</a> and <a href=""http://stackoverflow.com/questions/27234696/how-do-you-conduct-a-power-analysis-for-logistic-regression-in-r"">here</a>), the answer provided to this question has  outlines the steps to simulating a power analysis, <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>.</p>

<p>Say we take some data (data was linked to a <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">bootstrapping question</a>)</p>

<p>We create a regression that will predict <code>admit</code> based on the two continous variables <code>gpa</code> and <code>gre</code></p>

<ul>
<li>Now we have a <code>n=400</code>. </li>
<li>We can then elect our power level, <code>alpha = 0.5</code></li>
<li>The effect size you would like to detect, e.g., odds ratios  (we obtain this from our regression)</li>
</ul>

<p>So in following the detailed method provided by @gung <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>, I want to run the simulation. Here is the code I have adjusted, but my output is not correct. Can someone outline what I have not understood</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
head(mydata)

set.seed(1234)

my.mod &lt;- glm(admit ~ gre + gpa , data = mydata, family = ""binomial"")


repetitions &lt;- length(mydata$admit)

gre &lt;- mydata$gre
    gpa &lt;- mydata$gpa


significant = matrix(nrow=repetitions, ncol=4)

for(i in 1:repetitions){
  responses          = mydata$admit
      #responses          = rbinom(n=N, size=1, prob=mydata$admit)      # we can interchange this comment
  model              = glm(responses ~ gre + gpa, family = binomial(link=""logit""))
  significant[i,1:2] = (summary(model)$coefficients[2:3,4]&lt;.05)
      significant[i,3]   = sum(significant[i,1:2])
      modelDev           = model$null.deviance-model$deviance
  significant[i,4]   = (1-pchisq(modelDev, 2))&lt;.05
}



sum(significant[,1])/repetitions      # pre-specified effect power for gre

sum(significant[,2])/repetitions      # pre-specified effect power for gpa

sum(significant[,4])/repetitions  # power for likelihood ratio test of model

sum(significant[,3]==2)/repetitions   # all effects power

sum(significant[,3]&gt;0)/repetitions    # any effect power
</code></pre>
"
"0.0953462589245592","0.072667241951276","199336","<p>I have a dataset with hundreds of individual trees. These individuals were from seven sites that demonstrated different rates of land cover change over time (% change in area over 30 years). I'm looking to test how the rate of land cover change influences the growth trend of trees (outcomes for each individual could be positive, negative, or neutral - these are each columns in my dataset). </p>

<p>A reviewer suggested that logistic regression would be most appropriate for this data, but I have not been able to find a method of performing a multiple logistic regression of some sort that works for multiple binomial predictors. I could run individual logistic regressions for each outcome, however this strikes me as potentially inflating error. Does anyone have suggestions as to a test that could be used for this sort of data, or would it be best to use something like a bonferroni correction?</p>

<p>I'm working in R, if you have any specific suggestions for resources in the R programming environment!</p>

<p>Thanks in advance.</p>
"
"0.0603022689155527","0.0574484989621426","199504","<p>I would like to compare an average probability to a fixed probability value in order to determine if there is a significant difference between the two.</p>

<p>My participants had to detect and point a target that appeared among three distractors. The possibility they answered hazardously therefore is 1 out of 4 (i.e., .25). My dependent variable being binary (Correct answer: 1 ; Incorrect answer: 0) I am using logistic regression for my analyses:</p>

<pre><code>model1 &lt;- glm(Accuracy ~ Task * Masking,
              data = DF,
              family = binomial(link = ""logit""))
</code></pre>

<p>âŸ¶ <a href=""http://i.stack.imgur.com/AyPzG.png"" rel=""nofollow"">Plot of results</a></p>

<p>More precisely, I would like to know if the probability of report in the Reach-Masked condition is significantly different from 0.25.</p>

<p>How do I test this possibility in R?</p>

<p>Thank you for your time. Have a very nice day.</p>
"
"0.0426401432711221","0.0406222231851194","199787","<p>Hi I have a dataset where I have P number of predictors.  One of the predictors is ""Color"".  Color is always either ""Green"" or ""Red"". In the results of the glm call - which was btw - glm(target ~ color + age, data = d, family = binomial(logit)) - 
only ""colorGreen"" is there and it is not statistically significant.  Why does ""colorRed"" not have a row in the results?</p>

<p>Edit:  I am not looking for general logistic regression help.  This question is not a duplicate for another question that is a general question. This is a specific question. </p>
"
"0.0953462589245592","0.090834052439095","199912","<p>I have a large dataset with 4000 variables and 15000 observations. I am looking to build a predictive model using logistic regression. I believe that the glmnet package (using elastic net) is the best tool to use with such a large set of variables. Every variable of the 4000 is a moving average. I have split the dataset into two - training and testing.</p>

<p>When I run the code with glmnet I find something unusual happening. As I increase the number of variables for glmnet to select the model probabilities get more and more extreme which causes the misclassification rate to converge to 0%. I realise something is wrong but I cannot figure what it is.</p>

<p>Here is the code I have used:</p>

<pre><code>x &lt;- as.matrix(training[1:4000])
newx &lt;- as.matrix(testing[1:4000])

model &lt;- cv.glmnet(x, y, alpha = 0.5, family = 'binomial')

predict(model, type = ""coefficients"",s = model$lambda.min)
predict(model, newx, type= ""response"",s = model$lambda.min)
</code></pre>

<p>Is this overfitting?
I also read that categorical variables need to worked around with glmnet - none of the 4000 are categorical but they are grouped by external categorical vars.</p>

<p>I'm desperate for some help!</p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.0966987556830456","0.107476300250388","200031","<p>I have very easy question that I'm hoping someone can assist me with:</p>

<p>I ran an example logistic regression using this R code:</p>

<pre><code>     hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5)
        pass &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
        data &lt;- data.frame(hours, pass)
        mylogit &lt;- glm(pass ~ hours, data = data, family = ""binomial"") #Activates the logistic regression model
        summary(mylogit) #Summary of the model

    Call:
    glm(formula = pass ~ hours, family = ""binomial"", data = data)

    Deviance Residuals: 
         Min        1Q    Median        3Q       Max  
    -1.70557  -0.57357  -0.04654   0.45470   1.82008  

    Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
    hours         1.5046     0.6287   2.393   0.0167 *
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 27.726  on 19  degrees of freedom
    Residual deviance: 16.060  on 18  degrees of freedom
    AIC: 20.06

    Number of Fisher Scoring iterations: 5

    round(exp(cbind(OR = coef(mylogit), confint(mylogit))),3)

               OR 2.5 % 97.5 %
   (Intercept) 0.017 0.000  0.281
    hours       4.503 1.698 23.223
</code></pre>

<p>I know that by taking the exponent of the log-odds/coefficients for hours the odds of passing increase by a factor of 4.503 for a one-unit change in hours.  However, given that the explanatory variable (hours) is continuous, what is considered a 'one-unit change' i.e. going from 1 to 2 hours as one unit? or from 1.75 to 1.76 hours as one unit?  Also, is this interpretation of one-unit the same for regular OLS regression as well? I'm seeking to better understand the rules R applies to creating its regression coefficients.  </p>
"
"0.147709789175199","0.128992883200554","200703","<p>I'm using matched pairs logistic regression (1-1 matched case-control; Hosmer and Lemeshow 2000) to model differences between vegetation selected at nest sites vs. paired random sites. To do this, I created a data frame that contained the difference in vegetation measurements between nest and random sites (so nest minus random) and used R to fit a logistic regression model, using a vector of all 1's as the 'Response' and a no-intercept model.</p>

<p>Here's the data frame (I only include 1 of the covariates, grass density, for the example):</p>

<pre><code>nest&lt;-structure(list(VerGR = c(1.380952381, 1.952380953, 2.666666667, 
-3.809523809, 2.428571428, 2.142857143, 0.142857143, 2.095238095, 
1.952380952, 3.333333334, 3.190476191, -2.857142857, 2.857142858, 
-1.666666667, 0.523809524, 4.761904762, 0.571428571, 2.238095238, 
-2.809523809, 0.857142857, 1.523809524, -2.476190476, -0.428571428, 
-5.190476191, 4.142857143, 2.857142858, -2.476190476, 4.095238096, 
1.428571428, 1.714285714, -2.80952381, 3.142857143, 2.809523809, 
7.238095238, 2.523809523, 2.333333333, -0.095238096, -0.095238096, 
-0.142857143, 4.047619048, 4.761904759, -1.285714285, -1.190476191, 
2.523809524, -2.095238095, -2, 4.761904761, 8.952380952, 1.095238096, 
5.666666666, -0.714285714, 0, 2.809523809, -0.238095239, 3.666666667, 
0.904761905, -4.952380952, -3.666666667, 2, -0.619047619, 4.523809524, 
1.523809524, 4.619047619, 6.142857143, 3.19047619, -2.190476191, 
-1.666666667, 2.714285714, -1.285714286, 2.857142857, 2.761904762, 
2.809523809, -7.142857139, -5.952380949, -1.19047619, 1.523809524, 
-0.38095238, 5.571428571, 5.238095239, 2.047619048, 7.857142857, 
0.61904761, 2.523809524, -1.190476191), Response = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L)), .Names = c(""VerGR"", ""Response""), class = ""data.frame"", row.names = c(NA, 
-84L))
</code></pre>

<p>And the no-intercept logistic regression models I am running:</p>

<pre><code>grass.mod &lt;- glm(Response ~ VerGR - 1, data=nest, family=""binomial"")
grass2.mod &lt;- glm(Response ~ VerGR + I(VerGR^2) - 1, data=nest, family=""binomial"")
</code></pre>

<p>For the most part the models run fine, and give the same parameter estimates as models implemented using the 'clogit' function from the survival R package. The data set for the clogit models is slightly different, with Responses = 1 (nest) or = 0 (random point), and includes a column called 'PairID' to indicate nest-random pairs. Here's what the clogit models look like:</p>

<pre><code>library(survival)
grass.mod.clog &lt;- clogit(Response ~ VerGR + strata(PairID), data=full)
grass2.mod.clog &lt;- clogit(Response ~ VerGR + I(VerGR^2) + strata(PairID), data=full)
</code></pre>

<p>But when I run the glm's, I get these 2 warnings if using a quadratic term:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>I'm able to satisfy the first warning if I use more iterations in the glm formula, but I'm not sure what is happening with the second warning. I would be glad to use the 'clogit' function (which works with quadratic terms), but I'm unsure how to create prediction plots to visually display the data when going that route. Any suggestions?</p>

<p>Thanks,
Jay</p>
"
"0.0639602149066831","0.0609333347776791","201462","<p>I'm fitting a logistic regression model with <code>patient_group</code> (0,1) as response variable and the explanatory variable being an interaction between two SNPs. When running summary for the model, the alert 'Coefficients: (1 not defined because of singularities)' is shown, and I guess it is due to the fact that the combination AACT has 0 observations. </p>

<p>My question is whether the statistics are still valid, or is there a better way to analyse this kind of data? (The SNPs are located close to each other and are most likely strongly linked.)</p>

<pre><code>&gt; table(data$SNP1, data$SNP2)    
     CC CT
  TT 27  9
  AT 83 14
  AA 47  0
&gt; model &lt;- glm(patient_group ~ SNP1 * SNP2, data=data, family=""binomial"")
&gt; summary(model)
Call:
glm(formula = patient_group ~ SNP1 * SNP2, family = ""binomial"", 
data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2735  -0.9072  -0.7679   1.4742   1.8365  

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    -1.4816     0.4954  -2.991  0.00279 **
SNP1AT          0.8065     0.5471   1.474  0.14048   
SNP1AA          0.4112     0.5978   0.688  0.49158   
SNP2CT          1.7047     0.8339   2.044  0.04093 * 
SNP1AT:SNP2CT  -2.3289     1.0833  -2.150  0.03157 * 
SNP1AA:SNP2CT       NA         NA      NA       NA   

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 218.19  on 179  degrees of freedom
Residual deviance: 212.31  on 175  degrees of freedom
(26 observations deleted due to missingness)
AIC: 222.31

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.0953462589245592","0.090834052439095","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"NaN","NaN","202211","<p>I have a logistic regression in R whose goal is to predict the probability of default on some test data. </p>

<p><code>glm(default ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>What I'd like to do is 'bin' this data so that bins 1 to n each have a certain rate of default. How can I bin the logistic regression results in this way? For example, the bins on a sample set of 1000 might look like:</p>

<pre><code>Bin# P(Default) Count
1         4%     400
2         2%     300
3         1%     300
</code></pre>

<p>That is, I set in advance the probabilities I want each bin to have (.04,.02,.01) and then bins are created based around those settings.</p>
"
"0.0738548945875996","0.0469065029820195","203544","<p>I am running a logistic regression in R on data that is approximated with the following code: </p>

<pre><code>z &lt;- c(0,0,0,0,0,0,0,0,0,0)
y &lt;- round(runif(10, min = 0, max = 100))
y1 &lt;- c(z, y)
y2 &lt;- rep(100, 20)

x1 &lt;- runif(20)
x2 &lt;- runif(20)
response &lt;- as.data.frame(cbind(y1, y2, x1, x2))
response
</code></pre>

<p>Fitting the logistic model:</p>

<pre><code>lrfit &lt;- glm(cbind(y1, y2) ~ x1 + x2, data = response, family = binomial(logit))
</code></pre>

<p>I have fit the model using a two-column matrix with successes and failures (non-negative), but there are many 0s in the data (close to half). So, two questions. 1) How does R handle zeroes in the response variable in a logistic regression? And 2) How might this be problematic when interpreting our modeling results?</p>

<p>For context, this data shows the number of extinctions (y1) over 100 replicates (y2) in simulated landscapes with varying degrees of habitat availability (x1) and fragmentation (x2). Each row represents a different species. So, zeroes in y1 represent no extinctions across 100 replicates.</p>

<p>Thank you in advance.</p>
"
"0.143125289466427","0.125863355105511","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.0426401432711221","0.0406222231851194","204192","<p>I have a categorical response which i want to predict, so i am in the process of developing a logistic model.  I am using k-fold cross-validation for model selection. The first model, which was just an intercept model is throwing negative fitted values.</p>

<p>So i tried adding just 2 predictors to understand what was causing this, but the model with the 2 predictors is also predicting negative probabilities.</p>

<p>Below is the code that i used:</p>

<pre><code>    logistic_null1 &lt;- glm(SeriousDlqin2yrs ~ 1, family=binomial(), data=trainingdata)

logistic_null1 &lt;- glm(SeriousDlqin2yrs ~ age + income, family=binomial(), data=trainingdata)
</code></pre>

<p>I checked if may be the response Y is not a factors but doesn't seem to be the case either</p>

<pre><code>&gt; class(trainingdata$SeriousDlqin2yrs)
[1] ""factor""
&gt; check3 &lt;- as.data.frame(predict(logistic_null1, testdata))
&gt; summary(check3)
 predict(logistic_null1, testdata)
 Min.   :-4.609                   
 1st Qu.:-3.047                   
 Median :-2.700                   
 Mean   :-2.703                   
 3rd Qu.:-2.346                   
 Max.   :-1.601 
</code></pre>

<p>What could cause this</p>
"
"0.0870388279778489","0.0829197658508324","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.0953462589245592","0.090834052439095","206702","<p>I have some data to fit a logistic regression, although the data seems quite good, the resulted fit does not look as expected.</p>

<blockquote>
<pre><code>  paramValue      normality
1  3.69             0
2  1.16             0
3  6.12             1
4  2.78             1
5  1.45             1
6  3.56             0
</code></pre>
</blockquote>

<pre><code>mylogit &lt;- glm(normality ~paramValue,  family = binomial(link=""logit""))
summary(mylogit)
</code></pre>

<blockquote>
<pre><code>Call:
glm(formula = normality ~ paramValue, family = binomial(link = ""logit""))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.44994  -0.73312   0.08151   0.63377   1.41140  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.9945     0.9531  -2.093   0.0364 *
paramValue    1.2582     0.5655   2.225   0.0261 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 29.065  on 20  degrees of freedom
Residual deviance: 19.746  on 19  degrees of freedom
AIC: 23.746

Number of Fisher Scoring iterations: 5
</code></pre>
</blockquote>

<pre><code>    plot(paramValue,normality)

    x &lt;- seq(-1, 6, 0.1)

curve(predict(mylogit,data.frame(paramValue=x),type=""response""),add=TRUE, col=""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1f0DY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1f0DY.png"" alt=""enter image description here""></a></p>

<p>Did I do something wrong? Is there any way to force the regression to cross the origin?</p>
"
"0.0426401432711221","0.0406222231851194","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.0603022689155527","0.0574484989621426","206790","<p>I have a count variable that represents the number of new band foundings in a country-year. However, there is zero inflation as there are no foundings for most country-year. There is also overdispersion as the variance is greater than the mean number of foundings. </p>

<p>Under these circumstances, a zero-inflated negative binomial model would fit best however I don't really see how the logistic step fits with the nature of my data. In other words, I don't think the assumptions of ZINB are satisfied. Is there an alternative model that I could use to get around the issue? Any help would be appreciated.</p>

<p>UPDATE 1: </p>

<pre><code>summary(zinb1 &lt;- zeroinfl(foundings ~ disbandings + pop100k | disbandings + pop100k,
           data = casper, dist = ""negbin""))
</code></pre>

<p>However I get the following warning message: </p>

<pre><code>Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred 
2: In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>
"
"0.104446593573419","0.0995037190209989","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.0738548945875996","0.0703597544730292","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.0426401432711221","0.0406222231851194","208895","<p>I have a logistic regression model where Pstatus (a binary variable is coded as 1,2 (1 being apart, 2 being together in R see code below) has a coefficient of 0.8. I am wondering how I should interprete this coefficient of 0.8. And the dependent variable is alcohol consumption. </p>

<p>Is this positive coefficient telling me that being together increases the likelihood/log(odds ratio) of alcohol consumption? </p>

<pre><code>   logit.model&lt;-glm(alc~sex+age+famsize+Pstatus+Medu+Fedu+studytime+
   activities+romantic+famrel+freetime+goout+health+absences+grades,
   data=h,family=""binomial"")

   **Result** 
   PstatusT       0.800220   0.281027   2.847 0.004407 ** 

   Pstatus   : Factor w/ 2 levels ""A"",""T"": 1 2 2 2 2 2 2 1 1 2 ...
</code></pre>
"
"0.0953462589245592","0.090834052439095","209773","<p>I am estimating a model of the type (logistic regression with random slopes and random intercepts clustered by the variable ID):</p>

<pre><code>formula = result ~ year + (1 | ID) + (year | ID)
</code></pre>

<p>using</p>

<pre><code>glmer(formula, data = data1, family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 1)
</code></pre>

<p>However, I am getting results for three random effects:</p>

<pre><code>Random effects:
 Groups Name        Variance  Std.Dev.  Corr 
 ID    (Intercept) 3.645e-01 0.6037203      
 ID.1  (Intercept) 1.228e+00 1.1082860      
        year       3.043e-07 0.0005516 -1.00
</code></pre>

<p>Is there something I am specifying incorrectly?</p>

<p>On the other hand, the fixed effects are correct:</p>

<pre><code>Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 55.328196   8.052619   6.871 6.38e-12 ***
year       -0.027933   0.004008  -6.969 3.20e-12 ***
</code></pre>
"
"0.104446593573419","0.0995037190209989","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.0738548945875996","0.0703597544730292","210914","<p>I have a logistic regression model obtained in R comparing association between two index diagnoses (0 or 1) with <code>Age</code> (continuous) + <code>Sex</code> (Factor) + <code>Renal.Fn</code> (continuous). My variable of interest is <code>Renal.Fn</code>. </p>

<pre><code> model &lt;- glm(diagnosis ~ Age + Sex + Renal.Fn, data = data, family = ""binomial"")
</code></pre>

<p>Currently to obtain Odds Ratio:</p>

<pre><code>exp(coef(model))[4]       # Renal.Fn 0.9884664
exp(confint(model))[[4]]  # Renal.Fn 0.9848022 0.9920815
</code></pre>

<p>My interpretation: Per unit increase in renal function the odds of diagnosis of interest reduces.</p>

<p>I wish to demonstrate the opposite.  For example: Per unit decrease in renal function, the odds of diagnosis increases.</p>

<p>Question:</p>

<ol>
<li>Is it correct to take <code>1 / exp(coef)</code> to derive the odds per unit decrease?</li>
<li>Is it subsequently correct to take <code>1/exp(coef) ^10</code> to derive odds per ten unit decrease?</li>
</ol>
"
"0.0852802865422442","0.0812444463702388","211174","<p>We are oversampling the data to use in logistic regression. Aim  is to predict CTR(click probability) which is rare event scenario.
I have predicted the probabilities of click but CTR results are inflated as we over sampled positive class.</p>

<p>model2&lt;-SMOTE(V61 ~ ., z2, perc.over = 600,perc.under=100, learner = 'glm',family=binomial())</p>

<p>Is there any way to undo oversampling results so that I can get exact probabilities ? Based on research so far, one easiest way to divide the output probability by the multiplier we used in over sampling. I dont feel it would be the exact way as I have used synthetic minority over sampling technique(SMOTE) in R.</p>
"
"0.104446593573419","0.0995037190209989","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.0426401432711221","0.0406222231851194","212392","<p>I have just started learning classification techniques in R. I am using a Yelp dataset to predict whether a user is an elite user or not. I tried to build a trained model using logistic regression that I can use to predict on my test dataset. However, when I run glm() on my independent and dependent variables, I get following warning:</p>

<pre><code>logreg.model = glm(eliteStatus~nmonths+review_count+fans+total_compliments+
                total_votes+nfriends+AverageLeniencyScore,data=train_vardata,family=binomial(),maxit=100)
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<p>The other links suggest there might be a problem of perfect separation but I am failing to understand how my variables are accounting for it. All my IVs are continuous. I would really want to get rid of this warning while running glm().</p>
"
"0.0953462589245592","0.090834052439095","212430","<p>When building models with the <code>glm</code> function in R, one needs to specify the family. A family specifies an error distribution (or variance) function and a link function. For example, when I perform a logistic regression, I use the <code>binomial(link = ""logit"")</code> family.</p>

<p>What are (or represent) the error distribution (or variance) and link function in R ?</p>

<p>I assume that the link function is the type of model built (hence why using the <code>logit</code> function for the logistic regression. But I am not too sure about the error distribution function.</p>

<p>I had a look at R's documentation but could not find detailed information other than how to use them and what parameters can be specified.</p>
"
"0.14142135623731","0.134728672455117","212611","<p>I have ran these two Logistic Regression models (below) on some small data and I am able to interpret the output - significance and direction - of the regressors, but I do not know for sure how to interpret all the data which is supposed to tell me everything related to <strong>effect (size) etc</strong>. I did select my predictors properly by adding one each time and checking whether the model was still significant (which yielded the same result as an automatic stepAIC from the MASS package) and I also did some diagnostic checks (outliertest, VIF-score).</p>

<p>What (I think) I got from the models is:</p>

<ul>
<li><strong>R2</strong>: model1 only explains 4.8% of all variation and model2 6.6%, so no predictive power?</li>
<li><strong>C</strong>: model1 does not have acceptable discrimination, neither does model2 (&lt;0.7)</li>
</ul>

<p>Is there other <strong>important information that I am ignorant of</strong>? It seems that these models do <strong>not have much 'power'</strong> (according to <strong>R2</strong> and <strong>C</strong>), but how are they then <strong>still significant</strong> (there is also very significant behaviour (***) for regressors)?</p>

<p>*PS: Sorry if am missing obvious things - I do not have that strong of a statistical background. I am also finding it a hard time searching for all the parameters and metrics since they are often denoted by a one letter name (e.g. C, g) - which is not easy to search for if you do not know what you are looking for... So that's why I came to CrossValidated!</p>

<p>I have found <a href=""http://stats.stackexchange.com/questions/104485/logistic-regression-evaluation-metrics"">this question</a>, but it does not really have an answer since it's maybe way too vague? If someone else has a reading suggestion for my problem, that's also welcomed!*</p>

<h2>First model: Agentivity ~ Period + Genre</h2>

<pre><code>(from lrm)      
                     Model Likelihood      Discrimination    Rank Discrim.    
                       Ratio Test            Indexes           Indexes       
Obs          700    LR chi2      25.55    R2       0.048    C       0.602    
 strong      403    d.f.             4    g        0.440    Dxy     0.204    
 weak        297    Pr(&gt; chi2) &lt;0.0001    gr       1.553    gamma   0.240    
max |deriv| 3e-14                         gp       0.105    tau-a   0.100    
                                          Brier    0.236                     

(from glm)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3171  -0.9825  -0.8094   1.1882   1.6090  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 954.29  on 699  degrees of freedom
Residual deviance: 928.74  on 695  degrees of freedom
AIC: 938.74

Number of Fisher Scoring iterations: 4
</code></pre>

<h2>Second model: Type ~ Period</h2>

<pre><code>(from lrm)
                      Model Likelihood      Discrimination    Rank Discrim.    
                         Ratio Test           Indexes           Indexes       
Obs           872    LR chi2      36.70    R2       0.066    C       0.637    
 mediopassive 701    d.f.             2    g        0.552    Dxy     0.275    
 passive      171    Pr(&gt; chi2) &lt;0.0001    gr       1.736    gamma   0.401    
max |deriv| 9e-10                          gp       0.087    tau-a   0.087    
                                           Brier    0.151                     

(for glm)
Deviance Residuals: 
 Min       1Q   Median       3Q      Max  
-0.8645  -0.6109  -0.4960  -0.4960   2.0767  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 863.19  on 871  degrees of freedom
Residual deviance: 826.49  on 869  degrees of freedom
AIC: 832.49

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.11570838237598","0.122480611322833","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.148148749397529","0.151994441447782","213531","<p>I am trying to analyse my data using bam. And I would greatly appreciate your advice as to the appropriate analyses.</p>

<p>The experimental design is: 
There are two groups of participants, ""CAT"" and ""PA"" coded in the factor ""group.""
Within each group, there are two conditions, ""Label"" and ""Ideo"" coded in the factor ""cnd.""
The prediction is that performance of participants in the Label condition would be higher compared to the Ideo condition, and that this difference would be greater in the CAT group compared to the PA group. So, my hypothesis forces me to test for an interaction of group by condition. The dependent variable is accuracy (so I need a logistic model) which greatly depends on time (coded as ""ctrial""). ""Sbj"" codes participants that will be treated as random effects. </p>

<p>My understanding of gamms is that If I had two isotropic continuous variables I could use s(x1,x2) and If I had two non-isotropic variables I could use te(x1,x2) to model interaction. But this is not the case, because my variables of interest are factors.</p>

<p>I believe that the standard approach is that I should create a variable with four levels (group by condition)</p>

<pre><code>data$igc &lt;- as.factor(paste(as.character(data$group),as.character(data$cnd),sep=""."")) 
</code></pre>

<p>and use an additive model such as:</p>

<pre><code>bam(acc~ 1 + igc + s(ctrial, by=igc) + s(sbj, bs = ""re""), data=data, family=binomial)
</code></pre>

<p>I could then inspect the plots and find out if CAT. label - CAT.ideo is greater compared to PA.label- PA.ideo.</p>

<p>But do I really need the 4-level variable?
I mean, is my 4-level model the most parsimonious one?</p>

<p>If there is no differences between the two groups, wouldn't a model such as 
    bam(acc~ 1 + cnd + s(ctrial, by=cnd) + s(sbj, bs =""re""), data=data, family=binomial)
be more appropriate?</p>

<p>[I believe that this question is not the same as the one posted here: <a href=""http://stats.stackexchange.com/questions/32730/how-to-include-an-interaction-term-in-gam"">How to include an interaction term in GAM?</a>, as my question is related to model comparison] </p>

<p>My best guess is that I should do the following: </p>

<pre><code>Analysis of Deviance Table

Model 1: acc ~ 1 + igc + s(ctrial) + s(sbj, bs = ""re"")
Model 2: acc ~ 1 + group + s(ctrial, by = group) + s(sbj, bs = ""re"")
Model 3: acc ~ 1 + cnd + s(ctrial, by = cnd) + s(sbj, bs = ""re"")
Model 4: acc ~ 1 + igc + s(ctrial, by = igc) + s(sbj, bs = ""re"")    
  Resid. Df Resid. Dev     Df Deviance  Pr(&gt;Chi)    
1     18350     8495.0                              
2     18347     8497.3 2.1307   -2.262              
3     18346     8474.8 1.3922   22.419 4.861e-06 ***
4     18338     8456.5 7.7730   18.327   0.01667 *  
</code></pre>

<p>Is my understanding correct?
And is this the correct way to ""justify"" the use of a 4-level variable?</p>

<p>Thank you in advance for your time,</p>

<p>Fotis</p>
"
"0.0426401432711221","0.0406222231851194","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"0.0603022689155527","0.0574484989621426","214022","<p>I was trying out the Subselect R package to see how it worked and if it would be useful for a logistic regression problem I'm working on.  <a href=""https://cran.r-project.org/web/packages/subselect/vignettes/subselect.pdf"" rel=""nofollow"">Link</a> to the package.</p>

<p>I decided I would follow Example 4 on page 28 to see if I could perform the Anneal function on the glm I previously fit to my data.  I used the helper function, glmHmat, to extract the required matrices in the same way sa done in example 4.</p>

<pre><code>Fullmodel&lt;-glm(G,family=binomial,data)
Hmat &lt;- glmHmat(Fullmodel)
</code></pre>

<p>I then tried the anneal function and got the following error.</p>

<pre><code>test&lt;-anneal(Hmat$mat,1,10,H=Hmat$H,r=1,nsol=10,criterion = ""Wald"")
</code></pre>

<p>Yet, I got this error.</p>

<pre><code>Error in validmat(mat, p, tolval, tolsym, allowsingular = FALSE, algorithm) : 

 The covariance/total matrix supplied is not symmetric.
  Symmetric entries differ by up to 1.02445483207703e-07.
</code></pre>

<p>So, I thought I would test if this were true:</p>

<pre><code>isSymmetric(Hmat$mat,tol=1e-09)
[1] TRUE
isSymmetric(Hmat$H,tol=1e-09)
[1] TRUE
</code></pre>

<p>So I can't make heads or tails of this error message.  Any ideas?</p>
"
"0.0852802865422442","0.0812444463702388","214052","<p>We are currently collecting data for a study whose purpose is to show whether scientists are focusing more or less on a specific subject with time. To keep some privacy let's say the subject is <em>jelly beans</em>: we reviewed a thousand random studies and we checked whether they were about jelly beans or not. The dataset has only two columns and it looks like:</p>

<pre><code>| JellyBeans | Year |
|------------|------|
|    YES     | 2010 |
|    NO      | 2001 |
|    NO      | 2010 |
|    NO      | 2015 |
|    YES     | 2009 |
|    NO      | 2016 |
|    ...     | .... |
|    YES     | 1999 |
</code></pre>

<p>We thought of using logistic regression for the purpose as the DV is categorical. In R, this would look something like:</p>

<pre><code>logreg_jelly_year = glm(JellyBeans ~ Year, family = ""binomial"", data = dataset)
</code></pre>

<p>We have, however, some doubts about the validity of the procedure, in particular:</p>

<ol>
<li>Is there any specific assumption we have to check that could jeopardise the scientific value of the procedure?</li>
<li>Is the fact that <code>Year</code> is not truly continuous a problem?</li>
<li>Is there any other test or procedure that we should run on top or instead of logistic regression?</li>
</ol>
"
"0.0738548945875996","0.0703597544730292","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.0426401432711221","0.0406222231851194","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.170856428594066","0.162770747059404","216122","<p>As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. </p>

<p>Normally, we can use <code>glm</code> to obtain the logistic models by <code>glm(y ~ x1+x2, data = dat, family = binomial(logit))</code>. </p>

<p>For frm, we change <code>family = binomial(logit)</code> to <code>family = quasibinomial(logit)</code>.  </p>

<p>I noticed we can also use <code>family = binomial(logit)</code> to obtain frm's parameter since it gives the same estimated values. See the following example</p>

<pre><code>library(foreign)
mydata &lt;- read.dta(""k401.dta"")


glm.bin &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = binomial('logit'))
summary(glm.bin)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = binomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.074e+00  8.869e-02  12.110  &lt; 2e-16 ***
mrate        5.734e-01  9.011e-02   6.364 1.97e-10 ***
age          3.089e-02  5.832e-03   5.297 1.17e-07 ***
sole         3.636e-01  9.491e-02   3.831 0.000128 ***
totemp      -5.780e-06  2.207e-06  -2.619 0.008814 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: 1997.6

Number of Fisher Scoring iterations: 6
</code></pre>

<p>And for <code>family = quasibinomial('logit')</code>,</p>

<pre><code>glm.quasi &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = quasibinomial('logit'))
summary(glm.quasi)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = quasibinomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.074e+00  4.788e-02  22.435  &lt; 2e-16 ***
mrate        5.734e-01  4.864e-02  11.789  &lt; 2e-16 ***
age          3.089e-02  3.148e-03   9.814  &lt; 2e-16 ***
sole         3.636e-01  5.123e-02   7.097 1.46e-12 ***
totemp      -5.780e-06  1.191e-06  -4.852 1.26e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2913876)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>The estimated Beta from both <code>family</code> are the same, but the difference is the SE values.  However, to obtain the correct SE, we have to use <code>library(sandwich)</code> as in this <a href=""http://stackoverflow.com/questions/37584715/fractional-response-regression-in-r"">post</a>.</p>

<p>Now, my questions:</p>

<ol>
<li>What is the difference between these two codes?</li>
<li>Is frm about to obtain robust SE?</li>
</ol>

<p>If my understanding is not correct, please give some suggestions.</p>
"
"0.0639602149066831","0.0812444463702388","217639","<p>Using the glmer() function in the LME4-library in R I computed logistic models, of the form: Y ~ cat1 * cont1 + (1|Subject) where, obviously, Y is the binomial outcome variable (0 or 1), cat1 is a categorial variable (0,1,2) and cont1 is a continuous variable). Then, using confint(model, method = ""boot"") I computed confidence interval on the variables.</p>

<p>Now I would like to plot a graph of the chance P(Y==1), I want to plot P against cont1 for every cat1.</p>

<p>So you'd say: X = B(0) + B(cont1) * cont1 + B(cat1:1) * (cat1==1) + .... + etc
And then: P(Y==1) = 1/(1-exp(-X))</p>

<p>Which does exactly what I expect. But now, I want to incorporate the bootstrapped confidence intervals (so not std. error * 1.96!!) in the graph. I have the numbers, I do not know how to interpret them, what would be the formula for e.g. the 97.5 % line and the 2.5 % line?</p>

<p>Thanks in advance!</p>

<p><strong>EDIT</strong>
Is this the correct way?
Basically taking 10000 samples with replacement, same size as original data, creating the model, computing the predictions, and taking the 97.5th and 2.5th intervals of the predictions.</p>

<pre><code>prediction_pars = expand.grid(cont1= seq(-4,4,.05), cat1= as.factor(c(1,2,3)));

predictions = array(dim = c(10000, dim(prediction_pars)[1]));

for (i in 1:10000){
  new_sample = data[sample(nrow(data), samplesize, replace = T) , ];
  new_model  = glmer (Y ~ cont1 * cat1 + (1|Subject), dat=newdat, family=""binomial"");
  predictions[i , ] = predict(new_model, newdat = new_sample, re.form = NA);
}

hi = lo = array(dim = dim(prediction_pars)[1]);
for (i in (1, dim(prediction_pars)[1])){
  hi[i] = sort(predictions[,1]) [9750];
  lo[i] = sort(predictions[,1]) [ 250];
}
</code></pre>
"
"0.120604537831105","0.114896997924285","218471","<p>Apologies if this has been asked before, I have done a lot of googling but finding it hard to wrap my head around this particular problem.  Also apologies if I use the wrong terminology for things, I am self learning statistics and I am just starting.</p>

<p>I am using R to find a logistical model for a data set consisting of 314 rows.  I am using 3 variables to find a ""risk"" value.  The code I wrote so far is as follows.</p>

<pre><code>model = glm(binary_result ~ var1 + var2 + var3, data = import_data, family = binomial(link = ""logit""))

summary(model)
</code></pre>

<p>The estimated coefficients given were</p>

<pre><code>(Intercept)       -5.83951    
var1              0.06819   
var2              12.14347   
var3              -0.10594   
</code></pre>

<p>I can calculate my risk value as follows</p>

<pre><code>-5.83951 + 0.06819 * var1 + 12.14347 * var2 + -0.10594 * var3 = risk
</code></pre>

<p>In my data 57 out of 314 rows are equal to 1.</p>

<p>I now want to calculate an optimum threshold value for ""risk"" which will reduce my 1 to 0 ratio so that for every 10 rows I should only have 1 row equal to 1.  </p>

<p>Is R capable of doing this sort of thing, if so, how? I looked into 2 functions tfromx and optim.thresh already, but I am not sure if these are what I need or how I can apply them.</p>

<p>Please don't hesitate to ask more questions, I hope I made sense.</p>
"
"0.0301511344577764","0.0574484989621426","219241","<p>I would ask a question related to <a href=""http://stats.stackexchange.com/questions/157870/scikit-binomial-deviance-loss-function"">this one</a>.</p>

<p>I found an example of writing custom loss function for xgboost <a href=""http://dmlc.ml/rstats/2016/03/10/xgboost.html"" rel=""nofollow"">here</a>:</p>

<pre><code>loglossobj &lt;- function(preds, dtrain) {
  # dtrain is the internal format of the training data
  # We extract the labels from the training data
  labels &lt;- getinfo(dtrain, ""label"")
  # We compute the 1st and 2nd gradient, as grad and hess
  preds &lt;- 1/(1 + exp(-preds))
  grad &lt;- preds - labels
  hess &lt;- preds * (1 - preds)
  # Return the result as a list
  return(list(grad = grad, hess = hess))
}
</code></pre>

<p>Logistic loss function is</p>

<p>$$log(1+e^{-yP})$$</p>

<p>where $P$ is log-odds and $y$ is labels (0 or 1).</p>

<p>My question is: how we can get gradient (first derivative) simply equal to difference between true values and predicted probabilities (calculated from log-odds as <code>preds &lt;- 1/(1 + exp(-preds))</code>)?</p>
"
"0.0966987556830456","0.107476300250388","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.170856428594066","0.172345496886428","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0738548945875996","0.0703597544730292","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.14070529413629","0.143621247405357","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.0426401432711221","0.0406222231851194","224776","<p>So I have a dataset of presence (1) and absence (0) data, but it mainly consists of 0's (~80% of the 5200 observations). Now while constructing my binomial logistic model I am reading (<a href=""http://www.springer.com/us/book/9780387874579"" rel=""nofollow"">Zuurt <em>et al</em>. 2009</a>) as a guide. There is only a short description about the different link-choices for a binomial model and throughout the examples the standard logit-link is used. But the book also states that if you have more 0's than 1's, the cloglog-link is also an option.</p>

<p>How can I find out which model is better (just by comparing the AIC?) and is there any good description of the selection proces of these link-functions? Or maybe somebody here can give some advice.</p>
"
"0.148148749397529","0.151994441447782","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225645","<p>I am trying to perform a logistic regression with the following code </p>

<blockquote>
  <p><code>Y ~ x1+x2+x3,data=data, family=binomial(link=""logit"")</code>. </p>
</blockquote>

<p>However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? </p>
"
"0.113707048722992","0.121866669555358","225697","<p>Let me give a simple example,</p>

<pre><code>set.seed(100)
disease = sample(c(0,1),100,replace = TRUE)
snp1 = sample(c(""AA"",""AB"",""BB""),100,replace = TRUE)
snp2 = sample(c(""XX"",""XY"",""YY""),100,replace = TRUE)

summary(glm(disease~snp1*snp2, family = binomial))
</code></pre>

<p>output1</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  
snp1AB         5.232e-01  9.718e-01   0.538   0.5903  
snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .
snp2XY         4.074e-16  8.498e-01   0.000   1.0000  
snp2YY         1.504e+00  9.280e-01   1.621   0.1051  
snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  
snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  
snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  
snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71
</code></pre>

<p>Output2</p>

<pre><code>snp12 = interaction(snp1,snp2)
summary(glm(disease~snp12, family = binomial))


Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  
snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  
snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .
snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  
snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  
snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  
snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,</p>

<ol>
<li>Are both output1 and output2 same ? </li>
<li>If same, which one is more appropriate?</li>
<li>How to interpret the coefficients (and odds ratios) in each case?</li>
</ol>
"
"0.170560573084488","0.152333336944198","226330","<p>Overall, I'd like to be able to say that, for the logistic prediction for this row, ColA was more influential in driving up the resultant probability (ie, y_hat) than ColB. (We'll use y_hat as it's usually defined for logistic.) But is this possible? Some data scientists I've talked to say yes, but I've also seen push-back.</p>

<p>From what I've read, it seems that GLMs make it easiest to get at a per-row variable importance (see <a href=""http://stats.stackexchange.com/q/190482"">this limited discussion</a> on logit in particular, including push-back). But can they actually do it?</p>

<p>If B1 and B2 are coefficients and the cols in X represent our features, it would seem that if <em>B1</em>*X1 is greater than <em>B2</em>*X2 then <em>B1</em>*X1 would drive the resultant probability towards 1 more than <em>B2</em>*X2. Here's an example (which brings in a factor col, for a full treatment).</p>

<p>We create features X1 and X2, where X1 is random and X2 (I think we can agree) has a large positive impact on y:</p>

<pre><code>set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(1,0,1,0,1,0,1,0,1,0)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)
</code></pre>

<p>Now we create the logit model:</p>

<pre><code>fit.logit = glm(
formula = y~.,
data = df,
family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:       -1.2353      22.0041
Wald statistic:      -0.267        0.003
</code></pre>

<p>Now if we multiply <em>B1</em> and <em>B2</em> by <em>X1</em> and <em>X2</em> respectively and print the results:</p>

<pre><code>coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)

          X1       X2
1  -0.55087679 22.00411
2  -0.48751729  0.00000
3  -0.59755734 22.00411
4  -1.13510089  0.00000
5  -1.04245907 22.00411
6  -0.63908954  0.00000
7  -0.53998690 22.00411
8  -0.42395777  0.00000
9  -0.01916833 22.00411
10 -0.14575621  0.00000
</code></pre>

<p>We see that in the rows where X2 = 1 then <em>B2</em>*X2 (ie the second column) is much higher than <em>B1</em>*X1 (ie the first column). So it would seem that we could say that for those rows that X2 would be the dominant feature driving up the resultant prediction towards 1.</p>

<p>If one reverses the y dependency on X2 by replacing zeros for ones in X2, then after doing the multiplication, <em>B2</em>*X2 has a much lower value than <em>B1</em>*X1 when X2 = 1, which makes sense (since X2 now pushes y_hat towards 0 when X2 = 1). Thus, for these rows, X1 is actually more ""responsible"" for driving y_hat towards 1. (Note that if both results are negative, then the least negative would be the feature more responsible for y_hat being as high as it is.) Because of this, it would seem that this method of per-row feature ranking still works. What am I missing? </p>

<p>In case it helps, the code for the latter (reversed dependency) case is below: </p>

<pre><code># Reverse y dependency on X2
set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(0,1,0,1,0,1,0,1,0,1)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)

fit.logit = glm(
  formula = y~.,
  data = df,
  family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:        -1.235      -22.004
Wald statistic:      -0.267       -0.003

coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)
multiply_res

            X1        X2
1  -0.55087679   0.00000
2  -0.48751729 -22.00411
3  -0.59755734   0.00000
4  -1.13510089 -22.00411
5  -1.04245907   0.00000
6  -0.63908954 -22.00411
7  -0.53998690   0.00000
8  -0.42395777 -22.00411
9  -0.01916833   0.00000
10 -0.14575621 -22.00411
</code></pre>

<p>Overall, for logistic, can we accurately say (for example) that feature A drives y_hat toward 1 more than feature B, for this individual prediction? </p>

<p>Thanks, all!</p>
"
"0.159544807043493","0.151994441447782","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"NaN","NaN","228771","<p>I was trying to make a logistic model using both categorical as well as continuous dependent variables. But I am getting the following error.</p>

<pre><code>&gt; Logmodel = glm(Ever_60_in_12M ~ ., data= Train, family= binomial)
Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
  contrasts can be applied only to factors with 2 or more levels
</code></pre>

<p>Even if I removed the variable with only one category the same error is repeated. What can be the possible reason for this?</p>
"
"0.112815214963553","0.107476300250388","229211","<p>I have a logistic mixed model, where I look at the effect of two conditions over  accuracy (% correct) for different subjects. </p>

<pre><code>glmres = glmer(accuracy ~ condition + (1|subject), data = myData, family = binomial) 
</code></pre>

<p>All subjects saw the same 100 stimuli (words), but the experimental condition these words belonged to depended on the subject. So for example, for some subjects word A belonged to condition C1 and word B belonged to condition C2. For some subjects it was the other way around. The words were drawn from a bigger pool so I think I should include them as a random effect. </p>

<p>The problem is, the correspondence between words and conditions could not be (precisely) controlled and it is the case that for <em>most</em> subjects (about 70:30, depending on the word), A belonged to condition C1.
This means that the fixed effect <code>condition</code> and the (potential) random effect <code>(1|word)</code> are not completely independent. </p>

<p>My question is whether it would be incorrect to include two non-independent effects (where one is fixed, one is random). As a result of this dependency (I think) my results change a lot according to whether I include the random effect or not. </p>
"
"0.112815214963553","0.107476300250388","229336","<p>I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.</p>

<p>The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.</p>

<p>I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)</p>

<p>Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:</p>

<p>I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciated</p>

<pre><code>Here is the R output and sample of dataset:





 &gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+ 
                                  D2011+D2012,family=binomial, data=orf)
      summary(mod)

    Call:
    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) + 
        D2011 + D2012, family = binomial, data = orf)

    Deviance Residuals: 
       Min      1Q  Median      3Q     Max  
    -1.862  -1.293   1.023   1.065   1.318  

    Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)        0.3917290  0.1626769   2.408    0.016 *
    F2011              0.0003269  0.0002782   1.175    0.240  
    F2012             -0.0003596  0.0002786  -1.291    0.197  
    as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708  
    D2011             -0.0311978  0.0272068  -1.147    0.252  
    D2012              0.0226963  0.0274981   0.825    0.409  
    Small sample data set:

    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012
    155     150     1       0               0               0
    740     760     2       0               1               1
    1000    850     1       0               0               0
    1630    1520    1       1               1               1
    0       460     1       0               0               0
    1300    1335    1       0               1               1
    450     450     1       0               0               0
    390     730     1       1               0               1
    390     380     2       0               0               0
    600     600     2       0               0               0
</code></pre>
"
"0.155126306998506","0.157637380649093","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"NaN","NaN","229824","<p>I understand for squared loss, add a $\frac 1 2$ to objective function will simplify many derivations, since the derivative of a square has a constant $2$.</p>

<p>Are we doing similar things to logistic loss? If not why, residual deviance is twice the negative log likelihood?</p>

<p>Few lines of code to demo my question.</p>

<pre><code>fit=glm(vs~mpg+hp+wt,mtcars,family = binomial())
p=fit$fitted.values
y=mtcars$vs

# these two values are the same
fit$deviance/2
-sum(y*log(p)+(1-y)*log(1-p))
</code></pre>
"
"0.221348628437144","0.2438224066595","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.112815214963553","0.107476300250388","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.163636363636364","0.173213741660499","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0603022689155527","0.0574484989621426","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.104446593573419","0.0995037190209989","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.0603022689155527","0.0574484989621426","234498","<p>First, many thanks to the developers of <code>stan_glmer</code>. I have fit a mixed logistic model, <code>value ~ (1 | site_no) + Is.mid + sinWeek + cosWeek</code>. When fitted by <code>glmer</code>, it returns just the variance on <code>site_no</code>. From <code>stan_glmer</code>, I get coefficients (intercept effects) on each site, plus a variable named <code>b[(Intercept) site_no:_NEW_site_no]</code> that is reported by <code>stan_summary</code>.</p>

<p>I am after marginal posterior of the random effects. Is that what the <code>_NEW_</code> coefficient is giving me? i.e., Is this a marginal posterior not conditioning on the site in the <code>new = dataset</code> provided?</p>

<p>Also, how do I find a variance on the random effect, or is the model not specified correctly? The call is below. I welcome any other suggestions as well. Thank you in advance for any help in finding documentation for these <code>stan_glmer</code> outputs.</p>

<pre><code>stan.T.mod &lt;- stan_glmer(form, iter = 50000, warmup = 5000, thin = 1,
family = binomial(link = ""logit""),
prior = student_t(location = 0, df = 1, scale = 10),
prior_intercept = student_t(location = 0, df = 7, scale = 1),
prior_covariance = decov(shape = 10, scale = 1),
chains = 4, seed = 0305991, data = sub.data)
</code></pre>
"
"0.0953462589245592","0.072667241951276","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.0738548945875996","0.0469065029820195","234998","<p>For testing purposes I made up some correlated data in R like this:</p>

<pre><code>mydata = data.frame(
  outcome   = c(1, 0, 1, 0, 0, 1, 1, 0, 1, 1),
  predictor = c(0.1, -0.2, 0, 0.1, -0.3, 0.3, 0.2, -0.1, 0.1, 0.1)
)
</code></pre>

<p>Then I did this in order to create a logistic model that modeled this data:</p>

<pre><code>model1 = glm(family = binomial, formula = outcome ~ predictor, data = mydata)
</code></pre>

<p>Running <code>plot(model1)</code> yields the following plots:</p>

<p><a href=""http://i.stack.imgur.com/tbdpD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tbdpD.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/4O3jW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4O3jW.png"" alt=""enter image description here""></a></p>

<p>I need answers to some questions in order to understand how to perform diagnostics on such a logistic model. As someone with only an introductory course in statistics I'm having trouble gathering knowledge on how to interpret the plots.</p>

<ol>
<li>What do the ""Predicted Values"" in the first plot represent?</li>
<li>What does residual mean in the context of logistic regression?</li>
<li>Which of these plots can in any way be useful for model diagnostics based on real data? How?</li>
</ol>
"
"0.0953462589245592","0.090834052439095","235402","<p>I am a newbie at R. I am trying to do some logistic regressions. My predictors are categorical, and most have more than two levels.</p>

<p>A couple questions:
1. It looks like R already creates the relevant contrasts for the categorical predictors (I am used to SAS where I need to specify all the contrasts). Is this correct for R? As in, I do <strong>not</strong> need to manually create the contrasts myself?</p>

<ol start=""2"">
<li><p>the ""family=binomial"" step in the glm syntax, will I always need to write this regardless of the number of predictors, and even if the categorical predictors have more than two levels (DV is always binary)?</p></li>
<li><p>For interpreting the results, if I use a categorical predictor (e.g., lettergroup) with different levels (a, b, c, d) as my predictor, with the binary DV as 1=yes and 0=no, if R created the lettergroupb contrast, and it is a significant, positive coefficient, this means b is more likely to say yes compared to a, c, and d? thank you!</p></li>
</ol>
"
