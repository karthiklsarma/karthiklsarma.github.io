"V1","V2","V3","V4"
"0.0935560539449976","0.0961138662664425","  5270","<p>I have some data which, after lots of searching, I concluded would probably benefit from using a linear mixed effects model. I think I have an interesting result here, but I am having a little trouble figuring out how to interpret all of the results. This is what I get from the summary() function in R:</p>

<pre><code>&gt; summary(nonzero.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>My question involves the relationship between the dependent variable (""relative.sents.A"") and ""relative.sents.B"" once the random factors are factored out. I gather that the t-value of 5.357 for relative.sents.B should be significant.</p>

<p>But does this show what the direction of the effect is? I am thinking that because the coefficient for the slope is positive that this means that as relative.sents.B increases, so does my dependent variable. Is this correct?</p>

<p>The book I've been using briefly mentions that the correlation reported here is not a normal correlation, but goes into no details. Normally, I'd look there to figure out the direction and magnitude of the effect. Is that wrong?</p>

<p>If I'm wrong on both counts, then what is a good (hopefully reasonably straightforward) way to discover the direction and size of the effect?</p>
"
"0.140334080917496","0.144170799399664","  5333","<p>I have a linear mixed-effect model which I hope will answer the question of whether an increase in the frequency of use of one word leads to an increase of the frequency of use of that word by another person in a conversation, factoring out random effects of subject and topic of conversation. The basic model I've come up with looks like this:</p>

<pre><code>Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>The ""dependent"" variable is relative frequency of use by one person, and the fixed variable is relative frequency of use by another. I decided to see what the R^2 would be:</p>

<pre><code>&gt; cor(nonzero$relative.sents.A, fitted(nonzero.lmer))^2
[1] 0.6705905
</code></pre>

<p>To see what proportion of this is due to the fixed effect, I made a new model with only the random effects:</p>

<pre><code>&gt; summary(r.only.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ 1 + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -678.2 -666.3  343.1   -696.7  -686.2
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.2868e-04 0.0113435
 abstract (Intercept) 7.8525e-06 0.0028022
 Residual             3.7643e-04 0.0194017
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.029149   0.002088   13.96
</code></pre>

<p>...and tried the same thing:</p>

<pre><code>&gt; cor(nonzero$relative.sents.A, fitted(r.only.lmer))^2
[1] 0.6882534
</code></pre>

<p>To my surprise, without that fixed effect, R^2 seems to increase!</p>

<p>Does this mean my model is useless? If so, any suggestions on what might be wrong? Or am I somehow misinterpreting these results? </p>
"
"0.14104105809173","0.144897105252812","  6224","<p>I used the <code>lmer</code> function in the <code>lme4</code> package in order to assess the effects of 2 categorical fixed effects (1Âº Animal Group: rodents and ants; 2Âº Microhabitat: bare soil and under cover) on seed predation (a count dependent variable). I have 2 Sites, with 10 trees per site and 4 seed stations per tree. Site and Tree are my (philosophically) random factors, but given that I have only two level for Site, it must be treated as a fixed factor. I have questions about how to interpret the results:  </p>

<ol>
<li>I made a model selection criterion based on QAICc, but the best model (lower QAICc) does not result in any significant fixed effect and other models with higher QAIC (e.g. the Full Model) did find significant fixed factors. Does this make sense?  </li>
<li>Given a fixed factor that is important to the model, how do I distinguish which level of fixed factor is influencing the response variable?  </li>
</ol>

<p>Finally, correlation between the fixed factors implies an incorrect estimation of the model?  </p>

<pre><code>FullModel=lmer(SeedPredation ~ AnimalGroup*Microhabitat*Site + (1|Site:Tree) + 
                                   (1|obs), data=datos,  family=""poisson"") 

QAICc(FM)104.9896

    enterGeneralized linear mixed model fit by the Laplace approximation 
Formula: SP ~ AG * MH * Site + (1 | Site:Tree) + (1 | obs) 
   Data: datos 
   AIC   BIC logLik deviance
 101.8 125.6  -40.9     81.8
Random effects:
 Groups    Name        Variance Std.Dev.
 obs       (Intercept) 0.20536  0.45317 
 Site:Tree (Intercept) 1.19762  1.09436 
Number of obs: 80, groups: obs, 80; Site:Tree, 20

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)       0.01161    0.47608   0.024   0.9805  
AGR             -18.97679 3130.76500  -0.006   0.9952  
MHUC             -1.60704    0.63626  -2.526   0.0115 *
Site2            -0.91424    0.74506  -1.227   0.2198  
AGR:MHUC         19.92369 3130.76508   0.006   0.9949  
AGR:Site2         1.02241 4431.84919   0.000   0.9998  
MHUC:Site2        1.80029    0.86235   2.088   0.0368 *
AGR:MHUC:Site2   -3.49042 4431.84933  -0.001   0.9994  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) AGR    MHUC   Site2  AGR:MHUC AGR:S2 MHUC:S
AGR          0.000                                            
MHUC        -0.281  0.000                                     
Site2       -0.639  0.000  0.180                              
AGR:MHUC     0.000 -1.000  0.000  0.000                       
AGR:Site2    0.000 -0.706  0.000  0.000  0.706                
MHUC:Site2   0.208  0.000 -0.738 -0.419  0.000    0.000       
AGR:MHUC:S2  0.000  0.706  0.000  0.000 -0.706   -1.000  0.000 code here

BestModel=lmer(SP ~ AG * MH + (1|Site:Tree) + (1|obs), data=datos,  
               family = ""poisson"") 

QAICc(M) 101.4419

Generalized linear mixed model fit by the Laplace approximation 
Formula: SP ~ AG + AG:MH + (1 | Site:Tree) + (1 | obs) 
   Data: datos 
   AIC   BIC logLik deviance
 100.3 114.6 -44.15     88.3
Random effects:
 Groups    Name        Variance Std.Dev.
 obs       (Intercept) 0.76027  0.87194 
 Site:Tree (Intercept) 1.14358  1.06938 
Number of obs: 80, groups: obs, 80; Site:Tree, 20

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.5153     0.4061  -1.269    0.205
AGR          -18.7146  2603.4397  -0.007    0.994
AGA:MHUC      -0.7301     0.5045  -1.447    0.148
AGR:MHUC      17.7221  2603.4397   0.007    0.995
</code></pre>
"
"0.0935560539449976","0.0961138662664425","  9324","<p>I'd love a check if anyone is willing!</p>

<p>I am trying to see if there is a statistical difference in female size between sites. Over the years females were repeatedly sampled within sites. I have sampled females opportunistically. Meaning that females were sampled a different number of times between and within sites.</p>

<p>My formula is:</p>

<pre><code>&gt; lmerfit1&lt;-lmer(size ~ (1|FEMALE), data=Data)
&gt; lmerfit2&lt;-lmer(size ~ SITE+(1|FEMALE), data=Data)
&gt; anova(lmerfit1, lmerfit2)
Data: Data
Models:
lmerfit1: size ~ (1 | FEMALE)
lmerfit2: size ~ SITE + (1 | FEMALE)
         Df    AIC    BIC  logLik Chisq Chi Df Pr(&gt;Chisq)
lmerfit1  3 2167.8 2179.6 -1080.9                        
lmerfit2  4 2169.8 2185.5 -1080.9     0      1          **1**
</code></pre>

<p>A p value of <strong>1</strong> leaves me concerned. The other female traits I ran thru this same formula made sense.</p>

<p>thanks! </p>
"
"0.104598848163826","0.107458569276045","  9488","<h3>Question:</h3>

<ul>
<li>Is it possible to get log liklihood values for my stepwise glms?</li>
</ul>

<h3>Context:</h3>

<p>I am able to get a logliklihood value using lmer with the following model. My study involves unbalanced repeated females, two sites (females don't exchange between sites), 8 predictors, and a response. </p>

<pre><code>    (glmfit1 &lt;- lmer(Response ~ 1 + SITE + (1|SITE:FEMALE) + Variable A + Variable B +
                        Variable C, data = data))
</code></pre>

<p>Removing the repeated females and taking out the Site factor gives me this formula. </p>

<pre><code>(lrfit1 &lt;- glm(Response ~ 1+Variable A + Variable B + Variable C, data = data))
summary (lrfit1) #just gives me p values for my variables and an AIC but NO LOGLIK.
</code></pre>
"
"0.0661541201655622","0.0679627666030585"," 11462","<p>I'm trying to find the best model based on AIC using the stepwise (<code>direction = both</code>) model selection in R using the stepAIC in MASS package.</p>

<p>This is the script i used: </p>

<pre><code>stepAIC (glmer(decision ~ as.factor(Age) + as.factor(Educ) + as.factor(Child), family=binomial, data=RShifting), direction=""both"")
</code></pre>

<p>however I got this error result:</p>

<pre><code>Error in lmerFactorList(formula, fr, 0L, 0L) : 
  No random effects terms specified in formula
</code></pre>

<p>I tried to add <code>(1|town)</code> to the formula since town is the random effect (where the respondents are nested) and ran this script):</p>

<pre><code>stepAIC (glmer(decision ~ as.factor(Age) + as.factor(Educ) + as.factor(Child) + (1|town), family=binomial, data=RShifting), direction=""both"")
</code></pre>

<p>The result is this:</p>

<pre><code>Error in x$terms : $ operator not defined for this S4 class
</code></pre>

<p>I hope you could help me figure out how to solve this problem. Thanks a lot.</p>
"
"0.123763026191503","0.108982880436826"," 16013","<p>I've been looking at mixed effects modelling using the lme4 package in R.  I'm primarily using the <code>lmer</code> command so I'll pose my question through code that uses that syntax.  I suppose a general easy question might be, is it OK to compare any two models constructed in <code>lmer</code> using likelihood ratios based on identical datasets?  I believe the answer to that must be, ""no"", but I could be incorrect.  I've read conflicting information on whether the random effects have to be the same or not, and what component of the random effects is meant by that?  So, I'll present a few examples.  I'll take them from repeated measures data using word stimuli, perhaps something like <a href=""http://www.ualberta.ca/~baayen/publications/baayenCUPstats.pdf"" rel=""nofollow"">Baayen (2008)</a> would be useful in interpreting.</p>

<p>Let's say I have a model where there are two fixed effects predictors, we'll call them A, and B, and some random effects... words and subjects that perceived them.  I might construct a model like the following. </p>

<pre><code>m &lt;- lmer( y ~ A + B + (1|words) + (1|subjects) )
</code></pre>

<p>(note that I've intentionally left out <code>data =</code> and we'll assume I always mean <code>REML = FALSE</code> for clarity's sake)</p>

<p>Now, of the following models, which are OK to compare with a likelihood ratio test to the one above and which are not?</p>

<pre><code>m1 &lt;- lmer( y ~ A + B + (A+B|words) + (1|subjects) )
m2 &lt;- lmer( y ~ A + B + (1|subjects) )              
m3 &lt;- lmer( y ~ A + B + (C|words) + (A+B|subjects) )
m4 &lt;- lmer( y ~ A + B + (1|words) )                 
m5 &lt;- lmer( y ~ A * B + (1|subjects) )   
</code></pre>

<p>I acknowledge that the interpretation of some of these differences may be difficult, or impossible.  But let's put that aside for a second.  I just want to know if there's something fundamental in the changes here that precludes the possibility of comparing.  I also want to know whether, if LR tests are OK, AIC comparisons are as well.</p>
"
"0.133132598713698","0.136772429457052"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.0810219193941953","0.0832370498426794"," 22988","<p>I use lme4 in R to fit the mixed model</p>

<pre><code>lmer(value~status+(1|experiment)))
</code></pre>

<p>where value is continuous, status and experiment are factors, and I get</p>

<pre><code>Linear mixed model fit by REML 
Formula: value ~ status + (1 | experiment) 
  AIC   BIC logLik deviance REMLdev
 29.1 46.98 -9.548    5.911    19.1
Random effects:
 Groups     Name        Variance Std.Dev.
 experiment (Intercept) 0.065526 0.25598 
 Residual               0.053029 0.23028 
Number of obs: 264, groups: experiment, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.78004    0.08448   32.91
statusD      0.20493    0.03389    6.05
statusR      0.88690    0.03583   24.76

Correlation of Fixed Effects:
        (Intr) statsD
statusD -0.204       
statusR -0.193  0.476
</code></pre>

<p>How can I know that the effect of status is significant? R reports only $t$-values and not $p$-values.</p>
"
"0.192870746165604","0.198143811351516"," 24337","<p>It has been a few years since I fit a mixed model, so I have gone on a massive review session on old notes, books (Pinheiro and Bates, Faraway, etc) and going through the posts on SO and CV about mixed models. It has been great, but I am left with a few questions that are likely so basic they are missed in most of these materials.</p>

<p>The data set I am working with has a time column that starts at 1 and goes to 38 by period. I also have 3 variables, one for each, year, quarter and period. In my fixed effect, should I be modeling sales~Time+quarter+year+period or should I ignore the quarter, year and period variables and just use my aggregated time variable. Should I put ordered(time) inthere? On the one hand, I imagine having the additional quarter/period/year variables in there might be redundant, since they are already in the Time variable, but at the same time, it might be good to try and use them to control for seasonality?</p>

<p>I am wondering if my fixed effect should be</p>

<pre><code>Sales~Time+Quarter+year+period+Region+Policy
</code></pre>

<p>or</p>

<pre><code>Sales~Time+Region+Policy
</code></pre>

<p>Also, if anyone has advice on fitting models with more than 1 or two random effects, it would be much appreciated. In all of my notes, its mostly just specific designs with 1 random effect (sometimes intercept and sometimes slope + intercept), but rarely multiple. I currently am trying to look at 5 variables I would like to be random effects (at least look at them). Is it as simple as (1|A) + (1|B) + (1|C) + etc in lmer? Any trouble I am walking myself into?</p>

<p>Thank you as always CV.</p>

<p>Edit: I am interested in a change in policy that has potentially harmed sales. I have a 3 year data set and in the middle of that data set we changed our policy and I am fitting LMM to see first if there is actually a negative effect and second to get a vague idea of how big it is, if its a decrease of .001%, we don't care, but if its a larger number it should be examined further (which naturally will be my next task).</p>

<p>Edit2: For random effects (which I am still in the process of tinkering with) I have the product, who is paying and purchase quantity. What makes the cut here is still a work in progress, currently using LRT and AIC (mostly AIC, because there seems to be reservations about LRT with lmer sometimes). There are 38 months in the set and each month has sales values for each type of payment recieved (there are 5), each product sold (3), the region it is sold in (~30 here). The other variables I have are sales, number of things sold, the team that did the selling (in just because the change in policy relates to it) and a dummy variable for change in policy. The model I am working around right now is looking something like:</p>

<pre><code>m1 = lmer(Sales~ Time+Policy+Team+(Product|Territory)+(salesqty|Territory)+ (payer|Territory), data=data ) 
</code></pre>

<p>Though like I said, this is by no means final, just what I have running in R at this second.</p>
"
"0.192870746165604","0.198143811351516"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.155145163900903","0.159386815778093"," 24844","<p>I am running 3 models on 3 subsets of the same data.  The set up is as follows:</p>

<ol>
<li>Outcome (DV) is binary categorical</li>
<li>Time (IV) is repeated twice (pre and post)</li>
<li>Treatement (IV of interest) is binary categorical</li>
</ol>

<p>I am interested to know if at time 2 treatment has had an effect on outcome.  I used the lme4 package and used the following R code:</p>

<pre><code>tot.null&lt;-lmer(as.factor(outcome)~Time+(1|id), family=binomial(link='logit'),
             data=df.total)
tot.mod&lt;-lmer(as.factor(outcome)~trt*Time+(Time|id), 
             family=binomial(link='logit'), data=df.total)
anova(tot.null,tot.mod)
summary(tot.mod)
</code></pre>

<p><strong>Data head</strong></p>

<pre><code>   id             trt Time outcome
1   1 peer discussion   -1       1
2   2 peer discussion   -1       1
3   3 peer discussion   -1       0
4   4 peer discussion   -1       1
5   5 peer discussion   -1       1
</code></pre>

<p><strong>str of data</strong></p>

<pre><code>&gt; str(df.total)
'data.frame':   872 obs. of  4 variables:
 $ id     : int  1 2 3 4 5 6 7 8 9 10 ...
     $ trt    : Factor w/ 2 levels ""peer discussion"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Time   : num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
     $ outcome: num  1 1 1 1 1 1 1 0 1 0 ...
</code></pre>

<p>The problem is I get an error messoge on the <code>tot.mod</code>:</p>

<pre><code>&gt; tot.mod&lt;-glmer(as.factor(outcome)~trt*Time+(Time|id), family=binomial(link='logit'),
               data=df.total)
Warning message:
In mer_finalize(ans) : false convergence (8)
</code></pre>

<p>I think this is the reason the model is significant but none of the predictors are.  look at the inflated SEs.</p>

<p><strong>Comparison to the null model and the summary of full model</strong></p>

<pre><code>&gt; anova(tot.null,tot.mod)
Data: df.total
Models:
tot.null: as.factor(outcome) ~ Time + (1 | id)
tot.mod: as.factor(outcome) ~ trt * Time + (Time | id)
         Df    AIC    BIC  logLik  Chisq Chi Df            Pr(&gt;Chisq)    
tot.null  3 689.54 703.85 -341.77                                        
tot.mod   7 410.67 444.07 -198.34 286.86      4 &lt; 0.00000000000000022 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; summary(tot.mod)
Generalized linear mixed model fit by the Laplace approximation 
Formula: as.factor(outcome) ~ trt2 * Time + (Time | id) 
   Data: df.total 
   AIC   BIC logLik deviance
 410.7 444.1 -198.3    396.7
Random effects:
 Groups Name        Variance Std.Dev. Corr  
 id     (Intercept)  396.46  19.911         
        Time        1441.98  37.973   0.470 
Number of obs: 872, groups: id, 436

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) 10.09866    3.33921   3.024  0.00249 **
trt21        0.01792    5.10796   0.004  0.99720   
Time        -0.93753    5.79560  -0.162  0.87149   
trt21:Time  -0.84882   10.41073  -0.082  0.93502   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
           (Intr) trt21  Time  
trt21      -0.654              
Time        0.558 -0.365       
trt21:Time -0.311  0.473 -0.557
</code></pre>

<p>What's going on?  Why is the model significant but none of the betas?  In OLS I know this is an indicator of multi-colinearity among predictors.  I don't think that's the reason here.  Please help with understanding this problem as well as the error message (I think they may be connected).  What are some things I should check for?</p>

<p>The other two  models from the same data set (<code>split</code> on a different grouping variable) had no apparent problems.</p>

<p>Thank you in advance.</p>

<p><em>Using R 2.14.2, lme4 v. 0.999375-42 on a win 7 machine</em> </p>
"
"0.148540185556025","0.138728416404466"," 25025","<p>I asked a question but it was a bit long and confusing so I will attempt to keep this shorter and simple (original post <a href=""http://stats.stackexchange.com/questions/24971/mixed-effects-model-equations"">Mixed effects model equations</a>)</p>

<p>I am basically looking to see which factors  (e.g. A, B, C such as habitat type, site, disturbance rate) most affect the response value (y). I am finally understanding the concept of models and how to compare AIC values to see which combination of factors best explain y (have greater effect on), but I am new to R so wonder if my basic coding is correct.</p>

<p>Firstly I have all my data in a spreadsheet saved as a .csv file, so I read this file into R. I also open the package lme4.</p>

<p>Then I was thinking of using the following code (although letters would be the headings fo each set of values) </p>

<pre><code>m1&lt;-lmer(y ~ A + (1|E), REML=FALSE)
m2&lt;-lmer(y ~ B + (1|E), REML=FALSE)
m3&lt;-lmer(y ~ A + B + (1|E), REML=FALSE)
m4&lt;-lmer(y ~ A + C + (1|E), REML=FALSE)
m5&lt;-lmer(y ~ B + C + (1|E), REML=FALSE)
m6&lt;-lmer(y ~ A + B + C + (1|E), REML=FALSE)
m4&lt;-lmer(y ~ A + D + (1|E), REML=FALSE)
m5&lt;-lmer(y ~ B + D + (1|E), REML=FALSE)
m6&lt;-lmer(y ~ A + B + D + (1|E), REML=FALSE)
m7&lt;-lmer(y ~ C + (1|E), REML=FALSE)
m8&lt;-lmer(y ~ D + (1|E), REML=FALSE)
</code></pre>

<p>My basic questions are:</p>

<ol>
<li><p>Let us say that C and D are similar factors, and derived from the same data and I do not think it useful to see if they have additive effects, is it okay not to put them in the same model, i.e. mix and match as see fit, or should all combinations be incorporated?</p></li>
<li><p>I chose ML because not all sets of values are mixed and matched. Is this correct or should I use REML?</p></li>
<li><p>Some sets of values are non-continuous, e.g. brood number or habitat type (either 1 or 2). Should I be letting R know this. Someone mentioned coding each factor but I have no clue how to do this, or is the csv file enough and then perhaps code to let R know about these particular ones?</p></li>
<li><p>Lastly, is there also a way to see p-values to see if the factor has a significant effect rather than just being the best fit to explain y?</p></li>
</ol>

<p>I hope that this is more simple than my first post and easier to answer all at once. I really wanted to run this model today, but don't want to have to redo it all finding that I made a simple mistake.</p>
"
"0.123763026191503","0.127146693842964"," 25169","<p>After some amazing help, especially from @jbowman, I was ready to get into my models but have hit a snag that I don't understand.
I decided to try package MuMIn as it works with lme4 so that I am able to do all the models in once shot rather than running them each one at a time. However apart from loading it and then loading lme4 I have not had a chance to use it as I am getting an error and not sure why. Could someone please look at the code and see what I am doing wrong? My original post is here <a href=""http://stats.stackexchange.com/questions/24971/mixed-effects-model-equations"">Mixed effects model equations</a></p>

<p>It is basically telling me that it cannot find ""Feeding"" but when I type in the name of the df it lists all columns. So I tried to enter it as a <code>factor</code> (not <code>as.factor</code>) as you can see with no effect. I would like to attach my .csv file but I don't see an option to do this. :(</p>

<p>Here is what I am getting</p>

<pre><code>&gt; local({pkg &lt;- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Warning message:
package â€˜MuMInâ€™ was built under R version 2.14.1 
&gt; local({pkg &lt;- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Loading required package: Matrix
Loading required package: lattice

Attaching package: â€˜Matrixâ€™

The following object(s) are masked from â€˜package:baseâ€™:

    det


Attaching package: â€˜lme4â€™

The following object(s) are masked from â€˜package:statsâ€™:

    AIC, BIC

&gt; ABMtest.df&lt;-read.csv(file.choose(), header=T)
&gt; ABMtest.df$Brood&lt;-as.factor(ABMtest.df$Brood)
&gt; ABMtest.df$Site&lt;-as.factor(ABMtest.df$Site)
&gt; ABMtest.df$Age.class&lt;-as.factor(ABMtest.df$Age.class)
&gt; ABMtest.df$MF.vs.OF&lt;-as.factor(ABMtest.df$MF.vs.OF)
&gt; ABMtest.df$tide.h.l&lt;-as.factor(ABMtest.df$tide.h.l)
&gt; fm2test&lt;-lmer(Feeding~MF.vs.OF+Age.class+tide.h.l+Site+HDp+(1|Brood))
Error in eval(expr, envir, enclos) : object 'Feeding' not found
&gt; ABMtest.df$Feeding&lt;-factor(ABMtest.df$Feeding)
&gt; fm2test&lt;-lmer(Feeding~MF.vs.OF+Age.class+tide.h.l+Site+HDp+(1|Brood))
Error in eval(expr, envir, enclos) : object 'Feeding' not found
&gt; 
</code></pre>

<p>Last time after I listed everything I finally got it to run (but not sure how because i cannot replicate this. But last time I then typed the code:</p>

<pre><code>ms2test&lt;-dredge(fm2test, trace=TRUE, rank=""AICc"", REML=FALSE)
</code></pre>

<p>Which is supposed to give me the ranks of all the models using AICc and allows me to list the models etc. but it says something like not being able to find class ""mer"" with fixef. I have no idea.</p>

<p>If someone could see why Feeding is not recognised, or is this because I didn't list first? And then anyone who is familiar with the dredge command help with that then it would greatly appreciated.</p>
"
"0.147925109681887","0.151969366063391"," 25208","<p>I have been asking a number of separate questions as each are unique in themselves but connected to my learning process in running mixed effects models. I apologise if i am becoming a nuisance.</p>

<p>Basically I learned how to run a model correctly using lme4 (an lmer) and as I had many factors I was then able to use the MuMIn package to identify those models that best explained y rather than having to run them all by hand.
I would like to better understand Effect and then 95% CI using the model.avg command.</p>

<p>I have 9 factors (including the random but not y).
After using the dredge command I ran: </p>

<blockquote>
  <p>get.models(dd, subset=delta&lt;4)</p>
</blockquote>

<p>where dd is the data.frame from the dredge command (i.e. all possible combinations of factors in the global model). I ended up with 14 models listed (out of over 200), although only the top two had a delta&lt;2.</p>

<p>I know that papers will often report the importance of these factors, so would it be advisable to find the Estimate and 95% CI for all the factors for all 14 models or should I just look at those factors within the top two models (ie with a delta&lt;2)? I think the two models is what other papers have done.</p>

<p>Could someone tell me how to do this? Do I run all models that are in the top (either 2 or 14) and then use the avg.model command? Could someone let me know what the code is for this?</p>

<p>In addition, once I do this, would it be okay to then list my results below so that someone could help me decipher what they mean? I don't have a deep understanding of the intricacies, but it would be helpful for me to be able to further explain the magnitude of the effects of the most important factors rather than just the weights of the models themselves. I understand AICc and weights, but not the t values and such that come after each model.</p>

<p>I appreciate everyone's patience with me in this.</p>
"
"0.233890134862494","0.230673279039462"," 25333","<p>I have been attempting to set up a <code>lme</code> and have looked at numerous posts including '<code>R</code>'s <code>lmer</code> cheat-sheet' as well as reading a number of papers and other resources including <code>R</code> help, but I am still a little confused on how to write my model (I thought I had it). I have asked a number of questions on here and appreciate the help. I have also found alternative questions and approaches for those issues not resolved. </p>

<p>My main concern right now is whether my model is correct.
I studied broods of precocial chicks and watched each chick every other day for five minutes if possible. As chicks on the same day are completely non-independent the mean was found for each brood for each day. Variables that were recorded were the behaviours during that time and the habitats used. </p>

<p>There were seven broods. Three at one site and four at the other site. Only one site had a brood that consistently used mudflats rather than oceanfront habitats. As none of the data within a brood is truly independent, along with the very small number of broods, it became impossible to use conventional statistics to test the hypotheses and so it was suggested that mixed-effects models would be the best option as it would not only allow for all data to be used with a random effect of Brood ID to negate the pseudo-replication but also let me look at partial use of mudflats in one of the other broods that only used it periodically.</p>

<p>So, for this part of the analysis I would like to see which factors affect the amount of time feeding. I set up a global model with ten fixed variables plus <code>(1|Brood)</code>. <code>Site</code>, <code>tide.h.l</code>, <code>tide.inc.out</code>, <code>MF.vs.OF</code>, Human Disturbance Rate (<code>HDr</code>), Human Disturbance proportion of time(<code>HDp</code>), non-Human Disturbance (two variables as for Human Disturbance) and <code>Age</code> and <code>mean.foraging.rate</code>. 
As so:</p>

<pre><code>gm1&lt;-lmer(Feeding~Site+tide.level+MF.vs.OF+HDr+HDp+NHDr+NHDp+Age+mean.for.rate+(1|Brood), data=AllBrood, REML=TRUE)
</code></pre>

<p>I wished to put all the factors together to explore which ones really did influence the time spent feeding and used 'dredge' command to run all possible combinations and then averaged the models with an AICc $\Delta&lt;2$. I was expecting that the proportion of time being disturbed (<code>HDp</code> and <code>NHDp</code>) would be the most relevant as by default the greater time in other behaviours the less time for feeding. However, <code>MF.vs.OF</code> had a larger effect than <code>HDp</code> and <code>NHDp</code> but this may be because MF observations did not experience <code>HDp</code> at all so this may push the effect of this habitat. Surprisingly non-human disturbance rates rather than time had a greater effect (but these are quite even among habitats.</p>

<p>I was wondering whether there would be a better way to formulate the model to allow for this effect, or could I just keep it as is and just infer that it may be partly affected by the amount of disturbance within these habitats but as it has a greater effect that other factors are at play which would then lead me onto the next model which is going to explore observations that do not include disturbance which would allow me to tease the natural factors affecting feeding behaviour? I was going to run this second model with site still as a fixed effect and then run it with <code>(1|Site)</code> to remove site effect (if one is found).</p>

<p>I would prefer to keep it simple as I really want to use a <code>lme</code>, but don't have the understanding for more complex interactions. I posted the results of the <code>model.avg</code> for the top seven models in an earlier question about how to interpret the results. The link can be found here:
<a href=""http://stats.stackexchange.com/questions/25322/best-fit-models-with-delta2"">Relative variable importance values vs. magnitude of effect</a>. It would allow you to look at the results and what I am talking about, but at the same time, if someone would be able to look at that question too I would appreciate it.</p>

<p>P.S. I know that some may wonder why I am running models if I don't know the ins and outs, but I really do understand what they represent, I just don't understand the intricacies between variables and if Estimates or relative variable importance is more important as the study that is similar to mine only used the former and I expected them to be correlated</p>
"
"0.162525396577478","0.166968823211932"," 25371","<p>I am using the <code>glmer()</code> function from the lme4 package to run a GLMM using the poisson distribution.  In all the examples that I see, the random effects part of the output has a residual part that has been estimated from the data (surrounded by 2 asterisks on either side in the example below).  This information can then be used in interpreting the amount of variation explained by the random effect.  Here is an example:</p>

<pre><code>&gt; summary(M1)
Linear mixed model fit by REML 
Formula: Richness ~ NAP * fExp + (1 | fBeach) 
Data: RIKZ 
AIC   BIC    logLik   deviance REMLdev
236.5 247.3 -112.2    230.3    224.5
Random effects:
Groups   Name          Variance Std.Dev.
fBeach   (Intercept)   3.3072   1.8186  
**Residual             8.6605   2.9429**
Number of obs: 45, groups: fBeach, 9

Fixed effects:
              Estimate Std. Error t value
(Intercept)   8.8611     1.0208   8.681
NAP          -3.4637     0.6279  -5.517
fExp11       -5.2556     1.5451  -3.401
NAP:fExp11    2.0005     0.9461   2.114

Correlation of Fixed Effects:
           (Intr) NAP    fExp11
NAP        -0.181              
fExp11     -0.661  0.120       
NAP:fExp11  0.120 -0.664 -0.221
</code></pre>

<p>However, when I use my own data, I get output that does not include this information, and I am not sure why.  I want to know how much variation is explained by my random effects, but can't figure out how to access the information necessary to answer the question.  Any clues?  Is this a data/statistics issue or is this a knowing how to access the information issue?  I apologize if I'm asking in the wrong place.  The output I get looks similar to the following output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: y ~ z.score(x1) + z.score(x2) + z.score(x3) + z.score(x4) + z.score(x5) +      z.score(x6) + (1 | RE) 
Data: p 
AIC   BIC logLik deviance
419.5 454.7 -201.8    403.5
Random effects:
Groups Name        Variance Std.Dev.
RE     (Intercept) 0.021605 0.14699 
Number of obs: 600, groups: RE, 40

Fixed effects:
                  Estimate   Std. Error z value Pr(&gt;|z|)    
(Intercept)       1.70591    0.02911    58.60   &lt; 2e-16 ***
z.score(x1)       0.19087    0.03595    5.31    1.10e-07 ***
z.score(x2)      -0.14302    0.04083   -3.50    0.000460 ***
z.score(x3)      -0.16562    0.04020   -4.12    3.79e-05 ***
z.score(x4)       0.13229    0.03425    3.86    0.000112 ***
z.score(x5)      -0.10588    0.03985   -2.66    0.007885 ** 
z.score(x6)       0.17600    0.05798    3.04    0.002401 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) z.(x1) z.(x2 z.s(x3) z.(x4 z.(x5
z.scr(x1)  -0.051                                   
z.s(x2)     0.038  0.259                            
z.scr(x3)   0.045  0.156  0.113                     
z.(x4      -0.040  0.144 -0.052  0.044              
z.(x5       0.026 -0.368 -0.339 -0.072 -0.073       
z.scor(x6) -0.031 -0.020  0.002 -0.143 -0.004  0.004
</code></pre>

<p>Here is some sample data, to be fit with <code>glmer(y ~ x1 + (1|RE), data=d, family=poisson)</code>.</p>

<pre><code>d &lt;- data.frame(
  y  =  c(3, 5, 2, 6, 3, 7, 2, 3, 0, 4, 0,10, 1, 4, 0, 4, 2, 3, 0, 6, 
          3, 4, 2, 3, 2, 3, 3, 4, 0, 5, 6, 5, 4, 4, 0, 3, 1, 6, 0, 3, 2, 
          2, 1, 6, 2, 7, 0, 2, 0, 4, 0, 6, 4, 5, 1, 5, 1, 4, 1, 2, 3, 6, 
          6, 7, 0, 5, 0, 9, 1, 4, 5, 6, 1, 7, 1, 4, 1, 4, 0, 4, 1, 6, 1, 
          4, 0, 7, 1, 4, 0, 6, 0, 7, 2, 6, 0, 6, 1, 5, 0, 4, 1, 7, 2, 4, 
          1, 5, 1, 7, 2, 5, 0, 4, 3, 5, 1, 4, 0, 3, 0, 6, 0, 8, 3, 9, 0, 
          2, 3, 8, 0, 1, 0, 3, 0, 5, 0, 4, 4, 5, 0, 5, 1, 5, 3, 5, 1, 4, 
          3, 4, 4, 4, 4, 4, 4, 7, 1, 8, 1, 4, 0, 2, 2, 5, 1, 4, 1, 5, 1, 
          4, 2, 4, 2, 4, 0, 6, 1, 6, 0, 6, 1, 2, 1, 3, 1, 8, 1, 6, 1, 6, 
          0, 6, 1, 6, 2, 6, 2, 4, 0, 1, 1, 1, 1, 6, 5, 5, 1, 5, 2, 4, 2, 
          6, 1, 7, 1, 8, 2, 8, 1, 8, 2, 4, 1, 7, 3, 6, 4, 7, 3, 7, 1, 6, 
          3, 5, 1,10, 1, 7, 2, 5, 1, 5, 0, 6, 1, 8, 4, 7, 1, 6, 1, 9, 
          0, 9, 1, 3, 2, 5, 2, 9, 3, 5, 0, 2, 2, 3, 0, 5, 0, 5, 0, 4, 3, 
          6, 1,10, 2, 8, 0, 6, 0, 4, 2, 6, 2, 4, 2, 6, 1, 4, 0, 5, 2, 
          6, 1, 5, 2, 5, 1, 5, 1, 5),
  x1 = rep(c(0.1008, 0.0511, 0.1792, 1.0345), c(80, 80, 80, 60)),
  RE = rep(c(37, 88, 139, 190, 241, 292, 343, 394, 91, 142, 193, 244, 295, 
             346, 397, 40, 94, 145, 43, 196, 247, 298, 349, 400, 301, 352, 
             403, 250, 148, 199, 46, 97, 355, 406, 253, 304, 49, 100, 151, 
             202, 37, 88, 139, 190, 241, 292, 343, 394, 91, 142, 193, 244, 
             295, 346, 397, 40, 43, 94, 145, 247, 298, 349, 196, 400, 199, 
             250, 301, 352, 406, 46, 97, 148, 403, 49, 100, 151, 202, 253, 
             304, 355, 37, 88, 139, 190, 241, 292, 343, 394, 193, 244, 346, 
             397, 295, 40, 91, 142, 43, 94, 145, 196, 46, 97, 148, 151, 247, 
             400, 298, 349, 352, 199, 250, 301, 403, 253, 304, 355, 202, 406, 
             49, 100, 37, 88, 139, 190, 241, 292, 343, 394, 346, 397, 193, 
             244, 295, 40, 91, 142, 43, 94, 145, 196, 247, 298, 349, 400, 
             97, 148, 46, 199, 250, 301), each=2)
)
</code></pre>
"
"0.133132598713698","0.136772429457052"," 26401","<p>Here are some simulated data:</p>

<pre><code>    library(mvtnorm)
    I &lt;- 3 # positions (fixed factor)
    J &lt;- 4 # tubes (random factor)
    K &lt;- 4 # repeats 
    n &lt;- I*J*K
    set.seed(123)
    tube &lt;- rep(1:J, each=I)
    position &lt;- rep(LETTERS[1:I], times=J)
    Mu_i &lt;- 3*(1:I)
    Mu_ij &lt;- c(t(rmvnorm(J, mean=Mu_i)) )  
    tube &lt;- rep(tube, each=K)
    position &lt;- rep(position, each=K)
    Mu_ij &lt;- rep(Mu_ij, each=K)
    dat &lt;- data.frame(tube, position, Mu_ij)
    sigmaw &lt;- 2
    dat$y &lt;- rnorm(n, dat$Mu_ij, sigmaw)
    dat$tube &lt;- factor(dat$tube)

&gt; str(dat)
'data.frame':   48 obs. of  4 variables:
 $ tube    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ...
     $ position: Factor w/ 3 levels ""A"",""B"",""C"": 1 1 1 1 2 2 2 2 3 3 ...
 $ Mu_ij   : num  2.44 2.44 2.44 2.44 6.13 ...
     $ y       : num  3.24 2.66 1.33 6.01 7.12 ...
&gt; head(dat)
  tube position    Mu_ij        y
1    1        A 2.439524 3.241067
2    1        A 2.439524 2.660890
3    1        A 2.439524 1.327842
4    1        A 2.439524 6.013351
5    1        B 6.129288 7.124989
6    1        B 6.129288 2.196053
</code></pre>

<p>I fit a mixed model with R, it works well:</p>

<pre><code>&gt; library(lme4)
&gt; lmer(y ~ position + (0+position|tube), data=dat)
Linear mixed model fit by REML 
Formula: y ~ position + (0 + position | tube) 
   Data: dat 
   AIC   BIC logLik deviance REMLdev
 212.6 231.3  -96.3    194.8   192.6
Random effects:
 Groups   Name      Variance Std.Dev. Corr          
 tube     positionA 0.30123  0.54885                
          positionB 0.68317  0.82654  -0.695        
          positionC 1.66666  1.29099  -0.408  0.940 
 Residual           3.14003  1.77201                
Number of obs: 48, groups: tube, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.3533     0.5211   6.435
positionB     3.1098     0.8923   3.485
positionC     5.6138     1.0144   5.534

Correlation of Fixed Effects:
          (Intr) postnB
positionB -0.753       
positionC -0.651  0.744
</code></pre>

<p>But the same model does not work well with SAS:</p>

<pre><code>PROC MIXED DATA=dat ;
CLASS POSITION TUBE ;
MODEL y = POSITION ;
RANDOM POSITION / subject=TUBE type=UN G GCORR ;
RUN; QUIT;
</code></pre>

<p>gives</p>

<pre><code> Estimated G matrix is not positive definite.
                         Estimated G Matrix

               Row    Effect      position    tube        Col1        Col2        Col3

                 1    position    A           1        0.08895     -0.5823     -0.1545
                 2    position    B           1        -0.5823      0.1455      1.2431
                 3    position    C           1        -0.1545      1.2431      1.4835
</code></pre>

<p>Is it possible to remedy this failure ?</p>
"
"0.104598848163826","0.0644751415656268"," 27020","<p>I'm analysing reaction time data using mixed-effect modelling in R.  Data comes from 2 types of participant groups: native speakers and non-native speakers.  For the non-natives, I have proficiency scores (estimating their mastery of English).  The proficiency of native speakers is irrelevant, and coded as NA.  Does this mean that lmer will consider proficiency as a factor only for non-native speakers?</p>

<pre><code>'data.frame':   8373 obs. of  17 variables:
$Subject       : Factor w/ 21 levels 
$L1            : Factor w/ 3 levels ""English"",""German"",..: 
$Proficiency   : Factor w/ 12 levels:""0"",""0.6"",""0.61"",..: 8 8 8 8 8 8 8 8 8 8 ...  
$Target        : Factor w/ 243 levels 
$Relation      : Factor w/ 4 levels 
$Word.Order    : Factor w/ 2 levels ""HeadMod*"",""ModHead""
$Priming       : Factor w/ 2 levels ""PrHead"",""PrMod""
$Trial         : Factor w/ 481 levels 
$Target.RTinv  : num
</code></pre>

<p>I'm concerned that when I add Proficiency to my model, the AIC and BIC become negative. Is this something to be concerned about?</p>

<pre><code>Models:
dat.lmer5: -1000 * Target.RTinv ~ (1 | Subject) + (1 | Target) + L1 + Word.Order + Priming
dat.lmer8: -1000 * Target.RTinv ~ (1 | Subject) + (1 | Target) + L1 + Word.Order + Priming + Proficiency
          Df     AIC     BIC  logLik  Chisq Chi Df Pr(&gt;Chisq)
dat.lmer5  8 1859.68 1915.92 -921.84
dat.lmer8 17 -438.62 -329.59  236.31 2316.3      9  &lt; 2.2e-16 ***
</code></pre>
"
"0.115769710289734","0.135925533206117"," 29500","<p>Following grafts are taken from <a href=""http://personal.bgsu.edu/~jshang/AICb_assumption.pdf"">this article </a>. I'm newbie to bootstrap and trying to implement the parametric, semiparametric and nonparametric bootstrapping bootstrapping for linear mixed model with <code>R boot</code> package.</p>

<p><img src=""http://i.stack.imgur.com/b24bi.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/2fTO0.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/zANep.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/VfGOQ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/Y2Ng0.png"" alt=""enter image description here""></p>

<p><strong>R Code</strong></p>

<p>Here is my <code>R</code> code:</p>

<pre><code>library(SASmixed)
library(lme4)
library(boot)

fm1Cult &lt;- lmer(drywt ~ Inoc + Cult + (1|Block) + (1|Cult), data=Cultivation)
fixef(fm1Cult)


boot.fn &lt;- function(data, indices){
 data &lt;- data[indices, ]
 mod &lt;- lmer(drywt ~ Inoc + Cult + (1|Block) + (1|Cult), data=data)
 fixef(mod)
 }

set.seed(12345)
Out &lt;- boot(data=Cultivation, statistic=boot.fn, R=99)
Out
</code></pre>

<p><strong>Questions</strong></p>

<ol>
<li>How to do parametric, semiparametric and nonparametric bootstrapping for mixed models with <code>boot</code> package?</li>
<li>I guess I'm doing nonparametric bootstrapping for mixed model in my code.</li>
</ol>

<p>I found <a href=""http://www.r-project.org/conferences/useR-2009/slides/SanchezEspigares+Ocana.pdf"">these slides</a> but could not get the R package <code>merBoot</code>. Any idea where I can get this package. Any help will be highly appreciated. Thanks in advance for your help and time.</p>
"
"0.140334080917496","0.112132843977516"," 30797","<p>I would like to get the posterior simulations of the variance components of a lmer() model with the mcmcsamp() function. How to do ?</p>

<p>For instance, below is the result of a lmer() fitting :</p>

<pre><code>&gt; fit
Linear mixed model fit by REML
Formula: y ~ 1 + (1 | Part) + (1 | Operator) + (1 | Part:Operator)
   Data: dat
   AIC   BIC logLik deviance REMLdev
 97.55 103.6 -43.78    89.18   87.55
Random effects:
 Groups        Name        Variance Std.Dev.
 Part:Operator (Intercept) 2.25724  1.50241
 Part          (Intercept) 3.30398  1.81769
 Operator      (Intercept) 0.00000  0.00000
 Residual                  0.42305  0.65043
Number of obs: 25, groups: Part:Operator, 15; Part, 5; Operator, 3
</code></pre>

<p>Now I run mcmcsamp() :</p>

<pre><code>&gt; mm &lt;- mcmcsamp(fit, n=15000) 
</code></pre>

<p>I expected that the simulations of the residual variance are stored in the ""sigma"" node but this does not seem to fit the results of lmer() :</p>

<pre><code>&gt; sigmasims &lt;- mm@sigma[1,-(1:5000)]  # discard first 5000 simulations (burn-in)
&gt; summary(sigmasims)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.8647  1.4960  1.7040  1.7460  1.9480  3.7920 
</code></pre>

<p>Similarly I expected that the simulations of the other variance components are stored in the ""ST"" node but I get a similar observation.</p>
"
"0.224339536353082","0.230472954834034"," 31118","<p>I performed an experiment where I raised different families coming from two different source populations, where each family was split up into a different treatments. After the experiment I measured several traits on each individual. 
To test for an effect of either treatment or source as well as their interaction, I used a linear mixed effect model with family as random factor, i.e.</p>

<pre><code>lme(fixed=Trait~Treatment*Source,random=~1|Family,method=""ML"")
</code></pre>

<p>so far so good,
Now I have to calculate the relative variance components, i.e. the percentage of variation that is explained by either treatment or source as well as the interaction.</p>

<p>Without a random effect, I could easily use the sums of squares (SS) to calculate the variance explained by each factor. But for a mixed model (with ML estimation), there are no SS, hence I thought I could use Treatment and Source as random effects too to estimate the variance, i.e.</p>

<pre><code>lme(fixed=Trait~1,random=~(Treatment*Source)|Family, method=""REML"")
</code></pre>

<p>However, in some cases, lme does not converge, hence I used lmer from the lme4 package:</p>

<pre><code>lmer(Trait~1+(Treatment*Source|Family),data=DATA)
</code></pre>

<p>Where I extract the variances from the model using the summary function:</p>

<pre><code>model&lt;-lmer(Trait~1+(Treatment*Source|Family),data=regrexpdat)
results&lt;-model@REmat
variances&lt;-results[,3]
</code></pre>

<p>I get the same values as with the VarCorr function. I use then these values to calculate the actual percentage of variation taking the sum as the total variation.</p>

<p>Where I am struggling is with the interpretation of the results from the initial lme model (with treatment and source as fixed effects) and the random model to estimate the variance components (with treatment and source as random effect). I find in most cases that the percentage of variance explained by each factor does not correspond to the significance of the fixed effect.</p>

<p>For example for the trait HD,
The initial lme suggests a tendency for the interaction as well as a significance for Treatment. Using a backward procedure, I find that Treatment has a close to significant tendency. However, estimating variance components, I find that Source has the highest variance, making up to 26.7% of the total variance.</p>

<p>The lme:</p>

<pre><code>anova(lme(fixed=HD~as.factor(Treatment)*as.factor(Source),random=~1|as.factor(Family),method=""ML"",data=test),type=""m"")
                                      numDF denDF  F-value p-value
(Intercept)                                1   426 0.044523  0.8330
as.factor(Treatment)                       1   426 5.935189  0.0153
as.factor(Source)                          1    11 0.042662  0.8401
as.factor(Treatment):as.factor(Source)     1   426 3.754112  0.0533
</code></pre>

<p>And the lmer:</p>

<pre><code>summary(lmer(HD~1+(as.factor(Treatment)*as.factor(Source)|Family),data=regrexpdat))
Linear mixed model fit by REML 
Formula: HD ~ 1 + (as.factor(Treatment) * as.factor(Source) | Family) 
   Data: regrexpdat 
    AIC    BIC logLik deviance REMLdev
 -103.5 -54.43  63.75   -132.5  -127.5
Random effects:
 Groups   Name                                      Variance  Std.Dev. Corr                 
 Family   (Intercept)                               0.0113276 0.106431                      
          as.factor(Treatment)                      0.0063710 0.079819  0.405               
          as.factor(Source)                         0.0235294 0.153393 -0.134 -0.157        
          as.factor(Treatment)L:as.factor(Source)   0.0076353 0.087380 -0.578 -0.589 -0.585 
 Residual                                           0.0394610 0.198648                      
Number of obs: 441, groups: Family, 13

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.02740    0.03237  -0.846
</code></pre>

<p>Hence my question is, is it correct what I am doing? Or should I use another way to estimate the amount of variance explained by each factor (i.e. Treatment, Source and their interaction). For example, would the effect sizes be a more appropriate way to go?</p>

<p>Thanks!</p>

<p>Kay Lucek</p>
"
"0.210067544970746","0.225620349160598"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.126936952282557","0.130407394727531"," 32994","<p>I've been using the <code>MCMCglmm</code> package recently. I am confused by what is referred to in the documentation as R-structure and G-structure. These seem to relate to the random effects - in particular specifying the parameters for the prior distribution on them, but the discussion in the documentation seems to assume that the reader knows what these terms are. For example:</p>

<blockquote>
  <p>optional list of prior specifications having 3 possible elements: R (R-structure) G (G-structure) and B (fixed effects)............ The priors for the variance structures (R and G) are lists with the expected (co)variances (V) and degree of belief parameter (nu) for the inverse-Wishart</p>
</blockquote>

<p>...taken from from <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=MCMCglmm%3aMCMCglmm"">here</a>.</p>

<p><strong>EDIT: Please note that I have re-written the rest of the question following the comments from Stephane.</strong></p>

<p>Can anyone shed light on what R-structure and G-structure are, in the context of a simple variance components model where the linear predictor is 
$$\beta_0 + e_{0ij} + u_{0j} $$
with $e_{0ij} \sim N(0,\sigma_{0e}^2)$ and $u_{0j} \sim N(0,\sigma_{0u}^2)$</p>

<p>I made the following example with some data that comes with <code>MCMCglmm</code></p>

<pre><code>&gt; require(MCMCglmm)
&gt; require(lme4)
&gt; data(PlodiaRB)
&gt; prior1 = list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 0.002)))
&gt; m1 &lt;- MCMCglmm(Pupated ~1, random = ~FSfamily, family = ""categorical"", 
+ data = PlodiaRB, prior = prior1, verbose = FALSE)
&gt; summary(m1)


 G-structure:  ~FSfamily

         post.mean l-95% CI u-95% CI eff.samp
FSfamily    0.8529   0.2951    1.455      160

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units         1        1        1        0

 Location effects: Pupated ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)   -1.1630  -1.4558  -0.8119    463.1 &lt;0.001 ***
---

&gt; prior2 = list(R = list(V = 1, nu = 0), G = list(G1 = list(V = 1, nu = 0.002)))
&gt; m2 &lt;- MCMCglmm(Pupated ~1, random = ~FSfamily, family = ""categorical"", 
+ data = PlodiaRB, prior = prior2, verbose = FALSE)
&gt; summary(m2)


 G-structure:  ~FSfamily

         post.mean l-95% CI u-95% CI eff.samp
FSfamily    0.8325   0.3101    1.438    79.25

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units    0.7212  0.04808    2.427    3.125

 Location effects: Pupated ~ 1 

            post.mean l-95% CI u-95% CI eff.samp  pMCMC    
(Intercept)   -1.1042  -1.5191  -0.7078    20.99 &lt;0.001 ***
---

&gt; m2 &lt;- glmer(Pupated ~ 1+ (1|FSfamily), family=""binomial"",data=PlodiaRB)
&gt; summary(m2)
Generalized linear mixed model fit by the Laplace approximation 
Formula: Pupated ~ 1 + (1 | FSfamily) 
   Data: PlodiaRB 
  AIC  BIC logLik deviance
 1020 1029   -508     1016
Random effects:
 Groups   Name        Variance Std.Dev.
 FSfamily (Intercept) 0.56023  0.74849 
Number of obs: 874, groups: FSfamily, 49

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9861     0.1344  -7.336  2.2e-13 ***
</code></pre>

<p>So based on the comments from Stephane I think the G structure is for $\sigma_{0u}^2$. But the comments also say that the R structure is for $\sigma_{0e}^2$ yet this does not seem to appear in the <code>lme4</code> output.</p>

<p>Note that the results from <code>lme4/glmer()</code> are consistent with both examples from MCMC <code>MCMCglmm</code>.</p>

<p>So, is the R structure for $\sigma_{0e}^2$ and why doesn't this appear in the output for <code>lme4/glmer()</code> ?</p>
"
"0.123763026191503","0.108982880436826"," 34969","<p>Sorry if I'm missing something very obvious here but I am new to mixed effect modelling. </p>

<p>I am trying to model a binomial presence/absence response as a function of percentages of habitat within the surrounding area. My fixed effect is the percentage of the habitat and my random effect is the site (I mapped 3 different farm sites). </p>

<pre><code>glmmsetaside &lt;- glmer(treat~setas+(1|farm),
       family=binomial,data=territory)
</code></pre>

<p>When <code>verbose=TRUE</code>:</p>

<pre><code>0:     101.32427: 0.333333 -0.0485387 0.138083 
1:     99.797113: 0.000000 -0.0531503 0.148455  
2:     99.797093: 0.000000 -0.0520462 0.148285  
3:     99.797079: 0.000000 -0.0522062 0.147179  
4:     99.797051: 7.27111e-007 -0.0508770 0.145384  
5:     99.797012: 1.45988e-006 -0.0495767 0.141109  
6:     99.797006: 0.000000 -0.0481233 0.136883  
7:     99.797005: 0.000000 -0.0485380 0.138081  
8:     99.797005: 0.000000 -0.0485387 0.138083  
</code></pre>

<p>My output is this:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: treat ~ setasidetrans + (1 | farm) 

AIC   BIC logLik deviance
105.8 112.6  -49.9     99.8
Random effects:
 Groups Name        Variance Std.Dev.
farm   (Intercept)  0        0  
Number of obs: 72, groups: farm, 3

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.04854    0.44848  -0.108    0.914
setasidetrans  0.13800    1.08539   0.127    0.899

Correlation of Fixed Effects:
            (Intr)
setasidtrns -0.851
</code></pre>

<p>I basically do not understand why my random effect is 0? Is it because the random effect only has 3 levels? I don't see why this would be the case. I have tried it with lots of different models and it always comes out as 0.</p>

<p>It cant be because the random effect doesn't explain any of the variation because I know the habitats are different in the different farms.</p>

<p>Here is an example set of data using <code>dput</code>:</p>

<pre><code>list(territory = c(1, 7, 8, 9, 10, 11, 12, 13, 14, 2, 3, 4, 5, 
6, 15, 21, 22, 23, 24, 25, 26, 27, 28, 16, 17, 18, 19, 20, 29, 
33, 34, 35, 36, 37, 38, 39, 40, 30, 31, 32, 41, 45, 46, 47, 48, 
49, 50, 51, 52, 42, 43, 44, 53, 55, 56, 57, 58, 59, 60, 61, 62, 
54, 63, 65, 66, 67, 68, 69, 70, 71, 72, 64), treat = c(1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0), farm = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3), 
built = c(5.202332763, 1.445026852, 2.613422283, 2.261705833, 
2.168842186, 1.267473928, 0, 0, 0, 9.362387965, 17.55433115, 
4.58020626, 4.739300829, 8.638442377, 0, 1.220760647, 7.979990338, 
13.30789514, 0, 8.685544976, 3.71617163, 0, 0, 6.802926951, 
8.925512803, 8.834006678, 4.687723044, 9.878232478, 8.097800267, 
0, 0, 0, 0, 5.639651095, 9.381654651, 8.801754791, 5.692392532, 
3.865304919, 4.493438554, 4.826277798, 3.650995554, 8.20818417, 
0, 8.169597157, 8.62030666, 8.159474015, 8.608979238, 0, 
8.588288678, 7.185700856, 0, 0, 3.089524893, 3.840381223, 
31.98103158, 5.735501995, 5.297691011, 5.17141191, 6.007539933, 
2.703345394, 4.298077606, 1.469986793, 0, 4.258511595, 0, 
21.07029581, 6.737664009, 14.36176373, 3.056631919, 0, 32.49289428, 
0)
</code></pre>

<p>It goes on with around 10 more columns for different types of habitat (like <code>built</code>, <code>setaside</code> is one of them) with percentages in it.</p>
"
"0.162043838788391","0.166474099685359"," 37944","<p>I am currently using the R package <a href=""http://cran.r-project.org/web/packages/lme4/lme4.pdf"">lme4</a>.</p>

<p>I am using a linear mixed effects models with random effects:</p>

<pre><code>library(lme4)
mod1 &lt;- lmer(r1 ~ (1 | site), data = sample_set) #Only random effects
mod2 &lt;- lmer(r1 ~ p1 + (1 | site), data = sample_set) #One fixed effect + 
            # random effects
mod3 &lt;- lmer(r1 ~ p1 + p2 + (1 | site), data = sample_set) #Two fixed effects + 
            # random effects
</code></pre>

<p>To compare models, I am using the <code>anova</code> function and looking at differences in AIC relative to the lowest AIC model:</p>

<pre><code>anova(mod1, mod2, mod3)
</code></pre>

<p>The above is fine for comparing models. </p>

<p>However, I also need some simple way to interpret goodness of fit measures for each model. Does anyone have experience with such measures? I have done some research, and there are journal papers on R squared for the fixed effects of mixed effects models:</p>

<ul>
<li>Cheng, J., Edwards, L. J., Maldonado-Molina, M. M., Komro, K. A., &amp; Muller, K. E. (2010). Real longitudinal data analysis for real people: Building a good enough mixed model. Statistics in Medicine, 29(4), 504-520. doi: 10.1002/sim.3775  </li>
<li>Edwards, L. J., Muller, K. E., Wolfinger, R. D., Qaqish, B. F., &amp; Schabenberger, O. (2008). An R2 statistic for fixed effects in the linear mixed model. Statistics in Medicine, 27(29), 6137-6157. doi: 10.1002/sim.3429  </li>
</ul>

<p>It seems however, that there is some criticism surrounding the use of measures such as those proposed in the above papers.</p>

<p>Could someone please suggest a few easy to interpret, goodness of fit measures that could apply to my models?  </p>
"
"0.133132598713698","0.136772429457052"," 38177","<p>I have a GLMM with Poisson distribution and random spatial block. My experimental design is 2x2 factorial, with 4 blocks, resulting in 16 total data points. Here is the specification of the model in R using the lme4 package.</p>

<pre><code>lmer(rich ~ morph*caged + (1|block), family=poisson, data=bexData)
</code></pre>

<p>When I call summary on this object, I am returned</p>

<pre><code>   AIC   BIC logLik deviance
 18.58 22.44 -4.288    8.576
Random effects:
 Groups Name        Variance Std.Dev.
 block  (Intercept)  0        0      
Number of obs: 16, groups: block, 4
</code></pre>

<p>I have left out the fixed effect parameter tests and correlations for brevity.</p>

<p>Here are my primary questions:</p>

<ol>
<li><p>Can you use this output to calculate overdispersion?  </p>

<ul>
<li>I have read that overdispersion can be calculated as the residual deviance divided by the residual degrees of freedom. Is that 8.576 / (16 - 4)? (Zuur et al., Mixed Effects Models)</li>
</ul></li>
<li>If this calculation is correct, the estimator phi = 0.715. This indicates that there is not overdispersion in my data. 
<ul>
<li>Does this indicate that there is underdispersion? </li>
<li>Is this a problem? </li>
<li>Can anybody offer advice as to thresholds for over/underdispersion at which corrections to the models should be made? Zuur has said in one book that 5 is a common cutoff. Do people agree with that?</li>
<li>How can such corrections be made?</li>
</ul></li>
<li>I've also noticed here that the variance for the random effect is 0. 
<ul>
<li>Does this mean that there are precisely <em>no</em> error correlations between data points within my blocking factor?</li>
<li>If this is so, why would a generalised linear model of the form shown at bottom have an AIC substantially higher, around 55?</li>
<li>is AIC a reasonable method for choosing GLMM over GLM (as suggested by Zuur)?</li>
</ul></li>
</ol>

<p>.</p>

<pre><code>glm(rich ~ morph*caged, data=bexData, family=poisson)
</code></pre>
"
"0.0810219193941953","0.0832370498426794"," 38188","<p>I'm a new user of mixed models and through the material I've been reading there are always probability values (p>t) or (p>z) that estimate the importance of a level of a factor in the model. However, when using the <code>lmer()</code> function in R, which supposedly gives you those probabilities, I simply don't find them. Here is the output: </p>

<pre><code>Linear mixed model fit by REML 
Formula: Temp ~ depth + (1 | locality) 
   Data: qminmatrix 
   AIC   BIC logLik deviance REMLdev
 561.3 581.3 -273.7    551.5   547.3
Random effects:
 Groups   Name        Variance Std.Dev.
 locality (Intercept) 4.7998   2.1909  
 Residual             4.0433   2.0108  
Number of obs: 128, groups: locality, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.0103     1.1500  19.140
depth1        1.9564     0.6832   2.864
depth10       2.6624     0.5756   4.625
depth5        3.0209     0.4932   6.125
depthWS      -2.2585     0.5444  -4.149

Correlation of Fixed Effects:
        (Intr) depth1 dpth10 depth5
depth1  -0.157                     
depth10 -0.175  0.189              
depth5  -0.213  0.313  0.458       
depthWS -0.191  0.334  0.373  0.441
</code></pre>
"
"0.140334080917496","0.144170799399664"," 38524","<p>I got completely different results from lmer() and lme()! Just look at the coefficients' std.errors. Completely different in both cases. Why is that and which model is correct?</p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai)
&gt; mix1d = lme(logInd ~ 0 + crit_i + Year:crit_i, random = ~ 1 + Year|Taxon, data = datai)
&gt; 
&gt; summary(mix1d)
Linear mixed-effects model fit by REML
 Data: datai 
       AIC      BIC    logLik
  4727.606 4799.598 -2351.803

Random effects:
 Formula: ~1 + Year | Taxon
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev       Corr  
(Intercept) 9.829727e-08 (Intr)
Year        3.248182e-04 0.619 
Residual    4.933979e-01       

Fixed effects: logInd ~ 0 + crit_i + Year:crit_i 
                 Value Std.Error   DF   t-value p-value
crit_iA      29.053940  4.660176   99  6.234515  0.0000
crit_iF       0.184840  3.188341   99  0.057974  0.9539
crit_iU      12.340580  5.464541   99  2.258301  0.0261
crit_iW       5.324854  5.152019   99  1.033547  0.3039
crit_iA:Year -0.012272  0.002336 2881 -5.253846  0.0000
crit_iF:Year  0.002237  0.001598 2881  1.399542  0.1618
crit_iU:Year -0.003870  0.002739 2881 -1.412988  0.1578
crit_iW:Year -0.000305  0.002582 2881 -0.118278  0.9059
 Correlation: 
             crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF       0                                              
crit_iU       0      0                                       
crit_iW       0      0      0                                
crit_iA:Year -1      0      0      0                         
crit_iF:Year  0     -1      0      0      0                  
crit_iU:Year  0      0     -1      0      0      0           
crit_iW:Year  0      0      0     -1      0      0      0    

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-6.98370498 -0.39653580  0.02349353  0.43356564  5.15742550 

Number of Observations: 2987
Number of Groups: 103 
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 2961 3033  -1469     2893    2937
Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 Taxon    (Intercept) 6.9112e+03 83.13360        
          Year        1.7582e-03  0.04193 -1.000 
 Residual             1.2239e-01  0.34985        
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403 18.0295239   1.611
crit_iF       0.1848404 12.3352135   0.015
crit_iU      12.3405800 21.1414908   0.584
crit_iW       5.3248537 19.9323887   0.267
crit_iA:Year -0.0122717  0.0090916  -1.350
crit_iF:Year  0.0022365  0.0062202   0.360
crit_iU:Year -0.0038701  0.0106608  -0.363
crit_iW:Year -0.0003054  0.0100511  -0.030

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -1.000  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -1.000  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -1.000  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -1.000  0.000  0.000  0.000
&gt; 
</code></pre>
"
"0.147925109681887","0.151969366063391"," 38591","<p>I'm trying to compute a mixed model using jags (R2jags) and I got very strange divergence. The chains started so well, very well according to the results expected (also see <code>lmer</code> output of the same model below). But at certain point, the chains went crazy. Just look at the traceplot for <strong>delta_tau</strong> variable - the chains start so well, but then the green chain goes crazy, then blue and finally red... </p>

<p>Any ideas why this happens? Can't be in initial values, because the chains started so well. Maybe the priors? Why is the system unstable?</p>

<p><img src=""http://i.stack.imgur.com/rn08d.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/v2jof.png"" alt=""enter image description here""></p>

<p><strong>EDIT:</strong> Variables <code>gamma_tau</code> and <code>delta_tau</code> don't fall to exact zero, as you can see on these zoomed-in figures:</p>

<p><img src=""http://i.stack.imgur.com/qkORJ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/gdEpz.png"" alt=""enter image description here""></p>

<p>This is the jags model:</p>

<pre><code>model {

# likelihood
for (i in 1:N) {
    logInd[i] ~ dnorm(mu[i], eps_tau)
    mu[i] &lt;- alpha[crit[i]] + (beta[crit[i]] + delta[species[i]])*year[i] + gamma[species[i]] # ekviv mix1b/c podle me
}

# priors
eps_tau ~ dgamma(1.0E-3, 1.0E-3) 

for (j in 1:no_crit) {
    alpha[j] ~ dnorm(0, 0.0001)
    beta[j] ~ dnorm(0, 0.0001) 
}

for (k in 1:no_species) {
    gamma[k] ~ dnorm(0, gamma_tau)
    delta[k] ~ dnorm(0, delta_tau)
}

gamma_tau ~ dgamma(1.0E-3, 1.0E-3) 
delta_tau ~ dgamma(1.0E-3, 1.0E-3)
}
</code></pre>

<p>Code used to run jags (using R2jags):</p>

<pre><code>no_crit = length(levels(crit))

win.data = list(logInd = mydata$logInd, crit = (as.integer(crit)), 
    	year = mydata$Year, species = (as.integer(mydata$Taxon)),
    	N = nrow(mydata), no_crit = no_crit, no_species = length(levels(mydata$Taxon))
)

inits = function () { list(
    alpha = rnorm(no_crit, 0, 10000),
    beta = rnorm(no_crit, 0, 10000)
)}  

params = c(""alpha"", ""beta"", ""eps_tau"", ""gamma_tau"", ""delta_tau"")

# ni: 1000 -&gt; .. sec
ni &lt;- 20000
nt &lt;- 8
nb &lt;- 8000
nc &lt;- 3

out &lt;- R2jags::jags(win.data, inits, params, ""model.txt"",
    nc, ni, nb, nt,  
    working.directory = paste(getwd(), ""/tmp_bugs/"", sep = """")
)
R2jags::traceplot(out, mfrow = c(4, 2))
</code></pre>

<p>Here is output from the equivalent <code>lmer</code> model:</p>

<pre><code>&gt; summary(lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai2))
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai2 
  AIC  BIC logLik deviance REMLdev
 8558 8630  -4267     8495    8534
Random effects:
 Groups   Name        Variance   Std.Dev.   Corr  
 Taxon    (Intercept) 1.1682e-12 1.0808e-06       
          Year        5.3860e-07 7.3389e-04 0.000 
 Residual             8.7038e-01 9.3294e-01       
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403  8.8116915   3.297
crit_iF       0.1848404  6.0286726   0.031
crit_iU      12.3405800 10.3326242   1.194
crit_iW       5.3248537  9.7416915   0.547
crit_iA:Year -0.0122717  0.0044174  -2.778
crit_iF:Year  0.0022365  0.0030222   0.740
crit_iU:Year -0.0038701  0.0051799  -0.747
crit_iW:Year -0.0003054  0.0048836  -0.063

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -0.999  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -0.999  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -0.999  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -0.999  0.000  0.000  0.000
</code></pre>

<p>Thanks in advance!</p>
"
"0.124741405259997","0.144170799399664"," 41123","<p>I feel overwhelmed after attempting to dig into the literature on how to run my mixed model analysis following it up with using AIC to select the best model or models.  I do not think my data is that complicated, but I am looking for confirmation that what I have done is correct, and then advise on how to proceed.  I am unsure if I should be using lme or lmer and then with either of those, if I should be using REML or ML.</p>

<p>I have a value of selection and I want to know which covariates best influence that value and allow for predictions.  Here's some made up example data and my code for my test that I am working with:</p>

<pre><code>ID=as.character(rep(1:5,3))
season=c(""s"",""w"",""w"",""s"",""s"",""s"",""s"",""w"",""w"",""w"",""s"",""w"",""s"",""w"",""w"")
time=c(""n"",""d"",""d"",""n"",""d"",""d"",""n"",""n"",""n"",""n"",""n"",""n"",""d"",""d"",""d"")
repro=as.character(rep(1:3,5))
risk=runif(15, min=0, max=1.1)
comp1=rnorm(15, mean = 0, sd = 1)
mydata=data.frame(ID, season, time, repro, risk, comp1)
c1.mod1&lt;-lmer(comp1~1+(1|ID),REML=T,data=mydata)
c1.mod2&lt;-lmer(comp1~risk+(1|ID),REML=T,data=mydata)
c1.mod3&lt;-lmer(comp1~season+(1|ID),REML=T,data=mydata)
c1.mod4&lt;-lmer(comp1~repro+(1|ID),REML=T,data=mydata)
c1.mod5&lt;-lmer(comp1~time+(1|ID),REML=T,data=mydata)
c1.mod6&lt;-lmer(comp1~season+repro+time+(1|ID),REML=T,data=mydata)
c1.mod7&lt;-lmer(comp1~risk+season+season*time+(1|ID),REML=T,data=mydata)
</code></pre>

<p>I have ~19 models that explore this data with various combinations and up to a 2 way interaction terms, but always with ID as a random effect and comp1 as my dependent variable.  </p>

<ul>
<li>Q1. Which to use? lme or lmer? does it matter?</li>
</ul>

<p>In both of these, I have the option to use ML or REML - and I get drastically different answers - using ML followed by AIC I end up with 6 models all with similar AIC values and the model combinations simply do not make sense, whereas REML results in 2 of the most likely models being the best.  However, when running REML I cannot use anova any longer.  </p>

<ul>
<li>Q2. is the main reason to use ML over REML because of use with ANOVA?
This is not clear to me.</li>
</ul>

<p>I am still not able to run stepAIC or I do not know of another way to narrow down those 19 models.</p>

<ul>
<li>Q3. is there a way to use stepAIC at this point?</li>
</ul>
"
"0.115769710289734","0.101944149904588"," 41510","<p>How do you explain that ? There's only one operator but the mixed model returns an estimate for the <code>operator</code> random effect. Furthermore the <code>sample</code> effect is confounded with the interaction <code>sample:operator</code>. Below is the R code.</p>

<pre><code>&gt; dd
   sample operator         y
9      10      SCF 0.9153188
10     10      SCF 0.9884982
19    100      SCF 2.0798781
20    100      SCF 2.0464027
29   1000      SCF 3.0401590
30   1000      SCF 3.0114448
39  10000      SCF 4.1348324
40  10000      SCF 4.0840063
49  1e+05      SCF 5.1235795
50  1e+05      SCF 5.1106381
59  1e+06      SCF 6.0803404
60  1e+06      SCF 6.2353263
&gt; str(dd)
'data.frame':   12 obs. of  3 variables:
 $ sample  : Factor w/ 6 levels ""10"",""100"",""1000"",..: 1 1 2 2 3 3 4 4 5 5 ...
     $ operator: Factor w/ 1 level ""SCF"": 1 1 1 1 1 1 1 1 1 1 ...
 $ y       : num  0.915 0.988 2.08 2.046 3.04 ...
&gt; lmer(y ~ (1|sample)+(1|operator)+(1|sample:operator), data=dd) 
Linear mixed model fit by REML 
Formula: y ~ (1 | sample) + (1 | operator) + (1 | sample:operator) 
   Data: dd 
  AIC   BIC logLik deviance REMLdev
 18.6 21.03 -4.302    9.932   8.605
Random effects:
 Groups          Name        Variance   Std.Dev.
 sample:operator (Intercept) 1.87954740 1.370966
 sample          (Intercept) 1.87954925 1.370967
 operator        (Intercept) 0.00063096 0.025119
 Residual                    0.00283931 0.053285
Number of obs: 12, groups: sample:operator, 6; sample, 6; operator, 1

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.5709     0.7921   4.508
</code></pre>

<p>For those who are more familiar with SAS the corresponding code is:</p>

<pre><code>PROC MIXED DATA=dd;
CLASS sample operator;
MODEL y=;
RANDOM sample operator sample*operator;
RUN;
</code></pre>

<p>This is nothing but the crossed 2-way ANOVA with random effects.</p>
"
"0.114582297256771","0.117714964779443"," 41637","<p>I'm trying to fit a multivariate (i.e., multiple response) mixed model in <code>R</code>. Aside from the <code>ASReml-r</code> and <code>SabreR</code> packages (which require external software), it seems this is only possible in <code>MCMCglmm</code>. In the <a href=""http://www.jstatsoft.org/v33/i02/"" rel=""nofollow"">paper</a> that accompanies the <code>MCMCglmm</code> package (pp.6), Jarrod Hadfield describes the process of fitting such a model as like reshaping multiple response variables into one long-format variable and then suppressing the overall intercept. My understanding is that suppressing the intercept changes the interpretation of the coefficient for each level of the response variable to be the mean for that level. Given the above, is it therefore possible to fit a multivariate mixed model using <code>lme4</code>? For example:</p>

<pre><code>data(mtcars)
library(reshape2)
mtcars &lt;- melt(mtcars, measure.vars = c(""drat"", ""mpg"", ""hp""))
library(lme4)
m1 &lt;- lmer(value ~ -1 + variable:gear + variable:carb + (1 | factor(carb)),
    data = mtcars)
summary(m1)
#  Linear mixed model fit by REML 
#  Formula: value ~ -1 + variable:gear + variable:carb + (1 | factor(carb)) 
#     Data: mtcars 
#   AIC   BIC logLik deviance REMLdev
#   913 933.5 -448.5    920.2     897
#  Random effects:
#   Groups       Name        Variance Std.Dev.
#   factor(carb) (Intercept) 509.89   22.581  
#   Residual                 796.21   28.217  
#  Number of obs: 96, groups: factor(carb), 6
#  
#  Fixed effects:
#                    Estimate Std. Error t value
#  variabledrat:gear  -7.6411     4.4054  -1.734
#  variablempg:gear   -1.2401     4.4054  -0.281
#  variablehp:gear     0.7485     4.4054   0.170
#  variabledrat:carb   5.9783     4.7333   1.263
#  variablempg:carb    3.3779     4.7333   0.714
#  variablehp:carb    43.6594     4.7333   9.224
</code></pre>

<p>How would one interpret the coefficients in this model? Would this method also work for generalized linear mixed models?</p>
"
"0.104598848163826","0.107458569276045"," 43634","<p>I have 299 surveys collected from 299 individuals working at 26 different locations. I want to understand how the location specific features relate to the individual survey scores.  The only inference I have as to location features is gathered from the individual survey scores.  Is it a valid strategy to calculate means for each location based on the individual scores, and include this as a level 2 variable?  Further, does it also make sense to include the same variable but as the level 1 variable, with slope varying freely between locations, if I want to compare the relative usefulness of the mean (best estimate of 'reality') to a persons individual score? (their perception of reality and response biases).  </p>

<p>I feel like I may have some circularity in the logic.  My implementation in R for one of the variables of interest follows, any feedback is welcome!</p>

<pre><code>lmer(X21~X25+meanX25+(X25|X1),data=datai)
Linear mixed model fit by REML 
Formula: X21 ~ X25 + meanX25 + (X25 | X1) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 1079 1105 -532.7     1056    1065
Random effects:
 Groups   Name        Variance Std.Dev. Corr   
 X1       (Intercept) 0.384983 0.62047         
          X25         0.012382 0.11127  -1.000 
 Residual             1.936068 1.39143 
Number of obs: 299, groups: X1, 26
Fixed effects:
            Estimate Std. Error t value
(Intercept)  1.13616    0.38013   2.989
X25          0.56683    0.05265  10.766
meanX25      0.33897    0.12213   2.775

Correlation of Fixed Effects:
        (Intr) X25   
X25     -0.119       
meanX25 -0.838 -0.389
</code></pre>
"
"0.168660574814382","0.173271736553374"," 45278","<p>I m working on a piecewise linear growth model and I need help to understand how to write my <code>lmer()</code> code and how to interpret the <code>R</code> output.</p>

<p>My data are the sales return of different IDs over a period of time. I want to know how the sales-return (growth) changes after a certain event (breakpoint).
To define the breakpoint I inserted a coded variable.</p>

<pre><code>df = data.frame (
  ID = c(1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2),
  sales = c(1,4,10,12,20,26,28,30,31,32,33,2,5,9,12,15,19,26,27,29,31,32,34,36),
  var1 = c(1,2,3,3,3,3,3,3,3,3,3,1,2,3,4,4,4,4,4,4,4,4,4,4),
  var2 = c(0,0,0,0,0,0,1,2,3,4,5,0,0,0,0,0,0,0,1,2,3,4,5,6))
</code></pre>

<p>I need to apply a Multi-Level Model -- a 2-level-model to be more precise -- using the <code>lme4</code> package. I'm looking for the correct <code>lmer()</code> code to estimate this equation:</p>

<p>$$Y_{ti} = \pi _{0i} + (\gamma _{00}+\varepsilon _{0i})a_{1ti} + (\gamma _{10}+\varepsilon _{1i})a_{2ti} + e_{ti}$$</p>

<p>My data and variables:</p>

<blockquote>
  <p>level 1: individual ID-level    </p>
  
  <p>level 2: inter-individual level </p>
  
  <p>$\varepsilon _{0i}$: var1 (this is my first coded variable, period 1)</p>
  
  <p>$\varepsilon _{1i}$: var2 (this is my second coded variable, period 2)</p>
  
  <p>sales: dependent variable   ID: random effect (is this correct?)  </p>
  
  <p>var1 and var 2: fixed effects (is this correct?)</p>
</blockquote>

<p>I think the code for my model should be:</p>

<pre><code>test &lt;- lmer(sales ~ 0 + var1 + var2 + (1| ID), data=df)
</code></pre>

<p><strong>Q1:</strong> is this code appropriate?</p>

<p>My Output</p>

<pre><code>&gt; summary(test)
Linear mixed model fit by REML 
Formula: sales ~ 0 + var1 + var2 + (1 | ID) 
   Data: df 
   AIC   BIC logLik deviance REMLdev
 154.1 158.8 -73.05    147.8   146.1
Random effects:
 Groups   Name        Variance Std.Dev.
 ID       (Intercept) 11.646   3.4127  
 Residual             25.902   5.0894  
Number of obs: 24, groups: ID, 2

Fixed effects:
     Estimate Std. Error t value
var1   5.5828     0.7759   7.195
var2   3.5039     0.5646   6.206

Correlation of Fixed Effects:
     var1  
var2 -0.433
</code></pre>

<p><strong>Q2:</strong> Interpretation (sorry, I m not really familiar with statistical interpretation :( )<br>
Is it correct that:   </p>

<ul>
<li>fixed effects: var1 Estimate is my slope parameter for period 1?  </li>
<li>fixed effects: var2 Estimate is my slope parameter for period 2?  </li>
</ul>

<p>So I could say that the sales return growth is smaller in period 2 than in period 1?</p>
"
"0.0935560539449976","0.0961138662664425"," 47050","<p>I have data about seed predation (SP) in fruits of three differents colors (yellow, motted, dark) and in two fruiting seasons (2007, 2008). I performed a GLMM and the outcome showed that the interaction term (color:season) was significant, and some combinations of this interaction have significant Pr(>|z|), but I don't think they are the right significant combinations, because when I look the bwplot, this combinations seems to be very different from the other ones. So, I would like to know if there is any test ""a posteriori"" to know the p-values â€‹â€‹for each combination of color:season, and thereby be able to Know what conbination/s is/are really significant. </p>

<pre><code>m1=lmer(SP ~ color + season:color +(1|Site:tree), data=datosfl, family=""poisson"")
AIC   BIC logLik deviance
178.3 196.6 -81.14    162.3
Random effects:
Groups      Name        Variance Std.Dev.
obsBR       (Intercept) 0.064324 0.25362 
Site:tree   (Intercept) 0.266490 0.51623 
Number of obs: 73, groups: obsBR, 73; Sitio:Ã¡rbol, 37

                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            2.5089     0.2750   9.125   &lt;2e-16 ***
colorM                -0.1140     0.3242  -0.352   0.7250    
colorO                -0.6450     0.4178  -1.544   0.1227    
Temporada2008         -0.7343     0.3104  -2.365   0.0180 *  
colorM:Temporada2008   0.2505     0.4352   0.576   0.5648    
colorO:Temporada2008   1.1445     0.5747   1.992   0.0464 * 
</code></pre>
"
"0.188277926694887","0.182679567769276"," 48582","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/48696/generalized-linear-mixed-model-in-r-with-repeated-measures"">Generalized Linear Mixed Model in R with repeated measures</a>  </p>
</blockquote>



<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.188277926694887","0.182679567769276"," 48696","<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.175570071928499","0.210431828326727"," 51005","<p>I know there are a slew of lmer specification questions already floating around.  Please let me know if this is a duplicate, or if it is deemed off-topic, and I'll delete it.</p>

<p>I am using a forward stepwise approach in an attempt to optimially specify the random effects structure for my data.  The base model I am attempting to step-up from has a random effects correlation structure like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38476 38632 -19220    38440
Random effects:
 Groups Name        Variance Std.Dev. Corr          
 SubjID (Intercept)  2.7844  1.6687                 
        SOA.s.c     39.1269  6.2551    0.931        
        SOA.s.c2    15.7080  3.9633   -0.943 -0.997 
Number of obs: 44061, groups: SubjID, 38
</code></pre>

<p><strong>Sub issue</strong>:  I am (uneasily) at peace with the high correlations between the intercept, SOA.s.c, and SOA.s.c2.  Those values make some sense, although they are <strong>much</strong> higher than I would like.  I think I am correct in allowing them to be estimated so long as model still converges... but I stand ready to be corrected (and chided).</p>

<p>My step-up canidate models are the following:</p>

<ol>
<li><code>(1+SOA.s.c+SOA.s.c2|SubjID)+(1+SessionNum.c|SubjID)</code></li>
<li><code>(1+SOA.s.c+SOA.s.c2|SubjID)+(0+SessionNum.c|SubjID)</code></li>
<li><code>(1+SOA.s.c+SessionNum.c+SOA.s.c2|SubjID)</code></li>
</ol>

<p><strong>Actual Question</strong>:  Given that repeated measures were taken for each SubjID along each of the three above listed variables, are each of these three sensible options to test?  In particular, I am concerned with model 1, because it provides the lowest AIC and BIC, but appears to estimate yet another SubjID interept.  Is that really what it is doing?  Is that okay?</p>

<p><strong>For reference</strong>:</p>

<p>The random effects correlations for the first set, <code>(1+SOA.s.c+SOA.s.c2|SubjID)+(1+SessionNum.c|SubjID)</code>, look like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38284 38466 -19121    38242
Random effects:
 Groups Name         Variance  Std.Dev. Corr          
 SubjID (Intercept)   2.687227 1.63928                
        SOA.s.c      39.713777 6.30189   0.968        
        SOA.s.c2     16.853598 4.10531  -0.984 -0.995 
 SubjID (Intercept)   0.224172 0.47347                
        SessionNum.c  0.070583 0.26567  0.977         
Number of obs: 44061, groups: SubjID, 38
</code></pre>

<p>The random effects correlations for the second set, <code>(1+SOA.s.c+SOA.s.c2|SubjID)+(0+SessionNum.c|SubjID)</code>, look like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38307 38472 -19134    38269
Random effects:
 Groups Name         Variance  Std.Dev. Corr          
 SubjID (Intercept)   2.939991 1.71464                
        SOA.s.c      40.131292 6.33493   0.932        
        SOA.s.c2     15.941733 3.99271  -0.941 -0.996 
 SubjID SessionNum.c  0.066763 0.25839                
</code></pre>

<p>The random effects correlations for the second set, <code>(1+SOA.s.c+SOA.s.c2+SessionNUm.c|SubjID)</code>, look like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38286 38477 -19121    38242
Random effects:
 Groups Name         Variance Std.Dev. Corr                 
 SubjID (Intercept)   2.98820 1.72864                       
        SOA.s.c      39.85077 6.31275   0.932               
        SessionNum.c  0.07066 0.26582   0.316  0.050        
        SOA.s.c2     16.94324 4.11622  -0.947 -0.995 -0.052 
</code></pre>
"
"0.104598848163826","0.107458569276045"," 56380","<p>The <code>lme4</code> package in R includes the <code>cake</code> dataset. </p>

<pre><code>library(lme4)
head(cake[,2:4], 20)
   recipe temperature angle
1       A         175    42
2       A         185    46
3       A         195    47
4       A         205    39
5       A         215    53
6       A         225    42
7       B         175    39
8       B         185    46
9       B         195    51
10      B         205    49
11      B         215    55
12      B         225    42
13      C         175    46
14      C         185    44
15      C         195    45
16      C         205    46
17      C         215    48
18      C         225    63
19      A         175    47
20      A         185    29
</code></pre>

<p>I've analysed the <code>cake</code> dataset using two different models below. The first model is a 2 factor ANOVA:</p>

<pre><code>summary(aov(angle ~ temperature + recipe, cake))
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature   5   2100   420.1   6.918 4.37e-06 ***
recipe        2    135    67.5   1.112     0.33    
Residuals   262  15908    60.7                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...and the second is a mixed effects model, with <code>temperature</code> as a random effect:</p>

<pre><code>lmer(angle ~ recipe + (1| temperature), data=cake, REML=F)
Linear mixed model fit by maximum likelihood 
Formula: angle ~ recipe + (1 | temperature) 
   Data: cake 
  AIC  BIC logLik deviance REMLdev
 1893 1911 -941.7     1883    1877
Random effects:
 Groups      Name        Variance Std.Dev.
 temperature (Intercept)  6.4399  2.5377  
 Residual                60.2560  7.7625  
Number of obs: 270, groups: temperature, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   33.122      1.320  25.093
recipeB       -1.478      1.157  -1.277
recipeC       -1.522      1.157  -1.315

Correlation of Fixed Effects:
        (Intr) recipB
recipeB -0.438       
recipeC -0.438  0.500
</code></pre>

<p>Is someone able to provide a summary of what the mixed effect model has done differently to the ANOVA?</p>
"
"0.132308240331124","0.135925533206117"," 56600","<p>I have produced the following model: </p>

<pre><code>&gt;lmer(TotalPayoff~PgvnD*Type+Type*Asym+PgvnD*Asym+Game*Type+Game*PgvnD+Game*Asym+
                   (1|Subject)+(1|Pairing),REML=FALSE,data=table1)-&gt;m1

PgvnD=A percentage (numeric)
Asym= a factor 0 or 1
Type=a factor 1 or 2
Game= a factor 1 or 2
</code></pre>

<p>from this model the terms <code>Type</code>, <code>Game</code> and <code>PgvnD:Asym</code> were shown to be  significant by removal from the model. <code>PgvnD</code> and <code>Asym</code> on there own were not significant but were left in the model because the interaction between them was. The summary of this model is as follows;</p>

<pre><code>&gt; m7
Linear mixed model fit by maximum likelihood 
Formula: TotalPayoff ~ Type + PgvnD * Asym + Game + (1 | Subject) + (1 |Pairing) 
   Data: table1 
  AIC  BIC logLik deviance REMLdev
 1014 1038 -497.8    995.6   964.4
Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.000   0.0000 
 Pairing  (Intercept) 716.101  26.7601 
 Residual              89.364   9.4533 
Number of obs: 113, groups: Subject, 73; Pairing, 61

Fixed effects:
            Estimate Std. Error t value
(Intercept)   81.727      6.332  12.907
Type2          7.926      2.852   2.779
PgvnD         -8.466      7.554  -1.121
Asym1        -12.167      7.583  -1.604
Game2         15.374      7.147   2.151
PgvnD:Asym1   26.618      9.710   2.741

Correlation of Fixed Effects:
            (Intr) Type2  PgvnD  Asym1  Game2 
Type2       -0.188                            
PgvnD       -0.218 -0.038                     
Asym1       -0.620  0.081  0.189              
Game2       -0.483  0.009 -0.010 -0.015       
PgvnD:Asym1  0.233 -0.267 -0.766 -0.328 -0.011
</code></pre>

<p>Am I interpreting these results correctly?</p>

<ul>
<li><code>TotalPayoff</code> is higher when <code>Type=1</code> than in <code>Type=2</code>, it is also higher when <code>game=2</code> than when <code>game=1</code>. </li>
<li>Also <code>TotalPayoff</code> increases significantly with <code>PgvnD</code> if <code>Asym=1</code> but not if <code>ASym=0</code> (indicated by significant interaction term but non-significant single terms).</li>
</ul>

<p>Also I notice that the <code>Subject</code> random effect has SD and variance of 0. Can this then be removed from the model? What does this really mean?</p>
"
"0.214363849462162","0.220224533750418"," 56695","<p>I understand that we use random effects (or mixed effects) models when we believe that some model parameter(s) vary randomly across some grouping factor. I have a desire to fit a model where the response has been normalized and centered (not perfectly, but pretty close) across a grouping factor, but an independent variable <code>x</code> has not been adjusted in any way. This led me to the following test (using <em>fabricated</em> data) to ensure that I'd find the effect I was looking for if it was indeed there. I ran one <em>mixed</em> effects model with a random intercept (across groups defined by <code>f</code>) and a second <em>fixed</em> effect model with the factor f as a fixed effect predictor. I used the R package <code>lmer</code> for the mixed effect model, and the base function <code>lm()</code> for the fixed effect model. Following is the data and the results. </p>

<p>Notice that <code>y</code>, regardless of group, varies around 0. And that <code>x</code> varies consistently with <code>y</code> within group, but varies much more across groups than <code>y</code></p>

<pre><code>&gt; data
      y   x f
1  -0.5   2 1
2   0.0   3 1
3   0.5   4 1
4  -0.6  -4 2
5   0.0  -3 2
6   0.6  -2 2
7  -0.2  13 3
8   0.1  14 3
9   0.4  15 3
10 -0.5 -15 4
11 -0.1 -14 4
12  0.4 -13 4
</code></pre>

<p>If you're interested in working with the data, here is <code>dput()</code> output:</p>

<pre><code>data&lt;-structure(list(y = c(-0.5, 0, 0.5, -0.6, 0, 0.6, -0.2, 0.1, 0.4, 
-0.5, -0.1, 0.4), x = c(2, 3, 4, -4, -3, -2, 13, 14, 15, -15, 
-14, -13), f = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 
4L, 4L, 4L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")), 
.Names = c(""y"",""x"",""f""), row.names = c(NA, -12L), class = ""data.frame"")
</code></pre>

<p>Fitting the mixed effects model:</p>

<pre><code>&gt; summary(lmer(y~ x + (1|f),data=data))
Linear mixed model fit by REML 
Formula: y ~ x + (1 | f) 
   Data: data 
   AIC   BIC logLik deviance REMLdev
 28.59 30.53  -10.3       11   20.59
Random effects:
 Groups   Name        Variance Std.Dev.
 f        (Intercept) 0.00000  0.00000 
 Residual             0.17567  0.41913 
Number of obs: 12, groups: f, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.008333   0.120992   0.069
x           0.008643   0.011912   0.726

Correlation of Fixed Effects:
  (Intr)
x 0.000 
</code></pre>

<p>I note that the intercept variance component is estimated 0, and importantly to me, <code>x</code> is not a significant predictor of <code>y</code>.</p>

<p>Next I fit the fixed effect model with <code>f</code> as a predictor instead of a grouping factor for a random intercept:</p>

<pre><code>&gt; summary(lm(y~ x + f,data=data))

Call:
lm(formula = y ~ x + f, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.16250 -0.03438  0.00000  0.03125  0.16250 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.38750    0.14099  -9.841 2.38e-05 ***
x            0.46250    0.04128  11.205 1.01e-05 ***
f2           2.77500    0.26538  10.457 1.59e-05 ***
f3          -4.98750    0.46396 -10.750 1.33e-05 ***
f4           7.79583    0.70817  11.008 1.13e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1168 on 7 degrees of freedom
Multiple R-squared: 0.9484, Adjusted R-squared: 0.9189 
F-statistic: 32.16 on 4 and 7 DF,  p-value: 0.0001348 
</code></pre>

<p>Now I notice that, as expected, <code>x</code> is a significant predictor of <code>y</code>.</p>

<p><strong>What I am looking for</strong> is intuition regarding this difference. In what way is my thinking wrong here? Why do I incorrectly expect to find a significant parameter for <code>x</code> in both of these models but only actually see it in the fixed effect model?</p>
"
"0.0540146129294635","0.0554913665617863"," 57395","<p>I have the following model:</p>

<pre><code>&gt; model1&lt;-lmer(aph.remain~sMFS1+sAG1+sSHDI1+sbare+season+crop
  +(1|landscape),family=poisson)
</code></pre>

<p>...and this is the summary output.  </p>

<pre><code>&gt; summary(model1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: aph.remain ~ sMFS1 + sAG1 + sSHDI1 + sbare + season + crop 
         +      (1 | landscape) 
  AIC  BIC logLik deviance
 4057 4088  -2019     4039
Random effects:
 Groups    Name        Variance Std.Dev.
 landscape (Intercept) 0.74976  0.86588 
Number of obs: 239, groups: landscape, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.6613761  0.1344630  19.793  &lt; 2e-16 
sMFS1        0.3085978  0.1788322   1.726  0.08441   
sAG1         0.0003141  0.1677138   0.002  0.99851    
sSHDI1       0.4641420  0.1619018   2.867  0.00415 
sbare        0.4133425  0.0297325  13.902  &lt; 2e-16 
seasonlate  -0.5017022  0.0272817 -18.390  &lt; 2e-16 
cropforage   0.7897194  0.0672069  11.751  &lt; 2e-16
cropsoy      0.7661506  0.0491494  15.588  &lt; 2e-16 
</code></pre>

<p>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â </p>

<pre><code>Correlation of Fixed Effects:
           (Intr) sMFS1  sAG1   sSHDI1 sbare  sesnlt crpfrg
sMFS1      -0.007                                          
sAG1        0.002 -0.631                                   
sSHDI1      0.000  0.593 -0.405                            
sbare      -0.118 -0.003  0.007 -0.013                     
seasonlate -0.036  0.006 -0.006  0.003 -0.283              
cropforage -0.168 -0.004  0.016 -0.014  0.791 -0.231       
cropsoy    -0.182 -0.028  0.030 -0.001  0.404 -0.164  0.557
</code></pre>

<p>It is probably overdispersed, but how exactly do I calculate this? </p>

<p>Thanks very much.</p>
"
"0.148540185556025","0.166474099685359"," 58900","<p>I have very recently started learning about Generalised Linear Mixed Models and was using R to explore what difference it makes to treat group membership as either fixed or random effect. In particular, I am looking at the example dataset discussed here:</p>

<p><a href=""http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm"">http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm</a></p>

<p><a href=""http://www.ats.ucla.edu/stat/r/dae/melogit.htm"">http://www.ats.ucla.edu/stat/r/dae/melogit.htm</a></p>

<p>As outlined in this tutorial, the effect of Doctor ID is appreciable and I was expecting the mixed model with a random intercept to give better results. However, comparing AIC values for the two methods suggest that this model is worse:</p>

<pre><code>&gt; require(lme4) ; hdp = read.csv(""http://www.ats.ucla.edu/stat/data/hdp.csv"")
&gt; hdp$DID = factor(hdp$DID) ; hdp$Married = factor(hdp$Married)
&gt; GLM = glm(remission~Age+Married+IL6+DID,data=hdp,family=binomial);summary(GLM)

Call:
glm(formula = remission ~ Age + Married + IL6 + DID, family = binomial, 
data = hdp)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5265  -0.6278  -0.2272   0.5492   2.7329  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.560e+01  1.219e+03  -0.013    0.990    
Age         -5.869e-02  5.272e-03 -11.133  &lt; 2e-16 ***
Married1     2.688e-01  6.646e-02   4.044 5.26e-05 ***
IL6         -5.550e-02  1.153e-02  -4.815 1.47e-06 ***
DID2         1.805e+01  1.219e+03   0.015    0.988    
DID3         1.932e+01  1.219e+03   0.016    0.987   

[...]

DID405       1.566e+01  1.219e+03   0.013    0.990    
DID405       1.566e+01  1.219e+03   0.013    0.990    
DID406      -2.885e-01  3.929e+03   0.000    1.000    
DID407       2.012e+01  1.219e+03   0.017    0.987    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 10353  on 8524  degrees of freedom
Residual deviance:  6436  on 8115  degrees of freedom
AIC: 7256

Number of Fisher Scoring iterations: 17


&gt; GLMM = glmer(remission~Age+Married+IL6+(1|DID),data=hdp,family=binomial) ; m

Generalized linear mixed model fit by the Laplace approximation 
Formula: remission ~ Age + Married + IL6 + (1 | DID) 
Data: hdp 
AIC  BIC logLik deviance
7743 7778  -3867     7733
Random effects:
Groups Name        Variance Std.Dev.
DID    (Intercept) 3.8401   1.9596  
Number of obs: 8525, groups: DID, 407

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.461438   0.272709   5.359 8.37e-08 ***
Age         -0.055969   0.005038 -11.109  &lt; 2e-16 ***
Married1     0.260065   0.063736   4.080 4.50e-05 ***
IL6         -0.053288   0.011058  -4.819 1.44e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
         (Intr) Age    Marrd1
Age      -0.898              
Married1  0.070 -0.224       
IL6      -0.162  0.012 -0.033


&gt; extractAIC(GLM) ; extractAIC(GLMM)

[1]  410.000 7255.962
[1]    5.000 7743.188
</code></pre>

<p>Thus, my questions are:</p>

<p>(1) Is it appropriate to compare the AIC values provided by the two functions? If so, why does the fixed effect model do better?</p>

<p>(2) What is the best way to identify if fixed or random effects are more important (ie to quantify that the variability due to the doctor is more important than patient characteristics?</p>
"
"0.214585643468166","0.22045239158038"," 59912","<p>So I often do little self-experiments where I blind &amp; randomize things; these can be formulated as your normal <em>t</em>-tests, but sometimes the measured metrics have extensive baselines which seem like they could be used for more accurate answers. A bunch of reading upon <em>n</em>-of-1 and single-subject designs suggested that people have been moving to mixed/hierarchical/multilevel models for analyzing such setups (eg. Nelson 2012 <a href=""http://etd.lsu.edu/docs/available/etd-04252012-152015/unrestricted/NelsonDiss.pdf"" rel=""nofollow"">""Hierarchical linear modeling versus visual analysis of single subject design data""</a> or <a href=""http://www.eric.ed.gov/PDFS/EJ800974.pdf"" rel=""nofollow"" title=""Van den Noortgate et al 2007"">""The Aggregation of Single-Case Results using Hierarchical Linear Models""</a>).</p>

<p>As I understand it, the idea is to split the subject's data into experiment vs baseline, and treat those as the groups. I'm trying to understand how sensible this is with a recent experiment, so hopefully someone can point out if I go wrong in using <code>lmer</code> here.</p>

<hr>

<p>We start with a regular linear model which examines purely the experimental data (the numeric <code>Response</code> vs the binary <code>Intervention</code> variables) and ignores the extensive baseline phase before, during, and after the experiment:</p>

<pre><code>R&gt; experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max
-1.0156 -0.8889 -0.0156  0.1111  1.1111

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    3.0156     0.0889    33.9   &lt;2e-16
Intervention  -0.1267     0.1262    -1.0     0.32

Residual standard error: 0.711 on 125 degrees of freedom
  (145 observations deleted due to missingness)
Multiple R-squared:  0.008, Adjusted R-squared:  6.73e-05
F-statistic: 1.01 on 1 and 125 DF,  p-value: 0.317

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 % 97.5 %
(Intercept)   2.8397  3.192
Intervention -0.3765  0.123
</code></pre>

<p>The estimated coefficient is not statistically-significant: -0.38-0.12. But it's definitely slanted towards being negative. So this is the 'conservative' case, where we ignore the baseline entirely. What's the optimistic case? Well, it seems to me that the optimistic case is when we take the entire baseline and assume it is exactly the same as the 'off'/0 intervention in the experiment, in which case we get a narrower CI (because our estimate of the intercept has halved its standard error):</p>

<pre><code>R&gt; experiment$Intervention[is.na(experiment$Intervention)] &lt;- 0
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max 
-1.9924 -0.8889  0.0076  1.0076  1.1111 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    2.9924     0.0375   79.88   &lt;2e-16
Intervention  -0.1036     0.1012   -1.02     0.31

Residual standard error: 0.746 on 458 degrees of freedom
Multiple R-squared:  0.00228,   Adjusted R-squared:  0.000101 
F-statistic: 1.05 on 1 and 458 DF,  p-value: 0.307

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 %  97.5 %
(Intercept)   2.9188 3.06607
Intervention -0.3025 0.09538
</code></pre>

<p>It's narrowed to -0.30-0.10; still not statistically-significant, but closer.</p>

<p>It seems to me that a hierarchical model ought to produce a CI intermediate between the pessimistic and optimistic cases: it loses some power because it's estimating how different the two phases are before it does any combining.</p>

<p>Here is my multilevel model, split between baseline and experimental phases:</p>

<pre><code>library(lme4)
experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
experiment$Phase &lt;- ifelse(is.na(experiment$Intervention), TRUE, FALSE)
model &lt;- lmer(Response ~ Intervention + (1|Phase), data=experiment); summary(model)

...
 AIC BIC logLik deviance REMLdev
 286 297   -139      273     278
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5057   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)     3.016      0.136    22.2
Intervention   -0.127      0.126    -1.0

Correlation of Fixed Effects:
            (Intr)
Interventin -0.461

m &lt;- mcmcsamp((lmer(Response ~ Intervention + (1|Phase), data=experiment)), n = 100000)
HPDinterval(m, prob=0.95)$fixef

                lower   upper
(Intercept)  -45.3107 56.6558
Intervention  -0.3742  0.1191
</code></pre>

<p>The estimated CI comes out exactly in the middle, as expected:</p>

<ol>
<li>pessimistic   -0.38 0.12</li>
<li>hierarchical  -0.37 0.11</li>
<li>optimistic    -0.30 0.10</li>
</ol>

<p>So, my basic question is: is this a sane approach to take? It's spitting out answers that seem intuitively correct, but that might just be a coincidence.</p>

<hr>

<p>Incidentally, one might be worried about time trends. The randomization/blocking would fix that in the experimental period but not the baseline. Fortunately, that doesn't seem to be an issue:</p>

<pre><code>experiment$Time &lt;- 1:nrow(experiment)
summary(lmer(Response ~ Intervention + Time + (1|Phase), data=experiment))

...
 AIC BIC logLik deviance REMLdev
 298 312   -144      272     288
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5055   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)   3.42517    0.42325    8.09
Intervention -0.12398    0.12621   -0.98
Time         -0.00132    0.00129   -1.02

Correlation of Fixed Effects:
            (Intr) Intrvn
Interventin -0.128       
Time        -0.947 -0.021
</code></pre>
"
"0.0810219193941953","0.0832370498426794"," 63642","<p>I am quite new to <code>lmer</code> and quite confused about how to select the random effects part of the model.</p>

<p>The data I am working on is divided into dependent and independent variables. Each variable has two grouping factors. 1). sampling time (sample 1:7) and sampling column (1:24, where 1:3, 4:6 and so are replicates).</p>

<p>I have explored these different possibilities, but I am not sure on which is the correct one to use.</p>

<pre><code>fm1 &lt;- lmer(dependent ~ independent +(1 + independent|Sample)+(1+ independent|Column), data=Data)

fm2 &lt;- lmer(dependent ~ independent +(1|Sample)+(1|Column), data=Data)

fm3 &lt;- lmer(dependent ~ independent + (Sample|Column), data=Data)
</code></pre>

<p><code>fm3</code> is the model with the lowest AIC and BIC values, but I am not sure that this is the right one to use?</p>
"
"0.147925109681887","0.136772429457052"," 63872","<p>I am think that it is possible to analyse <strong>a model with just random effects</strong> but I am not sure as I have never done it. I am looking for guidance on whether it is appropriate, what assumptions I need to be aware of, and how to do it properly.</p>

<p>From my study of an insect; </p>

<ul>
<li>I have a response variable (age at death, ""age"")  </li>
<li>Two treatments
(""Treat1"" and ""Treat2"") both of which have two levels (Treat1 has
""A"" and ""B"", and Treat2 has ""P"" and ""Q"")  </li>
<li>There is also 40 genotypes
(1-40)  </li>
<li>With four replicates (w,x,y,z) of each combination of
Genotype/Treat1/Treat2 </li>
<li>Each replicate contains 50 individuals</li>
</ul>

<p>Put simply, my data looks like 32000 rows of this:</p>

<pre><code>Treat1  Treat2  Genotype  Block  Individual   Age   
A       P       1         w      1            23
A       P       1         w      2            35
A       P       1         w      3            44
.       .       .         .      .            .
.       .       .         .      .            .
.       .       .         .      .            .
B       Q       40        z      50           76     
</code></pre>

<p>I would like to know if each combination of Treat1 and Treat2 (AP,AQ,BP,BQ) have genetic genetic variation - i.e. is there variation between my 40 genotypes within each treatment combination?</p>

<p>I think I need a model for each of AP, AQ, BP, and BQ, along the lines of </p>

<pre><code>Age ~ Genotype [ Treat1 == ""A"" &amp; Treat2 == ""P""] * Block [ Treat1 == ""A"" &amp; Treat2 == ""P""]
</code></pre>

<p>Where  Genotype and Block are random effects. I hear Gamma distribtions are better to use in lifespan (time to death) models.</p>

<p><strong>My questions are:</strong></p>

<p>a. Is this an appropriate way to show whether or not my genotypes have variation?</p>

<p>b. Can I build the four models as defined above or is that a really poor way of doing it?</p>

<p>c. If possible, what functions should I be using in R (lm, glm, lmer... &amp; summary, summary.lm, aov, anova...)?</p>

<p>d. What should I expect, if gamma is more suitable than gaussian, to see when I compare <code>plot(model)</code> for gamma compared to gaussian?</p>

<hr>

<p>This is currently my model...</p>

<pre><code>AP= df$Treat1==""A"" &amp; df$Treat2==""P""
apmodel&lt;- lmer(df$Age[AP]~(1|df$Genotype[AP])+(1|df$Block[AP]))
summary(apmodel)
</code></pre>

<p>Which I think is right but I'm not sure what to do with the output..</p>

<pre><code>&gt; summary(apmodel)
Linear mixed model fit by REML 
Formula: df$Age[AP] ~ (1 | df$Genotype[AP]) + (1 | df$Block[AP]) 
       AIC   BIC logLik deviance REMLdev
     57343 57371 -28667    57336   57335
    Random effects:
     Groups           Name        Variance Std.Dev.
     df$Genotype[AP]  (Intercept) 17.23798 4.15186 
     df$Block[AP]     (Intercept)  0.15416 0.39263 
     Residual                     93.18777 9.65338 
    Number of obs: 7757, groups: df$line[AP], 40; df$Block[AP], 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  49.9948     0.6939   72.05
</code></pre>

<p><strong>Is there genetic variance??</strong></p>
"
"0.132308240331124","0.135925533206117"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.0810219193941953","0.0832370498426794"," 64352","<p>I'm facing a problem with a binomial <code>glmer</code> model. I want to find if differences in flower presence in pine trees is due to procedence of the tree.
My model is as follows: <code>FlorMas ~ Proc + (1|Blq)</code>.
Proc is a factor with nine levels, one of it (<code>TAMR</code>) presents no flower at all (variable value for all <code>TAMR</code> trees is 0).
This model gives me this output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: FlorMas ~ Proc + (1 | Blq) 
   Data: flower.data 
 AIC   BIC logLik deviance
 593 647.7 -285.5      571
Random effects:
 Groups Name        Variance Std.Dev.
 Blq    (Intercept) 0.18476  0.42983 
Number of obs: 1067, groups: Blq, 8

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -1.7668     0.2958  -5.974 2.32e-09 ***
ProcTAMR     -16.8758  1080.5608  -0.016  0.98754    
ProcARMY      -0.3543     0.3910  -0.906  0.36490    
ProcASPE      -1.4891     0.5260  -2.831  0.00464 ** 
ProcCOCA      -2.4947     0.7619  -3.274  0.00106 ** 
ProcMIMI      -1.2040     0.4930  -2.442  0.01459 *  
ProcORIA      -1.5360     0.5739  -2.676  0.00744 ** 
ProcPLEU      -1.9437     1.0538  -1.845  0.06511 .  
ProcPTOV       0.1693     0.3508   0.483  0.62945    
ProcSCRI       0.5060     0.3346   1.512  0.13050    

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I don't understand that values for <code>TAMR</code> procedence, as if it has all zero values it should be different from the others.
Any help will be appreciated.</p>
"
"0.187112107889995","0.18021349924958"," 64535","<p>My ecological question is: ""What are the trends in percent coral cover by island and depth across the state of Hawaii from 1999 to 2012?""  </p>

<p>I am trying to analyze this hierarchical data set using R with 10 transects at each depth, 2 depths per site, and site nested in island.</p>

<p>Data structure:</p>

<pre><code>Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.
 WYear: 0-13. It was suggested that I use this factor as a covariate for years.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.
 Year: 1999 â€“ 2012 (14 years)

Dependent variable: PercentCover
</code></pre>

<p>Currently, I am using the <code>lmer</code> function in the <code>lmerTest</code> package and this is the model that I've constructed.</p>

<pre><code>fit1 &lt;- lmer(PercentCover ~ WYear*Island*DepthCat +
             (1+WYear|Island/Site/DepthCat/Transect) + (1|Year), data=Benthic)
</code></pre>

<p>Unfortunately, the data are spotty (i.e., missing data in multiple years for a number of sites) so the model returns <code>[1] ""Asymptotic covariance matrix A is not positive!""</code>, even using arcsin transformed data. I can still run the summary statistics to get results, but I don't feel comfortable with the error message. Perhaps I have not structured the model correctly in terms of organizing the nested factors, but the number of observations for each of the levels in the summary stats seems correct. I tried different and simpler iterations of the model such as:</p>

<pre><code> fit1 &lt;- lmer(PercentCover ~ WYear + Island + DepthCat + (1+WYear|Transect/Site) + 
              (1|Year), data=Benthic)
</code></pre>

<p>which works, but doesn't give me the interaction information and returns a larger AIC suggesting that the model does not fit the data as well.</p>

<p>To deal with all of the missing data, I tried another approach by using the regression slope of percent cover over time as the dependent variable for each site X depth combination.</p>

<pre><code>Data structure:

Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.

Dependent variable: Trend
</code></pre>

<p>I used the following model, but the summary results did not make much sense, even after transforming the data.</p>

<pre><code> fit1&lt;-lmer(Trend ~ Island*DepthCat + (1| Island/Site/DepthCat/Transect), data=Benthic)
</code></pre>

<p>Any suggestions on improving my analytical approach would be appreciated.</p>
"
"0.155145163900903","0.159386815778093"," 65371","<p>I'm fitting a random effects model with <code>glmer</code> to some business data. The aim is to analyse sales performance by distributor, taking into account regional variation. I have the following variables:</p>

<ul>
<li><code>distcode</code>: distributor ID, with about 800 levels</li>
<li><code>region</code>: top-level geographical ID (north, south, east, west)</li>
<li><code>zone</code>: mid-level geography nested within <code>region</code>, about 30 levels in all</li>
<li><code>territory</code>: low-level geography nested within <code>zone</code>, about 150 levels</li>
</ul>

<p>Each distributor operates in only one territory. The tricky part is that this is summarised data, with one data point per distributor. So I have 800 data points and I'm trying to fit (at least) 800 parameters albeit in a regularised fashion.</p>

<p>I've fitted a model as follows:</p>

<pre><code>glmer(ninv ~ 1 + (1|region/zone/territory) + (1|distcode), family=poisson)
</code></pre>

<p>This runs without a problem, although it does print a note:</p>

<blockquote>
  <p>Number of levels of a grouping factor for the random effects
  is <em>equal</em> to n, the number of observations</p>
</blockquote>

<p>Is this a sensible thing to do? I get finite estimates of all the coefficients, and the AIC also isn't unreasonable. If I try a poisson GLMM with the identity link, the AIC is much worse so the log link is at least a good starting point.</p>

<p>If I plot the fitted values vs the response, I get what is essentially a perfect fit, which I guess is because I have one data point per distributor. Is that reasonable, or am I doing something completely silly?</p>

<p>This is using data for one month. I can get data for multiple months and get some replication that way, but I'd have to add new terms for month-to-month variation and possible interactions, correct?</p>

<hr>

<p>ETA: I ran the above model again, but without a <code>family</code> argument (so just a gaussian LMM rather than a GLMM). Now <code>lmer</code> gave me the following error:</p>

<blockquote>
  <p>Error in (function (fr, FL, start, REML, verbose)  : 
    Number of levels of a grouping factor for the random effects
  must be less than the number of observations</p>
</blockquote>

<p>So I'd guess that I'm not doing something sensible, as changing the family shouldn't have an effect. But the question now is, why did it work in the first place?</p>
"
"0.0935560539449976","0.0961138662664425"," 65390","<p>I have models like this:</p>

<pre><code>require(nlme)

set.seed(123)
n &lt;- 100
k &lt;- 5
cat &lt;- as.factor(rep(1:k, n))
cat_i &lt;- 1:k # intercept per kategorie
x &lt;- rep(1:n, each = k)
sigma &lt;- 0.2
alpha &lt;- 0.001
y &lt;- cat_i[cat] + alpha * x + rnorm(n*k, 0, sigma)
plot(x, y)

m1 &lt;- lm(y ~ x)
summary(m1)

m2 &lt;- lm(y ~ cat + x)
summary(m2)

m3 &lt;- lme(y ~ x, random = ~ 1|cat, na.action = na.omit)
summary(m3)
</code></pre>

<p>Now I am trying to assess whether the random effect should be present in the model. So I compare the models using AIC or anova, and I get the following error:</p>

<pre><code>&gt; AIC(m1, m2, m3)
   df       AIC
m1  3 1771.4696
m2  7 -209.1825
m3  4 -154.0245
Warning message:
In AIC.default(m1, m2, m3) :
  models are not all fitted to the same number of observations  
&gt; anova(m2, m3)
Error in anova.lmlist(object, ...) : 
  models were not all fitted to the same size of dataset
</code></pre>

<p>As you can see, in both cases I use the same dataset. I have found two remedies, but I don't consider them satisfying:</p>

<ol>
<li><a href=""https://stat.ethz.ch/pipermail/r-help/2012-March/307348.html"">Adding <code>method = ""ML""</code> to the lme() call</a> - not sure if it is good idea to change the method.</li>
<li>Using <code>lmer()</code> instead. Surprisingly, this works, despite the fact that lmer() uses REML method. However I dont like this solution because the <code>lmer()</code> doesn't show p-values for coefficients - I like to use older <code>lme()</code> instead.</li>
</ol>

<p>Do you have any idea if this is a bug or not and how can we go around that?</p>
"
"0.187112107889995","0.192227732532885"," 65489","<p>My problem can be summarized very simply: I'm using a linear mixed-effects model and I am trying to get p-values using pvals.fnc(). The problem is that this function seems to have some trouble estimating p-values directly from the t-values associated with model coefficients (Baayen et al., 2008), and I don't know what is going wrong with the way I do it (i.e. according to what I have read, it should work). So, I'm explaining my model below, and if you can point out what I am doing wrong and suggest changes I would really appreciate it!</p>

<p><strong>DESCRIPTION</strong>: I have a 2 by 2 within subjects design, fully crossing two <em>categorical</em> factors, ""Gram"" and ""Number"", each with two levels. This is the command I used to run the model:</p>

<pre><code>&gt;m &lt;- lmer(RT ~ Gram*Number + (1|Subject) + (0+Gram+Number|Subject) + (1|Item),data= data)
</code></pre>

<p>If I understand this code, I am getting coefficients for the two fixed effects (Gram and Number) and their interaction, and I am fitting a model that has by-subject intercepts and slopes for the two fixed effects, and a by-item intercept for them. Following Barr et al. (2013), I thought that this code gets rid of the correlation parameters. I don't want estimate the correlations because I want to get the p-values using pvals.fnc (), and I read that this function doesn't work if there are correlations in the model.</p>

<p>The command seems to work:</p>

<pre><code>&gt;m
Linear mixed model fit by REML 
Formula: RT ~ Gram * Number + (1 | Subject) + (0 + Gram + Number | Subject) + (1 |Item) 
   Data: mverb[mverb$Region == ""06v1"", ] 
   AIC   BIC logLik deviance REMLdev
 20134 20204 -10054    20138   20108
Random effects:
 Groups      Name        Variance  Std.Dev. Corr          
 Item       (Intercept)   273.508  16.5381               
 Subject     Gramgram        0.000   0.0000               
             Gramungram   3717.213  60.9689    NaN        
             Number1        59.361   7.7046    NaN -1.000 
 Subject     (Intercept) 14075.240 118.6391               
 Residual                35758.311 189.0987               
Number of obs: 1502, groups: Item, 48; Subject, 32

Fixed effects:
             Estimate Std. Error  t value
(Intercept)    402.520     22.321  18.033
Gram1          -57.788     14.545  -3.973
Number1         -4.191      9.858  -0.425
Gram1:Number1   15.693     19.527   0.804

Correlation of Fixed Effects:
            (Intr) Gram1  Numbr1
Gram1       -0.181              
Number1     -0.034  0.104       
Gram1:Nmbr1  0.000 -0.002 -0.011
</code></pre>

<p>However, when I try to calculate the p-values I still get an error message:</p>

<pre><code>&gt;pvals.fnc(m, withMCMC=T)$fixed
Error in pvals.fnc(m, withMCMC = T) : 
MCMC sampling is not implemented in recent versions of lme4
  for models with random correlation parameters
</code></pre>

<p>Am I making a mistake when I specify my model? Shouldn't pvals.fnc() work if I removed the correlations?</p>

<p>Thanks for your help!</p>
"
"0.132308240331124","0.135925533206117"," 68106","<p>I'm having trouble understanding the output of my <code>lmer()</code> model. It is a simple model of an outcome variable (Support) with varying State intercepts / State random effects:</p>

<pre><code>mlm1 &lt;- lmer(Support ~ (1 | State))
</code></pre>

<p>The results of <code>summary(mlm1)</code> are:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Support ~ (1 | State) 
   AIC   BIC logLik deviance REMLdev
 12088 12107  -6041    12076   12082
Random effects:
 Groups   Name        Variance  Std.Dev.
 State    (Intercept) 0.0063695 0.079809
 Residual             1.1114756 1.054265
Number of obs: 4097, groups: State, 48

Fixed effects:
            Estimate Std. Error t value
(Intercept)  0.13218    0.02159   6.123
</code></pre>

<p>I take it that the variance of the varying state intercepts / random effects is <code>0.0063695</code>. But when I extract the vector of these state random effects and calculate the variance</p>

<pre><code>var(ranef(mlm1)$State)
</code></pre>

<p>The result is: <code>0.001800869</code>, considerably smaller than the variance reported by <code>summary()</code>.</p>

<p>As far as I understand it, the model I have specified can be written:</p>

<p>$y_i = \alpha_0 + \alpha_s + \epsilon_i, \text{ for } i = \{1, 2, ..., 4097\}$</p>

<p>$\alpha_s \sim N(0, \sigma^2_\alpha), \text{ for } s = \{1, 2, ..., 48\}$</p>

<p>If this is correct, then the variance of the random effects ($\alpha_s$) should be $\sigma^2_\alpha$. Yet these are not actually equivalent in my <code>lmer()</code> fit. </p>
"
"0.114582297256771","0.117714964779443"," 69664","<p>I want to compare two â€‹GLMs with binomial dependent variables. The results are: </p>

<pre><code> m1 &lt;- glm(symptoms ~ 1,         data=data2)
 m2 &lt;- glm(symptoms ~ phq_index, data=data2)
</code></pre>

<p>The model test gives the following results: </p>

<pre><code>â€‹ anova(m1, m2)â€‹
         no AIC    logLik   LR.stat df  Pr(&gt;Chisq)   
 m1      1  4473.9 -2236.0                        
 m2      9  4187.3 -2084.7  302.62  8   &lt; 2.2e-16 ***
</code></pre>

<p>â€‹I am used to comparing these kinds of models using chi-squared values, a chi-squared difference, and a chi-squared difference test. Since all other models in the paper are compared this way, and since I'd like to report them in a table together: why exactly is this model test different from my other model tests in which I get chi-squared values and difference tests? Can I obtain chi-squared values from this test? </p>

<p>Results from other model comparisons (e.g., GLMER), look like this: </p>

<pre><code>    #Df AIC     BIC     logLik  Chisq   Chi     DF diff Pr(&gt;Chisq)
m3  13  11288   11393   -5630.9 392.16          
m4  21  11212   11382   -5584.9 92.02   300.14  8       0.001
</code></pre>
"
"0.124741405259997","0.144170799399664"," 70227","<p>I want to model the infection rates in bees based on weather conditions. The weather variables are rolling means for different time periods and durations. Dependent data is infection levels gathered in March and the independent variables are the weather aggregates (e.g. from 30 day period from Jan1-Jan30, 90 day period from Dec1-Feb28), a few thousands of them and highly correlated.</p>

<p>PCA techniques did not work since the infections are not so strongly related to weather. I have also tried Bayesian Model Averaging and Boosted Regression Trees, since variables could be selected based on variable importance they calculate.</p>

<p>But since, my data is longitudinal and my apiaries have a fixed location, I think mixed-models are a good choice. Is there a way to do variable selection based on mixed-models?</p>

<p>What I have done now is to<br>
1. run <code>glmer</code> for each of the independent variables separately,<br>
2. remove those variable whose p-values for fixed-effect estimates are below 0.05 (not sure if this is a right thing - if the estimate for a variable is not significant, that variable being the only one in the model, it is right to drop that variable, is it?)<br>
3. from the variables that are left over, test for correlation between the variables<br>
4. remove the variables that are highly correlated, giving preference to the variable that has the lowest AIC.  </p>

<p>Or should I at this stage, not worry about p-values of Intercepts and only focus on AIC (or BIC)? since some of the variables have high p-values but AICs lower than than those with low p-values. </p>

<p>I have tried reading up a lot, and there is no one fool-proof solution for variable selection, but would like to know if there is anything inherently wrong with my method. As I am not a statistician, equations often look like beautiful Arabic calligraphy and there lies my dead-end.</p>
"
"0.192870746165604","0.198143811351516"," 80866","<p>After weeks of reading and trying I decided to post my question here because I could not find a convincing solution.
I radio tracked two animals for several months and now I want to find out 1) what influences the activity of the animals and 2) when (hour after sunset) they are showing the highest activity and 3) what influences the travel distance.</p>

<p>For question 1) I created a generalized linear mixed model, with animal as a random factor looking like this:
glmer(cbind(active,inactive)~offspring+season+observation_time+temperature+precipitation+season:temperature+season:precipitation+temperature:precipitation,family=binomial)</p>

<p>offspring and season are factors coded with 0 and 1,observation_time in minutes, temperature is in Â°C and precipitation in mm.</p>

<p>The first lines of the summary() are showing that:</p>

<p>AIC       BIC    logLik  deviance 
 438.3251  460.0690 -209.1625  418.3251 </p>

<p>Random effects:
 Groups Name        Variance Std.Dev.
 Name   (Intercept) 0.06594  0.2568<br>
Number of obs: 65, groups: Name, 2</p>

<p>So my question is: is this model build up correctly? Is there an improvement possible or neccessary? 
It is very difficult to work with this data, because most diagnostic plots which are usually used to evaluate models are different because of the random factor.
I also wanted to boxcox transform the response but one animal showed no activity one day (the activity is zero) and therefore this is not possible.
I try to eliminate variables or interaction terms but in most cases I can only eliminate one interaction term. After that all variables and interaction terms seems to be significant. For variable selection I use differend aproaches (AIC, BIC, AVOVA).</p>

<p>For the 3) question I created a LMM like this: 
lmer(sqrt(distance)~aktivity+season+offspring+observation_time+temperature+precipitation+season:temperature+...(other interaction terms)</p>

<p>The result from summary():</p>

<p>REML criterion at convergence: 513.0257 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Name     (Intercept)  11.6     3.405<br>
 Residual             295.8    17.199<br>
Number of obs: 62, groups: Name, 2</p>

<p>Does any of this values tell me something about the goodness of my model?</p>

<p>How can I check if data transformation is neccessary? And if yes, which one?</p>

<p>I'm honest, I'm quite new in this field and all I learnd about R and statistics do not really work with glmm or lmm. Or it's to complicated for me.
I also created a gam without the random factor to check the relationship between the variables and response but I don't know what to do with the results (seems to be no linear relationship between activity and observation_time and rainfall).
How do I fit variables to my model? 
An other idea is to fit the model without the random factor and add the random factor afterwards. Would it be ok to do it like this?</p>

<p>For the second question - at which hour after sunset they are showing the highest activity - I have no idea how the model could be build up...</p>

<p>Sorry for the amount of questions but I'm working for weeks on this and it is very frustrating...
Thanks in advance for all your ideas and help!
Iris </p>
"
"0.148540185556025","0.138728416404466"," 81430","<p>I have a mixed model and the data looks like this:</p>

<pre><code>&gt; head(pce.ddply)
  subject Condition errorType     errors
1    j202         G         O 0.00000000
2    j202         G         P 0.00000000
3    j203         G         O 0.08333333
4    j203         G         P 0.00000000
5    j205         G         O 0.16666667
6    j205         G         P 0.00000000
</code></pre>

<p>Each subject provides two datapoints for errorType (O or P) and each subject is in either Condition G (N=30) or N (N=33).  errorType is a repeated variable and Condition is a between variable.  I'm interested in both main effects and the interactions.  So, first an anova:</p>

<pre><code>&gt; summary(aov(errors ~ Condition * errorType + Error(subject/(errorType)),
                 data = pce.ddply))

Error: subject
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Condition  1 0.00507 0.005065   2.465  0.122
Residuals 61 0.12534 0.002055               

Error: subject:errorType
                    Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
errorType            1 0.03199 0.03199   10.52 0.001919 ** 
Condition:errorType  1 0.04010 0.04010   13.19 0.000579 ***
Residuals           61 0.18552 0.00304                     
</code></pre>

<p>Condition is not significant, but errorType is, as well as the interaction.</p>

<p>However, when I use lmer, I get a totally different set of results:</p>

<pre><code>&gt; lmer(errors ~ Condition * errorType + (1 | subject),
                    data = pce.ddply)
Linear mixed model fit by REML 
Formula: errors ~ Condition * errorType + (1 | subject) 
   Data: pce.ddply 
    AIC    BIC logLik deviance REMLdev
 -356.6 -339.6  184.3     -399  -368.6
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.000000 0.000000
 Residual             0.002548 0.050477
Number of obs: 126, groups: subject, 63

Fixed effects:
                       Estimate Std. Error t value
(Intercept)            0.028030   0.009216   3.042
ConditionN             0.048416   0.012734   3.802
errorTypeP             0.005556   0.013033   0.426
ConditionN:errorTypeP -0.071442   0.018008  -3.967

Correlation of Fixed Effects:
            (Intr) CndtnN errrTP
ConditionN  -0.724              
errorTypeP  -0.707  0.512       
CndtnN:rrTP  0.512 -0.707 -0.724
</code></pre>

<p>So for lmer, Condition and the interaction are significant, but errorType is not.</p>

<p>Also, the lmer result is exactly the same as a glm result, leading me to believe something is wrong.</p>

<p>Can someone please help me understand why they are so different?  I suspect I am using lmer incorrectly (though I've tried many other versions like (errorType | subject) with similar results.</p>

<p>(I have seen researchers use both approaches in the literature with similar data.)</p>
"
"0.318799816363514","0.340359524239707"," 82102","<p>I hope this is an appropriate forum to post this question. I recently upgraded my R software from 2.15.0 to 3.0.2. I also upgraded the lme4 package from .999999-0 to 1.1-2. After doing so, the results from one of my linear mixed models analyses have changed a bit unexpectedly. In some respects, I was expecting some change, as the lme4 developers very clearly stated that they had made some significant changes to some fundamental components in the package. However, the changes that I am seeing (described below) make me think that something else is awry. I will start by explaining the experimental design, which is quite simple and then the issue at hand.</p>

<p>My experiment is a basic repeated measures design. I used 24 ""Items"" that each appeared in three different ""Conditions"" (SmallClause_Som, NoSmallClause, SmallClause_NoSom). Levels of Condition were rotated across three presentation lists such that each Subject (45 total, each assigned to a particular list) only saw one level of each item.</p>

<p>I used lmer() for the analysis. Condition was entered in as a Fixed effect and ""Subject"" and ""Item"" were entered as Random effects.</p>

<p>The problem:
Using the current version of R 3.0.2 and lme4 1.1-2 with NoSmallClause as the reference level (and no weighting on any of the contrasts), the ConditionSmallClause_Som/NoSmallClause contrast produces a t value of 1.680. </p>

<p>But, when I change reference level to SmallClause_Som (to observe the one remaining contrast) I get not only a change in the polarity of the effect (plus to minus, as expected), but the values change as well.</p>

<p>When I use R 2.15.0 and lme4 .999999-0 (on another computer), I do not experience this issue. I get slightly different values, but they do not change (apart from the polarity) when I change reference level.</p>

<p>My colleague also tried my analysis for me using R 3.0.2 and a version of lme4 (pre version 1.0) (I don't know exactly which version, but it was before the major changes) and he also does not experience the issue.</p>

<p>R 2.15.0 lme4 1.1-2 (older) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
 AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance   Std.Dev. Corr          
 Subject  (Intercept)                0.98998765 0.994981               
          ConditionSmallClause_Som   0.00203374 0.045097 -1.000        
          ConditionSmallClause_NoSom 0.00019873 0.014097  1.000 -1.000 
 Item     (Intercept)                0.96231875 0.980978               
          ConditionSmallClause_Som   0.89924400 0.948285 -0.020        
          ConditionSmallClause_NoSom 0.62128577 0.788217 -0.256  0.361 
 Residual                            1.68810777 1.299272               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                       Estimate Std. Error t value
(Intercept)                  2.9583     0.2584  11.447
ConditionSmallClause_Som     0.3639     0.2165   1.680
ConditionSmallClause_NoSom   0.1472     0.1878   0.784

Correlation of Fixed Effects:
            (Intr) CnSC_S
CndtnSmlC_S -0.116       
CndtnSmC_NS -0.260  0.392

&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
  AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance  Std.Dev. Corr          
 Subject  (Intercept)                0.9023239 0.949907               
          ConditionNoSmallClause     0.0020340 0.045099 1.000         
          ConditionSmallClause_NoSom 0.0035039 0.059194 1.000  1.000  
 Item     (Intercept)                1.8238288 1.350492               
          ConditionNoSmallClause     0.8992237 0.948274 -0.687        
          ConditionSmallClause_NoSom 0.9804329 0.990168 -0.604  0.670 
 Residual                            1.6881050 1.299271               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3174  10.468
ConditionNoSmallClause      -0.3639     0.2165  -1.680
ConditionSmallClause_NoSom  -0.2167     0.2243  -0.966

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.588       
CndtnSmC_NS -0.521  0.638
</code></pre>

<p>R 3.0.2 and lme4 1.1-2 (newer) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3942.557  4022.312 -1955.278  3910.557 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.9522   0.9758              
          ConditionSmallClause_NoSom 0.1767   0.4204    0.03      
          ConditionSmallClause_Som   0.1760   0.4196   -0.15  0.92
 Item     (Intercept)                1.2830   1.1327              
          ConditionSmallClause_NoSom 0.7782   0.8822   -0.41      
          ConditionSmallClause_Som   1.4901   1.2207    0.09  0.41
 Residual                            1.6466   1.2832              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  2.9583     0.2814  10.512
ConditionSmallClause_NoSom   0.1472     0.2133   0.690
ConditionSmallClause_Som     0.3639     0.2741   1.327

Correlation of Fixed Effects:
            (Intr) CSC_NS
CndtnSmC_NS -0.357       
CndtnSmlC_S -0.007  0.451
&gt; #anova (test.lmer3, test.lmer4)
&gt; 
&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)
Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3951.357  4031.113 -1959.679  3919.357 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.88980  0.9433              
          ConditionNoSmallClause     0.04299  0.2073   0.83       
          ConditionSmallClause_NoSom 0.01562  0.1250   0.90  0.67 
 Item     (Intercept)                2.39736  1.5483              
          ConditionNoSmallClause     0.72053  0.8488   -0.04      
          ConditionSmallClause_NoSom 1.87804  1.3704   -0.16  0.53
 Residual                            1.65166  1.2852              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3525   9.425
ConditionNoSmallClause      -0.3639     0.2004  -1.816
ConditionSmallClause_NoSom  -0.2167     0.2963  -0.731

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.045       
CndtnSmC_NS -0.160  0.514
</code></pre>

<p>My question:
What is going on here? Why is changing the reference level producing a shift from 1.327 to -1.816 in the t scores for the new version of lme4 whereas it produces the same (disregarding sign) value of 1.680/-1.680 in the old version's t scores? Only the older version seems to make sense to me.</p>

<p>1) Am I specifying my model incorrectly for the new version of lme4?</p>

<p>2) Am I missing some basic fundamental fact about how contrasts work? That is, is it possible to get different values just from changing the reference level? (the correlation values look a bit odd in the newer output).</p>

<p>3) Is this a bug in lme4?</p>

<p>4) Some other explanation...?</p>

<p>I have had some other odd issues as well with this same analysis using lme4 1.1-2. For example, if I don't clear the workspace and re-run an analysis, the values also will change between analyses (and also within the analysis as I change the reference level). This never happened to me on the earlier version (and it still does not happen when I run it on the earlier version now).</p>

<p>I hope someone can help with this. I found two other similar questions online (after much searching) but neither had any informative responses.</p>

<p>Thanks DT</p>
"
"0.181170519432929","0.173715454915118"," 82379","<p>I want to test the fixed and random effects of some covariates on a discrete variable with non negative values. In exploratory analysis I fitted a null Poisson GLM and an null Poisson GLMM. However, the GLMM underestimated the mean value of the response variable even after inclusion of fixed and/or random covariates. I also tried Bayesian approaches, zero-inflated models and negative binomial distributions but the ""problem"" remains.</p>

<p>Response variable mean: 0.7804<br>
GLM intercept: 0.7803772<br>
GLMM intercept: 0.6595108</p>

<p>Is the estimated intercept of the GLMM an indicative of poor fitting of the model? </p>

<pre><code>summary(banco2$caes)  
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.   
 0.0000  0.0000  0.0000  0.7804  1.0000 12.0000  


mod1 &lt;- glm(caes ~ 1, poisson, banco2)  
Deviance Residuals:  
    Min       1Q   Median       3Q      Max    
-1.2493  -1.2493  -1.2493   0.2381   6.5689  
Coefficients:  
            Estimate Std. Error z value Pr(&gt;|z|)      
(Intercept) -0.24798    0.01078  -23.01   &lt;2e-16 ***  
---  
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1  
(Dispersion parameter for poisson family taken to be 1)  
    Null deviance: 15304  on 11027  degrees of freedom  
Residual deviance: 15304  on 11027  degrees of freedom  
AIC: 27654  
Number of Fisher Scoring iterations: 5  

exp(mod1$coefficients[1])  
(Intercept)  
  0.7803772  


(mod2 &lt;- lmer(caes ~ 1 + (1 | setor), poisson, data = banco2))  
Generalized linear mixed model fit by the Laplace approximation  
Formula: caes ~ 1 + (1 | setor)  
   Data: banco2  
   AIC   BIC logLik deviance  
 13575 13590  -6785    13571  
Random effects:  
 Groups Name        Variance Std.Dev.  
 setor  (Intercept) 0.39817  0.63101  
Number of obs: 11028, groups: setor, 559  
Fixed effects:  
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -0.41626    0.02937  -14.18   &lt;2e-16 ***  

exp(fixef(mod2))  
(Intercept)  
  0.6595108  
</code></pre>

<p>Best regards!</p>
"
"0.132308240331124","0.135925533206117"," 85742","<p>I am trying to use an IT approach to analyse some ecological data. I have a mixed model with nested random effects (I'm using glmer in package lme4 in R). I initially fit the model with a Poisson error distribution, but it was overdispersed so I added an observation-level random effect to deal with the overdispersion. Model check plots are then very good. However, my next step in the IT approach is to generate a set of candidate models (using function 'dredge'). I have two choices now, and I don't know which is correct / best:</p>

<p>Choice 1. Start with the Poisson lognormal model (i.e. I have already dealt with the overdispersion) and generate a model set using AICc.</p>

<p>Choice 2. Start with the Poisson model, which is overdispersed, and generate a model set using QAICc and specifying the dispersion parameter.</p>

<p>Is it acceptable to do either of these, or should I stick to Choice 2, which is what the example code does in the MuMIn package help documentation? (However, if I go for choice 2, then the model check plots of the global model aren't great, and yet there's no way of checking the averaged model for goodness of fit - so how do I know the model fit is OK?).</p>

<p>Thank you very much for any help you can give.</p>
"
"0.243065758182586","0.249711149528038"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.104598848163826","0.0859668554208358"," 86495","<p>I don't know if this belongs here or in StackExchange, it is a mixed but probably pretty simple question. How do I normally report a Likelihood Ratio Test? I would love a good reference in your answer, I have searched but could not find any good answers.</p>

<pre><code>&gt; glmm0 &lt;- glmer(yngel ~ (1|nest), data, family=poisson(link=""log""))
&gt; glmm &lt;- glmer(yngel ~ age.level + (1|nest) + 0, data, family=poisson(link=""log""))
&gt; anova(glmm0, glmm)
Data: data
Models:
glmm0: yngel ~ (1 | nest)
glmm: yngel ~ age.level + (1 | nest) + 0
      Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
glmm0  2 682.33 689.38 -339.16   678.33                             
glmm   3 672.37 682.95 -333.18   666.37 11.959      1   0.000544 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My best guess so far is: I used likelihood ratio test to compare the model with the fixed effect to a model without it. The model including the fixed effect (age-level) was a better fit ($\chi^2(df=?)=11.96$, $p=0.00054$).</p>

<p>And I cannot actually figure out how many df to report from that. The there is 2 for one model and 3 for the other, and 1 between them.</p>

<p>Thank you for your help.</p>
"
"0.0467780269724988","0.0480569331332213"," 87445","<p>I have fitted random coefficient Poisson analysis in R. I have obtained the following results: </p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )</p>

<p>Formula: frequency ~ 1 + cc + ageveh + make + (1 | AREA) </p>

<p>Data: x </p>

<pre><code>  AIC       BIC    logLik  deviance 
</code></pre>

<p>1359.1477 1389.7370 -672.5739 1345.1477 </p>

<p>Random effects:</p>

<p>Groups Name        Variance Std.Dev.</p>

<p>AREA   (Intercept) 1.323    1.15 </p>

<p>Number of obs: 584, groups: AREA, 8</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|) </p>

<p>(Intercept) -0.12902    0.44432  -0.290   0.7715 </p>

<p>ccL          0.05656    0.12371   0.457   0.6475</p>

<p>agevehO      0.02136    0.09264   0.231   0.8177</p>

<p>make2       -0.45454    0.20632  -2.203   0.0276 *</p>

<p>make3       -0.31799    0.21422  -1.484   0.1377 </p>

<h2>make4       -0.29708    0.14469  -2.053   0.0401 *</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr) ccL    agevhO make2  make3
</code></pre>

<p>ccL      0.052    </p>

<p>agevehO -0.179 -0.232   </p>

<p>make2   -0.171 -0.007 -0.001 </p>

<p>make3   -0.156  0.022 -0.078  0.366 </p>

<p>make4   -0.300 -0.235  0.167  0.544  0.522</p>

<p>However I am unable to interpret the results. </p>
"
"0.104598848163826","0.0859668554208358"," 87510","<p>I have obtained the following results in R from a random coefficient Poisson analysis.</p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )
Formula: frequency ~ 1 + insgen + ageveh + make + area + (1 | ID) 
   Data: Panel </p>

<pre><code>  AIC       BIC    logLik   deviance 
</code></pre>

<p>1099.9670 1134.9262  -541.9835  1083.9670 </p>

<p>Random effects:</p>

<p>Groups Name          Variance   Std.Dev.</p>

<p>ID  (Intercept)     1.551e-11     3.939e-06</p>

<p>Number of obs: 584, groups: ID, 584</p>

<p>Fixed effects:</p>

<pre><code>          Estimate    Std. Error   z value   Pr(&gt;|z|)  
</code></pre>

<p>(Intercept)    -22.98292   8432.07738    -0.003     0.9978  </p>

<p>insgenM          0.02616     0.08806      0.297     0.7664  </p>

<p>ageveho          0.05889     0.08586      0.686     0.4928 </p>

<p>make             -0.10447    0.04126     -2.532    0.0113 *</p>

<p>area1             23.68571  8432.07738   0.003    0.9978  </p>

<p>area2             23.85969  8432.07738   0.003    0.9977 </p>

<p>area3             23.77374  8432.07738   0.003   0.9978  </p>

<hr>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr)  insgnM  ageveh  make    area1   area2 
</code></pre>

<p>insgenM   0.000  </p>

<p>ageveho   0.000  -0.037   </p>

<p>make      0.000   0.071 0.108 </p>

<p>area1    -1.000   0.000 0.000  0.000   </p>

<p>area2    -1.000  0.000 0.000  0.000  1.000</p>

<p>area3   -1.000  0.000  0.000  0.000  1.000  1.000       </p>

<p>I have included predictors:gender (Male,female), area which has 4 levels (area 1,2,3,4) and vehicle age (New and old) and make of car.</p>
"
"0.147925109681887","0.151969366063391"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0572911486283853","0.0980958039828688"," 87834","<p>I am doing linear mixed models using lme4 and this is the results of model comparison:</p>

<pre><code>&gt; anova(lmer5,lmer6,lmer32)

       Df   AIC   BIC logLik   Chisq Chi Df Pr(&gt;Chisq)    
lmer32  9 43172 43226 -21577                              
lmer6  21 43190 43315 -21574  6.3081     12     0.8998    
lmer5  26 43162 43317 -21555 37.9971      5  3.778e-07 ***
</code></pre>

<p>As you can see, the results show that one model is significantly better than the others and normally I will choose model with smallest logLik. However in this result, the logLik is negative. Do you think it is a good idea to choose model from logLik in this case, or should I choose it from AIC or BIC instead.</p>

<p>As no conclusion whether AIC is better than BIC, I am confused which one I should choose. What do you think?</p>
"
"0.0935560539449976","0.0720853996998319"," 90392","<p>I have run Generalized linear mixed model with glmer in lme4. I use R version 3.0.1. My dependent variable is binary (correct or wrong). And this is my results:</p>

<pre><code>&gt; glmer16 &lt;- glmer(result ~ (1|item) + (1|speaker) + vowel + sex + cat + dog + exposure + frequency + v00004 + v00024 + v00034 + v00044, data=data1.frame, family=binomial)
&gt; summary(glmer16)
Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: realisation ~ (1 | item) + (1 | speaker) + vowel + sex + cat +      dog + exposure + frequency + v00004 + v00024 + v00034 +      v00044 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
 881.7026  958.6402 -426.8513  853.7026 

Random effects:
 Groups  Name        Variance Std.Dev.
 speaker (Intercept) 7.0291   2.651   
 item    (Intercept) 0.5084   0.713   
Number of obs: 1800, groups: speaker, 50; item, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 15.52018    5.33634   2.908  0.00363 ** 
vowelhigh    0.16750    0.55907   0.300  0.76449    
vowellow     0.70981    0.63194   1.123  0.26134    
sexmale      1.37080    1.03228   1.328  0.18420    
cat         -0.11460    0.09537  -1.202  0.22953    
dog         -0.05460    0.03633  -1.503  0.13286    
exposure    -0.00404    0.01564  -0.258  0.79613    
frequency   -0.01709    0.15594  -0.110  0.91272    
v00004      -2.83445    0.66039  -4.292 1.77e-05 ***
v00024       0.29687    0.55868   0.531  0.59515    
v00034       0.43899    0.58656   0.748  0.45421    
v00044       0.36663    0.65130   0.563  0.57349    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My questions are: 1) Does v00004 decrease the result of 'wrong' or 'correct'? and 2) what does it mean by significant at the intercept?</p>
"
"0.140334080917496","0.144170799399664"," 90511","<p>My data has a binary response (correct/incorrect), one continuous predictor <code>score</code>, three categorical predictors (<code>race</code>, <code>sex</code>, <code>emotion</code>) and a random intercept for the random factor <code>subj</code>. All predictors are within-subject. One of the categorical factor has 3 levels, the other have two. </p>

<p>I need advice on obtaining ""global"" p-values for each categorical factor (in an ""ANOVA like"" way)</p>

<hr>

<p>Here is how I proceed :</p>

<p>I fitted a binomial GLMM using 'glmer' from the lme4 package (because 'glmmML' doesn't compute on my data and glmmPQL does not provide AIC) and did model selection using <code>drop1</code> repeatedly until no more terms can be dropped. Here is the final model (let's assume it has been validated):</p>

<pre><code>library(lme4)
M5 &lt;- glmer(acc ~ race + sex + emotion + sex:emotion + race:emotion + score +(1|subj), 
        family=binomial, data=subset)
# apparently using family with lmer is deprecated 
drop1(M5, test=""Chisq"")
summary(M5)
</code></pre>

<p><code>drop1</code> gives p-values for the higher level terms only (the two 2-way interactions + <code>score</code>). 
<code>summary</code>gives p-values for every term, but separates the different levels of each categorical factor.</p>

<p>How can I get ""global"" p-values for each factor? I need to report them even if they are not the most relevant or meaningful estimates of signifiance here. How should I proceed? I tried searching on the web and ended up reading about likelihood ratios or the ""Wald test"" but I am not sure if or how this would apply here.</p>

<p>(PS: This is a duplicate from my ""anonymous"" post here that needed editing: <a href=""http://stats.stackexchange.com/questions/90487/binomial-mixed-model-with-categorical-predictors-model-selection-and-getting-p"">Binomial mixed model with categorical predictors: model selection and getting p-values</a> Sorry about that.)</p>
"
"0.140334080917496","0.112132843977516"," 90758","<p>I have run LMM models with different reference categories and this yield different results: </p>

<pre><code>&gt; summary(lmer3)
Linear mixed model fit by maximum likelihood ['merModLmerTest']
Formula: v000001 ~ (1 | item) + (1 + color | speaker) + Language * color *      sex 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
16279.975 16377.355 -8119.988 16239.975 

Random effects:
 Groups   Name        Variance  Std.Dev.  Corr       
 speaker  (Intercept) 8.904e+05 9.436e+02            
          colorblue   1.821e+05 4.267e+02 -0.35      
          colorred    3.428e+05 5.855e+02 -0.44  1.00
 item     (Intercept) 9.502e-06 3.083e-03            
 Residual             1.067e+06 1.033e+03            
Number of obs: 962, groups: speaker, 53; item, 10

Fixed effects:
                                  Estimate Std. Error       df t value Pr(&gt;|t|)
(Intercept)                       10664.67     318.69    38.45  33.464   &lt;2e-16
Languagel2_like                     391.48     421.40    42.13   0.917   0.3642
colorblue                          -179.31     211.02    44.50  -0.850   0.4000
colorred                            116.96     241.44    36.27   0.484   0.6310
sexmale                            -168.01     450.11    38.26  -0.373   0.7110
Languagel2_like:colorblue           758.22     301.01    54.20   2.519   0.0147
Languagel2_like:colorred            463.37     344.01    45.73   1.344   0.1857
Languagel2_like:sexmale            -811.49     607.85    43.49  -1.326   0.1917
colorblue:sexmale                   342.76     294.97    42.57   1.162   0.2517
colorred:sexmale                     13.25     337.44    34.81   0.039   0.9689
Languagel2_like:colorblue:sexmale  -721.37     438.78    54.19  -1.644   0.1059
Languagel2_like:colorred:sexmale   -605.76     497.75    45.29  -1.216   0.2304

(Intercept)                       ***
Languagel2_like                      
colorblue                            
colorred                            
sexmale                              
Languagel2_like:colorblue         *  
Languagel2_like:colorred            
Languagel2_like:sexmale              
colorblue:sexmale                    
colorred:sexmale                     
Languagel2_like:colorblue:sexmale    
Languagel2_like:colorred:sexmale     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And this is the second model:</p>

<pre><code>&gt; summary(lmer43)
Linear mixed model fit by maximum likelihood ['merModLmerTest']
Formula: v000001 ~ (1 | item) + (1 + color3 | speaker) + Language * color3 *      sex 
   Data: data1.frame 

      AIC       BIC    logLik  deviance 
16279.975 16377.355 -8119.988 16239.975 

Random effects:
 Groups   Name        Variance  Std.Dev.  Corr       
 speaker  (Intercept) 7.945e+05 8.913e+02            
          color3white 1.821e+05 4.268e+02 -0.11      
          color3red   2.761e+04 1.661e+02 -0.24 -0.94
 item     (Intercept) 4.961e-06 2.227e-03            
 Residual             1.067e+06 1.033e+03            
Number of obs: 962, groups: speaker, 53; item, 10

Fixed effects:
                                   Estimate Std. Error       df t value
(Intercept)                        10485.36     305.33    39.57  34.341
Languagel2_like                     1149.70     399.61    43.91   2.871
color3white                          179.31     211.03    44.50   0.850
color3red                            296.27     167.08   125.59   1.773
sexmale                              174.75     430.05    38.94   0.406
Languagel2_like:color3white         -758.22     301.01    54.10  -2.519
Languagel2_like:color3red           -294.85     244.46   159.09  -1.206
Languagel2_like:sexmale            -1532.85     577.70    44.74  -2.648
color3white:sexmale                 -342.76     294.98    42.57  -1.162
color3red:sexmale                   -329.51     228.57   113.99  -1.442
Languagel2_like:color3white:sexmale  721.36     438.78    54.10   1.644
Languagel2_like:color3red:sexmale    115.61     351.65   162.98   0.329
                                   Pr(&gt;|t|)    
(Intercept)                         &lt; 2e-16 ***
Languagel2_like                     0.00627 ** 
color3white                         0.40004    
color3red                           0.07862 .  
sexmale                             0.68671    
Languagel2_like:color3white         0.01477 *  
Languagel2_like:color3red           0.22953    
Languagel2_like:sexmale             0.01114 *  
color3white:sexmale                 0.25171    
color3red:sexmale                   0.15215    
Languagel2_like:color3white:sexmale 0.10602    
Languagel2_like:color3red:sexmale   0.74275    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Is it possible to report results from two models? (I know that very little of people would do this but these two models give different pictures). What should I do? Which one should I believe?    </p>
"
"NaN","NaN"," 91805","<p>I am comparing models that I created with the lmer function from the lme4 package using ANOVA. Paricipants and verbs are my random factors, and RT are the Reaction Times that I measured. From the little statistics that I know, I would expect F-statictic values in the output, but instead I get chi-square.</p>

<p>Could you please help me understand why I get that, or whether I should (or could) change that?<br>
Here is my code and output:</p>

<pre><code>m0&lt;-lmer((sqrt(RT))~(1|Participant)+(1|Verbs), data=data, REML=FALSE)
m1&lt;-lmer((sqrt(RT))~(1|Participant)+(1|Verbs)+verbT, data=data, REML=FALSE)

anova(m0,m1)


Data: data
Models:
m0: (sqrt(RT)) ~ (1 | Participant) + (1 | Verbs)
m1: (sqrt(RT)) ~ (1 | Participant) + (1 | Verbs) + verbT
   Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m0  4 -693.68 -672.44 350.84  -701.68                         
m1  5 -693.71 -667.16 351.85  -703.71 2.0238      1     0.1548
</code></pre>
"
"0.174304591325488","0.226193759824356"," 93601","<p>I am a complete novice and dummy when it comes to statistics so I apologise in advance...</p>

<p>I have been asked to report the results of my GLMMs (I ran two) in a table. This table must state: effect, standard error, test statistic, and P value, for all fixed effects. </p>

<p>Unfortunately I am struggling to read my output. </p>

<p>The out put is as follows, if anyone would be kind enough to help I would be very grateful and will know for future reference which bit equates to what (also I have been told my degrees of freedom are different for both the tests, could someone explain why this is?).</p>

<pre><code>GLMM 1-run for predictors of step length. 
Response variable = step length. 
fixed effects = depth and direction threshold. 
random factor = individual

Models:
m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m2 3 373235 373259 -186615 373229 
m1 8 373225 373290 -186605 373209 19.767 5 0.001382 **
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>.</p>

<pre><code>GLMM 2 -run to investigate potential predictors of PDBA.
response variables = depth and step length. 
fixed effect = direction threshold.
random factor = Individual

Models:
m3: PDBA ~ Depth + (1 | ind) + thresholdepth
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m3 6 -48205 -48157 24109 -48217 
m2 11 -48430 -48341 24226 -48452 235.1 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Models:
m4: PDBA ~ step + (1 | ind) + step:threshold
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m4 6 -48206 -48158 24109 -48218 
m2 11 -48430 -48341 24226 -48452 233.81 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Hi, I think the package I used was was lme4? </p>

<p>I have run a summary for the first GLMM and this is what I got, I have no idea which parts are relevant though, I assume it doesn't all go in a table?! </p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                  Estimate Std. Error t value
(Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>The output from the Anova only gives me one p value for each GLMM and I think I need a p value for each of the fixed effects within the models?</p>

<p>Does anyone know what code I can run to get this?
Thank you</p>
"
"0.142712794073708","0.146614546314393"," 93892","<p>I need to get p values for the fixed effects in the following GLMM's I ran. Does anyone know of code that I can run that will give me the p values I need? At the moment the output from the ANOVA only gives me one p value and I believe I need a separate p value for each of the fixed effects in the models. </p>

<p>Thanks in advance.
Code is as follows -</p>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>When I ran GLMM 1 code this is what I got:</p>

<pre><code>m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
m2  3 373235 373259 -186615   373229                            
m1  8 373225 373290 -186605   373209 19.767      5   0.001382 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>summary</p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                 Estimate Std. Error t value
  (Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>OUTPUT FROM SUGGESTED CODE BY SETH (IN COMMENTS)</p>

<pre><code>Models:
m6: step ~ Depth + threshold + (1 | ind)
m5: step ~ Depth + threshold + Depth:threshold + (1 | ind)
   Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
m6  6 373227 373275 -186607   373215                           
m5  8 373225 373290 -186605   373209 5.2901      2      0.071 .
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.214585643468166","0.210431828326727"," 94302","<p>I am performing a parametric bootstrap to test whether I need a specific fixed effect in my model or not. I have mainly done this for exercise and I am interested if my procedure so far is correct.</p>

<p>First, I fit the two models to be compared. One of them includes the effect to be tested for and the other one does not. As I am testing for fixed effects I set <code>REML=FALSE</code>:</p>

<pre><code>    mod8 &lt;- lmer(log(value) 
                 ~ matching 
                 + (sentence_type | subject) 
                 + (sentence_type | context), 
                 data = wort12_lmm2_melt,
                 REML = FALSE)
    mod_min &lt;- lmer(log(value) 
                    ~ 1 
                    + (sentence_type | subject)
                    + (sentence_type | context),
                    data = wort12_lmm2_melt,
                    REML = FALSE)
</code></pre>

<p>Both models are fit on a balanced data set which includes few missing values. There are slightly above 11000 observations for 70 subjects. Every subject saw every item only one time. The dependent variable are reading times; sentence_type and matching are two-level factors. Context and subject are treated as random effects. Context has 40 levels.</p>

<p>I call anova():</p>

<pre><code>    anova(mod_min, mod8)
</code></pre>

<p>and get the output:</p>

<pre><code>    Data: wort12_lmm2_melt
    Models:
    mod_min: log(value) ~ 1 + (sentence_type |  subject) + (sentence_type | context)
    mod8: log(value) ~ matching + (sentence_type |  subject) + (sentence_type |   
    mod8:     context)
            Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
    mod_min  8 3317.6 3375.8 -1650.8   3301.6                            
    mod8     9 3310.9 3376.4 -1646.4   3292.9 8.6849      1   0.003209 **
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Mistrusting the almighty p I set up a parametric bootstrap by hand:</p>

<pre><code>    mod &lt;- mod8
    modnull &lt;- mod_min
    lrt.obs &lt;- anova(mod, modnull)$Chisq[2] # save the observed likelihood ratio test statistic
        n.sim &lt;- 10000  
        lrt.sim &lt;- numeric(n.sim)
        dattemp &lt;- mod@frame
        # pb &lt;- txtProgressBar(min = 0, max = n.sim, style = 3) # set up progress bar to satisfy need for control
        for(i in 1:n.sim) {
        # Sys.sleep(0.1) # progress bar related stuff
        ysim       &lt;- unlist(simulate(modnull) # simulate new observations from the null-model  
        modnullsim &lt;- lmer(ysim 
                           ~ 1
                           + (sentence_type | subject)
                           + (sentence_type | context),
                           data = dattemp,
                           REML = FALSE) # fit the null-model
        modaltsim  &lt;- lmer(ysim
                           ~ matching
                           + (sentence_type | subject)
                           + (sentence_type | context),
                           data = dattemp,
                           REML = FALSE) # fit the alternative model
        lrt.sim[i] &lt;- anova(modnullsim, modaltsim)$Chisq[2] # save the likelihood ratio test statistic
    # setTxtProgressBar(pb, i)
    }

    # assuming chi-squared distribution for comparison

    pchisq((2*(logLik(mod8)-logLik(mod_min))),
           df    = 1,
           lower = FALSE)
</code></pre>

<p>with the output:</p>

<pre><code>    'log Lik.' 0.003208543 (df=9)
</code></pre>

<p>compare to parametric bootstrap p-value</p>

<pre><code>    p_mod8_mod_min &lt;- (sum(lrt.sim&gt;=lrt.obs)+1)/(n.sim+1)  # p-value. alternative: mean(lrt.sim&gt;lrt.obs)
</code></pre>

<p>with the output:</p>

<pre><code>    [1] 0.00319968
</code></pre>

<p>Plot the whole thing:</p>

<pre><code>    xx &lt;- seq(0, 20, 0.1)
    hist(lrt.sim,
         xlim     = c(0, max(c(lrt.sim, lrt.obs))),
         col      = ""blue"", 
         xlab     = ""likelihood ratio test statistic"",
         ylab     = ""density"", 
         cex.lab  = 1.5, 
         cex.axis = 1.2, 
         freq     = FALSE)
    abline(v   = lrt.obs,
           col = ""orange"",
           lwd = 3)
    lines(density(lrt.sim),
          col = ""blue"")
    lines(xx,
          dchisq(xx, df = 1),
          col = ""red"")
    box()
</code></pre>

<p>which yields:</p>

<p><img src=""http://i.stack.imgur.com/bh4YO.png"" alt=""enter image description here""></p>

<p>I do have some questions though:</p>

<p>(1) Is the procedure correct or did I make a mistake?</p>

<p>(2) How is the histogram to be interpreted?</p>

<p>(3) Is the form of the histogram normal or extreme?</p>

<p>Thanks for any help!</p>
"
"0.192870746165604","0.186488293036721"," 94888","<p>I'm analysing some behavioural data using <code>lme4</code> in <code>R</code>, mostly following <a href=""http://www.bodowinter.com/tutorials.html"" rel=""nofollow"">Bodo Winter's excellent tutorials</a>, but I don't understand if I'm handling interactions properly. Worse, no-one else involved in this research uses mixed models, so I'm a bit adrift when it comes to making sure things are right.</p>

<p>Rather than just post a cry for help, I thought I should make my best effort at interpreting the problem, and then beg your collective corrections. A few other asides are:</p>

<ul>
<li>While writing, I've found <a href=""http://stackoverflow.com/questions/17794729/test-for-significance-of-interaction-in-linear-mixed-models-in-nlme-in-r"">this question</a>, showing that <code>nlme</code> more directly give p values for interaction terms, but I think it's still valid to ask with relation to <code>lme4</code>.</li>
<li><code>Livius'</code> answer to <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">this question</a> provided links to a lot of additional reading, which I'll be trying to get through in the next few days, so I'll comment with any progress that brings.</li>
</ul>

<hr>

<p>In my data, I have a dependent variable <code>dv</code>, a <code>condition</code> manipulation (0 = control, 1 = experimental condition, which should result in a higher <code>dv</code>), and also a prerequisite, labelled <code>appropriate</code>: trials coded <code>1</code> for this should show the effect, but trials coded <code>0</code> might not, because a crucial factor is missing.</p>

<p>I have also included two random intercepts, for <code>subject</code>, and for <code>target</code>, reflecting correlated <code>dv</code> values within each subject, and within each of the 14 problems solved (each participant solved both a control and an experimental version of each problem).</p>

<pre><code>library(lme4)
data = read.csv(""data.csv"")

null_model        = lmer(dv ~ (1 | subject) + (1 | target), data = data)
mainfx_model      = lmer(dv ~ condition + appropriate + (1 | subject) + (1 | target),
                         data = data)
interaction_model = lmer(dv ~ condition + appropriate + condition*appropriate +
                              (1 | subject) + (1 | target), data = data)
summary(interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## Linear mixed model fit by REML ['lmerMod']
## ...excluded for brevity....
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  subject  (Intercept) 0.006594 0.0812  
##  target   (Intercept) 0.000557 0.0236  
##  Residual             0.210172 0.4584  
## Number of obs: 690, groups: subject, 38; target, 14
## 
## Fixed effects:
##                                Estimate Std. Error t value
## (Intercept)                    0.2518     0.0501    5.03
## conditioncontrol               0.0579     0.0588    0.98
## appropriate                   -0.0358     0.0595   -0.60
## conditioncontrol:appropriate  -0.1553     0.0740   -2.10
## 
## Correlation of Fixed Effects:
## ...excluded for brevity.
</code></pre>

<p>ANOVA then shows <code>interaction_model</code> to be a significantly better fit than <code>mainfx_model</code>, from which I conclude that there's a significant interaction present (p = .035).</p>

<pre><code>anova(mainfx_model, interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## ...excluded for brevity....
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## mainfx_model       6 913 940   -450      901                          
## interaction_model  7 910 942   -448      896  4.44      1      0.035 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>From there, I isolate a subset of the data for which the <code>appropriate</code> requirement is met (i.e., <code>appropriate = 1</code>), and for it fit a null model, and a model including <code>condition</code> as an effect, compare the two models using ANOVA again, and lo, find that <code>condition</code> is a significant predictor.</p>

<pre><code>good_data = data[data$appropriate == 1, ]
good_null_model   = lmer(dv ~ (1 | subject) + (1 | target), data = good_data)
good_mainfx_model = lmer(dv ~ condition + (1 | subject) + (1 | target), data = good_data)

anova(good_null_model, good_mainfx_model)
</code></pre>

<p>Output:</p>

<pre><code>## Data: good_data
## models:
## good_null_model: dv ~ (1 | subject) + (1 | target)
## good_mainfx_model: dv ~ condition + (1 | subject) + (1 | target)
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## good_null_model    4 491 507   -241      483                          
## good_mainfx_model  5 487 507   -238      477  5.55      1      0.018 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
"
"0.114582297256771","0.117714964779443"," 99660","<p>In a mixed effect model where the intercept is random effect and the slope is fixed effect (see the code below), I understand the output of <code>summary(glmer(...))</code>. But I do not understand <code>coef(glmer(...))</code>; it will output the intercept for each sample. In the example, how are those 100 intercepts estimated? </p>

<pre><code>n &lt;- 100
x &lt;- runif(n,2,6)
a &lt;- -3 
b &lt;- 1.5
s &lt;- 1
N &lt;- 8
id &lt;- 1:n
r  &lt;- rnorm(length(x),0,s) # random factor
p &lt;- function(x,a,b) exp(a+b*x)/(1+exp(a+b*x))
y &lt;- rbinom(length(x), N, prob = p(x,a+r,b))

library(lme4)
model &lt;- glmer(cbind(y,N-y)~x+(1|id),family=binomial)
summary(model)
coef(model) # output here is what I do not understand

# are they estimates for r?
rr &lt;- coef(model)$id[,1]-summary(model)$coefficients[1,1]
plot(r,rr)
</code></pre>

<p>The likelihood of the model, I think, is:
$$
L_{i}=\int_{-\infty}^{\infty}f(y_i|N,a,b,r_{i})g(r_{i}|s)dr_{i}\\
L=\prod_{i=1}^{n} L_{i}
$$</p>

<p>where <em>f</em> is binomial pmf and <em>g</em> is normal pdf. So <em>r</em> should be integrated out. The number of the parameters of this model is 3 (<em>a</em>, <em>b</em>, and <em>s</em>). Although it seems there are 100 intercepts estimated, they are not really considered the parameters of the model, I think. <code>AIC(model)</code> is <code>-2*logLik(model)+3*2</code>. I would like to know what method will give those 100 intercepts.</p>
"
"0.229164594513541","0.235429929558885","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.123763026191503","0.127146693842964","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.362529709036866","0.372441231782465","109215","<p>I try to compute the marginal and conditional $R^2$ for a GLMM using a negative binomial distribution by following the procedure recommended by Nakagawa &amp; Schielzeth (2013) . Unfortunately, the supplementary material of their article does not include an example of a negative binomial distribution (see the online version of the article stated below, I also added their code below). 
I fitted my model using the glmmPQL function from the MASS package.</p>

<pre><code>full_model  &lt;- glmmPQL ( Y~ a + b + c,  random = ~ 1 +  A | location  
, family = negative.binomial (1.4 ) ,data= mydata 
</code></pre>

<p>In particular, I do have the following problems:</p>

<ol>
<li><p>First, I need to extract the fixed effect design matrix of my model. However, full_model @X or model.matrix(full_model) does not work. I also tried to set the argument x=TRUE before extracting the matrix. Well, this should not be too tricky, but the following problems are. </p></li>
<li><p>Second, I need to specify the distribution-speciï¬c variance of my model. Examples in the article (see table 2 &amp; and the supplementary R code of the online article) specify this for a binomial and a Poisson distribution. With some deeper statistical knowledge, it should not be difficult to specify this for a negative binomial distribution. </p></li>
<li><p>Finally, I would need to know if glmmPQL uses additive dispersion or to multiplicative dispersion. In the paper, they state: ""we only consider additive dispersion implementation of GLMMs although the formulae that we present below can be easily modiï¬ed for the use with GLMMs that apply to multiplicative dispersion. "" Thus, in case glmmPQL uses multiplicative dispersion, I would need further help to adjust the formula.</p></li>
</ol>

<p>Can anybody help?</p>

<p>Thanks, best 
Philipp</p>

<p>P.S. R-code is welcome.</p>

<p>Nakagawa &amp; Schielzeth (2013) A general and simple method for obtaining R 2 from generalized linear mixed-effects models. Methods in Ecology and Evolution 2013, 4, 133â€“142. doi: 10.1111/j.2041-210x.2012.00261.x</p>

<p>Their R script:</p>

<pre><code>  #A general and simple method for obtaining R2 from generalized linear mixed-effects models
  #Shinichi Nakagawa1,2 and Holger Schielzeth3
  #1 National Centre of Growth and Development, Department of Zoology, University of    Otago, Dunedin, New Zealand
  #2 Department of Behavioral Ecology and Evolutionary Genetics, Max Planck Institute for Ornithology, Seewiesen, Germany
  #3 Department of Evolutionary Biology, Bielefeld University, Bielefeld, Germany
  #Running head: Variance explained by GLMMs
  #Correspondence:
  #S. Nakagawa; Department of Zoology, University of Otago, 340 Great King Street,    Dunedin, 9054, New Zealand
  #Tel:  +64 (0)3 479 5046
  #Fax: +64 (0)3 479 7584
  #e-mail: shinichi.nakagawa@otago.ac.nz 


  ####################################################
  # A. Preparation
  ####################################################
  # Note that data generation appears below the analysis section.
  # You can use the simulated data table from the supplementary files to reproduce exactly the same results as presented in the paper.

  # Set the work directy that is used for rading/saving data tables
  # setwd(""/Users/R2"")

  # load R required packages
  # If this is done for the first time, it might need to first download and install the package
  # install.package(""arm"")
  library(arm)
  # install.package(""lme4"")
  library(lme4)


  ####################################################
  # B. Analysis
  ####################################################

  # 1. Analysis of body size (Gaussian mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # Read body length data (Gaussian, available for both sexes)
  Data &lt;- read.csv(""BeetlesBody.csv"")

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 | Container), data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model
  # fixef() extracts coefficents for fixed effects
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3] + fixef(mF)[4] * mF@X[, 4]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM
  # Equ. 26, 29 and 30
  # VarCorr() extracts variance components
  # attr(VarCorr(lmer.model),'sc')^2 extracts the residual variance
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + attr(VarCorr(mF), ""sc"")^2)

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +    VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + (attr(VarCorr(mF), ""sc"")^2))

  # AIC and BIC needs to be calcualted with ML not REML in body size models
  m0ML &lt;- lmer(BodyL ~ 1 + (1 | Population) + (1 | Container), data = Data, REML = FALSE)
  mFML &lt;- lmer(BodyL ~ Sex + Treatment + Condition + (1 | Population) + (1 | Container), data = Data, REML = FALSE)

  # View model fits for both models fitted by ML
  summary(m0ML)
  summary(mFML)


  # 2. Analysis of colour morphs (Binomial mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())
  # Read colour morph data (Binary, available for males only)
  Data &lt;- read.csv(""BeetlesMale.csv"")

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(Colour ~ 1 + (1 | Population) + (1 | Container), family = ""binomial"", data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(Colour ~ Treatment + Condition + (1 | Population) + (1 | Container), family = ""binomial"", data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model 
  # fixef() extracts coefficents for fixed effects 
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM
  # see Equ. 29 and 30 and Table 2
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + pi^2/3)

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +     VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + pi^2/3)


  # 3. Analysis of fecundity (Poisson mixed models)
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # Read fecundity data (Poisson, available for females only)
  Data &lt;- read.csv(""BeetlesFemale.csv"")

  # Creating a dummy variable that allows estimating additive dispersion in lmer 
  # This triggers a warning message when fitting the model
  Unit &lt;- factor(1:length(Data$Egg))

  # Fit null model without fixed effects (but including all random effects)
  m0 &lt;- lmer(Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit), family = ""poisson"", data = Data)

  # Fit alternative model including fixed and all random effects
  mF &lt;- lmer(Egg ~ Treatment + Condition + (1 | Population) + (1 | Container) + (1 | Unit), family = ""poisson"", data = Data)

  # View model fits for both models
  summary(m0)
  summary(mF)

  # Extraction of fitted value for the alternative model 
  # fixef() extracts coefficents for fixed effects 
  # mF@X returns fixed effect design matrix
  Fixed &lt;- fixef(mF)[2] * mF@X[, 2] + fixef(mF)[3] * mF@X[, 3]

  # Calculation of the variance in fitted values
  VarF &lt;- var(Fixed)

  # An alternative way for getting the same result
  VarF &lt;- var(as.vector(fixef(mF) %*% t(mF@X)))

  # R2GLMM(m) - marginal R2GLMM 
  # see Equ. 29 and 30 and Table 2 
  # fixef(m0) returns the estimate for the intercept of null model
  VarF/(VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] + log(1 + 1/exp(as.numeric(fixef(m0)))))

  # R2GLMM(c) - conditional R2GLMM for full model
  # Equ. XXX, XXX
  (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/(VarF +    VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] + log(1 + 
                                                                       1/exp(as.numeric(fixef(m0)))))


  ####################################################
  # C. Data generation
  ####################################################

  # 1. Design matrices 
  #---------------------------------------------------

  # Clear memory
  rm(list = ls())

  # 12 different populations n = 960
  Population &lt;- gl(12, 80, 960)

  # 120 containers (8 individuals in each container)
  Container &lt;- gl(120, 8, 960)

  # Sex of the individuals. Uni-sex within each container (individuals are sorted at the pupa stage)
  Sex &lt;- factor(rep(rep(c(""Female"", ""Male""), each = 8), 60))

  # Condition at the collection site: dry or wet soil (four indiviudal from each condition in each container)
  Condition &lt;- factor(rep(rep(c(""dry"", ""wet""), each = 4), 120))

  # Food treatment at the larval stage: special food ('Exp') or standard food ('Cont')
  Treatment &lt;- factor(rep(c(""Cont"", ""Exp""), 480))

  # Data combined in a dataframe
  Data &lt;- data.frame(Population = Population, Container = Container, Sex = Sex, Condition = Condition, Treatment = Treatment)


  # 2. Gaussian response: body length (both sexes)
  #---------------------------------------------------

  # simulation of the underlying random effects (Population and Container with variance of 1.3 and 0.3, respectively)
  PopulationE &lt;- rnorm(12, 0, sqrt(1.3))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.3))

  # data generation based on fixed effects, random effects and random residuals errors
  Data$BodyL &lt;- 15 - 3 * (as.numeric(Sex) - 1) + 0.4 * (as.numeric(Treatment) - 1) + 0.15 * (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container] + 
    rnorm(960, 0, sqrt(1.2))

  # save data (to current work directory)
  write.csv(Data, file = ""BeetlesBody.csv"", row.names = F)


  # 3. Binomial response: colour morph (males only)
  #---------------------------------------------------

  # Subset the design matrix (only males express colour morphs)
  DataM &lt;- subset(Data, Sex == ""Male"")

  # simulation of the underlying random effects (Population and Container with variance of 1.2 and 0.2, respectively)
  PopulationE &lt;- rnorm(12, 0, sqrt(1.2))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.2))

  # generation of response values on link scale (!) based on fixed effects and random effects
  ColourLink &lt;- with(DataM, 0.8 * (-1) + 0.8 * (as.numeric(Treatment) - 1) + 0.5 *    (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container])

  # data generation (on data scale!) based on negative binomial distribution
  DataM$Colour &lt;- rbinom(length(ColourLink), 1, invlogit(ColourLink))

  # save data (to current work directory)
  write.csv(DataM, file = ""BeetlesMale.csv"", row.names = F)


  # 4. Poisson response: fecundity (females only)
  #---------------------------------------------------

  # Subset the design matrix (only females express colour morphs)
  DataF &lt;- Data[Data$Sex == ""Female"", ]

  # random effects
  PopulationE &lt;- rnorm(12, 0, sqrt(0.4))
  ContainerE &lt;- rnorm(120, 0, sqrt(0.05))

  # generation of response values on link scale (!) based on fixed effects, random effects and residual errors
  EggLink &lt;- with(DataF, 1.1 + 0.5 * (as.numeric(Treatment) - 1) + 0.1 *   (as.numeric(Condition) - 1) + PopulationE[Population] + ContainerE[Container] +   rnorm(480, 
                                                                                                                                                         0, sqrt(0.1)))

  # data generation (on data scale!) based on Poisson distribution
  DataF$Egg &lt;- rpois(length(EggLink), exp(EggLink))

  # save data (to current work directory)
  write.csv(DataF, file = ""BeetlesFemale.csv"", row.names = F)
</code></pre>
"
"0.114582297256771","0.117714964779443","110573","<p>Let's say we have this: </p>

<pre><code> model2 &lt;- lmer(milk.amount~(1|cow), data=milk, REML=FALSE)
 model1 &lt;- lmer(milk.amount~(1|cow), data=milk)
 summary(model2)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: milk.amount ~ (1 | cow)
    Data: milk
     AIC      BIC   logLik deviance df.resid 
   186.5    191.6    -90.2    180.5       37 

Scaled residuals: 
     Min      1Q  Median      3Q     Max 
 -2.0244 -0.4104  0.1795  0.6621  1.3879 

Random effects:
 Groups   Name        Variance Std.Dev.
 cow      (Intercept) 6.755    2.599   
 Residual             2.999    1.732   
 Number of obs: 40, groups: cow, 10

Fixed effects:
             Estimate Std. Error t value
 (Intercept)  27.0150     0.8663   31.18
</code></pre>

<p>then</p>

<pre><code>summary(model1)
Linear mixed model fit by REML ['lmerMod']

Formula: milk.amount ~ (1 | cow)
   Data: milk
 REML criterion at convergence: 178.9

 Scaled residuals: 
      Min      1Q  Median      3Q     Max 
  -1.9981 -0.4136  0.1775  0.6561  1.4021 

 Random effects:
  Groups   Name        Variance Std.Dev.
  cow      (Intercept) 7.589    2.755   
  Residual             3.000    1.732   
  Number of obs: 40, groups: cow, 10

 Fixed effects:
              Estimate Std. Error t value
  (Intercept)  27.0150     0.9132   29.58
</code></pre>

<p>Why model1 (with REML) doesn't show AIC, BIC, logLik, deviance coefficients? Is it possibly due to some kind of software dependency?</p>
"
"0.175027350160361","0.166968823211932","111535","<p>This is my first post, so sorry if it not optimally written. I have a paired samples at two time points in two groups, undergoing the same intervention. I want to test the effect of my intervention on weight.</p>

<p>I'm using R. Here's some data to illustrate: my data frame called <code>df</code>:</p>

<pre><code>      patients   timepoint        group          Weight
        102            1            1            107.30
        104            1            1             94.10
        117            1            1            110.80
        121            1            1            108.90
        153            1            1             95.40
        155            1            1            105.10
        161            1            1             97.70
        162            1            1             83.60
        167            1            1             82.40
        173            1            1             86.40
        176            1            1             81.90
        177            1            1             90.90
        179            1            1             95.30
        101            1            2             81.30
        108            1            2             72.30
        113            1            2             68.50
        170            1            2             89.20
        171            1            2             77.50
        172            1            2             94.50
        175            1            2             78.30
        181            1            2             71.40
        182            1            2             72.80
        183            1            2             73.50
        186            1            2             87.90
        187            1            2             83.50
        188            1            2             70.10
        102            2            1            102.70
        104            2            1             90.40
        117            2            1            107.30
        121            2            1            107.50
        153            2            1             95.00
        155            2            1            102.80
        161            2            1             95.40
        162            2            1             78.30
        167            2            1             81.90
        173            2            1             85.30
        176            2            1             83.10
        177            2            1             90.50
        179            2            1             97.50
        101            2            2             78.40
        108            2            2             72.00
        113            2            2             66.80
        170            2            2             90.20
        171            2            2             77.60
        172            2            2             93.40
        175            2            2             80.30
        181            2            2             72.60
        182            2            2             71.40
        183            2            2             74.20
        186            2            2             88.70
        187            2            2             80.50
        188            2            2             71.20
</code></pre>

<p>Since this is paired data (between time points) and unpaired (between groups), I guess I must use a mixed linear model. Im going for the lme4 package in R.</p>

<p>""timepoints"" and ""group"" will be my fixed effects (I exhaust both). ""patients"" will be my random effect, which also picks up that I have multiple responses per patient. </p>

<p>I will use a random intercept model, but I actually expect that my patients differ with how they react to my experimental manipulation, so a random slopes model would be nice also. However, it seams I over-parametrize the model if doing so.</p>

<p>I will use the likelighood ratio using anova() for a full model and a reduced model.</p>

<pre><code>full = lmer(Weight ~ timepoint + group + (1|patients), data=df,
        REML=FALSE)

reduced = lmer(Weight ~ group + (1|patients), data=df,
                   REML=FALSE)
</code></pre>

<p>""timepoint"" is the main factor in question. I now test using anova():</p>

<pre><code>&gt; anova(reduced,full)
Data: df
Models:
reduced: Weight ~ group + (1 | patients)
full: Weight ~ timepoint + group + (1 | patients)
        Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
reduced  4 309.72 317.52 -150.86   301.72                          
full     5 306.02 315.78 -148.01   296.02 5.695      1    0.01701 *
</code></pre>

<p>Question is, Im I doing it correctly? And how do I interpret the result?</p>

<p>BUT, what I really want is to test if the effect of time is different on the two groups. How do I do this?</p>

<p>Thank you.</p>
"
"0.181170519432929","0.186123701694769","111569","<p>I have an experiment with a design in which subjects answer four items that are of four different types based on two factors (lets call the factors letter: ""a"" X ""b"" and big: ""A"" X ""a"", resulting in four types of questions A, a, B, b). The order of items (called here 1-4) is held constant and each subject answers one item of each type. The types are randomized. A subject can for example get question-type combinations: 1-a, 2-B, 3-b, 4-A; or 1-B, 2-b, 3-a, 4-A; etc.</p>

<p>I am interested in effects of question types, but expect that the random effects may play a role as well. I tried to use the following model:</p>

<pre><code>glmer(answer ~ (1|subject) + (big*letter|item) + big*letter, data = data, family = binomial(link = ""logit""))  
</code></pre>

<p>When I compare this model with one without random slopes:</p>

<pre><code>glmer(answer ~ (1|subject) + (1|item) + big*letter, data = data, family = binomial(link = ""logit""))
</code></pre>

<p>... the first model is not better in any way than the second:</p>

<pre><code>   Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)
m2  6 1242.1 1272.1 -615.04   1230.1                        
m1 15 1261.2 1336.2 -615.60   1231.2     0      9          1
</code></pre>

<p>So, my first question is whether the model is specified correctly given the design I have. The second question would be, why is it that including random slopes does not improve the model, even though it is possible to see from the data, that the effect of question type obviously differs between the items.</p>

<p>Edit:
Summary table for m1:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: answer ~ (1 | subject) + (big * letter | item) + big * letter 
   Data: data 

      AIC       BIC    logLik  deviance 
1261.2010 1336.2061 -615.6005 1231.2010 

Random effects:
 Groups  Name               Variance Std.Dev. Corr             
 subject (Intercept)        0.71862  0.8477                    
 item    (Intercept)        0.00000  0.0000                    
         bigTRUE            0.04241  0.2059     NaN            
         letterTRUE         0.10219  0.3197     NaN  1.00      
         bigTRUE:letterTRUE 0.05749  0.2398     NaN -1.00 -1.00
Number of obs: 1097, groups: subject, 275; item, 4

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          1.8297     0.1798  10.176  &lt; 2e-16 ***
bigTRUE             -0.9339     0.2413  -3.870 0.000109 ***
letterTRUE          -0.7073     0.2734  -2.587 0.009679 ** 
bigTRUE:letterTRUE   0.7458     0.3159   2.361 0.018212 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) bgTRUE ltTRUE
bigTRUE     -0.683              
letterTRUE  -0.602  0.698       
bgTRUE:TRUE  0.521 -0.786 -0.792
</code></pre>
"
"0.147925109681887","0.151969366063391","111836","<p>In R, using package lme4, I have used the following 2 mixed models to determine I have a signifacnt interaction between a covariate (continous, normally distributed) and a factor (three levels: herbivores, plants, predators):</p>

<pre><code>test1 &lt;- lmer( mode ~ sr * func.group + (1|community), data=nr.test, REML=""FALSE"")
test2 &lt; -lmer( mode ~ sr + func.group + (1|community), data=nr.test, REML=""FALSE"")
anova(test1, test2)

Data: nr.test
Models:
test2: mode ~ sr + func.group + (1 | community)
test1: mode ~ sr * func.group + (1 | community)
     Df    AIC    BIC  logLik  Chisq Chi Df Pr(&gt;Chisq)    
nr.test2  6 77.458 82.093 -32.729                             
nr.test1  8 62.570 68.751 -23.285 18.887      2  7.919e-05 ***
</code></pre>

<p>I have obviously plotted the interaction for each level of factor, and all three levels are positive relationships which cross over each other (i.e. different levels of slope steepness).</p>

<p>However, I would like to be able to identify which level of the factor are significantly different from one another, and which are also significantly different from a slope value of zero (i.e. which slopes are significant). </p>

<p>I have installed the R package phia, and used the command </p>

<pre><code>testInteractions(test1, pairwise=""func.group"", slope=""sr"")
</code></pre>

<p>and recieved the following output:</p>

<pre><code>Adjusted slope for sr 
Chisq Test: 
P-value adjustment method: holm
                     Value Df   Chisq Pr(&gt; Chisq)    
    herbivore-plant 0.11878  1  4.3061     0.03798 *  
 herbivore-predator 0.40567  1 65.3283   1.902e-15 ***
     plant-predator 0.28689  1 30.3474   7.224e-08 ***
</code></pre>

<p>It would appear for each level of factor the slopes are significantly different from one another, but how can I know which ones are significantly different from zero, and how would I report this?</p>
"
"0.104598848163826","0.107458569276045","111915","<p>I just fitted the following linear mixed effects model:</p>

<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
 Formula: price ~ variable + (1 | product)
    Data: podzbior

       AIC       BIC    logLik  deviance  df.resid 
 130840.14 130868.85 -65416.07 130832.14      9674 

Scaled residuals: 
     Min      1Q  Median      3Q     Max 
 -6.2824 -0.3099 -0.0547  0.2201 12.4291 

Random effects:
 Groups           Name     Variance Std.Dev.
 product         (Intercept) 427375   653.7   
 Residual                     25930   161.0   
 Number of obs: 9678, groups: product, 1222

Fixed effects:
                  Estimate Std. Error  t value
 (Intercept)     9.362e+02  1.899e+01    49.29
  variable      -7.521e-04  1.171e-04    -6.42

Correlation of Fixed Effects:
              (Intr)
  variable    -0.050
</code></pre>

<p>That was output from <code>summary(lmerModel)</code>, after the run of <code>lmer</code> I got this warning:</p>

<pre><code>Warning:
  In checkScaleX(X, ctrl = control) :
  Some predictor variables are on very different scales: consider rescaling
</code></pre>

<p>Q1 Predictor variable is numeric from 0 to something like 100k, how It should be scaled? </p>

<p>Random effects with confidence intervals chart for this model looks like this, is it OK?:</p>

<p><img src=""http://i.stack.imgur.com/wR3Lp.png"" alt=""""></p>

<p>I am pretty sure residuals are not OK. What should I do in this case?</p>

<p><img src=""http://i.stack.imgur.com/8hzOE.png"" alt=""""></p>

<p>How can I go deeper with this model diagnostic, besides checking p-values?</p>
"
"0.140334080917496","0.144170799399664","113322","<p>I have noticed that if there are interactions between hidden variables not in the model, then the variance estimates are inflated greatly compared to the predictive power of the model itself, and I'm trying to understand why this is.  Here is an illustrative example:</p>

<pre><code>library(lme4)
library(lmerTest)

x1 &lt;- sample(c(0,1),1000,replace=T)
x2 &lt;- sample(c(0,1),1000,replace=T)
y &lt;- x1*x2 + rnorm(1000,sd=0.1)
</code></pre>

<p>Now I execute lmer</p>

<pre><code>&gt; lmer(y~0+(1|x1))
Linear mixed model fit by maximum likelihood  ['merModLmerTest']
Formula: y ~ 0 + (1 | x1)
      AIC       BIC    logLik  deviance  df.resid 
 825.2395  835.0550 -410.6198  821.2395       998 
Random effects:
 Groups   Name        Std.Dev.
 x1       (Intercept) 0.3731  
 Residual             0.3626  
Number of obs: 1000, groups:  x1, 2
No fixed effect coefficients 
</code></pre>

<p>So the model says that x1 explains roughly 50% of the variance components</p>

<pre><code>&gt; my_model &lt;- lmer(y~0+(1|x1))
&gt; var_expl &lt;- as.data.frame(VarCorr(my_model))
&gt; var_expl &lt;- (var_expl[var_expl$grp == 'x1',4])/(var_expl[var_expl$grp == 'x1',4]+sum(var_expl[var_expl$grp != 'x1',4]))
&gt; var_expl
[1] 0.5143418
</code></pre>

<p>However, the model built using x1 has only ~35% linear variance explained in reality</p>

<pre><code>&gt; cor(predict(my_model,as.data.frame(x1)),y)^2
[1] 0.3464968
</code></pre>

<p>So if I were to interpret this as an experiment, I would say that there is a hidden confound x2 that affects the results of x1 directly... but I know that x1 and x2 are uncorrelated.  Again, trying to interpret this practically - there is a hidden factor that happens randomly if I do the experiment but I have not noticed it.  (Note, adding a main effect to x1 and an interaction term gives very comparable results)</p>

<p>Does this mean that interpreting the variance coefficients in a case with hidden confounds does not tell you anything at all about the predictive power of x1 in this case as a variable?  I would like to know if there's a way to correct for such hidden factors and when it is okay to interpret the variance components as percent variance explained (assuming that I only have one variable to look at, but potential confounders that I may not be able to know explicitly)</p>
"
"0.140334080917496","0.12815182168859","113682","<p>According to this document: <a href=""http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf"" rel=""nofollow"">http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf</a> </p>

<p>I can get the overall p-value of fix effects by comparing models that I would like to know to null model.</p>

<p>In my case I would like to know the effect of three-way interaction. 
So first, I run the null model:</p>

<pre><code>lmer86 &lt;- lmer(IntensityMax ~ (1|item) + (1+vowel3|speaker) + sex + vowel3 + Language, data=data1.frame, REML=FALSE, na.action=na.omit)
</code></pre>

<p>Then I run the model that I would like to test:</p>

<pre><code>lmer81 &lt;- lmer(IntensityMax ~ (1|item) + (1+vowel3|speaker) + sex*vowel3*Language, data=data1.frame, REML=FALSE, na.action=na.omit)
</code></pre>

<p>And finally, I compare these models using ANOVA function:</p>

<pre><code>anova(lmer81,lmer86)
</code></pre>

<p>And this is result:</p>

<pre><code>Data: data1.frame
Models:
lmer86: IntensityMax ~ (1 | item) + (1 + vowel3 | speaker) + sex + vowel3 + 
lmer86:     Language
lmer81: IntensityMax ~ (1 | item) + (1 + vowel3 | speaker) + sex * vowel3 * 
lmer81:     Language
       Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
lmer86 14 13349 13432 -6660.5    13321                             
lmer81 26 13319 13474 -6633.6    13267 53.813     12  2.951e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Then I know that three-way interaction is an important effect. Am I on the right track?</p>
"
"0.114582297256771","0.117714964779443","114678","<p>Can anyone tell us how to evaluate the fit of our generalized linear model with a poisson distribution? We can't really tell if the model is a good fit or not. Do you use the deviance to answer this question? If so, what does it tell us in the following example? </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: poisson  ( log )
Formula: vok ~ factor(koen) + (1 | group) + factor(obs) + rid + aggr +  
    offset(log(min))
   Data: data

     AIC      BIC   logLik deviance df.resid 
   156.1    172.8    -70.0    140.1       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5286 -0.6338 -0.3348  0.5913  4.8183 

Random effects:
 Groups Name        Variance Std.Dev.
 group  (Intercept) 0        0       
Number of obs: 60, groups:  group, 60

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -5.40345    0.37230 -14.514  &lt; 2e-16 ***
factor(koen)1  1.13549    0.38823   2.925  0.00345 ** 
factor(obs)2   0.84057    0.51918   1.619  0.10544    
factor(obs)3   0.55973    0.24933   2.245  0.02477 *  
factor(obs)4  -1.24449    0.55967  -2.224  0.02617 *  
rid            0.10088    0.01939   5.203 1.96e-07 ***
aggr           0.05890    0.02868   2.053  0.04003 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) fct()1 fct()2 fct()3 fct()4 rid   
factor(kn)1 -0.705                                   
factor(bs)2 -0.275 -0.107                            
factor(bs)3 -0.342 -0.065  0.302                     
factor(bs)4 -0.206  0.005  0.157  0.355              
rid         -0.106 -0.343  0.304  0.304  0.248       
aggr        -0.106 -0.129  0.103 -0.313 -0.241 -0.287
</code></pre>
"
"0.162043838788391","0.166474099685359","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.0661541201655622","0.0679627666030585","116935","<p>say we have to glmms</p>

<pre><code>mod1 &lt;- glmer(y ~ x + A + (1|g), data = dat)
mod2 &lt;- glmer(y ~ x + B + (1|g), data = dat)
</code></pre>

<p>These models are not nested in the usual sense of:</p>

<pre><code>a &lt;- glmer(y ~ x + A + (1|g),     data = dat)
b &lt;- glmer(y ~ x + A + B + (1|g), data = dat)
</code></pre>

<p>so we can't do <code>anova(mod1, mod2)</code> as we would with <code>anova(a ,b)</code></p>

<p>Can we use an AIC approach to say which is the best model instead?</p>
"
"0.183740442396139","0.188763886071787","118172","<p>I fitted a mixed logit model with crossed random effects in <code>lme4_1.1-7::glmer</code> (R version 3.1.1 / OS X 10.9.4 Mavericks).</p>

<p>Had to simplify the maximal random-effect structure justified by the design due to failed convergence; the final model is estimated without any problems:</p>

<pre><code>fitted_1 &lt;- glmer(DV ~ IV1.d*IV2.d + (IV1.d*IV2.d| SubjN) + (1|Items) +  
                       (0+IV1.d|Items) + (0+IV2.d|Items) + (0+IV1.d:IV2.d|Items), 
                  glmerControl(optimizer='bobyqa', optCtrl = list(maxfun=20000)), 
                  data=myPP, family=binomial) 
</code></pre>

<p><code>DV</code> is the binary response variable</p>

<p><code>IV1.d</code> and <code>IV2.d</code> are two within-subjects within-items categorical predictors, two levels each, deviation-contrast coded (values: -.5/.5)</p>

<p>I tried to compute confidence intervals for the beta parameters using profile likelihood via <code>confint.merMod()</code> but the computation seems to be failing.
For all betas, I got values <code>(-Inf Inf)</code> and warning messages of non-monotonic profiles. Reading on [R-sig-ME], this latter issue should mean there is something wonky with the profile. </p>

<p>I tried to simplify the random structure of the model until profile confidence intervals could be computed. Here is the model:</p>

<pre><code>fitted_4 &lt;- glmer(DV ~ IV1.d + IV2.d + IV1.d:IV2.d + (IV1.d + IV2.d| SubjN) +
                       (1|Items) + (0+IV1.d|Items) + (0 +IV2.d|Items), data=myPP, 
                  glmerControl(optimizer='bobyqa', optCtrl = list(maxfun=20000)),
                  family=binomial)
</code></pre>

<ol>
<li><p>I'm not understanding what causes the profile likelihood method to fail for the original <code>fitted_1</code> model but not for <code>fitted_4</code>. </p></li>
<li><p>Is there any other way I could obtain profile CI's for <code>fitted_1</code>?</p></li>
</ol>

<hr>

<pre><code>summary(fitted_1)

##       AIC      BIC   logLik deviance df.resid 
##    1074.0   1168.1   -519.0   1038.0     1362 

##  Scaled residuals: 
##      Min      1Q  Median      3Q     Max 
##  -2.2673 -0.3611 -0.2500 -0.1378  4.5826 

##  Random effects:
##   Groups  Name        Variance  Std.Dev.  Corr             
##   SubjN   (Intercept) 2.424e+00 1.557e+00                  
##           IV1.d       1.990e+00 1.411e+00  0.17            
##           IV2.d       6.065e-01 7.788e-01 -0.97 -0.29      
##           IV1.d:IV2.d 2.172e+00 1.474e+00 -0.19 -0.81  0.39
##   Items   (Intercept) 4.615e-03 6.793e-02                  
##   Items.1 IV1.d       3.233e-13 5.686e-07                  
##   Items.2 IV2.d       9.442e-01 9.717e-01                  
##   Items.3 IV1.d:IV2.d 4.801e-01 6.929e-01                  
##  Number of obs: 1380, groups:  SubjN, 88; Items, 12

##  Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
##  (Intercept) -2.40604    0.23196 -10.373   &lt;2e-16 ***
##  IV1.d        0.08249    0.36355   0.227   0.8205    
##  IV2.d        1.11046    0.43579   2.548   0.0108 *  
##  IV1.d:IV2.d  0.16386    0.71246   0.230   0.8181    

##  Correlation of Fixed Effects:
##              (Intr) IV1.d  IV2.d 
##  IV1.d        0.118              
##  IV2.d       -0.434 -0.083       
##  IV1.d:IV2.d -0.090 -0.628  0.064
</code></pre>

<p></p>

<pre><code>confint(fitted_1, method=""profile"", which='beta_')`

##               2.5 % 97.5 %
##   (Intercept)  -Inf    Inf
##   IV1.d        -Inf    Inf
##   IV2.d        -Inf    Inf
##   IV1.d:IV2.d  -Inf    Inf

##  Warning messages:
##  1: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  2: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  3: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  4: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
</code></pre>

<p></p>

<pre><code>summary(fitted_4)

##       AIC      BIC   logLik deviance df.resid 
##      1068     1136     -521     1042     1367 

##  Scaled residuals: 
##      Min      1Q  Median      3Q     Max 
##  -2.3575 -0.3555 -0.2522 -0.1613  4.6391 

##  Random effects:
##   Groups  Name        Variance Std.Dev. Corr       
##   SubjN   (Intercept) 2.23144  1.4938              
##           IV1.d       1.53606  1.2394    0.09      
##           IV2.d       0.31120  0.5579   -1.00 -0.18
##   Items   (Intercept) 0.01344  0.1159              
##   Items.1 IV1.d       0.00000  0.0000              
##   Items.2 IV2.d       0.92942  0.9641              
##  Number of obs: 1380, groups:  SubjN, 88; Items, 12

##  Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
##  (Intercept) -2.30252    0.21029 -10.949   &lt;2e-16 ***
##  IV1.d        0.17448    0.27729   0.629   0.5292    
##  IV2.d        0.80072    0.36862   2.172   0.0298 *  
##  IV1.d:IV2.d  0.01351    0.41660   0.032   0.9741    

##  Correlation of Fixed Effects:
##              (Intr) IV1.d  IV2.d 
##  IV1.d        0.012              
##  IV2.d       -0.274 -0.010       
##  IV1.d:IV2.d  0.006 -0.255 -0.038
</code></pre>

<p></p>

<pre><code>confint(fitted_4, which='beta_', method='profile')

##                    2.5 %     97.5 %
##  (Intercept) -2.74571641 -1.9052351
##  IV1.d       -0.37989551  0.7320931
##  IV2.d        0.03993436  1.5903197
##  IV1.d:IV2.d -0.80790153  0.8346440
</code></pre>

<hr>

<h2>UPDATE</h2>

<pre><code>## re-compute profiles for both random and fixed effects

pp &lt;- profile(fitted_1)

## 24 warnings with profile(fitted_1) of the types:
## In profile.merMod(fitted_1) : non-monotonic profile
## In optwrap(optimizer, par = start, fn = function(x) dd(mkpar(npar1,  ... :
   # convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded

(c_ci &lt;- confint(pp))

## 2.5 % 97.5 %
## .sig01          0    Inf
## .sig02         -1      1
## .sig03         -1      1
## .sig04         -1      1
## .sig05          0    Inf
## .sig06         -1      1
## .sig07         -1      1
## .sig08          0    Inf
## .sig09         -1      1
## .sig10          0    Inf
## .sig11          0    Inf
## .sig12          0    Inf
## .sig13          0    Inf
## .sig14          0    Inf
## (Intercept)  -Inf    Inf
## IV1.d        -Inf    Inf
## IV2.d        -Inf    Inf
## IV1.d:IV2.d  -Inf    Inf


## plot the profiles (all weird)
    ggplot(as.data.frame(pp),aes(.focal,.zeta))+
    geom_point()+geom_line()+
    facet_wrap(~.par,scale=""free_x"")+
    geom_hline(yintercept=0,colour=""gray"")+
    geom_hline(yintercept=c(-1.96,1.96),linetype=2,
               colour=""gray"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/IUtJx.jpg"" alt=""Plot_profile_fitted1.jpeg""></p>

<pre><code>### setting delta to a smaller value to make the profile stepsize smaller
system.time(pp2 &lt;- profile(fitted_1, delta = 0.1))

## user    system   elapsed 
## 64292.282   135.451 75676.403

## Warning messages:
## 1: In profile.merMod(orig.pp, delta = 0.1) : non-monotonic profile
## 2: display list redraw incomplete
## [...]

c_ci2 &lt;- confint(pp2)
c_ci2

## 2.5 % 97.5 %
## .sig01          0    Inf
## .sig02         -1      1
## .sig03         -1      1
## .sig04         -1      1
## .sig05          0    Inf
## .sig06         -1      1
## .sig07         -1      1
## .sig08          0    Inf
## .sig09         -1      1
## .sig10          0    Inf
## .sig11          0    Inf
## .sig12          0    Inf
## .sig13          0    Inf
## .sig14          0    Inf
## (Intercept)  -Inf    Inf
## IV1.d        -Inf    Inf
## IV2.d        -Inf    Inf
## IV1.d:IV2.d  -Inf    Inf


## plot of profiles (delta = 0.1)

ggplot(as.data.frame(pp2),aes(.focal,.zeta))+
    geom_point()+geom_line()+
    facet_wrap(~.par,scale=""free_x"")+
    geom_hline(yintercept=0,colour=""gray"")+
    geom_hline(yintercept=c(-1.96,1.96),linetype=2,
               colour=""gray"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/olzm2.jpg"" alt=""Plot of profiles delta = 0.1""></p>
"
"0.098728740664211","0.159386815778093","120768","<p>I'm using <code>glmer()</code> with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables ""flow"" and ""DNA"" are important but their p values in the model are well above 0.05?</p>

<p>Optimal model:</p>

<pre><code>a25 &lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,
             family=binomial, data=spore)
summary(a25)

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']  
Family: binomial  ( logit ) 
Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   
Data: spore
      AIC      BIC   logLik deviance df.resid 
     72.9     81.0    -32.4     64.9       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9318 -0.8163  0.4435  0.6848  1.6133 

Random effects:  
  Groups Name        Variance Std.Dev.  
  Root   (Intercept) 0.3842   0.6199   
  Number of obs: 56, groups:  Root, 9

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.97752    0.79252  -1.233    0.217   
Flow         3.82779    2.27165   1.685    0.092 . 
DNA          0.01616    0.01039   1.556    0.120  
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr) Flow   Flow -0.775        
     DNA    -0.576  0.227
</code></pre>

<p>Likelihood ratio test:</p>

<pre><code>a26 &lt;- update(a25,~.-DNA)
anova(a25,a26)

Data: spore 
Models: 
    a26: Status_qpcr ~ (1 | Root) + Flow 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
a26  3 74.802 80.878 -34.401   68.802                            
a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *

a27 &lt;- update(a25,~.-Flow)
anova(a25,a27)

Data: spore 
Models: 
    a27: Status_qpcr ~ (1 | Root) + DNA 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
a27  3 78.440 84.723 -36.220   72.440                             
a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **
</code></pre>
"
"NaN","NaN","121661","<p>I've got two models (all variable are count variables):</p>

<pre><code>frm.ct &lt;- glmer(frm ~ age + education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)

frm.oage &lt;- glmer(frm ~ education + socialrole +
              offset(log(words)) + (1|subkorpus), family=negative.binomial(1), 
data=daten.alle.kom)
</code></pre>

<p>I used this to compare them:</p>

<pre><code>anova(frm.ct, frm.oage)
</code></pre>

<p><img src=""http://i.stack.imgur.com/xb6Jm.png"" alt=""enter image description here""></p>

<p>AIC values tell me that <code>frm.oage</code> is the better model, right? but what do 0.0452 and 0.8315 mean?</p>
"
"0.272760425012416","0.239009185120432","122026","<p>I have a question regarding re-leveling in lme4 1.1-7. </p>

<p><strong>Experimental Design:</strong></p>

<p>Our experiment is an eyetracking while reading study (single sentence stimuli). We are analyzing four different continuous eyetracking DVs  over three different regions of interest.For all DVs, we first removed outliers, then took the log value, then residualized the result (subtracted the actual reading time from a predicted reading time per character - to control for word length differences).</p>

<p>The main manipulation is the categorical factor â€œconditionâ€ which has four levels. Condition is a within-subject (repeated measures) factor that represents four different versions of a single sentential item.</p>

<p>We also have a continuous predictor (Ospan) which is between-subject and is centered. I'm leaving that out of this question though since it does not seem to be related to my problem.</p>

<p>The experimental materials were distributed in a latin square rotation over four presentation lists. This ensured that each particular subject only saw one level of condition for each sentential item, but that each subject would also see an equal number of items representing each level of condition (thus this is a repeated measures design). There are 80 sentence items (each with four levels of condition). There were 45 participants across the four lists (somewhat unbalanced).</p>

<p><strong>My problem (well, one of them):</strong></p>

<p>Working with this model (which omits Ospan) on one eyetracking DV in one region (R02) of the sentence:</p>

<pre><code>(testFirstR02_lmer03 = lmer(RT2LogR ~ condition + (1 + condition | Subject) + (1 + condition | item), data = testFirst[testFirst$Region == ""R02"",],REML = FALSE))
</code></pre>

<p>I obtain convergence when I set the reference level (of condition) to StrongIs or RCE, but the model does not converge when I re-level to a reference level of PseudoC. It produces the following error messages:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.71338 (tol = 0.002, component 6)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>

<p>This is the releveling code that I am using (with variants in accordance to what I am setting the reference level to):</p>

<pre><code>testFirst$condition = factor(testFirst$condition,levels=c(""StrongIs"",""RCE"",""PseudoC"",""NonIs""))
</code></pre>

<p>If I remove the random slope specification of condition for the Item fixed effect, I can get convergence no matter how I set the reference level. </p>

<p>And if I remove only the intercept/slope interaction term for that fixed effect...</p>

<pre><code>(testFirstR02_lmer03 = lmer(RT2LogR ~ condition + (1 + condition | Subject) + (1 | item) + (0 + condition | item), data = testFirst[testFirst$Region == ""R02"",],REML = FALSE))
</code></pre>

<p>... then I get the following:</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p><strong>My Questions:</strong></p>

<p>1) Shouldn't the data behave similarly despite any re-leveling? I know that one can compute contrasts by hand without even using re-level - so I find this error message a bit confusing</p>

<p>2) I'm not sure how to interpret the final warning messages? What I would ""rescale"" in my variables - they are already scaled.</p>

<p>3) Does this behavior signal something inherently unstable in my data? The releveling problem does seem to only happen when I set the reference to PseudoC. I tried adding <code>complete.cases(testFirst$RT2LogR) &amp;</code> to my data specification, thinking that something was going wrong with NAs, but it did not help.</p>

<p>I have not attached any reproducible data, as it is a large data set. I can provide a link if necessary. Any help would be greatly appreciated.</p>

<p><strong>Edit (summary and singularity tests output)</strong></p>

<pre><code>&gt; #Change ref level
&gt; testFirst$condition = factor(testFirst$condition,levels=c(""PseudoC"",""NonIs"",""StrongIs"",""RCE""))
&gt; contrasts(testFirst$condition)
         NonIs StrongIs RCE
PseudoC      0        0   0
NonIs        1        0   0
StrongIs     0        1   0
RCE          0        0   1


&gt; #Model 3 Include condition slope for item

Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.71338 (tol = 0.002, component 6)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; summary(testFirstR02_lmer03.1)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT2LogR ~ condition + (1 + condition | Subject) + (1 + condition |      item)
   Data: testFirst[testFirst$Region == ""R02"", ]

     AIC      BIC   logLik deviance df.resid 
  2989.1   3159.3  -1469.5   2939.1     6670 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.6285 -0.5193  0.0227  0.5468  4.4020 

Random effects:
 Groups   Name              Variance Std.Dev. Corr             
 item     (Intercept)       0.000000 0.0000                    
          conditionNonIs    0.010466 0.1023    NaN             
          conditionStrongIs 0.010966 0.1047    NaN  0.17       
          conditionRCE      0.011935 0.1092    NaN  0.59  0.20 
 Subject  (Intercept)       0.014283 0.1195                    
          conditionNonIs    0.005491 0.0741   -0.61            
          conditionStrongIs 0.013175 0.1148   -0.47  0.75      
          conditionRCE      0.012684 0.1126   -0.55  0.75  0.75
 Residual                   0.083755 0.2894                    
Number of obs: 6695, groups:  item, 80; Subject, 45

Fixed effects:
                   Estimate Std. Error t value
(Intercept)       -0.444822   0.019226 -23.137
conditionNonIs    -0.014357   0.018888  -0.760
conditionStrongIs  0.043049   0.023169   1.858
conditionRCE      -0.002206   0.023181  -0.095

Correlation of Fixed Effects:
            (Intr) cndtNI cndtSI
conditnNnIs -0.476              
cndtnStrngI -0.442  0.496       
conditinRCE -0.489  0.626  0.553


&gt; #Model 3 Exclude condition slope for item

&gt; summary(testFirstR02_lmer03)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT2LogR ~ condition + (1 + condition | Subject) + (1 | item)
   Data: testFirst[testFirst$Region == ""R02"", ]

     AIC      BIC   logLik deviance df.resid 
  3091.7   3200.7  -1529.9   3059.7     6679 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.9890 -0.5127  0.0184  0.5609  4.5757 

Random effects:
 Groups   Name              Variance Std.Dev. Corr             
 item     (Intercept)       0.004827 0.06947                   
 Subject  (Intercept)       0.014292 0.11955                   
          conditionNonIs    0.004862 0.06973  -0.60            
          conditionStrongIs 0.012930 0.11371  -0.45  0.73      
          conditionRCE      0.012017 0.10962  -0.55  0.74  0.73
 Residual                   0.087334 0.29552                   
Number of obs: 6695, groups:  item, 80; Subject, 45

Fixed effects:
                    Estimate Std. Error t value
(Intercept)       -4.476e-01  2.082e-02 -21.503
conditionNonIs    -1.105e-02  1.468e-02  -0.753
conditionStrongIs  4.611e-02  1.992e-02   2.316
conditionRCE       7.436e-05  1.939e-02   0.004

Correlation of Fixed Effects:
            (Intr) cndtNI cndtSI
conditnNnIs -0.544              
cndtnStrngI -0.465  0.628       
conditinRCE -0.535  0.638  0.671
&gt; 


#Singularity test for both models

&gt; tt &lt;- getME(testFirstR02_lmer03.1,""theta"")
&gt; ll &lt;- getME(testFirstR02_lmer03.1,""lower"")
&gt; min(tt[ll==0])
[1] 0

&gt; tt &lt;- getME(testFirstR02_lmer03,""theta"")
&gt; ll &lt;- getME(testFirstR02_lmer03,""lower"")
&gt; min(tt[ll==0])
[1] 0.1889075
</code></pre>
"
"0.181525408155863","0.198143811351516","122336","<p>I have a problem with coding of a 2-level categorical predictor variable in R, and subsequently using it as a random slope in lmer().</p>

<p>I can keep the factor as numeric, coded using the treatment coding:</p>

<pre><code>&gt; unique (b$multi)
[1] 0 1
</code></pre>

<p>Running lmer() using a dataset coded in this way yields:</p>

<pre><code>&gt; l1 = glmer(OK ~ multi + (0 + multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
&gt; summary(l1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4806.5   4838.9  -2398.3   4796.5     4792 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-7.8294 -0.5560 -0.1548  0.5623 14.3342 

Random effects:
 Groups  Name        Variance Std.Dev.
 subject (Intercept) 1.84379  1.3579  
 item    (Intercept) 2.40306  1.5502  
 item.1  multi       0.04145  0.2036  
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>Above there is only one random slope related to <code>multi</code>. However, something very different happens when I convert the variable into a factor:</p>

<pre><code>&gt; b$multi = as.factor(b$multi)
&gt; levels (b$multi)
[1] ""0"" ""1""
</code></pre>

<p>When I fit a model using <code>multi</code> as a random slope variable:</p>

<blockquote>
  <p>l2 = glmer(OK ~ multi + (0+multi|item) + (1|subject)+ (1|item), family=""binomial"", data=b)
      Warning message:
      In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
        Model failed to converge: degenerate  Hessian with 1 negative eigenvalues</p>
</blockquote>

<p>... the model fails to converge and I get a very different random effects structure:</p>

<pre><code>&gt; summary(l2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: OK ~ multi + (0 + multi | item) + (1 | subject) + (1 | item)
   Data: b

     AIC      BIC   logLik deviance df.resid 
  4807.8   4853.1  -2396.9   4793.8     4790 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-8.3636 -0.5608 -0.1540  0.5627 15.2515 

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 subject (Intercept) 1.8375   1.3555       
 item    (Intercept) 0.9659   0.9828       
 item.1  multi0      1.5973   1.2638       
         multi1      1.0224   1.0111   1.00
Number of obs: 4797, groups:  subject, 123; item, 39
[...]
</code></pre>

<p>The number of parameters in the model clearly change (reflected by the change in AIC, etc.), and I get two random slopes. </p>

<p>My question is which way of coding the categorical variable is better? Intuition tells me that it is the first one, but I have seen recommendations for both ways of coding in various tutorials and classes about running GLMMs in R and this is why it baffles me. Both types of the predictor variable work identically in ordinary regression using lm().</p>
"
"0.104598848163826","0.107458569276045","122419","<p>I have a dataset with count dependent variables such as <code>greetings</code> and ordinal independent variables like <code>education</code> and <code>social</code> etc., but <code>education</code> has some zero values (which are represented by <code>NA</code>). Now I'm doing a variable selection with <code>glmer</code>. my question is, is it necessary to make a new data set without the <code>NA</code>s (which means with fewer observations for the whole dataset) when I compare the models with and without <code>education</code>? because the AIC values are different from the old and new dataset.</p>

<p>old dataset with <code>education</code></p>

<pre><code>model1 &lt;- glmer(greetings ~ education + socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.old)####AIC 1119.0
</code></pre>

<p>without <code>education</code></p>

<pre><code>model2 &lt;- glmer(greetings ~ socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.old)####AIC 1182.0
</code></pre>

<p>new dataset with <code>education</code>:</p>

<pre><code>model3 &lt;- glmer(greetings ~ education + socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.new)###AIC 1119.0
</code></pre>

<p>without <code>education</code>:</p>

<pre><code>model4 &lt;- glmer(greetings ~ socialrole + countedmembers + topic +
                 offset(log(words)) + (1|people), family=negative.binomial(1), 
            data=dat.new)###AIC 1117.3
</code></pre>

<p>thanks for any help!</p>
"
"0.210039541565307","0.207150719756946","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.168660574814382","0.173271736553374","124944","<p>I am doing various analysis on a small sample. Basically, we have an experiment where 14 subjects (UID 1 ~ 14) used one of the 6 instruments (MID 1 ~ 6) on 3 occasions (Sequence 1 ~ 3). Each time an outcome score was registered (between 1 ~ 100). </p>

<p>The test was double blind. The subjects were told they are measuring 3 different conditions while in reality they were either measuring conditions A, B, A or B, A, B (randomly assigned to the machines and users). The objective was to see if <code>A</code> and <code>B</code> are different or not.</p>

<p>To see if there is any significant difference between the ratings for the conditions A and B, I tried to fit a simple, random intercept model using the nlme package in R. I tried:</p>

<pre><code>f.1 &lt;- lme(Score ~ Condition, random = ~1|UID, data)
</code></pre>

<p>However, for some reason <code>lme</code> fails to fit the model: it gives no error or warning but the variance of the fitted random effect is essentially zero:</p>

<pre><code>&gt; summary(f.1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC   logLik
  349.3259 356.0815 -170.663

Random effects:
 Formula: ~1 | UID
         (Intercept) Residual
StdDev: 0.0009303203 15.98295

Fixed effects: Score ~ Condition 
               Value Std.Error DF   t-value p-value
(Intercept) 77.47619  3.487766 27 22.213700  0.0000
ConditionA  -0.85714  4.932446 27 -0.173776  0.8633
 Correlation: 
           (Intr)
ConditionA -0.707

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.9704269 -0.4677603  0.2472873  0.7835730  1.4628682 

Number of Observations: 42
Number of Groups: 14
</code></pre>

<p>I tried doing the same thing using <code>lme4</code> and got the same results. The estimates for the intercept and the <code>Condition</code> factor is almost identical to a linear model if I use <code>lm</code>.</p>

<p>I am trying hard to understand what <code>lme</code> or <code>lmer</code> fail to estimate the random effect. I generated some data by simulation and both routines had no problem fitting the model so I doubt there is something wrong with the syntax of what I have used.</p>

<p>The data is here:</p>

<pre><code>   UID MID Seq Score Condition
1    1   1   1    90  B
2    1   1   2    85  A
3    1   1   3    75  B
4    2   4   1    75  A
5    2   4   2    95  B
6    2   4   3    85  A
7    3   6   1    60  A
8    3   6   2    82  B
9    3   6   3    85  A
10   4   3   1    60  A
11   4   3   2    70  B
12   4   3   3    75  A
13   5   2   1    85  B
14   5   2   2    85  A
15   5   2   3    85  B
16   6   5   1    90  B
17   6   5   2    95  A
18   6   5   3   100  B
19   7   2   1    90  B
20   7   2   2    70  A
21   7   2   3    50  B
22   8   1   1    70  B
23   8   1   2    75  A
24   8   1   3    80  B
25   9   3   1    90  A
26   9   3   2    30  B
27   9   3   3    90  A
28  10   6   1    50  A
29  10   6   2    85  B
30  10   6   3    92  A
31  11   4   1    50  A
32  11   4   2    85  B
33  11   4   3    92  A
34  12   5   1    65  B
35  12   5   2    50  A
36  12   5   3    90  B
37  13   4   1    65  A
38  13   4   2    70  B
39  13   4   3    80  A
40  14   2   1    60  B
41  14   2   2   100  A
42  14   2   3    80  B
</code></pre>
"
"0.256347215641554","0.271851066412234","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.123763026191503","0.127146693842964","128750","<p>I am running a glmer model and I want to determine the total variance. My data is for survival and it is coded as 0 and 1, where 1 represents that the individual survived and 0 represents that the individual died. My data represents offspring from a full factorial cross where some individuals are full sibs or half sibs. </p>

<p>When running a glmer model, and there is no residual variance in the summary output. I have read that the residual variance should be (Ï€^2)/3 for generalized linear mixed models with binomial data and logit link function (Nakagawa, S., Schielzeth, H. 2010. Repeatability for Gaussian and non-Gaussian data: a practical guide for biologists. Biol. Rev. 85:935-956.).</p>

<p>Is this true? Or is there a different way to calculate the residual variance for glmer?</p>

<p>Here is my model and output:</p>

<pre><code>model6 = glmer(X09.Nov~(1|Dam)+(1|Sire)+(1|Sire:Dam), family=binomial, data=data)
summary(model6) 

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation 
      [glmerMod]
 Family: binomial  ( logit )
Formula: X09.Nov ~ (1 | Dam) + (1 | Sire) + (1 | Sire:Dam)
   Data: data

    AIC      BIC   logLik deviance df.resid 
 1274.4   1295.3   -633.2   1266.4     1375 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2747  0.3366  0.3931  0.4664  1.1090 

Random effects:
Groups   Name        Variance  Std.Dev. 
Sire:Dam (Intercept) 3.853e-01 6.207e-01
Sire     (Intercept) 4.181e-02 2.045e-01
Dam      (Intercept) 6.036e-09 7.769e-05
Number of obs: 1379, groups:  Sire:Dam, 49; Sire, 7; Dam, 7
Fixed effects:
            Estimate Std. Error z value     Pr    
(Intercept)   1.6456     0.1419    11.6 &lt;2e-16 *
</code></pre>
"
"0.209197696327652","0.204171281624485","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.0661541201655622","0.0679627666030585","131272","<p>I have a simple question, understanding the basic usage of the <code>lme4</code> package. I am following the tutorial by Bodo  Winter (<a href=""http://www.bodowinter.com/tutorial/bw_LME_tutorial.pdf"" rel=""nofollow"">http://www.bodowinter.com/tutorial/bw_LME_tutorial.pdf</a>).</p>

<p>In this tutorial, Bodo calculates a random effects model using the two commands:</p>

<pre><code> library(lme4)
 politeness=read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
 politeness.model = lmer(frequency ~ attitude + (1|subject) + (1|scenario), 
                         data=politeness)
 summary(politeness.model)
</code></pre>

<p>However, his printout of the output includes the AIC and BIC values (page 8), which are not included in the current version of lme4 (1.1.7). Do you have any idea why this is the case? Although, one can compute the two values using the maximum likelyhood algorithm (by using the REML=False option), I am confused why they are no longer included in the default output.</p>

<p>Thanks in advance</p>
"
"0.123763026191503","0.127146693842964","135840","<p>I have a data set that I expect there to be some variability among individuals; therefore, I chose to include <code>ID</code> as a random effect in the <code>glmer</code> model. However, when I run the model I get the following warning: </p>

<pre><code>model.5 &lt;- glmer(R0A1 ~ Dist_MP + (1|ID), data=secondorder, family=binomial)

Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
</code></pre>

<p>If I remove the random effect then the warning doesn't appear; therefore, I would assume that there is not enough variability among individuals (<code>ID</code>) for a random effect to be needed. Would you remove the random effect and just run a glm model? Also, how does the <code>family=binomial</code> code model the 0's and 1's in a data set? Does it consider 1's as the event? </p>

<pre><code>Summary output from glmer model:

Generalized linear mixed model fit by maximum
  likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: R0A1 ~ Dist_MP + (1 | ID)
   Data: secondorder

     AIC      BIC   logLik deviance df.resid 
 39451.7  39476.5 -19722.8  39445.7    28693 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.0876 -1.0372  0.2758  0.9567  1.7543 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 0        0       
Number of obs: 28696, groups:  ID, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.679e-01  1.505e-02   11.16   &lt;2e-16 ***
Dist_MP     -1.559e-03  8.771e-05  -17.77   &lt;2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr)
Dist_MP -0.614
</code></pre>
"
"0.114582297256771","0.117714964779443","136087","<p>So, building a LMM with the <code>lmer</code> function in <code>lme4</code>, you get the variance explained by the variance component.</p>

<pre><code>summary(esoph) # data on esophageal cancer from 'datasets' package

     agegp          alcgp         tobgp        ncases         ncontrols    
  25-34:15   0-39g/day:23   0-9g/day:24   Min.   : 0.000   Min.   : 1.00  
  35-44:15   40-79    :23   10-19   :24   1st Qu.: 0.000   1st Qu.: 3.00  
  45-54:16   80-119   :21   20-29   :20   Median : 1.000   Median : 6.00  
  55-64:16   120+     :21   30+     :20   Mean   : 2.273   Mean   :11.08  
  65-74:15                                3rd Qu.: 4.000   3rd Qu.:14.00  
  75+  :11                                Max.   :17.000   Max.   :60.00

# Just a hypothetical mixed model
m1 &lt;- lmer(ncase~ncontrols+(1|tobgp)+(1|agegp), data=esoph, REML=F)
summary(m1)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: ncases ~ ncontrols + (1 | tobgp) + (1 | agegp)
   Data: esoph

     AIC      BIC   logLik deviance df.resid 
   404.0    416.4   -197.0    394.0       83 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0267 -0.4770 -0.0704  0.3030  5.8282 

Random effects:
 Groups   Name        Variance Std.Dev.
 agegp    (Intercept) 2.2809   1.5103  
 tobgp    (Intercept) 0.0293   0.1712  
 Residual             4.4311   2.1050  
Number of obs: 88, groups:  agegp, 6; tobgp, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  1.65533    0.69209   2.392
ncontrols    0.05022    0.01882   2.668
</code></pre>

<p>I get the standard deviation for each of my variance components. Let's say I want to compute variance explained by my fixed effects (code from <a href=""http://jonlefcheck.net/2013/03/13/r2-for-linear-mixed-effects-models/"" rel=""nofollow"">here</a>):  </p>

<pre><code>(VarF &lt;- var(as.vector(lme4::fixef(m1) %*% t(m1@pp$X))))
# 0.4082163
</code></pre>

<p>Great! That's useful information! How do I go about calculating the standard deviation for this variance estimate?</p>
"
"0.170180070146122","0.174832774721926","143843","<p>Iâ€™m running a logit mixed-effects model on binary data with a 2x2 within-subjects design, with subjects and items as crossed random effects, and the two independent variables deviation-contrast coded.</p>

<p>Here are model specification and summary:</p>

<pre><code>mod1 &lt;- glmer(DV ~ devX1*devX2 + (devX1*devX2|Subject) + (devX1*devX2|Item), 
              data=mydata, family=binomial, glmerControl(optimizer='bobyqa', 
              optCtrl=list(maxfun=400000)))

     AIC      BIC   logLik deviance df.resid 
   628.9    734.3   -290.4    580.9      573 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0527 -0.5025 -0.2217  0.5654  4.0493 

Random effects:
 Groups  Name        Variance Std.Dev. Corr             
 Subject (Intercept) 0.1184   0.3440                    
         devX1       3.5387   1.8812   -0.74            
         devX2       0.2461   0.4961   -0.54  0.06      
         devX1:devX2 4.5912   2.1427    0.32 -0.84  0.07
 Item    (Intercept) 0.5568   0.7462                    
         devX1       0.2693   0.5190    0.48            
         devX2       0.3862   0.6215   -0.31 -0.51      
         devX1:devX2 2.2109   1.4869   -0.57  0.42 -0.31
Number of obs: 597, groups:  Subject, 30; Item, 20

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.47781    0.27602  -5.354 8.60e-08 ***
devX1        2.70622    0.55692   4.859 1.18e-06 ***
devX2        0.08229    0.45801   0.180    0.857    
devX1:devX2 -0.41055    0.99645  -0.412    0.680    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) devX1  devX2 
devX1       -0.498              
devX2       -0.179  0.046       
devX1:devX2 -0.021 -0.266 -0.657
</code></pre>

<p>The model doesÂ converge with full random structure without any problems. (It may be worth mentioning that the binned Pearson residual plot reveals that the model has some issues accounting for y = 0 original data points.)  </p>

<p>I'm encounteringÂ big convergence issues as soon as I include a centered continuous covariate (<code>Age</code>)Â asÂ fixed effect. It does not matter how much I simplify the random structure, the model will not converge.</p>

<pre><code>mod1.age &lt;- glmer(DV ~ devX1*devX2*cAge + (devX1*devX2|Subject) + (devX1*devX2|Item), 
                  data=mydata, family=binomial, glmerControl(optimizer='bobyqa', 
                  optCtrl=list(maxfun=400000)))

     AIC      BIC   logLik deviance df.resid 
   624.4    747.4   -284.2    568.4      569 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9512 -0.5140 -0.2234  0.5361  5.3189 

Random effects:
 Groups  Name        Variance  Std.Dev.  Corr             
 Subject (Intercept) 1.037e-11 3.220e-06                  
         devX1       2.692e+00 1.641e+00  0.28            
         devX2       3.864e-02 1.966e-01  0.08 -0.94      
         devX1:devX2 4.489e+00 2.119e+00 -0.50 -0.97  0.82
 Item    (Intercept) 5.280e-01 7.267e-01                  
         devX1       2.662e-01 5.159e-01  0.79            
         devX2       3.948e-01 6.284e-01 -0.36 -0.48      
         devX1:devX2 2.906e+00 1.705e+00 -0.59  0.02 -0.22
Number of obs: 597, groups:  Subject, 30; Item, 20

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -1.3832677  0.0019843  -697.1  &lt; 2e-16 ***
devX1             2.4397103  0.0020486  1190.9  &lt; 2e-16 ***
devX2             0.1386076  0.0019838    69.9  &lt; 2e-16 ***
cAge             -0.0091753  0.0016630    -5.5 3.44e-08 ***
devX1:devX2      -0.3524321  0.0028066  -125.6  &lt; 2e-16 ***
devX1:cAge        0.0150530  0.0019310     7.8 6.41e-15 ***
devX2:cAge        0.0121991  0.0018876     6.5 1.03e-10 ***
devX1:devX2:cAge  0.0005894  0.0019504     0.3    0.763    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
             (Intr) devX1  devX2  cAge   dvX1:X2 dvX1:A dvX2:A
devX1       -0.001                                           
devX2       -0.001  0.001                                    
cAge         0.002 -0.002 -0.002                             
devX1:devX2 -0.002  0.001  0.001 -0.001                      
devX1:cAge  -0.002  0.001  0.001 -0.043  0.002               
devX2:cAge  -0.001  0.001  0.001 -0.040  0.001  -0.017       
dvX1:dvX2:A  0.001 -0.001 -0.001 -0.019 -0.001  -0.020 -0.009
</code></pre>

<p>There is no problem of complete or quasi complete separation between <code>Age</code> and the binary DV. However, there is an almost perfect 1:1 match between <code>Age</code> and <code>Subject</code> (with <code>Subject</code> specified asÂ aÂ random effect in the models). In other words, for most values of <code>Age</code>, there is only one subject corresponding to that value, which makes <code>Age</code> a sort of another version of <code>Subject</code>.</p>

<p>Could this be what is causing severeÂ convergence problems? </p>

<p>If so, would transforming <code>Age</code> into a categorical variable (e.g., with 3 levels) be a suitable solution? I would like to avoid largely arbitrary choices about model specification.</p>

<p>What makes me doubt about this explanation though is that if I remove <code>Subject</code> as random effect, the resulting model still fails to converge.</p>

<pre><code>mod1.age4 &lt;- glmer(DV ~ devX1*devX2*cAge + (devX1*devX2|Item), data=mydata, 
                   family=binomial, glmerControl(optimizer='bobyqa', 
                   optCtrl=list(maxfun=400000)))
</code></pre>
"
"0.162043838788391","0.166474099685359","144815","<p>I'm encountering problems with the results of a <code>glmer</code> model (<code>lme4</code>-package).
Im trying to answer the question, whether a beaver is more likely to be present (<code>Status == 1</code>) or absent (<code>Status == 0</code>) with changing geomorphic and vegetation variables. My model formula looks like this:</p>

<pre><code>model1 &lt;- glmer(Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                         (1 | Location), family=""binomial"", data=Daten12, 
                control=glmerControl(optimizer=""Nelder_Mead""))
</code></pre>

<p>My output looks OK, as far as I can tell, the only peculiar thing being the high estimates of <code>slopecatCentered</code>:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                  (1 | Location)
Data: Datentest
Control: glmerControl(optimizer = ""Nelder_Mead"")

AIC      BIC     logLik    deviance   df.resid 
62.7     77.4    -25.3     50.7       80 

Scaled residuals: 
  Min        1Q    Median        3Q       Max 
-0.095917 -0.003971  0.000000  0.002706  0.079395 

Random effects:
Groups   Name        Variance Std.Dev.
Location (Intercept) 3682     60.68   
Number of obs: 86, groups:  Location, 43

Fixed effects:
                            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -18.5782     7.0847  -2.622 0.008734 ** 
SlopecatCentered             20.4162     5.6060   3.642 0.000271 ***
Canal_width                   0.4763     0.1584   3.007 0.002638 ** 
Distance1:Resource_biotopes   1.0442     0.4717   2.214 0.026861 *  
Distance2:Resource_biotopes   1.0379     0.4662   2.226 0.026010 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) SlpctC Cnl_wd Ds1:R_
SlopctCntrd -0.632                     
Canal_width -0.902  0.698              
Dstnc1:Rsr_ -0.663  0.560  0.458       
Dstnc2:Rsr_ -0.677  0.538  0.461  0.787    
</code></pre>

<p>My qqplot looks weird, though, and so does my residual vs. fitted plot:  </p>

<p><img src=""http://i.stack.imgur.com/8SJjD.jpg"" alt=""qqnorm plot with sjp.glmer(model,...)""></p>

<p><img src=""http://i.stack.imgur.com/wyZp7.jpg"" alt=""fitted vs. residual plot using plot(model)""></p>

<p>edit: I just had a closer look on my data: The <code>SlopecatCentered</code>variable is not a perfect predictor, but my random factor <code>Location</code>is causing this problem. In my raw data set, it denotes 43 different locations. One location has two <code>distance</code> in which most of the variables were measured, so my <code>location</code>variable has 43 * 2 = 86 entrys (in fact, that's the length of the data frame): </p>

<pre><code> &gt;Daten12$Loc
[1] 1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9  10 10 11 11 12 12 13 13 14 14 15 15 16 16 17
[34] 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33
[67] 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43
43 Levels: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ... 43
</code></pre>

<p>I changed that to 1-86 and ran a test model and the plot looked ok (I know that the random effect was futile in that test model, but I wanted to get to the root of the problem).</p>

<p>So apparantly, my raw data frame layout is wrong. But I got samples online to compare, and their layout looks similar, so I just don't know how to fix it.   </p>
"
"0.0935560539449976","0.0961138662664425","144904","<p>I have a data set containing various vegetation and geomorphic variables sampled in 3 <code>distances</code> on both <code>sides</code> of 43 drainage ditches (<code>Location</code>). Roughly half of these ditches are occupied by a beaver, the other half is empty. Now I want to run a model with the binomial response variable <code>Status</code> (""beaver == 1"" / ""beaver == 0"")
I'm struggeling with the order and layout of the nested and interaction effects using <code>glmer</code>. So far I've got</p>

<pre><code>fit &lt;- glmer(Status ~ BankslopeScaled + Connectivity + 
                      Canal_width + Distance:Food_crops + 
                      Distance:Edible_trees + 
                (1 | Distance/Side/Location), 
              data, family=binomial(link=""logit"")
</code></pre>

<p>but I'm not sure ifI still have pseudoreplication in my data or whether I correctly applied the formuly in order to estimate the influence of the predictors in every <code>distance</code> on both <code>sides</code> in each <code>Location</code>. </p>

<p>Like, if <code>food_crops</code> in the 3rd <code>distance</code> on the left <code>side</code> is lower than <code>edible_trees</code> in the 2nd <code>distance</code> on the right <code>side</code>, then ...</p>

<p>I kinda feel like there's something wrong with my random effects-term.</p>

<p>My out put looks like this:</p>

<pre><code>summary(fit)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ BankslopeScaled + Connectivity + Canal_width + Distance:Food_crops +  
Distance:Edible_trees + (1 | Distance/Side/Location)
Data: Satz

     AIC      BIC   logLik deviance df.resid 
   314.6    360.8   -144.3    288.6      245 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.18541 -0.71205  0.07243  0.82483  1.75303 

Random effects:
 Groups                   Name        Variance  Std.Dev. 
 Location:(Side:Distance) (Intercept) 2.834e-02 1.683e-01
 Side:Distance            (Intercept) 2.074e-10 1.440e-05
 Distance                 (Intercept) 2.085e-10 1.444e-05
 Number of obs: 258, groups:  Location:(Side:Distance), 258; Side:Distance, 6; Distance, 3

 Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)            -2.86517    0.79747  -3.593 0.000327 ***
 BankslopeScaled         1.76475    0.62541   2.822 0.004776 ** 
 Connectivity            0.10394    0.02729   3.809 0.000140 ***
 Canal_width             0.19138    0.11089   1.726 0.084364 .  
 Distance1:Food_crops    0.03667    0.09366   0.391 0.695441    
 Distance2:Food_crops    0.10852    0.08996   1.206 0.227694    
 Distance3:Food_crops    0.06303    0.08502   0.741 0.458510    
 Distance1:Edible_trees  0.02273    0.01327   1.712 0.086818 .  
 Distance2:Edible_trees -0.01750    0.02992  -0.585 0.558738    
 Distance3:Edible_trees  0.09769    0.07986   1.223 0.221201    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 [correlation of fixed effects snipped]    
</code></pre>

<p>A point into the right direction is much appreciated!</p>
"
"0.219616069742144","0.225620349160598","149732","<p>I'm trying to analyze the data from an experiment I conducted, and could use some guidance in relation to fixed vs. random effects.</p>

<p>The experiment was related to risk-seeking behavior in the context of hypothetical gambles, and implemented a 3 (Response Scale: Control vs. RI vs. ABR) x 3 (Stakes) X 5 (Endowment) factorial design. Response Scale was a between-subjects manipulation, and the levels of Stakes and Endowment were combined factorially to produce 15 different gamble scenarios, all of which were evaluated by each participant (i.e. gamble evaluation was within-subjects). The DV of interest for the particular analysis I'm working on is a binary indicator variable called ""Would.Play"" that describes whether a participant would choose to play the gamble if they were to encounter it in real life.</p>

<p>As a preliminary analysis, I'd like to be able to claim that there were no [or, as the data seem to indicate, <em>were</em>] meaningful differences in Would.Play as a result of random assignment to a particular Response Scale condition (designated by the factor variable ""Response.Scale"", ref=""Control"").</p>

<p>I can obviously do this with a binary logit for each of the 15 gambles (designated by the variable ""Gamble.Num""), but I'd like to avoid issues with multiple testing. My preference, therefore, is to fit a single model that accounts for the heterogeneity in gambles by fitting a separate intercept for each gamble.</p>

<p>I've come across two ways to do this, each of which seems to give different results: Dummy ""Fixed Effects"" modeling in glm() and ""random effects"" modeling in glmer() (see output below).</p>

<p>It seems possible that the difference in the estimated coefficients could be the result of the Dummy ""Fixed Effects"" approach taking Gamble.Num==1 as a reference level, but I don't have a very deep understanding of the math underlying these two techniques. I was hoping someone would be able to give me a quick explanation of (a) why the these two models appear to give different results; and (b) whether one of these approaches is better suited to answering my question of interest: is there a unique effect of Response.Scale on Would.Play, taking heterogeneity in gambles into account?</p>

<p>Below is a quick look at the data I'm using, and the output of the two models:</p>

<pre><code>## Data ##
head(analysis.0.data)
 Local.ID Condition Response.Scale RS.Code Gambles.First Gamble.Num Endowment Stakes
1        8         4             RI       1             0          1      -150     10
2        8         4             RI       1             0          2      -150     50
3        8         4             RI       1             0          3      -150    200
4        8         4             RI       1             0          4       -25     10
5        8         4             RI       1             0          5       -25     50
6        8         4             RI       1             0          6       -25    200
  Would.Play Perc.Risk
1          0         4
2          0         6
3          0         5
4          0         3
5          0         5
6          0         7


## Dummy ""Fixed Effects"" Model ##
summary(glm(Would.Play ~ Response.Scale + factor(Gamble.Num), family=""binomial"",     
data=analysis.0.data))

Call:
glm(formula = Would.Play ~ Response.Scale + factor(Gamble.Num), 
    family = ""binomial"", data = analysis.0.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7766  -0.7204  -0.4678   0.7006   2.5394  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          -1.14906    0.21987  -5.226 1.73e-07 ***
Response.ScaleRI     -0.06749    0.12815  -0.527  0.59844    
Response.ScaleABR    -0.91035    0.13843  -6.576 4.82e-11 ***
factor(Gamble.Num)2  -0.94090    0.35886  -2.622  0.00874 ** 
factor(Gamble.Num)3  -1.12416    0.37769  -2.976  0.00292 ** 
factor(Gamble.Num)4   0.31966    0.28379   1.126  0.25999    
factor(Gamble.Num)5  -0.63953    0.33303  -1.920  0.05482 .  
factor(Gamble.Num)6  -0.85860    0.35120  -2.445  0.01449 *  
factor(Gamble.Num)7   1.42100    0.26770   5.308 1.11e-07 ***
factor(Gamble.Num)8   0.35620    0.28268   1.260  0.20765    
factor(Gamble.Num)9  -0.51138    0.32379  -1.579  0.11425    
factor(Gamble.Num)10  2.10754    0.27298   7.720 1.16e-14 ***
factor(Gamble.Num)11  0.28248    0.28496   0.991  0.32154    
factor(Gamble.Num)12 -1.02908    0.36760  -2.799  0.00512 ** 
factor(Gamble.Num)13  2.49612    0.28133   8.873  &lt; 2e-16 ***
factor(Gamble.Num)14  1.72839    0.26867   6.433 1.25e-10 ***
factor(Gamble.Num)15  0.08524    0.29204   0.292  0.77039    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2649.2  on 2249  degrees of freedom
Residual deviance: 2096.4  on 2233  degrees of freedom
AIC: 2130.4

Number of Fisher Scoring iterations: 5


## GLMER ""Random-Effects"" Model##
summary(glmer(Would.Play ~ Response.Scale + (1|Gamble.Num), family=""binomial"", 
data=analysis.0.data))
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
[glmerMod]
 Family: binomial  ( logit )
Formula: Would.Play ~ Response.Scale + (1 | Gamble.Num)
   Data: analysis.0.data

     AIC      BIC   logLik deviance df.resid 
  2169.3   2192.1  -1080.6   2161.3     2246 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9011 -0.5461 -0.3522  0.5439  4.6708 

Random effects:
 Groups     Name        Variance Std.Dev.
 Gamble.Num (Intercept) 1.291    1.136   
Number of obs: 2250, groups:  Gamble.Num, 15

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -0.90254    0.30722  -2.938  0.00331 ** 
Response.ScaleRI  -0.06682    0.12707  -0.526  0.59897    
Response.ScaleABR -0.90170    0.13727  -6.569 5.07e-11 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Rs.SRI
Rspns.SclRI -0.202       
Rspns.ScABR -0.183  0.456
</code></pre>

<p>Thanks!</p>
"
"0.201826368903196","0.160220579875585","151079","<p>I have a data set of 2430 observations, with a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I want to to a mixed effects model using glmer. Here is what I entered into R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I then get the following warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>This is what my summary looks like:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting this bit in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So here's the new command:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>I get one less warning, but the other one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks weird:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>Does anyone have an idea what I can do to solve this? Or tell me what this warning even means? Please explain in a way that an R-newbie like myself can understand!</p>

<p>Any help is much appreciated!</p>
"
"0.140334080917496","0.144170799399664","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.132308240331124","0.118934841555352","153611","<p>I'm revising a paper on pollination, where the data are binomially distributed (fruit matures or does not). So I used <code>glmer</code> with one random effect (individual plant) and one fixed effect (treatment). A reviewer wants to know whether plant had an effect on fruit set -- but I'm having trouble interpreting the <code>glmer</code> results.</p>

<p>I've read around the web and it seems there can be issues with directly comparing <code>glm</code> and <code>glmer</code> models, so I'm not doing that. I figured the most straightforward way to answer the question would be to compare the random effect variance (1.449, below) to the total variance, or the variance explained by treatment. But how do I calculate these other variances? They don't seem to be included in the output below. I read something about residual variances not being included for binomial <code>glmer</code> -- how do I interpret the relative importance of the random effect?</p>

<pre><code>&gt; summary(exclusionM_stem)
Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: cbind(Fruit_1, Fruit_0) ~ Treatment + (1 | PlantID)

     AIC      BIC   logLik deviance df.resid 
   125.9    131.5    -59.0    117.9       26 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0793 -0.8021 -0.0603  0.6544  1.9216 

Random effects:
 Groups  Name        Variance Std.Dev.
 PlantID (Intercept) 1.449    1.204   
Number of obs: 30, groups:  PlantID, 10

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  -0.5480     0.4623  -1.185   0.2359   
TreatmentD   -1.1838     0.3811  -3.106   0.0019 **
TreatmentN   -0.3555     0.3313  -1.073   0.2832   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) TrtmnD
TreatmentD -0.338       
TreatmentN -0.399  0.509
</code></pre>
"
"0.155686684444045","0.173271736553374","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.0661541201655622","0.0679627666030585","155424","<p>In mixed-effects models within the <code>lmer</code> R function of <code>lme4</code> calculating the vector $\theta$ seems to be a key step, necessary to obtain the matrix $\Sigma(\theta)$, which is later decomposed as $\Sigma = TSST'$.</p>

<p>The parameter vector $\theta$ is defined as ""random-effects parameters estimates; these are parameterized as the relative Cholesky factors of each random effect term"".</p>

<p>They are generated through iteration, and get incorporated into the $\Sigma(\theta)$ matrix as in the following example:</p>

<p>In the <code>sleepstudy</code> dataset in <code>lme4</code> the 'Reaction' time in miliseconds depends on the number of days of sleep deprivation, and the random effect of the actual individual being tested or ""Subject.""</p>

<p>Fitting a probably nonsensical mixed-effects model as <code>fit &lt;- lmer(Reaction ~ 1 + (1 | Subject), sleepstudy)</code> would result in the following T, S and $\Sigma$ matrices in relation to $\theta$:</p>

<p>First off, let's call $\theta$ with <code>getME(fit,""theta"") # 0.807831</code>. From it we can calculate manually,</p>

<p><code>str(sleepstudy$Subject) # Factor w/ 18 levels</code></p>

<p>Hence,
<code>T &lt;- diag(1,18)</code>.</p>

<p><code>S &lt;- T * 0.807831</code>, and finally <code>Sigma &lt;- Sigma &lt;- S %*% t(S)</code>.</p>

<p>It is clear, then, that $\theta$ is key in the <code>lmer</code> function. It also sounds as though there is no algebraic formula for it. So the question is, <strong>How do you get $\theta$</strong> <strong>in a step-by-step, easy to follow sequence of operations?</strong></p>
"
"0.140334080917496","0.144170799399664","158539","<p>I'm doing model selection, analysing the effect of a number of variables on the number of shoots browsed by deer, using the number of shoots available as an offset variable. My data distribution is negative binomial.</p>

<p>Following the advices received during a course, I was first fitting a global model using <code>glm.nb</code> and noting the theta value obtained. After that, I was doing my model selection using the package <code>AICcmodavg</code> and <code>glm</code>. I specified the theta value for each model using the value of the first model in <code>glm</code> like this : <code>family=negative.binomial(theta = )</code>. My understanding here is that we specified a similar theta value to be able to compare the models.</p>

<p>So far, so good. But I needed to add a random effect to my model and my models didn't converged with <code>glmer.nb</code>. I thus switched to <code>glmmadmb</code>, where the theta value seems to have a different name, alpha, the negative binomial dispersion parameter. So, my questions:</p>

<p>1-Is alpha really the equivalent of theta ?</p>

<p>2-My models have very different alpha values (from 400 to 0.4000). Is there a range of ""normal"" negative binomial dispersion parameter value ?</p>

<p><em>EDIT: Running again my code this morning removed any values around 400. All alpha values are now similar. I think this was definitely a mistake and I think anyone obtaining very different values should be careful !</em></p>

<p>3-Should I still proceed with specifying a a same alpha values for all my models ? This can be achieve in <code>glmmadmb</code>, in my understanding, by using <code>start= list(log_alpha = )</code>.</p>

<p>Thanks everyone.</p>
"
"0.114582297256771","0.117714964779443","160445","<p>I have computed GLMM using glmer in R. My response variable is species richness and my explanatory variable is grazing treatment (with three categories: cattle, sheep and ungrazed). In the model I have included site as a fixed variable and also a new object with the same number of variations as I have to attempt to account for underdispersal (<code>obs</code>):</p>

<pre><code>model2&lt;-glmer(VegRichness~Grazing+(1|Site)+(1|obs),family=""poisson"",data=veg.rich)
</code></pre>

<p>My output is below and the questions I have about it are:</p>

<p>How do I interpret the fixed effects section?
Cattle grazing seems to be missing in the oputput, is this because it is somehow incorporated into the intercept?</p>

<pre><code>&gt; summary(model2)

Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]

Family: poisson  ( log ) 

Formula: VegRichness ~ Grazing + (1 | Site) + (1 | obs)
   Data: veg.rich


     AIC      BIC   logLik deviance df.resid 
    178.8    185.2    -84.4    168.8       22 

Scaled residuals: 

Min..........           1Q............           Median....       3Q.........        Max

-1.4936...      -0.5698.....       -0.1928...      0.4923...   1.3646 

Random effects:

 Groups  ... Name......        Variance..... Std.Dev.

 obs.........      (Intercept).. 0.00000....  0.0000 

 Site.........     (Intercept).. 0.03596....  0.1896

Number of obs: 27, groups:  obs, 27; Site, 3

Fixed effects:
                .......Estimate.... Std. Error..... z value... Pr(&gt;|z|)    
(Intercept)............      3.55358.......    0.12309.......  28.869.....  &lt; 2e-16 ***                                                                 
GrazingSheep......     0.01242......    0.07876........   0.158.......  0.87467    
GrazingUngrazed -0.27526.....    0.08503........  -3.237......  0.00121 ** 

---
Signif. codes:  0 x***x 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr)....GrzngS

GrazingShe......................... -0.322       
GrzngUngrzd...................... -0.298...  0.466
</code></pre>
"
"0.104598848163826","0.107458569276045","160780","<p>having only dim memories of this subject from uni, I'm struggling a bit with fitting a polynomial binomial mixed effects model with two nested random effects.</p>

<p>My data are counts of success and failures (s and f) at a variety of values of x.  There are two random effects, r1 and a nested random effect r2.</p>

<p>I would like to try fitting a polynomial model, up to ninth order and use the AIC values to find the minimum adequate model, but I'm unsure of the syntax.</p>

<p>So I've got three questions really:</p>

<p>Is the code below currently correct?</p>

<p>What is the syntax to fit a glmer polynomial?</p>

<p>Is this whole thing statistical nonsense, and should i try a different approach?</p>

<p>Thanks for your help, code below:</p>

<pre><code>require(lme4)

## SAMPLE SET:
## x fixed effect
## s count success
## f count fail
## r1 random effect 1
## r2 random effect 2

sample.set &lt;- data.frame(x = runif(1000,-100,100),
                         s = round(runif(1000,100,1000),0),
                         f = round(runif(1000,100,10000),0),
                         r1 = c(rep(""A"",250), rep(""B"",250), rep(""C"",250), rep(""D"",250)))

sample.set$r2 &lt;- sapply(sample.set$r1,function(x){if(x == ""A"")                {as.character(round(runif(1,1,5), 0 ))} else
                                                  if(x == ""B"") {as.character(round(runif(1,6,10), 0))} else
                                                  if(x == ""C"") {as.character(round(runif(1,11,15),0))} else
                                                               {as.character(round(runif(1,16,20),0))}})

prop.tab &lt;- cbind(sample.set$s,sample.set$f)

mm.model &lt;- glmer(prop.tab ~ sample.set$x + (sample.set$x | sample.set$r1) + (sample.set$x | sample.set$r2),
                  family = binomial, control = glmerControl(optimizer = ""bobyqa""), nAGQ = 0)
</code></pre>
"
"0.0954852477139755","0.117714964779443","160943","<p>Reading this <a href=""http://stats.stackexchange.com/a/78830/67822"">post</a> by @gung brought me to try to reproduce his superb illustrations, and led ultimately to question something I had read or heard, but that I'd like to understand more intuitively: Why is an OLS <code>lm</code> controlling for a correlated variable better (can I say 'better' tentatively?) than a mixed-effects model with different intersects and slopes?</p>

<p>Here's the toy example, again trying to parallel the post quoted above:</p>

<p>First the data:</p>

<pre><code>set.seed(0)    
x1 &lt;- c(rnorm(10,3,1),rnorm(10,5,1),rnorm(10,7,1))
x2 &lt;- rep(c(1:3),each=10)
    y1 &lt;- 2  -0.8 * x1[1:10] + 8 * x2[1:10] +rnorm(10)
    y2 &lt;- 6  -0.8 * x1[11:20] + 8 * x2[11:20] +rnorm(10)
    y3 &lt;- 8  -0.8 * x1[21:30] + 8 * x2[21:30] +rnorm(10)
y &lt;- c(y1, y2, y3)
</code></pre>

<p>And the different models:</p>

<pre><code>library(lme4)

fit1 &lt;- lm(y ~ x1)
fit2 &lt;- lm(y ~ x1 + x2)
fit3 &lt;- lmer(y ~ x1 + (1|x2))
fit4 &lt;- lmer(y ~ x1|x2, REML=F)
</code></pre>

<p>Comparing  Akaike information criterion (AIC) between models:</p>

<pre><code>AIC(fit1, fit2, fit3, fit4)

     df      AIC
     df      AIC
fit1  3 184.5330
fit2  4  97.6568
fit3  4 112.0120
fit4  5 114.8401
</code></pre>

<p>So it seems that the best model is <code>lm(y ~ x1 + x2)</code>, which I guess make sense given the strong correlation between <code>x1</code> and <code>x2</code> <code>cor(x1,x2) [1] 0.8619565</code>.</p>

<p>But the question is, What is the intuition behind this behavior, when the mixed model with varying intercepts and slopes seems to result in coefficients that fit the data beautifully?</p>

<pre><code>coef(lmer(y ~ x1|x2))
$x2

      x1       (Intercept)
1 -1.1595730    11.37746
2 -0.2586303    19.38601
3  0.2829754    24.20038

library(lattice)    
xyplot(y ~ x1, groups = x2, pch=19,
           panel = function(x, y,...) {
             panel.xyplot(x, y,...);
             panel.abline(a=coef(fit4)$x2[1,2], b=coef(fit4)$x2[1,1],lty=2,col='blue');
             panel.abline(a=coef(fit4)$x2[2,2], b=coef(fit4)$x2[2,1],lty=2,col='magenta');
             panel.abline(a=coef(fit4)$x2[3,2], b=coef(fit4)$x2[3,1],lty=2,col='green')
           })
</code></pre>

<p><img src=""http://i.stack.imgur.com/NYweu.png"" alt=""enter image description here""></p>

<p>I do realize that the graphical fit of the bivariate OLS can look pretty good as well:</p>

<pre><code>library(scatterplot3d)
plot1 &lt;- scatterplot3d(x1,x2,y, type='h', pch=16,
              highlight.3d=T)
plot1$plane3d(fit2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Jt2bz.png"" alt=""enter image description here""></p>

<p>... and I don't know if this invalidates the question. </p>
"
"0.20705601622961","0.166474099685359","164457","<p>I have seen questions about this on this forum, and I have also asked it myself in a previous post but I still haven't been able to solve my problem. Therefore I am trying again, formulating the question as clearly as I can this time, with as much detailed information as possible. </p>

<p>My data set has a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I am using a mixed effects model using glmer. Here is what I entered in R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I get 2 warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>My summary looks like this:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting the following in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So that's what I did:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>Now, the second warning is gone, but the first one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks odd:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>What I can do to solve this? Or can anyone tell me what this warning even means? (in a way that an R-newbie like myself can understand)</p>
"
"0.168660574814382","0.173271736553374","169549","<p>I am currently writing my master thesis about the effect of an insecticide (clothianidin) on the microflora of bumblebees. I received the bumblebees from an experiment with a nested study design. 16 fields were paired according to land use of the surroundings etc. In each field 2 boxes were placed, which contained 2 hives (colonies) each. </p>

<p>I was trying to determine if the treatment affects the prevalence of certain organisms including ABPV, N.bombi and Snodgrasella as you can see in this data frame:</p>

<pre><code>    structure(list(treatment = structure(c(2L, 2L, 2L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L), .Label = c(""Clothianidin"", 
""Control""), class = ""factor""), pair = structure(c(1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L), .Label = c(""P01"", 
""P02"", ""P03"", ""P04"", ""P05"", ""P10"", ""P11"", ""P12""), class = ""factor""), 
    field = structure(c(6L, 6L, 6L, 6L, 12L, 12L, 12L, 12L, 1L, 
    1L, 1L, 1L, 2L, 2L, 2L, 2L, 10L, 10L, 10L, 10L, 13L, 13L, 
    13L, 13L, 7L, 7L, 7L, 7L, 16L, 16L, 16L, 16L, 8L, 8L, 8L, 
    8L, 9L, 9L, 9L, 9L, 3L, 3L, 3L, 3L, 11L, 11L, 11L, 11L, 4L, 
    4L, 4L, 4L, 5L, 5L, 5L, 5L, 14L, 14L, 14L, 14L, 15L, 15L, 
    15L, 15L), .Label = c(""VR02"", ""VR03"", ""VR04"", ""VR05"", ""VR06"", 
    ""VR07"", ""VR09"", ""VR12"", ""VR13"", ""VR14"", ""VR16"", ""VR17"", ""VR18"", 
    ""VR20"", ""VR21"", ""VR23""), class = ""factor""), box.nested = c(12, 
    11, 12, 11, 23, 24, 23, 24, 1, 1, 2, 2, 4, 3, 3, 4, 20, 20, 
    19, 19, 25, 26, 25, 26, 14, 14, 13, 13, 31, 31, 32, 32, 16, 
    15, 15, 16, 18, 17, 17, 18, 6, 5, 5, 6, 21, 22, 22, 21, 8, 
    8, 7, 7, 9, 9, 10, 10, 28, 27, 28, 27, 29, 30, 30, 29), hive.nested = c(24L, 
    21L, 23L, 22L, 46L, 48L, 45L, 47L, 2L, 1L, 4L, 3L, 8L, 5L, 
    6L, 7L, 40L, 39L, 38L, 37L, 49L, 52L, 50L, 51L, 27L, 28L, 
    26L, 25L, 62L, 61L, 64L, 63L, 31L, 29L, 30L, 32L, 36L, 34L, 
    33L, 35L, 12L, 9L, 10L, 11L, 41L, 43L, 44L, 42L, 15L, 16L, 
    13L, 14L, 17L, 18L, 20L, 19L, 55L, 54L, 56L, 53L, 58L, 59L, 
    60L, 57L), ABPV.detected = structure(c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
    1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), .Dim = c(64L, 1L), .Dimnames = list(NULL, ""ABPV.detected"")), 
    N.bombi.detected = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 1, 
    1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(64L, 
    1L), .Dimnames = list(NULL, ""N.bombi.detected"")), Snodgrasella.detected = structure(c(0, 
    1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 
    1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1), .Dim = c(64L, 1L), .Dimnames = list(NULL, 
        ""Snodgrasella.detected""))), .Names = c(""treatment"", ""pair"", 
""field"", ""box.nested"", ""hive.nested"", ""ABPV.detected"", ""N.bombi.detected"", 
""Snodgrasella.detected""), class = ""data.frame"", row.names = c(NA, 
-64L))
</code></pre>

<p>I was trying estimate the treatment effect with a model that included pair, field, box and hive as (nested) random effects:</p>

<pre><code>library(lme4)
ABPV.prev &lt;- glmer(ABPV.detected ~ treatment 
                   + (1|pair/field/box.nested/hive.nested)
                   ,data=data.f, 
                   family=binomial)
summary(ABPV.prev)
</code></pre>

<p>The models of ABPV and N. bombi failed to converge, because I have so many zeros. ABPV was only found in one pair and N. bombi was only found in 2 pairs.</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.100788 (tol = 0.001, component 2)
</code></pre>

<p>The p-values of the models did indicate significant treatment effects, but I guess it's pair or field effects that cause the variation...</p>

<p>In another forum I read that it can be tested whether the failure to converge represents a real problem using this function..</p>

<pre><code>relgrad &lt;- with(ABPV.prev@optinfo$derivs,solve(Hessian,gradient))
max(abs(relgrad))
</code></pre>

<p>... and it does as the p-value is rather large p=0.1 (for ABPV).</p>

<p>I removed some of the random effects and it works and it works e.g. when I remove both hive.nested and box.nested from the model, though increasing the AIC (why?). </p>

<p>I also tried to include pair as a fixed effect: </p>

<pre><code>library(lme4)
ABPV.prev &lt;- glmer(ABPV.detected ~ treatment + pair 
                   + (1|field/box.nested/hive.nested)
                   ,data=data.f, 
                   family=binomial)
</code></pre>

<p>but it produced an error:</p>

<pre><code>Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate
</code></pre>

<p>My questions are </p>

<ol>
<li>How do I judge which random effects to include and which not? </li>
<li>How do I test if it is a pair (or field effect) not a treatment effect that causes the variation?</li>
</ol>

<p>N.B. In the Snodgrasella model it makes absolutely no difference if I exclude, any or all of the random effects. Why is that?</p>

<p>Hope you can help me, thanks!</p>
"
"0.140334080917496","0.144170799399664","172408","<p>I want to investigate whether Group1 affects Resp variable in my model.
Group1 is an ordinal variable that can assume 4 values (1 2 3 4).</p>

<p>Group2 is also an ordinal variable taht can assume 5 values (0 1 2 3 4)</p>

<p>I create the null model (without Group1).</p>

<pre><code>model.null =  lmer(Resp~Group2+Gender+Age+BMI+(1|Subject)+(1|Day_type),data=table_data,REML=FALSE)
</code></pre>

<p>I create the full model:</p>

<pre><code>model.full = lmer(Resp~Group1+Group2+Gender+Age+BMI+(1|Subject)+(1|Day_type),data=table_data,REML=FALSE)
</code></pre>

<p>I run anova to see whether the 2 models are significantly different:</p>

<pre><code>anova(model.null,model.full)
Data: table_data
Models:
model.null: Resp ~ Group2 + Gender + Age + BMI + (1 | Subject) + (1 | Day_type)
model.full: Resp ~ Group1 + Group2 + Gender + Age + BMI + (1 | Subject) + (1 | 
model.full:     Day_type)
           Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
model.null 11 18320 18391 -9148.8    18298                            
model.full 14 18314 18405 -9143.0    18286 11.588      3   0.008935 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>according to the output of anova I can conclude that are significantly different: i.e. Group1 affects the response.</p>

<p>Now I run</p>

<pre><code>summary(model.full)
</code></pre>

<p>and get</p>

<pre><code>Fixed effects:
             Estimate Std. Error t value
(Intercept)  2.428580   0.462824   5.247
Group1   2   0.119003   0.163572   0.728
Group1   3   0.210836   0.171478   1.230
Group1   4   0.562406   0.196697   2.859
Group2   1   0.069754   0.139780   0.499
Group2   2   0.139545   0.148745   0.938
Group2   3   0.094811   0.162958   0.582
Group2   4   0.600394   0.214628   2.797
GenderMale   0.451459   0.095112   4.747
Age          0.005298   0.005544   0.956
BMI         -0.003777   0.008470  -0.446
</code></pre>

<p>Are all the 4 values of Group1 significantly different? What can I say looking at this output?</p>
"
"0.175027350160361","0.179812578843619","173813","<p>I have a data obtained through forest inventory conducted yearly (1994-2015) in a West African country. 10 plots of equal sizes (1 ha each) were selected from  unmanaged natural forest and then species of trees and shrubs were identified and counted. Biodiversity indices like Abundance, Shannon, Simpson were calculated. I have chosen only 9 years in which data were collected in all the 10 plots and I discarded the incomplete years and considered ""Year"" as factor. </p>

<p>The data is structured as:</p>

<pre><code>str(BIData)
'data.frame':   90 obs. of  9 variables:
 $ Year          : Factor w/ 9 levels ""1994"",""1995"",..: 1 1 1 1 1 1 1 1 1 1 ...
     $ Plot          : Factor w/ 10 levels ""Bas Kolel"",""Bougou"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Richness      : int  8 21 13 14 8 10 6 10 8 20 ...
     $ Abundance     : int  286 1471 1121 466 242 97 250 790 208 2015 ...
 $ Shannon       : num  1.33 1.79 1.55 1.68 1.44 1.71 1.35 1.27 1.27 1.86 ...
     $ Simpson       : num  0.656 0.71 0.682 0.694 0.665 0.714 0.66 0.647 0.649 0.718 ...
 $ InverseSimpson: num  2.91 3.45 3.14 3.28 2.99 3.52 2.95 2.83 2.86 3.54 ...
     $ Topography    : Factor w/ 3 levels ""Plateau"",""Slope"",..: 3 1 1 3 3 2 2 2 3 1 ...
 $ Land_use      : Factor w/ 2 levels ""Cultivated"",""Pasture"": 1 2 2 2 1 1 2 2 1 2 ...
</code></pre>

<p>In addition, plots are located in different topography (slope, valley, plateau) and land use (cultivated, pasture).</p>

<p>I have the following two models in lmer and lme:</p>

<pre><code>model=lmer(Abundance~Year+Topography+Land_use+(1|Plot), method=""ML"", data=BIData)
model=lme(Abundance~Year+Topography+Land_use, random=~1|Plot, method=""ML"", data=BIData)
</code></pre>

<p>I got totally different results: My questions?</p>

<p>I m not an expert but I found that <code>lme</code> provides a kind of ""beautiful"" results with p-values. I can see many significant factors like years, topography and land use whereas in <code>lmer</code> only t-values without p-values. I don't know which one is correct for my data. In both cases, it shows good and acceptable residuals plots.</p>

<p>Please help me to understand which one is correct to my data.</p>

<hr>

<p>Thank you @fcoppens. No, I did not try that parameter. Here are the output of both <code>lme</code> and <code>lmer</code>.</p>

<p><strong><code>lmer</code></strong></p>

<pre><code>model=lmer(Abundance~Year+Topography+Land_use+(1|Plot), method=""ML"", data=BIData)
summary(model)
Linear mixed model fit by REML ['lmerMod']
Formula: Abundance ~ Year + Topography + Land_use + (1 | Plot)
   Data: BIData

REML criterion at convergence: 1106.2

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.5754 -0.5024 -0.0186  0.4015  3.4341 

Random effects:
 Groups   Name        Variance Std.Dev.
 Plot     (Intercept) 51753    227.5   
 Residual             48592    220.4   
Number of obs: 90, groups:  Plot, 10

Fixed effects:
                 Estimate Std. Error t value
(Intercept)       1073.15     252.41   4.252
Year1995             0.40      98.58   0.004
Year1996           -32.70      98.58  -0.332
Year1998          -198.10      98.58  -2.010
Year1999          -341.90      98.58  -3.468
Year2002          -295.80      98.58  -3.001
Year2004          -324.90      98.58  -3.296
Year2010          -291.60      98.58  -2.958
Year2015          -371.00      98.58  -3.763
TopographySlope   -756.87     206.36  -3.668
TopographyValley  -645.82     236.71  -2.728
Land_usePasture    178.07     200.85   0.887
</code></pre>

<p><strong><code>lme</code></strong></p>

<pre><code>model=lme(Abundance~Year+Topography+Land_use, random=~1|Plot, method=""ML"", data=BIData)
summary(model)
Linear mixed-effects model fit by maximum likelihood
 Data: BIData 
       AIC      BIC    logLik
  1264.675 1299.673 -618.3377

Random effects:
 Formula: ~1 | Plot
        (Intercept) Residual
StdDev:    171.5578 209.1232

Fixed effects: Abundance ~ Year + Topography + Land_use 
                     Value Std.Error DF   t-value p-value
(Intercept)      1073.1495  213.5506 72  5.025271  0.0000
Year1995            0.4000  100.4595 72  0.003982  0.9968
Year1996          -32.7000  100.4595 72 -0.325504  0.7457
Year1998         -198.1000  100.4595 72 -1.971938  0.0525
Year1999         -341.9000  100.4595 72 -3.403360  0.0011
Year2002         -295.8000  100.4595 72 -2.944469  0.0044
Year2004         -324.9000  100.4595 72 -3.234138  0.0018
Year2010         -291.6000  100.4595 72 -2.902661  0.0049
Year2015         -371.0000  100.4595 72 -3.693029  0.0004
TopographySlope  -756.8671  171.7008  6 -4.408058  0.0045
TopographyValley -645.8214  196.9543  6 -3.279041  0.0168
Land_usePasture   178.0654  167.1213  6  1.065486  0.3276
Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.6851599 -0.5159528 -0.0222693  0.4401886  3.6493837 

Number of Observations: 90
Number of Groups: 10 
</code></pre>
"
"0.155145163900903","0.130407394727531","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.148540185556025","0.152601258044912","174532","<p>I have been working on my PC to analyse my multilevel data. I am now working on a Mac and have run the same model. Some of the output is the same but some is quite different. I can't seem to work out why. Here is the model:</p>

<pre><code>&gt; loss.2 &lt;- glmer.nb(Loss_across.Chain ~ Posn.c*Valence.c + (Valence.c|mood.c/Chain), data = FinalData_forpoisson, control = glmerControl(optimizer = ""bobyqa"", check.conv.grad = .makeCC(""warning"", 0.05)))
</code></pre>

<p>On the PC I got this output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 8.782e-15 9.371e-08     
              Valence.c   9.608e-15 9.802e-08 0.48
 mood.c       (Intercept) 0.000e+00 0.000e+00     
              Valence.c   1.654e-14 1.286e-07  NaN
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -0.19255    0.04794  -4.016 5.92e-05 ***
Posn.c           -0.61011    0.04122 -14.800  &lt; 2e-16 ***
Valence.c        -0.27372    0.09589  -2.855  0.00431 ** 
Posn.c:Valence.c  0.38043    0.08245   4.614 3.95e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>On the Mac I got this output:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 1.242e-13 3.524e-07     
              Valence.c   4.724e-13 6.873e-07 0.98
 mood.c       (Intercept) 7.998e-16 2.828e-08     
              Valence.c   3.217e-14 1.793e-07 1.00
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       2.947e-05  4.794e-02   0.001    1.000
Posn.c            7.441e-05  4.122e-02   0.002    0.999
Valence.c        -4.011e-05  9.589e-02   0.000    1.000
Posn.c:Valence.c -6.672e-05  8.245e-02  -0.001    0.999

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>Does anyone know why the output might be different across the two platforms and how I might be able to get them to align?</p>
"
"0.162525396577478","0.166968823211932","175444","<p>I'm wondering if a treatment has an effect on my mite population. Therefore I've got a dataset with repeated measurements, some data is missing.</p>

<p>data:</p>

<pre><code>ID   Treatment    Mites   Time    Location    StartPopulation    X1bib
ID1  Control      7       t1      Loc1        5                  10000
ID1  Control      8       t2      Loc1        5                  10000
ID1  Control      10      t3      Loc1        5                  10000
ID1  Control      11      t7      Loc1        5                  10000
ID2  Control      12      t1      Loc2        11                 13000
ID2  Control              t2      Loc2        11                 13000
ID2  Control      14      t3      Loc2        11                 13000
ID3  Treatment    20      t1      Loc1        20                 12000
ID3  Treatment    22      t2      Loc1        20                 12000
ID3  Treatment            t3      Loc1        20                 12000
ID4  Treatment    20      t1      Loc11       18                 11500
and so on..
totally: 110 IDs; 7 different measurements (Time)
</code></pre>

<p>variables:</p>

<pre><code>ID:              factor, unique ID for each population
Treatment:       factor (""Treatment"" or ""Control"")
Mites:           numeric, the variable I'm interested in
Time:            factor with total 7 levels
Location:        factor with total 11 levels
StartPopulation: numeric (mean of Mites for t=-3, -2, -1 -&gt; before Treatment started)
X1bib:           numeric
</code></pre>

<p>I'm interested if my <code>Treatment</code> changed the <code>Mites</code> count - and if yes if there's an increase in it's effect over time. <code>StartPopulation</code> sure had an influence on <code>Mites</code>, <code>otherFactor</code> and <code>Location</code> could've had also.</p>

<p>As I use a mixed model I'd like to use <code>glmer</code> in <code>R</code>. My syntax looks like this:
(changed it, thank you for your answers so far)</p>

<pre><code>PPP &lt;- glmer(Mites ~ Treatment * Time + StartPopulation + X1bib + (1|ID) + (1|Location), data=vat_database, family=poisson)
</code></pre>

<p>which outputs:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Mites ~ Treatment * Time + StartPopulation + X1bib + (1 | ID) +      (1 | Location)
   Data: vat_database
     AIC      BIC   logLik deviance df.resid 
     Inf      Inf     -Inf      Inf      349 
Random effects:
 Groups   Name        Std.Dev.
 ID       (Intercept) 1       
 Location (Intercept) 1       
Number of obs: 367, groups:  ID, 78; Location, 9
Fixed Effects:
                  (Intercept)             TreatmentTreatment                     Timevmf_A2                     Timevmf_A3                     Timevmf_K1                     Timevmf_K2  
                    2.418e-01                      5.342e-01                      3.252e-01                      5.389e-01                      5.725e-01                      1.102e+00  
                   Timevmf_K3                     Timevmf_K4                StartPopulation                          X1bib  TreatmentTreatment:Timevmf_A2  TreatmentTreatment:Timevmf_A3  
                    1.079e+00                      7.893e-01                      1.486e-01                     -1.331e-06                     -4.664e-01                     -5.453e-01  
TreatmentTreatment:Timevmf_K1  TreatmentTreatment:Timevmf_K2  TreatmentTreatment:Timevmf_K3  TreatmentTreatment:Timevmf_K4  
                   -4.513e-01                     -5.476e-01                     -4.477e-01                     -6.858e-01  
fit warnings:
Some predictor variables are on very different scales: consider rescaling
convergence code 0; 1 optimizer warnings; 81500 lme4 warnings
</code></pre>

<p>Am I right considering that on <code>Time=""vmf_K1""</code> my <code>Treatment</code> Mite Population was -4.513e-01 smaller than my <code>Control</code> Mite Population? How about significances?</p>
"
"0.137521489411712","0.141281311948558","176766","<p>I'm running an analysis in R for a mixed model (using <code>lmer</code>).</p>

<pre><code>go&lt;-lmer(dN~treatment*time+(1|replicate), data=crab, REML=FALSE)
</code></pre>

<p>I have used AIC model selection to identify a top model set. Firstly I standardised input variables to a mean of zero and SD of 2, and then dredged the models to produce a candidate model list, and then reduced this list to a top model set using a deltaAIC cut off of >7. </p>

<p>My top model set includes 3 models (the interaction model, the model with two fixed effects, and a model with one fixed effect). I model-averaged across these models to produce coefficient estimates for each variable that I can report.</p>

<p>However, whilst I understand it is important to use standardised models to allow fair comparison for variables when averaging across models, I would also like to back-transform the coefficient/parameter estimates as the standardised values are not directly understandable when comparing with Figures showing the linear relationships i.e. I want the true slope values.</p>

<p>I have three questions:</p>

<p>(1) Presumably the values I need to back-transform (using inverse logit?) are the model-averaged coefficient estimates? (Or is the back-transformation carried out at an earlier stage?)</p>

<p>(2) What is the code and how do I back-transform these values? </p>

<p>(3) Now that I have both standardised and true coefficient estimates, how to report them?! Is it neccessary / good practice to report the standardised values given that model averaging was used, or can I just report the back-transformed values? </p>

<p>Below, if it is helpful, is my data and code.</p>

<pre><code>   time    treatment dN      replicate
    3.0       ice 11.6669         1
    3.0       ice 12.1120         2
    3.0       ice 11.3132         3
    3.0       ice 11.6912         4
    3.0       air 11.6373         5
    3.0       air 12.4235         6
    3.0       air 11.6117         7
    3.0       air 12.5151         8
   56.5       ice 11.5028         1
   56.5       ice 12.1031         2
   56.5       ice 11.2783         3
   56.5       ice 11.8608         4
   56.5       air 12.3022         5
   56.5       air 13.4229         6
   56.5       air 12.9646         7
   56.5       air 13.0974         8
  120.0       ice 11.6082         1
  120.0       ice 12.0306         2
  120.0       ice 11.2716         3
  120.0       ice 11.8967         4
  120.0       air 12.4769         5
  120.0       air 13.3197         6
  120.0       air 12.6280         7
  120.0       air 12.7871         8

library(lme4)
library(MuMIn)
library(arm)

stdz.model&lt;-standardize(go, standardize.y=FALSE)
model.set&lt;-dredge(stdz.model)
top.models&lt;-get.models(model.set, subset=delta &lt; 7)
avs&lt;-model.avg(top.models)
summary(avs)
</code></pre>

<p>Below is my coefficients output - I presume these are standardised, but I would like them to be ""true"" values instead:</p>

<pre><code>Model-averaged coefficients: 
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         12.1467     0.1146 106.015  &lt; 2e-16 ***
c.treatment         -0.9042     0.2292   3.946 7.94e-05 ***
z.time               0.3053     0.1176   2.597  0.00942 ** 
c.treatment:z.time  -0.5995     0.2235   2.682  0.00732 ** 
</code></pre>
"
"0.155686684444045","0.159943141433883","177057","<p>I am running an analysis using a mixed model with lmer in R. I am using AIC as my model selection process. I have a global model and am including within the selection process all subset models including the null.</p>

<p>My global model: </p>

<pre><code>lmer ( N ~ Time * Treatment + (1|Replicate)
</code></pre>

<p>where time is continuous, treatment a 2-level factor, and replicate is a random effect to account for repeated measures taken over time within the same indivdual. N is Nitrogen and is my response.</p>

<p>Firstly, for my global model I have standardised to a mean of zero and SD of 2, as I understand this is neccessary to allow comparability of variables across models. My top model set (delta AICc >7) has identified 2 models (the interaction model and the addtive model with the two fixed effects). </p>

<p>Thus I have model-averaged across these two models. I have the following output:</p>

<pre><code>Model-averaged coefficients: 
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                  12.48242    0.13051  95.645  &lt; 2e-16 ***
c.air.ice                    -1.07202    0.26102   4.107 4.01e-05 ***
z.time                        0.50782    0.06954   7.302  &lt; 2e-16 ***
c.air.ice:z.time             -0.38008    0.13722   2.770  0.00561 **
</code></pre>

<p>My question is how can I obtain parameter estimates that are ""normal"" - the output estimates are obviously useful for relative comparison of effect size between my variables, but importantly, I would like ""true"" slope values that I can report in conjunction with the accompanying Figure. For instance, in my model Time runs from 0 to 120 hours, and thus knowing a true slope value I can estimate from my model N at 120hrs using slope parameter*120. But if I used the standardised slope estimate above (z.time 0.50782) this would be N@120hrs = 0.50782*120 = 60.94 which is way, way off the y-axis scale!! </p>

<p>For comparison, the slope of Time for the interaction model is  0.008156, and the slope of Time for the additive model is 0.0059351. So I expect a true Time slope value in the region of 0.007.  </p>

<p>I have read that I need to back-transform these estimates, using an inverse-logit transformation of the form </p>

<p>p = 1/(1+1/e^x) </p>

<p>where e^x is exponential and x the parameter of interest. This gives the same value as using the invlogit function in the ""arm"" package in R. In this instance for my standardised slope: </p>

<pre><code>invlogit(0.50782)
</code></pre>

<p>=0.6242953 which is still off the y-axis!</p>

<p>So clearly the inverse-logit transformation is doing something different and not giving the value I would be expecting. </p>

<p>How can I get true parameter estimates? </p>
"
"0.193169076964429","0.198450298481877","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.144936415546343","0.136490714576164","178551","<p>I'm using a binomial glmer mixed effects model using and I have two questions. </p>

<ol>
<li>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</li>
</ol>

<p>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")</p>

<blockquote>
  <p>summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + stimulus + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5154.3   5259.5  -2562.2   5124.3     8193 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-25.0764  -0.2706  -0.1939   0.2472  10.5131 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.743    1.32<br>
Number of obs: 8208, groups:  Listener, 228</p>

<p>Fixed effects:
              Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)     2.7561     0.2657  10.371  &lt; 2e-16 <strong>*
ListgpTA        0.1741     0.3147   0.553 0.580128<br>
ListgpTQ        0.0810     0.2575   0.315 0.753094<br>
stimulushaaDD  -5.4415     0.2071 -26.272  &lt; 2e-16 <em></strong>
stimulushad    -4.2953     0.1822 -23.569  &lt; 2e-16 <strong></em>
stimulushaDD   -5.4946     0.2086 -26.337  &lt; 2e-16 <em></strong>
stimulushid    -5.1519     0.1994 -25.832  &lt; 2e-16 <strong></em>
stimulushiDD   -0.6708     0.1801  -3.724 0.000196 <em></strong>
stimulushiid   -5.8124     0.2186 -26.593  &lt; 2e-16 <strong></em>
stimulushiiDD  -5.5101     0.2091 -26.353  &lt; 2e-16 <em></strong>
stimulushud    -0.2016     0.1915  -1.053 0.292345<br>
stimulushuDD   -5.6188     0.2123 -26.462  &lt; 2e-16 <strong></em>
stimulushuud   -5.6107     0.2121 -26.450  &lt; 2e-16 *</strong></p>

<h2>stimulushuuDD  -5.3207     0.2038 -26.109  &lt; 2e-16 ***</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
              (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD
ListgpTA      -0.613<br>
ListgpTQ      -0.755  0.636<br>
stimulushaaDD -0.394 -0.007  0.004<br>
stimulushad   -0.440 -0.006  0.005  0.605<br>
stimulushaDD  -0.392 -0.007  0.003  0.555         0.601<br>
stimulushid   -0.407 -0.007  0.004  0.572         0.624       0.569<br>
stimulushiDD  -0.414  0.000  0.001  0.534         0.606       0.530<br>
stimulushiid  -0.376 -0.006  0.003  0.536         0.578       0.533<br>
stimulushiiDD -0.391 -0.007  0.003  0.554         0.600       0.551<br>
stimulushud   -0.386  0.000  0.000  0.497         0.564       0.493<br>
stimulushuDD  -0.385 -0.007  0.003  0.548         0.592       0.545<br>
stimulushuud  -0.386 -0.007  0.003  0.548         0.593       0.545<br>
stimulushuuDD -0.400 -0.007  0.004  0.564         0.613       0.561<br>
              stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD   0.554<br>
stimulushiid   0.549       0.506<br>
stimulushiiDD  0.568       0.529        0.533<br>
stimulushud    0.516       0.569        0.471        0.492<br>
stimulushuDD   0.562       0.521        0.527        0.544         0.484<br>
stimulushuud   0.562       0.522        0.528        0.545         0.485<br>
stimulushuuDD  0.579       0.543        0.542        0.560         0.505<br>
              stimulushuDD stimulushuud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD<br>
stimulushiid<br>
stimulushiiDD<br>
stimulushud<br>
stimulushuDD<br>
stimulushuud   0.539<br>
stimulushuuDD  0.554        0.554 </p>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<blockquote>
  <p>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"")
  summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5218.3   5253.4  -2604.2   5208.3     8203 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-21.9276  -0.2804  -0.2059   0.2740   9.4275 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.676    1.294<br>
 stimulus (Intercept) 4.949    2.225<br>
Number of obs: 8208, groups:  Listener, 228; stimulus, 12</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)  -1.3754     0.6792  -2.025   0.0429 *
ListgpTA      0.2284     0.3073   0.743   0.4572  </p>

<h2>ListgpTQ      0.1432     0.2513   0.570   0.5687</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
         (Intr) LstgTA
ListgpTA -0.235<br>
ListgpTQ -0.288  0.636</p>

<blockquote>
  <p></p>
</blockquote>

<p>Appreciating your help,</p>

<p>Shad</p>
"
"0.152028587660621","0.144170799399664","178682","<p>I'm using a binomial glmer mixed effects model using and I have two questions.</p>

<p>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")

summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + stimulus + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5154.3 5259.5 -2562.2 5124.3 8193

Scaled residuals: Min 1Q Median 3Q Max -25.0764 -0.2706 -0.1939 0.2472 10.5131

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.743 1.32
Number of obs: 8208, groups: Listener, 228

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) 2.7561 0.2657 10.371 &lt; 2e-16 * ListgpTA 0.1741 0.3147 0.553 0.580128
ListgpTQ 0.0810 0.2575 0.315 0.753094
stimulushaaDD -5.4415 0.2071 -26.272 &lt; 2e-16 stimulushad -4.2953 0.1822 -23.569 &lt; 2e-16 stimulushaDD -5.4946 0.2086 -26.337 &lt; 2e-16 stimulushid -5.1519 0.1994 -25.832 &lt; 2e-16 stimulushiDD -0.6708 0.1801 -3.724 0.000196 stimulushiid -5.8124 0.2186 -26.593 &lt; 2e-16 stimulushiiDD -5.5101 0.2091 -26.353 &lt; 2e-16 stimulushud -0.2016 0.1915 -1.053 0.292345
stimulushuDD -5.6188 0.2123 -26.462 &lt; 2e-16 stimulushuud -5.6107 0.2121 -26.450 &lt; 2e-16 *

stimulushuuDD -5.3207 0.2038 -26.109 &lt; 2e-16 ***

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD ListgpTA -0.613
ListgpTQ -0.755 0.636
stimulushaaDD -0.394 -0.007 0.004
stimulushad -0.440 -0.006 0.005 0.605
stimulushaDD -0.392 -0.007 0.003 0.555 0.601
stimulushid -0.407 -0.007 0.004 0.572 0.624 0.569
stimulushiDD -0.414 0.000 0.001 0.534 0.606 0.530
stimulushiid -0.376 -0.006 0.003 0.536 0.578 0.533
stimulushiiDD -0.391 -0.007 0.003 0.554 0.600 0.551
stimulushud -0.386 0.000 0.000 0.497 0.564 0.493
stimulushuDD -0.385 -0.007 0.003 0.548 0.592 0.545
stimulushuud -0.386 -0.007 0.003 0.548 0.593 0.545
stimulushuuDD -0.400 -0.007 0.004 0.564 0.613 0.561
stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD 0.554
stimulushiid 0.549 0.506
stimulushiiDD 0.568 0.529 0.533
stimulushud 0.516 0.569 0.471 0.492
stimulushuDD 0.562 0.521 0.527 0.544 0.484
stimulushuud 0.562 0.522 0.528 0.545 0.485
stimulushuuDD 0.579 0.543 0.542 0.560 0.505
stimulushuDD stimulushuud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD
stimulushiid
stimulushiiDD
stimulushud
stimulushuDD
stimulushuud 0.539
stimulushuuDD 0.554 0.554
</code></pre>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"") summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5218.3 5253.4 -2604.2 5208.3 8203

Scaled residuals: Min 1Q Median 3Q Max -21.9276 -0.2804 -0.2059 0.2740 9.4275

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.676 1.294
stimulus (Intercept) 4.949 2.225
Number of obs: 8208, groups: Listener, 228; stimulus, 12

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.3754 0.6792 -2.025 0.0429 * ListgpTA 0.2284 0.3073 0.743 0.4572

ListgpTQ 0.1432 0.2513 0.570 0.5687

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA ListgpTA -0.235
ListgpTQ -0.288 0.636
</code></pre>
"
"0.209197696327652","0.214917138552089","181559","<p>I am currently trying to get model-averaged estimates (and confidence intervals) for a GLMM I am running.</p>

<p>After obtaining a full set of candidate models using the <code>dredge()</code> function from the <a href=""https://cran.r-project.org/web/packages/MuMIn/index.html"" rel=""nofollow"">MuMIn package</a> in R and selecting the 95% confidence set using:  </p>

<pre><code>avgmod.95 &lt;- model.avg(all.mod, subset = cumsum(weight) &lt;= .95)
</code></pre>

<p>'summary(avgmod.95)' gives me both the full and conditional parameter estimates (see below output of the reproducible example). The full estimates treat every parameter equally and averages them over all models (i.e., even those not containing the parameter, for which it gives the parameter a value of 0). These are the parameter estimates I'd like to report.</p>

<p>So far so good, but I'd also like to report confidence intervals. Running the function <code>confint(avgmod.95)</code> (as suggested in several papers) gives one confidence interval for each parameter (see reproducible example output below). However, these confidence intervals match the <em>conditional</em> parameter estimates much more closely than they do the <em>full</em> parameter estimates (which I am interested in). </p>

<p>My questions are:  </p>

<ul>
<li><strong>Does the object generated with <code>model.avg()</code> only produce one set of confidence intervals, or does <code>confint()</code> give confidence intervals based on the conditional averages only?</strong>  </li>
<li><strong>If the latter is the case, is there a way around this to obtain CI's for the full averages?</strong></li>
</ul>

<p>Here's a reproducible example using the <code>mtcars</code> data:</p>

<pre><code>require(lme4)
require(MuMIn)

# Fit the global model
global.model &lt;- lmer(mpg ~ hp + drat*wt + (1|gear), data=mtcars)

# Set na.action to na.fail for dredge function to run
options(na.action=""na.fail"")

# Generate full model set
dr &lt;- dredge(global.model)

# Generate model average values of 95% confidence set of models
avgmod.95 &lt;- model.avg(dr, subset = cumsum(weight) &lt;= .95)

# Get a summary of model averaged values (both full and conditional)
summary(avgmod.95)

# Get CI's for model parameter averages
confint(avgmod.95)
</code></pre>

<p>Now <code>summary(avgmod.95)</code> gives: </p>

<pre><code>Call:
model.avg.model.selection(object = dr, subset = cumsum(weight) &lt;= 
    0.95)

Component model call: 
lmer(formula = mpg ~ &lt;4 unique rhs&gt;, data = mtcars)

Component models: 
     df logLik   AICc delta weight
134   6 -73.31 161.97  0.00   0.76
1234  7 -73.53 165.73  3.75   0.12
13    5 -77.29 166.89  4.92   0.07
123   6 -75.89 167.14  5.16   0.06

Term codes: 
   drat      hp      wt drat:wt 
      1       2       3       4 

Model-averaged coefficients:  
(full average) 
             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)  
(Intercept) 11.208222  13.650695   14.120116   0.794   0.4273  
drat         6.851292   3.665723    3.784583   1.810   0.0702 .
wt           3.244549   4.455566    4.586870   0.707   0.4793  
drat:wt     -2.276614   1.341948    1.377762   1.652   0.0985 .
hp          -0.005121   0.011796    0.011856   0.432   0.6658  

(conditional average) 
             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)   
(Intercept) 11.208222  13.650695   14.120116   0.794  0.42733   
drat         6.851292   3.665723    3.784583   1.810  0.07025 . 
wt           3.244549   4.455566    4.586870   0.707  0.47935   
drat:wt     -2.594754   1.107692    1.156718   2.243  0.02488 * 
hp          -0.029439   0.009162    0.009600   3.066  0.00217 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Relative variable importance: 
                     drat wt   drat:wt hp  
Importance:          1.00 1.00 0.88    0.17
N containing models:    4    4    2       2
</code></pre>

<p>and <code>confint(avgmod.95)</code> gives:</p>

<pre><code>                   2.5 %      97.5 %
(Intercept) -16.46669809 38.88314109
drat         -0.56635437 14.26893779
wt           -5.74555075 12.23464840
drat:wt      -4.86187937 -0.32762871
hp           -0.04825531 -0.01062219
</code></pre>

<p>Notice how the CI's confirm the conditional averages and not the full averages from the summary output.</p>
"
"0.0540146129294635","0.0554913665617863","185435","<p>I have a dataset with 3 colonies (A, B and C, see below). Each colony is divided into 2 treatments: control and DWV. I use a GLM to test wether there is a difference of the life expectancy <code>last.scan</code>between the treatment groups for each of the colonies. I am using the following command with <code>treatment</code>as a fixed factor and <code>colony</code>as a random factor:</p>

<pre><code>    fit_life = lmer(last.scan~treatment + (1|colony), data = data)
    Anova(fit_life, type = 3) # Type of treatment has a significant effect on the on the life expectancy.
    Response: last.scan
          Chisq Df Pr(&gt;Chisq)    
    (Intercept) 106.976  1  &lt; 2.2e-16 ***
    treatment    25.373  1  4.724e-07 ***
</code></pre>

<p>However, I am not sure about my strategy. I do not know how to find the difference for each of the colonies. Can I do that with a posthoc Tukey test?</p>

<p>I tested for overdispersion, but the proposed model has a lower AIC without overdispersion.</p>

<p>Here is an example of the dataset, RFID is an ID code.</p>

<pre><code>    &gt; head(data, 30)
                      RFID colony treatment last.scan
1  A0 01 03 C0 00 C0 20 01      A   Control        24
2  A0 01 03 C0 00 C0 20 0C      A   Control        21
3  A0 01 03 C0 00 C0 20 1D      A   Control        19
4  A0 01 03 C0 00 C0 20 1E      A   Control        18
5  A0 01 03 C0 00 C0 20 1F      A   Control        31
6  A0 01 03 C0 00 C0 20 21      A   Control        19
7  A0 01 03 C0 00 C0 20 2F      A   Control        18
8  A0 01 03 C0 00 C0 20 37      A   Control        16
9  A0 01 03 C0 00 C0 20 5E      A   Control        23
10 A0 01 03 C0 00 C0 20 79      A   Control        19
11 A0 01 03 C0 00 C0 20 7F      A   Control        17
12 A0 01 03 C0 00 C0 20 8C      A   Control        24
13 A0 01 03 C0 00 C0 20 92      A   Control        26
14 A0 01 03 C0 00 C0 20 95      A   Control        19
15 A0 01 03 C0 00 C0 20 98      A   Control        21
16 A0 01 03 C0 00 C0 20 B8      A   Control        21
17 A0 01 03 C0 00 C0 20 B9      A   Control        20
18 A0 01 03 C0 00 C0 20 D5      A   Control        17
19 A0 01 03 C0 00 C0 20 D9      A   Control        27
20 A0 01 03 C0 00 C0 20 E4      A   Control        26
21 A0 01 03 C0 00 C0 20 FE      A   Control        31
22 A0 01 03 C0 00 C0 3A 02      A       DWV        11
23 A0 01 03 C0 00 C0 3A 0C      A       DWV        22
24 A0 01 03 C0 00 C0 3A 0D      A       DWV        12
25 A0 01 03 C0 00 C0 3A 11      A       DWV        19
26 A0 01 03 C0 00 C0 3A 14      A       DWV        21
27 A0 01 03 C0 00 C0 3A 1D      A       DWV        24
28 A0 01 03 C0 00 C0 3A 24      A       DWV         9
29 A0 01 03 C0 00 C0 3A 2A      A       DWV        16
30 A0 01 03 C0 00 C0 3A 2C      A       DWV        23
</code></pre>
"
"0.147925109681887","0.151969366063391","186825","<p>I am trying to run a mixed model on over-dispersed non-integer data. My data are not counts, but are zero-inflated and over dispersed. The variable is distance (how far a gps point is from a central location) and as such looks like: 0.33, 64.73, 5.2 etc. I have been using a quasi-Poisson distribution as I have read that quasi can handle non-integer data (both Poisson and negative binomial cannot). I am using the <code>glmmPQL</code> function in package MASS as this allows quasi distributions with a random term (the identity of the individual that the gps point comes from).The functions <code>glmm</code> and <code>lmer</code> do not work with a quasi-Poisson distribution. Plotting the residuals indicates a lack of fit of this model.log-transforming the data to try and make it normal before hand also fails (the Shapiro-test for normality is significant). I am unsure how to fix this, as I seemingly have to use a quasi-distribution (link=""log"") because my data is not counts, non-integer and not normal but there is still overdispersion and lack of fit when using this distribution. </p>

<p>My question therefore is: <strong>How to model over-dispersed, non-integer data in a mixed model when quasi-Poisson does not seem to work?</strong>   </p>

<p>My code so far is:</p>

<pre><code>summary(glmmPQL(distance_from_centroid~Chick.Juv.Adult+Summer_winter, 
                random=~1|markingnumber, family=quasipoisson(link=""log""),
                data=centroid_distances))
</code></pre>

<p>Which results in:  </p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
 Data: centroid_distances 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 `Formula: ~1 | markingnumber
        (Intercept) Residual
StdDev:    1.157381 2.136811

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: distance._from_centroid ~ Chick.Juv.Adult + Summer_winter 
                      Value  Std.Error  DF   t-value p-value
(Intercept)       2.0670095 0.09403952 695 21.980221  0.0000
Chick.Juv.AdultC -0.2945360 0.06686399 695 -4.405002  0.0000
Chick.Juv.AdultJ -0.2005831 0.06727181 695 -2.981682  0.0030
Summer_winterW    0.1207721 0.04324588 695  2.792684  0.0054
 Correlation: 
                 (Intr) C.J.AC C.J.AJ
Chick.Juv.AdultC -0.565              
Chick.Juv.AdultJ -0.512  0.736       
Summer_winterW   -0.267  0.134  0.043

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.53759073 -0.48277169 -0.31041612  0.06314122  7.48672836 

Number of Observations: 1009
Number of Groups: 311 
</code></pre>

<p>Which when plotting the residuals gives me:</p>

<p><a href=""http://i.stack.imgur.com/3SKVU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3SKVU.jpg"" alt=""plot of residuals""></a></p>
"
"0.187436673802426","0.181234044274823","187499","<p>Background:
I have data on time to infection across multiple sites across a gradient. The design involves 2 latitudes (In and Out) with sites 1 and 2 nested within â€œInâ€ and sites 3 and 4 nested within â€œOut.â€ Within each of the four sites I have three transects and within each of the transects I have 5 plates.</p>

<p>Each of the plates is explicitly nested within each of the transects and each of the transects is explicitly nested within each of the sites. </p>

<p>I am interested in knowing if there is significant difference in infection across the In and Out gradient as well as across the four sites. Therefore I have made both gradient and site fixed factors (also because they have fewer than 4 levels). I am also interested in looking to see how much of the total variance is due to the transects and the plates, although this is secondary to the first question.</p>

<p><a href=""http://i.stack.imgur.com/ow2PH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ow2PH.jpg"" alt=""Expdesign""></a></p>

<p>Potential model specifications: </p>

<pre><code>a &lt;- glmer(Response~Gradient+Site +(1|Site), 
           data=surv, family=""poisson"", nAGQ=7)) 
b &lt;- glmer(Response~Gradient+Site +(1|Transect)+(1|Plate), 
           data=surv, family=""poisson"") 
c &lt;- glmer(Response~Gradient+Site +(1|Transect), 
           data=surv, family=""poisson"", nAGQ=7))
d &lt;- glmer(Response~Gradient+Site +(1|Transect/Plate), 
           data=surv, family=""poisson"", nAGQ=7))
e &lt;- glmer(Response~Gradient+Site +(1|Transect/Plate)+(1|Gradient/Site), 
           data=surv, family=""poisson""))
</code></pre>

<p>Questions:  </p>

<ol>
<li><p>I have seen examples were the lowest level of nesting was not put into the model specification. For example, not putting in â€œplateâ€ in my case (model a and c). How do you know to include the lowest level of your sampling design?  Would it be correct to compare the AICc values for each model and drop the models that include the lowest level of nesting (plate) if they have AICc values larger than the models that exclude them? My understanding is that comparing AICc is valid here as the fixed effects are the same across the models and only the random effects are changing.</p></li>
<li><p>If the answer to question 1 is yes, it is justifiable to discard models b, d, and e as they have higher AICc values than a, c. Now to choose between a and c. Including â€œsiteâ€ as both a fixed and random factor significantly changes the model outcomes compared to the other models. What would be the interpretation of having both site as a random and fixed effect? I have seen other models specified this way, that is why I am asking.</p></li>
<li><p>Is it justifiable to break up the models into two?  I ask because I often get the message: "" fixed-effect model matrix is rank deficient so dropping 1 column / coefficientâ€ which makes me think I donâ€™t have enough data for the number of terms I have included in the model. </p></li>
</ol>

<p>For example: </p>

<pre><code>f &lt;- glmer(Response~Gradient+(1|Site),  data=surv, family=""poisson"", nAGQ=7)) 
g &lt;- glmer(Response~Site+( 1|Transect), data=surv, family=""poisson"")) 
</code></pre>

<p>Note: I will check for overdispersion as I am using a Poisson distribution.</p>

<p>Example data:</p>

<pre><code>Gradient    Site    Transect    Plate   Response
In  Site1   Transect1   6   11
In  Site1   Transect1   18  27
In  Site1   Transect1   28  51
In  Site1   Transect2   3   19
In  Site1   Transect2   29  19
In  Site1   Transect2   36  27
In  Site1   Transect2   43  51
In  Site1   Transect3   19  19
In  Site1   Transect3   25  27
In  Site1   Transect3   9   19
In  Site1   Transect3   46  NA
In  Site1   Transect3   49  19
In  Site2   Transect4   5   27
In  Site2   Transect4   16  20
In  Site2   Transect4   24  20
In  Site2   Transect4   42  27
In  Site2   Trasect5    20  20
In  Site2   Trasect5    33  20
In  Site2   Trasect5    7   27
In  Site2   Transect6   2   20
In  Site2   Transect6   38  20
In  Site2   Transect6   48  27
In  Site2   Transect6   17  62
In  Site2   Transect6   21  20
Out Site3   Trasect7    4   19
Out Site3   Trasect7    26  19
Out Site3   Trasect7    27  51
</code></pre>
"
"0.0884021615653596","0.0908190670306883","189648","<p>I have run an AIC model selection in R, (following Grueber etal 2011 J.Evo.Bio), and standardised my global model to a mean of 0 and SD of 2, using the ""arm"" package.</p>

<p>AICc selection identified 2 models, so I model-averaged across these two models to provide a final model summary to report in my paper.</p>

<p>However, I want to be able to report a real slope value, rather than a transformed one. How do I back transform a parameter estimate?</p>

<p>Analysis:</p>

<p>I want to test if treatment (kept in air or ice) affects nitrogen (dN) within mantis shrimps over time. I have repeated measures per mantis shrimp (unique.id) that I use as a random factor to account for non-independence within an individual.</p>

<p>my code:</p>

<pre><code>model.1&lt;-lmer(dN ~ time * air.ice + (1|unique.id), data=mantis, REML=FALSE)

stdz.mod&lt;-standardize(model.1, standardize.y=FALSE)

model.set&lt;-dredge(stdz.mod)

top.models&lt;-get.models(model.set, subset=delta &lt; 7)
model.sel(top.models) 
# This identifies two models - the interaction model and main effects model

model.average&lt;-model.avg(top.models) 
summary(model.average) 
</code></pre>

<p>output: </p>

<pre><code>Model-averaged coefficients: 
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                  12.48242    0.13051  95.645  &lt; 2e-16 ***
c.air.ice                    -1.07202    0.26102   4.107 4.01e-05 ***
z.time                        0.50782    0.06954   7.302  &lt; 2e-16 ***
c.air.ice:z.time             -0.38008    0.13722   2.770  0.00561 ** 
</code></pre>

<p>I believe in the summary, that ""z.time"" is the slope estimate for ""air"" treatment. 
This value is far too high (I would expect something like 0.005), and I 
presume this is becuase it was standardised then averaged. How do I produce 
a real slope estimate from this value? </p>

<p>Dataframe = mantis:</p>

<pre><code>time    air.ice     dN      unique.id
2       ice     11.7089     33
2       ice     11.2278     34
2       ice     11.8489     35
2       ice     11.5513     36
2       air     12.4128     37
2       air     11.966      38
2       air     11.9962     39
2       air     12.7095     40
30      ice     11.8607     33
30      ice     11.7557     34
30      ice     12.5074     35
30      ice     11.9425     36
30      air     12.9556     37
30      air     12.8848     38
30      air     12.5634     39
30      air     13.8633     40
58      ice     12.0178     33
58      ice     11.6344     34
58      ice     12.2158     35
58      ice     11.9903     36
58      air     13.1065     37
58      air     12.7058     38
58      air     12.6269     39
58      air     13.7472     40
93      ice     11.9723     33
93      ice     11.7992     34
93      ice     12.4468     35
93      ice     11.8938     36
93      air     13.1773     37
93      air     13.1525     38
93      air     12.8422     39
93      air     14.2871     40
120     ice     12.0828     33
120     ice     11.6767     34
120     ice     12.545      35
120     ice     12.2501     36
120     air     13.2407     37
120     air     13.1212     38
120     air     12.7531     39
120     air     14.25655    40
</code></pre>
"
"0.233890134862494","0.240284665666106","189933","<p>I am seeking advice on how to effectively eliminate autocorrelation from a linear mixed model. My experimental design and explanation of fixed and random factors can be found here from an earlier question I asked: </p>

<p><a href=""http://stats.stackexchange.com/questions/188929/crossed-fixed-effects-model-specification-including-nesting-and-repeated-measure"">Crossed fixed effects model specification including nesting and repeated measures using glmm in R</a></p>

<p>I have treated day as numeric even though I only have four sampling time points (so I could treat it as a categorical predictor as well). Aside: Although four sample points is very few, I donâ€™t think that this is the root of the problem as this same dataset is giving me this residual autocorrelation issues using a different response variable that has 24 time points.</p>

<p>My issue is that I have tried a number of different autocorrelation structures and canâ€™t seem to achieve the random, non-significant residuals needed to confirm a lack of autocorrelation. I am using the function <code>lme</code> in the R package <code>nlme</code> to deal with autocorrelation. </p>

<p>I have tried the various autocorrelation classes  with variations to form</p>

<p>1) <code>corAR1</code> (autoregressive process of order 1).</p>

<p>2) <code>corARMA</code> (autoregressive moving average process)</p>

<p>3) <code>corCAR1</code> (continuous autoregressive process)</p>

<p>4) <code>corGaus</code> (Gaussian spatial correlation)</p>

<p>With form varying in the following ways with these different autocorrelation classes:</p>

<pre><code>form=~1
form=~1| TankNumb/RecruitID2
form=~Day| TankNumb/RecruitID2
</code></pre>

<p>If we look at a model without the time factor ""Day"" added, the ACF and PACF plots look like this. </p>

<pre><code>lme4_lognormal_notime&lt;-lmer(Arealog~Temperature*Culture+(1|TankNumb/RecruitID2), data=growthSR_noNA)

acf(residuals(lme4_lognormal_notime, retype=""normalized""))
pacf(residuals(lme4_lognormal_notime, retype=""normalized""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/xVdrb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xVdrb.jpg"" alt=""enter image description here""></a></p>

<p>Also, if I look at the residuals of the model without â€œDayâ€ included, I do not see any strong pattern in the residuals that would make me think there is a temporal autocorrelation problem.</p>

<pre><code>plot(residuals(lme4_lognormal_Ben_notime, retype=""normalized"")~growthSR_noNA$Day)
</code></pre>

<p><a href=""http://i.stack.imgur.com/wU1ZC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wU1ZC.jpg"" alt=""enter image description here""></a></p>

<p>Now for two different models with autocorrelation structure to hopefully eliminate autocorrelation:</p>

<pre><code>nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/Oe3et.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oe3et.jpg"" alt=""enter image description here""></a></p>

<pre><code>nlme_lognormal_mult_cortime&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~Day|TankNumb/RecruitID2), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pUgA1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUgA1.jpg"" alt=""enter image description here""></a></p>

<pre><code>ARMA_nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corARMA(form=~1, p=0, q=1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/6TMeL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6TMeL.jpg"" alt=""enter image description here""></a></p>

<p>The AIC suggests that the simplest correlation structure is the best. </p>

<pre><code>AIC(nlme_lognormal_mult,nlme_lognormal_mult_cor, nlme_lognormal_mult_cortime,ARMA_nlme_lognormal_Ben_mult_cor)

                               df      AIC
nlme_lognormal_mult              15 1233.997
nlme_lognormal_mult_cor          16 1184.389
nlme_lognormal_mult_cortime      16 1235.997
ARMA_nlme_lognormal_Ben_mult_cor 16 1198.451
</code></pre>

<p>As I mentioned above, I have tried a number of different <code>cor</code> functions (the four listed above) and different <code>form</code> specifications. They all end up with ACF/PCF plots like the last two models with a first lag at below 0.2 in the ACF plot and a PCF plot with the first three lags around 0.10.</p>

<p>I have also read a number of sites describing how to specify corARMA models based on diagnosing the ACF plots and have tried a number of variations of p and q parameters. </p>

<p>Questions: </p>

<ol>
<li>Does anyone have some advice on which type of correlation structure that might elimate this autocorrelation problem based on the patterns in my ACF/PCF plots? Should I be diagnosing based on a model with or without Day included?</li>
</ol>

<p>2.Is there ever an acceptable level of autocorrelation? 
This post (<a href=""http://stats.stackexchange.com/questions/80823/do-autocorrelated-residual-patterns-remain-even-in-models-with-appropriate-corre"">Do autocorrelated residual patterns remain even in models with appropriate correlation structures, &amp; how to select the best models?</a>) states that small amounts of autocorrelation probably won't impact the model coefficients very much. ""The estimate is slightly larger than zero so will have negligible effect on the model fit and hence you might wish to leave it in the model if there is a strong a priori reason to assume residual autocorrelation."" Potentially there is some autocorrelation that is not being caused by temporal autocorrelation, like outliers? Is there a cut-off, for example, autocorrelation below 0.1? I have extremely small 95% confidence intervals, so it doesn't take a lot of autocorrelation in my models to be significantly too much.</p>

<p>Any advice is appreciated! </p>
"
"0.306460826759913","0.33300324577919","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.277609016948829","0.270188353160797","197435","<p>This is my first time posting. I hope I've included an appropriate amount of info. I have many questions, but try to highlight the obstacles I've been facing.</p>

<p>I am trying to run three GLMMs in R (3.2.2) on three separate response variables (various measures of sociality between pairs of animals), but with the same set of fixed effects and interactions. Two of the response variables are integers, but the other is a continuous numeric. The response variables are:</p>

<ol>
<li>traveling together- out of all the times I followed my subject, how many of those times were the other individuals present (<strong>Travel</strong>)</li>
<li>proximity- out of all the instantaneous scans to see who was within 5m of my subject, how often was that other individual within 5m (<strong>Within_Five</strong>) </li>
<li>touching- out of all the hours/minutes I watched my subject how many minutes spent touching (<strong>total_touch</strong>)</li>
</ol>

<p>The fixed effects are features of the pair, such as age difference (<strong>Age_Diff</strong>) and whether the partner is the <strong>mother</strong>, <strong>brother</strong>, or <strong>cousin</strong>. I want to have age of the subject (<strong>subject_age</strong>) as an interaction as the social behavior may change with age.</p>

<p>Given Subjects (a subset of the population that I observed) and Partners (anyone in the population who they may be interacting with, including ) are repeated, I am treating both as random effects, e.g. (1 | Partner). </p>

<pre><code>&gt; str(dat)
'data.frame':   954 obs. of  29 variables:
 $ Pair          : Factor w/ 954 levels ""Abrams_Barron"",..: 159 268 269 378     700 334 601 920 179 75 ...
 $ Subject       : Factor w/ 18 levels ""Abrams"",""Barron"",..: 3 6 6 8 14 7 12 18 4 2 ...
  $ Partner       : Factor w/ 54 levels ""Abrams"",""Barron"",..: 54 3 4 7 11 16    18 19 21 23 ...
 $ mother        : Factor w/ 2 levels ""NoMom"",""Mom"": 1 1 1 1 1 1 1 1 1 1 ...
 $ brother       : Factor w/ 2 levels ""NoBro"",""bro"": 2 2 2 2 2 2 2 2 2 2 ...
 $ cousin        : Factor w/ 2 levels ""NoCuz"",""cuz"": 2 1 1 1 1 1 1 1 1 1 ...
 $ subject_age   : num  14.3 17.7 17.7 18.7 19.7 ...
 $ partner_age   : num  8.67 41.69 31.69 12.39 24.68 ...
 $ Age_Diff      : num  -5.59 24.01 14.01 -6.29 5 ...
 $ Total_Scans   : int  314 309 309 313 289 314 310 321 305 283 ...
 $ Total_Hours   : num  43.2 44.3 44.3 45.1 40.9 ...
 $ Total_Minutes : num  2593 2656 2656 2707 2456 ...
 $ Total_Follows : int  45 46 46 47 43 48 46 48 46 45 ...
 $ Within_Five   : int  9 13 12 20 26 10 6 4 30 9 ...
 $ total_touch   : num  0 0 1.77 6.19 31.07 ...
 $ Touch_Rate_Min: num  0 0 0.000667 0.002287 0.012649 ...
 $ Touch_Rate    : num  0 0 0.04 0.137 0.759 ...
 $ Travel        : int  25 28 27 24 30 21 23 5 26 17 ...
 $ Travel_Rate   : num  0.556 0.609 0.587 0.511 0.698 ...
</code></pre>

<p>I have used lme4 in the past, but since some of my response variables have lots of zeros, it seems appropriate to run a zero-inflation model, and I have been trying glmmADMB. With some of the models, I have been getting warning messages.</p>

<p>For the Travel model, I don't think I need a zero-inflation model, but I get a warning when I run a poisson model with glmer</p>

<pre><code>&gt; travel.poisson.FULL2 &lt;- glmer(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = poisson)
Warning messages:
1: Some predictor variables are on very different scales: consider rescaling 
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
&gt; summary(travel.poisson.FULL2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Travel ~ subject_age * brother + subject_age * Age_Diff + subject_age *      cousin + subject_age * mother + log(Total_Follows) + (1 |  
    Subject) + (1 | Partner)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
  8989.2   9052.4  -4481.6   8963.2      941 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5702 -1.7767 -0.3958  0.9003 16.4475 

Random effects:
 Groups  Name        Variance Std.Dev.
 Partner (Intercept) 0.25078  0.5008  
 Subject (Intercept) 0.01283  0.1133  
Number of obs: 954, groups:  Partner, 54; Subject, 18

Fixed effects:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -4.9997116  1.7689865  -2.826  0.00471 ** 
subject_age             0.0246246  0.0138692   1.775  0.07582 .  
brotherbro              1.2107468  0.4163344   2.908  0.00364 ** 
Age_Diff                0.0226314  0.0086427   2.619  0.00883 ** 
cousincuz               1.0076641  0.3756843   2.682  0.00731 ** 
motherMom               1.9488415  0.6762811   2.882  0.00396 ** 
log(Total_Follows)      1.7389194  0.4464744   3.895 9.83e-05 ***
subject_age:brotherbro -0.0362557  0.0260557  -1.391  0.16408    
subject_age:Age_Diff   -0.0002216  0.0003799  -0.583  0.55972    
subject_age:cousincuz  -0.0635028  0.0236412  -2.686  0.00723 ** 
subject_age:motherMom  -0.1105007  0.0413857  -2.670  0.00758 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) sbjct_ brthrb Ag_Dff cosncz mthrMm l(T_F) sbjct_g:b s_:A_D sbjct_g:c
subject_age -0.354                                                                     
brotherbro  -0.014  0.058                                                              
Age_Diff    -0.071  0.443 -0.001                                                       
cousincuz   -0.007  0.095  0.011  0.109                                                
motherMom   -0.001  0.000  0.023 -0.126 -0.004                                         
lg(Ttl_Fll) -0.990  0.227  0.006  0.001 -0.006  0.002                                  
sbjct_g:brt  0.015 -0.056 -0.988  0.005 -0.012 -0.025 -0.008                           
sbjct_g:A_D  0.027 -0.192  0.008 -0.714 -0.150  0.168 -0.002 -0.013                    
sbjct_g:csn  0.006 -0.092 -0.012 -0.106 -0.988  0.004  0.006  0.013     0.147          
sbjct_g:mtM  0.004 -0.002 -0.024  0.123  0.003 -0.991 -0.004  0.028    -0.169 -0.003   
fit warnings:
Some predictor variables are on very different scales: consider rescaling
convergence code: 0
Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>Whereas, when I run with glmmadmb, I don't get a warning</p>

<pre><code>&gt; travel.poisson.FULL &lt;- glmmadmb(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = F)
&gt; summary(travel.poisson.FULL)

Call:
glmmadmb(formula = Travel ~ subject_age * brother + subject_age * 
    Age_Diff + subject_age * cousin + subject_age * mother + 
    log(Total_Follows) + (1 | Subject) + (1 | Partner), data = dat, 
    family = ""poisson"", zeroInflation = F)

AIC: 7430.9 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)            -9.52e+00   6.32e+00   -1.50    0.132  
subject_age             3.28e-02   4.56e-02    0.72    0.471  
brotherbro              9.49e-01   4.29e-01    2.21    0.027 *
Age_Diff                2.70e-02   1.28e-02    2.11    0.035 *
cousincuz               5.36e-02   4.27e-01    0.13    0.900  
motherMom              -1.21e+00   8.39e-01   -1.45    0.148  
log(Total_Follows)      2.75e+00   1.60e+00    1.72    0.086 .
subject_age:brotherbro -2.85e-02   2.66e-02   -1.07    0.284  
subject_age:Age_Diff    2.64e-05   4.11e-04    0.06    0.949  
subject_age:cousincuz  -3.11e-03   2.70e-02   -0.12    0.908  
subject_age:motherMom   7.76e-02   4.99e-02    1.56    0.120  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=954, Subject=18, Partner=54 
Random effect variance(s):
Error in VarCorr(x) : 
  could not find symbol ""rdig"" in environment of the generic function
</code></pre>

<p>For the touching model (which has lots of 0s, so it seemed appropriate to run a zero inflation model), I also got a warning.</p>

<pre><code>touch.poisson.FULL &lt;- glmmadmb(total_touch ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Scans) +  (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = T)

&gt;Warning messages:
1: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  non-integer response values in discrete family
2: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  Convergence failed:log-likelihood of gradient= -1.15888
</code></pre>

<p>What accounts for the differences in warnings produced by glmmadmb and glmer?</p>

<p>Some related questions:</p>

<p>Is it appropriate to put all of these effects and interactions into one model? Do these warnings and differences between packages suggest my model is unstable?</p>

<p>I have been trying to learn as much as possible about GLMMs and using them in R. I know there are a lot of different statistical approaches, but I want to ensure I am modeling my data appropriately and producing reliable results. Any and all help or advice is appreciated, and I am happy to provide more information.</p>
"
"0.147925109681887","0.151969366063391","200164","<p>I have the following output for a logit mixed effect model in R (glmer) with a categorical outcome DV (correct vs. inversion), three categorical predictors, syntax ('s vs. of), anomaly (-AN +AN vs. +AN -AN), and group (adv vs. int vs. ns), random effects for participants and items, and random slope by syntax for participants:</p>

<blockquote>
  <p>Formula: Correct ~ Syntax * Animacy * Prof.Group.2 + (1 + Syntax | Part.name) +      (1 | Item)</p>
  
  <p>AIC      BIC   logLik deviance df.resid<br>
  649.5     727.9    -308.8     617.5       972 </p>
  
  <p>Scaled residuals:<br>
  Min      1Q  Median      3Q     Max<br>
  -1.7053 -0.3398 -0.2366 -0.1384  8.1297 </p>
  
  <p>Random effects:<br>
  Groups    Name        Variance Std.Dev. Corr<br>
  Part.name (Intercept) 0.5351   0.7315<br>
             Syntaxof    1.3095   1.1443   -0.20<br>
  Item      (Intercept) 0.6096   0.7808<br>
  Number of obs: 988, groups:  Part.name, 42; Item, 32</p>
  
  <p>Fixed effects:
                                   Estimate Std. Error z value Pr(>|z|)<br>
  (Intercept)                      -2.8948     0.7824  -3.700 0.000216***<br>
  Syntaxof                         2.1327     1.0346   2.061 0.039264 *<br>
  Animacy+AN -AN                   0.3792     0.7931   0.478 0.632578<br>
  Prof.Group.2int                  1.4940     0.7973   1.874 0.060938 .<br>
  Prof.Group.2ns                   0.4285     0.8092   0.529 0.596463<br>
  Syntaxof:Animacy+AN -AN          -3.8122     1.5389  -2.477 0.013243 *<br>
  Syntaxof:Prof.Group.2int         -4.2264     1.2750  -3.315 0.000917 ***<br>
  Syntaxof:Prof.Group.2ns          -2.1531     1.1017  -1.954 0.050658 .<br>
  Animacy+AN -AN:Prof.Group.2int   -0.8196     0.8305  -0.987 0.323708<br>
  Animacy+AN -AN:Prof.Group.2ns    -1.3309     0.9456  -1.408 0.159267<br>
  Syntaxof:Animacy+AN -AN:Prof.Group.2int4.0961     1.8099   2.263 0.023631 *<br>
  Syntaxof:Animacy+AN -AN:Prof.Group.2ns 2.8800     1.7587   1.638 0.101496    </p>
</blockquote>

<p>The output unit for the coefficients are log odds. I don't know how to code the data to test the following hypothesis:</p>

<p>(1) Are the log odds of a correct answer greater for +AN 's -AN compared to -AN of +AN?<br>
(2) Are the log odds of a correct answer greater for +AN of -AN compared to -AN 's +AN?</p>

<p>The contrasts shown above are treatment coded (i.e. summary(model)) and none of them actually test my hypotheses. The line </p>

<blockquote>
  <p>Syntaxof:Animacy+AN -AN          -3.8122     1.5389  -2.477 0.013243 *  </p>
</blockquote>

<p>tells me the difference between of and 's (reference level) in log odds is significantly smaller for +AN -AN compared to -AN +AN (reference level),  according to past posts on Cross-Validated.</p>

<p>Can anyone suggest a way to code the data to get these contrasts? I understand merging the two variables syntax and animacy into one predictor with four levels might be easier but this is not what the reviewers asked for. </p>

<p>Thank you.</p>
"
"0.192870746165604","0.198143811351516","207395","<p>I have bird nesting data and I am trying to see whether the nest treatment has any significant effects on the survival of the nestling. My original data set is relatively small (n=101). The response variable is binomial (No treatment = 0,  treatment = 1) as is the fixed effect (survived = 1, died = 0). </p>

<p>A copy of my original data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZed1pwMFh4ekhGV00/view?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p>I have obtained the following results from my model:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood  (Laplace Approximation)
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_original
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

  AIC      BIC   logLik deviance df.resid 
109.8    120.2    -50.9    101.8       97 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9725  0.1557  0.2853  0.3653  1.2021 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 3.2860   1.8127  
Year   (Intercept) 0.5109   0.7148  
Number of obs: 101, groups:  Nest, 39; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   1.6228     0.7258   2.236   0.0254 *
Treatment     0.9063     0.7676   1.181   0.2377  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment -0.152
</code></pre>

<p>To account for the possible influence of small sample size, I produced a bootstrap data set using the following code:</p>

<pre><code>bootstrapdata &lt;- data.frame()
for (i in 1:1000){
  boot &lt;- sample(1:nrow(Treatment_original), replace=TRUE)
  bootdata &lt;- Treatment_original[boot,]
  bootstrapdata &lt;- rbind(bootstrapdata, bootdata)
}
</code></pre>

<p>The bootstrap data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZeS3VQVHcwMmw4MkE/view?usp=sharing"" rel=""nofollow"">here</a>.</p>

<p>I then ran the above model on the bootstrap data set, which produced the following results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_bootstrap
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

 AIC      BIC   logLik deviance df.resid 
2957.0   2985.8  -1474.5   2949.0     9996 

Scaled residuals: 
  Min      1Q  Median      3Q     Max 
-3.5915  0.0001  0.0002  0.0026  2.4168 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 511.888  22.625  
Year   (Intercept)   4.251   2.062  
Number of obs: 10000, groups:  Nest, 38; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  16.0123     1.9144   8.364   &lt;2e-16 ***
Treatment     1.5813     0.1465  10.795   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment 0.009 
</code></pre>

<p>I would like to know how to interpret the bootstrap model results. Can I now say that the nest treatment had a positive effect on nestling survival?</p>

<p>The original data set showed no significant effect of nest treatment. Should I rather be using these results and adjusting the p value for false discovery rate?</p>

<p>I am unsure as to which results are correct. Should I report both results? What inferences can I make from these results? </p>
"
"0.0661541201655622","0.0679627666030585","210344","<p>I have some whale tourism data that I am trying to model to see which factors significantly affect the number of encounters between whales and tourists. </p>

<p>I have two years worth of data and have converted the data into presence absence data for individual whales during each month. I am using the MuMIN package to dredge my a global model with an optimal random effects structure that I have already determined by comparing AIC. </p>

<p>My global model is: </p>

<pre><code>Par5 &lt;- glmer(PA ~ Hours + Length + Sex
          +Hours*Length + Hours*Sex + Length*Sex 
          + (1|Id) + (1|Year/Month), data=edata, family=binomial)
</code></pre>

<p>Where PA= presence/absence (1 or 0)</p>

<p>Hours = log transformed tour hours</p>

<p>Length= length of the whale</p>

<p>Sex=sex of the whale</p>

<p>Id = whale ID</p>

<p>and I also have year and month </p>

<p>in my table of top ranked models I am finding that my null model is the 4th best ranked model with a delta AIC of 2.33 and I get a warning when I include interactions: 
1  Model failed to converge with max|grad| = 0.0229445
2 Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?</p>

<p>I suspect that something is going very wrong, if someone could shed some light as to what is going very wrong, I'd be very appreciative.</p>

<p>Alicia</p>
"
"0.114582297256771","0.117714964779443","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.144936415546343","0.173715454915118","212397","<p>I would like to test the effect of a treatment (""crop"") on species richness. I would rather use a glm for richness as it is a kind of count data.</p>

<p>Besides, I have a nested sampling design (5 values per plot, 5 plot per treatment). Thus I should use a GLMM.</p>

<p>So I write my model :</p>

<pre><code>&gt; GLMM_ric = glmer(richness ~ Crop + (1| Plot),  family=poisson)
&gt; summary(GLMM_ric)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
 glmerMod]
 Family: poisson ( log )
 Formula: richness ~ Crop + (1 | Plot)
 Data: Com_agg

 AIC      BIC   logLik deviance df.resid 
433.8    446.9   -211.9    423.8       95 

Scaled residuals: 
   Min       1Q   Median       3Q      Max 
-1.33174 -0.41445 -0.08382  0.39853  1.73324 

Random effects:
 Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.08432  0.2904  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   1.9621     0.1503  13.056   &lt;2e-16 ***
 CropM        -0.5351     0.2211  -2.420   0.0155 *  
 CropYR       -0.3814     0.2181  -1.748   0.0804 .  
 CropOR       -0.3393     0.2175  -1.560   0.1188    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Correlation of Fixed Effects:
        (Intr) CropM  CropYR
 CropM  -0.678              
 CropYR -0.686  0.467       
 CropOR -0.687  0.468  0.475
</code></pre>

<p>and then a simpler model to compare with :</p>

<pre><code> &gt; GLMM_ric0 = glmer(richness ~ (1| Plot), data=Com_agg, family=poisson,    glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))

 &gt;summary(GLMM_ric0)

 Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod]
  Family: poisson ( log )
 Formula: richness ~ (1 | Plot)
    Data: Com_agg
 Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

 AIC      BIC   logLik deviance df.resid 
 433.3    438.5   -214.7    429.3       98 

 Scaled residuals: 
 Min       1Q   Median       3Q      Max 
 -1.27211 -0.39830 -0.03309  0.38204  1.66734 

 Random effects:
  Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.1251   0.3537  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)  1.64739    0.09114   18.07   &lt;2e-16 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And then I compare both models :</p>

<pre><code>&gt; anova(GLMM_ric0, GLMM_ric)
Data: Com_agg
Models:
GLMM_ric0: richness ~ (1 | Plot)
GLMM_ric: richness ~ Crop + (1 | Plot)
              Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
GLMM_ric0  2 433.32 438.53 -214.66   429.32                         
GLMM_ric   5 433.84 446.86 -211.92   423.84 5.4851      3     0.1395
</code></pre>

<p>So according to my anova, the factor ""crop"" is not significant. Yet in the summary of my model some of the modalities appear to be significant. How should I interpret this ?</p>

<p>I have looked around for a while (e.g. <a href=""http://stats.stackexchange.com/questions/9587/glmm-test-of-significance"">here</a> or <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">here</a>) but I could not find much for this precise situation.</p>
"
"0.175027350160361","0.179812578843619","213072","<p>Iâ€™m using the <code>glmer</code> function from the <code>lme4</code> package in <code>R</code> to model species richness adjacent to aquaculture sites. I have 6 sites: 2 in production, 2 were in production the last years but not anymore at the time of the sampling (fallow), and 2 that were never under production (references). Photographs along transects away from the aquaculture sites were taken each 20-40 m from 0 to 200 m and reference sites were at 1500 m from aquaculture sites. These transects were repeated 7 times over a period of 2 years to determine if the community changed over time.</p>

<p>Iâ€™ve followed the steps described in the excellent book from Zuur et al. (2009) <em><a href=""http://rads.stackoverflow.com/amzn/click/1441927646"" rel=""nofollow"">Mixed Effects Models and Extensions in Ecology with R</a></em> and my best model is:</p>

<p>(<em>Note that predictors</em> <code>Distance</code>, <code>Depth</code> <em>and</em> <code>Beggiatoa.sp.</code> <em>have been standardized to remove an</em> <code>lme4</code> <em>error message.</em>)</p>

<pre><code>glmm.8 &lt;- glmer(sr ~ Distance+Depth+fSubstrate+Beggiatoa.sp.+
                     Distance:Beggiatoa.sp.+(1|fSite),
                glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)),
                family=poisson, data=datsc)

summary(glmm.8)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
   ['glmerMod']
Family: poisson  ( log )
Formula: sr ~ Distance + Depth + fSubstrate + Beggiatoa.sp. + 
   Distance:Beggiatoa.sp. +      (1 | fSite)
Data: datsc
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

   AIC      BIC   logLik deviance df.resid 
2279.7   2328.8  -1129.9   2259.7      992 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5171 -0.6376 -0.2008  0.4326  4.9375 

Random effects:
Groups Name        Variance Std.Dev.
fSite  (Intercept) 0.1831   0.4279  
Number of obs: 1002, groups:  fSite, 6

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             1.49171    0.50388   2.960 0.003072 ** 
Distance                2.05809    0.59940   3.434 0.000596 ***
Depth                  -0.09093    0.02966  -3.066 0.002171 ** 
fSubstrateCoarse       -0.09929    0.08299  -1.196 0.231514    
fSubstrateFine         -0.62376    0.08606  -7.248 4.24e-13 ***
fSubstrateFloc         -1.75314    0.30211  -5.803 6.51e-09 ***
fSubstrateMedium       -0.35201    0.07625  -4.617 3.90e-06 ***
Beggiatoa.sp.           2.42190    1.09521   2.211 0.027011 *  
Distance:Beggiatoa.sp.  3.30995    1.37755   2.403 0.016271 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Distnc Depth  fSbstC fSbstrtFn fSbstrtFl fSbstM Bggt..
Distance     0.885                                                       
Depth        0.052  0.027                                                
fSubstrtCrs -0.076 -0.028 -0.024                                         
fSubstratFn -0.177 -0.058 -0.145  0.325                                  
fSubstrtFlc  0.047  0.132 -0.022  0.066  0.155                           
fSubstrtMdm -0.088 -0.029 -0.102  0.314  0.380     0.097                 
Beggiat.sp.  0.927  0.947  0.039 -0.024 -0.088     0.080    -0.027       
Dstnc:Bgg..  0.925  0.950  0.037 -0.024 -0.089     0.119    -0.027  0.996
</code></pre>

<p><strong>My question is: How do I validate this model to see if it meets the required assumptions?</strong></p>

<p>I did a series of plots but I'm not sure if they are the appropriate ones and if they are, if they violate the assumptions.</p>



<pre><code>EP &lt;- residuals(glmm.8,type=""pearson"")
plot(EP~fitted(glmm.8))
</code></pre>

<p><a href=""http://i.stack.imgur.com/K8YnQ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/K8YnQ.jpg"" alt=""enter image description here""></a></p>



<pre><code>qqnorm(EP)
qqline(EP)
</code></pre>

<p><a href=""http://i.stack.imgur.com/owp47.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/owp47.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Distance, EP, xlab=""Distance"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/4X05s.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4X05s.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Depth, EP, xlab=""Depth"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/cKdYC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cKdYC.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$fSubstrate, EP, xlab=""Substrate"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/LUgG8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LUgG8.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Beggiatoa.sp., EP, xlab=""Beggiatoa.sp."", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/cg3oO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cg3oO.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(fitted(glmm.8)~predict(glmm.8))
</code></pre>

<p><a href=""http://i.stack.imgur.com/LgHDV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LgHDV.jpg"" alt=""enter image description here""></a></p>



<p>I looked on this and other websites and I couldn't find a ""perfect"" method to validate Poisson GLMM models. I believe a good answer to my question would be relevant to many people.  If needed I can provide a subset of my data but this question can probably be answered without it. Still, let me know if need it.</p>


"
"0.132308240331124","0.135925533206117","213470","<p><strong>Background</strong></p>

<p>Although my data should have a multinomial dependent variable, I have settled for a binary as I could not understand too much of MCMCglmm. The data is a time series cross sectional, so am looking at each individual outcome vis-a-vis the others. I don't have much experience with statistics, so I really need to know if am on the right path of actually coming up with values for a dynamic linear model or way off. Since i need effects from the independent variables.</p>

<p><strong><em>The data sample</em></strong></p>

<p>The data is for students who applied for university courses and were admitted within a period of 3 years. The data mainly has the grades in the subjects done in their pre-entry level exams. Each student can do a maximum of 4, but in the model below <code>NA</code> values are filled with <code>0</code> (Not sure if a correct assumption). </p>

<p><strong>The problem</strong></p>

<ol>
<li>How do I get time varying effects?</li>
<li>How do i extract the effect of time? </li>
<li>what does it imply when time is expressed as a random effect?</li>
</ol>

<p>Every input is highly appreciated, Thank you.</p>

<p>Below is a result from the <code>glmer</code> function with formula</p>

<p><code>glmllb2 &lt;- glmer(logi ~ history + c.r.e + economics + geography + literature + f.art + entrepreneurship + luganda + kiswahili + french + i.r.e + historyc + historycsq + (1 | called), family = binomial(""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 100, data = data.apriori.llb2)</code></p>

<p>where <code>history+c.r.e + ... + i.r.e</code> are subject grades that predict student admission into a course <code>logi</code> (as a binary) while <code>historyc</code> is a grand mean centered variable for history and <code>historycsq</code> is the squared variable for <code>historyc</code>. <code>called</code> is time in years re-scaled to <code>1,2 and 3</code></p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Adaptive Gauss-Hermite Quadrature, nAGQ = 100) [glmerMod]
 Family: binomial  ( logit )
Formula: logi ~ history + c.r.e + economics + geography + literature +  
    f.art + entrepreneurship + luganda + kiswahili + french +      i.r.e + historyc + historycsq + (1 | called)
   Data: data.apriori.llb2
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  1778.2   1874.8   -875.1   1750.2     7317 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
 -3.843  -0.123  -0.054  -0.025 213.612 

Random effects:
 Groups Name        Variance Std.Dev.
 called (Intercept) 0.09975  0.3158  
Number of obs: 7331, groups:  called, 3

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -13.27747    0.52128 -25.471  &lt; 2e-16 ***
history            0.55942    0.04656  12.016  &lt; 2e-16 ***
c.r.e              0.45941    0.03652  12.580  &lt; 2e-16 ***
economics          0.69835    0.04509  15.489  &lt; 2e-16 ***
geography          0.49442    0.04137  11.950  &lt; 2e-16 ***
literature         0.77936    0.04129  18.877  &lt; 2e-16 ***
f.art              0.50219    0.04387  11.447  &lt; 2e-16 ***
entrepreneurship   0.46377    0.04504  10.297  &lt; 2e-16 ***
luganda            0.49340    0.07643   6.456 1.08e-10 ***
kiswahili          0.52691    0.10498   5.019 5.20e-07 ***
french             0.65225    0.09133   7.142 9.22e-13 ***
i.r.e              0.59269    0.08265   7.171 7.44e-13 ***
historycsq         0.03721    0.01794   2.075    0.038 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) histry c.r.e  ecnmcs ggrphy litrtr f.art  entrpr lugand kiswhl french i.r.e 
history     -0.559                                                                             
c.r.e       -0.608  0.143                                                                      
economics   -0.340 -0.083 -0.002                                                               
geography   -0.569  0.187  0.624 -0.059                                                        
literature  -0.625  0.169  0.540  0.086  0.709                                                 
f.art       -0.614  0.231  0.588  0.118  0.525  0.585                                          
entrprnrshp -0.554  0.191  0.564 -0.031  0.583  0.654  0.596                                   
luganda     -0.305  0.086  0.289  0.065  0.305  0.337  0.283  0.281                            
kiswahili   -0.242  0.158  0.173  0.073  0.161  0.195  0.195  0.186  0.099                     
french      -0.265  0.079  0.314 -0.002  0.257  0.268  0.290  0.305  0.144  0.094              
i.r.e       -0.256  0.038  0.349  0.041  0.253  0.251  0.231  0.242  0.049  0.083  0.137       
historycsq   0.153 -0.253 -0.235 -0.107 -0.275 -0.204 -0.202 -0.231 -0.106 -0.112 -0.119 -0.102
fit warnings:
fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
</code></pre>
"
"0.175027350160361","0.166968823211932","214645","<p>I'm studying the effect of pH and cross-types on mortality of fish. Treatment is categorical (2 levels: control and low pH) and cross-types is also categorical (4 levels: parents wild male x wild female (WMWF), wild male x farmed female (WMFF), farmed male x wild female (FMWF), and farmed male x farmed female (FMFF)). There was 6 tanks in total (3 control and 3 at low pH) and each tank had 15 fish of each cross-type (60 fish total/tank). Since mortality is a count and that there was higher mortality in one of the control tank, I used Poisson GLMM to account for the tank effect.</p>

<p>Here's the model and summary results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: poisson  ( log )
Formula: mortality.count ~ Tr * Cross + (1 | Tank)
Data: pHdat

 AIC      BIC   logLik deviance df.resid 
93.8    104.4    -37.9     75.8       15 

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-1.1311 -0.4171 -0.2554  0.1608  1.2889 

Random effects:
Groups Name        Variance Std.Dev.
Tank   (Intercept) 2.225    1.492   
Number of obs: 24, groups:  Tank, 6

Fixed effects:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)       -1.666e+00  1.377e+00  -1.210   0.2264  
TrLOWpH            3.053e+00  1.647e+00   1.854   0.0637 .
CrossFMWF          9.810e-01  6.770e-01   1.449   0.1473  
CrossWMFF          9.810e-01  6.770e-01   1.449   0.1474  
CrossWMWF          2.248e-05  8.165e-01   0.000   1.0000  
TrLOWpH:CrossFMWF -1.754e+00  8.378e-01  -2.094   0.0363 *
TrLOWpH:CrossWMFF -1.243e+00  7.970e-01  -1.560   0.1188  
TrLOWpH:CrossWMWF -6.190e-01  9.415e-01  -0.658   0.5109  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) TrLOWH CrFMWF CrWMFF CrWMWF TLOWH:CF TLOWH:CWMF
TrLOWpH     -0.835                                                
CrossFMWF   -0.358  0.299                                         
CrossWMFF   -0.358  0.299  0.727                                  
CrossWMWF   -0.296  0.248  0.603  0.603                           
TLOWH:CFMWF  0.289 -0.297 -0.808 -0.588 -0.487                    
TLOWH:CWMFF  0.304 -0.313 -0.618 -0.849 -0.512  0.614             
TLOWH:CWMWF  0.257 -0.265 -0.523 -0.523 -0.867  0.520    0.547 
</code></pre>

<p>As you can see, the fish in low pH tanks of the cross FMWF died (weakly but still) significantly less than the baseline (FMFF).</p>

<p>Now I wanted to see if there was significant differences between the cross for each treatment (not only compared to the baseline) so I used <code>lsmeans</code>. Here's the results:</p>

<pre><code>lsmeans(glmm.0,pairwise~Tr*Cross,adjust=""tukey"")
$lsmeans
Tr    Cross     lsmean        SE df  asymp.LCL asymp.UCL
CTRL  FMFF  -1.6658411 1.3770371 NA -4.3647842  1.033102
LOWpH FMFF   1.3873820 0.9065822 NA -0.3894865  3.164251
CTRL  FMWF  -0.6848201 1.2991734 NA -3.2311531  1.861513
LOWpH FMWF   0.6141089 0.9548037 NA -1.2572719  2.485490
CTRL  WMFF  -0.6848677 1.2991756 NA -3.2312050  1.861470
LOWpH WMFF   1.1251162 0.9192163 NA -0.6765147  2.926747
CTRL  WMWF  -1.6658186 1.3770335 NA -4.3647547  1.033117
LOWpH WMWF   0.7683761 0.9422428 NA -1.0783859  2.615138

Results are given on the log (not the response) scale. 
Confidence level used: 0.95 

$contrasts
contrast                     estimate        SE df z.ratio p.value
CTRL,FMFF - LOWpH,FMFF  -3.053223e+00 1.6467764 NA  -1.854  0.5828
CTRL,FMFF - CTRL,FMWF   -9.810210e-01 0.6770180 NA  -1.449  0.8342
CTRL,FMFF - LOWpH,FMWF  -2.279950e+00 1.6738073 NA  -1.362  0.8744
CTRL,FMFF - CTRL,WMFF   -9.809735e-01 0.6770224 NA  -1.449  0.8342
CTRL,FMFF - LOWpH,WMFF  -2.790957e+00 1.6537652 NA  -1.688  0.6954
CTRL,FMFF - CTRL,WMWF   -2.248243e-05 0.8165311 NA   0.000  1.0000
CTRL,FMFF - LOWpH,WMWF  -2.434217e+00 1.6666742 NA  -1.461  0.8284
LOWpH,FMFF - CTRL,FMWF   2.072202e+00 1.5822427 NA   1.310  0.8956
LOWpH,FMFF - LOWpH,FMWF  7.732732e-01 0.4935656 NA   1.567  0.7704
LOWpH,FMFF - CTRL,WMFF   2.072250e+00 1.5822445 NA   1.310  0.8956
LOWpH,FMFF - LOWpH,WMFF  2.622658e-01 0.4206134 NA   0.624  0.9986
LOWpH,FMFF - CTRL,WMWF   3.053201e+00 1.6467732 NA   1.854  0.5828
LOWpH,FMFF - LOWpH,WMWF  6.190059e-01 0.4688054 NA   1.320  0.8914
CTRL,FMWF - LOWpH,FMWF  -1.298929e+00 1.6103573 NA  -0.807  0.9928
CTRL,FMWF - CTRL,WMFF    4.754102e-05 0.4999815 NA   0.000  1.0000
CTRL,FMWF - LOWpH,WMFF  -1.809936e+00 1.5895153 NA  -1.139  0.9483
CTRL,FMWF - CTRL,WMWF    9.809985e-01 0.6770119 NA   1.449  0.8342
CTRL,FMWF - LOWpH,WMWF  -1.453196e+00 1.6029417 NA  -0.907  0.9855
LOWpH,FMWF - CTRL,WMFF   1.298977e+00 1.6103590 NA   0.807  0.9928
LOWpH,FMWF - LOWpH,WMFF -5.110073e-01 0.5164052 NA  -0.990  0.9760
LOWpH,FMWF - CTRL,WMWF   2.279928e+00 1.6738042 NA   1.362  0.8744
LOWpH,FMWF - LOWpH,WMWF -1.542672e-01 0.5563606 NA  -0.277  1.0000
CTRL,WMFF - LOWpH,WMFF  -1.809984e+00 1.5895171 NA  -1.139  0.9483
CTRL,WMFF - CTRL,WMWF    9.809510e-01 0.6770162 NA   1.449  0.8343
CTRL,WMFF - LOWpH,WMWF  -1.453244e+00 1.6029435 NA  -0.907  0.9855
LOWpH,WMFF - CTRL,WMWF   2.790935e+00 1.6537621 NA   1.688  0.6954
LOWpH,WMFF - LOWpH,WMWF  3.567401e-01 0.4927939 NA   0.724  0.9963
CTRL,WMWF - LOWpH,WMWF  -2.434195e+00 1.6666710 NA  -1.461  0.8284

Results are given on the log (not the response) scale. 
P value adjustment: tukey method for comparing a family of 8 estimates 
Tests are performed on the log scale
</code></pre>

<p>Now I don't find the significant difference identified by the GLMM.</p>

<p><strong>Why the GLMM indicates a significant difference and lsmeans not, and why do I get NAs for my df in lsmeans?</strong></p>
"
"0.175027350160361","0.179812578843619","217511","<p>I want to compare several models built using the codes I have written in R for a mixed-effects model. I already knew that <code>anova()</code> function in car package provides <code>AIC</code>, which is a factor that we can use to compare models in a mixed-effects modelling analysis. However, I realised that <code>model.sel</code> function in <code>MuMIn</code> package seems to do the same thing and I need help as I am confused and the R help did not quite help. How are these functions different? Which one should I use? (I had 100 participants, 50 from each language group, from which each 25 participants in a language group received a different either list A or list B of the items.)</p>

<p>[packages <code>lme4</code> and <code>lmertest</code> were used to build the models]
Following are the codes I used to build the models:</p>

<pre><code>m1.1.1&lt;-lmer (RT~ Language*Col + (1+Col|Subject) + (1+Language|Item), data=RQ1.lmm.data.1) 
m1.1.2&lt;-lmer(RT~ Language*Col + (1|Subject) + (1+Language|Item), data=RQ1.lmm.data.1) 
m1.1.3&lt;-lmer(RT~ Language*Col + (1+Col|Subject) + (1|Item), data=RQ1.lmm.data.1)   
m1.1.4&lt;-lmer(RT~ Language*Col + (1|Subject) + (1|Item), data=RQ1.lmm.data.1) 
</code></pre>

<p>This is the function I used for comparing the models in order to find the most optimal model:</p>

<pre><code>anova(m1.1.1, m1.1.2, m1.1.3, m1.1.4)
</code></pre>

<p>[as this function shows the AIC for each model]</p>

<p>Resulting table:</p>

<pre><code>Data: RQ1.lmm.data.1 Models:

..3: RT ~ Language*Col + (1|Subject) + (1|Item)
..1: RT ~ Language*Col + (1|Subject) + (1+Language|Item)
..2: RT ~ Language*Col + (1+Col|Subject) + (1|Item)
object: RT ~ Language * Col + (1 + Col|Subject)+(1 + Language | Item)

   Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
..3     7 15912 15947 -7949.2    15898                             
..1     9 15874 15919 -7928.1    15856 42.322      2  6.456e-10 ***
..2     9 15902 15947 -7941.9    15884  0.000      0          1    
object 11 15865 15920 -7921.6    15843 40.634      2  1.501e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>As I have also seen the following function to be used for what seems like the same type of comparison, I used it and the following table was the result</p>

<pre><code>model.sel(m1.1.1, m1.1.2, m1.1.3, m1.1.4)

Model selection table 

       (Int) Cll Lng Cll:Lng      random df    logLik    AICc delta weight

m1.1.1 820.3   +   +       + 1+C|S+1+L|I 11 -7901.947 15826.1  0.00 0.988
m1.1.2 819.2   +   +       +     S+1+L|I  9 -7908.437 15835.0  8.90 0.012
m1.1.3 851.4   +   +       +     1+C|S+I  9 -7922.365 15862.9 36.76 0.000
m1.1.4 850.5   +   +       +         S+I  7 -7929.688 15873.5 47.34 0.000
Models ranked by AICc(x) 
Random terms: 
1+C|S = â€˜1 + Col|Subjectâ€™
1+L|I = â€˜1 + Language|Itemâ€™
S = â€˜1|Subjectâ€™
I = â€˜1|Itemâ€™
</code></pre>

<p>I'm guessing the difference between the two is only the <code>AIC</code> being the corrected version in the <code>model.sel</code> function (which seems to be more appropriate for small sample sizes, but not exactly sure of it.
And am I correct to interpret that, based on the results, my core model (model m1.1.1) is the most optimal model in this case?</p>

<p>(any comments on the interpretation of the model would be much appreciated as I am  quite new to this type of analysis technique.)</p>
"
"0.123763026191503","0.127146693842964","218970","<p>For a specific analysis I want to calculate the <em>variance partition coefficient</em> (VPC). I am using the following formula:</p>

<pre><code> test &lt;- glmer(SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek + 
             (1|POSCODN), data = dataScaled, family = binomial)

&gt; summary(test)
     Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
       Family: binomial  ( logit )
       Formula: SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek +  (1 | POSCODN)
       Data: dataScaled

   AIC      BIC   logLik deviance df.resid 
  43707.5  43757.9 -21847.8  43695.5    32684 

Scaled residuals: 
   Min      1Q  Median      3Q     Max 
 -1.3263 -0.8378 -0.7164  1.1263  2.4788 

  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Number of obs: 32690, groups:  POSCODN, 173

Fixed effects:
          Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   -0.56437    0.01798 -31.387  &lt; 2e-16 ***
 Herkomst1      0.49571    0.02980  16.633  &lt; 2e-16 ***
 OuderPersoon1  0.29911    0.02433  12.295  &lt; 2e-16 ***
 statusscore14 -0.09900    0.01353  -7.316 2.56e-13 ***
 M_SpoWeek     -0.08658    0.01225  -7.067 1.58e-12 ***

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   Correlation of Fixed Effects:
        (Intr) Hrkms1 OdrPr1 stts14
Herkomst1   -0.436                     
OuderPersn1 -0.553  0.168              
statusscr14 -0.068  0.233  0.013       
M_SpoWeek   -0.044 -0.029  0.138 -0.034
</code></pre>

<p>Because I only have information in the outcome about POSCODN at random effects I am not sure how to calculate the VPC. How can I get an extra row there with information about residuals. For example: </p>

<pre><code>  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Residual             258.357 16.0735 
 Number of obs: 32690, groups:  POSCODN, 173
</code></pre>

<p>Or do you have other suggestions how to calculate the VPC?</p>

<p>Thanks!</p>
"
"0.104598848163826","0.107458569276045","219674","<p>I am fitting a mixed model with the command </p>

<pre><code>model=lmer(Activity ~ 1 + Novelty*Valence*ROI + (1 | Subject))
</code></pre>

<p>Activity is a measure of brain activity, Novelty and Valence are categorical variables coding the type of stimulus used to elicit the response and ROI is a categorical variable coding three regions of the brain that we have sampled this activity from. Subject is an ID number for the individuals the data was sampled from (n=94).</p>

<p>In attempting to get a single estimate for ROI (since it has 3 levels), I ran</p>

<pre><code>model_test=lmer(Activity ~ 1 + Novelty*Valence*ROI - ROI + (1 | Subject))
anova(model,model_test)
</code></pre>

<p>The output of my anova command suggests these models are equivalent</p>

<pre><code>object: Activity ~ 1 + Novelty * Valence * ROI + (1 | Subject)
..1: Activity ~ 1 + Novelty * Valence * ROI - ROI + (1 | Subject)
       Df     AIC     BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
object 14 -721.88 -641.78 374.94  -749.88                        
..1    14 -721.88 -641.78 374.94  -749.88     0      0          1
</code></pre>

<p>Playing around with different models, this seems to happen any time I eliminate an effect or interaction that is qualified by a higher-order interaction. What is the correct way to get an estimate for this categorical variable?</p>
"
"0.210067544970746","0.215810768762311","222949","<p>I am developing GLMM's in order to assess habitat selection (using GLMMs' coeficients to construct Resource selection functions). 
I have (telemetry) data from 5 study areas, and each area has a different number of individuals monitored. </p>

<p>To develop GLMM's, the dependend variable is binary (1-used locations; 0-available locations), and I have a initial set of 14 continuous variables (8 land cover variables; 2 distance variables, to artificial areas and water sources; 4 topographic variables): a buffer was placed around each location and the area of each land cover within that buffer was accounted for; distances were measured from each point to the nearest feature, and topographic variables were obtained using DEM rasters. I tested for correlation using Spearman's Rank, so not all 14 were used in the GLMMs. All variables were transformed using z-score.</p>

<p>As random effect, I used individual ID (In another question (""GLMM: relationship between AIC, R squared and overdispersion?""), it became clear that using study areas as random effect was not useful nor correct).</p>

<p>I constructed a GLMM with 9 variables (not correlated) and a random effect, then used ""dredge()"" function and ""model.avg(dredge)"" to sort models by AIC values. 
This was the result (only models of AICc lower than 2 represented):</p>

<pre><code>[1]Call:
model.avg(object = dredge.m1.1)

Component model call: 
glmer(formula = Used ~ &lt;512 unique rhs&gt;, data = All_SA_Used_RP_Area_z, family = 
     binomial(link = ""logit""))

Component models: 
          df   logLik    AICc  delta weight
123578     8 -4309.94 8635.89   0.00   0.14
1235789    9 -4309.22 8636.44   0.55   0.10
123789     8 -4310.52 8637.04   1.14   0.08
1235678    9 -4309.75 8637.50   1.61   0.06
12378      7 -4311.78 8637.57   1.67   0.06
1234578    9 -4309.79 8637.58   1.69   0.06
</code></pre>

<p>Variables 1 and 2 represent the distance variables; from 3 to 8 land cover variables, and 9 is a topographic variable.
 Weights seem to be very low, even if I average all those models as it seems to be common when delta values are low. Even with this weights, I constructed GLMMs for each of the combinations, and the results were simmilar for all 6 combinations. Here are the results for the first one (GLMM + overdispersion + r-squared):</p>

<pre><code>Random effects:
 Groups    Name        Variance Std.Dev.
 ID.CODE_1 (Intercept) 13.02    3.608   
Number of obs: 32670, groups:  ID.CODE_1, 55

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.54891    0.51174  -1.073 0.283433    
3       -0.22232    0.04059  -5.478 4.31e-08 ***
5       -0.05433    0.02837  -1.915 0.055460 .  
7       -0.13108    0.02825  -4.640 3.49e-06 ***
8       -0.15864    0.08670  -1.830 0.067287 .  
1         0.28438    0.02853   9.968  &lt; 2e-16 ***
2         0.11531    0.03021   3.817 0.000135 ***     
Residual deviance: 0.256           
r.squaredGLMM():
       R2m        R2c 
0.01063077 0.80039950 
</code></pre>

<p>This is what I get from this analysis: </p>

<p>1) Variance and SD of the random effect seems fine (definitely better than the ""0"" I got when using Study Areas as random effect);</p>

<p>2) Estimate values make sense from what I know of the species and the knowledge I have of the study areas;</p>

<p>3) Overdispersion values seem good, and R-squared values don't seem very good (at least when considering only fixed effects) but, as I read in several places, AIC and r-squared are not always in agreement. </p>

<p>4) Weight values seem very low. Does it mean the models are not good?</p>

<p>Then what I did was construct a GLM (""glm()""), so no random effect was used. I used the same set of variables used in [1], and here are the results (only models of AICc lower than 2 represented):</p>

<pre><code>[2] Call:
model.avg(object = dredge.glm_m1.1)

Component model call: 
glm(formula = Used ~ &lt;512 unique rhs&gt;, family = binomial(link = ""logit""), data = 
     All_SA_Used_RP_Area_z)

Component models: 
          df   logLik     AICc   delta weight
12345678   9 -9251.85 18521.70    0.00   0.52
123456789 10 -9251.77 18523.54    1.84   0.21
1345678    8 -9253.84 18523.69    1.99   0.19
</code></pre>

<p>In this case, weight values are higher. </p>

<p>Does this mean that it is better not to use a random effect? (I am not sure I can compare GLMM with GLM results, correct me if I am doing wrong assumptions)</p>
"
"0.162043838788391","0.152601258044912","223008","<p>Good morning all!I am trying to run a binomial gmler model.
My response variable is a binomial variable:  extra pair paternity -->( 1 or 0) I am looking at several continuous variables like weight, tarsus and number of eggs lost. However, I am having problems with my random effects. I would appreciate any help, because I am already 2 days trying to figure out!! Thanks a lot!!!</p>

<p>my data:      </p>

<pre><code>ring_id    nest nest_id number_eggs number_chicks lost_eggs ring_year tarsus weight
1 BD29285 WH00060       6          10            10         0      2016    210   1700
2 BD29286 WH00060       6          10            10         0      2016    200   1510
3 BD29287 WH00060       6          10            10         0      2016    199   1540
4 BD29288 WH00060       6          10            10         0      2016    209   1780
5 BD29289 WH00060       6          10            10         0      2016    199   1670
6 BD29290 WH00060       6          10            10         0      2016    199   1670
  number_epy epy_wpy EPP_nest Epfather
1          0     WPY        0         
2          0     WPY        0         
3          0     WPY        0         
4          0     WPY        0         
5          0     WPY        0         
6          0     WPY        0         
&gt; 
</code></pre>

<p>This is my code</p>

<pre><code>m &lt;- lmer(EPP_nest ~ weight + tarsus + lost_eggs + (1|nest_id) + (1| ring_id) ,family = 'binomial', data=chicks)

summary (m)
</code></pre>

<p>output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod
]
 Family: binomial  ( logit )
Formula: EPP_nest ~ weight + tarsus + lost_eggs + (1 | nest_id) + (1 |      ring_id)
   Data: chicks
Control: structure(list(optimizer = c(""bobyqa"", ""Nelder_Mead""), calc.derivs = TRUE,  
    use.last.params = FALSE, restart_edge = FALSE, boundary.tol = 1e-05,  
    tolPwrss = 1e-07, compDev = TRUE, nAGQ0initStep = TRUE, checkControl = structure(list( 
        check.nobs.vs.rankZ = ""ignore"", check.nobs.vs.nlev = ""stop"",  
        check.nlev.gtreq.5 = ""ignore"", check.nlev.gtr.1 = ""stop"",  
        check.nobs.vs.nRE = ""stop"", check.rankX = ""message+drop.cols"",  
        check.scaleX = ""warning"", check.formula.LHS = ""stop"",  
        check.response.not.const = ""stop""), .Names = c(""check.nobs.vs.rankZ"",  
    ""check.nobs.vs.nlev"", ""check.nlev.gtreq.5"", ""check.nlev.gtr.1"",  
    ""check.nobs.vs.nRE"", ""check.rankX"", ""check.scaleX"", ""check.formula.LHS"",  
    ""check.response.not.const"")), checkConv = structure(list( 
        check.conv.grad = structure(list(action = ""warning"",  
            tol = 0.001, relTol = NULL), .Names = c(""action"",  
        ""tol"", ""relTol"")), check.conv.singular = structure(list( 
            action = ""ignore"", tol = 1e-04), .Names = c(""action"",  
        ""tol"")), check.conv.hess = structure(list(action = ""warning"",  
            tol = 1e-06), .Names = c(""action"", ""tol""))), .Names = c(""check.conv.grad"",  
    ""check.conv.singular"", ""check.conv.hess"")), optCtrl = list()), .Names = c(""optimizer"",  
""calc.derivs"", ""use.last.params"", ""restart_edge"", ""boundary.tol"",  
""tolPwrss"", ""compDev"", ""nAGQ0initStep"", ""checkControl"", ""checkConv"",  
""optCtrl""), class = c(""glmerControl"", ""merControl""))

     AIC      BIC   logLik deviance df.resid 
    56.0     77.5    -22.0     44.0      259 

Scaled residuals: 
      Min        1Q    Median        3Q       Max 
-0.001717 -0.001158 -0.000051  0.020852  0.041496 

Random effects:
 Groups  Name        Variance Std.Dev.
 ring_id (Intercept)    0      0.00   
 nest_id (Intercept) 6613     81.32   
Number of obs: 265, groups:  ring_id, 265; nest_id, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.106e+01  2.222e+01  -0.498  0.61854    
weight      -6.211e-04  7.607e-03  -0.082  0.93493    
tarsus      -6.362e-03  1.157e-01  -0.055  0.95615    
lost_eggs   -6.395e+00  1.777e+00  -3.599  0.00032 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
          (Intr) weight tarsus
weight    -0.252              
tarsus    -0.857 -0.272       
lost_eggs  0.212 -0.170 -0.070
convergence code: 0
Model failed to converge with max|grad| = 0.00997492 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>So I am quite lost.... and I would really apreciate any help!!!!! </p>

<p>Thank you very much!!1
Best, Mara</p>
"
"0.140334080917496","0.144170799399664","223160","<p>It's my first question here, I hope I'll ask it correctly. I am trying to find out how to analyse non-integer, count data (yes!). I am looking at the effect of a given treatment on habitat suitability for some birds, measured as number of territories. Some of the territories are inbetween two plots with different treatments, such that I had to distribute the territories between the plots. I end up with half and quarter territories. </p>

<p><strong>EDIT</strong> My dataset looks like this:</p>

<pre><code>   year         plot    treatment   territories    location surface
1  1985         1569         ctrl           1.0     Cheyres     1.2
2  1986         1569         ctrl           1.0     Cheyres     1.2
3  1987         1569            1           0.0     Cheyres     1.2 
4  1988         1569            2           2.0     Cheyres     1.2
5  1989         1569            3           6.5     Cheyres     1.2
6  1990         1569            1           1.5     Cheyres     1.2
</code></pre>

<p>Where year, plot, location and treatment are factors.</p>

<p>I've tried a GLMM with Poisson distribution (in R):</p>

<pre><code>glmmacrsci1 &lt;- glmer(territories ~ treatment * (1|year) * (1|location/plot), 
                     offset=surface, family=""poisson"", data=acrsci)
</code></pre>

<p>When running this, I get the usual non-integer warnings (e.g.):</p>

<pre><code>In dpois(y, mu, log = TRUE) : non-integer x = 1.500000
</code></pre>

<p>and I get infinite AIC, BIC, and deviance:</p>

<pre><code>$AICtab
 AIC      BIC   logLik deviance df.resid 
 Inf      Inf     -Inf      Inf      775 
</code></pre>

<p>Most other questions related to non-integer counts were about rates, which can apparently be circumvented by using an offset. However I don't think it's possible in my case.</p>

<p>My questions to you:</p>

<p>1) Is it correct to use a GLMM with Poisson distribution with such data? (I don't think so but glmer seems to work anyway)</p>

<p>2) Can you think of any alternative to Poisson for my data?</p>

<p>Thanks in advance!</p>
"
"0.269657247519029","0.284724915265818","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.0763881981711804","0.117714964779443","225904","<p>I collected the following variables, as I thought they might be in some relationship, but without any strict hypothesis. Note: this is a repeated measures design.</p>

<ul>
<li><strong>phy</strong> = continuous DV (a physiological measure)</li>
<li><strong>lag</strong> = interval, quadratic, IV (a setting of the experiment, ranges from -5 to +5 seconds)</li>
<li><strong>group</strong> = factor, 2 level, IV (two different conditions, say males VS females)</li>
<li><strong>quest1</strong> = continuous covariate (a questionnaire related to the measured stuff)</li>
<li><strong>quest2</strong> = continuous covariate (another scale of the same questionnaire)</li>
<li><strong>id</strong> = factor subject id</li>
</ul>

<p>I collected 15 subjects, each measured for 11 lags, for a total of 165 data points.
<strong>My goal is to decide wether there is a credible difference between the two groups in the phy response,</strong> controlling for all the other variables, and to describe such difference.</p>

<p>So my logic was to build a full model:</p>

<pre><code>MODEL1 &lt;- lmer(phy ~ lag * I(lag^2) * group * quest1 * quest2 + (1|id))
</code></pre>

<p>and then to stepwise remove the interactions and to compare the models with likelihood ratio test, AIC and BIC, trough the anova() function.</p>

<p>My results are:</p>

<pre><code>        Df    AIC    BIC  logLik deviance   Chisq Chi Df Pr(&gt;Chisq)    
MODEL8  10 385.70 416.76 -182.85   365.70                              
MODEL4  13 390.16 430.54 -182.08   364.16  1.5380      3  0.6735395    
MODEL5  13 420.45 460.82 -197.22   394.45  0.0000      0  1.0000000    
MODEL6  18 365.84 421.74 -164.92   329.84 64.6106      5   1.35e-12 ***
MODEL7  18 397.84 453.75 -180.92   361.84  0.0000      0  1.0000000    
MODEL9  18 390.63 446.53 -177.31   354.63  7.2144      0  &lt; 2.2e-16 ***
MODEL10 18 465.24 521.14 -214.62   429.24  0.0000      0  1.0000000    
MODEL11 18 413.73 469.64 -188.87   377.73 51.5046      0  &lt; 2.2e-16 ***
MODEL2  19 408.63 467.64 -185.31   370.63  7.1007      1  0.0077053 ** 
MODEL3  19 367.78 426.79 -164.89   329.78 40.8490      0  &lt; 2.2e-16 ***
MODEL1  34 355.60 461.20 -143.80   287.60 42.1812     15  0.0002108 ***
</code></pre>

<p>Where MODEL1 is the full model, and the higher numbers are increasingly simple models. So except BIC, which is obviously penalizing the model complexity, AIC and likelihood test are telling me to keep a 5-way interaction!?</p>

<p>My questions are:</p>

<ol>
<li>Is my logic right?</li>
<li>Should I believe in such a complex model?</li>
<li>Is it ok with my relatively few datapoints 15 subjects x 11 repetitions</li>
<li>How can I even begin to interpret or to plot the effects?</li>
</ol>

<p>thank you for any suggestion on how to proceed!</p>
"
"0.0467780269724988","0.0480569331332213","230721","<p>I have the following model  </p>

<pre><code>fit1 &lt;- glmer(Res~FA+FB+FC+(1|fsite), family=binomial(), data=DATA)
</code></pre>

<p>the result of <code>summary()</code> is:  </p>

<pre><code>summary(fit1)
Generalized linear mixed model fit by maximum likelihood 
 (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Res ~ FA + FB + FC + (1 | fsite)
   Data: DATA

     AIC      BIC   logLik deviance df.resid 
   202.3    229.9    -92.1    184.3      150 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.1768 -0.6167 -0.4967  0.6815  2.0132 

Random effects:
 Groups Name        Variance Std.Dev.
 fsite  (Intercept) 0        0       
Number of obs: 159, groups:  fsite, 28

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.55573    0.55830   2.787 0.005327 ** 
FA2         -0.11914    0.37344  -0.319 0.749692    
FB2         -1.38652    0.39967  -3.469 0.000522 ***
FC2         -0.14976    0.61984  -0.242 0.809076    
FC3         -0.06794    0.63171  -0.108 0.914350    
FC4         -1.20114    0.61670  -1.948 0.051452 .  
FC5         -1.44951    0.62817  -2.308 0.021025 *  
FC6         -1.13590    0.65427  -1.736 0.082538 .  
---
Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1

Correlation of Fixed Effects:
        (Intr) fspcs2    FB2    FC2    FC3    FC4    FC5 
FA2     -0.466                                          
FB2     -0.456  0.169                                   
FC2     -0.572 -0.021  0.017                            
FC3     -0.596  0.050  0.036  0.506                     
FC4     -0.582 -0.005  0.020  0.519  0.509              
FC5     -0.558  0.019 -0.038  0.508  0.500  0.511       
FC6     -0.391 -0.101 -0.288  0.485  0.467  0.486  0.492
</code></pre>

<ul>
<li>Why are the variance and Std.Dev of the random effects zero?</li>
<li>How do I check for overdispersion in this model?</li>
<li>What should do if there is overdispersion?</li>
</ul>
"
"0.238685836226471","0.254293387685927","231765","<p>Please bear with me and this potentially misinformed question.</p>

<p>Can I re-code the averaged model as a normal linear model to check r^2?</p>

<p>I know that with averaged models AIC is considered the main important value as per everyone important</p>

<p>From my understanding AIC is basically letting us know that our modelâ€™s line of best fit is in the right place, and tilted correctly (that we have the best line for the data based on the covariates included in the model), without telling us how close the model is to the points itâ€™s trying to fit.</p>

<p>Conversely R2 is telling us how close to the line the points actually fall (how well our model fits the data).  </p>

<p>Why donâ€™t we look at r2 for averaged models? 
Is it just because thereâ€™s no easy R code translation from averaged model object in MuMIn to normal linear model that can have r2 evaluated?</p>

<p>I think I have a work-around:   </p>

<p>My understanding of the MuMIn function is that the mod.avg() function averages models and returns an RVI (relative variable importance to the model) which gives the proportional influence each covariate should have in the optimized model and the summed AIC weights of the models going in to the averaged model give us how confident we are that the averaged model contains the best model.</p>

<p>However, being confident that the averaged model contains the best model doesnâ€™t necessarily mean that the averaged model actually fits the response variable. You can be confident that you have the best out of a bad lot of models and still not fit the data very well. Thus, Iâ€™m not entirely convinced that AIC tells us everything we need to know about the fit of an averaged model.</p>

<p>Getting an r2 from a regular linear model object is easy in the MuMIn package the r.squaredGLMM() and r.squaredLR() functions do this quite nicely.
However you canâ€™t calculate an r2 for an averaged model object because MuMIn doesnâ€™t know what to do with it.</p>

<p>For example: </p>

<pre><code>##### in R

#starting with a global model

Test&lt;-lme(y ~a + b + c + d + e, random=~1|f),method=""ML"")

summary(Test)


# dredge global model, get the top models, and average them

all&lt;-dredge(Test) 

all 

top&lt;-get.models(all,delta&lt;2)

top

modavg&lt;-model.avg(top)

summary(modavg)

#####
</code></pre>

<p>The optimal averaged model returned by the model averaging process has omitted covariate â€œeâ€, and assigned the following RVI weightings to the retained covariates:</p>

<pre><code>averaged model&lt;-lme(y ~a + b + c + d , random=~1|f),method=""ML"")


RVI a=1 b=0.81, c=0.32, d=0.12
</code></pre>

<p>But the model averaged object is not a linear model object, so I canâ€™t perform other model investigating functions on it (i.e. plotting it etc.), like I can on a normal linear model object.</p>

<p>As far as I can tell, an averaged model is essentially just a subset of a regular linear model with only the important covariates retained and then weighted by the model averaging process.</p>

<p>So can I re-code the covariates retained by the model averaging to incorporate the RVI weighting, and replicate the averaged model as a regular model object?</p>

<p>For example:</p>

<pre><code>##### in R

#recode the averaged model identified covariates weighted by the RVI determined by the averaged model


a1&lt;-(a*1)


b1&lt;-(b*0.81)


c1&lt;-(c*0.32)


d1&lt;-(d*0.12)


avg.model.recode&lt;-lme(y ~a1 + b1 + c1 + d1 , random=~1|f),method=""ML"")


# re-fit to lme4 for R2 examination


avg.model.lmer&lt;-lmer(y ~a1 + b1 + c1 + d1 +(1|f),data=dataset)

summary(avg.model.lmer)


r.squaredGLMM(avg.model.lmer)


r.squaredLR(avg.model.lmer)


sem.model.fits(avg.model.lmer, aicc = TRUE)
#####
</code></pre>

<p>Conversely, can I check the averaged model fit by plotting the re-coded averaged model fitted values  against the response variable to see how well my fitted values  predicts the response variable ?  </p>

<pre><code>##### In R

plot(avg.model.recode)


Fitted&lt;-fitted(avg.model.recode)


plot(y~Fitted) ## Plot model fitted points versus observed response variable 
values.


fits&lt;-lm(y~Fitted)


abline(fits,lty=2,lwd=2)


# add confidence intervals?


confin&lt;-confint(fits,full=TRUE) 


confin


abline(coef=confin[,1],lty=2)


abline(coef=confin[,2],lty=2)


##this summary also gives r2 values


summary(fits)

#####
</code></pre>

<p>What do you think? Iâ€™m not a statistician, and just use R without really understanding how the math works, so Iâ€™m sorry if this was a stupid question because the math is all wrong. </p>
"
"0.147925109681887","0.136772429457052","233831","<p>I would like to make a statistical analysis in R using <code>lmer()</code>.
I analyzed 6 speaker voice during 18 months. The months divided into 4 periods: quarters 1, 2, 3, and 4.</p>

<p>My data:</p>

<p>dependent variable: X continuous
fixed factor: quarters
random factor: speakers</p>

<p>my formula is: </p>

<pre><code>m &lt;- lmer(X ~ quarters + (1 | speaker), data = v)
</code></pre>

<p><strong>the output is</strong>:</p>

<pre><code>Formula: X ~ quarters + (1 | speaker)
   Data: v

     AIC      BIC   logLik deviance df.resid 
  4219.4   4253.0  -2103.7   4207.4     1966 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.60919 -0.72724 -0.05942  0.65965  2.70657 

Random effects:
 Groups   Name        Variance Std.Dev.
 speaker  (Intercept) 0.002817 0.05308 
 Residual             0.493093 0.70221 
Number of obs: 1972, groups:  speaker, 6

Fixed effects:
              Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)    0.17173    0.05105   24.30000   3.364  0.00255
quartersQ2         -0.25136    0.05040 1972.00000  -4.988 6.64e-07
quartersQ4         -0.43192    0.05281 1951.40000  -8.179 4.44e-16
quartersQ5         -0.44146    0.05832 1869.50000  -7.569 5.86e-14
Correlation of Fixed Effects:
      (Intr) TQeQ2  TQeQ4 
quartersQ2 -0.748              
quartersQ4 -0.735  0.719       
quartersQ5 -0.673  0.654  0.630
</code></pre>

<p>My question: why don't I see the factor effect rather than the result for each group in the quarters?</p>

<p>What I am doing wrong?</p>

<hr>

<p>Thank for the answering!
Sorry about it, I am not perfect in statistical analysis.
I would like to test that there is a differences among four quarters.</p>

<p>My first question is that this analysis is correct? How can it interpreted?
And I have show in the example analysis (sleepstudy), that in the output the ""Fixed Effects"" it can be seen the fixed facor names, not the groups names of the fixed factor.
What is a difference? Why in the output can be seen the groups name (excluded the first quarter Q1).</p>

<p>It is not worng that there is a high Correlation of Fixed Effects?</p>

<p>Sorry about my bad knowledge:(</p>

<p>Thank</p>
"
"0.162043838788391","0.166474099685359","235018","<p>I am trying to use lmer function from lme4 package to estimate differences between two response curves from a control and treatment responses over time, leaving Subjects as random effect. Here the data:</p>

<pre><code>&gt; df
   Day Subject    Levels   Response
1   10    A001   Control 0.19672131
2   10    A002 Treatment 0.16830515
3   10    A003   Control 0.21355398
4   10    A004   Control 0.18644068
5   10    A005 Treatment 0.17231538
6   10    A007 Treatment 0.18448729
7   11    A001   Control 0.23774081
8   11    A002 Treatment 0.25000000
9   11    A003   Control 0.17288616
10  11    A004   Control 0.25843209
11  11    A005 Treatment 0.29505507
12  11    A007 Treatment 0.27315358
13  12    A001   Control 0.37851189
14  12    A002 Treatment 0.39753941
15  12    A003   Control 0.30925738
16  12    A004   Control 0.45247148
17  12    A005 Treatment 0.37485050
18  12    A007 Treatment 0.41668477
19  13    A001   Control 0.47589286
20  13    A002 Treatment 0.48965316
21  13    A003   Control 0.46696617
22  13    A004   Control 0.50611299
23  13    A005 Treatment 0.41968785
24  13    A007 Treatment 0.51708049
25  14    A001   Control 0.58793970
26  14    A002 Treatment 0.45247189
27  14    A003   Control 0.43121189
28  14    A004   Control 0.56663276
29  14    A005 Treatment 0.37929057
30  14    A007 Treatment 0.46441606
31  15    A001   Control 0.44310684
32  15    A002 Treatment 0.38066676
33  15    A003   Control 0.32576304
34  15    A004   Control 0.39422772
35  15    A005 Treatment 0.28628568
36  15    A007 Treatment 0.34023209
37  16    A001   Control 0.25967359
38  16    A002 Treatment 0.20789686
39  16    A003   Control 0.23629368
40  16    A004   Control 0.22833444
41  16    A005 Treatment 0.24163539
42  16    A007 Treatment 0.21100646
43  17    A001   Control 0.17009653
44  17    A002 Treatment 0.13781610
45  17    A003   Control 0.19149637
46  17    A004   Control 0.21317316
47  17    A005 Treatment 0.17746651
48  17    A007 Treatment 0.15096285
49  18    A001   Control 0.15408115
50  18    A002 Treatment 0.16038546
51  18    A003   Control 0.18361628
52  18    A004   Control 0.18867523
53  18    A005 Treatment 0.20131984
54  18    A007 Treatment 0.19504027
55  19    A001   Control 0.21285064
56  19    A002 Treatment 0.19435679
57  19    A003   Control 0.23979739
58  19    A004   Control 0.24010952
59  19    A005 Treatment 0.20209201
60  19    A007 Treatment 0.25806452
61  20    A001   Control 0.23613019
62  20    A002 Treatment 0.20014232
63  20    A003   Control 0.26122983
64  20    A004   Control 0.26375544
65  20    A005 Treatment 0.17656201
66  20    A007 Treatment 0.22391777
67  21    A001   Control 0.20523904
68  21    A002 Treatment 0.18967355
69  21    A003   Control 0.22878808
70  21    A004   Control 0.26186233
71  21    A005 Treatment 0.18644467
72  21    A007 Treatment 0.18347698
73  22    A001   Control 0.19849361
74  22    A002 Treatment 0.16430202
75  22    A003   Control 0.23331322
76  22    A004   Control 0.25791045
77  22    A005 Treatment 0.18159936
78  22    A007 Treatment 0.17076203
79  23    A001   Control 0.17558492
80  23    A002 Treatment 0.12551814
81  23    A003   Control 0.21406131
82  23    A004   Control 0.22028128
83  23    A005 Treatment 0.17529323
84  23    A007 Treatment 0.14576150
85  24    A001   Control 0.15733775
86  24    A002 Treatment 0.12099877
87  24    A003   Control 0.22833499
88  24    A004   Control 0.15324628
89  24    A005 Treatment 0.15217124
90  24    A007 Treatment 0.09604689
</code></pre>

<p>Now I try to fit a 6th order polynomial with a base model with no categorical variables, one to assess the intercept and one to assess the interaction between terms</p>

<pre><code>library(lme4)

model.base=lmer(Response ~ poly(Day, 6, raw=FALSE)+(Day | Subject), df)
model.1=lmer(Response ~ poly(Day, 6, raw=FALSE)+Levels+(Day | Subject), df)
model.2=lmer(Response ~ poly(Day, 6, raw=FALSE)*Levels+(Day | Subject), df)
</code></pre>

<p>Then I use <code>anova</code> function to assess the model improvement</p>

<pre><code>&gt; anova(model.base,model.1,model.2)
refitting model(s) with ML (instead of REML)
Data: df
Models:
model.base: Response ~ poly(Day, 6, raw = FALSE) + (Day | Subject)
model.1: Response ~ poly(Day, 6, raw = FALSE) + Levels + (Day | Subject)
model.2: Response ~ poly(Day, 6, raw = FALSE) * Levels + (Day | Subject)
           Df     AIC     BIC logLik deviance   Chisq Chi Df Pr(&gt;Chisq)   
model.base 11 -302.85 -275.35 162.42  -324.85                             
model.1    12 -309.60 -279.61 166.80  -333.60  8.7579      1   0.003083 **
model.2    18 -313.00 -268.00 174.50  -349.00 15.3978      6   0.017378 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and now my question is how can I plot the fitted data from the model and their confidence interval around the fitted lines similar to this example in <code>ggplot</code></p>

<pre><code>ggplot(df, aes(Day, Response, color = Levels)) +
  geom_point()+
  scale_x_continuous(breaks = c(seq(10,26,2)), limits = c(9.5,26.5))+
  stat_smooth(method=""lm"", se=TRUE, 
              formula=y ~ poly(x, 6, raw=FALSE))
</code></pre>

<p><a href=""http://i.stack.imgur.com/9pEhF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pEhF.jpg"" alt=""Model fit""></a></p>

<p>So far I have tried <code>confint</code>, <code>effects</code> and <code>lsmeans</code> packages to extract the confidence intervals, being unsuccessful.</p>

<p>Do you have any idea how this could be done?</p>
"
