"V1","V2","V3","V4"
"0.0847047099775253","0.10696563746014","  1266","<p>The following question is one of those holy grails for me for some time now, I hope someone might be able to offer a good advice.</p>

<p>I wish to perform a non-parametric repeated measures multiway anova using R.</p>

<p>I have been doing some online searching and reading for some time, and so far was able to find solutions for only some of the cases: friedman test for one way nonparametric repeated measures anova, ordinal regression with {car} Anova function for multi way nonparametric anova, and so on.  The partial solutions is NOT what I am looking for in this question thread.  I have summarized my findings so far in a post I published some time ago (titled: <a href=""http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/"">Repeated measures ANOVA with R (functions and tutorials)</a>, in case it would help anyone) </p>

<p>.</p>

<p>If what I read online is true, this task might be achieved using a mixed Ordinal Regression model (a.k.a: Proportional Odds Model).</p>

<p>I found two packages that seems relevant, but couldn't find any vignette on the subject:</p>

<ul>
<li><a href=""http://cran.r-project.org/web/packages/repolr/"">http://cran.r-project.org/web/packages/repolr/</a></li>
<li><a href=""http://cran.r-project.org/web/packages/ordinal/"">http://cran.r-project.org/web/packages/ordinal/</a></li>
</ul>

<p>So being new to the subject matter, I was hoping for some directions from people here.</p>

<p>Are there any tutorials/suggested-reading on the subject?  Even better, can someone suggest a simple example code for how to run and analyse this in R (e.g: ""non-parametric repeated measures multiway anova"") ?</p>

<p>Thanks for any help,
Tal</p>
"
"0.142054117143114","0.143509461970482","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"0.105880887471907","0.10696563746014","  7775","<p>Does anyone have suggestions or packages that will calculate the coefficient of partial determination?</p>

<p>The coefficient of partial determination can be defined as the percent of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.</p>

<p>The calculation for the partial r^2 is relatively straight forward after estimating your two models and generating the ANOVA tables for them. The calculation for the partial r^2 is:</p>

<p>(SSEreduced - SSEfull) / SSEreduced</p>

<p>I've written this relatively simple function that will calculate this for a multiple linear regression model. I'm unfamiliar with other model structures in R where this function may not perform as well:</p>

<pre><code>partialR2 &lt;- function(model.full, model.reduced){
    anova.full &lt;- anova(model.full)
    anova.reduced &lt;- anova(model.reduced)

    sse.full &lt;- tail(anova.full$""Sum Sq"", 1)
    sse.reduced &lt;- tail(anova.reduced$""Sum Sq"", 1)

    pR2 &lt;- (sse.reduced - sse.full) / sse.reduced
    return(pR2)

    }
</code></pre>

<p>Any suggestions or tips on more robust functions to accomplish this task and/or more efficient implementations of the above code would be much appreciated.</p>
"
"0.133929906036485","0.101476513723761","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.105880887471907","0.10696563746014"," 11127","<p>I have 2 dependent variables (DVs) each of whose score may be influenced by the set of 7 independent variables (IVs). DVs are continuous, while the set of IVs consists of a mix of continuous and binary coded variables. (In code below continuous variables are written in upper case letters and binary variables in lower case letters.)</p>

<p>The aim of the study is to uncover how these DVs are influenced by IVs variables. I proposed the following multivariate multiple regression (MMR) model:</p>

<pre><code>my.model &lt;- lm(cbind(A, B) ~ c + d + e + f + g + H + I)
</code></pre>

<p>To interpret the results I call two statements:</p>

<ol>
<li><code>summary(manova(my.model))</code></li>
<li><code>Manova(my.model)</code></li>
</ol>

<p>Outputs from both calls are pasted below and are significantly different. Can somebody please explain which statement among the two should be picked to properly summarize the results of MMR, and why? Any suggestion would be greatly appreciated.</p>

<p>Output using <code>summary(manova(my.model))</code> statement:</p>

<pre><code>&gt; summary(manova(my.model))
           Df   Pillai approx F num Df den Df    Pr(&gt;F)    
c           1 0.105295   5.8255      2     99  0.004057 ** 
d           1 0.085131   4.6061      2     99  0.012225 *  
e           1 0.007886   0.3935      2     99  0.675773    
f           1 0.036121   1.8550      2     99  0.161854    
g           1 0.002103   0.1043      2     99  0.901049    
H           1 0.228766  14.6828      2     99 2.605e-06 ***
I           1 0.011752   0.5887      2     99  0.556999    
Residuals 100                                              
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>Output using <code>Manova(my.model)</code> statement:</p>

<pre><code>&gt; library(car)
&gt; Manova(my.model)

Type II MANOVA Tests: Pillai test statistic
  Df test stat approx F num Df den Df    Pr(&gt;F)    
c  1  0.030928   1.5798      2     99   0.21117    
d  1  0.079422   4.2706      2     99   0.01663 *  
e  1  0.003067   0.1523      2     99   0.85893    
f  1  0.029812   1.5210      2     99   0.22355    
g  1  0.004331   0.2153      2     99   0.80668    
H  1  0.229303  14.7276      2     99 2.516e-06 ***
I  1  0.011752   0.5887      2     99   0.55700    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>
"
"0.195234709844974","0.197234889993286"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0947027447620757","0.071754730985241"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.133929906036485","0.118389266011054"," 13446","<p>I am new to R and using <code>rpart</code> for building a regression tree for my data.I wanted to use all the input variables for building the tree, but the rpart method using only a couple of inputs as shown below. As we can see, I have provided 10 inputs, but rpart used only two inputs. Please let me know how can force rpart method to use all the input variables. Thanks.</p>

<pre><code>rm = rpart(uloss ~ tc_b + ublkb + mpa_a + mpa_b + 
     sys_a + sys_b + usr_a, data = data81, method=""anova"")
&gt; princtp(rm)  

Regression tree:
rpart(formula = uloss ~ tc_b + ublkb + mpa_a + mpa_b + sys_a + 
    sys_b, data = data81, weights = usr_a, method = ""anova"")

Variables actually used in tree construction:
[1] mpa_a tc_b     
Root node error: 647924/81 = 7999
n= 81    
       CP nsplit rel error  xerror     xstd
1 0.403169      0   1.00000 1.04470 0.025262
2 0.092390      1   0.59683 0.66102 0.015238
3 0.081084      2   0.50444 0.70702 0.013123
4 0.045304      3   0.42336 0.58683 0.012129
5 0.010000      4   0.37805 0.51930 0.011942
</code></pre>

<p>One more question:</p>

<p>I have used rpart.control for minsplit=2, and got the following for another data.
Inorder to avoid overfititng the data, do I need to use splits 3 or splits 7.  Shouldn't I use splits 7? Please let me know.</p>

<p>Variables actually used in tree construction:
[1] ct_a  ct_b  usr_a</p>

<p>Root node error: 23205/60 = 386.75</p>

<p>n= 60 </p>

<pre><code>        CP nsplit rel error  xerror     xstd
1 0.615208      0  1.000000 1.05013 0.189409
2 0.181446      1  0.384792 0.54650 0.084423
3 0.044878      2  0.203346 0.31439 0.063681
4 0.027653      3  0.158468 0.27281 0.060605
5 0.025035      4  0.130815 0.30120 0.058992
6 0.022685      5  0.105780 0.29649 0.059138
7 0.013603      6  0.083095 0.21761 0.045295
8 0.010607      7  0.069492 0.21076 0.042196
9 0.010000      8  0.058885 0.21076 0.042196
</code></pre>
"
"0.0669649530182425","0.0676510091491738"," 18738","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/12398/how-to-interpret-f-and-p-value-in-anova"">How to interpret F- and p-value in ANOVA?</a>  </p>
</blockquote>



<p>I found that I can use ANOVA also for ONE Model, doing something like:</p>

<pre><code>&gt; anova(lm(a~b))
Analysis of Variance Table

Response: a
           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    
b           1 0.002679 0.0026791  11.191 0.0009001 ***
Residuals 398 0.095282 0.0002394                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I know that ANOVA check the means BUT what test is that if I use only ONE model?
If the p.value is above 0.05 it means that the regression fit good?</p>
"
"0.171165004875037","0.185269918744954"," 18909","<p>I have an ordinal variable related to an outcome that is comprised of many levels and IÂ´d like to collapse the number of ordinal values as much as possible. </p>

<pre><code>&gt; require(ipred)
&gt; require(party)
&gt; data(GBSG2)
&gt; head(GBSG2)
  horTh age menostat tsize tgrade pnodes progrec estrec time cens
1    no  70     Post    21     II      3      48     66 1814    1
2   yes  56     Post    12     II      7      61     77 2018    1
3   yes  58     Post    35     II      9      52    271  712    1
4   yes  59     Post    17     II      4      60     29 1807    1
5    no  73     Post    35     II      1      26     65  772    1
6    no  32      Pre    57    III     24       0     13  448    1
&gt; table(GBSG2$tgrade)

  I  II III 
 81 444 161 
&gt; ctree(Surv(time,cens)~tgrade,data=GBSG2) -&gt; mn
&gt; plot(mn)
</code></pre>

<p><img src=""http://i.stack.imgur.com/WYIUd.png"" alt=""enter image description here""></p>

<p>Would it be correct to claim that <code>tgrade</code> here could be collapsed into two instead of three values?</p>

<p>edit:</p>

<p>Running the usual parametric analysis I get:</p>

<pre><code>&gt;     anova(i1,i2)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade == ""I""
   loglik  Chisq Df P(&gt;|Chi|)  
1 -1776.0                      
2 -1778.1 4.3049  1     0.038 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt;     anova(i1,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade != ""III""
  loglik  Chisq Df P(&gt;|Chi|)    
1  -1776                        
2  -1784 16.033  1 6.225e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; anova(i2,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade == ""I""
 Model 2: ~ tgrade != ""III""
   loglik  Chisq Df P(&gt;|Chi|)    
1 -1778.1                        
2 -1784.0 11.728  0 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; extractAIC(i1)
[1]    2.000 3555.975
&gt; extractAIC(i2)
[1]    1.00 3558.28
&gt;   extractAIC(i3)
[1]    1.000 3570.008  
</code></pre>

<p>Hence the i1 model provides a better fit than i2 and i3, and i2 fits significantly better than i3. So now all three categories are warranted with respect to survival, which is at odds with the ctree approach. Can anyone explain this? Is this due to the conditional nature of ctree instead of the semiparametric nature of cox regression?</p>
"
"0.0473513723810378","0"," 19361","<p>Here is a sample output:</p>

<pre><code>anova(fit1,fit2);
Quantile Regression Analysis of Deviance Table

Model: op ~ inp1 + inp2 + inp3 + inp4 + inp5 + inp6 + inp7 + inp8 + inp9
Joint Test of Equality of Slopes: tau in {  0.15 0.3  }

  Df Resid Df F value Pr(&gt;F)
1  9     1337  0.5256 0.8568

Warning messages:
1: In summary.rq(x, se = ""nid"", covariance = TRUE) : 93 non-positive fis
2: In summary.rq(x, se = ""nid"", covariance = TRUE) : 138 non-positive fis
</code></pre>

<p>How to interpret the above results??
Does the <code>anova()</code> function give the best model, for <code>tau=0.15</code> vs. <code>tau=0.3</code>?</p>
"
"0.115986700954059","0.117174985029676"," 19469","<p>Here is an example: I have a set of observations of different individuals from lots of different families of grasses:</p>

<pre><code>individual#, Fam, Genus, Factor1(3 levels), Factor2(7 levels), Factor3(5 levels), Response1(3 levels), Response2(3 levels)
</code></pre>

<p>What I am hoping to discover is whether the frequency of occurrences of Response1 and 2 are linked to family groups, and whether Factors 1 - 3 (things like soil type, sun exposure etc) have an impact. </p>

<p>Example: </p>

<pre><code>family,  resp1a,  resp1b,   resp1c 
1,       14%(20), 16%(24),  67%(98),  Total N = 147  
2,       38%(98), 86%(220), 48%(123), Total N = 256
...
</code></pre>

<p>First, I need to see whether these differences in responses between families is significant (chi-squared?). Secondly, I need to see if one of the 3 factors has an effect on the response.</p>

<p>Now it seems in my basic understanding, that if the response(s) were continuous measurement, ANOVA/MANOVA would work. Easy-peasy. However, since everything is discreet categories (including the independent and dependent variables) I can't do this. Additionally, since the responses are not mutually exclusive, this seems to violate an assumption of the log-linear model.</p>

<p>I've scoured, and keep bouncing around between Multinomial Logistic Regression, or just independent Chi-Square tests, or... hell I don't know anymore.</p>

<p>And yes, I am trying to swim before learning to float.</p>

<p>Oh, and this is all happening in R.</p>
"
"0.0847047099775253","0.0855725099681116"," 20002","<p>I was always under the impression that regression is just a more general form of ANOVA and that the results would be identical. Recently, however, I have run both a regression and an ANOVA on the same data and the results differ significantly. That is, in the regression model both main effects and the interaction are significant, while in the ANOVA one main effect is not significant. I expect this has something to do with the interaction, but it's not clear to me what is different about these two ways of modeling the same question. If it's important, one predictor is categorical and the other is continuous, as indicated in the simulation below. </p>

<p>Here is an example of what my data looks like and what analyses I'm running, but without the same p-values or effects being significant in the results (my actual results are outlined above):</p>

<pre><code>group&lt;-c(1,1,1,0,0,0)
moderator&lt;-c(1,2,3,4,5,6)
score&lt;-c(6,3,8,5,7,4)

summary(lm(score~group*moderator))
summary(aov(score~group*moderator))
</code></pre>
"
"0.206658331615378","0.219214324140707"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.145090731539525","0.135302018298348"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.125279955557839","0.126563449052859"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"NaN","NaN"," 29329","<p>I was wondering, what is the meaning of operators in anova or regression formulas in R</p>

<p>For example</p>

<ul>
<li>""<strong>+</strong>"" aov &lt;- aov(x~time+sample, data=data) -> repeated mesures anova?</li>
<li>""<strong>*</strong>"" aov &lt;- aov(x~time*sample, data=data) -> two way anova?</li>
<li>""<strong>/</strong>"" aov &lt;- aov(x~time/sample, data=data) -> ?</li>
<li>""<strong>:</strong>"" aov &lt;- aov(x~time:sample, data=data) -> ?</li>
</ul>

<p>And also are there more operators for this kind of formulas?</p>
"
"0.133929906036485","0.135302018298348"," 29981","<p>Let's have some linear model, for example just simple ANOVA:</p>

<pre><code># data generation
set.seed(1.234)                      
Ng &lt;- c(41, 37, 42)                    
data &lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      
fact &lt;- as.factor(rep(LETTERS[1:3], Ng)) 

m1 = lm(data ~ 0 + fact)
summary(m1)
</code></pre>

<p>Result is as follows:</p>

<pre><code>Call:
lm(formula = data ~ 0 + fact)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30047 -0.60414 -0.04078  0.54316  2.25323 

Coefficients:
      Estimate Std. Error t value Pr(&gt;|t|)    
factA  -0.9142     0.1388  -6.588 1.34e-09 ***
factB   0.1484     0.1461   1.016    0.312    
factC   1.0990     0.1371   8.015 9.25e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8886 on 117 degrees of freedom
Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 
F-statistic: 36.23 on 3 and 117 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Now I try two different methods to estimate confidence interval of these parameters</p>

<pre><code>c = coef(summary(m1))

# 1st method: CI limits from SE, assuming normal distribution
cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], 
    high = c[,1] + qnorm(p = 0.975) * c[,2])

# 2nd method
confint(m1)
</code></pre>

<h2>Questions:</h2>

<ol>
<li>What is the distribution of estimated linear regression coefficients? Normal or $t$?</li>
<li>Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.</li>
</ol>

<p>Thank you very much!</p>

<p>data ~ 0 + fact</p>

<p><strong>EDIT after an answer</strong>:</p>

<p>The answer is exact, this will give exactly the same result as <code>confint(m1)</code>!</p>

<pre><code># 3rd method
cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], 
    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])
</code></pre>
"
"0.107382819049576","0.126563449052859"," 31494","<p>I'm very new to all this, and I am testing different ways to perform a two-way type III ANOVA on my data.</p>

<ul>
<li>I have tried <code>anova()</code> from the <code>stats</code> package, after fitting a linear regression with <code>lm()</code>;</li>
<li>I have tried <code>Anova()</code> from the <code>car</code> package, using the same linear regression (and this gives me the same result as <code>anova()</code> when I use <code>type=""II""</code> - I thought <code>anova()</code> used type I SS by default?).</li>
<li>And I am now trying to use <code>ezANOVA()</code> from the <code>ez</code> package.</li>
</ul>

<p>With this last one, I can't understand what the <code>wid=.()</code> argument is (even reading the help), and as it is not optional, I can't leave it blank. What I am trying to use is as follows, with its result:</p>

<pre><code>&gt; attach(data)
&gt; library(""ez"")
&gt; ezANOVA(data=data, dv=.(AG.DW), wid=.(), within=.(Genotype, Treatment), type=3)
Warning: Converting """" to factor for ANOVA.
Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>Is this the right script? What is <code>wid</code> and what should I fill it with?</p>

<p>Concerning my data, the columns <code>Genotype</code> and <code>Treatment</code> are my two factors, and I want to see if there is an interaction when looking at the aboveground dry weight of my plants (column <code>AG.DW</code>). My data is balanced.</p>

<p>I am sorry if information is missing or inaccurate here: this is my first contribution here, and I am only discovering statistics at the moment (and I can't see how to join my data file).</p>
"
"0.227286587428982","0.229615139152771"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.183391076651825","0.172918590828624"," 32498","<p>I realize that similar questions have already been asked and answered, but I am in need of a bit more detail and specific advice as I am new to PCA and statistical methods in general. My question is also a bit broader because I will be putting it in context and I need to know if I'm even headed in the right direction.</p>

<p>I have a great deal of data. For each datapoint, there is one continuous response variable that I am interested in examining. Let's call it X.</p>

<p>There are also five or six categorical variables, most of which have between three and ten possible values. One of them, however (let's call it A), has 154 possible values, and to complicate things further, each datapoint can fall in 1-4 of those 154 categories. For the vast majority of them, they just take one of the 154 values, but about 10% of them take two or three values, and maybe 0.5% of them take four values. (I am actually considering including a discrete but quantitative variable that will be equal to the number of values taken by S, as I think it might also be a relevant factor affecting X.)</p>

<p>My ultimate goal here is twofold: to create a predictive model with multiple regression, and to use ANOVA to determine how much each of my variables' variance explains the variance in X.</p>

<p>Someone more familiar with statistics than I suggested that I start with PCA because both multiple regression and ANOVA assume that all factors are independent. I'm pretty sure there are some correlations between a few of my factors (though I have no idea what they are) so I figured PCA would be a good way to begin disentangling.</p>

<p>My questions are:</p>

<ol>
<li><p>Can I perform PCA given the categorical nature of my data? If so, what method should I use to ""dummy code"" the variables? If not, what method would be more effective?</p></li>
<li><p>Will including a single discrete, quantitative variable (the number of values taken by A) complicate matters?</p></li>
<li><p>Will PCA even do what I want? (namely, disentangling the variables so I can then use multiple regression and ANOVA)</p></li>
<li><p>Whatever you recommend, is it possible in R, and if so, how? (I haven't even downloaded R yet but it's been recommended to me and it's free so I'm inclined to give it a swing. I have some programming experience in Python and C++ so in theory I can learn it without too much difficulty.)</p></li>
</ol>

<p>Thanks very much in advance. </p>
"
"0.133929906036485","0.118389266011054"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.142054117143114","0.143509461970482"," 35407","<p>I'm currently running a perception experiment: </p>

<ul>
<li>DV: error (this in degrees- how much an observer was away from the real answer) </li>
<li>IV: the time bin (5 levels) in which the unique stimulus appeared</li>
<li>IV2: True/False- whether the unique stimulus occurred before or after stimulus2. </li>
<li>Covariate: Distance from the fixation point this stimulus appeared (continuous)</li>
</ul>

<p>So, I have decided to use repeated measures ANCOVA since, all the observers were exposed to all 5 levels of the IV multiple times. </p>

<p>Currently I have written: </p>

<pre><code>data.aov &lt;-aov(error~(timebin*after*distance) + 
               Error(subject/timebin*after*distance), d)
summary(data.aov)
</code></pre>

<p><strong>Is this the correct way to specify my repeated measures ANCOVA in R?</strong></p>

<p>Also, I wanted to run this analysis in SPSS to check that I get the same results. However, SPSS doesn't like the format of my data. 
For repeated measures analysis, the 5 bins should be in 5 different columns, but in my data file, they are all together under the same heading <code>timebins</code>.</p>

<p><strong>How can I run the repeated measures ANCOVA in SPSS if my data are in long format?</strong></p>

<hr>

<p>Thanks everyone for great answers! </p>

<p>@Marcus, just one thing- regarding regression. 
If I were to use regression, would I be looking at trends? 
I am actually more interested in comparing bin 1 and bin 5. This is why I was going to use a t-test, but now it seems like I am using within-subjects anova. </p>

<p>Also, I've had a look at distance and (as you said) might not be a covariate, since it was randomised for every condition! </p>

<p>Then the codes should look like this? </p>

<pre><code>data.aov &lt;-aov(error~(timebin*after) + Error(subject/timebin*after), d)
summary(data.aov)
</code></pre>

<p>But what is the difference between using <code>*</code> and using <code>+</code> ? 
Should I use <code>*</code> to get an interaction effect? </p>
"
"0.0669649530182425","0.0676510091491738"," 35778","<p>I have a 2 level repeated measures DV of accuracy, and a covariate of response bias. As response bias increases, accuracy level 1 increases while level 2 decreases.  I want to see if there is a difference in the means of the groups after controlling for response bias. </p>

<p>I can't do it with an ANCOVA.  Can I just manually calculate expected values based on a regression equation, and run an ANOVA?  </p>

<p>I'd like to do it in R or SPSS, so specifics for either would be welcome, but not necessary.</p>
"
"0.0820149827720712","0.0828552264999161"," 37466","<p>I am taking a graduate course in Applied Statistics that uses the following textbook (to give you a feel for the level of the material being covered): <a href=""http://amzn.com/0471072044"">Statistical Concepts and Methods</a>, by G. K. Bhattacharyya and R. A. Johnson.</p>

<p>The Professor requires us to use SAS for the homeworks. </p>

<p>My question is that: is there a Java library(ies), that can be used instead of SAS for problems typically seen in such classes.</p>

<p>I am currently trying to make do with <a href=""http://commons.apache.org/math/"">Apache Math Commons</a> and though I am impressed with the library (it's ease of use and understandability) it seems to lack even simple things such as the ability to draw histograms (thinking of combining it with a charting library).</p>

<p>I have looked at Colt, but my initial interest died down pretty quickly. </p>

<p>Would appreciate any input -- and I've looked at similar questions on Stackoverflow but have not found anything compelling.</p>

<p>NOTE: I am aware of R, SciPy and Octave and java libraries that make calls to them -- I am looking for a Java native library or set of libraries that can together provide the features I'm looking for.</p>

<p>NOTE: The topics covered in such a class typically include: one-samle and two-sample tests and confidence intervals for means and medians, descriptive statistics, goodness-of-fit tests, one- and two-way ANOVA, simultaneous inference, testing variances, regression analysis, and categorical data analysis.</p>
"
"0.0669649530182425","0.0676510091491738"," 39322","<p>I need to explore the determinants of wheat yield using multiple regression. There are 120 observations. Do I need to use ANOVA? I am using R studio. My results need to include a consideration of more than one functional form, one or more interaction terms for a select set of variables, and the use of dummy variables.</p>

<p>Variable Descriptions:</p>

<pre><code>yld wheat yield in kg/ha  
R Growing season rainfall (mm)  
N nitrogen fertilization rate (kg/ha)  
PrevCereal 1 if precious crop was a cereal, 0 otherwise  
RegionCode Location code (0/1)  
</code></pre>
"
"0.170727801083421","0.159209453430407"," 40385","<p>I would like to test in what regression fits my data best. My dependent variable is a count, and has a lot of zeros. </p>

<p>And I would need some help to determine what model and family to use (poisson or quasipoisson, or zero-inflated poisson regression), and how to test the assumptions.</p>

<ol>
<li>Poisson Regression: as far as I understand, the strong assumption is that dependent variable mean = variance. How do you test this? How close together do they have to be? Are unconditional or conditional mean and variance used for this? What do I do if this assumption does not hold? </li>
<li>I read that if variance is greater than mean we have overdispersion, and a potential way to deal with this is including more independent variables, or family=quasipoisson. Does this distribution have any other requirements or assumptions? What test do I use to see whether (1) or (2) fits better - simply <code>anova(m1,m2)</code>?</li>
<li>I also read that negative-binomial distribution can be used when overdispersion appears. How do I do this in R? What is the difference to quasipoisson?</li>
<li><p>Zero-inflated Poisson Regression: I read that using the vuong test checks what models fits better.  </p>

<p><code>&gt; vuong (model.poisson, model.zero.poisson)</code></p>

<p>Is that correct? What assumptions does a zero-inflated regression have? </p></li>
<li><p><a href=""http://www.ats.ucla.edu/stat/"">UCLA's Academic Technology Services, Statistical Consulting Group</a> has a <a href=""http://www.ats.ucla.edu/stat/R/dae/zipoisson.htm"">section</a> about zero-inflated Poisson Regressions, and test the zeroinflated model (a) against the standard poisson model (b):  </p>

<p><code>&gt; m.a &lt;- zeroinfl(count ~ child + camper | persons, data = zinb)</code><br>
<code>&gt; m.b &lt;- glm(count ~ child + camper, family = poisson, data = zinb)</code><br>
<code>&gt; vuong(m.a, m.b)</code></p></li>
</ol>

<p>I don't understand what the <code>| persons</code> part of the first model does, and why you can compare these models. I had expected the regression to be the same and just use a different family. </p>
"
"NaN","NaN"," 40884","<p>How do I test for Lack Of Fit (F-test) using R? I've seen a similar question, but that was for SPSS and it was just said that is can be easily done in R, but not how. </p>

<p>I know in simple linear regression I would use <code>anova(fm1,fm2)</code>, <code>fm1</code> being my model, <code>fm2</code> being the same model with <code>x</code> as a factor (if there are several <code>y</code> for <code>x</code>).
How do I do it in multiple linear regression? </p>
"
"0.150360801748797","0.165710452999832"," 41390","<p>I am looking for a test similar to a 2-way ANOVA that would work on a binary response variable. My response variable is survival of plant seedlings (alive or dead).  My explanatory variables are Treatment (3 treatment groups) and Site (3 sites).  </p>

<p>First, I would like to know whether Treatment, Site and their interaction have a significant effect on survival.  Second, if either Treatment or Site is significant, I would like to test all pairs of treatment groups or sites to know which pairs of levels are significantly different, as I would normally do with an ANOVA.</p>

<p>I have considered several options:</p>

<ol>
<li><p>Transform the response variable, for example through an arcsin transformation, and then perform an ANOVA. This does not work on my data because at one of the sites I measure 100% survival.  Therefore there is 0 variability at this site and no transformation will change that.</p></li>
<li><p>Logistic regression with Treatment and Site recoded as dummy variables.  The results do not seem to give me a test of significance of Treatment, Site and interaction term -  Instead, I get the relative importance of each treatment group and each site separately.  Furthermore, it seems that I cannot test all the pairs of treatment groups or sites, I can only compare one ""baseline"" or ""default"" group to each of the two remaining groups.</p></li>
<li><p>Chi-square test on each explanatory variable separately.  This has the obvious drawback of not being able to test the interaction term.  Also I suspect that I am omitting important information if I am comparing survival across the 3 treatment groups without taking into account that this survival data is grouped in 3 different sites.  Does this bias the results?</p></li>
</ol>

<p>Can anyone recommend a different test or what the best approach would be in my case?</p>

<p><strong>UPDATE:</strong> Logistic regression can in fact give a test of significance of each independent variable.  In R, I discovered I can use glm to contruct a model and then the anova function to extract p-values for each IV:</p>

<pre><code>mymodel &lt;- glm(Survival ~ Treatment*Site, data=survivaldata, family=""binomial"")
anova(mymodel, test=""Chisq"")
</code></pre>
"
"0.105880887471907","0.10696563746014"," 45939","<p>When I try to use the data and example for HLR from <a href=""http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv"" rel=""nofollow"">this example dataset</a> taken from this post in the R Tutorial Series on <a href=""http://rtutorialseries.blogspot.com/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">hierarchical linear regression</a> the results don't match when I try to use the same method in SPSS.  Is it because SPSS is using a different type of sum of squares (III)?</p>

<p>The F values match for the final model, but not the 2nd one and some of the sum of squares seem off.</p>

<pre><code>#R method
url &lt;- ""http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv""
datavar &lt;- read.csv(url, header=T)

#create three linear models using lm(FORMULA, DATAVAR)
#one predictor model
onePredictorModel &lt;- lm(ROLL ~ UNEM, datavar)
#two predictor model
twoPredictorModel &lt;- lm(ROLL ~ UNEM + HGRAD, datavar)
#three predictor model
threePredictorModel &lt;- lm(ROLL ~ UNEM + HGRAD + INC, datavar)

#get summary data for each model using summary(OBJECT)
summary(onePredictorModel)
summary(twoPredictorModel)
summary(threePredictorModel)

#compare successive models using anova(MODEL1, MODEL2, MODELi)
test&lt;- anova(onePredictorModel, twoPredictorModel, threePredictorModel)
</code></pre>

<p>Below here is the code for SPSS.</p>

<pre><code>*SPSS method
data list free /YEAR ROLL UNEM HGRAD INC.
begin data
1   5501    8.1 9552    1923
2   5945    7   9680    1961
3   6629    7.3 9731    1979
4   7556    7.5 11666   2030
5   8716    7   14675   2112
6   9369    6.4 15265   2192
7   9920    6.5 15484   2235
8   10167   6.4 15723   2351
9   11084   6.3 16501   2411
10  12504   7.7 16890   2475
11  13746   8.2 17203   2524
12  13656   7.5 17707   2674
13  13850   7.4 18108   2833
14  14145   8.2 18266   2863
15  14888   10.1    19308   2839
16  14991   9.2 18224   2898
17  14836   7.7 18997   3123
18  14478   5.7 19505   3195
19  14539   6.5 19800   3239
20  14395   7.5 19546   3129
21  14599   7.3 19117   3100
22  14969   9.2 18774   3008
23  15107   10.1    17813   2983
24  14831   7.5 17304   3069
25  15081   8.8 16756   3151
26  15127   9.1 16749   3127
27  15856   8.8 16925   3179
28  15938   7.8 17231   3207
29  16081   7   16816   3345
end data.


REGRESSION /MISSING LISTWISE 
/STATISTICS COEFF OUTS R ANOVA CHANGE 
/CRITERIA=PIN (.05) POUT(.10) 
/NOORIGIN /DEPENDENT ROLL 
/METHOD=ENTER UNEM 
/METHOD=ENTER HGRAD 
/METHOD=ENTER INC.
</code></pre>

<p>Or did I mess something up in the procedure for SPSS?</p>
"
"0.348790557749457","0.364514397919604"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.0966555841283824","0.117174985029676"," 48455","<p>I have 1 categorical factor (3 treatments) and 1 continuous factor (weight) and then I have 5 continuous response variables.</p>

<p>From what I have read, I should not use a two way ANOVA as one of the factors is continuous.  Is this correct? Should I be using a Multiple Regression instead?</p>

<p>I was advised that I can use ANOVA, but I'm not sure if this is correct based on what I have read.  I could convert the continuous factor to categorical, but I have also read on this site that this is not the preferred option.</p>

<p>My aim with the data is to see if there is a significant difference between the 3 treatments in regards to the response variables, which would be a standard one-way ANOVA, but I also want to see if weight effects the response variables.  </p>

<p>My analysis will be with R.</p>
"
"0.222307843495279","0.214820805887739"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0669649530182425","0.0676510091491738"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"0.0947027447620757","0.071754730985241"," 52516","<p>I have a data set with the following:</p>

<p>N = 60;
x = developmental stage (range 25 to 44);
y = proportion of 10 minute trial performing a behavior (range 0 to 0.81; 30 zeros)</p>

<p>A scatterplot produces a quadratic looking curve where those in mid-development clearly performed the behavior for more time. Most of the zeros are in the youngest and oldest individuals. If I break up the data into 5 groups according to developmental stage, an ANOVA/Tukey strongly supports this pattern. However, I would like to analyze this data continuously without breaking it into groups.</p>

<p>I have considered arcsine square root transformed proportion data in a linear regression, but I am unsure if that can incorporate a quadratic term, and this analysis results in a very small R squared value (less than 0.1). I have also considered arcsine square root transformed proportion data in a GLM containing a quadratic term or a beta regression (zeros??), but am not sure where to go from here.</p>

<p>I am planning to say in the paper that the individuals in mid-development perform the behavior more than those in early or late development, but am struggling to interpret the data in a way that supports that statement.</p>

<p>I appreciate any suggestions, thank you!</p>
"
"0.0820149827720712","0.0828552264999161"," 53432","<p>I have 3 categorical variables (CVa, CVb, CVc) all 0 or 1. Two continuous variables (IV1, IV2) are confounding my observational study. The multiple regression </p>

<pre><code>lm(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2)
</code></pre>

<p>is showing great significance for CVa</p>

<pre><code>              Estimate   Std. Error t value Pr(&gt;|t|)
(Intercept)  -1.414684   1.498886  -0.944  0.35233
CVa1         -0.841076   0.256946  -3.273  0.00255 **
CVb1         -0.413594   0.168753  -2.451  0.01990 * 
CVc1         -0.328669   0.183652  -1.790  0.08298 . 
IV1          -0.011768   0.006519  -1.805  0.08049 . 
IV2           0.487658   0.211015   2.311  0.02743 * 
CVa1:CVb1     0.321766   0.238869   1.347  0.18743   
CVa1:CVc1     0.741290   0.259402   2.858  0.00744 **
</code></pre>

<p>I thought that ANCOVA (between factor CVa) must also show significance, but</p>

<pre><code>summary(aov(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2))
</code></pre>

<p>is not showing any significance for CVa</p>

<pre><code>          Df Sum Sq Mean Sq F value  Pr(&gt;F)   
CVa        1  0.368  0.3681   3.093 0.08817 . 
CVb        1  0.427  0.4275   3.593 0.06709 . 
CVc        1  0.015  0.0148   0.125 0.72629   
IV1        1  0.585  0.5849   4.916 0.03384 * 
IV2        1  0.693  0.6935   5.828 0.02166 * 
CVa:CVb    1  0.126  0.1262   1.061 0.31069   
CVa:CVc    1  0.972  0.9716   8.166 0.00744 **
Residuals 32  3.807  0.1190
</code></pre>

<p>Am I doing ANOVA instead of ANCOVA? If yes, how do I control for IV1, IV2 to get that F-value they usually report in papers?</p>

<p>Just in case, <code>lsmeans(m2,pairwise ~ CVa * CVb)</code> reports that main effect of CVa is significant when controlled for IV1, IV2</p>

<pre><code>$`CVa:CVb pairwise differences`
               estimate        SE df  t.ratio p.value
0, 0 - 1, 0  0.47043119 0.1725208 32  2.72681 0.04807
</code></pre>
"
"0.0947027447620757","0.071754730985241"," 55662","<p>I am doing an ANCOVA model in order to explain the gap in a given distance. So I have a control group, and I have several quantitative variables, right now I am trying to evaluate the impact of the quantitative variables individually. But I think the results I get are incoherent, lest see:</p>

<pre><code>                           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
as.factor(groupe)           1  738.8   738.8  21.931 1.03e-05 ***
BASE0008                    1   36.6    36.6   1.087  0.29992    
as.factor(groupe):BASE0008  1  270.0   270.0   8.015  0.00576 ** 
Residuals                  87 2930.9    33.7 
</code></pre>

<p>These are the results from the ANOVA table, we could say that the group has a significant effect so does the interaction, but when I look at the results of the regression model, I find this:</p>

<pre><code>                              Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)                    1.16666    3.38404   0.345  0.73111   
as.factor(groupe)NAV           4.48191    3.79515   1.181  0.24084   
BASE0008                       0.26027    0.08651   3.009  0.00343 **
as.factor(groupe)NAV:BASE0008 -0.26902    0.09503  -2.831  0.00576 **
</code></pre>

<p>Well, the interaction is still relevant, but it looks like there is not effect of the group and the quantitative variable is more important to determinate the output of the experiment. I want to know if my interpretation is accurate: What can I say about the group? </p>
"
"0.133929906036485","0.118389266011054"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.200894859054728","0.202953027447522"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.142054117143114","0.127563966195984"," 58874","<p>As the title says, what I'd like to do is stepwise introduction of predictor variables to a mixed-effects model. I'm going to first say what I'd be doing if it were stepwise linear regression, just to make sure I've got that part right, and then describe the full model to which I want to apply an analogous approach.</p>

<p>I have a student population who took a pretest, then a tutorial, then a posttest. The tutorial involved doing problems from several categories with feedback, and the users could control which category the next problem would come from and when to stop the tutorial.</p>

<p>I want to create a model that will account for posttest performance using pretest score and some measures of behavior during the tutorial, including total number of problems done, accuracy, and probability of switching category. The last of these is of greatest theoretical interest. There are other variables I'm not mentioning for simplicity.</p>

<p>For the linear regression approach, I first did a simple regression using posttest score as the DV and including the main effects (only) of pretest score, tutorial accuracy, and number of problems as predictors. Then, I added probability of switching as an additional predictor, and compared the resulting model to the previous one to see if it had significantly better explanatory power (it did). The R code I used is below.</p>

<pre><code>lm1 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials, data=subj.data )
lm2 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials + probCategorySame, data=subj.data )
anova( lm1, lm2 )
</code></pre>

<p>So far so good? OK, next, I switched to a mixed model in order to include a binary within-subjects factor, 'test question type'. Both pretest and posttest have values for each level of this factor for every subject. (It's unrelated to the 'problem category' I mentioned for the tutorial.) The other predictors, however, only have one value for each participant. My models then became:</p>

<pre><code>library( nlme )
lm1 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials, random=~1|sid, method=""REML"", data=D )
lm2 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials + probCategorySame, random=~1|sid, method=""REML"", data=D )
</code></pre>

<p>However, I don't know how to test whether the second model resulted in a significant improvement over the first model. Is that the right question I should be asking and, if so, how should I do it?</p>
"
"0.0473513723810378","0"," 59434","<p>I have to compare the slopes of 2 regression lines with R. The 2 regressions are made with the same parameters in 2 different locations.</p>

<p>I did my regressions with the function lm(). Now I have the results but I don't know how to compare them..</p>

<p>I tried with student test or ANOVA but it requires more than the 2 slopes.</p>

<p>I looked for an answer during something like 2 hours but didn't find, so I ask the question here : How should I do my test? </p>

<p>Thank you in advance,
b.raoul</p>
"
"0.117188667781924","0.135302018298348"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.125279955557839","0.126563449052859"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.177172612243394","0.178987746151269"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.142054117143114","0.143509461970482"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.16451742565458","0.166202907140464"," 76250","<p>I am new to statistics and I am trying to understand the difference between ANOVA and linear regression. I am using R to explore this. I read various articles about why ANOVA and regression are different but still the same and how the can be visualised etc. I think I am pretty there but one bit is still missing.</p>

<p>I understand that ANOVA compares the variance within groups with the variance between groups to determine whether there is or is not a difference between any of the groups tested. (<a href=""https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA"" rel=""nofollow"">https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA</a>)</p>

<p>For linear regression, I found a post in this forum which says that the same can be tested when we test whether b (slope) = 0.
(<a href=""http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared"">Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?</a>)</p>

<p>For more than two groups I found a website stating:</p>

<p>The null hypothesis is: $\text{H}_0: Âµ_1 = Âµ_2 = Âµ_3$</p>

<p>The linear regression model is: $y = b_0 + b_1X_1 + b_2X_2 + e$</p>

<p>The output of the linear regression is, however, then the intercept for one group and the difference to this intercept for the other two groups. 
(<a href=""http://www.real-statistics.com/multiple-regression/anova-using-regression/"" rel=""nofollow"">http://www.real-statistics.com/multiple-regression/anova-using-regression/</a>)</p>

<p>for me, this looks like that actually the intercepts are compared and not the slopes?</p>

<p>Another example where they compare intercepts rather than the slopes can be found here:
(<a href=""http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/"" rel=""nofollow"">http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/</a>)</p>

<p>I am now struggling to understand what is actually compared in the linear regression? the slopes, the intercepts or both? </p>
"
"0.0473513723810378","0.047836487323494"," 76625","<p>I'm looking for a way to run a repeated-measures multiple regression in R, which would take care of sphericity - either by applying some corrections (such as Huynh-Feldt), or by avoiding the problem in some other way.</p>

<p>I have 2 factorial repeated measure variables: 3- and 2-level (<code>roi_ant</code>, <code>roi_lat</code>), and a quantitative between-subject variable (<code>pred</code>), and one dependent quantitative variable (<code>mv</code>). I want to test for a full model including all possible interactions between the two within-subject and the between-subject variables (i.e., <code>mv ~ pred * roi_ant * roi_lat</code>). I am most interested in the slope of <code>pred</code> - whether it is different from 0, and whether it changes depending on <code>roi_ant</code> and <code>roi_lat</code>.</p>

<p>I have tried to do repeated-measures MANCOVA (that would remove the sphericity assumption) using car::Anova package, but if I got <a href=""http://r.789695.n4.nabble.com/car-Anova-Can-it-be-used-for-ANCOVA-with-repeated-measures-factors-td4637324.html"" rel=""nofollow"">this</a> discussion right, <code>car::Anova()</code> is not able to handle such a design.</p>

<p><code>ezANOVA()</code> performs ANCOVA with applying corrections for sphericity, but reports only the repeated-measure variables (after removing the influence of the covariate), and does not report anything on the covariate <code>pred</code>.</p>

<p>Mixed linear models (<code>lme4</code>) should handle this design well, but since <code>mcmsamp</code> is still not implemented, it is difficult to get p-values out of them.</p>

<p>Do you have any other suggestions? </p>
"
"0.183391076651825","0.185269918744954"," 76918","<p>I have a question about how to do analysis of an experiment that has already been done, I hope you can help me with some advice!</p>

<p>I will try to keep it as simple as possible, but will give some detail so you know what I'm talking about!</p>

<p>What has been done is a ""screening trial"" to look at the activity of about 50 subjects (fungi) as antagonists (against a pest), the 50 individuals are members of groups (species), but some groups have many more members than others</p>

<p>I have results of several types of screening tests for each of the 50 individuals, with reps of each.  The screening tests look at different aspects, like growth rate, direct effects, and indirect effects.  </p>

<p>I can rank the isolates by their results in each screening test, and there looks like a lot of variability.</p>

<p>I want to be able to report the findings of screens, for each screening test and also to see if some individuals are in top ranks in different screening tests (and also the opposite, if some are great at some tests but not at others). I think what I want is to know if the results of the tests correlate for each individual....? </p>

<p>I am not sure how to say - this individual is the best - how can I tell if it is different than the next in the rank?
If I list the top ten from each screening test, I would like to know that they are statistically different from those I excluded from the list.  I would also like to compare them as groups, to be able to say, this species was the best, but with different numbers of representatives within the species, I dont think I could do this (please advise)</p>

<p>This seems like it would be a common research experiment, for example, for testing drugs in medical experiments, so I am looking for examples of what others have done to present this type of result.</p>

<p>I have seen a similar experiment to what I have to analyse but that had been done on a small scale, and the researchers used ANOVA to test differences among individuals and among groups, and some posthoc test to give each group little letters designating their means different than other groups.  </p>

<p>This seems to be unwieldy for 50 subjects, and I'm not sure about this.... I think I need some kind of mixed model regression to put all the test results in a model to test for correlation/covariance, but my understanding is weak!</p>

<p>I have been learning R and would like to do this analysis using R.</p>

<p>Can you give me advice/suggestions?  I would appreciate any help in understanding and clarifying this problem and solutions!  </p>
"
"0.107382819049576","0.126563449052859"," 76935","<p>I have data from a psychology experiment in which participants were assigned to one of 3 training conditions and then gave responses for 16 trials for each combination of two within-subjects factors, section and format.  Each response is classified as correct or incorrect.  I originally calculated a % correct for each participant for each combination of section and format, then used a mixed ANOVA to analyze these percentage scores with condition as a between-subjects factor and section &amp; format as within-subjects factors.  However, a reviewer of my manuscript commented that ANOVA is inappropriate for this accuracy data because it's not a genuine continuous variable and has various properties making it inappropriate to use ANOVA.  I think logistic regression WOULD be applicable to this data because it is, after all, binary choice data originally.  Assuming that's right (please tell me if not), how can I do this in R?</p>

<p>Specifically, I need to do logistic regression with correctness/incorrectness of responses as my DV, condition as a b-s factor, and section &amp; format as w-s factors.  (I do not have any continuous predictors.)  Here is what my data looks like:</p>

<pre><code>nsubj       = 150
nsection    = 3
nformat     = 3
ntrials     = 16
subjid      = rep( 1:nsubj, each=nformat*nsection )
condition   = c( replicate( nsubj, rep( sample( c( 'condition 1', 'condition 2', 'condition 3' ), 1 ), nformat*nsection ) ) )
section     = rep( rep( c( 'a', 'b', 'c' ), each=nformat ), nsubj )
format      = rep( rep( c( 'x', 'y', 'z' ), nsection ), nsubj )
nCorrect    = sample( 1:ntrials, nsubj*nsection*nformat, replace=TRUE )
nIncorrect  = ntrials - nCorrect
D           = data.frame( subjid=factor(subjid), condition=factor(condition), section=factor(section), format=factor(format), nCorrect=nCorrect, nIncorrect=nIncorrect )
D$accuracy  = D$nCorrect / (D$nCorrect+D$nIncorrect)
</code></pre>

<p>I tried this, but I know it is wrong because it does not account for the repeated measures, and may inflate significance in other ways (I keep getting significant results from randomly generated data, which seems wrong):    </p>

<pre><code>fit &lt;- glm( cbind( D$nCorrect, D$nIncorrect ) ~ section*format*condition, family=binomial(""logit""), data=D )
anova( fit, test=""Chisq"" )
</code></pre>
"
"0.0947027447620757","0.071754730985241"," 79830","<p>I'm using R to develop regression models, and I need to compare two different models' performance. The question that arises is, ""Is Model 1 statistically better than model 2?"" and I don't seem to have a way to answer that question.</p>

<p>Background: Model 1 consists of Variable A regressed on the endpoint. Model 2 consists of Variables B, C, and D regressed on the endpoint. Both models are developed using lm - ordinary least squares, nothing too fancy here.</p>

<p>Given that these are not nested models, I cannot compare them using ANOVA.</p>

<p>I can look at the R2 of actual vs predicted for each model, and I see that Model 2 is better, but how do I determine if it is statistically significantly better?</p>

<p>I've also used the Concordance Correlation Coefficient, but again, I can't find a way to prove significance. The best I've come up with is that the rho for Model 2 is better than Model 1, but that rho value is within the 95% confidence limits of the rho for Model 1.</p>

<p>I should throw in there that my assessment of predicted vs actual has been on a 60 observation hold-out set (240 observations in the training set).</p>
"
"0.0473513723810378","0.047836487323494"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.142769759542091","0.129809192515054"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.125279955557839","0.126563449052859"," 82698","<p>I've run a simulation study in order to determine type I error rate of a statistic.My simulation design includes threes factors as sample size (4 levels), test length or number of items (3 levels) and estimator (3 levels). The statistic is developed to measure person fit with test data in educational testing situation.I've replicated the analysis in each cell (i.e. the design is fully-crossed) 100 times.</p>

<p>Now, I have the results and type I error rates range from 0.005 to 0.105 (i.e. across the whole analysis). I want to analyze how factors affect type I error rate using something similar to ANOVA. I tried Beta Regression in R using <code>betareg</code> package but I received this error message:</p>

<blockquote>
  <p>invalid dependent variable, all observations must be in (0, 1)</p>
</blockquote>

<p>Any idea on how to determine the effect of design factors on type I error rate?</p>
"
"0.0473513723810378","0.047836487323494"," 85798","<p>If I do a multiple regression such as:</p>

<pre><code>df&lt;-data.frame(y1=rnorm(100,2,3),
y2=rnorm(100,3,2),
x1=rbinom(100,1,0.5),
x2=rnorm(100,100,10))

fit&lt;-lm(cbind(y1,y2)~x1+x2,data=df)
&gt; anova(fit)
Analysis of Variance Table

            Df  Pillai approx F num Df den Df Pr(&gt;F)    
(Intercept)  1 0.75423  147.306      2     96 &lt;2e-16 ***
x1           1 0.00720    0.348      2     96 0.7069    
x2           1 0.00928    0.450      2     96 0.6391    
Residuals   97                                          
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I am wondering how to explain this ANOVA object where two models have different responses and the same set of predictors.</p>
"
"0.189405489524151","0.191345949293976"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.14973818705887","0.151272255204013"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.142054117143114","0.111618470421486"," 90904","<p>I'm new to using CART trees, but have been asked to do so for a project I'm working on. I've had success running the scripts (from both RPART and PARTY packages) but I can't seem to get exactly what I'm looking for. I'm working with spectral data (Red, NIR, NDVI...) for 80 trees in four categories (Mesic-control, Mesic-fertilized, Xeric-control and Xeric-fertilized). There are significant differences in the mean values for spectral bands among the four categories and I'd like to use those differences to develop an algorithm for assigning category to unknown trees. </p>

<p>Here's a dummy tree I made using the RPART package:
<img src=""http://i.stack.imgur.com/7XUQ3.jpg"" alt=""RPART tree""></p>

<pre><code>fit &lt;- rpart(Category ~ red.top + NIR.top + R.NIR.top, method=""anova"", data=CCA)
plot(fit, uniform=T,main=""Classification Tree for Kyphosis"")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
</code></pre>

<p>And here's another tree I made with PARTY:
<img src=""http://i.stack.imgur.com/ourPe.jpg"" alt=""PARTY tree""></p>

<pre><code>library(party)
fit &lt;- ctree(Category ~ red.top + NIR.top + R.NIR.top, data=CCA)
plot(fit, main=""Conditional Inference Tree for Kyphosis"")
gtree &lt;- ctree(Category ~ ., data = CART)
plot(gtree)
</code></pre>

<p>Both look fine, except they don't really do what I want. The RPART one looks good, but I can't figure out how to determine the category identity of the trees in each 'leaf' and the PARTY one is what I want, except the tree is way simplified compared to the regression tree in the first example. My ultimate goal is to essentially combine the two and create a larger regression tree that uses more of the 'rules' from the data and gets me to output 'leaves' with categorical information and some predictive power. I'm not really too hung up on whether I use regression or categorization--as long as it has utilitarian value.</p>

<p>So, I guess what I'm really looking for is better scripts for either package that give me a more detailed tree with visual output (bar graphs on the leaves) or a way to determine the identity of the groups created by the RPART tree.</p>
"
"0.0947027447620757","0.095672974646988"," 91179","<p>Doing a poisson regression like this: <code>model&lt;-glm(y~x*z,family=poisson)</code> with one predictor being a factor, I would use <code>anova(model,test=""Chisq"")</code> to test the overall effect of the interaction rather than <code>summary(model)</code>, which would give me two interaction terms (one for each of two levels in the factor variable that is not the intercept).</p>

<p>When I do this, I get an output with five columns: </p>

<p><code>Df, Deviance, Resid. Df, Resid.Dev, Pr(&gt;Chi)</code></p>

<p>If I do the same with <code>test=""F""</code> instead I get an F value, so I assume there is a reason R doesn't report a chi-squared or LRT value.</p>

<p>Is this right? I would have thought that I should report a test statistic with my P value. Can I calculate it from the residual deviance and residual degrees of freedom? Should I report the residual deviance and df with my P value and leave it at that?</p>
"
"0.105880887471907","0.10696563746014"," 91700","<p>I am trying to understand the steps behind the linear regression process. I already have a linear model like:</p>

<p><code>lmodel1 &lt;- lm(y~x1+x2+x3, data=dataset)</code></p>

<p>for which R calculates several different things (<code>Coefficients, Intercept, Residuals, F-statistic</code> and <code>p-value</code>) among  others.</p>

<p>At this point, i am mostly interested in <code>F-statistic</code> and <code>p-value</code>.
So far, i have concluded to the following:</p>

<p>The process is iterative and begins taking under consideration every variable. In order to achieve an optimal <code>y</code> some <code>x</code> variables have to be ""taken out"". This comes as a result of calculating F-statistic, which quantifies the importance of each <code>xi</code> and the dependent variable <code>y</code>.
When <code>F value</code> is smaller than <code>p-value</code>(?) that variable is removed.
Next step of the process is to compare that <code>F-statistic</code> of a <code>xi</code> independent variable, with an <code>F-to-enter</code> and <code>F-to-remove</code> in order see if the removed variable will be re-inserted to the equation.(?)</p>

<p>Now, please do correct me if i am wrong regarding the steps desribed above.
Is that what happens under <code>lm()</code>'s hood. Are those the right variables.?</p>

<p>R-wise speaking how does these values can be shown, inserted or calculated in a multiple linear regression model.? How is <code>ANOVA</code> related to the above?</p>

<p>I am afraid R's <code>summary</code> and <code>help</code>  take too much for granted.</p>

<p>Thanks in advance for any suggestions.</p>
"
"0.0669649530182425","0.0676510091491738"," 91747","<p>I have a logistic regression with data that are kind of like this:</p>

<pre><code>y &lt;- rep(c(""A"", ""A"", ""B""), each = 30)
x &lt;- c(     rep(1, 12), rep(2, 18), rep(3, 16), rep(4, 12), rep(5, 2),
            rep(1, 3), rep(2, 5), rep(3, 8), rep(4, 10), rep(5, 4)  )

da &lt;- data.frame(y = y, x = x)
table(da)
   x
y    1  2  3  4  5
  A 12 18 16 12  2
  B  3  5  8 10  4
</code></pre>

<p>I'd like to show that there are more A's than B's in <code>y</code> (instead of a Bernoulli with <code>p</code> = 0.5), even after controlling for <code>x</code>, so I fitted two logistic regression models and used an ANOVA to compare them.</p>

<pre><code>mlogis.x              &lt;- glm(y ~ x,     family = binomial, da)
mlogis.x.no_intercept &lt;- glm(y ~ x + 0, family = binomial, da)

summary(mlogis.x.no_intercept)
summary(mlogis.x)

anova(mlogis.x.no_intercept, mlogis.x, test = ""Chisq"")
</code></pre>

<p>I have a few questions:</p>

<ul>
<li>Does what I did make sense overall?</li>
<li>Is it okay to not have an intercept in the more basic model and then add it to the full(er) model?</li>
<li>The coefficient for <code>x</code> changes sign between the two models, how should I interpret this?</li>
</ul>
"
"0.0947027447620757","0.095672974646988"," 92575","<p>I'm pretty new to stats, so this may be dumb.
I've been running a bunch of models on randomly generated data to try and develop my understanding of type 1 error.</p>

<p>I've noticed that using <code>glm(family=binomial)</code> I get more type 1 errors than I should when giving a binomial input (a two-column matrix of success and failure). In the code below, the first loop generates a thousand logistic regression models for random data, but I get type 1 errors (p &lt; .05) about 15% of the time!</p>

<p>The second loop runs the a similar thing as a Bernoulli test (just single vector of zeros and ones in the y). Here I get what I want to see, about 50 type 1 errors per 1000 models.</p>

<p>Can anyone explain this to me? I see that if I change the possible values for the random numbers in my success/failure y-matrix (the variable I call range here) I can lower the type 1 errors, but I don't understand why. </p>

<p>This gives me about triple the number of type 1 errors that I expect.</p>

<pre><code>#Binomial glm
fit.p=c()
for(i in 1:1000){
    range=0:10
    y=matrix(sample(range,2000,replace=T),ncol=2,nrow=1000)
    x=rnorm(1000,100,50)      
    fit=glm(y~x,family=binomial(link='logit'))               
    fit.p[i]=anova(fit,test='Chisq')[2,5]                            
}
print(length(which(fit.p&lt;.05)))
</code></pre>

<p>This works fine. About 50 errors per 1000</p>

<pre><code>#Bernoulli glm
fit2.p=c()
for(i in 1:1000){
    y=sample(0:1,1000,replace=T)
    x=rnorm(1000,100,50)      
    fit2=glm(y~x,family=binomial(link='logit'))               
    fit2.p[i]=anova(fit2,test='Chisq')[2,5]                            
}
print(length(which(fit2.p&lt;.05)))
</code></pre>
"
"0.0669649530182425","0.0676510091491738"," 93225","<p>I am doing multiple regression with Gas production (l/d) as response variable, and flow rate (l/h) and COD influent (g/l) as explanatory variables in R. I made two models. </p>

<pre><code>mod1 &lt;- gas ~ flow + COD 

r^2 = 0.64

f stat= 56.62(2,64) with 0.0000.... p value.

mod2 &lt;- gas ~ flow*COD

r^2 = 0.81

f stat = 89.14 (3,63) with also very low p value.
</code></pre>

<p>Now for both cases, what the r^2 says and the f stat says?
I did the anova test of the both models. how to interpret them? </p>

<pre><code>Model 1: Gas ~ Flow + CODin

Model 2: Gas ~ Flow * CODin

Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
1     64 24307                                  
2     63 12835  1     11472 56.311 2.631e-10 ***
</code></pre>
"
"0.0669649530182425","0.0676510091491738"," 93438","<p>I've been wondering something for a while. If you run a simple regression model in R and then perform a step-wise selection (it doesn't have to be the way I typed the code below), how do you extract some of the ""relevant"" information of the model in R suchs as Odds Ratios and/or confidence intervals? </p>

<p>The general model would look the following:</p>

<pre><code>glm&lt;-glm(y~x+z+n+p, family=binomial(link=""logit))
step(glm, direction=""backward"", test=""F"")
</code></pre>

<p>Interestingly, in the newest version of R, I can't save an object as the step function. </p>

<pre><code>stepmodel&lt;-step(glm, direction...)
</code></pre>

<p>Only runs the stepwise selection model. 
So performing <code>step$anova</code>or <code>conf(step)</code>- or even better <code>exp(conf(step))</code>etc. doesn't seem possible....</p>

<p>Any ideas on this? Also, would any of you know of a package, where a stepwise variable selection of a cox-regression model is possible? </p>

<p>Thanks a mill for your thoughts and help. </p>

<p>Cheers, </p>

<p>Oliver</p>
"
"0.171165004875037","0.185269918744954"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.200894859054728","0.202953027447522"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.0334824765091213","0.0676510091491738","100670","<p>What is the purpose of the ANOVA table? I once learned that you can only interpret the significance (p-value) of a multi-level discrete variable, or an interaction effect using the ANOVA table. Why? Why can't you use the p-value outputs of the regression? Why do people look at the ANOVA table in practice? </p>

<p>GLM</p>

<pre><code>Coefficients:
                                    Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                          -1.9800     1.3697  -1.446    0.148
ConnectivityHIGH                      1.9214     1.6361   1.174    0.240
SusceptibilityHIGH                    0.8636     1.7183   0.503    0.615
ConnectivityHIGH:SusceptibilityHIGH  -0.6555     2.1348  -0.307    0.759
</code></pre>

<p>ANOVA</p>

<pre><code>                            Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)
NULL                                           19     3.6712         
Connectivity                 1  2.43379        18     1.2374   0.1187
Susceptibility               1  0.19710        17     1.0403   0.6571
Connectivity:Susceptibility  1  0.09598        16     0.9443   0.7567
</code></pre>
"
"0.0473513723810378","0.047836487323494","100751","<p>In one of the slides of a statistics course that I followed the following about using drop1 or ANCOVA is stated.</p>

<blockquote>
  <p>Using drop1 the p-values shown are p-values for deleting one variable at a time from the full model, whereas the p-values in the output of anova are sequential, as in a step-up strategy. This problem does not arise in ANOVA or linear regression, only in ANCOVA and mixed models.)</p>
</blockquote>

<p>I looked at it for 20 minutes, and cannot understand that if this is true why someone would still use the command <code>anova()</code> in R anyway.</p>

<p>Does anyone have an idea?</p>

<p>The reason why I ask this question here is because the subject was given a few months ago.</p>
"
"0.105880887471907","0.10696563746014","101020","<p>I am well aware how to read the model summary in R for a regression model when a factor is included. The ""first"" level, in terms of ABC, is regarded as the base level to which all further levels of that factor are compared to. In an ANOVA-style model the baseline value is found in the intercept (equals the mean response value for base-level class).</p>

<p>However, if one or several factors are mixed with continuous predictors, then how can I see what the base-level values are at all? to what would I compare? </p>

<p>In the model output below, there are two factors:</p>

<ol>
<li>LandUse (4 Levels)</li>
<li>Type_LU (4 Levels)</li>
</ol>

<p>I see now for example that <code>LandUseLow</code> is 0.35 units higher than the base-line <code>LandUseHigh</code>. Would one now simply look at the mean response for the class <code>LandUseHigh</code> and compare? Is it that simple? </p>

<pre><code>My_model: 
                  Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)
(Intercept)      4.6086772  1.7754606   1.7773711   2.593  0.00951 **
DiTempRange     -0.1409464  0.0764872   0.0765969   1.840  0.06575 .
LandUseLow       0.3520743  0.5989777   0.5997903   0.587  0.55721
LandUseMedium    0.2741413  0.3668149   0.3675811   0.746  0.45579
LandUseNone     -1.0128945  0.5735342   0.5744652   1.763  0.07787 .
MAP              0.0048810  0.0009128   0.0009142   5.339    1e-07 ***
Rivier           0.3502782  0.2252743   0.2257546   1.552  0.12076
TempRange        0.0823410  0.0606546   0.0607942   1.354  0.17560
Tmean           -0.1762862  0.0994486   0.0996353   1.769  0.07684 .
TYPE_LUconserva -0.9312487  0.4770681   0.4781244   1.948  0.05145 .
TYPE_LUprivate  -0.4839229  0.3289011   0.3296201   1.468  0.14207
TYPE_LUstate     0.0004062  0.4079678   0.4089744   0.001  0.99921
logVRM           0.1370973  0.1140166   0.1142342   1.200  0.23008
logTWI          -0.0735540  0.4195267   0.4202589   0.175  0.86106
logDAH           1.7132823  3.5937000   3.6028996   0.476  0.63441
</code></pre>
"
"0.0947027447620757","0.071754730985241","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.0947027447620757","0.071754730985241","104548","<p>I followed <a href=""http://rtutorialseries.blogspot.hk/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">this tutorial</a> to learn Hierarchical Linear Regression (HLR) in R, but couldn't understand how to interpret its sample output of <code>&gt;anova(model1,model2,model3)</code></p>

<p><img src=""http://i.stack.imgur.com/MxXIM.png"" alt=""enter image description here""></p>

<p>The tutorial simply says </p>

<blockquote>
  <p>each predictor added along the way is making an important contribution to the overall model.</p>
</blockquote>

<p>But I would like some more details to <strong>quantify</strong> the contribution of each explanatory variable, like:</p>

<ol>
<li><p>""UNEM"" explains <code>X</code> (or <code>X%</code>) variance</p></li>
<li><p>Adding the ""HGRAD"" variable explains <code>Y</code> (or <code>Y%</code>) more variance</p></li>
<li><p>Adding the ""INC"" variable further explains <code>Z</code> (or <code>Z%</code>) more variance</p></li>
</ol>

<p>So, can I get the value of <code>X</code>, <code>Y</code>, and <code>Z</code> using the above ANOVA table? How? Specifically, what do <code>Res.Df</code>, <code>RSS</code>, <code>Sum of Sq</code> mean in this ANOVA table?</p>
"
"0.0669649530182425","0.0338255045745869","108899","<p>Can anyone explain the theory (or the formula) about computing Sum Sq (bold highligh below) related to regression items?  The Wikipedia <a href=""http://en.wikipedia.org/wiki/Partition_of_sums_of_squares"" rel=""nofollow"">link</a> gives an introduction on how to calculate the total, model, and regression sum of squares. Is it similar to the Sum Sq computation? Is the regression sum of squares equal to (0.000437+ 0.002545+ 0.060984+ 0.062330+ 0.060480)?</p>

<pre><code>TraingData &lt;- data.frame(x1 = c(3.532,2.868,2.868,3.532,2.868,2.536,3.864),
                         x2 = c(1.992,1.992,1.328,1.328,1.328,1.66,1.66),
                         y  = c(9.040330254,8.900894412,8.701929163,9.057944749,
                                8.701929163,8.74317832,9.10859913)
                         )
lm.sol &lt;- lm(y~1+x1+x2+I(x1^2)+I(x2^2)+I(x1*x2), data=TraingData)
anova(lm.sol)

Analysis of Variance Table

Response: y
            Df   **Sum Sq**     Mean       Sq F    value Pr(&gt;F)
x1          1   0.000437  0.000437    0.1055    0.8001
x2          1   0.002545  0.002545    0.6141    0.5768
I(x1^2)     1   0.060984  0.060984   14.7162    0.1623
I(x2^2)     1   0.062330  0.062330   15.0409    0.1607
I(x1 * x2)  1   0.060480  0.060480   14.5945    0.1630
Residuals   1   0.004144  0.004144  
</code></pre>
"
"0.0546766551813808","0.0828552264999161","109077","<p>I am trying to check the assumptions of a two-way ANCOVA. 
So in my model I have</p>

<ul>
<li>two factors (F1, F2)</li>
<li>one dummy coded two level covariate (C)</li>
<li>one dependent variable (D)</li>
</ul>

<p>In order to check the assumption of homogeneity of regression slopes I tried
to perform an ANOVA with type 3 sums for the model D ~ F1*F2*C to see whether any
interactions with the covariate might be significant.
Using the Anova function from the car package this corresponds to</p>

<pre><code>modd&lt;-aov(D~F1*F2*C)
Anova(modd,type=3)
</code></pre>

<p>However, I encounter the following Error message:</p>

<pre><code>Error in Anova.III.lm(mod, error, singular.ok = singular.ok, ...) : 
 there are aliased coefficients in the model
</code></pre>

<p>My question is, whether it makes sense for the homogeneity test to force R to compute the
ANOVA anyway by supplying the singular.ok=T option or what else I should do in order to 
check the assumption.</p>
"
"0.0820149827720712","0.0828552264999161","111902","<p>I am conducting a two-sample test (1-way ANOVA with 2 treatments), and the goal is to estimate the ratio of cell means assuming that the data are lognormal. A simple approach is to log the response and fit a model </p>

<p>$\log Y = b_0 + b_1 * X$</p>

<p>and then estimate the ratio as</p>

<p>$R = e^{b_1}$</p>

<p>However, that gives the ratio of geometric cell means rather than arithmetic cell means. </p>

<p>I assumed that if I fit a ""proper"" lognormal model using either <code>gamlss</code> in R or <code>PROC GLIMMIX</code> in SAS, I will get the ratio of arithmetic means, but for some reason both procedures generate the same slope as the $\log Y$ regression.</p>

<p>This is odd because when I use this approach with Poisson or Negative Binomial regression, I do get the ratio of arithmetic means. What am I missing?</p>

<hr>

<p>P.S.</p>

<p>I think I identified the source of confusion, but I don't have an explanation for it. A lognormal setup with the identity link function is:</p>

<p>$\log Y_1 \sim N(b_0, \sigma^2)$</p>

<p>$\log Y_2 \sim N(b_0 + b_1, \sigma^2)$</p>

<p>which implies </p>

<p>$\frac{E[Y_2]}{E[Y_1]} = \frac{e^{b_0 + b_1 +\sigma^2/2}}{e^{b_0 + \sigma^2/2}} = e^{b_1}$</p>

<p>To me, it means that $e^{b_1}$ should have a point estimate equal to the ratio of arithmetic means for the original response.</p>

<p>On the other hand,</p>

<p>$E[\log Y_1] = b_0$</p>

<p>$E[\log Y_2] =  b_0 + b_1$</p>

<p>$b_0$ is estimated as arithmetic mean of $\log Y_1$, $b_0 + b_1$ is estimated as arithmetic mean of $\log Y_2$. Hence, $e^{b_1}$ should have
a point estimate equal to the ratio of geometric means for the original response, and it does, given the output from those two packages. Where did I make a mistake?</p>
"
"0.105880887471907","0.10696563746014","113756","<p>I'd like to test the <em>anova rbf kernel</em> included in the <strong>kernlab</strong> package in <strong>caret</strong>. Following excelent tutorial (<a href=""https://topepo.github.io/caret/custom_models.html"" rel=""nofollow"">https://topepo.github.io/caret/custom_models.html</a>) I've come up with the following code:</p>

<pre><code>SVManova &lt;- list(type = ""Regression"", library = ""kernlab"", loop = NULL)
prmanova &lt;- data.frame(parameter = c(""C"", ""sigma"", ""degree"", ""eps""),
                     class = rep(""numeric"", 4),
                     label = c(""Cost"", ""Sigma"", ""Degree"", ""Eps""))
SVManova$parameters &lt;- prmanova
    svmGridanova &lt;- function(x, y, len = NULL) {
    library(kernlab)
    sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)
    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,
                C = 2 ^(-5:len), degree = 1:2) # len = tuneLength in train
    }
    SVManova$grid &lt;- svmGridanova
svmFitanova &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  ksvm(x = as.matrix(x), y = y,
       kernel = ""anovadot"",
       kpar = list(sigma = param$sigma, degree = param$degree),
       C = param$C, epsilon = param$epsilon,
       prob.model = classProbs,
       ...) #default type = ""eps-svr""
}
SVManova$fit &lt;- svmFitanova
    svmPredanova &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata)
    SVManova$predict &lt;- svmPredanova
svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type=""probabilities"")
SVManova$prob &lt;- svmProb
    svmSortanova &lt;- function(x) x[order(x$C), ]
SVManova$sort &lt;- svmSortanova
</code></pre>

<p>I then asked for the model to train some dataset:</p>

<pre><code>set.seed(100) #use the same seed to train different models
svrFitanova &lt;- train(R ~ .,
                data = trainSet,
                method = SVManova,
                preProc = c(""center"", ""scale""),
                trControl = ctrl, tuneLength = 20,
                allowParallel = TRUE) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = ""ROC""
#Print the results
svrFitanova
</code></pre>

<p>But I get the following error:</p>

<pre><code>Error in train.default(x, y, weights = w, ...) : 
  The tuning parameter grid should have columns C, sigma, degree, eps
</code></pre>

<p>I don't see why this error occurs.... tune grid has four columns as requested... Any ideas? Thanks</p>
"
"0.26785981207297","0.262147660453049","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.1570467354963","0.129809192515054","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.157594893307774","0.172476907882941","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"NaN","NaN","115748","<p>What is the best way to simulate ANOVA data for a 2 x 2 design with interaction using a regression approach? I want to generate the data so that I know the true regression coefficients of the model when running lm() in R. </p>

<pre><code>lm(A * B, data=df)
</code></pre>

<p>Thanks in advance</p>
"
"0.105880887471907","0.10696563746014","116196","<p>My goal is to investigate a dependent variable which is metric (time in hours). The independent variables include 3 metric, 2 binary (factors), and one factor variable, which consists of 11 districts of a city.</p>

<p>I tried to conduct a GLM.</p>

<p>Can I put all this together in one model? It seems to be difficult to interpret the output!
Should I rather use different/various models, with only one factor per regression?
If I use the GLM which kind of family and link function should I use?</p>

<p>The idle time seems to be a right skewed distribution and thefore I chose a gamma family with a inverse link function. </p>

<p>The output of a model wich contains all the independent variables as decribed above: </p>

<p><img src=""http://i.stack.imgur.com/N1hEi.jpg"" alt=""enter image description here""></p>

<p>How can I eleminate the NAs? Or what do they actually mean? </p>

<p>Moreover the ANOVA test was conducted to get a closer look on the district variable called Bezirk, which shows massive differences in the mean value! Is this consistent with small coefficients in the GLM regression? (The means vary between from 3,7 in T.Mitte and 15 in T.Treptow)</p>

<p>Best regards</p>
"
"0.0820149827720712","0.0552368176666107","116487","<p>I need to predict payment day of the month (1-31) for each client (I have at most 9 month of payments and on average is 5). I have both categorical variables and numerical. I tried to use rpart to do a regression tree (method='anova') but I'm not sure if it's using the nominal variables. </p>

<p>I also tried a regression (linear to start) and doesn't work good either, but it's better then the regression tree.</p>

<p>If I use a Weibull for this, will it mean that each client is going to have a parameter of shape and scale? what about the other variables? How can I insert them into the distribution?</p>

<p>So, what model would you recommend?</p>

<p>Thanks</p>
"
"0.0473513723810378","0.047836487323494","117489","<p>From my limited statistical knowledge, I could use MANOVA if I had multiple independent variables (x1, x2...xn). What can I do (specifically in R) with one ""x"" variable and multiple ""y"" groups? I'm trying to see if there is any relationship between the y's with respect to their regression with x. I've already set up a loop that computes bivariate, piecewise linear regressions between each pair (x-y1, x-y2, ... x-yn), but that does not include any analysis of variation between the y variables. Does anybody know how I might do this (in a statistically sound manner, of course) in R? My data looks like this:</p>

<pre><code>x         y1       y2      y3      y4      y5
4.19    5.51    19.76   50.00   19.36   54.07
8.60    10.16   33.01   82.99   38.48   44.95
8.03    7.82    31.29   79.05   40.12   59.18
6.64    8.99    27.13   69.13   30.44   59.02
7.03    8.22    25.29   74.45   36.02   50.88
1.50    5.90    10.69   22.88   10.34   34.50
4.36    7.61    19.27   44.47   20.06   24.62
7.17    8.30    26.72   68.68   31.61   20.16
2.68    5.61    14.25   37.07   15.20   67.75
7.91    7.75    30.93   82.01   38.62   65.36
3.74    5.24    16.42   40.17   17.54   15.19
</code></pre>
"
"0.133929906036485","0.101476513723761","117783","<p>Despite having only a single binary outcome for each ID, there are multiple correlated measurements for the same test for each ID at different timepoints. The individual IDÂ´s are obviously independent, but the measurements of the same test at different time-points are not independent. The aim is to see whether the test can discern if the patients is cured or not. </p>

<p>-I cannot decide whether IÂ´d need to fit a mixed-level logistic regression model or if a regular logistic regression model would suffice. Is it possible somehow to fit a logistic regression model with time varying covariates like in cox models?</p>

<p>-Is it possible to use a clustering or CART based model (allowing for the longitudinal independent variable) instead that would be easier to comprehend in a clinical setting?</p>

<p>-I could turn the question around and do repeated measure Anova with <code>summary(aov(val~Long_term*time+Error(id),data=stat))</code>- however, 1)only time and the interaction term are significant, leaving me uncertain how to treat the other main effect of Long_term in face of only a significant interaction and 2)it feels contraintuitive to set an independent variable as a dependent variable when using anova.</p>

<pre><code>&gt; dput(stat)
structure(list(Long_term = structure(c(2L, 2L, 2L, 2L, 2L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,  2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L,  2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L,  2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,  2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,  1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L,  2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L,  2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,  2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L), .Label = c(""No"",  ""Yes""), class = ""factor""), id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L,  8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L,  2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,  16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,  11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L,  5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,  19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,  14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,  9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L,  3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,  17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,  12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L,  7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,  1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,  15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,  10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L,  4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,  18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,  13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,  8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L,  2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,  16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,  11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L,  5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,  19L), time = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  12, 12, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,  18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 24, 24, 24, 24,  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 30, 30, 30, 30,  30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 36,  36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,  36, 36, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,  42, 42, 42, 42, 42, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,  48, 48, 48, 48, 48, 48, 48, 48, 54, 54, 54, 54, 54, 54, 54, 54,  54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 60, 60, 60, 60, 60,  60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 66, 66,  66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66,  66, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,  72, 72, 72, 72, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,  78, 78, 78, 78, 78, 78, 78, 84, 84, 84, 84, 84, 84, 84, 84, 84,  84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 90, 90, 90, 90, 90, 90,  90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 96, 96, 96,  96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,  102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102,  102, 102, 102, 102, 102, 102), val = c(273, 194, 618, 755, 802,  395, 2438, 482, 502, 692, 607, 618, 579, 864, 579, 453, 673,  572, 707, 57, 373, 1197, 1026, NA, 697, 712, NA, NA, 616, NA,  NA, NA, NA, NA, NA, NA, 76, 1128, 560, 76, 819, 982, 303, 1294,  267, 1117, 346, 996, 652, 95, 951, 3250, 1584, 948, 981, 465,  411, 57, 197, 535, 498, 87, 1382, 210, 1649, 96, 450, 252, 42,  1086, 2137, 1395, 464, 1388, 532, 67, 25, 230, 566, 545, 38,  691, 216, 1412, 33, 151, 113, 29, 663, 806, 528, 240, 1508, 421,  50, 39, NA, 182, 412, 32, 414, 232, 868, 791, 201, 86, 33, 250,  345, 224, 381, 1069, 536, NA, NA, NA, 500, 312, NA, 287, 97,  227, 653, 69, 69, NA, NA, 225, 308, 256, 963, 420, NA, NA, NA,  368, 605, NA, 399, 69, 77, 20, 39, 70, NA, 122, 306, 103, 175,  807, 530, NA, NA, NA, 246, 443, NA, 363, 87, 39, NA, 25, 63,  NA, 163, 289, 172, 128, 1019, 582, NA, NA, NA, 231, 820, NA,  284, NA, 40, NA, NA, NA, NA, 238, 288, 217, NA, 903, 471, NA,  NA, NA, 236, 577, NA, 461, NA, 691, NA, NA, NA, NA, 158, 170,  168, NA, 681, 434, NA, NA, NA, 399, 634, NA, 85, NA, 83, NA,  NA, NA, NA, 72, 419, NA, NA, 912, NA, NA, NA, NA, NA, 635, NA,  295, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,  NA, NA, 138, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, 251, NA, NA, NA, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 37, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 132,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), row.names = c(NA,  -323L), .Names = c(""Long_term"", ""id"", ""time"", ""val""), class = ""data.frame"") 
</code></pre>
"
"0.29947637411774","0.294980897647825","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.177721986002948","0.209466539620948","120549","<p>It is a basic question but I could not find clear answer on my reading. I am trying to find independent predictors of Infant.Mortality in data frame 'swiss' in R. </p>

<pre><code>&gt; head(swiss)
             Fertility Agriculture Examination Education Catholic Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96             22.2
Delemont          83.1        45.1           6         9    84.84             22.2
Franches-Mnt      92.5        39.7           5         5    93.40             20.2
Moutier           85.8        36.5          12         7    33.77             20.3
Neuveville        76.9        43.5          17        15     5.16             20.6
Porrentruy        76.1        35.3           9         7    90.57             26.6
</code></pre>

<p>Following are the results using lm and I find only Fertility to be a significant predictor: </p>

<pre><code>&gt; fit = lm(Infant.Mortality~., data=swiss)
&gt; summary(fit)

Call:
lm(formula = Infant.Mortality ~ ., data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.2512 -1.2860  0.1821  1.6914  6.0937 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.667e+00  5.435e+00   1.595  0.11850
Fertility    1.510e-01  5.351e-02   2.822  0.00734    #  &lt;&lt;&lt;&lt; NOTE P VALUE HERE
Agriculture -1.175e-02  2.812e-02  -0.418  0.67827
Examination  3.695e-02  9.607e-02   0.385  0.70250
Education    6.099e-02  8.484e-02   0.719  0.47631
Catholic     6.711e-05  1.454e-02   0.005  0.99634

Residual standard error: 2.683 on 41 degrees of freedom
Multiple R-squared:  0.2439,    Adjusted R-squared:  0.1517 
F-statistic: 2.645 on 5 and 41 DF,  p-value: 0.03665
</code></pre>

<p>Following are the graphs:</p>

<pre><code>plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/lopHb.png"" alt=""enter image description here""></p>

<p>On performing stepwise regression, following are the results: </p>

<pre><code>&gt; step &lt;- stepAIC(fit, direction=""both""); 
Start:  AIC=98.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

              Df Sum of Sq    RSS     AIC
- Catholic     1     0.000 295.07  96.341
- Examination  1     1.065 296.13  96.511
- Agriculture  1     1.256 296.32  96.541
- Education    1     3.719 298.79  96.930
&lt;none&gt;                     295.07  98.341
- Fertility    1    57.295 352.36 104.682

Step:  AIC=96.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education

              Df Sum of Sq    RSS     AIC
- Examination  1     1.320 296.39  94.551
- Agriculture  1     1.395 296.46  94.563
- Education    1     5.774 300.84  95.252
&lt;none&gt;                     295.07  96.341
+ Catholic     1     0.000 295.07  98.341
- Fertility    1    72.609 367.68 104.681

Step:  AIC=94.55
Infant.Mortality ~ Fertility + Agriculture + Education

              Df Sum of Sq    RSS     AIC
- Agriculture  1     4.250 300.64  93.220
- Education    1     6.875 303.26  93.629
&lt;none&gt;                     296.39  94.551
+ Examination  1     1.320 295.07  96.341
+ Catholic     1     0.255 296.13  96.511
- Fertility    1    79.804 376.19 103.758

Step:  AIC=93.22
Infant.Mortality ~ Fertility + Education

              Df Sum of Sq    RSS     AIC
&lt;none&gt;                     300.64  93.220
- Education    1    21.902 322.54  94.525
+ Agriculture  1     4.250 296.39  94.551
+ Examination  1     4.175 296.46  94.563
+ Catholic     1     2.318 298.32  94.857
- Fertility    1    85.769 386.41 103.017
&gt; 
&gt; 
&gt; step$anova
Stepwise Model Path 
Analysis of Deviance Table

Initial Model:
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

Final Model:
Infant.Mortality ~ Fertility + Education


           Step Df     Deviance Resid. Df Resid. Dev      AIC
1                                      41   295.0662 98.34145
2    - Catholic  1 0.0001533995        42   295.0663 96.34147
3 - Examination  1 1.3199421028        43   296.3863 94.55125
4 - Agriculture  1 4.2499886025        44   300.6363 93.22041
&gt; 
&gt; 
</code></pre>

<p>Summary shows Education also has trend towards significant association: </p>

<pre><code>summary(step)

Call:
lm(formula = Infant.Mortality ~ Fertility + Education, data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6927 -1.4049  0.2218  1.7751  6.1685 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.63758    3.33524   2.590 0.012973
Fertility    0.14615    0.04125   3.543 0.000951
Education    0.09595    0.05359   1.790 0.080273

Residual standard error: 2.614 on 44 degrees of freedom
Multiple R-squared:  0.2296,    Adjusted R-squared:  0.1946 
F-statistic: 6.558 on 2 and 44 DF,  p-value: 0.003215
</code></pre>

<p>What do I conclude? Is Education an important predictor or not?</p>

<p>Also, do the graphs using plot(fit) add any significant information?</p>

<p>Thanks for your help.</p>

<hr>

<p>Edit: 
I ran shapiro test on all columns and found 2 are not normally distributed: </p>

<pre><code>Fertility : P= 0.3449466 (Normally distributed) 
Agriculture : P= 0.1930223 (Normally distributed) 
Examination : P= 0.2562701 (Normally distributed) 
Education : P= 1.31202e-07 (--- NOT Normally distributed! ---) 
Catholic : P= 1.20461e-07 (--- NOT Normally distributed! ---) 
Infant.Mortality : P= 0.4978056 (Normally distributed) 
</code></pre>

<p>Does that make a difference? </p>
"
"0.0669649530182425","0.0676510091491738","121020","<p>I am trying to do Logistic Regression in R.</p>

<p>My data set contains more than 50 variables. Some of them are factor (qualitative variable) and others are independent variable(quantitative). I would like to get the significance of the variables from their p-value.</p>

<p>So far I came to know, I can do ANCOVA test to calculate p value of the factors. It (ANCOVA) combines features of both ANOVA and regression. It augments the ANOVA model with one or more additional quantitative variables, called covariates, which are related to the response variable.</p>

<p>How can I calculate P-value of quantitative variables? Or if I am wrong about ANCOVA, what other possibilities are available?</p>

<p>Any suggestion or help will be appreciated.</p>
"
"0.0669649530182425","0.0338255045745869","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.0669649530182425","0.0676510091491738","125455","<p>I want to meta-analyze the interaction effect of a 2x2 ANOVA.</p>

<p>(I am <em>not</em> talking about an interaction in the meta-regression, as in <a href=""http://stats.stackexchange.com/questions/71404/main-effects-and-interaction-in-multivariate-meta-analysis-network-meta-analysi"">this question</a> but about an interaction as the focal effect that should be meta-analytically summarized).</p>

<p><strong>What is the best way to code the interaction effect size for a subsequent meta-analysis?</strong>
(preferably in the <code>metafor</code> package)</p>
"
"0.105880887471907","0.10696563746014","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.164029965544142","0.15190124858318","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.0334824765091213","0.0676510091491738","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.17290276521281","0.157206739073326","127134","<p><strong>Updated</strong></p>

<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure (expressed as decimal of year) = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 16-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
Mechanism - Mechanism of injury = Fall &lt;2m, Fall &gt;2m, Shooting/stabbing, RTC (Road Traffic Collision), Other
neuroFirst - Location of first admission (Neurosurgical Unit) = NSU vs. Non-NSU
rcteye - Pupil reactivity = NA / Both unreactive = O, 1 reactive = 1, both reactive = 2
rcteyeYN - dummy = 0 or 1 for presence or absence of data
GCS - Glasgow Coma Scale = 3-15
GCSYN - dummy = 0 or 1 for presence or absence of data
</code></pre>

<p>Dummy variables were included to enable a larger sample size where the majority of cases were excluding  <code>GCS</code> and <code>rcteye</code> variables (missing not at random).</p>

<p>In order to test for interactions, initially I ran the following:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN + GCS + GCSYN + rcs(Yeardecimal))^2, data = ASDH_Paper1.1)
</code></pre>

<p>but when I did I got the following error:</p>

<pre><code>singular information matrix in lrm.fit (rank= 151 ).  Offending variable(s):
GCSYN * Yeardecimal''' GCSYN * Yeardecimal' GCSYN * Yeardecimal GCS * Yeardecimal''' GCS * Yeardecimal GCS * GCSYN rcteyeYN * Yeardecimal''' rcteyeYN * Yeardecimal'' rcteyeYN * Yeardecimal rcteyeYN * GCSYN rcteye * Yeardecimal''' rcteye * Yeardecimal rcteye * rcteyeYN Mechanism=RTC * Yeardecimal''' Mechanism=Other * Yeardecimal''' Mechanism=Fall &gt; 2m * Yeardecimal''' Mechanism=Shooting / Stabbing * Yeardecimal Mechanism=RTC * Yeardecimal Mechanism=Other * Yeardecimal Mechanism=Fall &gt; 2m * Yeardecimal neuroFirst * Yeardecimal ISS'' * Yeardecimal''' ISS * Yeardecimal''' ISS'' * Yeardecimal'' ISS'' * Yeardecimal ISS' * Yeardecimal ISS * Yeardecimal ISS'' * GCSYN ISS'' * rcteyeYN ISS'' * Mechanism=RTC Age'' * Yeardecimal''' Age'' * Yeardecimal'' Age''' * Yeardecimal' Age''' * Yeardecimal Age'' * Yeardecimal Age' * Yeardecimal Age * Yeardecimal Age'' * GCSYN Age''' * rcteyeYN 
Error in lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + neuroFirst + Mechanism +  : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>The only way I could run the model is with an adjustment. <code>Yeardecimal</code> is excluded from any interaction as is the interaction of <code>GCS:GCSYN</code> and <code>rcteye:rcteyeYN</code> which produced the same error as written above. It made sense to exclude the interactions between a variable and its missing dummy but I am not sure what to do about <code>Yeardecimal</code>:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN) * (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + GCS + GCSYN) + rcs(Yeardecimal), data = ASDH_Paper1.1)
</code></pre>

<p>From this model the following interactions were identified with an <code>anova</code> output:</p>

<pre><code>&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor                                                Chi-Square d.f. P     
 Age  (Factor+Higher Order Factors)                    130.42      52  &lt;.0001
  All Interactions                                      78.68      48  0.0034
  Nonlinear (Factor+Higher Order Factors)               46.53      39  0.1901
 ISS  (Factor+Higher Order Factors)                    181.65      42  &lt;.0001
  All Interactions                                      52.43      39  0.0738
  Nonlinear (Factor+Higher Order Factors)               55.01      28  0.0017
 neuroFirst  (Factor+Higher Order Factors)              37.68      16  0.0017
  All Interactions                                      11.54      15  0.7136
 Mechanism  (Factor+Higher Order Factors)               63.72      52  0.1277
  All Interactions                                      58.35      48  0.1455
 rcteye  (Factor+Higher Order Factors)                 242.07      15  &lt;.0001
  All Interactions                                      19.39      14  0.1507
 rcteyeYN  (Factor+Higher Order Factors)               204.58      15  &lt;.0001
  All Interactions                                      29.88      14  0.0079
 GCS  (Factor+Higher Order Factors)                    162.81      15  &lt;.0001
  All Interactions                                      11.62      14  0.6365
 GCSYN  (Factor+Higher Order Factors)                   94.50      15  &lt;.0001
  All Interactions                                      41.74      14  0.0001
 Yeardecimal                                            51.96       4  &lt;.0001
  Nonlinear                                             10.27       3  0.0164
 Age * ISS  (Factor+Higher Order Factors)               11.90      12  0.4534
  Nonlinear                                              9.40      11  0.5851
  Nonlinear Interaction : f(A,B) vs. AB                  9.40      11  0.5851
  f(A,B) vs. Af(B) + Bg(A)                               7.96       6  0.2411
  Nonlinear Interaction in Age vs. Af(B)                 8.75       9  0.4605
  Nonlinear Interaction in ISS vs. Bg(A)                 8.58       8  0.3790
 Age * neuroFirst  (Factor+Higher Order Factors)         2.66       4  0.6166
  Nonlinear                                              2.05       3  0.5624
  Nonlinear Interaction : f(A,B) vs. AB                  2.05       3  0.5624
 Age * Mechanism  (Factor+Higher Order Factors)         17.58      16  0.3493
  Nonlinear                                             13.82      12  0.3127
  Nonlinear Interaction : f(A,B) vs. AB                 13.82      12  0.3127
 Age * GCS  (Factor+Higher Order Factors)                6.24       4  0.1819
  Nonlinear                                              3.89       3  0.2741
  Nonlinear Interaction : f(A,B) vs. AB                  3.89       3  0.2741
 Age * GCSYN  (Factor+Higher Order Factors)             20.11       4  0.0005
  Nonlinear                                              8.86       3  0.0312
  Nonlinear Interaction : f(A,B) vs. AB                  8.86       3  0.0312
 ISS * neuroFirst  (Factor+Higher Order Factors)         3.23       3  0.3571
  Nonlinear                                              0.87       2  0.6480
  Nonlinear Interaction : f(A,B) vs. AB                  0.87       2  0.6480
 ISS * Mechanism  (Factor+Higher Order Factors)         23.95      12  0.0206
  Nonlinear                                             20.66       8  0.0081
  Nonlinear Interaction : f(A,B) vs. AB                 20.66       8  0.0081
 ISS * GCS  (Factor+Higher Order Factors)                0.77       3  0.8570
  Nonlinear                                              0.42       2  0.8102
  Nonlinear Interaction : f(A,B) vs. AB                  0.42       2  0.8102
 ISS * GCSYN  (Factor+Higher Order Factors)              6.53       3  0.0886
  Nonlinear                                              2.35       2  0.3085
  Nonlinear Interaction : f(A,B) vs. AB                  2.35       2  0.3085
 neuroFirst * Mechanism  (Factor+Higher Order Factors)   2.45       4  0.6533
 neuroFirst * GCS  (Factor+Higher Order Factors)         0.00       1  0.9726
 neuroFirst * GCSYN  (Factor+Higher Order Factors)       1.39       1  0.2382
 Mechanism * GCS  (Factor+Higher Order Factors)          0.10       4  0.9987
 Mechanism * GCSYN  (Factor+Higher Order Factors)        1.74       4  0.7828
 Age * rcteye  (Factor+Higher Order Factors)             8.66       4  0.0702
  Nonlinear                                              7.29       3  0.0633
  Nonlinear Interaction : f(A,B) vs. AB                  7.29       3  0.0633
 ISS * rcteye  (Factor+Higher Order Factors)             4.18       3  0.2424
  Nonlinear                                              1.49       2  0.4744
  Nonlinear Interaction : f(A,B) vs. AB                  1.49       2  0.4744
 neuroFirst * rcteye  (Factor+Higher Order Factors)      0.10       1  0.7460
 Mechanism * rcteye  (Factor+Higher Order Factors)       3.44       4  0.4867
 rcteye * GCS  (Factor+Higher Order Factors)             2.30       1  0.1297
 rcteye * GCSYN  (Factor+Higher Order Factors)           2.57       1  0.1090
 Age * rcteyeYN  (Factor+Higher Order Factors)           7.23       4  0.1242
  Nonlinear                                              7.23       3  0.0649
  Nonlinear Interaction : f(A,B) vs. AB                  7.23       3  0.0649
 ISS * rcteyeYN  (Factor+Higher Order Factors)           2.47       3  0.4814
  Nonlinear                                              0.11       2  0.9462
  Nonlinear Interaction : f(A,B) vs. AB                  0.11       2  0.9462
 neuroFirst * rcteyeYN  (Factor+Higher Order Factors)    0.12       1  0.7280
 Mechanism * rcteyeYN  (Factor+Higher Order Factors)     1.81       4  0.7701
 rcteyeYN * GCS  (Factor+Higher Order Factors)           3.70       1  0.0543
 rcteyeYN * GCSYN  (Factor+Higher Order Factors)         8.74       1  0.0031
 TOTAL NONLINEAR                                       102.74      64  0.0015
 TOTAL INTERACTION                                     178.52     103  &lt;.0001
 TOTAL NONLINEAR + INTERACTION                         241.87     111  &lt;.0001
 TOTAL                                                 889.91     123  &lt;.0001
</code></pre>

<p>The <code>summary</code> function revealed the following results:</p>

<pre><code>             Effects              Response : Survive 

 Factor                                    Low    High   Diff. Effect       S.E.   Lower 0.95 Upper 0.95    
 Age                                         37.6   72.0 34.40         0.15   0.38   -0.58      8.900000e-01
  Odds Ratio                                 37.6   72.0 34.40         1.16     NA    0.56      2.430000e+00
 ISS                                         20.0   26.0  6.00        -1.34   0.31   -1.95     -7.400000e-01
  Odds Ratio                                 20.0   26.0  6.00         0.26     NA    0.14      4.800000e-01
 neuroFirst                                   0.0    1.0  1.00        -0.23   0.37   -0.95      5.000000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.80     NA    0.39      1.650000e+00
 rcteye                                       0.0    2.0  2.00         3.20   0.50    2.22      4.170000e+00
  Odds Ratio                                  0.0    2.0  2.00        24.41     NA    9.24      6.452000e+01
 rcteyeYN                                     0.0    1.0  1.00        -3.34   0.44   -4.21     -2.480000e+00
  Odds Ratio                                  0.0    1.0  1.00         0.04     NA    0.01      8.000000e-02
 GCS                                          0.0   12.0 12.00         1.94   0.49    0.98      2.890000e+00
  Odds Ratio                                  0.0   12.0 12.00         6.94     NA    2.67      1.799000e+01
 GCSYN                                        0.0    1.0  1.00        -1.32   0.45   -2.20     -4.400000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.27     NA    0.11      6.400000e-01
 Yeardecimal                               2005.5 2012.4  6.85         0.20   0.12   -0.03      4.400000e-01
  Odds Ratio                               2005.5 2012.4  6.85         1.22     NA    0.97      1.550000e+00
 Mechanism - Fall &gt; 2m:Fall &lt; 2m              1.0    2.0    NA        -0.89   0.35   -1.58     -2.000000e-01
  Odds Ratio                                  1.0    2.0    NA         0.41     NA    0.21      8.200000e-01
 Mechanism - Other:Fall &lt; 2m                  1.0    3.0    NA         0.25   0.42   -0.58      1.080000e+00
  Odds Ratio                                  1.0    3.0    NA         1.28     NA    0.56      2.930000e+00
 Mechanism - RTC:Fall &lt; 2m                    1.0    4.0    NA        -0.68   0.43   -1.52      1.700000e-01
  Odds Ratio                                  1.0    4.0    NA         0.51     NA    0.22      1.190000e+00
 Mechanism - Shooting / Stabbing:Fall &lt; 2m    1.0    5.0    NA        18.97 116.63 -209.63      2.475600e+02
  Odds Ratio                                  1.0    5.0    NA 172906690.96     NA    0.00     3.272814e+107

Adjusted to: Age=54.2 ISS=25 neuroFirst=0 Mechanism=Fall &lt; 2m rcteye=1 rcteyeYN=0 GCS=3 GCSYN=0 
</code></pre>

<p>Remaining questions are:</p>

<p><strong>1</strong> - Is my dummy variable treatment for variables missing not at random appropriate, including the exclusion of interactions with the main term?</p>

<p><strong>2</strong> - Can I resolve the issues with assessing interaction of the Yeardecimal term?</p>

<p><strong>3</strong> - Should I exclude non-significant interaction terms? I read that exclusion only of a ""chunk"" is advised - <a href=""http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model"">Including the interaction but not the main effects in a model</a></p>

<p><strong>4</strong> - Is the odds ratio for each variable the ""Effect"" column? If so, is this the OR between the lowest and highest value of each variable?</p>
"
"0.215079393764029","0.186242470932502","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.0473513723810378","0.047836487323494","130476","<p>This model is a simple linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>And this model adds <code>cyl</code> as a random effect:</p>

<pre><code>library(lme4)
mtcars_mixed_effects &lt;- lmer(mpg ~ wt + (1 | cyl), mtcars)
</code></pre>

<p>Is there a way to test whether adding <code>cyl</code> as random effect is worthwhile? I tried this but it threw an error:</p>

<pre><code>anova(mtcars_mixed_effects, mtcars_lm)
</code></pre>

<p>(please disregard the fact that <code>cyl</code> only has three groups, I'm just using one of R's built in datasets to make question reprodicible).</p>
"
"0.105880887471907","0.10696563746014","131093","<p>I hope this is not a duplicate but I cannot find the answer to this question. In a linear model
$$Y_i = \beta_1 X_{i,1} + \dots + \beta_{p-1} X_{i,p-1} + \varepsilon_i, \qquad i = 1, \ldots, n$$
with the usual assumptions, is the regression sum of squares, $SSR$, still</p>

<p>$$ SSR = \sum_{i=1}^n (\hat{y_i} - \overline{y})^2 \text{ ?}$$
where $\hat{y_i} = X \hat{\beta}$ is the $i$-th fitted value, $\hat{\beta} = (X^TX)^{-1}X^T y$ and $X$ is the design matrix without the column of ones that it would have if we couldn't assume $\beta_0 = 0$.</p>

<p>Now, I'm asking this question because using the ""anova"" function in R, you can obtain the $SSR$ by simply adding the corresponding $SSR$'s of each variable (I believe this is called a type I decomposition), but this doesn't match the $SSR$ as calculated above for a model with $\beta_0 = 0$.</p>

<p>Am I missing something or did I just screw up calculating it? </p>

<p>I had a sample of 2 variables, $X_1$ and $X_2$ with $n=11$ observations, as follows:
$x_1 = (1,4,9,11,3,8,5,10,2,7,6)^T$, $x_2 = (8,2,-8,-10,6,-6,0,-12,4,-2,-4)^T$ and $y=(6,8,1,0,5,3,2,-4,10,-3,5)^T$.</p>

<p>I introduced them in R as y, x1 and x2. Then using anova(lm(y~0+x1+x2)) I got Sum Sq of 14.279 for x1 and 161.846 for x2. Their sum is 176.154.</p>

<p>However, using the design matrix with $x_1$ and $x_2$ as its columns, I got $\beta = (\beta_1, \beta_2)^T = (0.7211, 0.8089)^T$ (which matches the ones obtained in R) so $SSR = 96.37352$, which is obviously different from the one obtained in R.</p>
"
"0.1570467354963","0.158655679740622","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.107382819049576","0.108482956331022","131401","<p>I am running a pooled OLS regression using the plm package in R. Though, my question is more about basic statistics, so I try posting it here first ;)</p>

<p>Since my regression results yield heteroskedastic residuals I would like to try using heteroskedasticity robust standard errors. As a result from <code>coeftest(mod, vcov.=vcovHC(mod, type=""HC0""))</code> I get a table containing estimates, standard errors, t-values and p-values for each independent variable, which basically are my ""robust"" regression results.</p>

<p>For discussing the importance of different variables I would like to plot the share of variance explained by each independent variable, so I need the respective sum of squares. However, using function <code>aov()</code>, I don't know how to tell R to use robust standard errors.</p>

<p>Now my question is: How do I get the ANOVA table/sum of squares that refers to robust standard errors? Is it possible to calculate it based on the ANOVA table from regression with normal standard errors?</p>

<p>Edit:</p>

<p>In other words and disregarding my R-issues:</p>

<p>If R$^2$ is not affected by using robust standard errors, will also the respective contributions to explained variance by the different explanatory variables be unchanged?</p>

<p>Edit:</p>

<p>In R, does <code>aov(mod)</code> actually give a correct ANOVA table for a panelmodel (plm)?</p>
"
"0.189405489524151","0.191345949293976","131459","<p>I'm trying to implement a joint test of the two coefficients comprising a quadratic term in a 2-stage least squares regression.  The quadratic term is endogenous.  I'm using <code>AER</code> in R, and <code>ivreg</code>'s <code>anova</code> method is not giving me the same result as the manual Wald test that I'm checking it with.  I'd basically like to know whether my own manual method is correct or not.  If not, why not, and if so, what <code>AER</code> is doing differently.</p>

<pre><code>1&gt; rm(list=ls())
1&gt; set.seed(1)
1&gt; N &lt;- 100
1&gt; z &lt;- rnorm(N) #The instrument
1&gt; u &lt;- rnorm(N) #The error term
1&gt; x &lt;- 1 + z - .1*z^2 + u + rnorm(N) # x is correlated with the error term u (endogeneity) and the instrument z
1&gt; ex &lt;- 1 + rnorm(N) #an exogenous variable
1&gt; y &lt;- 1 + x-.1*x^2  + ex + u 
1&gt; x2 &lt;- x^2
1&gt; z2 &lt;- z^2
1&gt; summary(lm(y~x+x2+ex))

Call:
lm(formula = y ~ x + x2 + ex)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.91064 -0.57302  0.04697  0.43678  1.62413 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.51137    0.13751   3.719 0.000337 ***
x            1.22946    0.05578  22.042  &lt; 2e-16 ***
x2          -0.03512    0.02145  -1.637 0.104893    
ex           0.97401    0.07933  12.278  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7699 on 96 degrees of freedom
Multiple R-squared:  0.8952,    Adjusted R-squared:  0.892 
F-statistic: 273.5 on 3 and 96 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>BIAS</p>

<pre><code> 1&gt; library(AER)

1&gt; miv = ivreg(y~x+x2+ex|z+z2+ex)
1&gt; summary(miv)

Call:
ivreg(formula = y ~ x + x2 + ex | z + z2 + ex)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.1212 -0.7533 -0.2623  0.6458  3.5927 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.4767     0.4785   3.086  0.00265 ** 
x             1.0678     0.1351   7.902 4.58e-12 ***
x2           -0.2185     0.1014  -2.154  0.03373 *  
ex            0.8690     0.1409   6.167 1.65e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.245 on 96 degrees of freedom
Multiple R-Squared: 0.7259, Adjusted R-squared: 0.7173 
Wald test: 43.79 on 3 and 96 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>MUCH BETTER WITH THE VALID INSTRUMENTS</p>

<p>Wald test using built-in method, versus manual Wald test:</p>

<pre><code>1&gt; mnull = ivreg(y~ex)
        1&gt; anova(miv,mnull,vcov=vcov(miv))
        Wald test

        Model 1: y ~ x + x2 + ex | z + z2 + ex
        Model 2: y ~ ex
          Res.Df Df      F    Pr(&gt;F)    
        1     96                        
        2     98 -2 31.512 3.011e-11 ***
        ---
        Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

        1&gt; #Wald test
        1&gt; B = coef(miv)
        1&gt; r = matrix(c(0,0),2)
        1&gt; R = t(matrix(c(   0,1,0,0
        1+      ,0,0,1,0),4))
        1&gt; V = vcov(miv)
        1&gt; W = t(R%*%B -r) %*% solve(R%*%V%*%t(R)) %*% (R%*%B -r)
        1&gt; W/2
                [,1]
        [1,] 31.5123
</code></pre>

<p>Why is my wald statistic twice the one given by the method?</p>

<pre><code>    1&gt; 1-pf(W/2,nrow(R),N-nrow(R))
                 [,1]
    [1,] 2.706191e-11
</code></pre>

<p>And why are the p-values similar, but not identical?</p>
"
"0.0820149827720712","0.0828552264999161","133107","<p>I have a dataset with seven dependent variables and three independent variables. Now I want to test the significance of a multivariate regression model as a whole. </p>

<p>My model looks like this (using r):</p>

<pre><code>my.model &lt;- lm(cbind(res1,res2,res3,res4,res5,res6,res7) ~ indep1 + indep2+indep1:indep2, data=df)
</code></pre>

<p>now I can use </p>

<pre><code>summary(manova(my.model)) 
</code></pre>

<p>for model testing. I am not sure about this,  how can I interpret the results when I want to test the model as a whole?</p>

<p>this is result of the summary:</p>

<pre><code>                 Df   Pillai approx F num Df den Df    Pr(&gt;F)    
indep1           1 0.067716   35.383      7   3410 &lt; 2.2e-16 ***
indep2           1 0.209308  128.954      7   3410 &lt; 2.2e-16 ***
indep1:indep2    1 0.006977    3.422      7   3410  0.001191 ** 
Residuals                 3416                              
</code></pre>

<p>Many thanks in advance!</p>
"
"NaN","NaN","134012","<p>STATA has recently implemented effect size calculations for regression models in their postestimation procedures <a href=""http://blog.stata.com/2013/09/05/measures-of-effect-size-in-stata-13/"" rel=""nofollow"">measures of effect size in Stata 13</a>. It takes a regression or ANOVA object as input, and returns eta-squared or omega squared as well as their confidence intervals. </p>

<p>Is anyone aware of any such or similar implentation in R software, or how this could be achieved using the contents of lm or aov objects?</p>
"
"0.142054117143114","0.111618470421486","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"NaN","NaN","136137","<p>I am performing a GLM on count data (insurance claims) and I wish to compare Overdispersed Poisson Regression (ODP) against Negative Binomial regression. I would know whether there is a practical index (AIC, logLik) that in standard R could support me in fitting which one to use. I am selecting significant predictors with backard deletion (using anova(fittedModel, test=""Chisq"") type III tests). Theferore it is not assumed the final model within each distribution family to have the same predictor sets.</p>
"
"0.134764368352983","0.151272255204013","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.105880887471907","0.10696563746014","139411","<p>In SPSS, when performing binary logistic regression using multiple categorical predictors, a significance level is detailed for the variable overall in addition to each category. This strikes me as useful as the model is built up as the addition of a predictor may negate the effect of previously added variables. What is this overall significance a measure of?</p>

<p>In R, the summary of my GLMs do not include this information. As I am building models I can anova(model0, model1) to test the impact of a new addition. However, how would one then detect if a previous predictor had become insignificant overall? Is the significance of a single category sufficient to warrant inclusion?</p>

<p>I have read the following article which was helpful:</p>

<p><a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a></p>
"
"0.164029965544142","0.165710452999832","140183","<p>The age old question of comparing sums of squares (SS) between programs has reared its ugly head again. </p>

<p>I am trying to replicate output in SPSS, that was computed using Type 3 Sums of Squares, in R. </p>

<p>I understand that with multiple regressions, there are several ways to get Type 3 SS in R (to match Type 3 output from SPSS). </p>

<p>However, I am running a mixed model using aov (which uses Type 1 SS) and even when I try all the ""usual"" fixes,"" my estimates don't match the Type 3 SS output from SPSS. </p>

<p>First of all, when I run the SPSS syntax using ""/METHOD=SSTYPE(1)"" the results match those I get using this code:</p>

<pre><code>  mymodel&lt;-aov(data=longdat,  DV ~ 1 + Task + Cue + Compatibility + Cue:Task + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + Error(subject/Cue/Compatibility/Cue*Compatibility))
  summary(mymodel)
</code></pre>

<p>So I know the analyses are the same when they use Type 1 SS. </p>

<p>However, when I use:</p>

<pre><code> options(contrasts = c(""contr.sum"",""contr.poly""))
 tt&lt;-lm(DV ~ 1 + Task + Cue + Compatibility + Cue:Task  + Compatibility:Task + Cue:Compatibility + Cue:Compatibility:Task + 1/subject/Cue/Compatibility    /(Cue*Compatibility), data=longdat)
 drop1(tt, ~., test=""F"")
</code></pre>

<p>The results do not match the SPSS Type 3 output. </p>

<p>In attempts to get matching output, I have also tried the Anova function (which can give Type 3 SS)</p>

<pre><code>  Anova(mymodel, type=3, test.statistic=""F"") 
</code></pre>

<p>but I get this error <code>""Error in terms.formula(formula, data = data) : 'data' argument is of the wrong type.""</code></p>

<p>I have also tried using <code>lmer</code>.</p>

<p>Can someone help me get Type 3 Sums of Squares for a mixed model in R?</p>

<p>Thank you! </p>
"
"0.206399847046907","0.175591085521747","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.170727801083421","0.172476907882941","142317","<p>I want to do <strong>multivariate</strong> (with more than 1 response variables) <strong>multiple</strong> (with more than 1 predictor variables) <strong>nonlinear regression</strong> in <strong>R</strong>.</p>

<p>The data I am concerned with are 3D-coordinates, thus they interact with each other, i.e. the x,y,z-coordinates are not independent. So I cannot just call the <em>nls</em> separately for each response variable (which I tried at first). </p>

<p>A subset of the data-frame with 3D-coordinates where x,y,z are the predictive variables and a,b,c the response variables:</p>

<pre><code>              x           y         z           a            b         c
1  -2.26470e-03 -0.05081670 0.0811701 -0.00671079 -0.045721600 0.0705679
2  -9.13106e-05 -0.00670734 0.0724838 -0.00676299 -0.001638430 0.0588486
3   3.81399e-04  0.03556000 0.0782059 -0.00783726  0.038503800 0.0641364
4   1.42293e-03  0.06133920 0.0708688 -0.00820760  0.062697100 0.0572740
5  -5.06043e-02  0.04759040 0.0418189 -0.05949350  0.040427800 0.0266159
6   5.92963e-02  0.04183450 0.0431029  0.05124780  0.038396500 0.0327903
7  -4.44213e-02 -0.00909717 0.0459059 -0.05021130 -0.005634520 0.0329833
8  -3.75400e-02 -0.00625770 0.0567296 -0.04255200 -0.000666089 0.0436465
9  -2.37768e-02 -0.00707318 0.0581552 -0.03048950 -0.001260670 0.0457355
10 -1.56645e-02 -0.01326670 0.0540247 -0.02101350 -0.009021990 0.0413755
</code></pre>

<p><strong>My question:</strong> Is it possible to call the <em>nls</em> function with more than 1 (in my case 3) response variables? In other words is it possible to substitute <em>y</em> in <code>nls(y ~ f(x,y,z, parameters), data)</code> with something like <em>c(a,b,c)</em> or <em>cbind(a,b,c)</em>, such that <code>nls(cbind(a,b,c) ~ f(x,y,z, parameters), data)</code> ?</p>

<p>In the post <a href=""http://stackoverflow.com/questions/12161659/how-to-write-r-formula-for-multivariate-response"">How to write R formula for multivariate response?</a> it is shown that one can combine several response variables with <em>cbind</em> in the case of linear modeling with the <em>lm</em> function.
This doesn't seem to work for nonlinear modeling with <em>nls</em> .., because the <em>nls</em> call in the code sample at the bottom of my question throws the following error:</p>

<p><code>Error in parse(text = x) : &lt;text&gt;:2:0: unexpected end of input
1: ~ 
   ^</code></p>

<p>which I could not find a solution for online concerning my case of a multivariate regression..</p>

<hr>

<p>My web-searches to my main question only gave me results concerning <em>multivariate <strong>linear</strong> regression</em>, which for example included <a href=""http://stats.stackexchange.com/questions/11127/multivariate-multiple-regression-in-r/11132#11132"">solutions with the manova function</a>..</p>

<p>Therefore, <strong>my question asked in a more general way:</strong> How do you in general solve such a non-linear multivariate multiple regression problem in R which takes into account interactions/dependencies between variables?</p>

<p>Here is my code where </p>

<ul>
<li>function <em>f</em> computes the rotations of coordinates about three axes
in the order x-axis, y-axis, and then z-axis (unfortunately I cannot
include the pic of the equation I wrote in LaTeX here since I haven't
got 10 reputation points yet);</li>
<li><em>rot_data_all</em> is structured as the data-subset above, just with more rows;</li>
<li>alpha1, alpha2 and so on are the parameters which nonlinear
regression should approximate:</li>
</ul>

<p>The code:</p>

<pre><code>f &lt;-function(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s) { 
      a &lt;- alpha1 + s*(cos(theta)*cos(phi)*x - cos(theta)*sin(phi)*y + sin(theta)*z)
      b &lt;- alpha2 + s*((sin(gamma)*sin(theta)*cos(phi) + cos(gamma)*sin(phi))*x 
                          + (-sin(gamma)*sin(theta)*sin(phi) + cos(gamma)*cos(phi))*y
                          - sin(gamma)*cos(phi)*z)
      c &lt;- alpha3 + s*((cos(gamma)*sin(theta)*cos(phi) + sin(gamma)*sin(phi))*x 
                          + (cos(gamma)*sin(theta)*sin(phi) + sin(gamma)*cos(phi))*y
                          + cos(gamma)*cos(phi)*z)
      return(c(a,b,c))
    }

    rot.nls &lt;- nls(cbind(a, b, c) ~ f(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s), 
                   data = rot_data_all, 
                   start = c(alpha1 = 0, alpha2 = 0, alpha3 = 0, gamma = 0.1, theta = 0.1, phi = 0.1, s = 0.1), trace = TRUE)
</code></pre>

<hr>

<p>I hope to find a solution which is general enough to also solve other transformations which cannot be easily linearized like the set of equations for <strong>projective transformation</strong>, i.e. something like the following function:</p>

<pre><code>f.proj &lt;-function(x, y, z, betas) {
  a &lt;- (betas[1,1]*x + betas[1,2]*y + betas[1,3]*z + betas[1,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  b &lt;- (betas[2,1]*x + betas[2,2]*y + betas[2,3]*z + betas[2,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  c &lt;- (betas[3,1]*x + betas[3,2]*y + betas[3,3]*z + betas[3,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  return(c(a,b,c))
}
</code></pre>

<hr>

<p>I am happy to provide more information if needed! Thank you so much!</p>
"
"0.255136726136358","0.257750602609084","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.128492783587882","0.129809192515054","144096","<p>I have data from gene expression arrays and I have clinical data associated with the samples used. I am using gene expression (discrete), age at diagnosis (discrete) and ethnicity (categorical) to build a regression model. I'd like to predict gene expression by age at diagnosis and ethnicity so the model would be something like:</p>

<p>$$y = b_0 + b_1x_1 + b_2x_2 + \epsilon$$</p>

<p>in R it looks something like:</p>

<pre><code>y &lt;- as.numeric(gene_expression_myFavoriteGenes)
age &lt;- age_diagnosis 
ethnicity &lt;- ethnicity 
l &lt;- lm(y ~ age + ethnicity)
</code></pre>

<p>now...when I look at the coefficients I get something like:</p>

<pre><code>Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -0.442785   0.151175  -2.929 0.008299 ** 
age_diagnosis    -0.005616   0.002422  -2.319 0.031090 *  
Caucasian        -0.910633   0.115870  -7.859 1.53e-07 ***
Hispanic         -0.801088   0.125429  -6.387 3.13e-06 ***
Honduran         -0.682405   0.210694  -3.239 0.004114 ** 
Peurto Rican     -0.679251   0.209620  -3.240 0.004100 ** 
South Asian      -1.237134   0.213569  -5.793 1.14e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1815 on 20 degrees of freedom
Multiple R-squared:  0.7795,    Adjusted R-squared:  0.7023 
F-statistic:  10.1 on 7 and 20 DF,  p-value: 2.21e-05
</code></pre>

<p>I'm a lil confused on how to interpret the data.
I first built the model without correcting for ethnicity, nothing was significant. 
then I added ethnicity, checked with <code>anova(fist_lm, second_lm)</code> and the difference is significant.</p>

<p>so now I have all ethnicity associated with a significant p-value and and adjusted R^2 of 0.7 ...so, my questions, sorry if they sound silly, are:</p>

<ul>
<li>in this specific case, can age + ANY ethnicity significantly predict expression of myFavoriteGene?</li>
<li>models with other genes only have one or two significant ethnicity, does that mean that those are the only ones that can reliably predict expression of myFavoriteGene?</li>
</ul>

<p>------- edit ---------</p>

<p>here an example of how my data look:</p>

<pre><code>&gt; data[1:10,1:6]
           skin.AA_2  skin.AA_3  skin.AA_4  skin.AA_5  skin.AA_6   skin.AA_7
100_g_at   5.5526731  4.7001569  5.3724104  5.3700587  5.7571421  5.76974711
1000_at    7.7757596  5.4761710  7.3896019  5.5514442  8.2559761  7.14107706
1001_at    1.4554496  0.3315354  1.3311387  1.4979105  2.0579317  1.50734217
1002_f_at -0.4427732 -0.7381431 -0.3714425 -0.3159300 -0.2270056  0.31245288
1003_s_at  1.7908548  1.2590320  1.4839795  1.6727171  1.8550568  1.99870500
1004_at    1.8082815  0.9940647  1.4085169  1.8658939  1.9275267  2.25192977
1005_at    3.1792907 10.2456153  6.1170771  9.9058017  8.3695269  5.02225258
1006_at   -0.3059731 -0.8761517 -0.7151807 -0.4620902 -0.5923052  0.02495093
1007_s_at  9.9911387 10.2839949 10.1105075  9.9944011 10.3866696 10.31211765
1008_f_at  7.9190579  4.5957139  4.0043624  4.6297893  4.2067368  7.62499810

&gt; clinicalData[1:10,1:5]  
patient_ID        ethnicity age_diagnosis age colname_data_matrix_SB
1      AA1005 African American            39  47              skin.AA_1
2      AA1007        Caucasian            32  50              skin.AA_3
3      AA1008        Caucasian            50  50              skin.AA_4
4      AA1009        Caucasian            18  68              skin.AA_5
5      AA1010        Caucasian            31  40              skin.AA_6
6      AA1015        Caucasian            41  43                       
7      AA3001         Hispanic            49  49              skin.AA_7
8      AA3006         Hispanic            48  51              skin.AA_8
9      AA3007         Hispanic            51  51              skin.AA_9
10     AA3008         Hispanic            49  50             skin.AA_10
</code></pre>
"
"0.134764368352983","0.151272255204013","145657","<p>Basically I'm attempting to recreate the results of an example from class in R. What I'm trying to do is decide whether it's best to use a single regression line for an entire data set or two lines based on a categorical variable. The teacher indicates there are three steps to this:</p>

<ol>
<li>Determine if two different lines are required</li>
<li>If yes, determine if they differ in slope</li>
<li>If yes, determine if they differ in intercept</li>
</ol>

<p>Here is my data:</p>

<pre><code>&gt; example
   Predictor Response Group
1         21       11     A
2         24       21     A
3         26       23     A
4         29       29     A
5         35       34     A
6         45       51     A
7         51       59     A
8         68       73     A
9         72       83     A
10        76       95     A
11        17       11     B
12        21       55     B
13        26       34     B
14        28       44     B
15        32       26     B
16        36       34     B
17        40       15     B
18        45       21     B
19        51       16     B
20        68       21     B
</code></pre>

<p>I've realized that if I add the interaction and group terms to the model:</p>

<pre><code>ex_mod &lt;- lm(Response ~ Predictor,data = example)
ex_mod2 &lt;- lm(Response ~ Predictor + Group + Predictor:Group,data = example)
</code></pre>

<p>And then perform ANOVA on this. I get the right answer for step 1:</p>

<pre><code>&gt; anova(ex_mod,ex_mod2)
Analysis of Variance Table

Model 1: Response ~ Predictor
Model 2: Response ~ Predictor + Group + Predictor:Group
  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    
1     18 6616.4                                 
2     16 1583.8  2    5032.6 25.42 1.078e-05 ***   
</code></pre>

<p>Which means I need different lines, but now I need to know if they differ in slope or y-intercept or both. And here is where I'm stuck. I cant seem to get the right answer (F = 293.17 for slope, and F = 170.77 for intercept). </p>

<p>The teacher indicates that the next steps are: 1) to generate RSS in which the slope is fixed,but the y-intercepts are allowed to vary; and 2) generate RSS in which the y-intercept is fixed, but the slopes are allowed to vary.</p>

<p>I apologize if the question is confusing or simplistic, but I dont know how to proceed from here.</p>

<p>Thanks</p>
"
"NaN","NaN","145774","<p>In ANOVA table , I got</p>

<p>$F$-statistic $=330.285$ </p>

<p>degrees of freedom (df) due to regression $= 6$</p>

<p>Error degrees of freedom $= 9$</p>

<p>But i don't know how to calculate $p-$value ?</p>

<p>I have tried in <code>R</code> software as</p>

<blockquote>
  <p>pf ( 330.285, 6, 9, lower.tail = F )</p>
  
  <p>[1] 1</p>
</blockquote>

<p>But it doesn't match with the result which is actually .000</p>

<p>Where am i doing mistake ?</p>
"
"0.0947027447620757","0.095672974646988","145790","<p>I'm trying to figure out how to produce an ANOVA Table in R for a multiple regression model. So far I can only produce it for each regressor, and the Mean Square is calculating as the same as Sum Of Squares.</p>

<pre><code>&gt; anova(nflwin.lm)
Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
pass_yard     1  76.193  76.193  26.172 3.100e-05 ***
percent_rush  1 139.501 139.501  47.918 3.698e-07 ***
oppo_rush     1  41.400  41.400  14.221 0.0009378 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I'm trying to produce something like</p>

<pre><code>Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
Model         3  76.193  76.193  26.172 3.100e-05 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.115986700954059","0.117174985029676","147033","<p>I am trying to calculate standard errors of group means for a two-way-anova. I found two ways to do this (<code>predict.lm(, se = T)</code> and <code>summary.lm()</code>:</p>

<pre><code>set.seed(42234)
exmpl &lt;- data.frame(DV = rnorm(40) + rep(3:6 * 10, each = 10), #  Dependent Variable
                    IV1 = factor(rep(LETTERS[1:2], each = 20)), # Independent Variable (Treatment) 1 
                    IV2 = factor(rep(rep(LETTERS[3:4], each = 10), 2))) #  Independent Variable (Treatment) 2

exmpl.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl) #  Example data was generated without interactions

summary(exmpl.lm)
as.data.frame(predict(exmpl.lm, data.frame(IV1 = c('A', 'B', 'A', 'B'),
                                           IV2 = c('C', 'C', 'D', 'D')), se = T))
</code></pre>

<p>The standard errors of some group means differ. I managed to recalculate the standard errors given by <code>predict()</code> with the explanations from <a href=""http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf"" rel=""nofollow"">Practical Regression and Anova using R</a> from Faraway (section 3.5). I couldn't find any information about the algorithms used by the <code>summary()</code> function. Any ideas?
 What I find really confusing is that you can change the output by relabelling one factor:</p>

<pre><code>exmpl2 &lt;- exmpl
exmpl2$IV1 &lt;- factor(exmpl2$IV1, levels = LETTERS[2:1])

exmpl2.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl2)
summary(exmpl2.lm)
summary(exmpl.lm)
</code></pre>

<p>In the first example group A-C has a standard error of 0.2013. In the second example the standard error is given as 0.2324. The data of both examples is the same, only the order of the labels of a categorial (not ordinal) variable were changed. How does this influence the statistical model?</p>
"
"0.115986700954059","0.0976458208580634","148808","<p>I'm trying to evaluate the value of an object, depending on his characteristics. In order to do this, I'm building the following regression model <code>price ~ .</code>, using similar objects and for each variable I got min 20 observations</p>

<p>I encountered following problem: none of the regression models worked for all my data, so I decided to use all of the followings methods:</p>

<pre><code>model.lm &lt;- lm(price ~ .)
model.lmLog &lt;- lm(log(price) ~ .)
model.ltsReg &lt;- ltsReg(price ~ .)
model.ltsRegLog &lt;- lts(log(price) ~ .)
model.lmrob &lt;- lmrob(price ~ .)
model.lmrobLog &lt;- lmrob(log(price) ~ .)
model.lmRob &lt;- lmRob(price ~ .)
model.lmRobLog &lt;- lmRob(log(price) ~ .)
model.glm &lt;- glm(price ~ .)
model.glmLog &lt;- glm(price ~ ., family=gaussian(link=""log""))
</code></pre>

<p>My question is: how can I decide which of this models fits best for the current data, without plotting the results?</p>

<p>As far as I know, the <code>r-squared</code> aren't trusty, because I the data is corrupted, so will be the <code>r-squared</code>.</p>

<p>Any ideas?</p>

<p>Thank you!</p>

<p><strong>[UPDATE]</strong></p>

<p>what do you think about using <code>BIC</code> or <code>AIC</code> and choosing the one with the lowest value?</p>

<p>what do you think about choosing the variables for the regression upon the analysis of <code>anova</code>?</p>

<p>I have 17 variables from which 10 are dummy variables, is that a problem?</p>
"
"0.105880887471907","0.10696563746014","152358","<p>I'm looking at two functions for testing goodness of fit for a logit regression model (glm).</p>

<p>Firstly anova(), used as </p>

<pre><code>anova(null model, test_model, test=""Chisq"")
</code></pre>

<p>This is pretty straightforward, as it provides the residual deviance of the two models.</p>

<p>The second, pchisq() test is where I'm getting confused, particularly around the 'TAIL' option of the function.</p>

<p>I'm using it like this..</p>

<pre><code>pchisq(test_model$deviance, df=test_model$df.residual, lower=FALSE)
</code></pre>

<p>It is my understanding that when using lower=FALSE (same as lower.tail=FALSE), our null hypothesis becomes <strong>'the model being tested is different from our null model'</strong>. </p>

<p>Therefore a low p-val from our pchisq() test (say 0.002) would provide support to reject the null hypothesis (p=0.01), ultimately stating that our test model is not significantly different than our null model.</p>

<p>Have I correctly interpreted the results of pchisq() when used with lower.tail=FALSE?</p>

<p>Edit - For refrence, including the 'help' portion of the arguments section of pchisq() function.</p>

<pre><code>lower.tail
logical; if TRUE (default), probabilities are P[X â‰¤ x], otherwise, P[X &gt; x].
</code></pre>
"
"0.0947027447620757","0.071754730985241","152474","<p>I have a continuous outcome variable and several different <code>lasso</code> models to predict the outcome. Something like</p>

<pre><code>outcome ~ explain1 + explain2 + confounders
outcome ~ explain3 + explain4 + confounders
outcome ~ explain1 + explain2 + explain3 + explain4 + confounders
</code></pre>

<p>I know that I can calculate several goodness of fit measures like <a href=""http://en.wikipedia.org/wiki/PRESS_statistic"" rel=""nofollow"" title=""PRESS"">PRESS</a> or just the <a href=""http://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow"" title=""mean square error"">mean square error</a>. And models with lower PRESS/MSE predict the outcome more accurately. But how can I quantify this difference, ideally with a p-value, which indicates if one model is significantly better than the other?</p>

<p>For ""normal"" regression models I could use anova, but this doesn't seem to work for lasso models (at least in <code>R</code>).</p>
"
"0.142054117143114","0.143509461970482","153122","<p>As we know, if we are doing many tests or multiple comparison, we don't use the same $\alpha$ value and use some $\alpha$ correction methods like Bonferroni. This is done because when we do multiple tests, we have higher chance of getting something as significant compared to doing for fewer numbers of tests. </p>

<p>But my main question is this: </p>

<p>1) It is said if you are comparing multiple sample means using ANOVA and once you find there is some significant difference then you can do a <em>post hoc</em> analysis by doing pairwise comparison. But now you don't have to actually do a Bonferroni correction. Why is that? Isn't this <em>post hoc</em> analysis same as other pairwise t test where we use Bonferroni correction?</p>

<p>2) If Bonferroni correction is required because more tests leads to more chances of getting something significant then why we don't use the same thing, where we are doing something like regression where we are testing significance of $\beta$ estimates, or whether a variable is significant or not for feature selection using p value/F score? In that case also we are doing multiple comparison in checking whether each variable is significant or not. Then why don't we use Bonferroni correction on critical $\alpha$ there?</p>

<p>Please advise.</p>
"
"0.0710270585715568","0.071754730985241","153527","<p>I am looking at the <code>wine</code> dataset from the R <code>FactoMineR</code>package to find out whether the categorical variable soil type affects the feature set. Basically there are four distinct soil types and 29 different features and there are 20 observations</p>

<pre>
obs&nbsp;soil&nbsp;f1&nbsp;f2&nbsp;...&nbsp;f29
  1&nbsp;   1&nbsp;11&nbsp;87&nbsp;...&nbsp;20
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
  7&nbsp;   2&nbsp;13&nbsp;10&nbsp;...&nbsp;10
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 15&nbsp;   3&nbsp;77&nbsp;53&nbsp;...&nbsp;54
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 20&nbsp;   4&nbsp;88&nbsp;81&nbsp;...&nbsp;21
</pre> 

<p>Given that I cannot use ANOVA or logistic regressions what other ways are there to figure out if the soil type affects the features? I am considering using box plots for visual inspection, but 29 box plots seems like a bit much. Are there any succinct plot types that could give me this kind of information?</p>
"
"0.142054117143114","0.143509461970482","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.133929906036485","0.118389266011054","153719","<p>Consider the following dataset:</p>

<pre><code>#   color  type region_west region_cent region_east region_west_pct region_cent_pct region_east_pct
# 1   red shirt          24          17          48          0.2697          0.1910          0.5393
# 2  blue shirt          24          18          44          0.2791          0.2093          0.5116
# 3   red  pant          42          13          33          0.4773          0.1477          0.3750
# 4  blue  pant          46          17          41          0.4423          0.1635          0.3942
# 5   red   hat          46          38           8          0.5000          0.4130          0.0870
# 6  blue   hat          40          11          21          0.5556          0.1528          0.2917
</code></pre>

<p><code>color</code> and <code>type</code> should be self explanatory - we can say the region column represent ""sales"" and percent of sales by row.</p>

<p>What are some approaches for answering questions such as:</p>

<ol>
<li>Is <code>color</code> and/or <code>type</code> statistically different by region? Which regions? (e.g. post-hoc testing)  </li>
<li>How many (or what percent) <code>color = red</code> items should I put in the West?</li>
<li>How many (or what percent) <code>type = pant</code> items should I place in the East?</li>
<li>How would you express a confidence interval around the number of <code>color = red</code> items in the West? What about a confidence interval for the percentage? </li>
<li>How are you correcting for making multiple comparisons? (e.g. Bonferonni)</li>
</ol>

<p><hr>
<strong>Additional Assumptions:</strong> Assume these values represent a true population total sales. That is, all possible sales from the West, Central, and East region -- effectively demand. Additionally, we can assume the sales were made online and the customer resides in one of the three regions. This is essentially a warehouse distribution problem - say I have three warehouses, West, Central, and East - how much of each product should I place in each warehouse if these distributions are the assumed demand quantities.
<hr>
My initial thoughts are <code>chi-square</code>, <code>ANOVA</code>, and/or <code>regression/glm/gam</code> but I thought this is a ""neat"" little example hitting on a lot of fundamentals represented on this board, so I'm hoping to get some variety in the responses. </p>

<p>Here's the original dataset:</p>

<pre><code>df &lt;- structure(list(color = structure(c(1L, 2L, 1L, 2L, 1L, 2L), .Label = c(""red"", ""blue""), class = ""factor""), type = structure(c(1L, 1L, 2L, 2L, 3L, 3L), .Label = c(""shirt"", ""pant"", ""hat""), class = ""factor""),     region_west = c(24L, 24L, 42L, 46L, 46L, 40L), region_cent = c(17L,     18L, 13L, 17L, 38L, 11L), region_east = c(48L, 44L, 33L,     41L, 8L, 21L), region_west_pct = c(0.2697, 0.2791, 0.4773,     0.4423, 0.5, 0.5556), region_cent_pct = c(0.191, 0.2093,     0.1477, 0.1635, 0.413, 0.1528), region_east_pct = c(0.5393,     0.5116, 0.375, 0.3942, 0.087, 0.2917)), .Names = c(""color"", ""type"", ""region_west"", ""region_cent"", ""region_east"", ""region_west_pct"", ""region_cent_pct"", ""region_east_pct""), out.attrs = structure(list(    dim = 2:3, dimnames = structure(list(Var1 = c(""Var1=red"",     ""Var1=blue""), Var2 = c(""Var2=shirt"", ""Var2=pant"", ""Var2=hat""    )), .Names = c(""Var1"", ""Var2""))), .Names = c(""dim"", ""dimnames"")), row.names = c(NA, -6L), class = ""data.frame"")
</code></pre>

<p>Here's the dataset in a  ""tidy"" format:</p>

<pre><code>df.tidy &lt;- structure(list(color = structure(c(1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L), .Label = c(""red"", ""blue""), class = ""factor""), type = structure(c(1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L, 1L, 1L, 2L, 2L, 3L, 3L), .Label = c(""shirt"", ""pant"", ""hat""), class = ""factor""),     region = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L,     2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L,     2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L,     1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L    ), .Label = c(""region_west"", ""region_cent"", ""region_east""    ), class = ""factor""), sales = c(24L, 24L, 42L, 46L, 46L,     40L, 17L, 18L, 13L, 17L, 38L, 11L, 48L, 44L, 33L, 41L, 8L,     21L, 24L, 24L, 42L, 46L, 46L, 40L, 17L, 18L, 13L, 17L, 38L,     11L, 48L, 44L, 33L, 41L, 8L, 21L, 24L, 24L, 42L, 46L, 46L,     40L, 17L, 18L, 13L, 17L, 38L, 11L, 48L, 44L, 33L, 41L, 8L,     21L), pct = c(0.2697, 0.2791, 0.4773, 0.4423, 0.5, 0.5556,     0.2697, 0.2791, 0.4773, 0.4423, 0.5, 0.5556, 0.2697, 0.2791,     0.4773, 0.4423, 0.5, 0.5556, 0.191, 0.2093, 0.1477, 0.1635,     0.413, 0.1528, 0.191, 0.2093, 0.1477, 0.1635, 0.413, 0.1528,     0.191, 0.2093, 0.1477, 0.1635, 0.413, 0.1528, 0.5393, 0.5116,     0.375, 0.3942, 0.087, 0.2917, 0.5393, 0.5116, 0.375, 0.3942,     0.087, 0.2917, 0.5393, 0.5116, 0.375, 0.3942, 0.087, 0.2917    )), row.names = c(NA, -54L), class = ""data.frame"", .Names = c(""color"", ""type"", ""region"", ""sales"", ""pct""))
</code></pre>

<p>Feel free to expand the dataset to a larger example.</p>
"
"0.164029965544142","0.165710452999832","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.200894859054728","0.180402691064464","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.0947027447620757","0.095672974646988","158578","<p>Do centered variables have to stay in matrix form when using them in a regression equation?</p>

<p>I have centered a few variables using the <code>scale</code> function with <code>center=T</code> and <code>scale=F</code>. I then converted those variables to a numeric variable, so that I can manipulate the data frame for other purposes. However, when I run an ANOVA, I get slightly different F values, just for that variable, all else is the same. </p>

<p>Edit:</p>

<p>What's the difference between these two:</p>

<pre><code>scale(A, center=TRUE, scale=FALSE)  
</code></pre>

<p>Which will embed a matrix within your data.frame</p>

<p>AND</p>

<pre><code>scale(df$A, center=TRUE, scale=FALSE)
    df$A = as.numeric(df$A)
</code></pre>

<p>Which makes variable A numeric, and removes the matrix notation within the variable? </p>

<p>Example of what I am trying to do, but the example doesn't cause the problem I am having:</p>

<pre><code>library(car)
library(MASS)
mtcars$wt_c &lt;- scale(mtcars$wt, center=TRUE, scale=FALSE)
mtcars$gear &lt;- as.factor(mtcars$gear)
mtcars1     &lt;- as.data.frame(mtcars)

# Part 1
rlm.mpg   &lt;- rlm(mpg~wt_c+gear+wt_c*gear, data=mtcars1)
anova.mpg &lt;- Anova(rlm.mpg, type=""III"")

# Part 2
# Make wt_c Numeric
mtcars1$wt_c &lt;- as.numeric(mtcars1$wt_c)
rlm.mpg2     &lt;- rlm(mpg~wt_c+gear+wt_c*gear, mtcars1)
anova.mpg2   &lt;- Anova(rlm.mpg2, type=""III"")
</code></pre>
"
"0.0947027447620757","0.095672974646988","158713","<p>I have a question regarding multiple regression with an unbalanced grouping factor. Essentially what I am doing is an ANCOVA, but the interaction term ends up significant (which is interesting!) so I've chosen not to call it a true ANCOVA.</p>

<h1>The Data</h1>

<p>The dataset is comprised of 72 individuals who responded to many different measures for the purposes of conducting a cluster analysis to uncover relatively heterogenous subgroups within the dataset. Three clusters resulted form this analysis, where the resulting cluster sizes were n=30, n=32, and n=10. These clusters were interpreted for the purpose of a descriptive analysis. </p>

<p>An independent dataset describes these same 72 individuals on two separate continuous measures: <strong>score</strong>, and <strong>dv</strong>. The hope for my current project is to asses the effect of <strong>group</strong> (cluster membership, unbalanced) and <strong>score</strong> (and the interaction) on the <strong>dv</strong>. </p>

<h1>The Data (Example)</h1>

<pre><code>g1    &lt;-rep(1,30)
g2    &lt;-rep(2,32) 
g3    &lt;-rep(3,10)
group &lt;-as.factor(c(g1,g2,g3))
score &lt;-as.numeric(sample(1:10,72,replace=T))
dv    &lt;-as.numeric(sample(1:7,72,replace=T))
data  &lt;-data.frame(cbind(group, score, dv))
head(data)

head(data)
      group     score       dv
1     1          9          5
2     1          3          6
3     1         10          6
4     1         10          6
5     1         10          6
6     1          4          5
</code></pre>

<h1>My Question</h1>

<p>1) Can I run an analysis despite my groups being so unbalanced? If I understand correctly, by using type III SS, all groups will be weighted equally but I'm not sure if this solves my issue so simply.</p>

<p>For example:</p>

<pre><code>lm&lt;-lm(dv~1+score*group,data=data)
library(car)
Anova(lm,type=""III)
</code></pre>

<p>2) If not, am I unable to proceed in some other way? </p>

<p>I am looking for any suggestions / guidance as I try to sort this out.</p>

<p>Thanks!</p>
"
"0.246044948316214","0.23935954322198","159745","<p>I want to test a regression model with neuroticism as focal predictor, agreeableness as moderator and RT variability as dependent measure (covariates: attentional control and mean RT). Previously, I have used the modprobe macro in SPSS by Andrew Hayes for this (see: <a href=""http://link.springer.com/article/10.3758%2FBRM.41.3.924"" rel=""nofollow"">http://link.springer.com/article/10.3758%2FBRM.41.3.924</a>). I am in the process of transitioning to R, however, and would like to learn how to run a similar routine there. I have set up my regression model as follows:</p>

<pre><code>m3&lt;-lm(data=stp2_sub2, all_SD~Neuroticism*Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # full interaction model
m33&lt;-lm(data=stp2_sub2, all_SD~Neuroticism+Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # reduced model
</code></pre>

<p>I know that I can obtain F-change and p-change, using:</p>

<pre><code>anova(m3, m33) # provides F-change and p-change
</code></pre>

<p>What I still donâ€™t know yet is how to obtain the R squared change value, which gives me the effect size of the interaction effect. I have already posted a similar question at one of the sister websites (<a href=""http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#"">http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#</a>), although it seems that my question was slightly off-topic there. The users there have been very helpful (especially @gung), but I still have some remaining questions. Hence me posting here.</p>

<p>Basically, the recommendations have been so far to compute (a) semi-partial r-squared or (b) partial eta squared. For (a) semi-partial r-squared a custom-written function already exists (see: <a href=""http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615"">http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615</a>). Unfortunately I am a bit at a loss as to how to exactly adapt it to my own case, particularly the following bit: y = 4 + .5*x1 - .3*x2 + rnorm(10, mean=0, sd=1). Moreover, even if I did succeed at adapting y to my own needs, I would still need to know how to compute the r change value. The function for semi-partial r-squared yields a single r value (i.e. it doesnâ€™t provide a change value, yet). In my case, would I need to, in a first step, run this function for (1) the full model (here: m3) as well as for (2) the reduced model (here: m33), thereby providing me with two semi-partial r-squared values (full vs. reduced)? In a second step, would I then subtract semi-partial r-squared (reduced) from semi-partial r-squared (full), with the outcome being the r-change value that I need?</p>

<p>As for b, I have been told to use the following Â« formula Â» ( SSE(reduced)-SSE(full) ) / SSE(reduced) to compute partial eta squared for the interaction effect. I have found code for computing the sum of squared errors (see: <a href=""http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq"">http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq</a>) and have applied this to my case as follows:</p>

<pre><code>SSEfull&lt;-sum(m3$residuals^2) # sum of squared errors/residual sum of squares for the full regression model
SSEred&lt;-sum(m33$residuals^2) # sum of squared errors/residual sum of squares for the reduced regression model

pes&lt;-(SSEred-SSEfull)/SSEred # computing partial eta-squared
</code></pre>

<p>I would like to know whether I have correctly implemented the code to compute partial eta squared for the interaction effect. Moreover, given that I need the r squared change value, I was wondering how I might be able to convert the partial eta squared value to the r squared change value that I need. That said, as mentioned above, I just need to know how to compute the r squared change value for my interaction model. Ultimately, I donâ€™t mind whether I do this following suggestions (a) or (b) (but note that I still have some questions in this regard) or use a completely different way. Indeed, perhaps thereâ€™s already a function/package in R that calculates the r change value (I havenâ€™t found one yet)? I feel like being very close to finding what I need, but just need a final few pointers. Apologies for the long-winded way of writing this simple question, but I thought it could be of use to show what thought (even if not yet conclusive) has already gone into this. Any guidance on how to best go about this would be much appreciated.</p>
"
"0.105880887471907","0.10696563746014","163410","<p>How can I determine whether one coding of a linear predictor leads to a better fit of the corresponding regression model than the other?
In the following example, the restricted cubic spline coding of albumin leads to a higher chi-square value of the resulting model compared to the linear coding. However, it has also more degrees of freedom. As I understand it, I cannot use the log likelihood test in this case, since both models are not nested.</p>

<p>What should I do?</p>

<pre><code>&gt; library(rms)
&gt; 
&gt; data(pbc)
&gt; d &lt;- pbc
&gt; rm(pbc, pbcseq)
&gt; d$status &lt;- ifelse(d$status != 0, 1, 0)
&gt; 
&gt; dd = datadist(d)
&gt; options(datadist='dd')
&gt; 
&gt; # linear model
&gt; m1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
&gt; anova(m1)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 albumin    73.51      1    &lt;.0001
 TOTAL      73.51      1    &lt;.0001
&gt; 
&gt; # rcs model
&gt; m2 &lt;- cph(Surv(time, status) ~  rcs(albumin, 4), data=d)
&gt; anova(m2)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 albumin    82.80      3    &lt;.0001
  Nonlinear  4.73      2    0.094 
 TOTAL      82.80      3    &lt;.0001
</code></pre>

<h2>UPDATE #1</h2>

<p>I thought plotting both models would be a good way to decide whether a linear coding or a restricted cubic spline coding would be best. In this case (see below), I would think that the more complex coding is not better. However, the core of my question aimed to reinforce the eyeballing by a statistical test. But as I understand you correct, this is prone to over-fitting?</p>

<p><a href=""http://i.stack.imgur.com/UD5wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UD5wK.png"" alt=""enter image description here""></a></p>
"
"0.115986700954059","0.0976458208580634","163922","<p>I am trying to find any evidence of warming in monthly times series data of water temperature over a 21-year period that is serially correlated. Essentially I am looking to determine a global trend, like what can be done with OLS regression with data that is from independent observations. I am at a crossroads in trying to determine whether a seasonal ARIMA model or a linear mixed model with a trend component as detailed by Crawley on page 799 of ""The R Book"" (2nd ed.) is the most appropriate method to use. I therefore explored both techniques, but got very contradicting answers!</p>

<p>ARIMA modelling gave me a seasonal ARIMA of form (2,0,2)(0,0,1)[12], indicating that no differencing is required and therefore that the series is stationary with NO trend.</p>

<p>However, the linear mixed affects modelling, comparing two models with and without a trend component using ANOVA and maximum likelihood indicated a highly significant trend (R notation):</p>

<pre><code>model2: ave ~ sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

model1: ave ~ index + sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

ANOVA(model2,model1)

      Df  AIC     BIC     logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
model2 5 346.82   364.49    -168.41   336.82                           
model1 6 338.54   359.74    -163.27   326.54 10.28      1   0.001345 **
</code></pre>

<p>How can this be? What am I missing? Is it about assuming whether the trend is a parametric form (appropriate for linear mixed model) or whatever weird shape (appropriate for ARIMA)? If so how do I go about choosing which approach to adopt?</p>

<p>Thank you kindly for any advice.</p>
"
"0.177172612243394","0.178987746151269","164228","<p>GLM (family=binomial) is foucusd on when the response is dichotomous(yes/no, male/female, etc..). I'm wondering how to judge if the model we built is good eough? As we know, in OLS regression some criterion like R^2 and adjusted R^2 can tell us how much variations are explained but not for GLM. See example I performed:</p>

<pre><code>    &gt; summary(fit.full)
    Call:
    glm(formula = ynaffair ~ gender + age + yearsmarried + children + 
    +religiousness + education + occupation + rating, family = binomial(), 
    data = Affairs)

    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
    -1.6575  -0.7459  -0.5714  -0.2552   2.5099  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    0.71792    0.96165   0.747 0.455336    
    gendermale     0.28665    0.23973   1.196 0.231811    
    age           -0.04494    0.01831  -2.454 0.014142 *  
    yearsmarried   0.09686    0.03236   2.993 0.002758 ** 
    childrenyes    0.37088    0.29466   1.259 0.208147    
    religiousness -0.32230    0.09003  -3.580 0.000344 ***
    education      0.01795    0.05088   0.353 0.724329    
    occupation     0.03210    0.07194   0.446 0.655444    
    rating2       -0.02312    0.58177  -0.040 0.968303    
    rating3       -0.84532    0.57619  -1.467 0.142354    
    rating4       -1.13916    0.55740  -2.044 0.040981 *  
    rating5       -1.61050    0.56649  -2.843 0.004470 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 608.22  on 589  degrees of freedom
    AIC: 632.22
</code></pre>

<p>After removed the insignificant variables, the reduced model look like below,although the AIC decreasd, we still do not know if this is the model with the lowest AIC we can achieved:</p>

<pre><code>    &gt; summary(fit.reduced)
    Call:
    glm(formula = ynaffair ~ age + yearsmarried + religiousness + 
        +rating, family = binomial(), data = Affairs)

    Deviance Residuals: 
    Min        1Q      Median      3Q      Max  
   -1.5117  -0.7541  -0.5722  -0.2592   2.4123  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    1.10220    0.71849   1.534 0.125014    
    age           -0.03588    0.01740  -2.062 0.039224 *  
    yearsmarried   0.10113    0.02933   3.448 0.000565 ***
    religiousness -0.32571    0.08971  -3.631 0.000282 ***
    rating2        0.11848    0.57258   0.207 0.836068    
    rating3       -0.70168    0.56671  -1.238 0.215658    
    rating4       -0.96190    0.54230  -1.774 0.076109 .  
    rating5       -1.49502    0.55550  -2.691 0.007118 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 613.63  on 593  degrees of freedom
    AIC: 629.63
</code></pre>

<p>And we perform the ANOVA, suggesting that the reduced model with
four predictors fits as well as the full model:</p>

<pre><code>    &gt; anova(fit.reduced, fit.full, test=""Chisq"")
    Analysis of Deviance Table

    Model 1: ynaffair ~ age + yearsmarried + religiousness + +rating
    Model 2: ynaffair ~ gender + age + yearsmarried + children + 
             +religiousness + education + occupation + rating
    Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
     1       593     613.63                     
     2       589     608.22  4   5.4124   0.2475
</code></pre>
"
"0.0669649530182425","0.0676510091491738","164314","<p>I have a data set with performance and training data that looks something like (this is not the exact data, but gives a general idea):</p>

<pre><code>&gt;dat
Performance Training
1           1
0           1
1           2
0           2
1           3
1           3
</code></pre>

<p>I want to find if there is are any significant differences between performance means for the respective levels of performance in R.  I have tried linear regression and anova, such as: <code>summary(lm(performance~training))</code> or <code>summary(aov(performance~training))</code> both of which yield non-significant results.  However, when I do a T-test to compare some of the means manually it is telling be significant differences exist.  Any thoughts on how to code what I am looking for or what might be going on here?</p>
"
"0.14973818705887","0.136145029683612","164705","<p>I'm working on analyzing a time series of physical variables in many lakes in Florida for an associate, and I've run into an issue. I'm attempting to run a regression for each time series of physical variables in each lake. I can get regression results in R easily, but they don't match up with my coworker's JMP results. Anyway, here's a sample from the data:</p>

<pre><code>Year = seq(1987,2015)
TP = c(14, 12, 14, 14, 17, 16, 15, 12, 18, 14, 15, 18, 18, 21, 21, 17, 17, 20, 19, 17, 18, 18, 26, 20, 18, 21, 21, 20, 18)
summary(lm(TP~Year))
</code></pre>

<p>gives </p>

<pre><code>Call:
lm(formula = TP ~ Year)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7310 -1.3724 -0.4305  0.9685  6.3675 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -502.90542   98.13981  -5.124 2.18e-05 ***
Year           0.26010    0.04904   5.303 1.35e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.21 on 27 degrees of freedom
Multiple R-squared:  0.5102,    Adjusted R-squared:  0.4921 
F-statistic: 28.12 on 1 and 27 DF,  p-value: 1.35e-05
</code></pre>

<p>His JMP analysis spits out the following:</p>

<pre><code>Parameter Estimates
Term        Estimate    Std Error   t Ratio Prob&gt;|t|
Intercept   -500.4634   96.74332    -5.17   &lt;.0001*
Year        0.2588707   0.048347    5.35    &lt;.0001*
</code></pre>

<p>For all lakes and all parameters of interest, the SS, slope estimates, etc. are all slightly off. I have looked into different types of Sum of Squares for ANOVA, but changing to different types (e.g. Type III using Anova()) still doesn't get the results to match up. What am I missing? Any assistance would be appreciated.</p>

<p>Edit: Thanks for y'all's help. Sorry for the belated response, I had to meet up with my colleague. To address the questions:</p>

<ul>
<li>I have hardcoded the data in my question, but it's merely a subset of a much larger dataset from Excel. We are using the same data and the remainder of my code is working properly. <a href=""https://www.dropbox.com/s/k21v38sfdbm0ola/LWFormatted.csv?dl=0"" rel=""nofollow"">Here's what the actual data look like.</a></li>
<li>I know OLS isn't great, but it's being used for some really basic trend descriptions for informing stakeholders. I may pursue a better option in the future.</li>
<li>The JMP model is setup using Y by X with the Bivariate option, then applying a linear regression. Below is a screenshot.</li>
</ul>

<p>Thanks again for your help!</p>

<p><a href=""http://i.stack.imgur.com/ndtBr.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ndtBr.jpg"" alt=""JMP Input""></a></p>
"
"0.125279955557839","0.126563449052859","165110","<p>I'm struggling with the interpretation of a regression model where a categorial variable (5 levels) is dummy coded. Here is the result of my calculation in R:</p>

<pre><code>Call:
lm(formula = DV ~ Age + Gender + factor(Categorial) + 
Continuous 1 + Continuous 2 + Continuous 3, 
data = dat)

Residuals:
 Min       1Q   Median       3Q      Max 
-1.30058 -0.25326  0.00349  0.28123  1.49877 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)           -0.42367    0.30694  -1.380  0.16842   
Age                   -0.05949    0.02026  -2.936  0.00356 **
Gender                -0.01800    0.04828  -0.373  0.70952   
factor(Categorial)2   -0.30625    0.12645  -2.422  0.01596 * 
factor(Categorial)3   -0.03441    0.07752  -0.444  0.65736   
factor(Categorial)4   -0.12603    0.09914  -1.271  0.20453   
factor(Categorial)5   -0.08417    0.13269  -0.634  0.52630    
Continuous 1           0.12080    0.04346   2.779  0.00575 **
Continuous 2          -0.06592    0.04383  -1.504  0.13354   
Continuous 3          -0.06230    0.03475  -1.793  0.07392 . 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4259 on 336 degrees of freedom
  (6 observations deleted due to missingness)
Multiple R-squared:  0.1315,    Adjusted R-squared:  0.1057 
F-statistic: 5.089 on 10 and 336 DF,  p-value: 6.353e-07
</code></pre>

<p>Ok. Age, Factor 2 of the categorial variable and the first continuous variable are significant predictors of the dependent variable. so far so good. </p>

<p>What I'm not understanding is:</p>

<ol>
<li><p>The reference category of the dummy coded categorial variable is the intercept and the first category of the categorial variable. right? How do I interpret this? </p></li>
<li><p>When doing an anova with the categorial variable as a independent variable, this factor is a significant predictor. With the results of the linear model, one could conclude that this is only due to category 2, right?</p></li>
<li><p>Can I test contrasts with this linear regression model (e.g. Category1 vs. Category2)?</p></li>
<li><p>Should I include interactions?</p></li>
</ol>

<p>I'd be glad for any help :-)</p>
"
"0.125279955557839","0.126563449052859","167159","<p>I'm currently analyzing a dataset about quality of fodder. The data for e.g. crude protein content are given in %, I don't have absolute values. Because of the data structure (I don't give you the details here), I opted for the beta-regression from R's betareg package.
I want to know the influence of various factors (and their interactions) on the crude protein content. Some factors have two levels (e.g. fertilization: with/without), others have three or more (e.g. site: site1, site2, site3 or the harvest time of the year where my samples come from (cut: spring, summer, autumn).
I have specified a model that works quite well on the data I have, residuals look well-behaved, predictors and model parameters are good, etc.
Now I would love to have some more information than the summary(mymodel) gives. I would like to test not only if other factor levels are different from my reference level but if they are significantly different from each other. In my example that would be: On which of my sites is the crude protein level in the fodder larger than on others and are those differences all significant? 
I would think about doing a post-hoc test similar to e.g. the Tukey I could do if I had done a simple ANOVA. I googled, searched a lot online and asked people, but I can't find anything that works with betareg-objects.</p>

<p>So my question is: How do I do a post-hoc test for parameters of betareg-models in R? Or is this a bad idea? If so, why? Or is there no method yet? Please help me, thank you a lot!</p>

<p>I'm not a statistics/math pro, I learned almost everything I know by the modelling books by Alain Zuur and this site, so please be gentle.</p>

<p>My model (in the simplified version of my dataset) is specified like this:</p>

<pre><code>library(betareg)
prot&lt;-protein/100
mymodel&lt;-betareg(prot~ sward * Fert + site + cut + repetition | site +cut, data=mydata, link=""loglog"")

str(mydata)
 'data.frame':   848 obs. of  58 variables:
  $ protein     num  16.4 16.5 13.7 13.5 15 ...
  $ cut          : Factor w/ 3 levels ""autumn"",""spring"",..: 1 1 2 2 3 3 1 2 2 3 ...
  $ loc          : Factor w/ 3 levels ""G"",""O"",""S"": 1 1 1 1 1 1 1 1 1 1 ...
  $ repetition   : Factor w/ 4 levels ""I"",""II"",""III"",..: 1 1 1 1 1 1 1 1 1 1 ...
  $ Fert         : Factor w/ 2 levels ""fertilized"",""non-fertilized"": 1 1 1 1 1 1 1 1 1 1 ...
  $ sward        : Factor w/ 2 levels ""diverse"",""species-poor"": 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>

<p>a reproducible but very simplified example code would be (note that my actual model is much larger!):</p>

<pre><code>site&lt;-c(""site1"",""site2"",""site3"",""site1"",""site2"",""site3"",""site1"",""site2"",""site3"",""site2"")
prot&lt;-c(0.1642038, 0.1650442, 0.1369376, 0.1350139, 0.1502178, 0.1515794, 0.1354457, 0.1301206, 0.1311298, 0.1308463)
fert&lt;-c(""with"",""without"",""with"",""without"",""with"",""without"",""with"",""without"",""with"",""without"")
cut&lt;-c(""autumn"",""autumn"",""spring"", ""spring"", ""summer"", ""summer"", ""autumn"", ""spring"", ""spring"", ""summer"")

mymodel&lt;-betareg(prot~ fert + site + cut | site, link=""loglog"")
</code></pre>
"
"0.171165004875037","0.172918590828624","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.126270326349434","0.127563966195984","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.107382819049576","0.108482956331022","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.105880887471907","0.10696563746014","173207","<p>I have a dataset gpa2 ddata that can be found here <a href=""https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1"" rel=""nofollow"">https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1</a></p>

<p>I estimate the model colgpa = athelte.</p>

<pre><code>gpa2 &lt;- read.csv(~""/path/ddata.csv"")
model1 &lt;- lm(formula = colgpa ~ athlete, data = gpa2)
summary(model1)
</code></pre>

<p>And now I want to see if I can get <strong>Std.Error by this formula</strong>
$$se(\beta_j) = \frac{\hat \sigma_u}{SST_j(1âˆ’R_j^2)}$$
where $$SST_j = \sum_{i=1}^n (x_{ij} - \bar x_j)^2$$ is the total sample variation in $x_j$
and $R_j^2$ is the $R^2$ from regressing $x_j$ on all the other independent variables.</p>

<p>From this answer 
<a href=""http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression"">How are the standard errors of coefficients calculated in a regression?</a>
we know that the standard error of the estimated slope, $se(\beta_1)$ in our case, is
$$\sqrt{\widehat{\textrm{Var}}(\hat{b})} = \sqrt{[\hat{\sigma}^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}]_{22}} = \sqrt{\frac{n \hat{\sigma}^2}{n\sum x_i^2 - (\sum x_i)^2}}.$$</p>

<p>I do this with an anova-table, and try to </p>

<pre><code>anova(model1) # the anova table
# now I take out elements of the anova
model1_hatsigmau &lt;- anova(model1)[[3]][2] #takes row 3 column 2 in the anova-table.
model1_MSathlete &lt;- anova(model1)[[3]][1]
model1_SSathlete &lt;- anova(model1)[[2]][1]
numerator &lt;- n*model1_hatsigmau
meanathlete &lt;- mean(gpa2$athlete)
denominator &lt;- n*model1_SSathlete - (n*meanathlete)^2 
sqrt(numerator /  denominator) # should be se(beta_1) for model1. 
</code></pre>

<p>But I get </p>

<pre><code>&gt; sqrt(numerator /  denominator) # should be se(beta_1) for model1.
[1] 0.270643
</code></pre>

<p>And not $0.04824$ as in the summary-output (see below). So the problem is that $$0.270643 \neq 0.04824$$</p>

<pre><code>&gt; summary(model1)

Call:
lm(formula = colgpa ~ athlete, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.66603 -0.43603  0.00397  0.46397  1.61851 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.66603    0.01045 255.212  &lt; 2e-16 ***
athlete     -0.28453    0.04824  -5.898 3.97e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.656 on 4135 degrees of freedom
Multiple R-squared:  0.008343,  Adjusted R-squared:  0.008104 
F-statistic: 34.79 on 1 and 4135 DF,  p-value: 3.966e-09
</code></pre>
"
"0.133929906036485","0.135302018298348","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.0669649530182425","0.0676510091491738","175111","<p>I've understood that relative importance of predictors is a tricky question. Suggested methods range from very complex models to very simple variable transformations. I've understood that the brightest still debate which way to go on this matter. I'm looking for an easy but still appealing method to approach this in survival analysis (Cox regression).</p>

<p>My aim is to answer the question: which predictor is the most important one (in terms of predicting the outcome). The reason is simple: clinicians want to know which risk factor to adress first. I understand that ""important"" in clinical setting is not equal to ""important"" in the regression-world, but there is a link.</p>

<p>Should I compute the proportion of explainable log-likelihood that is explained by each variable (see Frank Harrell <a href=""http://stats.stackexchange.com/questions/155246/which-variable-relative-importance-method-to-use"">post</a>), by using:</p>

<pre><code>library(survival); library(rms)
data(lung)
S &lt;- Surv(lung$time, lung$status)
f &lt;- cph(S ~ rcs(age,4) + sex, x=TRUE, y=TRUE, data=lung)
plot(anova(f), what='proportion chisq')
</code></pre>

<p>As I understand it, its only possible to use the 'proportion chisq' for Cox models and this should suffice to convey some sense of each variables relative importance. Or should I perhaps use the default plot(anova()), which displays Wald Ï‡2 statistic minus its degrees of freedom for assessing the partial effect of each variable?</p>

<p>I would appreciate some guidance if anyone has any experience on this matter.</p>
"
"NaN","NaN","175418","<p>I ran the mixed effects model using the lme4 package as follows:
lmer9&lt;- lmer(y ~ station + (1|station:tow) + (1|tow), data=my.data)
 where station is the fixed factor and tow a random one...</p>

<p>However I am interested in obtaining the ANOVA for the regression model. 
I ran anova(lmer9) but it gives the ANOVA (F stat ) only for the fixed factor and doesn't give details about the random effects term and the interaction term.</p>

<p>Please suggest ways in which I could obtain a complete ANOVA...</p>

<p>Thanks,
Praniti</p>
"
"0.151862239065766","0.153418068129659","175597","<p>I am using the <code>quantreg</code> package in R to develop quantile estimates at different taus, then using <code>anova</code> to test whether the Beta Estimates at different quantiles are equal ($H_0$) or not ($H_1$). Thus </p>

<pre><code>library(quantreg)
data(Mammals) # sample data in quantreg
</code></pre>

<p>for taus 0.1, 0.25, 0.5, 0.75 and 0.9</p>

<pre><code>fit1 &lt;- rq(weight ~ speed + hoppers + specials, tau = .1, data = Mammals)
fit2 &lt;- rq(weight ~ speed + hoppers + specials, tau = .25, data = Mammals)
fit3 &lt;- rq(weight ~ speed + hoppers + specials, tau = .5, data = Mammals)
fit4 &lt;- rq(weight ~ speed + hoppers + specials, tau = .75, data = Mammals)
fit5 &lt;- rq(weight ~ speed + hoppers + specials, tau = .9, data = Mammals)

anova(fit1, fit2, fit3, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Results in </p>

<pre><code>Quantile Regression Analysis of Deviance Table

Model: weight ~ speed + hoppers + specials
Tests of Equality of Distinct Slopes: tau in {  0.1 0.25 0.5 0.75 0.9  }

             Df Resid Df F value  Pr(&gt;F)  
speed         4      531  1.0952 0.35810  
hoppersTRUE   4      531  2.5898 0.03599 *
specialsTRUE  4      531  1.3774 0.24046  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However changing the order of the models to say;</p>

<pre><code>anova(fit3, fit1, fit2, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Produces the <strong>exact same result!</strong></p>

<h2>My question is basically, what gives?</h2>

<p><strong>(1)</strong> Is <code>anova</code> truly comparing all the models to one another (ie all estimates from different taus, ${_nC_r} = {_5C_2} = 10$ <strong>separate</strong> comparisons) </p>

<p><strong>OR</strong> </p>

<p><strong>(2)</strong> Is <code>anova</code> selecting the model with the lowest tau and comparing the remaining models to that?</p>

<p>I've extracted (and annotated) the relevant segments of the of <code>anova</code> function called in the <code>quantreg</code> environment bellow.</p>

<pre><code>getAnywhere(anova.rqlist)
sum.fit1 &lt;- summary(fit1, covariance=TRUE); sum.fit2 &lt;- summary(fit2, covariance=TRUE); 
sum.fit3 &lt;- summary(fit3, covariance=TRUE); sum.fit4 &lt;- summary(fit4, covariance=TRUE); 
sum.fit5 &lt;- summary(fit5, covariance=TRUE)
objects &lt;- list(); objects[[1]] &lt;- sum.fit1; objects[[2]] &lt;- sum.fit2 
objects[[3]] &lt;- sum.fit3; objects[[4]] &lt;- sum.fit4; objects[[5]] &lt;- sum.fit5
taus &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)
m &lt;- length(taus)
n &lt;- length(fit1$y)
    Omega &lt;- outer(taus, taus, pmin) - outer(taus, taus) ##!!!HERE!!!###
    J &lt;- objects[[1]]$J 
# From help file on summary.rq: J is Unscaled Outer product of gradient matrix returned if cov=TRUE and se != ""iid"". The Huber sandwich is cov = tau (1-tau) Hinv %*% J %*% Hinv. 
p &lt;- dim(J)[1]
H &lt;- array(unlist(lapply(objects, function(x) x$Hinv)), c(p, p, m))
# From help file on summary.rq: Hinv : inverse of the estimated Hessian matrix returned if cov=TRUE and se %in% c(""nid"",""ker"") , note that for se = ""boot"" there is no way to split the estimated covariance matrix into its sandwich constituent parts.    
H &lt;- matrix(aperm(H, c(1, 3, 2)), p * m, p) %*% t(chol(J))
W &lt;- (H %*% t(H)) * (kronecker(Omega, outer(rep(1, p), rep(1, p)))) ##!!!HERE!!!###
coef &lt;- unlist(lapply(objects, function(x) coef(x)[, 1]))
Tn &lt;- pvalue &lt;- rep(0, p - 1)
ndf &lt;- m - 1
ddf &lt;- n * m - (m - 1)
for (i in 2:p) {
  E &lt;- matrix(0, 1, p)
  E[1, i] &lt;- 1
  D &lt;- kronecker(diff(diag(m)), E)
  Tn[i - 1] &lt;- t(D %*% coef) %*% solve(D %*% W %*% 
                                         t(D), D %*% coef)/ndf
  pvalue[i - 1] &lt;- 1 - pf(Tn[i - 1], ndf, ddf)
}
pvalue
</code></pre>

<p>The reason i care is that if explanation <strong>(1)</strong> is being implemented then all the estimates are truly being compared, while if explanation <strong>(2)</strong> is being implemented, then technically the models are only being compared to minimum tau and <strong>NOT</strong> to one another. </p>

<p><strong>Note:</strong> The lines that define <code>Omega</code> and <code>W</code> suggest to me that the latter interpretation <strong>(2)</strong> is being implemented, but I'm not sure.</p>
"
"0.16451742565458","0.166202907140464","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.1570467354963","0.129809192515054","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.1570467354963","0.129809192515054","179498","<p>I'm working on a project with R and I don't think I'm using the appropriate linear regression or plot, I've made both but they don't seem to match.  The study is an ANOVA comparing $CO_2$ emissions per capita with 5 groups of income levels and a relevant linear regression.  For the linear regression I want use $CO_2$ as the dependent variable and $GDP$ as the independent variable and the 5 $income$ levels as dummy variables.</p>

<p>Begin by ordering the variables and remove the intercept:</p>

<pre><code>income_factor = factor(Data01$income, levels=c(""Low income"", 
""Lower middle income"", ""Upper middle income"", ""High income: OECD"", ""High
income: nonOECD"")) 

lm.r = lm(CO2 ~ income_factor -1, data=Data01)
</code></pre>

<p>Gives</p>

<pre><code>summary(lm.r)
Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
income_factorLow income             0.2318     0.6943   0.334  0.73902    
income_factorLower middle income    1.7727     0.6355   2.789  0.00603 ** 
income_factorUpper middle income    4.7685     0.6271   7.604 4.12e-12 ***
income_factorHigh income: OECD      8.7926     0.7305  12.036  &lt; 2e-16 ***
income_factorHigh income: nonOECD  19.4642     1.3667  14.242  &lt; 2e-16 ***
</code></pre>

<p>So that we may write the linear regression in the form:</p>

<p>$$ CO_2 = \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 $$</p>

<p>Where $X_i$ is a dummy variable 1 at the level of income and 0 otherwise</p>

<p>For the corresponding plot I used:</p>

<pre><code> plot &lt;- ggplot(data=Data01, aes(x=GDP, y=CO2, colour=factor(income)))
 plot + stat_smooth(method=lm, fullrange=FALSE) + geom_point()
</code></pre>

<p>Which gives the graph</p>

<p><a href=""http://i.stack.imgur.com/5Iw53.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Iw53.png"" alt=""CO2 ~ GDP""></a></p>

<p>But here is my confusion, it looks like there is the <em>lm</em> term in the plot, but I don't think it is using the same values taken from the previous linear regression.  As Looking at summary from the linear regression, High income: OECD the estimate is 8.79, but the line for it is pretty much flat.</p>

<p>While I was typing this I realized that the graph has $GDP$ as the X-axis, but is not included in the linear regression.  Would multiplying by $income$_$factor*GDP$ help?</p>
"
"0.0669649530182425","0.0676510091491738","182100","<p>I created an algorithm and I tested it against a current algorithm.</p>

<p>The results are in this form:</p>

<pre><code>Power   Processes   Method  Time(s)
1          3          1     19,94
1          4          1     20,04
1          5          1     20,06
1          6          1     19,95
1          7          1     20,1
1          8          1     20,03
1          3          0     30,3 
...
</code></pre>

<p>for each method where my method is ""1"" and the other message is represented by ""0"". 
Process indicates the available processing power (I only have 4 servers therefore 3,4 processes may run each on a single server, 5-8 servers have to share resources - not indicated in the example table)</p>

<p>I've made 10 replications each test.</p>

<p>I wanted to create a linear regression comparing both models in order to show that even with more processes my algorithm runs faster. But the graphs and statics I could generate with ANOVA didn't really help me?</p>

<p>Which methods do I have to use?
And how may I generate graphs explaining the differences between the regressions?</p>
"
"0.0820149827720712","0.0828552264999161","186464","<p>I am working on a data set (n= 230) with a categorical dependent variable (outcome: 0/1) and six categorical independent variables (mostly, with only two levels). </p>

<p>There is a certain degree of multicollinearity between two variables (X1 and X6. Anova model comparison shows that a model with X1 performs slightly better than one containing X6) and <strong>a quasi-complete separation issue regarding X4</strong> (due to an empty cell).</p>

<p>I first ran a Random Forest model (all variables were included. Ntree = 5000, mtry = 3). The result was that X1, X2 and X3 are by far the most significant predictors. X4, X5 and X6 seem to have almost no discriminative power (especially X4 whose value  in vimp() is 0.00).The model seems to be reliable (C = 0.73).  </p>

<p><strong>Question 1</strong>: does it make sense at this point to fit Binary Logistic Regression only on the most important predictors obtained through the Random Forest model (X1, X2, X3) without even considering the other three?</p>

<p><strong>Question 2:</strong> In order to avoid the separation problem with Binary Logistic Regression would it make sense to get rid of X4? 
I am quite sure that the empty cell is a bias of my data set. Moreover, this category as a whole represents only 3% of the data (The contingency table is a: 140 <strong>b:0</strong> c:86 <strong>d:6</strong>).</p>
"
"0.0669649530182425","0.0676510091491738","187509","<p>With a small book-exercise with four metric variables on 10 cases (one dependent/outcome, three independent/predictor) I ran <em>linear regression</em> in <code>SPSS</code> and <code>R</code>, and <em>ANOVA</em> (in <code>SPSS</code> declaring the predictors as ""covariates"").<br>
I found the output of the <em>SSQ</em> (Sum-of-Squares) different - and obviously from this also the F-test statistic and the p-values. Except from the last predictor the displayed values are different (the predictors in <code>R</code>may be reordered and the analysis be rerun to find all <code>SPSS</code>- coefficients).                   </p>

<p>By reengineering the computations in matrix-formulae I could reproduce the SPSS-values as well as the R-values and found, that <code>SPSS</code> uses the (partial) SSQ based on the logic of the ""usefulness""-coefficients for each predictor (which is sort of semipartial coefficient), while <code>R</code> simply uses the (hierarchically) partial SSQ. <em>(Unfortunately I'm not sure how to express that two approaches correctly so this toy-characterizing might be improved)</em> .             </p>

<p><strong><em>Q:</em></strong> Has that property of different focuses/philosophies been discussed anywhere? Is there some advantage of one over the other?   </p>

<p><hr>
Data: (taken from M. Backhaus et al., multivariate Verfahren)             </p>

<pre><code>predictors                   outcome-item
---------------------------+-------------
Preis   VerkFoer  Vertreter  Absatzmenge
12.50      2000      109      2298
10.00       550      107      1814
 9.95      1000       99      1647
11.50       800       70      1496
12.00         0       81       969
10.00      1500      102      1918
 8.00       800      110      1810
 9.00      1200       92      1896
 9.50      1100       87      1715
12.50      1300       79      1699
</code></pre>

<p>The comparision of the output:
<a href=""http://i.stack.imgur.com/zNJDB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zNJDB.png"" alt=""bild""></a></p>
"
"0.0473513723810378","0.047836487323494","190239","<p>I am using the cp argument in the rpart function in R.  I would like to understand exactly how lack-of-fit is calculated for decision trees.  Please provide a simple example if possible.  Thanks.    </p>

<p>After investigating this in greater detail, I've solved part of the question.  In the example dataset I was using, we were building a regression tree.  In this instance, rpart uses the anova method by default.  For the anova method, the overall R-squared must increase by cp at each step.  </p>

<p>Since we have our predicted values and actual values from the training set, we are able to calculate SSR and SST to obtain R Squared (SSR/SST).  We can then compare the R-squared from the previous step to see if it increased by more than the complexity parameter.  </p>

<p>Now, I still do not know how the complexity parameter is used in a classification tree.</p>
"
"0.201906930241171","0.2039754673061","192173","<p>I'm working on the similarity of categorical regression with exclusively  dummy variables and ANOVA. There are lots of references, like Gujarati &amp; Porter (2009), which have mentioned that those two are equivalent. Everything is okay when distribution of residuals is normal, variances are homogeneous and regression model is significant. My questions are there. We have a category with 3 levels (red,blue,green), a numeric variable ""allscore""( -5 &lt;= allscore &lt;= +5). I played with R and made data and ran models (regression and variance).</p>

<pre><code># creating data 
bluescore  &lt;- rnorm(n=100, mean=-1, sd=1)
redscore   &lt;- rnorm(n=100, mean=2,  sd=1)
greenscore &lt;- rnorm(n=100, mean=.1, sd=2)
for (i in 1:100) {
  if (bluescore[i] &lt; -5)  bluescore[i]  &lt;- -5
  if (bluescore[i] &gt; 5)   bluescore[i]  &lt;-  5
  if (redscore[i] &lt; -5)   redscore[i]   &lt;- -5
  if (redscore[i] &gt; 5)    redscore[i]   &lt;-  5
  if (greenscore[i] &lt; -5) greenscore[i] &lt;- -5
  if (greenscore[i] &gt; 5)  greenscore[i] &lt;-  5
}
color &lt;- as.factor(c(rep(1,100), rep(2,100), rep(3,100)))
allscore &lt;- c(bluescore, redscore, greenscore)
table &lt;- data.frame(color, allscore)
randtable &lt;- table[sample(nrow(table)),]
finaltable &lt;- data.frame(randtable$color, randtable$allscore)
colnames(finaltable) &lt;- c(""color"", ""score"")
# plot
plot(randtable$allscore ~ randtable$color, data=finaltable)
# saving data for SPSS
library(rio)
export(finaltable, ""dummy.sav"")
write.csv(finaltable, ""finaltable.csv"")
# making dummy variables
dummyred   &lt;- NULL
dummygreen &lt;- NULL
dummyblue  &lt;- NULL
for(i in 1:NROW(finaltable)) {
  if (randtable$color[i]==2) dummyred[i]=1 else dummyred[i]=0
      if (randtable$color[i]==3) dummygreen[i]=1 else dummygreen[i]=0
  if (randtable$color[i]==1) dummyblue[i]=1 else dummyblue[i]=0
}
t1 = cbind(randtable, dummyred, dummygreen)
# run regression model 
mosel.1 &lt;- lm(formula = allscore~dummyred + dummygreen + dummyblue -1, data=t1)
ttt &lt;- summary(mosel.1)
ttt

# **test of homogenity**
# Bartlettâ€™s test
bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)
# Leveneâ€™s test
library(car)

leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
# Fligner-Killeen test
fligner.test(randtable$allscore ~ randtable$color, data=finaltable)
# ANOVA mode
hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
hh
summary(hh)
# post hoc test
TukeyHSD(hh)
</code></pre>

<p>Output would be something like this:  </p>

<p><a href=""http://i.stack.imgur.com/v0o8x.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v0o8x.png"" alt=""enter image description here""></a></p>

<pre><code>Call:
lm(formula = allscore ~ dummyred + dummygreen + dummyblue - 1, 
    data = t1)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.0152 -0.7880  0.0043  0.8088  3.3731 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
dummyred    2.02102    0.13273  15.227  &lt; 2e-16 ***
dummygreen  0.01525    0.13273   0.115    0.909    
dummyblue  -1.04294    0.13273  -7.858 7.24e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.327 on 297 degrees of freedom
Multiple R-squared:  0.4971,    Adjusted R-squared:  0.4921 
F-statistic: 97.87 on 3 and 297 DF,  p-value: &lt; 2.2e-16

&gt;  
&gt; # test of homogenity
&gt; # Bartlettâ€™s test
&gt; bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)

    Bartlett test of homogeneity of variances

data:  randtable$allscore by randtable$color
Bartlett's K-squared = 94.825, df = 2, p-value &lt; 2.2e-16

&gt; # Leveneâ€™s test
&gt; library(car)
&gt; 
&gt; leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value    Pr(&gt;F)    
group   2  43.995 &lt; 2.2e-16 ***
      297                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # Fligner-Killeen test
&gt; fligner.test(randtable$allscore ~ randtable$color, data=finaltable)

    Fligner-Killeen test of homogeneity of variances

data:  randtable$allscore by randtable$color
Fligner-Killeen:med chi-squared = 66.204, df = 2, p-value = 4.207e-15

&gt; # ANOVA mode
&gt; hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
&gt; hh
Call:
   aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

Terms:
                randtable$color Residuals
Sum of Squares         484.3572  523.2176
Deg. of Freedom               2       297

Residual standard error: 1.327281
Estimated effects may be unbalanced
&gt; summary(hh)
                 Df Sum Sq Mean Sq F value Pr(&gt;F)    
randtable$color   2  484.4  242.18   137.5 &lt;2e-16 ***
Residuals       297  523.2    1.76                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # post hoc test
&gt; TukeyHSD(hh)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

$`randtable$color`
         diff        lwr       upr p adj
2-1  3.063958  2.6218117  3.506104 0e+00
3-1  1.058184  0.6160382  1.500330 1e-07
3-2 -2.005774 -2.4479195 -1.563628 0e+00
</code></pre>

<ul>
<li>Is variance homogeneity check essential for regression model as assumption (because it compares means and equivalent to ANOVA)?</li>
<li>What is assumption for this regression model?</li>
<li>How can I interpret ""greendummy"" variable insignificance? Can I omit it from model? What theory support this omission? Is it means green color has no effect on scores? Is it equivalent to heterogeneity of variances?</li>
<li>How about ANOVA model, what can I say about the results?  </li>
<li>Can I remove green level from ANOVA?</li>
</ul>

<blockquote>
  <p>Gujarati, Damodar N.; Porter, Dawn C. (2009): Basic econometrics. 5th
  ed. Boston: McGraw-Hill Irwin (The McGraw-Hill series, economics).</p>
</blockquote>
"
"0.117188667781924","0.135302018298348","193752","<p>I have one response variable $Y$ and one predictor $X$. I am trying to fit a polynomial regression model and try to compare different model with different highest power term, the output of ANOVA in R is the following</p>

<pre><code>Analysis of Variance Table
Model 1: Y ~ X
Model 2: Y ~ X + I(X^2)
Model 3: Y ~ X + I(X^2) + I(X^3)
Model 4: Y ~ poly(X, 5)

  Res.Df    RSS   Df  Sum of Sq       F    Pr(&gt;F)    
    504    19472                                   
    503    15347  1    4125.1     151.693 &lt; 2.2e-16 ***
    502    14616  1     731.8     26.909 3.104e-07 ***
    500    13597  2    1018.4     18.726 1.438e-08 ***

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I understand how to derive all these numbers in the table, but there are some contradiction. Here is my question: in this table, the last model is the biggest model in the sense that it contains all the predictors in all previous models, the ""Sum of Sq"" for Model 4 is 1018.4 = 14616-13597, i.e., the difference of the sum of residuals between model 3 and model 4 and the F statistic for Model 4, which is 18.726 is obtained by $\frac{RSS_3-RSS_4}{502-500}\div\frac{13597}{500}$, i.e., the difference in RSS between model 3 and model 4 divide by the difference of degree of freedom and then divide by the MSE of model 4. This makes a lot of sense. However, when I compute the F statistic for model 3, I am so confused. The ""Sum of Sq"" for model 3 is obtained via $731.8=15347-14616$, i.e,. the difference in RSS of model 2 and model 3. But the F statistic for model 3 is obtained via $\frac{RSS_2-RSS_3}{503-502}\div\frac{13597}{500}$, i.e., the difference in RSS of model 2 and model 3 divide by their difference in degree of freedom, BUTTTT then divide by the MSE of model 4. In my mind, it should finally divide the MSE of model 3 rather than model 4, since we are comparing the difference between model 2 and model 3. </p>
"
"0.200894859054728","0.191677859255993","197710","<p><strong>Experiment:</strong></p>

<p>I have 2 groups and both groups undergo 2 set of evaluations, one with MRI scanner and the other in the lab to test for their behavior. Both these evaluations are known to have statistically significant relationship with age and gender. </p>

<p>Statistical questions:</p>

<p>Whether there is: </p>

<p>1) statistically significant difference between the 2 groups on each evaluation? </p>

<p>2) any relationship between and within the 2 groups between each evaluation?  </p>

<p><strong>Model:</strong></p>

<p>I model the problem as </p>

<p>$\text{MRI_measure} = \beta_{0} + \beta_{1} \text{Age} + \beta_{2} \text{Gender} + \beta_{3} \text{Group} $</p>

<p>$\text{Lab_measure} = \beta_{0}+ \beta_{1}  \text{Age} + \beta_{2}  \text{Gender} +\beta_{3}  \text{Group}$</p>

<p>[Age is continuous and Gender, Group are factors/categorical] </p>

<p>In R: </p>

<pre><code>MRI_model&lt;-lm(cbind(MRI_measure, Lab_measure) ~ age+gender+group, data=data) 
</code></pre>

<p><strong>Result of R:</strong> </p>

<p><code>manova(MRI_model)</code> suggests that yes indeed all the slopes are significantly different than 0 suggesting a relationship between my measures. </p>

<p><strong>Questions</strong> </p>

<p>1) In order to test whether the difference in the MRI_measure is statistically significant between the 2 groups, I use MRI_model$fitted.values for each dependent measure and do a statistical test (either t-test or Wilcox) and claim that the difference is significant. </p>

<p>In the paper I write, multivariate multiple linear regression was performed for the groups while controlling for age and gender. The regressed out MRI_measure was statistically compared to see if the difference is different. </p>

<p>I am assuming that the predicted/fitted.values in model are the regressed out variables. Can I show this and use this result? Is this right? </p>

<p>If no, what is the correct way to statistically compare whether my 2 groups differ in their MRI measure and lab measure when controlled for age and gender. Any R library, literature, possibly a script will be greatly appreciated. </p>

<p>2) I also want to see if there is any relationship between MRI_measure and Lab_measure within the group after they are controlled for age and gender. What is the correct way to do this in R? </p>

<p>Further, I also want to see if there is any significantly different association between the 2 groups for my set of dependent variables. I am thinking of first finding the correlation between 2 dependent variable in each group and test if this correlation is statistically different between the 2 groups? Is this logic right? And if it is, how do I compare the correlation? If not, what is the right way to do this? Any R library, literature, possibly a script will be greatly appreciated. </p>
"
"0.201173686196623","0.181841583682237","198484","<p>Consider this example:</p>

<pre><code>foo &lt;-data.frame(x=c(0.010355057,0.013228936,0.016313905,0.019261687,0.021710159,0.023973474,0.025968176,0.027767232,0.029459730,0.030213807,0.023582566,0.008689883,0.006558429,0.005144958),
                 y=c(971.3800,1025.2271,1104.1505,1034.2607,902.6324,713.9053,621.4824,521.7672,428.9838,381.4685,741.7900, 979.7046,1065.5245,1118.0616))
Model3 &lt;- lm(y~poly(x,3),data=foo)
Model4 &lt;- lm(y~poly(x,4),data=foo)
</code></pre>

<p>For <code>Model3</code>, the <code>poly(x,3)</code> term is not significant:</p>

<pre><code>&gt; summary(Model3)

Call:
lm(formula = y ~ poly(x, 3), data = foo)

Residuals:
   Min     1Q Median     3Q    Max 
-76.47 -51.61  -0.55  38.22 100.57 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   829.31      17.85  46.463 5.14e-13 ***
poly(x, 3)1  -819.37      66.78 -12.269 2.37e-07 ***
poly(x, 3)2  -373.05      66.78  -5.586 0.000232 ***
poly(x, 3)3   -87.85      66.78  -1.315 0.217740    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 66.78 on 10 degrees of freedom
Multiple R-squared:  0.9483,    Adjusted R-squared:  0.9328 
F-statistic: 61.15 on 3 and 10 DF,  p-value: 9.771e-07
</code></pre>

<p>However, for <code>Model4</code> it is:</p>

<pre><code>&gt; summary(Model4)

Call:
lm(formula = y ~ poly(x, 4), data = foo)

Residuals:
    Min      1Q  Median      3Q     Max 
-34.344 -19.982   1.229  18.499  33.116 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  829.310      7.924 104.655 3.37e-15 ***
poly(x, 4)1 -819.372     29.650 -27.635 5.16e-10 ***
poly(x, 4)2 -373.052     29.650 -12.582 5.14e-07 ***
poly(x, 4)3  -87.846     29.650  -2.963 0.015887 *  
poly(x, 4)4  191.543     29.650   6.460 0.000117 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 29.65 on 9 degrees of freedom
Multiple R-squared:  0.9908,    Adjusted R-squared:  0.9868 
F-statistic: 243.1 on 4 and 9 DF,  p-value: 3.695e-09
</code></pre>

<p>Why does this happen? Note that the estimate of all coefficients is the same in both cases, since the polynomials are orthogonal. However, the significance is not. This seems to me difficult to understand: if I performed a degree 3 regression, it looks like I could drop the <code>poly(x, 4)3</code> term, thus reverting to a degree 2 orthogonal regression. However, if I performed a degree 4 regression, I shouldn't, even though the coefficients of the common terms have exactly the same estimate. What do I conclude? Probably that one should never trust subset selection :) An <code>anova</code> analysis says that the difference among the degree 2, degree 3 and degree 4 models is significant:</p>

<pre><code>&gt; Model2 &lt;- lm(y~poly(x,2),data=foo)     
&gt; anova(Model2,Model3,Model4)
Analysis of Variance Table

Model 1: y ~ poly(x, 2)
Model 2: y ~ poly(x, 3)
Model 3: y ~ poly(x, 4)
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1     11 52318                                   
2     10 44601  1      7717  8.7782 0.0158868 *  
3      9  7912  1     36689 41.7341 0.0001167 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>EDIT: following a suggestion in comments, I add the residual vs fitted plots for <code>Model2</code>, <code>Model3</code> and <code>Model4</code></p>

<p><a href=""http://i.stack.imgur.com/9ZU8h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ZU8h.png"" alt=""enter image description here""></a>` </p>

<p>It's true that the maximum residual error is more or less the same for <code>Model2</code> and <code>Model3</code>, and it becomes nearly one third going from <code>Model3</code> to <code>Model4</code>. There seems to be still some kind of trend in the residuals, though it is less evident than for <code>Model2</code> and <code>Model3</code>. However, why does this invalidate the <em>p</em>-values? Which hypothesis of the linear model paradigm is violated here? I seem to remember that the residuals only had to be uncorrelated with the predictor. However, if they also have to uncorrelated among themselves, then clearly this assumption is violated and the <em>p</em>-values based on the t-test are invalid.</p>
"
"0.0820149827720712","0.0552368176666107","200053","<p>I'm interested in analyzing a data set in which I have 6 dependent variables = foreclosure rates across 500+ zip codes for 6 consecutive years, and multiple  predictor variables. I have already run individual regressions using a quasi-poisson distribution to see how the predictors impact the DVs independently, like so: </p>

<pre><code>    model1&lt;- glm(Rate2008 ~ P1 + P2 + P3, data=mydata, family=quasipoisson())
    model2&lt;- glm(Rate2009 ~ P1 + P2 + P3, data=mydata, family=quasipoisson())
    model3&lt;- glm(Rate2010 ~ P1 + P2 + P3, data=mydata, family=quasipoisson())
    model4&lt;- glm(Rate2011 ~ P1 + P2 + P3, data=mydata, family=quasipoisson())
    model5&lt;- glm(Rate2012 ~ P1 + P2 + P3, data=mydata, family=quasipoisson())
    model6&lt;- glm(Rate2013 ~ P1 + P2 + P3, data=mydata, family=quasipoisson())
</code></pre>

<p>The problem, of course, is that these DVs are not independent, and so I need a way to run the regression whereby I can include all 6 in the model, allowing me to take into consideration this dependence. Everything I've googled so far suggests something like this (from a previous cross validate question: <a href=""http://stats.stackexchange.com/questions/11127/multivariate-multiple-regression-in-r"">Multivariate multiple regression in R</a> ): </p>

<pre><code>  Y &lt;- cbind(mydata$Rate2008, mydata$Rate2009,
  mydata$Rate2010, mydata$Rate2011, mydata$Rate2012, mydata$Rate2013)

  model7&lt;- lm(Y ~ P1 + P2 + P3, data=mydata,)
  summary(manova(model7))
</code></pre>

<p>My questions are: </p>

<p>1) Because these are rates (continuous, but do not go below zero), shouldn't I be using glm (and the poisson or quasi-poisson distribution) instead of lm? Or does this not matter given the multivariate/dependent nature of the DVs?</p>

<p>2) I'm not sure how to interpret the results from model 7 - the output doesn't distinguish between the six variables inputted into the 'Y' matrix. Any ideas why? </p>
"
"0.171165004875037","0.172918590828624","201105","<p>I'm about to have data from intercept surveys conducted in parks. The goal of the survey is to determine which characteristics of parks users find most important to park quality (do they care a lot about safety, a little about the facilities, and not at all about who else is there?).</p>

<p>We've designed a survey with open-ended questions to answer this question. The current plan is to take down the responses, and then, once we have them, group them into categories (safety, facilities, social environment, accessibility, etc). </p>

<p>For example, one question on the survey asks the user why they came to park. </p>

<p>Each user's response (we're allowing them to list as many reasons as they like, but are asking for primary reasons first, then secondary reasons and so on) will then be associated with some field coding. For one user it might be, say, facilities and park aesthetics, for another it might be easy access. We'll also have some demographic data (age, sex, ethnicity, activity at the park) for each user.</p>

<p><strong>Question 1:</strong> We want to determine which of the categories is most important to users, and if possible, by how much. I've never done any categorical data analysis, and I have no idea what to do here. For some questions we're just going to have counts: 16 people came for facilities, 10 for open spaces, etc.</p>

<p><strong>Question 2:</strong> A separate series of questions asks users to categorize park quality on a Likert-like scale (low to high quality), and also to rate sub-components of park quality in the same way (quality of facilities, from low to high, and so on). We want to determine which predictors have the largest effect on perceived park quality here as well.</p>

<p><strong>I want to know what type of models to fit to our data, and why.</strong></p>

<p>I'm presuming we want some categorical analogue of regression. I want to pick up theoretical underpinnings, learn how to fit models in R, and also how to perform diagnostics on them. </p>

<p>Once I've decided on the appropriate analysis and have picked up the necessary background, I'd like to pre-register my data analysis plan. I've never done this before and am curious what the convention is for this.</p>

<p>Some details about the sample of parks: the city Parks and Recreation department has selected 10 parks for us to visit. Their park selection criteria is not entirely known, but I think they want to visit some well developed and some under developed parks. There are five pairs of parks that the Parks department thinks are comparable. In each pair of parks, one has recently undergone renovation, and the other hasn't.</p>

<p>My background:</p>

<p>I have taken a first course in math-stat, a course on linear regression, and am halfway through a course on experimental design/ANOVA/EM/Bootstrap. I have some pure math, multi, lin-alg and optimization background as well. I have some limited experience in R as well.</p>
"
"0.0669649530182425","0.0676510091491738","204119","<p>There is a package in R called <code>pwr</code>. This is useful to make power analysis when designing the sampling of a project. here are few examples: </p>

<pre><code>library(pwr)
pwr.anova.test(k = 4, f = 0.5, sig.level = 0.05, power = .9)
pwr.2p.test(h = 0.5, sig.level =0.05, power = .9)
pwr.f2.test(u = 4, f2 = .5, sig.level = 0.05, power = .8)
</code></pre>

<p>However, is it possible to run a power analysis for a spline regression (or a generalized additive model (GAM))? I want to know how may organisms I would have to sample to detect an effect of selection, that is a shift in morphology of the beak of birds of only 0.5Â mm, given that my sig.level = 0.05 and that I have 4 species. </p>

<p>Also, Iâ€™m recapturing birds in a population each year since 2003. Is there a power calculation to estimate how many birds should I sample to get a probability of recapture of 25%? Iâ€™m running a recapture model in Bayesian statistics, so there is not a function in the package <code>pwr</code> that can do this. </p>
"
"0.0966555841283824","0.117174985029676","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"NaN","NaN","205227","<p>I'm a little new to R and I haven't done stats in a while. I know a one way ANOVA is the same as a linear regression, but is there a difference between a two way ANOVA and a linear regression with two covariates? And if they are different I'm not sure which one I performed. Below is my sample code:</p>

<pre><code>data.frame[[""Acute""]] = factor(data.frame[[""Acute""]])
data.frame[[""Frequency""]] = factor(data.frame[[""Frequency""]])
DishMortalityVsTime.Total.Acute.Freq = aov(Dish.Mortality ~ Time * Acute * Frequency, data=data.frame)
summary(DishMortalityVsTime.Total.Acute.Freq)
</code></pre>

<p>and the output</p>

<pre><code>                      Df Sum Sq Mean Sq F value               Pr(&gt;F)    
Days                   1  1.352  1.3524  65.189  0.00000000000000429 ***
Acute                  2  5.885  2.9423 141.822 &lt; 0.0000000000000002 ***
Frequency              3  0.539  0.1795   8.653  0.00001279126504853 ***
Days:Acute             2  1.672  0.8361  40.302 &lt; 0.0000000000000002 ***
Days:Frequency         3  0.050  0.0165   0.796                0.496    
Acute:Frequency        6  0.787  0.1311   6.320  0.00000192315201011 ***
Days:Acute:Frequency   6  0.038  0.0064   0.309                0.932    
Residuals            552 11.452  0.0207 
</code></pre>

<p>Any help would be appreciated, Thanks!</p>
"
"0.115986700954059","0.117174985029676","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.0966555841283824","0.117174985029676","213804","<p>I am running some linear regressions in R. I am dealing with a linear dependent and linear as well as categorical independent variables using <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html"" rel=""nofollow"">lm</a>. So far, I have looked at the output that <code>summary(model)</code> gives me. </p>

<p>Other studies instead run <a href=""http://www.inside-r.org/packages/cran/car/docs/Anova"" rel=""nofollow"">Anova()</a> from the <a href=""https://cran.r-project.org/web/packages/car/index.html"" rel=""nofollow"">car</a> package on their linear model, which returns a similar table. The docs for <code>Anova()</code> state that it</p>

<blockquote>
  <p>Calculates type-II or type-III analysis-of-variance tables for model objects. </p>
</blockquote>

<p>I am under the impression that this <code>Anova()</code> returns an F instead of the t-statistic but is ~ equivalent in what its tell me. (sample output below). So I was wondering</p>

<ul>
<li><p>Are standard R <code>summary(lm)</code> and car <code>Anova(lm)</code> indeed doing pretty much the same calculations here? If not, what is the difference?</p></li>
<li><p>They both report the same p-value, however the F-statistic at the bottom of the standard output is different from the <code>Anova()</code> one. Why is that?</p></li>
<li><p>What are applications where one would choose one over the other?</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>Sample output:</p>

<p>Standard R</p>

<pre><code>summary(linreg)
...
         Estimate    t value    Pr(&gt;|t|)
Age      -18.016     -3.917     0.000107
Gender   -45.4912    -4.916     1.35e-06
---
Residual standard error: 85.81 on 359 degrees of freedom
F-statistic: 16.71 on 2 and 359 DF, p-value: 1.147e-07
</code></pre>

<p>Anova() output</p>

<pre><code>Anova(linreg)

Anova Table (Type II tests)

           Sum Sq    F value   Pr (&gt;F)
Age        112997    15.345    0.0001072
Gender     1777936   24.164    1.348e-06
</code></pre>
"
"0.189405489524151","0.191345949293976","214449","<p>I am trying to determine if immigration status is a determinant of risk preferences. To do this, I am using the 2014 Health and Retirement Study data which has approximately ~20,000 participants and is representative of residents in the US over age 50. I have two different measures of risk preferences that I am using in this analysis.</p>

<p>The first risk measure is based on a 0-10 rating that participants gave themselves for how risky they are in general situations. The regression using this risk measure looks like this:</p>

<pre><code>fit1_usesRaw &lt;-vglm(Risk_Pct ~ is.native + is.male + oh + cjs + ms + age2 + tw,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"", ""is.male"", and ""ms""(marital status). Is.native has a positive coefficient and is.male and ms each have negative coefficients. </p>

<p>The second risk measure is based on the percentage of the participant's retirement accounts kept in stocks. The regression for this risk measure looks like this:</p>

<pre><code>fit2_usesRaw &lt;-vglm(PCT_Stocks_MF_1 ~ is.native + is.male + oh + cjs + ms + age2 + tw + sme,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"" (negative coefficient), ""is.male"" (positive coefficient), ""age2"" (positive coefficient), ""tw"" (total wealth, negative coefficient), and ""cjs"" (current job status, negative coefficient). </p>

<p>How can I test to see if the differences between the two risk measures are significant? Is there any way to determine which risk measure is ""right""? I tried a ANOVA test, but I'm unsure if that would be correct. The output for the Anova was:</p>

<pre><code> Anova(w1.mod, test = ""Roy"")

Type II MANOVA Tests: Roy test statistic
          Df test stat approx F num Df den Df    Pr(&gt;F)    
is.native  1  0.008895    7.441      2   1673 0.0006063 ***
is.male    1  0.048695   40.733      2   1673 &lt; 2.2e-16 ***
age2       1  0.020496   17.145      2   1673  4.26e-08 ***
oh         1  0.000329    0.275      2   1673 0.7596626    
ms         1  0.002546    2.130      2   1673 0.1191937    
tw         1  0.002674    2.236      2   1673 0.1071594    
sme        1  0.000688    0.576      2   1673 0.5624582    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Does this tell me there are significant differences between ""is.native"", ""is.male"" and ""age2"" using the two different risk measures? Or am I reading this output completely incorrectly?</p>

<p>Thanks for any help!</p>
"
"0.0820149827720712","0.0828552264999161","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.0947027447620757","0.095672974646988","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.0820149827720712","0.0828552264999161","218879","<p>I'm looking to model percent change in transaction year over year for sales people grouped in certain categories (7 categories total). The percent change in transactions would be Q1 of the current year divided by the number of transactions from Q1 of the previous year which gives me a percent change (positive or negative). </p>

<p>Since this is count data the correct model to use is the Poisson distribution and eventually I would like to do a Poisson regression to look at the effects of predictor variables on percent change in transactions.</p>

<p>But my question is, if I want to compare the means between the different groups what is the analogous version of the ANOVA for Poisson assumptions. (if that's the correct way to put it) </p>
"
"0.105880887471907","0.10696563746014","221011","<p>I have two monthly time series: </p>

<ul>
<li>one for house prices expressed in annual change growth rates: $\left( \text{ln}(X_t) - \text{ln}(X_{t-12})\right) - \left( \text{ln}(X_{t-1}) - \text{ln}(X_{t-1-12})\right)$;</li>
<li>the other simply in growth rates: $\text{ln}(X_t) - \text{ln}(X_{t-1})$. </li>
</ul>

<p>Here is the data: </p>

<pre><code>House Prices = [1]  0.009189829  0.022612618  0.003952796 -0.015179184  0.001903336 -0.028779902  0.025668239 -0.011237850
  [9]  0.014782630 -0.018844480 -0.023547458  0.020613233  0.029281069 -0.010539781  0.006707366  0.023693144
 [17] -0.002632498  0.148738752 -0.154539337  0.013908319 -0.002294980  0.013274177  0.010043605 -0.007862785
 [25] -0.018297295 -0.003167249  0.022984841  0.001666694 -0.001310199 -0.131548705  0.114723242 -0.003431495
 [33]  0.000953231 -0.010096108 -0.009434595 -0.037774255  0.030877947 -0.011245971 -0.018800312 -0.012805013
 [41]  0.001326392 -0.012034079 -0.045279346 -0.017308170  0.002490863 -0.007340975  0.005052948 -0.024053201
 [49] -0.004190424 -0.028607790  0.004678486  0.026626293 -0.015166864  0.006988983  0.038257855  0.020798177
 [57]  0.008175391  0.021294030 -0.013331432  0.030969145  0.017065249 -0.002672683  0.019435476 -0.037047871
 [65]  0.001844432  0.007663458  0.034406137 -0.049379845 -0.012527106 -0.012859680  0.012954488 -0.015463951
 [73] -0.025509006  0.006318645  0.012977464  0.019940525 -0.025592828  0.020774198 -0.033613414  0.018338077
 [81]  0.001765807  0.009236604 -0.041413104  0.030227358  0.017180849  0.012593360 -0.039001526 -0.004994992
 [89]  0.037766071 -0.043167230 -0.016613786  0.023199890 -0.016214873 -0.012282560  0.065978520 -0.031465767
 [97]  0.006355108 -0.000449523 -0.005810647  0.016823517 -0.021988463  0.026178014  0.007654339 -0.008356379
[105]  0.013273736  0.031645473 -0.046408064  0.022334664  0.008517194 -0.014892335  0.019147342  0.007955040
[113]  0.014122506 -0.035722162  0.018174284  0.021410306 -0.038943797 -0.014517888  0.032750195  0.022506553
[121] -0.003870785  0.130924075 -0.057934974 -0.174228244  0.016937619  0.010647759  0.015691962 -0.033174094
[129]  0.038263205  0.003456250 -0.013422897

B = [1] -0.0223848461  0.0102749646  0.0913403867 -0.0758207770 -0.0053898407 -0.0204047336  0.0050358986
  [8]  0.0195335195 -0.0200303353 -0.0045390828  0.0056380761 -0.0004492945  0.0040043649  0.0012918928
 [15] -0.0104850394  0.0047110190  0.0049805985 -0.0046957178  0.0095002549  0.0202597343 -0.0183526932
 [22]  0.0237185217 -0.0137022065  0.0133787918 -0.0212629487  0.0070512978  0.0959447868 -0.0801519036
 [29] -0.0362526334 -0.0000278572  0.0269014993  0.0009862920 -0.0329868357  0.0283667004 -0.0135186142
 [36] -0.0004975495  0.0053822189  0.0108219907 -0.0078419784  0.0418340658 -0.0316367599 -0.0092324801
 [43] -0.0192830637  0.0336003682  0.0021479539 -0.0146426306  0.0003717930  0.0216259502 -0.0323127786
 [50]  0.0033077606 -0.0123735085 -0.0014757035  0.0266339779 -0.0228959378  0.0002848944  0.0133572802
 [57] -0.0093035312 -0.0034350607  0.0052349772  0.0115210916 -0.0122443122  0.0435497970 -0.0100099291
 [64]  0.0267252321 -0.0654005679  0.0088385287 -0.0089122237  0.0155299273 -0.0027394997 -0.0126183268
 [71]  0.0090999709  0.0017039487 -0.0144843611  0.0269128625  0.0042663583  0.0220574344 -0.0523831016
 [78] -0.0059331639  0.0171559908  0.0125030653  0.0151902738  0.0471484001 -0.0477394702  0.0888317354
 [85] -0.1044700154  0.0234134906 -0.0215966718  0.0157974035  0.0970094980 -0.1049559862 -0.0290578406
 [92]  0.0617653831 -0.0132202439  0.0022117274  0.0091225692  0.0424813190 -0.0614889434  0.0163745828
 [99] -0.0112793057  0.0666179349 -0.0352838073 -0.0259179501  0.0269557599  0.0127882202 -0.0430512536
[106]  0.0862308560 -0.0633012329  0.0596481270  0.0900367605 -0.0303162498 -0.0153738373 -0.0442218848
[113] -0.0116158350 -0.0531058308  0.2036373944  0.1598602057 -0.3837940703 -0.0069112146 -0.0192015196
[120]  0.0110269191 -0.0351135484  0.0439917033  0.0522746614  0.0036354828 -0.0414276671 -0.0361649669
[127]  0.0080753079  0.0352684982 -0.0282391428 -0.0141622744  0.0045799464
</code></pre>

<p>I am studying if <code>B</code> has an effect on <code>House prices</code>. For this reason first a take a simple liner regression between the two and I get a negative and significant estimate at the 95% confidence interval: (-0.0004189 *). </p>

<p>Wanting to reach a step forward I undertake a Granger causality test as following:</p>

<p>I) Determine the optimal number of lags using the AIC/BIC test using:</p>

<pre><code>select.lags&lt;-function(x,y,max.lag=20) {
  y&lt;-as.numeric(y)
  y.lag&lt;-embed(y,max.lag+1)[,-1,drop=FALSE]
  x.lag&lt;-embed(x,max.lag+1)[,-1,drop=FALSE]

  t&lt;-tail(seq_along(y),nrow(y.lag))

  ms=lapply(1:max.lag,function(i) lm(y[t]~y.lag[,1:i]+x.lag[,1:i]))

  pvals&lt;-mapply(function(i) anova(ms[[i]],ms[[i-1]])[2,""Pr(&gt;F)""],max.lag:2)
  ind&lt;-which(pvals&lt;0.05)[1]
  ftest&lt;-ifelse(is.na(ind),1,max.lag-ind+1)

  aic&lt;-as.numeric(lapply(ms,AIC))
  bic&lt;-as.numeric(lapply(ms,BIC))
  structure(list(ic=cbind(aic=aic,bic=bic),pvals=pvals,
                 selection=list(aic=which.min(aic),bic=which.min(bic),ftest=ftest)))
}

s&lt;-select.lags(Topic.15,House.Prices,20)
t(s$selection)
plot.ts(s$ic)
</code></pre>

<p>As a result I get:     </p>

<pre><code>aic bic ftest
14  12  13   
</code></pre>

<p>Here is when it comes the first doubt: why are they giving me different results? Nevertheless, when I do the Granger causality test for both directions using these numbers as possible lags I get in all high significant results (***) only in the direction that <code>B</code> is causing <code>House prices</code> movements:</p>

<pre><code>lmtest::grangertest(Topic.15,House.Prices,12)
lmtest::grangertest(House.Prices,Topic.15,12)
</code></pre>

<p>I do not seem to see the direction of the cause, is it possitive or negative (an increase in <code>B</code> produces an increase or a drop in <code>House prices</code> at time $t+1$?).<br>
Another question, is the conclusion valid that changes in <code>B</code> produce changes in <code>House prices</code>? What are the weakness in this line of argument?</p>
"
"0.115986700954059","0.0781166566864507","222431","<p>See this file here: <a href=""http://www.math.uvic.ca/~nathoo/stat359-material/decay.TXT"" rel=""nofollow"" title=""Decay.txt"">Decay.TXT</a>. </p>

<p>I first tried to fit the logarithmic model first </p>

<pre><code>dec = read.table('decay.txt', header=T)
attach(dec)
logy &lt;- log(y)
model1 &lt;- lm(logy ~ x)
</code></pre>

<p>And now I try to fit a quadratic model into the data: </p>

<pre><code>model2 &lt;- lm(y ~ x + I(x^2))
</code></pre>

<p>I now attempt to get the r^2 (r-squared values) of the models. </p>

<pre><code>(rsq1 &lt;- summary(model1)$r.squared)
[1] 0.8307964

(rsq2 &lt;- summary(model2)$r.squared)
[1] 0.9079788
</code></pre>

<p>Obviously as seen, here the coefficient of determination of the quadratic regression model is better than the exponential regression model (for this dataset). </p>

<p>However, the critical appraisal of these models says otherwise. For the exponential model, we see that: 
<a href=""http://i.stack.imgur.com/ha55L.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ha55L.jpg"" alt=""Exponential regression""></a></p>

<p>And for the quadratic polynomial model, we see that: 
<a href=""http://i.stack.imgur.com/B30jv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B30jv.jpg"" alt=""Quadratic regression""></a></p>

<p>The QQ Plot reveals something strange with the residuals for the Quadratic regression i.e ""Not Normally Distributed"". </p>

<p>Now I am trying to use ANOVA, to compare the two models. And I type: </p>

<pre><code>anova(model1, model2)  
</code></pre>

<p>and there is an error regarding the missing variable <code>y</code>. What is the right way to conduct the <code>anova</code> ?    </p>
"
"0.115986700954059","0.0781166566864507","222514","<p>See this file here: <a href=""http://www.math.uvic.ca/~nathoo/stat359-material/decay.TXT"" rel=""nofollow"" title=""Decay.txt"">Decay.TXT</a>. </p>

<p>I first tried to fit the logarithmic model first </p>

<pre><code>dec = read.table('decay.txt', header=T)
attach(dec)
logy &lt;- log(y)
model1 &lt;- lm(logy ~ x)
</code></pre>

<p>And now I try to fit a quadratic model into the data: </p>

<pre><code>model2 &lt;- lm(y ~ x + I(x^2))
</code></pre>

<p>I now attempt to get the r^2 (r-squared values) of the models. </p>

<pre><code>(rsq1 &lt;- summary(model1)$r.squared)
[1] 0.8307964

(rsq2 &lt;- summary(model2)$r.squared)
[1] 0.9079788
</code></pre>

<p>Obviously as seen, here the coefficient of determination of the quadratic regression model is better than the exponential regression model (for this dataset). </p>

<p>However, the critical appraisal of these models says otherwise. For the exponential model, we see that: 
<a href=""http://i.stack.imgur.com/ha55L.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ha55L.jpg"" alt=""Exponential regression""></a></p>

<p>And for the quadratic polynomial model, we see that: 
<a href=""http://i.stack.imgur.com/B30jv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B30jv.jpg"" alt=""Quadratic regression""></a></p>

<p>The QQ Plot reveals something strange with the residuals for the Quadratic regression i.e ""Not Normally Distributed"". </p>

<p>Now I am trying to use ANOVA, to compare the two models. And I type: </p>

<pre><code>anova(model1, model2)  
</code></pre>

<p>and there is an error regarding the missing variable <code>y</code>, because the response variables are different. </p>

<p>How can I statistically compare the two models? </p>
"
"0.0669649530182425","0.0676510091491738","227073","<p>I have a small perplexity some of you might be able to help me with. 
I have fitted a linear model in R of the form</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
</code></pre>

<p>and I want to obtain Likelihood Ratio Tests on the regression coefficients for <code>X1</code> and <code>X2</code>. 
One way to get them is using:</p>

<pre><code>anova(fullmodel, test=""LRT"")
</code></pre>

<p>But, in my understanding, if I use <code>anova</code> on the full model it removes covariates and performs LRT sequentially, indeed results differed depending on ordering of predictors.
<code>drop1</code>, on the other hand, drops one covariate at a time and leaves the rest untouched; thus I could use:</p>

<pre><code>drop1(fullmodel, test=""Chisq"")
</code></pre>

<p>This should work. Yet, out of curiosity, I also tried the following:</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
reducedmodel1 = lm(Y ~ X1)
reducedmodel2 = lm(Y ~ X2)

anova(fullmodel, reducedmodel1, test=""LRT"")
anova(fullmodel, reducedmodel2, test=""LRT"")
</code></pre>

<p>In my understanding, the two procedures (<code>drop1</code> and the two separate <code>anova</code>) have identical meaning and should give exactly the same p-values. That's not the case, though; they differ already at the 3rd decimal number. 
Can anyone explain to me why this happens? Am I doing something wrong?</p>
"
"0.177172612243394","0.178987746151269","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"0.115986700954059","0.117174985029676","229997","<p>My response variable is a combining of two variables which I would like to rescale with different weights. ratio:0.5/0.5; 0.25/0.75; 0.1/0.9.
Now I would like to test which is the best fit.
The three models have overdispersion so I used quasipoisson regression.</p>

<p>Following former instructions in this matter, I used the F test, but it didn't work because of the different response variable. I got this massage:</p>

<pre><code>    anova(glm0.5_0.5,glm0.75_0.25,glm0.9_0.1, test = ""F"")        


Model: quasipoisson, link: log

Response: data$dep0.5_0.5

Terms added sequentially (first to last)


             Df Deviance Resid. Df Resid. Dev       F    Pr(&gt;F)    
NULL                           575    11386.0                      
academic95    1   3720.1       574     7666.0 231.505 &lt; 2.2e-16 ***
israeli_ac95  1   1239.1       573     6426.8  77.113 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Warning message:
In anova.glmlist(c(list(object), dotargs), dispersion = dispersion,  :
  models with response â€˜c(""data$dep0.75_0.25"", ""data$dep0.9_0.1"")â€™ removed because response differs from model 1
</code></pre>

<p>what should I do?</p>
"
"0.1570467354963","0.158655679740622","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.183391076651825","0.172918590828624","231059","<p>So first of all I did some research on this forum, and I know <a href=""http://stats.stackexchange.com/questions/140991/comparing-difference-between-two-polynomial-regression-models-in-r"">extremely similar</a>  questions have been asked but they usually haven't been answered properly or sometimes the answer are simply not detailed enough for me to understand. So this time my question is : I have two sets of data, on each, I do a polynomial regression like so :</p>

<pre><code>Ratio&lt;-(mydata2[,c(2)])
Time_in_days&lt;-(mydata2[,c(1)])
fit3IRC &lt;- lm( Ratio~(poly(Time_in_days,2)) )
</code></pre>

<p>The polynomial regressions plots are:</p>

<p><a href=""http://i.stack.imgur.com/T7r3i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7r3i.png"" alt=""enter image description here""></a></p>

<p>The coefficients are :</p>

<pre><code>&gt; as.vector(coef(fit3CN))
[1] -0.9751726 -4.0876782  0.6860041
&gt; as.vector(coef(fit3IRC))
[1] -1.1446297 -5.4449486  0.5883757 
</code></pre>

<p>And now I want to know, if there is a way to use an R function to do a test that would tell me whether or not there is a statistical significance in the difference between the two polynomials regression knowing that the relevant interval of days is [1,100].</p>

<p>From what I understood I can not apply directly the anova test because the values come from two different sets of data nor the AIC, which is used to compare model/true data.</p>

<p>I tried to follow the instructions given by @Roland in the related question but I probably misunderstood something when looking at my results :</p>

<p>Here is what I did : </p>

<p>I combined both my datasets into one.</p>

<p><code>f</code> is the variable factor that @Roland talked about. I put 1s for the first set and 0s for the other one.</p>

<pre><code>y&lt;-(mydata2[,c(2)])
x&lt;-(mydata2[,c(1)])
f&lt;-(mydata2[,c(3)])

plot(x,y, xlim=c(1,nrow(mydata2)),type='p')

fit3ANOVA &lt;- lm( y~(poly(x,2)) )

fit3ANOVACN &lt;- lm( y~f*(poly(x,2)) )
</code></pre>

<p>My data looks like this now :</p>

<p><a href=""http://i.stack.imgur.com/dNpMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dNpMQ.png"" alt=""enter image description here""></a></p>

<p>The red one is <code>fit3ANOVA</code> which is still working but I have a problem with the blue one <code>fit3ANOVACN</code> the model has weird results. I don't know if the fit model is correct, I do not understand what @Roland meant exactly.</p>

<p>Considering @DeltaIV solution I suppose that in that case :
<a href=""http://i.stack.imgur.com/HLLp9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLLp9.png"" alt=""enter image description here""></a>
The models are significantly different even though they overlap. Am I right to assume so ?</p>
"
"0.165729803333632","0.167427705632229","231435","<p>So basically I was trying to compare two models in R using the anova function, here is what my data looks like :</p>

<p><a href=""http://i.stack.imgur.com/gVvJD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gVvJD.png"" alt=""enter image description here""></a></p>

<p>I wanted to compare tose two plots (know if there was a statistical difference), in order to do so I was advised to compare two regression, one model where regression was applied to all the points I had and one model where in the regression model I took into account the fact variable.</p>

<p><code>log(y) ~ poly(x, 3) * fact</code>  vs <code>log(y) ~ poly(x, 3)</code> </p>

<p>(First I tried to use <code>y ~ poly(x,3)</code>, but the <code>log(y)</code> seemed graphically like a way better fit, this might be one of the mistakes I made)</p>

<p>Using the Anova test would then have told me whether the models where statistically different.</p>

<p>But when doing so, I had weird results. When I compared the two models, I had very low p-value (<code>&lt;2.2e-16</code>) even though, they intuitively didn't seem that different.</p>

<p>Just to make sure everything was working correctly, I wanted to know if the degree for the polynomial regression that used (3) was a good fit. So I did an ANOVA test on <code>log(y) ~ poly(x, n) * fact</code>  vs <code>log(y) ~ poly(x, n+1) * fact</code>. Until the values were not significant anymore, but it went on (I had p-values<code>&lt;2.2e-16</code>) till n=10. When I made the same algorithm on the regessions without the <code>* fact</code>, it only got through n=3, which graphically seemed way more logical. I do know that I shouldn't only trust the graphical aspect of things, but I'm afraid I may be overfitting my models, chosing the wrong model, using the <code>anova()</code> function the wrong way.</p>

<p>Here is the graphical aspect (regressions with fact taken into account) of things that made me wonder whether my results were accurate or not : (polynomial degrees are 6,9,10 in that order)</p>

<p><a href=""http://i.stack.imgur.com/4pTOy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4pTOy.png"" alt=""Degree 6""></a></p>

<p><a href=""http://i.stack.imgur.com/FcOqK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FcOqK.png"" alt=""Degree 9""></a></p>

<p><a href=""http://i.stack.imgur.com/eyf4H.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eyf4H.png"" alt=""Degree 10""></a></p>

<p>Maybe the <code>anova()</code> function in R isn't as accurate for models with grouping variables which is why when I compare the models with <code>* fact</code> to the ones without it I always get a p-value which is <code>&lt;2.2e-16</code>.</p>

<p>I also believe I may have chosen a wrong model by doing a polynomial regression on my functions which is why all my results would be useless.</p>

<p>I don't know if adding the actual data would be useful to anyone but if so I'll edit it in.</p>
"
"0.0473513723810378","0.047836487323494","232825","<p>I have three regression binomial models in R (GLMM) for which I have the following anova table.</p>

<p><a href=""http://i.stack.imgur.com/iA3qR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iA3qR.png"" alt=""anovaII""></a></p>

<p>Now, which is the model of the best fit based on the metrics in the table? Is it model 2 since it has the lowest BIC value? </p>

<p>I read somewhere that the best fitting model should have a lower residual deviance, AIC and BIC values than other models. The p-value threshold here is p&lt;0.5.  </p>

<p>In addition, how can I calculate the Chisq,Chi Df and p-value of model2 in R since these are not provided in the first row? </p>
"
"0.105880887471907","0.10696563746014","233619","<p>I have data from an EEG experiment. Originally, the design was balanced, but because in EEG you loose a lot of data to noise and malfunctioning equipment, the end-result is unbalanced data. </p>

<p>I am working with R. In R, my data looks like this:</p>

<pre><code>          Name                 Condition Channel EpochCountStd EpochCountDeviant ErpMinTime ErpMinVoltage ErpMinAv Frequency
380   AY11042016B1-Deci.EEG      HFMM     AF3           423               103        203          -1.2     -1.1      High
388 AvdP23052016B1-Deci.EEG      HFMM     AF3           410               101        144          -3.7     -3.2      High
397   EW20042016B1-Deci.EEG      HFMM     AF3           457               118        123          -2.6     -2.3      High
413  IG160312016B1-Deci.EEG      HFMM     AF3           435               105        214          -2.2     -1.6      High
422  IJB18042016B1-Deci.EEG      HFMM     AF3           408               110        121          -1.3     -1.1      High
439   MC31032016B1-Deci.EEG      HFMM     AF3           438               101        116          -4.0     -3.3      High
        Hemisphere  Region      Lexicality Subject
380       Left AnterioFrontal       Word Subject08
388       Left AnterioFrontal       Word Subject14
397       Left AnterioFrontal       Word Subject12
413       Left AnterioFrontal       Word Subject02
422       Left AnterioFrontal       Word Subject10
439       Left AnterioFrontal       Word Subject05
</code></pre>

<p>For present purposes, my dependent variable is <code>ErpMinAv</code>, a continuus numerical variable and my independent variables are <code>Lexicality</code> (2 levels), <code>Frequency</code>(2 levels), <code>Hemisphere</code>(3 levels) and <code>Region</code> (5 levels).</p>

<p>Moreover, I have tested for sphericity and the result is highly significant, meaning my data violate the sphericity assumptions. </p>

<p>I have already run anovas and regressions on my data, but I am always onsure of some things:</p>

<ol>
<li>Should I be using anova if I am violating the sphericity assumption? I understand most statistical programs include corrections in the tests themselves, but from what I have read on the internet, this is not necessarily the case in R. I have seen people recommending the use of <code>lm()</code> or <code>lme4()</code>. </li>
<li><p>How should I order my independent variables in the R syntax? So far, I tried:</p>

<pre><code>MM_Model &lt;- aov(ErpMinAv ~ Lexicality*Frequency*Hemisphere*Region 
                + Error(Subject),data = MM_Table)
</code></pre></li>
</ol>

<p>But I have seen different ways of ordering the factors (e.g. with sums instead of multiplication signs) and many different ways of writing the <code>Error()</code> term (e.g. like this: <code>Error(Subject/(v1*v2*v3*v4</code>). I have not, however, been able to find explanations for a case where there is no sphericity and the data is unbalanced. </p>
"
