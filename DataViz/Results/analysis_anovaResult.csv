"V1","V2","V3","V4"
"NaN","NaN","  1866","<p>Following to the recent questions we had <a href=""http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova/1823#1823"">here</a>.</p>

<p>I was hopping to know if anyone had come across or can share <strong>R code for performing a custom power analysis based on simulation for a linear model?</strong></p>

<p>Later I would obviously like to extend it to more complex models, but lm seems to right place to start. Thanks.</p>
"
"0.0821541921922785","0.107724577173552","  2104","<p>Thank you for reading. I am trying to get sphericity values for a purely within subject design. I have been unable to use ezANOVA, or Anova(). Anova works if I add a between subject factor, but I have been unable to get sphericity for a purely within subject design. Any advice?</p>

<p>I already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online.</p>

<p>My original ANOVA</p>

<pre><code>anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) 
anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))

&gt; str(subset(p12bl, exps==1))
'data.frame':   192 obs. of  12 variables:
 $ exps     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Order    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ threshold: Factor w/ 2 levels "" Suprathreshold"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ SET      : Factor w/ 2 levels "" A"","" B"": 1 1 1 1 1 1 1 1 1 1 ...
 $ subject  : Factor w/ 16 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ stim     : chr  ""S1C1"" ""S1C1"" ""S1C1"" ""S1C1"" ...
 $ resp     : num  6.01 5.63 0 2.57 6.81 ...
 $ id       : int  1 2 3 4 5 6 7 8 9 10 ...
 $ X1       : Factor w/ 1 level ""S"": 1 1 1 1 1 1 1 1 1 1 ...
 $ sucrose  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...
 $ X3       : Factor w/ 1 level ""C"": 1 1 1 1 1 1 1 1 1 1 ...
 $ citral   : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...

subset(p12b,exps==1)
   exps Order       threshold SET observ S1C1 S1C2 S1C3 S1C4 S2C1 S2C2 S2C3 S2C4 S3C1 S3C2 S3C3 S3C4 S4C1 S4C2 S4C3 S4C4
1     1     1  Suprathreshold   A      1  6.0  7.1  7.5  8.6 15.0 15.4 15.0 13.1 16.9   13 13.1 16.5   24   16   21   20
2     1     1  Suprathreshold   A      2  5.6  0.8  4.0  5.6  5.6 11.3 12.9 14.5 18.5   15 12.9 14.5   24   26   29   28
3     1     1  Suprathreshold   A      3  0.0  0.0  1.7  0.0  5.0  8.4  8.4  5.0 11.7   20 18.5 16.8   29   37   37   30
4     1     1  Suprathreshold   A      4  2.6  3.3  9.1 16.3  5.4 10.0  9.6 16.8 13.5   12 22.2 23.1   19   20   22   23
5     1     1  Suprathreshold   A      5  6.8  5.3 15.4 14.5 11.5  8.3 14.5 14.2  8.9   17 11.2 15.1   24   23   19   19
6     1     1  Suprathreshold   A      6  2.6  2.8  2.6  5.2 13.4 15.6 13.7 13.0 13.7   15 16.0 18.9   22   24   25   25
7     1     1  Suprathreshold   A      7  1.3  5.8 10.2  9.8 11.9 12.3 17.7 16.7 11.4   19 19.2 21.1   16   19   18   19
8     1     1  Suprathreshold   A      8  2.0  5.6  3.9  2.0  4.9  5.2  7.5  4.9 20.2   21  8.2  9.5   30   26   32   45
9     1     1  Suprathreshold   A      9  9.4 11.3 11.7 12.1 14.7 13.8 12.6 14.9 15.2   15 15.9 13.9   17   18   15   18
10    1     1  Suprathreshold   A     10  4.5 17.8 18.5 21.6  5.8 10.9 17.0 20.2  6.6   10 17.8 18.7   12   12   16   19
11    1     1  Suprathreshold   A     11  9.8 13.0 16.1 18.0 10.5 11.6 15.4 17.3 10.1   14 15.2 16.7   13   15   15   17
12    1     1  Suprathreshold   A     12  9.6 10.4 13.3 11.3 12.1 12.6 13.6 13.6 14.9   16 15.1 16.3   16   18   18   17
</code></pre>

<p>Sample output</p>

<pre><code>ezANOVA( data = subset(p12bl, exps==1)  , dv= .(resp), sid = .(observ), within = .(sucrose,citral), between = NULL, collapse_within = FALSE)
Note: model has only an intercept; equivalent type-III tests substituted.
$ANOVA
          Effect DFn DFd  SSn  SSd     F       p p&lt;.05   pes
1        sucrose   3  33 4953 3263 16.70 9.0e-07     * 0.603
2         citral   3  33  410  553  8.16 3.3e-04     * 0.426
3 sucrose:citral   9  99   56  791  0.77 6.4e-01       0.066
</code></pre>

<p>Warning messages:
1: You have removed one or more Ss from the analysis. Refactoring ""observ"" for ANOVA. 
2: Too few Ss for Anova(), reverting to aov(). See ""Warning"" section of the help on ezANOVA. </p>
"
"0.0642824346533225","0.0655590899062897","  2290","<p>This is a follow-up to the <a href=""http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova"">repeated measures sample size</a> question.</p>

<p>I am planning a repeated measures experiment. We record energy usage for 12 months, then give (a randomly assigned) half of the customers continuous information about their energy usage (perform the treatment), and record their energy usage for another 12 months. A similar study performed in the past showed a 5% reduction in energy usage.</p>

<p>I want to estimate the required sample size using $\alpha=0.05, \beta=0.1$. G*Power 3 has a tool for repeated measures power analysis. However, it requires two inputs that I am not entirely familiar with:</p>

<ul>
<li>$\lambda$ - the noncentrality parameter (How do I estimate this?)</li>
<li>$f$ - the effect size (I believe that this is the square root of Cohen's $f^2$)</li>
</ul>

<p>According to Wikipedia's effect size page:</p>

<blockquote>
  <p>Cohen's $f^2= {R^2_{AB} - R^2_A \over 1 - R^2_{AB}}$ where $R^2_A$ is the variance accounted for by a set of one or more independent variables $A$, and $R^2_{AB}$ is the combined variance accounted for by $A$ and another set of one or more independent variables $B$.</p>
</blockquote>

<p>However, my expected 5% change in energy consumption does not tell me how much variability will be explained. Is there any way to make this conversion?</p>

<p>If you know of a way to do this power analysis in R, I would love to hear it. I am planning to simulate some data and try using lmer from the lme4 package.</p>
"
"0.123091490979333","0.125536099672233","  3874","<p>I have data from patients treated with 2 different kinds of treatments during surgery.
I need to analyze its effect on heart rate. 
The heart rate measurement is taken every 15 minutes. </p>

<p>Given that the surgery length can be different for each patient, each patient can have between 7 and 10 heart rate measurements. 
So an unbalanced design should be used. 
I'm doing my analysis using R. And have been using the ez package to do repeated measure mixed effect ANOVA. But I do not know how to analyse unbalanced data. Can anyone help?</p>

<p>Suggestions on how to analyze the data are also welcomed.</p>

<p>Update:<br>
As suggested, I fitted the data using the <code>lmer</code> function and found that the best model is:</p>

<pre><code>heart.rate~ time + treatment + (1|id) + (0+time|id) + (0+treatment|time)
</code></pre>

<p>with the following result:</p>

<pre><code>Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 id       time        0.00037139 0.019271        
 id       (Intercept) 9.77814104 3.127002        
 time     treat0      0.09981062 0.315928        
          treat1      1.82667634 1.351546 -0.504 
 Residual             2.70163305 1.643665        
Number of obs: 378, groups: subj, 60; time, 9

Fixed effects:
             Estimate Std. Error t value
(Intercept) 72.786396   0.649285  112.10
time         0.040714   0.005378    7.57
treat1       2.209312   1.040471    2.12

Correlation of Fixed Effects:
       (Intr) time  
time   -0.302       
treat1 -0.575 -0.121
</code></pre>

<p>Now I'm lost at interpreting the result. 
Am I right in concluding that the two treatments differed in affecting heart rate? What does the correlation of -504 between treat0 and treat1 means?</p>
"
"0.0524863881081478","0.0535287727572189","  4544","<p>Please provide R code which allows one to conduct a between-subjects ANOVA with -3, -1, 1, 3 contrasts.  I understand there is a debate regarding the appropriate Sum of Squares (SS) type for such an analysis.  However, as the default type of SS used in SAS and SPSS (Type III) is considered the standard in my area.  Thus I would like the results of this analysis to match perfectly what is generated by those statistics programs.  To be accepted an answer must directly call aov(), but other answers may be voted up (espeically if they are easy to understand/use).</p>

<pre><code>sample.data &lt;- data.frame(IV=rep(1:4,each=20),DV=rep(c(-3,-3,1,3),each=20)+rnorm(80))
</code></pre>

<p><strong>Edit:</strong> Please note, the contrast I am requesting is not a simple linear or polynomial contrast but is a contrast derived by a theoretical prediction, i.e. the type of contrasts discussed by Rosenthal and Rosnow.</p>
"
"0.104972776216296","0.0936753523251331","  5817","<p>I want to know if a covariate for each subject interacts with three types of trials, and the difficulty of those trials. My dependent measures are accuracy and response times (RT). For this question, Iâ€™d like to focus on RTs. Traditionally, people in my field have dichotomized the covariate of interest and used ANOVAs for analysis. I would like to treat the covariate as the continuous variable it is, and treat the subjects as random effects. I want to analyze this using mixed-models in <code>R</code> (<code>nlme</code>).</p>

<p>The first 2 trial types can be either easy or hard and the third trial type is a combination of the first 2. These trials can be easy-easy, easy-hard, hard-easy, hard-hard.</p>

<p>I expect people who have higher scores on the covariate to show a smaller difference between hard and easy RTs for at least 1 trial type.</p>

<p>This is a repeated-measures design with each subject completing 3 blocks of 40 trials of each of the trial types (for trialtypes 1 &amp; 2: 20 easy, 20 hard; for trialtype3, 10 easy-easy, 10 easy-hard, 10 hard-hard, 10 hard-easy). Stated differently, each subject completes 3 blocks of 120 trials with the various trialtypes randomly ordered. </p>

<p>Only RTs for correct trials will be analyzed (resulting in an unbalanced design for RT data). Besides a counter-balancing of response keys, this is a completely within-subjects design.</p>

<p>To summarize, what is the model (or models) that will allow me to test for interactions between trialtypes, difficulty, and the covariate using <code>nlme</code> in <code>R</code>? </p>
"
"0.123521130997592","0.136472128413867","  6865","<p>How can I test effects in a Split-Plot ANOVA using suitable model comparisons for use with the <code>X</code> and <code>M</code> arguments of <code>anova.mlm()</code> in R? I'm familiar with <code>?anova.mlm</code> and Dalgaard (2007)[1]. Unfortunately it only brushes Split-Plot Designs. Doing this in a fully randomized design with two within-subjects factors:</p>

<pre><code>N  &lt;- 20  # 20 subjects total
P  &lt;- 3   # levels within-factor 1
Q  &lt;- 3   # levels within-factor 2
DV &lt;- matrix(rnorm(N* P*Q), ncol=P*Q)           # random data in wide format
id &lt;- expand.grid(IVw1=gl(P, 1), IVw2=gl(Q, 1)) # intra-subjects layout of data matrix

library(car)        # for Anova()
fitA &lt;- lm(DV ~ 1)  # between-subjects design: here no between factor
resA &lt;- Anova(fitA, idata=id, idesign=~IVw1*IVw2)
summary(resA, multivariate=FALSE, univariate=TRUE)  # all tests ...
</code></pre>

<p>The following model comparisons lead to the same results. The restricted model doesn't include the effect in question but all other effects of the same order or lower, the full model adds the effect in question.</p>

<pre><code>anova(fitA, idata=id, M=~IVw1 + IVw2, X=~IVw2, test=""Spherical"") # IVw1
anova(fitA, idata=id, M=~IVw1 + IVw2, X=~IVw1, test=""Spherical"") # IVw2
anova(fitA, idata=id, M=~IVw1 + IVw2 + IVw1:IVw2,
                      X=~IVw1 + IVw2, test=""Spherical"")          # IVw1:IVw2
</code></pre>

<p>A Split-Splot design with one within and one between-subjects factor:</p>

<pre><code>idB  &lt;- subset(id, IVw2==1, select=""IVw1"")          # use only first within factor
IVb  &lt;- gl(2, 10, labels=c(""A"", ""B""))               # between-subjects factor
fitB &lt;- lm(DV[ , 1:P] ~ IVb)                        # between-subjects design
resB &lt;- Anova(fitB, idata=idB, idesign=~IVw1)
summary(resB, multivariate=FALSE, univariate=TRUE)  # all tests ...
</code></pre>

<p>These are the <code>anova()</code> commands to replicate the tests, but I don't know why they work. <strong>Why do the tests of the following model comparisons lead to the same results?</strong></p>

<pre><code>anova(fitB, idata=idB, X=~1, test=""Spherical"") # IVw1, IVw1:IVb
anova(fitB, idata=idB, M=~1, test=""Spherical"") # IVb
</code></pre>

<p>Two within-subjects factors and one between-subjects factor:</p>

<pre><code>fitC &lt;- lm(DV ~ IVb)  # between-subjects design
resC &lt;- Anova(fitC, idata=id, idesign=~IVw1*IVw2)
summary(resC, multivariate=FALSE, univariate=TRUE)  # all tests ...
</code></pre>

<p><strong>How do I replicate the results given above with the corresponding model comparisons for use with the <code>X</code> and <code>M</code> arguments of <code>anova.mlm()</code>? What is the logic behind these model comparisons?</strong></p>

<p>EDIT: suncoolsu pointed out that for all practical purposes, data from these designs should be analyzed using mixed models. However, I'd still like to understand how to replicate the results of <code>summary(Anova())</code> with <code>anova.mlm(..., X=?, M=?)</code>.</p>

<p>[1]: <a href=""http://lib.stat.cmu.edu/R/CRAN/doc/Rnews/Rnews_2007-2.pdf"">Dalgaard, P. 2007. New Functions for Multivariate Analysis. R News, 7(2), 2-7.</a></p>
"
"0.161976510249954","0.15693371016017","  7675","<p>An experiment I conducted recently used a 2 (between participants) x 3 (within participants) design. That is, participants were randomly allocated to one of two conditions, and then completed three similar tasks each (in a counterbalanced order). </p>

<p>In each of these tasks, participants made binary choices (2AFC) in a number of trials, each of which had a normatively correct answer. <br />
In every trial, participants were presented a distractor, which was assumed to bias responses towards one of the alternatives. The tasks differed only in presence and magnitude of this distractor (i.e. no distractor vs. distractor of small and large magnitude).</p>

<p>I would like to examine the error rates (deviations from the normatively correct answer) across these conditions. I hypothesize that the error rate will increase when a distractor is present, but will not increase further when the magnitude of the distractor is increased. Also, I expect this increase to differ between the between-subjects conditions. The latter interaction is the central interest.</p>

<p>From <a href=""http://stats.stackexchange.com/questions/3874/unbalanced-mixed-effect-anova-for-repeated-measures"">discussions here</a> and from the literature (Dixon, 2008; Jaeger, 2008), I gather that logit mixed-models are the appropriate analysis method, and that, in R, the <a href=""http://lme4.r-forge.r-project.org/"" rel=""nofollow"">lme4 package</a> is the tool of choice. <br />
While I could compute some basic analyses (e.g. random intercept model, random effects ANCOVA) with lme4, I am stuck as to how to apply the models to the design in question -- I have the feeling that I am very much thinking in terms of HLMs, and have not yet quite understood the entirety of mixed effects models. Therefore, I would be very grateful for your help.</p>

<p>I have two basic questions:</p>

<ol>
<li><p>In a first analysis, I would like to consider the error rates in only those trials in which participants were biased towards the <em>wrong</em> answer. The first model would therefore look only at trials in which the bias would point away from the correct answer.</p>

<p>If my observations were independent, i would probably use a model like this:</p>

<p><code>correct ~ condition + distractor + condition:distractor</code></p>

<p>... but obviously, they aren't: Observations are grouped within a task (with a constant presence of a distractor) and within participants. My question, then, is this: How do I add the random effects to reflect this?</p></li>
<li><p>(If I haven't lost you already :-) ) Would it be possible to include all trials (those where the bias would be into the direction of the wrong and of the correct answer), and include this difference (i.e. direction of the bias) as a trial-level predictor?</p>

<p>In my imagination of HLMs, such a predictor (at the level of the trial) would depend on on the magnitude of the distractor present (at the level of the block), which again would depend on the condition of the participant (plus, possibly, a unique factor for each participant).<br />The interactions would then emerge â€ºautomaticallyâ€¹ as cross-level interactions. How would such a model be specified in the â€ºflatâ€¹ lme4 syntax? (Would such a model make sense at all?)</p></li>
</ol>

<p>Ok, I hope all of this makes sense -- I will gladly elaborate otherwise. Again, I would be most grateful for any ideas and comments regarding this analysis, and would like to thank you for taking the time and trouble to respond.</p>

<p><strong>References</strong></p>

<p>Dixon, P. (2008). Models of accuracy in repeated-measures designs. <em>Journal of Memory and Language, 59</em>(4), 447-456. doi: 10.1016/j.jml.2007.11.004</p>

<p>Jaeger T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. <em>Journal of Memory and Language, 59</em>(4), 434-446. doi: 10.1016/j.jml.2007.11.007</p>
"
"0.104972776216296","0.107057545514438","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.111901355435757","0.125536099672233"," 10095","<h3>Assumptions:</h3>

<p>In an ANOVA where the normality assumptions are violated, the Box-Cox transformation can be applied to the response variable. The <code>lambda</code> can be estimated by the using maximum likelihood to optimize the normality of the model residuals.</p>

<h3>Question:</h3>

<p>When the estimates for <code>lambda</code> in the null model and the full model differ, how should <code>lambda</code> be estimated?</p>

<h3>My Data:</h3>

<p>In my data the lambda estimate for the null model is <code>-2.3</code> and the lambda estimate for the full model is <code>-2.8</code>. Transforming the response using these different parameters and preforming the ANOVA leads to different F-statistics.</p>

<p>I have produced below a simplified version of the analysis using beta distributions with different parameters to simulate non-normality.  Unfortunately, in this example the results of the ANOVA are insensitive to the different estimates of <code>lambda</code>. So, it doesn't fully illustrate the problem.</p>

<pre><code>library(ggplot2)
library(MASS)
library(car)


#Generating random beta-distributed data
n=200
df &lt;- rbind(
  data.frame(x=factor(rep(""a1"",n)), y=rbeta(n,2,5)), # more left skewed
  data.frame(x=factor(rep(""a2"",n)), y=rbeta(n,2,2))) # less left skewed

print(qplot(data=df, color=x, x=y, geom=""density""))

print(""Untransformed Analaysis of Variance:"")
m.null &lt;- lm(y ~ 1, df)
m.full &lt;- lm(y ~ x, df)
print(anova(m.null, m.full))

# Estimate Maximum Liklihood Box-Cox transform parameters for both models
bc.null &lt;- boxcox(m.null); bc.null.opt &lt;- bc.null$x[which.max(bc.null$y)]
bc.full &lt;- boxcox(m.full); bc.full.opt &lt;- bc.full$x[which.max(bc.full$y)]

print(paste(""ML Box-Cox estimate for null model:"",bc.null.opt))
print(paste(""ML Box-Cox estimate for full model:"",bc.full.opt))

df$y.bc.null &lt;- bcPower(df$y, bc.null.opt)
df$y.bc.full &lt;- bcPower(df$y, bc.full.opt)

print(qplot(data=df, x=x, y=y.bc.null, geom=""boxplot""))
print(qplot(data=df, x=x, y=y.bc.full, geom=""boxplot""))


print(""Analysis of Variance with optimial Box-Cox transform for null model"")
m.bc_null.null &lt;- lm(y.bc.null ~ 1, data=df)
m.bc_null.full &lt;- lm(y.bc.null ~ x, data=df)
print(anova(m.bc_null.null, m.bc_null.full))

print(""Analysis of Variance with optimial Box-Cox transform for full model"")
m.bc_full.null &lt;- lm(y.bc.null ~ 1, data=df)
m.bc_full.full &lt;- lm(y.bc.null ~ x, data=df)
print(anova(m.bc_full.null, m.bc_full.full))
</code></pre>
"
"0.193818833050555","0.217434902816339"," 11079","<p>I need an help because I donÂ´t know if the command for the ANOVA analysis I am 
performing in R is correct. Indeed using the function aov I get the following error: <code>In aov (......) Error() model is singular</code></p>

<p>The structure of my table is the following: subject, stimulus, condition, sex, response</p>

<p>Example:</p>

<pre><code>subject  stimulus condition sex    response
subject1    gravel  EXP1    M      59.8060
subject2    gravel  EXP1    M      49.9880
subject3    gravel  EXP1    M      73.7420
subject4    gravel  EXP1    M      45.5190
subject5    gravel  EXP1    M      51.6770
subject6    gravel  EXP1    M      42.1760
subject7    gravel  EXP1    M      56.1110
subject8    gravel  EXP1    M      54.9500
subject9    gravel  EXP1    M      62.6920
subject10   gravel  EXP1    M      50.7270
subject1    gravel  EXP2    M      70.9270
subject2    gravel  EXP2    M      61.3200
subject3    gravel  EXP2    M      70.2930
subject4    gravel  EXP2    M      49.9880
subject5    gravel  EXP2    M      69.1670
subject6    gravel  EXP2    M      62.2700
subject7    gravel  EXP2    M      70.9270
subject8    gravel  EXP2    M      63.6770
subject9    gravel  EXP2    M      72.4400
subject10   gravel  EXP2    M      58.8560
subject11   gravel  EXP1    F      46.5750
subject12   gravel  EXP1    F      58.1520
subject13   gravel  EXP1    F      57.4490
subject14   gravel  EXP1    F      59.8770
subject15   gravel  EXP1    F      55.5480
subject16   gravel  EXP1    F      46.2230
subject17   gravel  EXP1    F      63.3260
subject18   gravel  EXP1    F      60.6860
subject19   gravel  EXP1    F      59.4900
subject20   gravel  EXP1    F      52.6630
subject11   gravel  EXP2    F      55.7240
subject12   gravel  EXP2    F      66.4220
subject13   gravel  EXP2    F      65.9300
subject14   gravel  EXP2    F      61.8120
subject15   gravel  EXP2    F      62.5160
subject16   gravel  EXP2    F      65.5780
subject17   gravel  EXP2    F      59.5600
subject18   gravel  EXP2    F      63.8180
subject19   gravel  EXP2    F      61.4250
.....
.....
.....
.....
</code></pre>

<p>As you can notice each subject repeated the evaluation in 2 conditions (EXP1 and EXP2).</p>

<p>What I am interested in is to know if there are significant differences between 
the evaluations of the males and the females.</p>

<p>This is the command I used to perform the ANOVA with repeated measures:</p>

<pre><code>aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)
summary(aov1)
</code></pre>

<p>I get the following error:</p>

<pre><code>&gt; aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)
Warning message:
In aov(response ~ stimulus * sex + Error(subject/(stimulus * sex)),  :
Error() model is singular
&gt; summary(aov1)

Error: subject
          Df  Sum Sq Mean Sq F value Pr(&gt;F)
sex        1  166.71  166.72   1.273  0.274
Residuals 18 2357.29  130.96               

Error: subject:stimulus
              Df Sum Sq Mean Sq F value Pr(&gt;F)    
stimulus       6 7547.9 1257.98 35.9633 &lt;2e-16 ***
stimulus:sex   6   94.2   15.70  0.4487 0.8445    
Residuals    108 3777.8   34.98                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Error: Within
           Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 420 9620.6  22.906               
&gt; 
</code></pre>

<p>The thing is that looking at the data it is evident for me that there is a 
difference between male and females, because for each stimulus I always get
a mean higher for the males rather than the females. 
Therefore the ANOVA should indicate significant differences....</p>

<p>Is there anyone who can suggest me where I am wrong?</p>

<p>Finally, I know that in R there are two libraries on linear mixed models called 
nlme and lme4, but I have never used it so far and I donÂ´t know if I have to utilize it for my case.
Is it the case to utilize it? If yes, could you please provide a quick R example
of a command which could solve my problem?</p>

<p>Thanks in advance!</p>

<p>Best regards</p>

<hr>

<p>Dear all, 
I am stuck now ;-( Indeed I understood everything you suggested me but still I donÂ´t get significance in the ANOVA results, and definitively there is an error, because results cannot be non-significant. Indeed looking at the means for each stimulus, it is possible to notice that males gave always higher evaluations than females.</p>

<p>To prove this I discarded for a moment the effect of the repeated measures, and I performed an ANOVA separately on both the two conditions (EXP1 and EXP2) during which the evaluations were given.
What I get is significant differences between males and female, in both EXP1 and EXP2.</p>

<p>Now, why when I perform the ANOVA with repeated measures I donÂ´t get the same behavior?</p>

<p>My design is the following:
-sex is a between-subjects factor (with two levels)
-stimulus is a within-subjects factor (with 3 assumed levels)
-condition is a within-subjects factor (with 2 levels)
-all factors are fully crossed</p>

<p>I tried, both the ways suggested but without achieving significance: </p>

<pre><code>mDf &lt;- aggregate(response ~ subject + sex, data=scrd, FUN=mean)
summary(aov(response ~ sex, data=mDf))     # ANOVA with just the between-effect
</code></pre>

<p>and</p>

<pre><code>aov1 = aov(response ~ sex*stimulus*condition + Error(subject/(stimulus*condition)), data=scrd)
summary(aov1)
</code></pre>

<p>Instead if I perform the ANOVA on the two subtables of EXP 1 and 2 I get significant differences. </p>

<pre><code>table_EXP1 &lt;- subset(scrd, condition == ""EXP1"")
table_EXP2 &lt;- subset(scrd, condition == ""EXP2"")


fit_table_EXP1 &lt;- lm(response ~ stimulus*sex, data=table_EXP1) 
summary(fit_table_EXP1 )
anova(fit_table_EXP1 )


fit_table_EXP2 &lt;- lm(response ~ stimulus*sex, data=table_EXP2) 
summary(fit_table_EXP2)
anova(fit_table_EXP2)
</code></pre>

<p>....how can this be possible?...it is a contraddiction....</p>

<p>HELP!</p>

<p>Please enlighten me!</p>

<p>Thanks in advance</p>

<p>Cheers</p>
"
"0.0909090909090909","0.092714554082312"," 11257","<h3>Context:</h3>

<p>I am analysing some impact assessment data (measuring invertebrate richness in response to pollution), but they are unbalanced - there are not data for every site at sampling occasion, and there were more datapoints recorded <em>after</em> the impact than <em>before</em> the impact. </p>

<p>I am a new user to R, and have gathered through reading on this site and others that the standard anova packages <code>aov()</code> and <code>ezAnova()</code> can't deal with unbalanced designs. I assume I should instead be using a package like <code>lme4</code>. </p>

<p>However, I am not sure how to structure my data, or program the analysis. One of the problems is that I'm not sure how to incorporate sampling dates as the repeated measures aspect of my design. </p>

<p>My data has 5 columns Site code,   Date,   BeforeAfter,   ControlImpact,   Richness. </p>

<h3>Questions:</h3>

<ul>
<li>How should I set up my data for conducting repeated measures analysis with unbalanced data in R?</li>
<li>Should I use <code>lme4</code> or some other package?</li>
</ul>
"
"0.0663906130309292","0.0677091369065533"," 11370","<p>I have data from an experiment that is testing how the order of two studying methods (visual or auditory) affects word recall. For analysis a multi-factor anova with a repeated measure is appropriate, but I am not sure if I am structuring my data correctly. </p>

<p>This is the command I'm using:
<code>aov(recalled_items~task*order)+Error(subject/task)+(order))</code></p>

<p>Here is an example of the data structure:</p>

<pre><code>Subject    Task    Order    Recalled Items
A        Visual    First       13
A       Auditory   Second      22
B        Visual    First       14
B       Auditory   Second      28
C        Visual    Second      10
C       Auditory   First       15
D        Visual    Second      14
D       Auditory   First       29
</code></pre>

<ul>
<li>Does R know to compare Visual 1 and Visual 2 recall values and Auditory 1 and Auditory 2 recall values? </li>
</ul>

<p>I am worried that because of the way I structured my data R is just comparing Visual 1 and Auditory 1 and as a result I am getting no effect.</p>
"
"NaN","NaN"," 12398","<p>I am new to statistics and I currently deal with ANOVA. I carry out an ANOVA test in R using</p>

<pre><code>aov(dependendVar ~ IndependendVar)
</code></pre>

<p>I get â€“ among others â€“ an F-value and a p-value. </p>

<p>My null hypothesis ($H_0$) is that all group means are equal. </p>

<p>There is a lot of information available on <a href=""http://onlinestatbook.com/2/analysis_of_variance/one-way.html"">how F is calculated</a>, but I don't know how to read an F-statistic and how F and p are connected. </p>

<p>So, my questions are:</p>

<ol>
<li>How do I determine the critical F-value for rejecting $H_0$?</li>
<li>Does each F have a corresponding p-value, so they both mean basically the same? (e.g., if $p&lt;0.05$, then $H_0$ is rejected) </li>
</ol>
"
"0.0829882662886615","0.0846364211331916"," 12643","<p>Here is an ANOVA model with one between-subject factor (condition; 4 levels) and one within-subject factor (trial_seq; 20 levels).</p>

<pre><code>amod = aov(decision_quality ~ condition*trial_seq +
       Error(user_id/trial_seq), data = d.task1)
summary(amod)
</code></pre>

<p>I want to do  a pairwise analysis for both the between-subject factor and the within-subject factor. I found that it is not straightforward to use a function like TukeyHSD() for a within-subject factor.  I researched this problem and found some suggestions in <a href=""http://stats.stackexchange.com/questions/575/post-hocs-for-within-subjects-tests"">another thread</a>.</p>

<p>However, my lack of statistical background prevented me from fully understanding vignettes coming with the multcomp package. Anyway, I tried some random statements like:</p>

<pre><code>summary(glht(amod, linfct = mcp(trial_seq = ""Tukey"")))
</code></pre>

<p>However, it generates the following error:</p>

<pre><code>Error in model.matrix.aovlist(model) : 
  â€˜glhtâ€™ does not support objects of class â€˜aovlistâ€™
Error in summary(glht(amod, linfct = mcp(trial_seq = ""Tukey""))) : 
  error in evaluating the argument 'object' in selecting a method for     
function 'summary': Error in factor_contrasts(model) : 
  no â€˜model.matrixâ€™ method for â€˜modelâ€™ found!
</code></pre>

<ul>
<li>What did I do wrong?</li>
</ul>
"
"0.0829882662886615","0.0677091369065533"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.111901355435757","0.125536099672233"," 14088","<p>I am trying to move from using the <code>ez</code> package to <code>lme</code> for repeated measures ANOVA (as I hope I will be able to use custom contrasts on with <code>lme</code>).</p>

<p>Following the advice from <a href=""http://blog.gribblelab.org/2009/03/09/repeated-measures-anova-using-r/"">this blog post</a> I was able to set up the same model using both <code>aov</code> (as does <code>ez</code>, when requested) and <code>lme</code>. However, whereas in the example given in <a href=""http://blog.gribblelab.org/2009/03/09/repeated-measures-anova-using-r/"">that post</a> the <em>F</em>-values do perfectly agree between <code>aov</code> and <code>lme</code> (I checked it, and they do), this is not the case for my data. Although the <em>F</em>-values are similar, they are not the same. </p>

<p><code>aov</code> returns a f-value of 1.3399, <code>lme</code> returns 1.36264. I am willing to accept the <code>aov</code> result as the ""correct"" one as this is also what SPSS returns (and this is what counts for my field/supervisor).</p>

<p>Questions:</p>

<ol>
<li><p>It would be great if someone could explain why this difference exists and how I can use <code>lme</code> to provide credible results. (I would also be willing to use <code>lmer</code> instead of <code>lme</code> for this type of stuff, if it gives the ""correct"" result. However, I haven't used it so far.)</p></li>
<li><p>After solving this problem I would like to run a contrast analysis. Especially I would be interested in the contrast of pooling the first two levels of factor (i.e., <code>c(""MP"", ""MT"")</code>) and compare this with the third level of factor (i.e., <code>""AC""</code>). Furthermore, testing the third versus the fourth level of factor (i.e., <code>""AC""</code> versus <code>""DA""</code>).</p></li>
</ol>

<p>Data:</p>

<pre><code>tau.base &lt;- structure(list(id = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 
22L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 
19L, 20L, 21L, 22L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L), .Label = c(""A18K"", 
""D21C"", ""F25E"", ""G25D"", ""H05M"", ""H07A"", ""H08H"", ""H25C"", ""H28E"", 
""H30D"", ""J10G"", ""J22J"", ""K20U"", ""M09M"", ""P20E"", ""P26G"", ""P28G"", 
""R03C"", ""U21S"", ""W08A"", ""W15V"", ""W18R""), class = ""factor""), factor = structure(c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""MP"", ""MT"", ""AC"", ""DA""
), class = ""factor""), value = c(0.9648092876, 0.2128662077, 1, 
0.0607615485, 0.9912814024, 3.22e-08, 0.8073856412, 0.1465590332, 
0.9981672618, 1, 1, 1, 0.9794401938, 0.6102546108, 0.428651501, 
1, 0.1710644881, 1, 0.7639763913, 1, 0.5298989196, 1, 1, 0.7162733447, 
0.7871177434, 1, 1, 1, 0.8560509327, 0.3096989662, 1, 8.51e-08, 
0.3278862311, 0.0953598576, 1, 1.38e-08, 1.07e-08, 0.545290432, 
0.1305621416, 2.61e-08, 1, 0.9834051136, 0.8044114935, 0.7938839461, 
0.9910112678, 2.58e-08, 0.5762677121, 0.4750002288, 1e-08, 0.8584252623, 
1, 1, 0.6020385797, 8.51e-08, 0.7964935271, 0.2238374288, 0.263377904, 
1, 1.07e-08, 0.3160751898, 5.8e-08, 0.3460325565, 0.6842217296, 
1.01e-08, 0.9438301877, 0.5578367224, 2.18e-08, 1, 0.9161424562, 
0.2924856039, 1e-08, 0.8672987992, 0.9266688748, 0.8356425464, 
0.9988463913, 0.2960361777, 0.0285680426, 0.0969063841, 0.6947998266, 
0.0138254805, 1, 0.3494775301, 1, 2.61e-08, 1.52e-08, 0.5393467752, 
1, 0.9069223275)), .Names = c(""id"", ""factor"", ""value""), class = ""data.frame"", row.names = c(1L, 
6L, 10L, 13L, 16L, 17L, 18L, 22L, 23L, 24L, 27L, 29L, 31L, 33L, 
42L, 43L, 44L, 45L, 54L, 56L, 58L, 61L, 64L, 69L, 73L, 76L, 79L, 
80L, 81L, 85L, 86L, 87L, 90L, 92L, 94L, 96L, 105L, 106L, 107L, 
108L, 117L, 119L, 121L, 124L, 127L, 132L, 136L, 139L, 142L, 143L, 
144L, 148L, 149L, 150L, 153L, 155L, 157L, 159L, 168L, 169L, 170L, 
171L, 180L, 182L, 184L, 187L, 190L, 195L, 199L, 202L, 205L, 206L, 
207L, 211L, 212L, 213L, 216L, 218L, 220L, 222L, 231L, 232L, 233L, 
234L, 243L, 245L, 247L, 250L))
</code></pre>

<p>And the code:</p>

<pre><code>require(nlme)

summary(aov(value ~ factor+Error(id/factor), data = tau.base))

anova(lme(value ~ factor, data = tau.base, random = ~1|id))
</code></pre>
"
"0.138865930150177","0.131507833509084"," 14978","<p>When running a repeated measures ANOVA in SPSS, it's possible to 'Save' the residuals as new variables in the data editor.</p>

<p>But the values output do not match the residuals given in R, and seem to be residuals for a between-subjects model. Unless I am missing something? Is SPSS giving the wrong residuals?</p>

<p>Example in R:</p>

<pre><code>set.seed(1)  # hopefully this keeps things the same every time!

  # create a data frame with each line representing one subject,
  # and create first and second observations for some experiment

DF &lt;- data.frame(participant=factor(1:5), first=rnorm(5, 10, 5), second=rnorm(5, 20, 5))

DF
</code></pre>

<p>-</p>

<pre><code>  participant     first   second
1           1  6.867731 15.89766
2           2 10.918217 22.43715
3           3  5.821857 23.69162
4           4 17.976404 22.87891
5           5 11.647539 18.47306
</code></pre>

<p>-</p>

<pre><code>  # reshape it for an ANOVA in R
DFlong &lt;- reshape(DF, direction=""long"", varying=c(""first"", ""second""), v.names=""value"", idvar=""participant"", times=c(1, 2), timevar=""group"")

DFlong
</code></pre>

<p>-</p>

<pre><code>    participant group     value
1.1           1     1  6.867731
2.1           2     1 10.918217
3.1           3     1  5.821857
4.1           4     1 17.976404
5.1           5     1 11.647539
1.2           1     2 15.897658
2.2           2     2 22.437145
3.2           3     2 23.691624
4.2           4     2 22.878907
5.2           5     2 18.473058
</code></pre>

<p>-</p>

<pre><code>my.aov &lt;- aov(value ~ group + Error( participant / group ), DFlong)
summary(my.aov)
</code></pre>

<p>-</p>

<pre><code>Error: participant
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  4 86.474  21.619               

Error: participant:group
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
group      1 251.469 251.469  19.871 0.01118 *
Residuals  4  50.619  12.655                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>-</p>

<pre><code>my.aov$""participant:group""$residuals
</code></pre>

<p>-</p>

<pre><code>        6          7          8          9         10 
0.7066837 -1.0533061 -5.5440267  3.6252135 -2.2654355 
</code></pre>

<p>-</p>

<pre><code># import into SPSS:
write.table(DF, ""C:/test.txt"", row.names=FALSE)
</code></pre>

<p>Then load SPSS, and run:</p>

<pre><code>GET DATA  /TYPE = TXT
 /FILE = 'C:\test.txt'
 /DELCASE = LINE
 /DELIMITERS = "" ""
 /QUALIFIER = '""'
 /ARRANGEMENT = DELIMITED
 /FIRSTCASE = 2
 /IMPORTCASE = ALL
 /VARIABLES =
 participant F1.0
 first F16.14
 second F16.13
 .
CACHE.
EXECUTE.
DATASET NAME DataSet1 WINDOW=FRONT.
</code></pre>

<p>Now change the variable types to scale (in the 'variables' tab - I don't know the syntax for this). Then run:</p>

<pre><code>GLM
  first second
  /WSFACTOR = factor1 2 Polynomial
  /METHOD = SSTYPE(3)
  /SAVE = RESID
  /CRITERIA = ALPHA(.05)
  /WSDESIGN = factor1 .
</code></pre>

<p>Or, do the above SPSS commands using the GUI: File->Read text data... find C:\test.txt, import it, remember to specify that the file has variable names as the first case, and run:</p>

<ol>
<li><p>Analyze->General Linear Model->Repeated Measures...</p></li>
<li><p>Set number of levels to 2</p></li>
<li><p>Put variables into analysis, 'first' and 'second'.</p></li>
<li><p>Open 'Save...' dialog box, check 'Residuals->Unstandardized'</p></li>
<li><p>Run analysis, SPSS creates two variables of residuals:</p>

<pre><code>RES_1    RES_2
-3.78    -4.78
  .27     1.76
-4.82     3.02
 7.33     2.20
 1.00    -2.20
</code></pre></li>
</ol>

<p>Note these values are different to R. So has SPSS got it wrong?</p>
"
"0.0428549564355483","0.0437060599375265"," 16644","<p>This is a second question related to previous post:
<a href=""http://stats.stackexchange.com/questions/16641/is-this-mixed-anova-correctly-specified-in-r"">Is this mixed ANOVA correctly specified in R?</a></p>

<p>To reiterate, the setup is as follows:</p>

<ul>
<li>Groups: two groups of 8 subjects (16 total)</li>
<li>Two conditions: alert and passive</li>
<li>Measurements: responses for three different stimuli (A, B, and C) measured in each condition</li>
</ul>

<p>Experiment: Testing the order of conditions</p>

<ul>
<li>Group one: Alert A, B and C followed by Passive A, B, and C </li>
<li>Group two: Passive A, B, and C followed by Alert A, B, and C</li>
</ul>

<h3>New Question:</h3>

<ul>
<li>If I wanted to use the responses measured in the passive condition as a co-variate to the alert responses, how would I perform an ANCOVA analysis in R?</li>
</ul>

<p>I am not sure how to handle this considering the repeated factors and between subject measures.</p>
"
"0.0663906130309292","0.0677091369065533"," 17997","<p>I need to fit orthogonal contrasts to the following model. The following example data. </p>

<pre><code>   rep &lt;- c(rep( 1, 15), rep(2, 15))
    parent1 &lt;- c(1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4)
    parent2 &lt;- c(1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5,1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5)
    yld &lt;- c(10, 12, 13, 14, 15, 12, 13, 14, 15, 13, 14, 15, 14, 15, 15,11, 13, 12, 13, 15, 14, 12, 13, 15, 12, 15, 15, 13, 15, 16)
    mydf &lt;- data.frame(rep, parent1, parent2, yld)
    mydf$parent1 &lt;- as.factor(mydf$parent1)
    mydf$parent2 &lt;- as.factor(mydf$parent2)
</code></pre>

<p>Parent1 and Parent2 are factors. </p>

<pre><code>    mode11 &lt;- lm(yld ~ rep + parent1:parent2, data = mydf)
    anova(mode11) 
</code></pre>

<p>I need to fit contrasts, I suppose the order 
of the treatments for the coefficients for the interaction is:</p>

<pre><code>Parent1    1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4    
Parent2    1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5 

# contrasts for Parent1:Parent2 interaction 
mycontrast &lt;- c(-4, -4, -4, -4, 4,  2, 2, 2, 2, 2,  2, 2, 2, 2,2)
</code></pre>

<p>I want to apply above orthogonal contrasts (mycontrasts). I am used to SAS to this short of analysis however not in R.  I have two questions:</p>

<p>(a) How can I properly apply contrast to above model? </p>

<p>(b) I am concerned what is order of the treatments ( see my assumed orders, are they right?) , while writing coefficients - I assume this should be either alphabetical or  order in file. </p>
"
"0.117851130197758","0.109265149843816"," 18006","<p>I need to fit orthogonal contrasts to the following model. See the following example data:</p>

<pre><code>   rep &lt;- c(rep( 1, 15), rep(2, 15))
    parent1 &lt;- c(1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4)
    parent2 &lt;- c(1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5,1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5)
    yld &lt;- c(10, 12, 13, 14, 15, 12, 13, 14, 15, 13, 14, 15, 14, 15, 15,11, 13, 12, 13, 15, 14, 12, 13, 15, 12, 15, 15, 13, 15, 16)
    mydf &lt;- data.frame(rep, parent1, parent2, yld)
    mydf$parent1 &lt;- as.factor(mydf$parent1)
    mydf$parent2 &lt;- as.factor(mydf$parent2)
</code></pre>

<p>Parent1 and Parent2 are factors. </p>

<pre><code>    mode11 &lt;- lm(yld ~ rep + parent1:parent2, data = mydf)
    anova(mode11) 
</code></pre>

<p>I need to fit contrasts, I suppose the order 
of the treatments for the coefficients for the interaction is:</p>

<pre><code>Parent1    1, 2, 3, 4, 5, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4    
Parent2    1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5 

# contrasts for Parent1:Parent2 interaction 
mycontrast &lt;- c(-4, -4, -4, -4, 4,  2, 2, 2, 2, 2,  2, 2, 2, 2,2)
</code></pre>

<p>I want to apply above orthogonal contrasts (mycontrasts). 
I am used to SAS to this short of analysis however not in R.<br>
I have two questions:</p>

<p>(a) How can I properly apply contrast to above model? </p>

<p>(b) I am concerned about the order of the treatments while writing coefficients. 
See my assumed orders: are they correct? I assume this should be either alphabetical or ordered in file.</p>

<p>Edits: The following my own progress to answer the question </p>

<pre><code>&gt;     mode11$coefficients # to see all coefficients 

&gt;     mode11$coefficients[1:5] # just first five to save space here 
     (Intercept)               rep parent11:parent21 parent1**2**:parent2**1** parent1**3**:parent2**1** 
     1.500000e+01      1.087998e-15     -4.500000e+00                NA                NA 
</code></pre>

<p>Should orthogonal contrast ordered in the way the comparisions are ordered here - 1-2, 2-1, 3-1 and so on. As the design is unbalanced you can see some combinations are missing, so should we provide a value like 0 for them. </p>

<pre><code>  contrasts(parent1:parent2) &lt;- cbind(c(-4, -4, -4, -4, 4,  2, 2, 2, 2, 2,
</code></pre>

<p>2, 2, 2, 2,2,-4, -4, -4, -4, 4, 2, 2, 2, 2, 2,  2, 2, 2, 2,2 ))
    Error in <code>contrasts&lt;-</code>(<code>*tmp*</code>, value = c(-4, -4, -4, -4, 4, 2, 2, 2,  : 
      contrasts apply only to factors
    In addition: Warning messages:
    1: In <code>*tmp*</code>:parent2 :
      numerical expression has 30 elements: only the first used
    2: In <code>*tmp*</code>:parent2 :
      numerical expression has 30 elements: only the first used</p>
"
"0.0556702214268904","0.0567758373078348"," 18084","<p>There's a lot about collinearity with respect to continuous predictors but not so much that I can find on categorical predictors. I have data of this type illustrated below.   </p>

<p>The first factor is a genetic variable (allele count), the second factor is a disease category. Clearly the genes precede the disease and are a factor in showing symptoms that lead to a diagnosis. However, a regular analysis using type II or III sums of squares, as would be commonly done in psych with SPSS, misses the effect. A type I sums of squares analysis picks it up, when the appropriate order is entered because it is order dependent. Further, there are likely to be extra components to the disease process which are not related to the gene that are not well identified with type II or III, see <strong>anova(lm1)</strong> below vs lm2 or Anova.</p>

<p><em>Example data:</em>  </p>

<pre><code>set.seed(69)
iv1 &lt;- sample(c(0,1,2), 150, replace=T)
iv2 &lt;- round(iv1 + rnorm(150, 0, 1), 0)
iv2 &lt;- ifelse(iv2&lt;0, 0, iv2)
iv2 &lt;- ifelse(iv2&gt;2, 2, iv2)
dv  &lt;- iv2 + rnorm(150, 0, 2)
iv2 &lt;- factor(iv2, labels=c(""a"", ""b"", ""c""))
df1 &lt;- data.frame(dv, iv1, iv2)

library(car)
chisq.test(table(iv1, iv2))          # quick gene &amp; disease relations
lm1 &lt;- lm(dv~iv1*iv2, df1);    lm2 &lt;- lm(dv~iv2*iv1, df1)
anova(lm1);                    anova(lm2)
Anova(lm1, type=""II"");         Anova(lm2, type=""II"")
</code></pre>

<ol>
<li><strong>lm1</strong> with type I SS to me seems the appropriate way to analyse the data given the background theory. Is my assumption correct?  </li>
<li>I'm used to explicitly manipulated orthogonal designs, where these problems don't usually pop up. Is it difficult to convince reviewers that this is the best process (assuming point 1 is correct) in the context of an SPSS centric field?  </li>
<li>And what to report in the stats section? Any extra analysis, or comments that should go in?</li>
</ol>
"
"0.0371134809512603","0.0378505582052232"," 18404","<p>I want to regress two series (one big series divided in half) with the mean of the big series. 
I do that because I would like to ""investigate"" the relationship between those two subseries and the mean.</p>

<p>Does this make any sense for you?</p>

<p>When running the code below, I don't understand why I don't get p-value:</p>

<pre><code>&gt; x  = rnorm(200)
&gt; m  = mean(x) 
&gt; anova(lm(rep(m, 100) ~ x[1:100]), lm(rep(m, 100) ~ x[101:200])) 
Analysis of Variance Table

Model 1: rep(m, 100) ~ x[1:100]
Model 2: rep(m, 100) ~ x[101:200]
  Res.Df RSS Df Sum of Sq F Pr(&gt;F)
1     98   0                      
2     98   0  0         0   
</code></pre>
"
"0.0524863881081478","0.0535287727572189"," 18738","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/12398/how-to-interpret-f-and-p-value-in-anova"">How to interpret F- and p-value in ANOVA?</a>  </p>
</blockquote>



<p>I found that I can use ANOVA also for ONE Model, doing something like:</p>

<pre><code>&gt; anova(lm(a~b))
Analysis of Variance Table

Response: a
           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    
b           1 0.002679 0.0026791  11.191 0.0009001 ***
Residuals 398 0.095282 0.0002394                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I know that ANOVA check the means BUT what test is that if I use only ONE model?
If the p.value is above 0.05 it means that the regression fit good?</p>
"
"0.139175553567226","0.132476953718281"," 18909","<p>I have an ordinal variable related to an outcome that is comprised of many levels and IÂ´d like to collapse the number of ordinal values as much as possible. </p>

<pre><code>&gt; require(ipred)
&gt; require(party)
&gt; data(GBSG2)
&gt; head(GBSG2)
  horTh age menostat tsize tgrade pnodes progrec estrec time cens
1    no  70     Post    21     II      3      48     66 1814    1
2   yes  56     Post    12     II      7      61     77 2018    1
3   yes  58     Post    35     II      9      52    271  712    1
4   yes  59     Post    17     II      4      60     29 1807    1
5    no  73     Post    35     II      1      26     65  772    1
6    no  32      Pre    57    III     24       0     13  448    1
&gt; table(GBSG2$tgrade)

  I  II III 
 81 444 161 
&gt; ctree(Surv(time,cens)~tgrade,data=GBSG2) -&gt; mn
&gt; plot(mn)
</code></pre>

<p><img src=""http://i.stack.imgur.com/WYIUd.png"" alt=""enter image description here""></p>

<p>Would it be correct to claim that <code>tgrade</code> here could be collapsed into two instead of three values?</p>

<p>edit:</p>

<p>Running the usual parametric analysis I get:</p>

<pre><code>&gt;     anova(i1,i2)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade == ""I""
   loglik  Chisq Df P(&gt;|Chi|)  
1 -1776.0                      
2 -1778.1 4.3049  1     0.038 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt;     anova(i1,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade
 Model 2: ~ tgrade != ""III""
  loglik  Chisq Df P(&gt;|Chi|)    
1  -1776                        
2  -1784 16.033  1 6.225e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; anova(i2,i3)
Analysis of Deviance Table
 Cox model: response is  Surv(time, cens)
 Model 1: ~ tgrade == ""I""
 Model 2: ~ tgrade != ""III""
   loglik  Chisq Df P(&gt;|Chi|)    
1 -1778.1                        
2 -1784.0 11.728  0 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; extractAIC(i1)
[1]    2.000 3555.975
&gt; extractAIC(i2)
[1]    1.00 3558.28
&gt;   extractAIC(i3)
[1]    1.000 3570.008  
</code></pre>

<p>Hence the i1 model provides a better fit than i2 and i3, and i2 fits significantly better than i3. So now all three categories are warranted with respect to survival, which is at odds with the ctree approach. Can anyone explain this? Is this due to the conditional nature of ctree instead of the semiparametric nature of cox regression?</p>
"
"0","0"," 19361","<p>Here is a sample output:</p>

<pre><code>anova(fit1,fit2);
Quantile Regression Analysis of Deviance Table

Model: op ~ inp1 + inp2 + inp3 + inp4 + inp5 + inp6 + inp7 + inp8 + inp9
Joint Test of Equality of Slopes: tau in {  0.15 0.3  }

  Df Resid Df F value Pr(&gt;F)
1  9     1337  0.5256 0.8568

Warning messages:
1: In summary.rq(x, se = ""nid"", covariance = TRUE) : 93 non-positive fis
2: In summary.rq(x, se = ""nid"", covariance = TRUE) : 138 non-positive fis
</code></pre>

<p>How to interpret the above results??
Does the <code>anova()</code> function give the best model, for <code>tau=0.15</code> vs. <code>tau=0.3</code>?</p>
"
"0.0829882662886615","0.0846364211331916"," 19954","<p>Edit: I think the data was poorly given, varables coke, pepsi, and sprite were just given in 2 columns when I got it, but it has nothing to do with before and after treatment?</p>

<p>here's the original questions since someone asked
For the study, 2 males and 2 females are picked randomly from NYC, and a random sample of 2 males and 2 females are picked from LA.  Each person is given 6 drinks (2 from each brand), where the order of the drinks for each person is randomized.  Each person rates the drinkâ€™s taste, Y,  on a scale of  60 to 100, where 100 is the best tasted. </p>

<p>Please let me know if I can provide any other info.  Pretty much I am asked to fit ANOVA.
End of Edit#</p>

<p>I would like to understand ANOVA in R, I am working in this data set.</p>

<pre><code>dat &lt;- structure(list(city = structure(c(2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L), .Label = c(""LA"", ""NY""), class = ""factor""), sex = structure(c(2L, 
2L, 1L, 1L, 2L, 2L, 1L, 1L), .Label = c(""F"", ""M""), class = ""factor""), 
    rater = 1:8, frit0 = c(77L, 72L, 78L, 84L, 77L, 78L, 83L, 
    81L), frit1 = c(76L, 71L, 78L, 84L, 76L, 77L, 86L, 84L), 
    utz0 = c(78L, 73L, 78L, 84L, 74L, 81L, 85L, 83L), utz1 = c(80L, 
    75L, 80L, 86L, 72L, 73L, 81L, 79L), weiss0 = c(78L, 76L, 
    84L, 90L, 78L, 79L, 88L, 86L), weiss1 = c(81L, 73L, 81L, 
    87L, 81L, 82L, 92L, 90L)), .Names = c(""city"", ""sex"", ""rater"", 
""coke0"", ""coke1"", ""pepsi0"", ""pepsi1"", ""sprite0"", ""sprite1""), 
 class = ""data.frame"", row.names = c(NA, -8L))
</code></pre>

<p>I want to do an analysis of variance, check assumptions, and interpret results.  I am sure this is done many times by the experts, so, what is the correct step to do it?</p>

<p>I have tried </p>

<pre><code>am1 &lt;- aov(coke1 ~ coke0 + sex, data=dat2)
summary(am1)
</code></pre>

<p>Is this correct? Or even relevant? thanks in advance!</p>
"
"0.123091490979333","0.114123726974758"," 20026","<p>I'm familiar with post-hoc testing with <code>ANOVA</code> for exploring differences between a sequence of groups, but recently I've been reading about Change Point Analysis (especially the <code>R</code> packages <code>bcp</code>, <code>changepoint</code> and <code>strucchange</code>). </p>

<p>It looks like those packages only handle data where there is one data point per unit of time. I'm curious if they can be used with data where there are multiple data points per unit of time. Here's some example data representing the measurement of a single continuous variable on a number of specimens that have been dated to specific moments in time (no repeated measurements):</p>

<pre><code>a&lt;-data.frame(time=""1000"",x=rnorm(10,12,3))
b&lt;-data.frame(time=""2000"",x=rnorm(50,13,4))
c&lt;-data.frame(time=""3500"",x=rnorm(50,12,4))
d&lt;-data.frame(time=""5000"",x=rnorm(7,14,5))
e&lt;-data.frame(time=""7000"",x=rnorm(20,10,3))
f&lt;-data.frame(time=""7500"",x=rnorm(15,11,3))
g&lt;-data.frame(time=""9000"",x=rnorm(15,10,5))
h&lt;-data.frame(time=""9500"",x=rnorm(35,30,2))
i&lt;-data.frame(time=""10000"",x=rnorm(30,28,4))
a2i&lt;-rbind(a,b,c,d,e,f,g,h,i) 

library(ggplot2)
a2i$time&lt;-as.numeric(levels(a2i$time))[a2i$time] 
ggplot(a2i,aes(time,x))+stat_smooth()+geom_point()
</code></pre>

<p><img src=""http://i.stack.imgur.com/cASXS.png"" alt=""enter image description here""></p>

<p>Here's what I'd be most grateful for some advice on...</p>

<p>Q1. Would it be valid to do the Change Point Analysis on a vector like the means or medians of the groups? That would allow me to start with a 'one data point per unit of time' input format which would suit the <code>R</code> packages, as I understand them. I've seen it done with environmental data like monthly gas concentrations (from daily observations), but I thought I'd check.  </p>

<p>Q2. Is there a kind of Change Point Analysis that I can do on the raw data in <code>a2g</code> that will give me some measures of the probabilities of changes across the sequence? For example, something that will detect the change from time=9000 to time=9500, using all the data points in the sample? I'm guessing that if it was possible, someone would already have implemented and I just need a pointer to the relevant function.</p>

<p>Q3. In case Q2 can be answered 'yes', would the method change if the distribution of each group's values was non-normal (unlike my sample data)?</p>

<p>Q4. If Change Point Analysis is completely the wrong approach here, please let me know. I'm basically just curious about methods other than <code>ANOVA</code> for these kinds of data. Any other suggestions would be most welcome.</p>
"
"0.0757575757575758","0.0772621284019266"," 20305","<p>After finding a significant treatment in two-way anova, how do I report where the differences are? Every text I have read has left me at: Ah, the results are significant, so let's move on.  In one-way anova I could use Tukey's HSD to find the means that differed, making my report much more effective. Right now I don't know how to do that with two-factor anova.</p>

<p>Here is an example from a text done in R:</p>

<p>Is there a difference in Friday tardy rates at different plants?</p>

<pre><code>&gt; mlate
        day  plant absences
1   march-4 plant1       19
2  march-11 plant1       22
3  march-18 plant1       20
4   march-4 plant2       18
5  march-11 plant2       20
6  march-18 plant2       16
7   march-4 plant3       27
8  march-11 plant3       32
9  march-18 plant3       28
10  march-4 plant4       22
11 march-11 plant4       27
12 march-18 plant4       26

&gt; anova(lm(absences ~ plant+day, data=mlate))
Analysis of Variance Table

Response: absences
          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
plant      3 216.250  72.083  41.191 0.0002134 ***
day        2  30.167  15.083   8.619 0.0172128 *  
Residuals  6  10.500   1.750                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>So I can see that both the plant and the day have an effect on worker's being late, but how do I compare the means like I would with a tukey's hsd in a one-way test?</p>
"
"0.154773914683409","0.165740126283802"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.0371134809512603","0.0378505582052232"," 20726","<p>In this paper</p>

<blockquote>
  <p>Anderson, M.J. (2001). <a href=""http://esa.hhog.net/PDF/MUVE/6_NewMethod_MANOVA1_2.pdf"">A new method for non-parametric multivariate analysis of variance</a>. <em>Austral Ecology</em>, <strong>26</strong>, 32â€“46</p>
</blockquote>

<p>I found very useful MANOVA approach that can, to some extent, cope with non-normality. I was wondering if it is possible and plausible to include Mahalanobis distances as  similarity method. In fact my question is about R <a href=""http://cran.r-project.org/web/packages/vegan/index.html"">vegan</a> package function <code>adonis</code> that implements this type of MANOVA but does not allow Mahalanobis distances as similarity measure. It allows though through <code>designdist</code> function the creation of any distance measure to be used in adonis, but I totally can't deal with that syntax.</p>

<p>Any experiences regarding this?</p>
"
"0.123521130997592","0.136472128413867"," 21112","<p>I am trying to reproduce the anova table for a 2x2 cross-over design. I used the data listed in tables 2.1 and 2.2 of the ""Design and Analysis of cross-over trials"" book, by Jones and Kenward. I am giving all the code so that you can reproduce the calculations.     </p>

<pre><code>g1AB &lt;- read.table(textConnection(""
Label Per1 Per2
7 121.905 116.667
8 218.5 200.5
9 235 217.143
13 250 196.429
14 186.19 185.5
15 231.563 221.842
17 443.25 420.5
21 198.421 207.692
22 270.5 213.158
28 360.476 384
35 229.75 188.25
36 159.091 221.905
37 255.882 253.571
38 279.048 267.619
41 160.556 163
44 172.105 182.381
58 267 313
66 230.75 211.111
71 271.19 257.619
76 276.25 222.105
79 398.75 404
80 67.778 70.278
81 195 223.158
82 325 306.667
86 368.077 362.5
89 228.947 227.895
90 236.667 220
"")-&gt;con,header=T)
close(con)

g2BA &lt;- read.table(textConnection(""
Label Per1 Per2
3 138.333 138.571
10 225 256.25
11 392.857 381.429
16 190 233.333
18 191.429 228
23 226.19 267.143
24 201.905 193.5
26 134.286 128.947
27 238 248.5
29 159.5 140
30 232.75 276.563
32 172.308 170
33 266 305
39 171.333 186.333
43 194.737 191.429
47 200 222.619
51 146.667 183.81
52 208 241.667
55 208.75 218.81
59 271.429 225
68 143.81 188.5
70 104.444 135.238
74 145.238 152.857
77 215.385 240.476
78 306 288.333
83 160.526 150.476
84 353.81 369.048
85 293.889 308.095
99 371.190 404.762
"")-&gt;con,header=T)
close(con)

n1 &lt;- nrow(g1AB)
n2 &lt;- nrow(g2BA)
p &lt;- 2 # periods
</code></pre>

<p>Some quantities that may be useful:</p>

<pre><code>y11. &lt;- sum(g1AB$Per1)
y12. &lt;- sum(g1AB$Per2)
y21. &lt;- sum(g2BA$Per1)
y22. &lt;- sum(g2BA$Per2)
y1.. &lt;- sum(c(g1AB$Per1,g1AB$Per2))
y2.. &lt;- sum(c(g2BA$Per1,g2BA$Per2))
y... &lt;- y1.. + y2..

y11.bar &lt;- 1/n1 * y11.
y12.bar &lt;- 1/n1 * y12.
y21.bar &lt;- 1/n2 * y21.
y22.bar &lt;- 1/n2 * y22.

y1..bar &lt;- 1/(p*n1)*(y11.+y12.)
y2..bar &lt;- 1/(p*n2)*(y21.+y22.)

y...bar &lt;- 1/(p*(n1+n2)) * (y1.. + y2..)
</code></pre>

<p>In order to perform the analysis of variance in R, I created the following dataset</p>

<pre><code>mydata1 &lt;- data.frame(PEFR=g1AB$Per1,Subjects=g1AB$Label,Time=1,Groups=""AB"",Treatment=1)
mydata2 &lt;- data.frame(PEFR=g2BA$Per1,Subjects=g2BA$Label,Time=1,Groups=""BA"",Treatment=2)
mydata3 &lt;- data.frame(PEFR=g1AB$Per2,Subjects=g1AB$Label,Time=2,Groups=""AB"",Treatment=2)
mydata4 &lt;- data.frame(PEFR=g2BA$Per2,Subjects=g2BA$Label,Time=2,Groups=""BA"",Treatment=1)
mydata &lt;- rbind(mydata1,mydata2,mydata3,mydata4)
mydata$Subjects&lt;-factor(mydata$Subjects)
</code></pre>

<p>(I have read that) The correct anova table is obtained using the following command</p>

<pre><code>res &lt;- summary(PEFR.aov &lt;- aov(PEFR~Groups+Time+Treatment+Error(Subjects), data=mydata))

Error: Subjects
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Groups     1  10573   10573   0.899  0.347
Residuals 54 634866   11757               

Error: Within
          Df Sum Sq Mean Sq F value  Pr(&gt;F)   
Time       1    480   479.6   1.470 0.23061   
Treatment  1   3026  3026.1   9.276 0.00359 **
Residuals 54  17617   326.2 
</code></pre>

<p>1) Can you please explain to me the use of the Error term in the formula?</p>

<p>2) The anova table is the same to that of table 2.10 of the book, except the SS for Time (Period) which has a value of 396.858 instead of 480. According to Table 2.8 this quantity is calculated using</p>

<pre><code>&gt; n1*n2/(2*(n1+n2))*(y11.bar-y12.bar+y21.bar-y22.bar)^2
[1] 396.8583
</code></pre>

<p>Also, the Treatment SS is calculated by</p>

<pre><code>n1*n2/(2*(n1+n2))*(y11.bar-y12.bar-y21.bar+y22.bar)^2
[1] 3026.12
</code></pre>

<p>If the formula was specified like this</p>

<pre><code>PEFR~Groups+Treatment+Time+Error(Subjects)
</code></pre>

<p>the SS for Time would be correct, but the Treatment SS would have a value of 3109. How can I get 3026 for Treatment and 397 for Time, as I see in the book? What should be taken care of in the order the variables are specified in the formula? </p>

<p>Thank you for your time.  </p>
"
"0.0642824346533225","0.0655590899062897"," 21692","<p>Please help!
I have recently been criticized for using pairwise comparisons to explain all three levels of a factor within a negative binomial GLM rather than all levels at once. I was told that it is ""long-winded"" and ""uneccessary"". I was under the impression that in GLMs one cannot bulk all levels of a factor together to obtain a test statistic and corresponding p-value.</p>

<p>Obviously if a factor is ""insignificant"" at any level then carrying out a post-hoc analysis is pointless. My levels all have there own p-values therefore I discussed these values from the below global model. I was told to do an ANOVA instead which I don't believe is suitable for overdispersed, zero-inflated data.</p>

<p>p-value for all levels of a factor anyone?</p>

<p>(Below, lower field layer 0, upper field layer 1 and change1 is in intercept)</p>

<pre><code>    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
   -2.4284  -0.7956  -0.3862   0.4045   2.4233  

    Coefficients:
                            Estimate Std. Error z value Pr(&gt;|z|)   
    (Intercept)                    4.3410884  1.8219786   2.383  0.01719 * 
    Height                         0.0373584  0.0119929   3.115  0.00184 **
    Width                         -0.0007891  0.0008246  -0.957  0.33859   
    MeanMin                       -0.1731877  0.1404434  -1.233  0.21752   
    as.factor(Site_Treat)2        -0.4080256  0.2480438  -1.645  0.09998 . 
    as.factor(Change)2            -0.4940398  0.1755487  -2.814  0.00489 **
    as.factor(Change)3            -0.1613766  0.1763677  -0.915  0.36019   
    as.factor(Lower_Field_Layer)1  0.4873488  0.2931585   1.662  0.09643 . 
    as.factor(Lower_Field_Layer)2 -0.3292409  0.3717863  -0.886  0.37585   
    as.factor(Upper_Field_Layer)2 -0.0081040  0.3257734  -0.025  0.98015   
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

   (Dispersion parameter for Negative Binomial(4.7795) family taken to be 1)

    Null deviance: 96.392  on 46  degrees of freedom
    Residual deviance: 47.968  on 37  degrees of freedom
    AIC: 403.94
</code></pre>

<p>Best wishes,
Platypezid</p>
"
"0.0742269619025206","0.0757011164104465"," 22543","<p>I am a linguist trying to decide on the appropriate statistical test for some data, and I'm a bit at sea.</p>

<p>The data relates to how often participants in bilingual conversations use words from the other language (codeswitches, in the jargon).  I want to compare the variance (number of codeswitches) between participants in each conversation, and also between each participant and all participants as a whole.</p>

<p>On a wild guess (!) I wonder if the proper test would be ANOVA?  If not that, then what?  Is there any ""idiot's guide to choosing a test"" out there, along the lines of ""if A, B and C apply, use this test""?</p>

<p>I want to use R for the analysis, so any tips or references for more detail on the appropriate test for this data in R would also be very welcome.</p>

<p>Thanks in advance!</p>
"
"0.148711432973085","0.133821931893047"," 22731","<p><strong>Method:</strong> I presented 15 participants with audiovisual clips. There were six different clips I presented, and for each clip there was emotional incongruence between visual and auditory information. The combinations were: </p>

<ul>
<li>visual negative with auditory positive </li>
<li>visual negative with auditory neutral </li>
<li>visual positive with auditory negative</li>
<li>visual positive with auditory neutral</li>
<li>visual neutral with auditory negative</li>
<li>visual neutral with auditory positive</li>
</ul>

<p>I asked participants to make an emotional judgements, whether the clip was happy, angry or neutral. There were five repetitions. </p>

<p><strong>Research question</strong>: Are participants driven by visual or auditory information in their emotional choices, when they watch displays with incongruent emotional information?</p>

<p><strong>Basic analysis:</strong> Because there were no â€œcorrectâ€ responses per se for the incongruent stimuli, I calculated the tendency to respond correctly when emotion was presented auditorily or visually. The tendency was estimated by subtracting the proportion of â€œauditory correctâ€ responses from the proportion of â€œvisual correctâ€ responses for the six incongruent conditions. For example if the incongruent display combined visual negative with auditory positive information, and participants responded that it was a negative interaction, then it was counted as visual response. The estimated indices varied from 1 (participants always responded correctly to the visual information) to âˆ’1 (participants always responded correctly to the auditory information). Below is a figure produced in R that shows averaged tendency scores, with standard error bars. <a href=""http://dl.dropbox.com/u/2505196/exp2tend.dat"" rel=""nofollow"">The data is available here</a> - columns represent conditions, rows - tendency scores for each participant.</p>

<p><img src=""http://i.stack.imgur.com/9cAsP.png"" alt=""enter image description here""></p>

<p>Now, getting to the point I have two questions:</p>

<blockquote>
  <p><strong>Question 1:</strong> What would be the best way to check for significant differences between those different clips?</p>
</blockquote>

<p>I know I could simply run a series of 30 paired t test's but it doesn't seem as a pretty solution. I don't think I can use ANOVA here - I have no clue how I could define levels for all those incongruent conditions. Maybe I could organize this data in some different way, but I really not sure what it would be.</p>

<blockquote>
  <p><strong>Question 2:</strong> Is there a better way you could use to visualize this?</p>
</blockquote>

<p>I don't love the above plot - in fact I really don't like it. The oversized legend is very heavy to read, and so are the patterns differentiating each condition. I could use color instead, but I am a bit color blind to be honest, so I would prefer to avoid it. </p>

<p><strong>EDIT</strong></p>

<p><em>Question 1</em> I decided to just compare the 6 pairs which had the same type of incongreunt information using paired t test. Works kind-of-ok to get the differences.</p>

<p><em>Question 2</em> has been answered well by @AndyW and @gung - I decided to use pure SE bars with vertical orientation of x axis. Done in R using <code>segplot()</code> - I need to tweak the details, but it's roughly the idea.</p>

<p><img src=""http://i.stack.imgur.com/9wvAN.png"" alt=""enter image description here""></p>
"
"0.111901355435757","0.125536099672233"," 23197","<p>I'm analyzing data from an unbalanced factorial experiment both with <code>SAS</code> and <code>R</code>. Both <code>SAS</code> and <code>R</code> provide similar Type I sum of squares but their Type III sum of squares are different from each other. Below are <code>SAS</code> and <code>R</code> codes and outputs. </p>

<pre><code>DATA ASD;
INPUT Y T B;
DATALINES;
 20 1 1
 25 1 2
 26 1 2
 22 1 3
 25 1 3
 25 1 3
 26 2 1
 27 2 1
 22 2 2
 31 2 3
;

PROC GLM DATA=ASD;
CLASS T B;
MODEL Y=T|B;
RUN;
</code></pre>

<p><strong>Type I SS from SAS</strong>    </p>

<pre><code>Source  DF       Type I SS     Mean Square    F Value    Pr &gt; F
T       1     17.06666667     17.06666667       9.75    0.0354
B       2     12.98000000      6.49000000       3.71    0.1227
T*B     2     47.85333333     23.92666667      13.67    0.0163
</code></pre>

<p><strong>Type III SS from SAS</strong></p>

<pre><code>Source  DF     Type III SS     Mean Square    F Value    Pr &gt; F
T       1     23.07692308     23.07692308      13.19    0.0221
B       2     31.05333333     15.52666667       8.87    0.0338
T*B     2     47.85333333     23.92666667      13.67    0.0163
</code></pre>

<p><strong>R Code</strong></p>

<pre><code>Y &lt;- c(20, 25, 26, 22, 25, 25, 26, 27, 22, 31)
T &lt;- factor(x=rep(c(1, 2), times=c(6, 4)))
B &lt;- factor(x=rep(c(1, 2, 3, 1, 2, 3), times=c(1, 2, 3, 2, 1, 1)))
Data &lt;- data.frame(Y, T, B)
Data.lm &lt;- lm(Y~T*B, data = Data)
anova(Data.lm)
drop1(Data.lm,~.,test=""F"") 
</code></pre>

<p><strong>Type I SS from R</strong>    </p>

<pre><code>Analysis of Variance Table

Response: Y
          Df Sum Sq Mean Sq F value  Pr(&gt;F)  
T          1 17.067  17.067  9.7524 0.03543 *
B          2 12.980   6.490  3.7086 0.12275  
T:B        2 47.853  23.927 13.6724 0.01629 *
Residuals  4  7.000   1.750                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><strong>Type III SS from R</strong></p>

<pre><code>Single term deletions

Model:
Y ~ T * B
       Df Sum of Sq    RSS     AIC F value  Pr(&gt;F)  
&lt;none&gt;               7.000  8.4333                  
T       1    28.167 35.167 22.5751 16.0952 0.01597 *
B       2    20.333 27.333 18.0552  5.8095 0.06559 .
T:B     2    47.853 54.853 25.0208 13.6724 0.01629 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Am I missing something here? If not which one is correct Type III SS?</p>
"
"0.174077655955698","0.16946508860008"," 25632","<h1>Books to Learn Statistics using R</h1>

<h2>What exactly is the book I'm looking for.</h2>

<p>What I am looking for is a book that teaches you statistics while using R to give you hands-on experience and thus end up helping you learn R together. I've seen on amazon many books that attempts to do that, but not with R. Examples are Minitab and SAS.</p>

<h2>Are the R Book and Statistical Computing an option? - <em>Still not answered</em>.</h2>

<p><a href=""http://rads.stackoverflow.com/amzn/click/0470510242"">The R Book</a> and <a href=""http://rads.stackoverflow.com/amzn/click/0471560405"">Statistical Computing: An Introduction to Data Analysis using S-Plus</a> seems viable, but a reader opinion here would be helpful and welcome.</p>

<h2>How the book relate to statistics courses?</h2>

<p>To be even more precise on what I was looking for, consider these two courses learning outcomes on statistics from a math department at the university Im currently a student:</p>

<p><a href=""http://www.stevens.edu/ses/math/courses/ma331/index.php"">Intermediate Statistics</a> and <a href=""http://www.stevens.edu/ses/math/courses/ma222/index.php"">Probability &amp; Statistics</a>, that is, I'm looking in a book a normal statistics course going to intermediate level but rather than just board and paper having you learning and using R instead. That also means I am looking for a book that assume I want to learn statistics from the beginning. </p>

<h2>This book is for researchers too.</h2>

<p>I am also a software engineer researcher, but I guess the current situation where you are found with mountains of data and want to learn statistics to go on writing code to automate that is pretty much applicable to many other fields. </p>

<p>That means I'm am not interested on learning every single detail of every single property for every single curve, but am more concerned on making sense of data for my research domain, although I would not mind if the book wanted to go deep on that. </p>

<p>As a final motivation, I find myself reading scientific papers in different sort of communities that claim results based on statistical inference while there is no readable proof if the statistics assumptions/constraints are being violated or not. </p>

<p>A R book that is not much about statistics won't ensure I am not following up on this practice, which is also why I decided looking for a book that is akin to a statistics course using R rather than playing around with a overview book. </p>

<h2>Related questions in Cross Validated.</h2>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/1668/what-books-provide-an-overview-of-computational-statistics-as-it-applies-to-comp"">What books provide an overview of computational statistics as it applies to computer science?</a> - Differs that the question looks for an overview while this is to learn statistics using R.</li>
<li><a href=""http://stats.stackexchange.com/questions/614/open-source-statistical-textbooks"">Open source statistical books</a> gives a list of open source (open books) available online. </li>
</ul>

<h2>Answers and feedback for this question.</h2>

<h3>@Julie</h3>

<p>Suggested books were few I already come across but are an example that unfortunately doesn't suits me:</p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/0387790535"">Introductory Statistics with R</a>, <a href=""http://rads.stackoverflow.com/amzn/click/1584884509"">Using R for Introductory Statistics</a>, <a href=""http://rads.stackoverflow.com/amzn/click/0470022981"">Statistics: An Introduction using R</a> are few of the books that I already looked on amazon but are about an statistics overview or make assumptions that requires previous statistics knowledge. The problem with overview books is mostly about not calling attention to the assumptions, constraints and provide enough explanation to result in make sense of the information. </p>

<p>If you believe there is no book that could fit on this needing as well or think the R book or the Statistical Computing: An Introduction to Data Analysis using S-Plus would fit this, I would also appreciate this type of answer.</p>

<h3>@Christopher Aden</h3>

<p>Introduction to Probability and Statistics Using R seems to be the closest one but still broad general to what I was looking for.</p>

<p><em>What I was expecting for is a book such as <a href=""http://rads.stackoverflow.com/amzn/click/1429224266"">David S. Moore, The Basics of Statistics</a> because:</em></p>

<ul>
<li>It covers all statistics subjects. </li>
<li>It uses two tools, miniTab and other to give hands-on learning on the just explained method.</li>
<li>It very much highlight assumptions and constraints. This is very important for a researcher who has not taken a in depth statistics course and want to use statistics. Hardly overview books will cover them, which is dangerous for researchers.
<ul>
<li>You can see the book table of contents <a href=""http://www.whfreeman.com/Catalog/product/basicpracticeofstatistics-fifthedition-moore/tableofcontents"">here</a>. Notice how the focus is statistics and the tool usage is to improve understanding and get the student to know how to use tools to do the statistics after learning in an easier way. Its not about the tool, its about statistics! </li>
</ul></li>
</ul>

<p><em>I want exactly the same thing, but using R.</em></p>

<h3>@Gregory Demin</h3>

<p>It uses R as pedagogy examples, assumes you want to learn statistics and best of all, it is open source. Unfortunately, does not cover ANOVA nor ANCOVA, or more advanced subjects.</p>

<h3>@Peter Ellis</h3>

<p>Good suggestion for a textbook that covers what is wanted in this question.</p>

<h2>Books in the asker opinion that answer the question.</h2>

<p>@Peter Ellis and @Gregory Demin.</p>

<h2>Collection of R Books on Amazon</h2>

<p>Amazon discussion about R books for different students background may be found <a href=""http://www.amazon.com/gp/richpub/syltguides/fullview/R3ET07XHWH16TF"">here</a>. </p>

<h2>Video Lectures teaching Statistics using R</h2>

<p>Google Tech Talks from 2007 that also motivated this question and covers more about Data Mining rather than statistics but using R together <a href=""http://www.youtube.com/watch?v=zRsMEl6PHhM&amp;feature=BFa&amp;list=LL6zEW1-HcVxlw7hOFjJHneg&amp;lf=mh_lolz"">here</a>.</p>
"
"0.144021460796607","0.156061849468725"," 25875","<p>experts,</p>

<p>I would like to ask for advice regarding the analysis of a dataset I am currently working on. </p>

<p>In the experiment subjects were tested in a reaction time task (RT as dependent variable). In random order every subject was tested in two treatment conditions (factor 'A': placebo vs. drug). Also for subject in a third session the level of a blood value was measured (a numeric covariate 'B'). This covariate 'B' is stable within a subject and believed to be a predictor for the effect of factor 'A'. Moreover subjects were grouped according to their genotype (factor 'C'). Also I coded the first or second testing in factor 'D' in order to model potential learning effects.</p>

<p>Please find below some sample code. </p>

<pre><code># some example data
subject &lt;- c(1,1,2,2,3,3,4,4)
A &lt;- rep(c('placebo', 'drug'), 4) # factor A
B &lt;- c(1,1,4,4,8,8,12,12) # covariate B
C &lt;- c('x','x','y','y','x','x','y','y')
D &lt;- c('1st','2nd','1st','2nd','2nd','1st','2nd','1st')
RT &lt;- c(2,12,1,16,2,26,3,39)

data &lt;- as.data.frame(cbind(subject, A, B, C, D, RT))
data$B &lt;- as.numeric(as.character(data$B))
data$RT &lt;- as.numeric(as.character(data$RT))
</code></pre>

<p>I would know how to estimate the overall effect of the covariate 'B' using lm() and how to estimate the overall effect of factor 'A' using aov(). </p>

<pre><code># Model effect of covariate B using lm()
data.lm &lt;- lm(RT~B, data=data)
summary(data.lm)

# Model effect of factor A using aov()
data.aov &lt;- aov(RT~A+Error(subject/A),data=data)
summary(data.aov)
</code></pre>

<p>From the comments I understood that the correct ANCOVA model to test effects of A, B and their interaction would be as followed:</p>

<pre><code># ANCOVA for effects of A, B and their interaction
data.aov2 &lt;- aov(RT~ B * A + Error(subject/A), data = data)
summary(data.aov2)
</code></pre>

<p>However if I would like to look at B, C and their interaction the output from summary() does not provide significances anymore which makes me assume the model is not correct:</p>

<pre><code># ANOVA for effects B, C and their interaction
data.aov3 &lt;- aov(RT ~ B * C + Error(subject), data=data)
summary(data.aov3)
</code></pre>

<p>Also of course I would like to know whether it is possible to estimate the most comprehensive model including all factors (A, B, C, D) and their interaction (?). Probably the dataset is too small to estimate this. But how can I know whether my data is sufficient to allow for the estimation of such a complicated model?</p>

<pre><code># ANOVA for effects B, C and their interaction
data.aov4 &lt;- aov(RT ~ A * B * C * D + Error(subject/A), data=data)
summary(data.aov4)
</code></pre>

<p>Also I would like to know whether the same models can be implemented in the same way in case there are repeated-measures of the dependent variable 'RT'? E.g.:</p>

<pre><code>data2 &lt;- rbind(data, data, data, data, data)
data2$RT &lt;- rnorm(nrow(data2))

# ANOVA for effects B, C and their interaction
data.aov5 &lt;- aov(RT ~ A * B * C * D + Error(subject/A), data=data2)
summary(data.aov5)
</code></pre>

<p>Any help regarding this (including hints for an implementation in R as well as literature) is highly appreciated.</p>

<p>Many thanks!
Jokel</p>
"
"0.111901355435757","0.125536099672233"," 26461","<p>We have a data set with two covariates and a categorical grouping variable and want to know if there are significant differences between the slope or intercept among the covariates associated with the different grouping variables.  We've used anova() and lm() to compare the fits of three different models: 1) with a single slope and intercept, 2) with different intercepts for each group, and 3) with a slope and an intercept for each group.  According to the anova() general linear test, the second model is the most appropriate of the three, there is a significant improvement to the model by including a separate intercept for each group.  However, when we look at the 95% confidence intervals for these intercepts -- they all overlap, suggesting there aren't significant differences between the intercepts.  How can these two results be reconciled?  We thought another way of interpreting the results of the model-selection method was that there has to be at least one significant difference among the intercepts... but perhaps this is not correct?</p>

<p>Below is the R code to replicate this analysis.  We've used the dput() function so you can work with exactly the same data we're grappling with.</p>

<pre><code># Begin R Script
# &gt; dput(data)
structure(list(Head = c(1.92, 1.93, 1.79, 1.94, 1.91, 1.88, 1.91, 
1.9, 1.97, 1.97, 1.95, 1.93, 1.95, 2, 1.87, 1.88, 1.97, 1.88, 
1.89, 1.86, 1.86, 1.97, 2.02, 2.04, 1.9, 1.83, 1.95, 1.87, 1.93, 
1.94, 1.91, 1.96, 1.89, 1.87, 1.95, 1.86, 2.03, 1.88, 1.98, 1.97, 
1.86, 2.04, 1.86, 1.92, 1.98, 1.86, 1.83, 1.93, 1.9, 1.97, 1.92, 
2.04, 1.92, 1.9, 1.93, 1.96, 1.91, 2.01, 1.97, 1.96, 1.76, 1.84, 
1.92, 1.96, 1.87, 2.1, 2.17, 2.1, 2.11, 2.17, 2.12, 2.06, 2.06, 
2.1, 2.05, 2.07, 2.2, 2.14, 2.02, 2.08, 2.16, 2.11, 2.29, 2.08, 
2.04, 2.12, 2.02, 2.22, 2.22, 2.2, 2.26, 2.15, 2, 2.24, 2.18, 
2.07, 2.06, 2.18, 2.14, 2.13, 2.2, 2.1, 2.13, 2.15, 2.25, 2.14, 
2.07, 1.98, 2.16, 2.11, 2.21, 2.18, 2.13, 2.06, 2.21, 2.08, 1.88, 
1.81, 1.87, 1.88, 1.87, 1.79, 1.99, 1.87, 1.95, 1.91, 1.99, 1.85, 
2.03, 1.88, 1.88, 1.87, 1.85, 1.94, 1.98, 2.01, 1.82, 1.85, 1.75, 
1.95, 1.92, 1.91, 1.98, 1.92, 1.96, 1.9, 1.86, 1.97, 2.06, 1.86, 
1.91, 2.01, 1.73, 1.97, 1.94, 1.81, 1.86, 1.99, 1.96, 1.94, 1.85, 
1.91, 1.96, 1.9, 1.98, 1.89, 1.88, 1.95, 1.9, 1.94, NA, 1.84, 
1.83, 1.84, 1.96, 1.74, 1.91, 1.84, 1.88, 1.83, 1.93, 1.78, 1.88, 
1.93, 2.15, 2.16, 2.23, 2.09, 2.36, 2.31, 2.25, 2.29, 2.3, 2.04, 
2.22, 2.19, 2.25, 2.31, 2.3, 2.28, 2.25, 2.15, 2.29, 2.24, 2.34, 
2.2, 2.24, 2.17, 2.26, 2.18, 2.17, 2.34, 2.23, 2.36, 2.31, 2.13, 
2.2, 2.27, 2.27, 2.2, 2.34, 2.12, 2.26, 2.18, 2.31, 2.24, 2.26, 
2.15, 2.29, 2.14, 2.25, 2.31, 2.13, 2.09, 2.24, 2.26, 2.26, 2.21, 
2.25, 2.29, 2.15, 2.2, 2.18, 2.16, 2.14, 2.26, 2.22, 2.12, 2.12, 
2.16, 2.27, 2.17, 2.27, 2.17, 2.3, 2.25, 2.17, 2.27, 2.06, 2.13, 
2.11, 2.11, 1.97, 2.09, 2.06, 2.11, 2.09, 2.08, 2.17, 2.12, 2.13, 
1.99, 2.08, 2.01, 1.97, 1.97, 2.09, 1.94, 2.06, 2.09, 2.04, 2, 
2.14, 2.07, 1.98, 2, 2.19, 2.12, 2.06, 2, 2.02, 2.16, 2.1, 1.97, 
1.97, 2.1, 2.02, 1.99, 2.13, 2.05, 2.05, 2.16, 2.02, 2.02, 2.08, 
1.98, 2.04, 2.02, 2.07, 2.02, 2.02, 2.02), Site = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c(""ANZ"", ""BC"", ""DV"", ""MC"", 
""RB"", ""WW""), class = ""factor""), Leg = c(2.38, 2.45, 2.22, 2.23, 
2.26, 2.32, 2.28, 2.17, 2.39, 2.27, 2.42, 2.33, 2.31, 2.32, 2.25, 
2.27, 2.38, 2.28, 2.33, 2.24, 2.21, 2.22, 2.42, 2.23, 2.36, 2.2, 
2.28, 2.23, 2.33, 2.35, 2.36, 2.26, 2.26, 2.3, 2.23, 2.31, 2.27, 
2.23, 2.37, 2.27, 2.26, 2.3, 2.33, 2.34, 2.27, 2.4, 2.22, 2.25, 
2.28, 2.33, 2.26, 2.32, 2.29, 2.31, 2.37, 2.24, 2.26, 2.36, 2.32, 
2.32, 2.15, 2.2, 2.29, 2.37, 2.26, 2.24, 2.23, 2.24, 2.26, 2.18, 
2.11, 2.23, 2.31, 2.25, 2.15, 2.3, 2.33, 2.35, 2.21, 2.36, 2.27, 
2.24, 2.35, 2.24, 2.33, 2.32, 2.24, 2.35, 2.36, 2.39, 2.28, 2.36, 
2.19, 2.27, 2.39, 2.23, 2.29, 2.32, 2.3, 2.32, NA, 2.25, 2.24, 
2.21, 2.37, 2.21, 2.21, 2.27, 2.27, 2.26, 2.19, 2.2, 2.25, 2.25, 
2.25, NA, 2.24, 2.17, 2.2, 2.2, 2.18, 2.14, 2.17, 2.27, 2.28, 
2.27, 2.29, 2.23, 2.25, 2.33, 2.22, 2.29, 2.19, 2.15, 2.24, 2.24, 
2.26, 2.25, 2.09, 2.27, 2.18, 2.2, 2.25, 2.24, 2.18, 2.3, 2.26, 
2.18, 2.27, 2.12, 2.18, 2.33, 2.13, 2.28, 2.23, 2.16, 2.2, 2.3, 
2.31, 2.18, 2.33, 2.29, 2.26, 2.21, 2.22, 2.27, 2.32, 2.24, 2.25, 
2.17, 2.2, 2.26, 2.27, 2.24, 2.25, 2.09, 2.25, 2.21, 2.24, 2.21, 
2.22, 2.13, 2.24, 2.21, 2.3, 2.34, 2.35, 2.32, 2.46, 2.43, 2.42, 
2.41, 2.32, 2.25, 2.33, 2.19, 2.45, 2.32, 2.4, 2.38, 2.35, 2.39, 
2.29, 2.35, 2.43, 2.29, 2.33, 2.31, 2.28, 2.38, 2.32, 2.43, 2.27, 
2.4, 2.37, 2.27, 2.41, 2.32, 2.38, 2.23, 2.33, 2.21, 2.34, 2.19, 
2.34, 2.35, 2.35, 2.31, 2.33, 2.41, 2.53, 2.39, 2.17, 2.16, 2.38, 
2.34, 2.33, 2.33, 2.29, 2.43, 2.28, 2.34, 2.38, 2.3, 2.29, 2.43, 
2.36, 2.24, 2.35, 2.38, 2.4, 2.36, 2.42, 2.28, 2.45, 2.33, 2.32, 
2.33, 2.31, 2.44, 2.37, 2.4, 2.35, 2.33, 2.31, 2.36, 2.43, 2.38, 
2.4, 2.38, 2.46, 2.33, 2.38, 2.23, 2.24, 2.39, 2.36, 2.19, 2.32, 
2.37, 2.39, 2.34, 2.39, 2.23, 2.25, 2.29, 2.39, 2.35, NA, 2.28, 
2.35, 2.38, 2.34, 2.17, 2.29, NA, 2.26, NA, NA, NA, 2.24, 2.33, 
2.23, 2.28, 2.29, 2.23, 2.2, 2.27, 2.31, 2.31, 2.26, 2.28)), .Names = c(""Head"", 
""Site"", ""Leg""), class = ""data.frame"", row.names = c(NA, -312L
)) 

# plot graph
library(ggplot2)

qplot(Head, Leg, 
    color=Site, 
    data=data) + 
        stat_smooth(method=""lm"", alpha=0.2) + 
        theme_bw()
</code></pre>

<p><img src=""http://i.stack.imgur.com/QMIBf.jpg"" alt=""enter image description here""></p>

<pre><code># create linear models
lm.1 &lt;- lm(Leg ~ Head, data)
lm.2 &lt;- lm(Leg ~ Head + Site, data)
lm.3 &lt;- lm(Leg ~ Head*Site, data)

# evaluate linear models
anova(lm.1, lm.2, lm.3)
anova(lm.1, lm.2)

# &gt; anova(lm.1, lm.2)
# Analysis of Variance Table
# Model 1: Leg.3.1 ~ Head.W1
# Model 2: Leg.3.1 ~ Head.W1 + Site
  # Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
# 1    302 1.25589                                 
# 2    297 0.91332  5   0.34257 22.28 &lt; 2.2e-16 ***


# examining the multiple-intercepts model (lm.2)
summary(lm.2)
coef(lm.2)
confint(lm.2)

# extracting the intercepts
intercepts &lt;- coef(lm.2)[c(1, 3:7)]
intercepts.1 &lt;- intercepts[1]
intercepts &lt;- intercepts.1 + intercepts
intercepts[1] &lt;- intercepts.1
intercepts

# extracting the confidence intervals
ci &lt;- confint(lm.2)[c(1, 3:7),]
ci[2:6,] &lt;- ci[2:6,] + confint(lm.2)[1,]
ci[,1]

# putting everything together in a dataframe
labels &lt;- c(""ANZ"", ""BC"", ""DV"", ""MC"", ""RB"", ""WW"")
ci.dataframe &lt;- data.frame(Site=labels, Intercept=intercepts, CI.low = ci[,1], CI.high = ci[,2])
ci.dataframe

# plotting intercepts and 95% CI
qplot(Site, Intercept, geom=c(""point"", ""errorbar""), ymin=CI.low, ymax=CI.high, data=ci.dataframe, ylab=""Intercept &amp; 95% CI"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/40PNp.jpg"" alt=""ancova intercepts""></p>

<p>Just to summarize -- the problem is that the 95% CIs for the intercepts all overlap, but the model selection method suggests that the best model is one that fits different intercepts.  So I'm inclined to think either our model selection method is flawed or the 95% CIs for the intercept estimates were calculated incorrectly.  Any thoughts would be greatly appreciated!</p>
"
"0.0742269619025206","0.0757011164104465"," 26810","<p>I ran a simple psychology experiment that included 4 conditions, each containing 8 blocks of training. There were different participants in each condition. Hence, condition and block and subject and block are crossed, but subject is nested in condition. I'm trying to do a basic repeated measures anova to test for effects of block and condition. I have an unbalanced design, and a mixed effects model. My approach was to use the lme4 package in conjunction with the car package. I am running R version 2.14 on mac os x lion.</p>

<p>Here is what I've done:</p>

<pre><code>library(nlme)
library(car)

rm( list = ls() )

data &lt;- read.table(""anova_data"", header = TRUE)

condition &lt;- factor(data$condition)
block &lt;- factor(data$block)
subject &lt;- factor(data$subject)
accuracy &lt;- data$accuracy

fm1 &lt;- lmer( accuracy ~ block*condition + (1|subject %in% condition) )

Anova(fm1)
</code></pre>

<p>My problem is that this returns a summary table without F values, like such:</p>

<pre><code>Analysis of Deviance Table (Type II tests)

Response: accuracy

                 Chisq Df Pr(&gt;Chisq)
     block           17.169  7    0.01634 *  

condition       68.294  3  9.897e-15 ***

block:condition 26.481 21    0.18869
</code></pre>

<p>Any help is greatly appreciated.</p>
"
"0.102172997627094","0.10420216305674"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.0524863881081478","0.0535287727572189"," 26855","<p>As the general consensus seems to be to use mixed-models via <code>lmer()</code> in R instead of classical ANOVA (for the often cited reasons, like unbalanced designs, crossed random effects etc.), I would like to give it a try with my data. However I am worried that I would be able to ""sell"" this approach to my supervisor (who is expecting classical analysis with a p-value in the end) or later to the reviewers.</p>

<p>Could you recommend some nice examples of published articles that used mixed-models or <code>lmer()</code> for different designs like repeated-measures or multiple within- and between-subject designs for the field biology, psychology, medicine?</p>
"
"0.0981930408849676","0.100143163995996"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"0.143739893644017","0.136821609468585"," 28486","<p>I've performed a three-way repeated measures ANOVA; what post-hoc analyses are valid? </p>

<p>This is a fully balanced design (2x2x2) with one of the factors having a within-subjects repeated measure. I'm aware of multivariate approaches to repeated measures ANOVA in R, but my first instinct is to proceed with a simple aov() style of ANOVA:</p>

<pre><code>aov.repeated &lt;- aov(DV ~ IV1 * IV2 * Time + Error(Subject/Time), data=data)
</code></pre>

<p>DV = response variable</p>

<p>IV1 = independent variable 1 (2 levels, A or B)</p>

<p>IV2 = independent variable 2 (2 levels, Yes or No)</p>

<p>IV3 = Time (2 levels, Before or After)</p>

<p>Subject = Subject ID (40 total subjects, 20 for each level of IV1: nA = 20, nB = 20)</p>

<pre><code>summary(aov.repeated)

    Error: Subject
          Df Sum Sq Mean Sq F value   Pr(&gt;F)   
IV1       1   5969  5968.5  4.1302 0.049553 * 
IV2       1   3445  3445.3  2.3842 0.131318   
IV1:IV2   1  11400 11400.3  7.8890 0.007987 **
Residuals 36  52023  1445.1                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Error: Subject:Time
               Df Sum Sq Mean Sq F value   Pr(&gt;F)   
Time            1    149   148.5  0.1489 0.701906   
IV1:Time        1    865   864.6  0.8666 0.358103   
IV2:Time        1  10013 10012.8 10.0357 0.003125 **
IV1:IV2:Time    1    852   851.5  0.8535 0.361728   
Residuals      36  35918   997.7                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Alternatively, I was thinking about using the nlme package for a lme style ANOVA:</p>

<pre><code>aov.repeated2 &lt;- lme(DV ~ IV1 * IV2 * Time, random = ~1|Subject/Time, data=data)
summary(aov.repeated2)

Fixed effects: DV ~ IV1 * IV2 * Time 
                                Value Std.Error DF   t-value p-value
(Intercept)                      99.2  11.05173 36  8.975972  0.0000
IV1                              19.7  15.62950 36  1.260437  0.2156
IV2                              65.9  15.62950 36  4.216385  0.0002 ***
Time                             38.2  14.12603 36  2.704228  0.0104 *
IV1:IV2                         -60.8  22.10346 36 -2.750701  0.0092 **
IV1:Time                        -26.2  19.97722 36 -1.311494  0.1980
IV2:Time                        -57.8  19.97722 36 -2.893295  0.0064 **
IV1:IV2:Time                     26.1  28.25206 36  0.923826  0.3617
</code></pre>

<p>My first instinct post-hoc of significant 2-way interactions with Tukey contrasts using glht() from multcomp package:</p>

<pre><code>data$IV1IV2int &lt;- interaction(data$IV1, data$IV2)
data$IV2Timeint &lt;- interaction(data$IV2, data$Time)

aov.IV1IV2int &lt;- lme(DV ~ IV1IV2int, random = ~1|Subject/Time, data=data)
aov.IV2Timeint &lt;- lme(DV ~ IV2Timeint, random = ~1|Subject/Time, data=data)

IV1IV2int.posthoc &lt;- summary(glht(aov.IV1IV2int, linfct = mcp(IV1IV2int = ""Tukey"")))
IV2Timeint.posthoc &lt;- summary(glht(aov.IV2Timeint, linfct = mcp(IV2Timeint = ""Tukey"")))

IV1IV2int.posthoc
#A.Yes - B.Yes == 0        0.94684   
#B.No - B.Yes == 0         0.01095 * 
#A.No - B.Yes == 0         0.98587    I don't care about this
#B.No - A.Yes == 0         0.05574 .  I don't care about this
#A.No - A.Yes == 0         0.80785   
#A.No - B.No == 0          0.00346 **

IV2Timeint.posthoc 
#No.After - Yes.After == 0           0.0142 *
#Yes.Before - Yes.After == 0         0.0558 .
#No.Before - Yes.After == 0          0.5358   I don't care about this
#Yes.Before - No.After == 0          0.8144   I don't care about this
#No.Before - No.After == 0           0.1941  
#No.Before - Yes.Before == 0         0.8616
</code></pre>

<p>The main problem I see with these post-hoc analyses are some comparisons that aren't useful for my hypotheses.</p>

<p>Any suggestions for an appropriate post-hoc analysis are greatly appreciated, thanks.</p>

<p><strong>Edit:</strong> <a href=""http://stats.stackexchange.com/questions/5250/multiple-comparisons-on-a-mixed-effects-model"">Relevant question and answer that points toward testing manual contrast matrices</a></p>
"
"0.166165035230439","0.177534854723893"," 28649","<p>What (with justification) is a valid post-hoc for a two-way ANOVA main effect, if no interaction is present?</p>

<p>Example two-way fixed effect ANOVA:</p>

<pre><code>&gt; aov.example &lt;- aov(Response ~ IV1 * IV2, data=data)
&gt; summary(aov.example)

              Df Sum Sq Mean Sq F value   Pr(&gt;F)   
IV1            1  13.10  13.099  0.7222 0.40547  
IV2            4 315.56  78.891  4.3498 0.01081 *
IV1:IV2        4 141.00  35.251  1.9436 0.14240  
Residuals     20 362.74  18.137          
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I see a few general options, but I'm unsure which is most appropriate:</p>

<ol>
<li><p><strong>Post-hoc with a multiple comparison test using two-way model</strong></p>

<pre><code>&gt; TukeyHSD(aov.example, which=""IV2"")

Tukey multiple comparisons of means
   95% family-wise confidence level

Fit: aov(formula = Response ~ IV1 * IV2, data = data)

$IV2
               diff        lwr        upr     p adj
B-A      -2.1485711  -9.506162  5.2090200 0.9031415
C-A      -2.3382727  -9.695864  5.0193184 0.8733517
D-A       1.4732257  -5.884365  8.8308168 0.9735981
E-A      -8.0515205 -15.409112 -0.6939294 0.0277241
C-B      -0.1897016  -7.547293  7.1678895 0.9999912
D-B       3.6217968  -3.735794 10.9793879 0.5905067
E-B      -5.9029494 -13.260541  1.4546416 0.1559460
D-C       3.8114984  -3.546093 11.1690895 0.5439632
E-C      -5.7132478 -13.070839  1.6443432 0.1785266
E-D      -9.5247462 -16.882337 -2.1671552 0.0074740
</code></pre></li>
<li><p><strong>Post-hoc with a multiple comparison test using one-way model</strong></p>

<pre><code>&gt; TukeyHSD(aov(Response ~ IV2, data=data))

Tukey multiple comparisons of means
   95% family-wise confidence level

Fit: aov(formula = Response ~ IV2, data = data)

$IV2
               diff        lwr        upr     p adj
B-A      -2.1485711  -9.858179  5.5610365 0.9224401
C-A      -2.3382727 -10.047880  5.3713349 0.8976378
D-A       1.4732257  -6.236382  9.1828333 0.9794638
E-A      -8.0515205 -15.761128 -0.3419130 0.0375667
C-B      -0.1897016  -7.899309  7.5199060 0.9999933
D-B       3.6217968  -4.087811 11.3314044 0.6456949
E-B      -5.9029494 -13.612557  1.8066581 0.1951992
D-C       3.8114984  -3.898109 11.5211060 0.6014671
E-C      -5.7132478 -13.422855  1.9963597 0.2212053
E-D      -9.5247462 -17.234354 -1.8151387 0.0102178
</code></pre>

<p>Overall, in this example the significant comparisons do not change, but the adjusted p-values are lower using the two-way ANOVA model. Which is most appropriate for a two-way main effect post-hoc?</p></li>
<li><p><strong>Post-hoc with subset levels of other independent variable</strong> </p></li>
</ol>

<p>I'm pretty sure this is not valid, given the lack of any effect across the other independent variable. However, when one level of IV1 has larger effects between levels in IV2 compared to other IV1 levels, wouldn't this strongly skew the overall analysis? In other words, even though overall there is a main effect, could this effect not be significant in the the other level of IV1?</p>

<pre><code>    &gt; aov.level1 &lt;- aov(Response ~ IV2, data=data[1:15,])
    &gt; summary(aov.level1)

                Df Sum Sq Mean Sq F value  Pr(&gt;F)  
    IV2          4 334.55  83.639  3.8413 0.03837 *
    Residuals   10 217.74  21.774                  

    &gt; aov.level2 &lt;- aov(Response ~ IV2, data=data[16:30,])
    &gt; summary(aov.level2)

                Df Sum Sq Mean Sq F value Pr(&gt;F)  
    IV2          4 122.01  30.503  2.1037 0.1551
    Residuals   10 145.00  14.500 
</code></pre>

<p>Level 1 of IV1 has a significant IV2 main effect, but Level 2 does not. There are also differences in significant multiple comparisons. My concern is that I'm committing a Type I error using the previous two methods.</p>

<pre><code>    &gt; TukeyHSD(aov.level1)

    Tukey multiple comparisons of means
        95% family-wise confidence level

    Fit: aov(formula = Response ~ IV2, data = data[1:15, ])

    $IV2
                     diff        lwr         upr     p adj
    B-A       -7.86237771 -20.401217  4.67646157 0.3054028
    C-A       -7.90634218 -20.445181  4.63249709 0.3008150
    D-A       -0.94962690 -13.488466 11.58921237 0.9989922
    E-A      -12.55848654 -25.097326 -0.01964727 0.0496014
    C-B       -0.04396448 -12.582804 12.49487479 1.0000000
    D-B        6.91275080  -5.626088 19.45159008 0.4167559
    E-B       -4.69610883 -17.234948  7.84273044 0.7342625
    D-C        6.95671528  -5.582124 19.49555455 0.4111062
    E-C       -4.65214436 -17.190984  7.88669492 0.7404793
    E-D      -11.60885964 -24.147699  0.92997964 0.0729541

    &gt; TukeyHSD(aov.level2)

    Tukey multiple comparisons of means
        95% family-wise confidence level

    Fit: aov(formula = Response ~ IV2, data = data[16:30, ])

    $IV2
                   diff        lwr       upr     p adj
    B-A       3.5652355  -6.667185 13.797656 0.7795224
    C-A       3.2297968  -7.002624 13.462218 0.8321507
    D-A       3.8960783  -6.336343 14.128499 0.7231230
    E-A      -3.5445545 -13.776975  6.687866 0.7829174
    C-B      -0.3354387 -10.567860  9.896982 0.9999634
    D-B       0.3308428  -9.901578 10.563264 0.9999654
    E-B      -7.1097900 -17.342211  3.122631 0.2257310
    D-C       0.6662815  -9.566139 10.898702 0.9994435
    E-C      -6.7743513 -17.006772  3.458070 0.2618999
    E-D      -7.4406328 -17.673054  2.791788 0.1942037
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/16760/why-does-post-hoc-test-on-pair-of-group-means-become-significant-when-looking-at"">This post has some relevant information.</a></p>
"
"0.139175553567226","0.141939593269587"," 28876","<p>I've been running some power simulations for a one-way ANOVA in R, and my problem is that the results from the simulation doesn't match the result from g*power or <em>pwr.anova.test</em> from the ""pwr""-package. As an example, let's compare simulated power to analytical power using these values:</p>

<pre><code>group_size &lt;- c(40,40,40)
means &lt;- c(0.2,0,-0.2)
sds &lt;- c(1,1,1)
</code></pre>

<p>Analytical power analysis, f = 0.1632993 is calculated from the means and standard deviations above. </p>

<pre><code>size &lt;- 10
plot_df &lt;- data.frame()
power &lt;- 0
while(power &lt; 0.80) {
  power &lt;- pwr.anova.test(k=3, n=size, f= 0.1632993, sig.level=0.05)$power
  plot_df &lt;- rbind(plot_df, data.frame(""n"" = size, ""power"" = power))
  size &lt;- size + 2
  print(power)
}
</code></pre>

<p>Simulated power analysis</p>

<pre><code>set.seed(1001)
run_sim &lt;- function() {
# generate all data
create_sim_data &lt;- function(i) {
  #stdev bias correction
  c4 &lt;- (sqrt(2/(group_size[1] - 1))) * (gamma(group_size[1]/2)/gamma((group_size[1] - 1)/2))
  sds2 &lt;- sds / c4
  # pre-allocate matrix
  test_matrix &lt;- matrix(nrow=sims, ncol=sum(group_size))
  # nested loops to create simulated data for all runs
   for(j in 1:sims) {
     for(i in 1:length(group_size)) {
       # col_start &amp; cold_end is used to have the different groups on the same row
       col_start &lt;- sum(group_size[1:i])-(group_size[i]-1)
       col_end &lt;- cumsum(group_size)[i]
       # generate data with rnorm
       test_matrix[j,col_start:col_end] &lt;- rnorm(group_size[i], mean = means[i], sd = sds2[i])
     }
   }
    return(test_matrix)
}
# extract results from simulations
get_power &lt;- function() {
  sig &lt;- rep(NA, sims)
  eta_2 &lt;- rep(NA, sims)
  omega_2 &lt;- rep(NA, sims)
  for(i in 1:sims) {
    # perform ANOVA on data
    result &lt;- summary(aov(test_matrix[i,] ~ group))
    # calculate effect size
    eta_2[i] &lt;- result[[1]]$'Sum Sq'[1] / sum(result[[1]]$'Sum Sq')
    omega_2[i] &lt;- (result[[1]]$'Sum Sq'[1] - (result[[1]]$Df[1] * result[[1]]$'Mean Sq'[2])) / (sum(result[[1]]$'Sum Sq') + result[[1]]$'Mean Sq'[2])
        # get p-value from ANOVA
        sig_result &lt;- result[[1]]$'Pr(&gt;F)'[1]
    # check sig.level
    sig[i] &lt;- sig_result &lt; 0.05
  }
  out &lt;- list(""power"" = mean(sig), ""eta_2"" = mean(eta_2), ""omega"" = mean(omega_2))
}

power &lt;- 0
plot_df &lt;- data.frame()
eta &lt;- NULL
omega &lt;- NULL
# repeat the simulation until the desired power is found
while(power &lt; 0.8) {
  # regenerate grouping as group_size increases 
  group &lt;- c(rep(1, group_size[1]), rep(2,group_size[2]), rep(3,group_size[3]))
  # create data matrix
  test_matrix &lt;- create_sim_data()
  # get anova power
  result &lt;- get_power()
  # extract power value
  power &lt;- result$power
      # save eta-squared each iteration
      eta &lt;- rbind(eta, result$eta_2)
  # save eta-squared each iteration
  omega &lt;- rbind(omega, result$omega)
  cat(""power ="", power, ""group size ="", group_size,""\n\n"")
  # save group size and power for each iteration
  plot_df &lt;- rbind(plot_df, data.frame(""group_n"" = group_size[1], ""power"" = power))
  # increase group size with 2
  group_size &lt;- group_size + 2
}

out &lt;- list(""power"" = plot_df, ""f"" = sqrt(eta / (1 - eta)), ""omega"" = omega)
return(out)
}
sims &lt;- 1000
sim &lt;- run_sim()
</code></pre>

<p>This will generate a difference of about 10 % between the two methods (the short line is from simulation) </p>

<p><img src=""http://i.stack.imgur.com/REjgP.png"" alt=""plot""></p>

<p>My thoughts are that the difference is due to Cohen's <em>f</em> being an biased estimator of the population effect size. But how should I interpret my results, are my simulations overestimating the power? If so, how can I get it to match the output from the analytical power estimation. </p>

<p><strong>To summarize my question: why doesn't the two methods give the same output when fed with the same means and standard deviations?</strong></p>

<p>I'd be glad for any pointers were I wen't wrong. Thanks in advance!</p>
"
"0.0656079851351847","0.0669109659465237"," 30664","<p>Maybe this is an over-the-top question but I have many doubts regarding my recent analysis about deer skull measurements and how to proceed with the analysis. This is a sample of my dataset:</p>

<pre><code>   Factor1 population manage foraging height biome abundance area  forest plough 
 -0.6033788 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
  0.3250981 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
  0.5577059 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
 -0.1596194 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
 -1.3089952 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
 -2.1693392 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
 -0.9669080 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
 -1.8857842 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
  0.7242678 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
  1.6815373 ADA_BEC   best    fields   plain  agS    1500    73154  61154 12000
</code></pre>

<p>Factor1 are factor scores of all individuals (567) divided into 12 populations (population column). Others are either factor (manage (4lvl), foraging (3lvl), height (2lvl) and biome (4lvl)) or continuous, different for every population, (abundance, area (in ha), which totals forest+ploughland (also in ha)). Now I tried with all traditional uni-variate statistics such as anova, ancova, lm but, off course, my design is unballanced. My question is are there any general modeling solutions to incoporating all of these in a maximal model, simplifying it further and decide what is the most influential factor. Could mixed-effects model be used? Basic data are in fact, 50 measurements on every individual skull.   </p>
"
"0.111340442853781","0.11355167461567"," 32072","<p>Just when I thought I'd had a grip on how to do an analysis of variance this particular data set had me startled: it's a collection of response times (in ms) to a linguistic input. To be precise, it's part of a reading time experiment, and I'm trying to see if there's a significant effect of two factors.</p>

<p>My experiment had a 2x2 factorial design, with 2 factors binary <code>Conflicting</code> and <code>ContextPresent</code>. The only value I'm interested in for the purpose of this question is one particular random variable, a response time. No transformations have been done on it, except removal of outliers via 3-sigma rule (I used mean + 3 * standard deviation to determine outliers.)</p>

<p>So I run my anova in R:</p>

<pre><code>&gt; anova(lm(TextDisplay9.RT ~ Conflicting * ContextPresent, data=items.cropped))
Analysis of Variance Table

Response: TextDisplay9.RT
                            Df   Sum Sq Mean Sq F value  Pr(&gt;F)
Conflicting                  1   111185  111185  7.0591 0.00808 **
ContextPresent               1    73591   73591  4.6723 0.03102 *
Conflicting:ContextPresent   1      352     352  0.0223 0.88128
Residuals                  651 10253667   15751
</code></pre>

<p>Jolly ho, I get pretty good results for a main effect on both my factors, and no interaction. That's fine. But here's the corresponding box-and-whiskers graph:</p>

<pre><code>&gt; ggplot(items.cropped,aes(Conflicting,TextDisplay9.RT)) + geom_boxplot() +
  facet_grid(.~ ContextPresent)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R9d2V.png"" alt=""Boxplot of suspiciously equal looking stuff""></p>

<p>So this plot actually makes it seem like there <em>shouldn't</em> be a main effect of either variable! They're all too similar! Yes, the scale is rather squished, because of the outliers, but the means are really really close!</p>

<pre><code>&gt; with(items.cropped,mean(items.cropped[Conflicting==""semantic conflict"" &amp; ContextPresent == ""mentioned in context"",]$TextDisplay9.RT,na.rm=T))
    [1] 431.8659
    &gt; with(items.cropped,mean(items.cropped[Conflicting==""semantic conflict"" &amp; ContextPresent == ""not mentioned in context"",]$TextDisplay9.RT,na.rm=T))
[1] 454.5305
&gt; with(items.cropped,mean(items.cropped[Conflicting==""no semantic conflict"" &amp; ContextPresent == ""mentioned in context"",]$TextDisplay9.RT,na.rm=T))
    [1] 407.485
    &gt; with(items.cropped,mean(items.cropped[Conflicting==""no semantic conflict"" &amp; ContextPresent == ""not mentioned in context"",]$TextDisplay9.RT,na.rm=T))
[1] 427.2188
</code></pre>

<p>Does it sound possible that there could be a main effect? Or did I somehow misuse ANOVAs? I could provide the data if needed!</p>

<p>Thanks very much for any suggestions.</p>
"
"0.0642824346533225","0.0655590899062897"," 32099","<p>I need to compare results from data with different residual DF as my $x$ variable has different levels. The following is just an example (in R, for demonstration purpose, but this is not a R question):</p>

<pre><code># first case 
set.seed (123)
data1 &lt;- data.frame (y = rnorm (100, 5, 2), 
 x = sample (c(""A"", ""B""), 100, replace = T))
anova(lm(y~ x, data = data1))
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value Pr(&gt;F)
x          1   2.07  2.0669  0.6177 0.4338
Residuals 98 327.89  3.3459               

# second case: 
 set.seed (123)
data2 &lt;- data.frame (y = rnorm (100, 5, 2), 
 x = sample (c(""A"", ""B"", ""C"", ""D"", ""E""), 100, replace = T))
anova(lm(y~ x, data = data2))
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value Pr(&gt;F)
x          4   4.89  1.2224  0.3572 0.8384
Residuals 95 325.07  3.4218       
</code></pre>

<p>Here I have two different DF for the residuals (95 vs. 96) and $x$ (1 vs. 4): Is it valid to compare p-values as such? I know that the F-test considers $x$ and residual while calculating p-value. Is there any extra-caution needed? </p>
"
"0.0981930408849676","0.100143163995996"," 32248","<p>I'm trying to report an effect size for a Linear Mixed-Model we've fitted in R. Right now I'm looking at reporting partial eta squared or eta squared. However, to do so I need to calculate the Sums of Squared Error. We're using the <code>lme()</code> function, which does not report MSE or effect sizes. I am not the one doing the primary analysis, so I don't know if we can switch to using the <code>ez</code> package, as described in <a href=""http://stats.stackexchange.com/questions/2962/omega-squared-for-measure-of-effect-in-r"">Omega squared for measure of effect in R?</a>.</p>

<p>The <code>lmeObject</code> returned by the <code>lme()</code> function has some information, but I am not sure which is the most appropriate. When using <code>anova()</code> on the <code>lmeObject</code>, it reports the denominator degrees of freedom for the F-test as 56683, so with a value for MSE I can calculate SSE and reverse-engineer the F-tests to get the SStreatments I need for partial eta squared. I have 2 fixed effects and one random effect (for a repeated-measures design).</p>

<p>I've looked at <code>lmeObject$sigma</code> and calculated the sums of the squares of <code>lmeObject$residuals[,1]</code>, but they don't agree (I'm squaring the <code>sigma</code> and dividing the SS by the degrees of freedom).</p>

<p>Any R masters out there that can tell how to calculate MSE or SSE from an <code>lmeObject</code>?</p>
"
"0.104972776216296","0.107057545514438"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0","0"," 34873","<p>My experiment is a completely randomized block design.  The objective is to find whether a variable is different between species $a$, $b$, $c$. The measurement was taken 2 times (June, July) in each year (2011, 2012). I was wondering whether repeated measures ANOVA is a correct method to use? If it is, would you please help me to write up the syntax for the ANOVA and post-hoc analysis? </p>

<p>My data are like the following: </p>

<pre><code>dat &lt;- read.table(text = ""species block   year    time    variable
a   1   2011    June    1
a   2   2011    June    2
a   3   2011    June    3
b   1   2011    June    4
b   2   2011    June    5
b   3   2011    June    6
c   1   2011    June    7
c   2   2011    June    8
c   3   2011    June    9
a   1   2011    July    10
a   2   2011    July    11
a   3   2011    July    12
b   1   2011    July    13
b   2   2011    July    14
b   3   2011    July    15
c   1   2011    July    16
c   2   2011    July    17
c   3   2011    July    18
a   1   2012    June    19
a   2   2012    June    20
a   3   2012    June    21
b   1   2012    June    22
b   2   2012    June    23
b   3   2012    June    24
c   1   2012    June    25
c   2   2012    June    26
c   3   2012    June    27
a   1   2012    July    28
a   2   2012    July    29
a   3   2012    July    30
b   1   2012    July    31
b   2   2012    July    32
b   3   2012    July    33
c   1   2012    July    34
c   2   2012    July    35
c   3   2012    July    36"", header=TRUE)
</code></pre>
"
"0.123091490979333","0.125536099672233"," 35407","<p>I'm currently running a perception experiment: </p>

<ul>
<li>DV: error (this in degrees- how much an observer was away from the real answer) </li>
<li>IV: the time bin (5 levels) in which the unique stimulus appeared</li>
<li>IV2: True/False- whether the unique stimulus occurred before or after stimulus2. </li>
<li>Covariate: Distance from the fixation point this stimulus appeared (continuous)</li>
</ul>

<p>So, I have decided to use repeated measures ANCOVA since, all the observers were exposed to all 5 levels of the IV multiple times. </p>

<p>Currently I have written: </p>

<pre><code>data.aov &lt;-aov(error~(timebin*after*distance) + 
               Error(subject/timebin*after*distance), d)
summary(data.aov)
</code></pre>

<p><strong>Is this the correct way to specify my repeated measures ANCOVA in R?</strong></p>

<p>Also, I wanted to run this analysis in SPSS to check that I get the same results. However, SPSS doesn't like the format of my data. 
For repeated measures analysis, the 5 bins should be in 5 different columns, but in my data file, they are all together under the same heading <code>timebins</code>.</p>

<p><strong>How can I run the repeated measures ANCOVA in SPSS if my data are in long format?</strong></p>

<hr>

<p>Thanks everyone for great answers! </p>

<p>@Marcus, just one thing- regarding regression. 
If I were to use regression, would I be looking at trends? 
I am actually more interested in comparing bin 1 and bin 5. This is why I was going to use a t-test, but now it seems like I am using within-subjects anova. </p>

<p>Also, I've had a look at distance and (as you said) might not be a covariate, since it was randomised for every condition! </p>

<p>Then the codes should look like this? </p>

<pre><code>data.aov &lt;-aov(error~(timebin*after) + Error(subject/timebin*after), d)
summary(data.aov)
</code></pre>

<p>But what is the difference between using <code>*</code> and using <code>+</code> ? 
Should I use <code>*</code> to get an interaction effect? </p>
"
"0.0742269619025206","0.0567758373078348"," 37466","<p>I am taking a graduate course in Applied Statistics that uses the following textbook (to give you a feel for the level of the material being covered): <a href=""http://amzn.com/0471072044"">Statistical Concepts and Methods</a>, by G. K. Bhattacharyya and R. A. Johnson.</p>

<p>The Professor requires us to use SAS for the homeworks. </p>

<p>My question is that: is there a Java library(ies), that can be used instead of SAS for problems typically seen in such classes.</p>

<p>I am currently trying to make do with <a href=""http://commons.apache.org/math/"">Apache Math Commons</a> and though I am impressed with the library (it's ease of use and understandability) it seems to lack even simple things such as the ability to draw histograms (thinking of combining it with a charting library).</p>

<p>I have looked at Colt, but my initial interest died down pretty quickly. </p>

<p>Would appreciate any input -- and I've looked at similar questions on Stackoverflow but have not found anything compelling.</p>

<p>NOTE: I am aware of R, SciPy and Octave and java libraries that make calls to them -- I am looking for a Java native library or set of libraries that can together provide the features I'm looking for.</p>

<p>NOTE: The topics covered in such a class typically include: one-samle and two-sample tests and confidence intervals for means and medians, descriptive statistics, goodness-of-fit tests, one- and two-way ANOVA, simultaneous inference, testing variances, regression analysis, and categorical data analysis.</p>
"
"0.128564869306645","0.131118179812579"," 37944","<p>I am currently using the R package <a href=""http://cran.r-project.org/web/packages/lme4/lme4.pdf"">lme4</a>.</p>

<p>I am using a linear mixed effects models with random effects:</p>

<pre><code>library(lme4)
mod1 &lt;- lmer(r1 ~ (1 | site), data = sample_set) #Only random effects
mod2 &lt;- lmer(r1 ~ p1 + (1 | site), data = sample_set) #One fixed effect + 
            # random effects
mod3 &lt;- lmer(r1 ~ p1 + p2 + (1 | site), data = sample_set) #Two fixed effects + 
            # random effects
</code></pre>

<p>To compare models, I am using the <code>anova</code> function and looking at differences in AIC relative to the lowest AIC model:</p>

<pre><code>anova(mod1, mod2, mod3)
</code></pre>

<p>The above is fine for comparing models. </p>

<p>However, I also need some simple way to interpret goodness of fit measures for each model. Does anyone have experience with such measures? I have done some research, and there are journal papers on R squared for the fixed effects of mixed effects models:</p>

<ul>
<li>Cheng, J., Edwards, L. J., Maldonado-Molina, M. M., Komro, K. A., &amp; Muller, K. E. (2010). Real longitudinal data analysis for real people: Building a good enough mixed model. Statistics in Medicine, 29(4), 504-520. doi: 10.1002/sim.3775  </li>
<li>Edwards, L. J., Muller, K. E., Wolfinger, R. D., Qaqish, B. F., &amp; Schabenberger, O. (2008). An R2 statistic for fixed effects in the linear mixed model. Statistics in Medicine, 27(29), 6137-6157. doi: 10.1002/sim.3429  </li>
</ul>

<p>It seems however, that there is some criticism surrounding the use of measures such as those proposed in the above papers.</p>

<p>Could someone please suggest a few easy to interpret, goodness of fit measures that could apply to my models?  </p>
"
"0.0642824346533225","0.0655590899062897"," 40507","<p>This is my first question is this forum. I have the a problem that I want to solve with the <code>lme</code> function of the <code>nlme</code> package of <code>R</code>. </p>

<p>The goal of my study is analyse if the cholesterol level (quantitative) change in a series of patients depending of 3 different treatments and the genetic profile (SNPs, a factor variable with three categories). We have two repeated measures of the cholesterol (before the intervention and 1 year after).</p>

<p>The general question is to know if some of the treatment ""works"" (change in the cholesterol) for some of the 3 categories of the SNP.</p>

<p>My analysis in <code>R</code> is the next, but I don't know if it is correct:</p>

<pre><code>w.lme.1 &lt;- lme( colesterol ~  tiempo * treatment + SNP ,
              random = ~1 | ID , data=xx, na.action=na.omit, method=""ML"")

w.lme.2 &lt;- lme( colesterol ~  tiempo * treatment * SNP ,
              random = ~1 | ID , data=xx, na.action=na.omit, method=""ML"")

anova(w.lme.1, w.lme.2)
</code></pre>

<p>where:</p>

<ul>
<li>ID , identification of the individuals</li>
<li>time, 2 factors : baseline time and 1-year after</li>
<li>treatment, 3 levels</li>
<li>SNP, 3 levels</li>
</ul>

<p>R output:</p>

<pre><code>&gt; head(xx)
   ID colesterol tiempo treatment SNP
1   1     312.47      0         1   1
2   1     221.61     12         1   1
3  10     221.33      0         1   1
4  10     227.67     12         1   1
5 100     229.07      0         2   1
6 100         NA     12         2   1
&gt; str(xx)
'data.frame':   594 obs. of  5 variables:
 $ ID        : int  1 1 10 10 100 100 101 101 102 102 ...
     $ colesterol: num  312 222 221 228 229 ...
 $ tiempo    : Factor w/ 2 levels ""0"",""12"": 1 2 1 2 1 2 1 2 1 2 ...
     $ treatment : Factor w/ 3 levels ""1"",""2"",""3"": 1 1 1 1 2 2 1 1 3 3 ...
 $ SNP       : Factor w/ 3 levels ""0"",""1"",""2"": 2 2 2 2 2 2 1 1 1 1 ...
</code></pre>
"
"0.111340442853781","0.11355167461567"," 41123","<p>I feel overwhelmed after attempting to dig into the literature on how to run my mixed model analysis following it up with using AIC to select the best model or models.  I do not think my data is that complicated, but I am looking for confirmation that what I have done is correct, and then advise on how to proceed.  I am unsure if I should be using lme or lmer and then with either of those, if I should be using REML or ML.</p>

<p>I have a value of selection and I want to know which covariates best influence that value and allow for predictions.  Here's some made up example data and my code for my test that I am working with:</p>

<pre><code>ID=as.character(rep(1:5,3))
season=c(""s"",""w"",""w"",""s"",""s"",""s"",""s"",""w"",""w"",""w"",""s"",""w"",""s"",""w"",""w"")
time=c(""n"",""d"",""d"",""n"",""d"",""d"",""n"",""n"",""n"",""n"",""n"",""n"",""d"",""d"",""d"")
repro=as.character(rep(1:3,5))
risk=runif(15, min=0, max=1.1)
comp1=rnorm(15, mean = 0, sd = 1)
mydata=data.frame(ID, season, time, repro, risk, comp1)
c1.mod1&lt;-lmer(comp1~1+(1|ID),REML=T,data=mydata)
c1.mod2&lt;-lmer(comp1~risk+(1|ID),REML=T,data=mydata)
c1.mod3&lt;-lmer(comp1~season+(1|ID),REML=T,data=mydata)
c1.mod4&lt;-lmer(comp1~repro+(1|ID),REML=T,data=mydata)
c1.mod5&lt;-lmer(comp1~time+(1|ID),REML=T,data=mydata)
c1.mod6&lt;-lmer(comp1~season+repro+time+(1|ID),REML=T,data=mydata)
c1.mod7&lt;-lmer(comp1~risk+season+season*time+(1|ID),REML=T,data=mydata)
</code></pre>

<p>I have ~19 models that explore this data with various combinations and up to a 2 way interaction terms, but always with ID as a random effect and comp1 as my dependent variable.  </p>

<ul>
<li>Q1. Which to use? lme or lmer? does it matter?</li>
</ul>

<p>In both of these, I have the option to use ML or REML - and I get drastically different answers - using ML followed by AIC I end up with 6 models all with similar AIC values and the model combinations simply do not make sense, whereas REML results in 2 of the most likely models being the best.  However, when running REML I cannot use anova any longer.  </p>

<ul>
<li>Q2. is the main reason to use ML over REML because of use with ANOVA?
This is not clear to me.</li>
</ul>

<p>I am still not able to run stepAIC or I do not know of another way to narrow down those 19 models.</p>

<ul>
<li>Q3. is there a way to use stepAIC at this point?</li>
</ul>
"
"0.117363131703255","0.11969397463728"," 41925","<p>I want to check whether a group of patients are significantly different from their control group. However, I also want to check if the p-value is still significant in case a covariate is taken into account. For that I created a simple data frame in R:</p>

<pre><code>data &lt;- data.frame(group = c(rep(""CTRL"", 10), rep(""P"", 10)), 
    response = c(10,11,14,16,17,17,19,20,21,22, 10,11,11,11,12,13,14,14,15,16),
    age = c(40,41,45,43,50,51,55,57,60,62, 40,42,43,43,45,46,46,50,52,54))
</code></pre>

<p>First of all, I performed a normal ANOVA using the <code>lm</code> function. I tried 3 different ways to see if the results are the same. And they are:</p>

<pre><code>anova(lm(data$response ~ data$group))

summary(lm(data$response ~ data$group))

summary(aov(data$response ~ data$group)
</code></pre>

<p>To check if the covariate <code>age</code> is correlated with the <code>response</code> variable I performed a correlation test:</p>

<pre><code>cor.test(data$response, data$age)
</code></pre>

<p>Seeing a high and significant correlation I concluded that the significance between the control group and the patient group might be due to the effect of age.</p>

<p>I am now unsure how to perform the ANCOVA analysis to check if the effect between the two groups is really there or if it is just because of the covariate age.</p>

<p>To check this I did the following:</p>

<pre><code>m1 &lt;- lm(data$response ~ data$group + data$age)

m2 &lt;- lm(data$response ~ data$group)

anova(m1, m2)
</code></pre>

<p>Comparing the two models resulted in a significant p-value. I would have thought of a p-value of much less significant then the one obtained with the normal anova. Is it actually right to compare the two models or can I achieve this by using only one model? I am really stuck here and hoping to find some helpful answers here.</p>
"
"0.111340442853781","0.100934821880595"," 43361","<p>As a follow-up to <a href=""http://stats.stackexchange.com/questions/41390/test-for-effects-of-two-categorical-variables-on-a-binary-response-variable"">this question</a>, I have the following data:</p>

<pre><code>   Site Treatment Survival
1   BED        DN      1.0
2   BED        DN      1.0
3   BED        DN      1.0
4   BED        MB      1.0
5   BED        MB      1.0
6   BED        MB      0.9
7   BED    Forest      0.4
8   BED    Forest      0.5
9   BED    Forest      0.4
10  BRO        DN      0.9
11  BRO        DN      1.0
12  BRO        DN      1.0
13  BRO        MB      1.0
14  BRO        MB      1.0
15  BRO        MB      1.0
16  BRO    Forest      1.0
17  BRO    Forest      1.0
18  BRO    Forest      1.0
19  LAP        DN      0.8
20  LAP        DN      0.4
21  LAP        DN      0.6
22  LAP        MB      0.5
23  LAP        MB      1.0
24  LAP        MB      0.7
25  LAP    Forest      0.2
26  LAP    Forest      0.2
27  LAP    Forest      0.4
</code></pre>

<p>on which I ran a binomial glm :</p>

<pre><code>&gt; glm.out &lt;- glm(Survival~Site*Treatment, data=surv,
    family=""binomial"", weights=rep(10, nrow(surv)))
&gt; anova(glm.out, test=""Chisq"")

Analysis of Deviance Table
Model: binomial, link: logit
Response: Survival
Terms added sequentially (first to last)

               Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|)    
NULL                              26    138.254              
Site            2   63.098        24     75.155 1.988e-14 ***
Treatment       2   42.991        22     32.164 4.620e-10 ***
Site:Treatment  4   13.874        18     18.290  0.007707 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>All effects are significant, so now I want to do post-hoc comparisons.  I have discovered the <code>glht</code> function in the multcomp package, which seems to do what I want.  I read somewhere that with 2 independent variables, you should set <code>interaction_average=TRUE</code> to get a result equivalent to a TukeyHSD for a linear model.</p>

<pre><code>&gt; Treat.comp &lt;- glht(glm.out, mcp(Treatment=""Tukey"", interaction_average=TRUE))
&gt; summary(Treat.comp)
</code></pre>

<p>which gives me these strange results:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses
Multiple Comparisons of Means: Tukey Contrasts
Fit: glm(formula = Survival ~ Site * Treatment, family = ""binomial"", 
    data = surv.san, weights = rep(10, nrow(surv.san)))

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)
DN - MB == 0       -0.202   3316.127   0.000        1
Forest - MB == 0   -1.886   3316.127  -0.001        1
Forest - DN == 0   -1.684   3316.127  -0.001        1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>Why are my P-values = 1 for all comparisons even though the Treatment effect was highly significant?  </p>

<p>If I try using <code>glht</code> with <code>interaction_average=FALSE</code> I get more reasonable results but with a warning message:</p>

<pre><code>Warning message:
In mcp2matrix(model, linfct = linfct) :
  covariate interactions found -- default contrast might be inappropriate
</code></pre>

<p>Can someone help me to understand what I am doing wrong?  Thank-you!!!</p>
"
"0.0642824346533225","0.0655590899062897"," 44387","<p>I have ran a nested ANOVA on a data set and the results suggest that there is an association for the higher level with an insignificant P-value but also an association for the lower level with a significant P-value. Is this even possible, or have I done something completely wrong?</p>

<p><em>Details</em></p>

<p>I have a data set comprising of: Groups (A,B,C...) - each group contains 3 different genotypes (1,2,3...). The same genotype doesn't occur in two different groups.  For example, I have group A with genotypes 1,2,3 and Group B with genotypes 4,5,6, and group C with genotypes 7,8,9. I then have several measurements of height for each genotype. I had to perform an analysis that can test for differences between the three groups as well as for the genotypes within each of the groups. I chose nested ANOVA, but now I'm slightly doubting myself. A huge thank you for any help!</p>
"
"NaN","NaN"," 44581","<p>I have two generalized linear models <code>mod1</code> and <code>mod2</code>. </p>

<pre><code>mod1 &lt;- glm(Score ~ Height + Gene, data=mydata, family=binomial)

mod2 &lt;- glm(Score ~ Height * Gene, data=mydata, family=binomial)
</code></pre>

<p>I want to perform an analysis of deviance to test the significance of the interaction term.</p>

<p>At first I did <code>anova(mod1,mod2)</code>, and I used the function <code>1 - pchisq()</code> to obtain a p-value for the deviance result I got from the anova table.</p>

<p>I also did another test: <code>anova(mod2, test=""Chisq"")</code>. This gave a table for all the terms added sequentially first to last, and I obtained a very different p-value  value for the interaction term <code>Height:Gene</code>...</p>

<p><strong>Which one should I use?</strong></p>
"
"0.10562681853293","0.0957551797098243"," 45264","<p>An R cookbook <a href=""http://wiki.stdout.org/rcookbook/Statistical%20analysis/ANOVA/"" rel=""nofollow"">http://wiki.stdout.org/rcookbook/Statistical%20analysis/ANOVA/</a> has an example of using aov() for mixed design ANOVAs.</p>

<p>I'll copy it here:</p>

<pre><code>data &lt;- read.table(header=T, con &lt;- textConnection('
 subject sex   age before after
       1   F   old    9.5   7.1
       2   M   old   10.3  11.0
       3   M   old    7.5   5.8
       4   F   old   12.4   8.8
       5   M   old   10.2   8.6
       6   M   old   11.0   8.0
       7   M young    9.1   3.0
       8   F young    7.9   5.2
       9   F   old    6.6   3.4
      10   M young    7.7   4.0
      11   M young    9.4   5.3
      12   M   old   11.6  11.3
      13   M young    9.9   4.6
      14   F young    8.6   6.4
      15   F young   14.3  13.5
      16   F   old    9.2   4.7
      17   M young    9.8   5.1
      18   F   old    9.9   7.3
      19   F young   13.0   9.5
      20   M young   10.2   5.4
      21   M young    9.0   3.7
      22   F young    7.9   6.2
      23   M   old   10.1  10.0
      24   M young    9.0   1.7
      25   M young    8.6   2.9
      26   M young    9.4   3.2
      27   M young    9.7   4.7
      28   M young    9.3   4.9
      29   F young   10.7   9.8
      30   M   old    9.3   9.4
'))
close(con)
</code></pre>

<p>Then reshape it:</p>

<pre><code>library(reshape2)

# Make sure subject column is a factor
data$subject &lt;- factor(data$subject) 

# Convert it to long format
data.long &lt;- melt(data, id = c(""subject"",""sex"",""age""), # keep these columns the same
              measure = c(""before"",""after""),       # Put these two columns into a new column
              variable.name=""time"")                # Name of the new column

# subject sex   age   time value
#       1   F   old before   9.5
#       2   M   old before  10.3
#...
</code></pre>

<p>Now analyze using a mixed anova:</p>

<pre><code>aov.after.age.time &lt;- aov(value ~ age*time + Error(subject/time), data=data.long)
summary(aov.after.age.time)
</code></pre>

<p>But when there are more than two predictor variables, the R examples show that the between subject factors are added again after the error term:</p>

<pre><code>#e.g., from R cookbook
#aov.bww &lt;- aov(y ~ b1*b2*w1 + Error(subject/(w1)) + b1*b2, data=data.long)

# which would translate in our case as:
aov.bww &lt;- aov(value ~ sex*age*time + Error(subject/time) + sex*age, data=data.long)
summary(aov.bww)
</code></pre>

<p>But why is b1*b2, or in our case sex*age, specified twice? It doesn't seem to make a difference when we remove them after the Error() term:</p>

<pre><code>aov.bww2 &lt;- aov(value ~ sex*age*time + Error(subject/time), data=data.long)
summary(aov.bww2)
</code></pre>

<p>Can anyone explain why the examples have those extra terms? The R manual just has this example, where the between factors are not specified twice:</p>

<pre><code># fm &lt;- aov(yield ~ v + n*p*k + Error(farms/blocks), data=farm.data)
</code></pre>

<hr>

<p><strong>Edit:</strong></p>

<p>I have checked the references from the R Cookbook and found other web sites also specify the terms twice in their mixed design examples. See here:
<a href=""http://www.personality-project.org/R/r.anova.html"" rel=""nofollow"">http://www.personality-project.org/R/r.anova.html</a>
where they have the example:</p>

<pre><code>aov.ex5 = aov.ex5 = aov(Recall ~ (Task*Valence*Gender*Dosage) +
Error(Subject/(Task*Valence)) + (Gender*Dosage), data.example5 )
</code></pre>

<p>and see here
<a href=""http://www.statmethods.net/stats/anova.html"" rel=""nofollow"">http://www.statmethods.net/stats/anova.html</a>
with their example:</p>

<pre><code># Two Within Factors W1 W2, Two Between Factors B1 B2
fit &lt;- aov(y ~ (W1*W2*B1*B2) + Error(Subject/(W1*W2)) + (B1*B2),
data=mydataframe)
</code></pre>

<p>Which is presumably where the cookbook got their info from.</p>
"
"0.139175553567226","0.151402232820893"," 45377","<p>My question is, how to get effect sizes for a linear mixed model?</p>

<p>I am using the following model in SPSS:</p>

<pre><code>MIXED
  transfer  BY distance training rotation sequence  WITH pretest
  /CRITERIA = CIN(95) MXITER(100) MXSTEP(5) SCORING(1) SINGULAR(0.000000000001) 
     HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)
  /FIXED = pretest rotation sequence distance training distance*training  | SSTYPE(3)
  /METHOD = REML
  /REPEATED = distance | SUBJECT(ResponseID) COVTYPE(CS) .
</code></pre>

<p>I've also done it in R, where it looks like this:</p>

<pre><code>library( nlme )
options(contrasts=c(""contr.sum"",""contr.poly""))
lm1 &lt;- lme(transfer ~ training * distance + rotation + sequence + pretest, 
           random=~1|ResponseID, method=""REML"", data=wide.data )
fit &lt;- anova.lme(lm1,type='marginal')
print( fit )
</code></pre>

<p>The dv, transfer, measures pretest to posttest improvement for a given level of distance. Distance is a categorical within-subjects variable with either 2 or 3 levels depending on the dataset. Training is a categorical between-subjects variable with either 2 or 4 levels depending on the dataset. Rotation and sequence are binary categorical between-subjects variables. Pretest is a covariate with different values for each level of distance.</p>

<p>I've already submitted the results of the analysis for publication, and got a reviewer comment asking for odds ratios. Ideally I'd like to get odds ratios for each effect in the model. If I can't get odds ratios, some other measure of effect sizes might be OK. Most important, I need to get it for the training * distance interaction and the covariate (pretest) because these are the only significant effects.</p>

<p>Ideally I'd get it straight from SPSS but as far as I can see, SPSS can't do it. I couldn't figure out how to do it in R either. Second best would be to put the output of one of these into some other software that could calculate it for me. I found some free software that could compute eta-squared for regular mixed ANOVA but not for linear mixed model.</p>

<p>Other questions about effect sizes seem to relate to other models, not LMMs. I did see another similar question about LMMs which is currently unanswered.</p>

<p>Any ideas?</p>
"
"0.111340442853781","0.11355167461567"," 46259","<p>My data set has the following variables:</p>

<ul>
<li>Treatment (4 types- fixed) </li>
<li>Location (8 locations- fixed)</li>
<li>Position in Location (3 positions per location- fixed)</li>
<li>Samples are taken in each position (3 samples per position-random)
<ul>
<li>Time (two sampling times - fixed)</li>
<li>Mineralisation rate (as result of analysis of samples taken)</li>
</ul></li>
</ul>

<p>Two locations are used to test each treatment (ie 4 treatments, 2 locations per treatment, 8 locations total). </p>

<p>I want to do a split-plot (/nested?) repeated measures (/mixed model?) ANOVA in <code>R</code> using the above variables. </p>

<p>Q.1. Does this sound suitable? </p>

<p>My goal is to see if there is an affect of 
1) position, 
2) treatment, 
3) time and 
4) interaction of all (ie pos*treat*, pos*time, treat*time, pos*treat*time) 
on mineralization rates. </p>

<p>Q 2. Is location nested in treatment? Is sample nested in position?</p>

<p>Q 3. What are the between- and within- factors? </p>

<p>Q 4. What is the subject/plot? 
    - Is it the location or position or sample or rate? </p>

<p>Q 5. How can I put time as repeated measures in my R formula?</p>

<p>Q 6. Would you use aov, lme, or ezANOVA?</p>

<p>Q 7. How do I code the seperate independent variables, and their interactions into a proper R formula?</p>

<p>I have literally been trying to figure this out for days and I cannot seem to find an answer that makes sense... </p>
"
"0.138865930150177","0.141623820702091"," 47692","<p>I have a run an unbalanced 2x2x2x2 Type II ANOVA in R and am having trouble following up on the results. Here is the output:</p>

<pre><code>       Effect DFn DFd             F           p p&lt;.05             ges
2           cond   1 127  3.2359349424 0.074414031       0.0110769653158
3             sf   1 127  1.6981345415 0.194889782       0.0058436648717
5             ba   1 127  1.5404865586 0.216833055       0.0012264293759
9             tt   1 127  1.9253260611 0.167700584       0.0054448755666
4        cond:sf   1 127  0.1846599042 0.668127012       0.0006387833954
6        cond:ba   1 127  5.8799698820 0.016721105     * 0.0046651103251
7          sf:ba   1 127  1.4992638464 0.223051114       0.0011936498636
10       cond:tt   1 127  0.5266890712 0.469337439       0.0014954062256
11         sf:tt   1 127  0.0768302867 0.782090431       0.0002184199961
13         ba:tt   1 127 11.5004885802 0.000927851     * 0.0087996011237
8     cond:sf:ba   1 127  0.0042138896 0.948344162       0.0000033589171
12    cond:sf:tt   1 127  0.1197411309 0.729888009       0.0003403692520
14    cond:ba:tt   1 127  0.3878677814 0.534539033       0.0002993221940
15      sf:ba:tt   1 127  0.0001339682 0.990783282       0.0000001034158
16 cond:sf:ba:tt   1 127  0.4820119706 0.488780454       0.0003719473651
</code></pre>

<p><strong>cond</strong> and <strong>sf</strong> are between-subjects factors. <strong>ba</strong> and <strong>tt</strong> are within (or repeated). The unbalanced nature of the experiment has meant that I have used a type II anova.</p>

<p>You can see that we have a suggestively significant main effect of <strong>cond</strong> (2) and two significant interactions (6 &amp; 13). I have graphed the interactions and they seem logical, but I am sure that <strong>cond</strong> is contributing somewhat to the interactions.</p>

<p>I am at a loss at how to proceed. I suppose I wish to do some kind of post-hoc analysis concentrating on the interactions. I have investigated a number of different R packages (afex, phia, contrasts etc), but have yet to work out what I am actually doing with these interactions. </p>

<p>My data looks like this:</p>

<pre><code>str(xx)
'data.frame':   524 obs. of  6 variables:
$ ba  : Factor w/ 2 levels ""before"",""after"": 1 1 1 1 1 1 1 1 1 1 ...
    $ tt  : Factor w/ 2 levels ""targ"",""calm"": 1 1 1 1 1 1 1 1 1 1 ...
$ p   : Factor w/ 131 levels ""1"",""2"",""3"",""4"",..: 4 8 9 10 13 18 19 22 25 29 ...
    $ cond: Factor w/ 2 levels ""Control"",""Spider"": 1 1 1 1 1 1 1 1 1 1 ...
$ sf  : Factor w/ 2 levels ""Fear"",""No-Fear"": 1 1 1 1 1 1 1 1 1 1 ...
    $ eda : num  1.478 -0.56 -0.27 -0.902 -0.483 ...
</code></pre>

<p>Moreover consider <a href=""http://books.google.co.uk/books/about/Foundations_of_Behavioral_Statistics.html?id=8sLOa8vHl7YC"" rel=""nofollow"">Thompson (2006)</a> :</p>

<blockquote>
  <p>As noted by Rosnow and Rosenthal (1989a), the cell means â€œare the
  combined effects of the interaction, the row effects [a main effect],
  the column effects [a second main effect], and the grand meanâ€ (p.
  144). By the same token, simple post hoc tests of the cell means also
  do not yield insight about the origins of interaction effects,
  because the interaction effects are not uniquely a function of the
  cell means (Boik, 1979).</p>
</blockquote>

<p>So I guess post-hoc t-tests (with adjusted p-values) are out? </p>

<p><strong>Update:</strong> Following advice I have <em>found</em> from <a href=""http://stats.stackexchange.com/users/442/henrik"">@henrik</a> (<a href=""https://groups.google.com/forum/?fromgroups=#!topic/ez4r/RpwYT6pEva0"" rel=""nofollow"">here</a>) I have been investigating the phia package and the <strong>testInteractions</strong> function. I have been getting some results (for a type III anova - so not the type II I am after) but, again, I am way out of my depth here:</p>

<p>e.g., </p>

<pre><code>&gt; testInteractions(m2[[""lm""]], pairwise = ""ba"", ""cond"", idata = m2[[""idata""]],     adjustment = ""none"")              
Multivariate Test: Pillai test statistic
P-value adjustment method: none
                      Value Df test stat approx F num Df den Df  Pr(&gt;F)  
after-before : Control -0.31910  1  0.045305   6.0268      1    127 0.01544 *
after-before :  Spider  0.14243  1  0.008045   1.0300      1    127 0.31208  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><strong>Latest Update:</strong></p>

<p>I seem to be getting further bogged down with this. Using lmer as referenced <a href=""http://www.uni-kiel.de/psychologie/rexrepos/Univariate/ANOVA/anovaMixed.html"" rel=""nofollow"">here</a> gives this:</p>

<pre><code>fit.1 &lt;-lmer(eda ~ 1 + cond * sf * ba *tt +(ba *tt | p), xx)
</code></pre>

<p>which leads to:</p>

<pre><code>&gt; anova(fit.1)
Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
cond           1 0.00330 0.00330  0.0158
sf             1 0.04236 0.04236  0.2035
ba             1 0.07408 0.07408  0.3559
tt             1 0.34651 0.34651  1.6649
cond:sf        1 0.05633 0.05633  0.2707 
cond:ba        1 2.38017 2.38017 11.4359
sf:ba          1 0.45349 0.45349  2.1789
cond:tt        1 0.03832 0.03832  0.1841
sf:tt          1 0.03197 0.03197  0.1536
ba:tt          1 2.39403 2.39403 11.5025
cond:sf:ba     1 0.02595 0.02595  0.1247 
cond:sf:tt     1 0.00667 0.00667  0.0320
cond:ba:tt     1 0.08078 0.08078  0.3881
sf:ba:tt       1 0.00003 0.00003  0.0001
cond:sf:ba:tt  1 0.10034 0.10034  0.4821
</code></pre>

<p>Which seems to be confirming the earlier finding of a significant interaction between cond * ba and ba * tt (but is it type II?). Still not sure if any of this is correct.</p>

<p><strong>To clarify then: I am looking for advice about what to do next, in terms of understanding the significant interactions.</strong></p>
"
"0.0829882662886615","0.0846364211331916"," 47816","<p>I'm trying to look for difference in timing (ie. earlier/later) in a variable measured at regular intervals between two groups.</p>

<p>This seems like a simple experimental design, and working in R, I'm able to visualize the data in a way that makes sense to me, but somehow I'm getting confused when it comes to testing for  significance.</p>

<p>The data consist of weekly measurements of number of flowers for each individual, within and outside of the greenhouse. To take a small example:</p>

<pre><code>expand.grid(week=(1:6),treatment=c(""greenhouse"",""outside""),individual=1:2)-&gt;df
c(0,3,10,2,0,0,0,0,0,2,18,0,0,1,19,0,0,0,0,0,1,2,15,1)-&gt;flowers
data.frame(cbind(df,flowers))-&gt;df
</code></pre>

<p>Visually,</p>

<pre><code>qplot(week,flowers,data=df,facets=treatment~.)
</code></pre>

<p>If my interest is simply to determine whether there's a significant difference in the time of flowering between the treatments; should I be doing a repeated measures ANOVA and looking at the interaction?</p>

<p>Simplifying (?) the problem even further, what if I remove the quantity of flowers, and just consider how many individuals are flowering? So the summarized data would be</p>

<pre><code>ddply(df, .(treatment,week), function(d) length(d[d$flowers&gt;0,""flowers""]))-&gt;indiv
</code></pre>

<p>Which looks like this:</p>

<pre><code> qplot(week,V1,data=indiv,facets=treatment~.)
</code></pre>

<p>Here, my first thought was that I can just think of these as two distributions, and compare with a t-test; however, only individuals and not individualsxweek are independent, so perhaps this should also be a repeated measures ANOVA? Or do I need to venture into the world of more complex time-series math?</p>

<p>Thanks for your help!</p>

<p><strong>Edit</strong>
As an update, I'm now also considering failure-time / survival analysis as a possible appropriate method.</p>
"
"0.0757575757575758","0.092714554082312"," 48455","<p>I have 1 categorical factor (3 treatments) and 1 continuous factor (weight) and then I have 5 continuous response variables.</p>

<p>From what I have read, I should not use a two way ANOVA as one of the factors is continuous.  Is this correct? Should I be using a Multiple Regression instead?</p>

<p>I was advised that I can use ANOVA, but I'm not sure if this is correct based on what I have read.  I could convert the continuous factor to categorical, but I have also read on this site that this is not the preferred option.</p>

<p>My aim with the data is to see if there is a significant difference between the 3 treatments in regards to the response variables, which would be a standard one-way ANOVA, but I also want to see if weight effects the response variables.  </p>

<p>My analysis will be with R.</p>
"
"NaN","NaN"," 49292","<p>According to <a href=""http://blog.gribblelab.org/2009/03/09/repeated-measures-anova-using-r/"" rel=""nofollow"">this post</a> the best way to do post hoc analysis in ANOVA in R is t-test + ptukey. Because lme + glht ""essentially assumes the sphericity assumption holds"".</p>

<p>How are t-test + ptukey ""concerned about the sphericity""? What are the disadvantages of this method?</p>
"
"0.0371134809512603","0.0378505582052232"," 49924","<p>I'm learning R and trying to understand how <code>lm()</code> handles factor variables &amp; how to make sense of the ANOVA table. I'm fairly new to statistics, so please be gentle with me.</p>

<p>Here's some movie data from Rotten Tomatoes. I'm trying to model the score of each movie based on the mean scores for all of the movies in 4 groups: those rated G, PG, PG-13, and R.</p>

<pre><code>download.file(""http://www.rossmanchance.com/iscam2/data/movies03RT.txt"", destfile = ""./movies.txt"")
movies &lt;- read.table(""./movies.txt"", sep = ""\t"", header = T, quote = """")
lm1 &lt;- lm(movies$score ~ as.factor(movies$rating))
anova(lm1)
</code></pre>

<p>and the ANOVA output:</p>

<pre><code>## Analysis of Variance Table
## 
## Response: movies$score
##                           Df Sum Sq Mean Sq F value Pr(&gt;F)
## as.factor(movies$rating)   3    570     190    0.92   0.43
## Residuals                136  28149     207
</code></pre>

<p>I understand how to get all the numbers in this table, EXCEPT <code>Sum Sq</code> and <code>Mean Sq</code> for <code>as.factor(movies$rating)</code>. Can someone please explain how that <code>Sum Sq</code> is calculated from my data? I know that <code>Mean Sq</code>is just <code>Sum Sq</code> divided by <code>Df</code>.</p>
"
"0.178562318481451","0.174824239750106"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0524863881081478","0.0267643863786095"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"0.0909090909090909","0.092714554082312"," 51826","<p>I have a question on how a statistician would normally interpret an anova output. Say I have anova output from R.</p>

<pre><code>&gt; summary(fitted_data)

Call:
lm(formula = V1 ~ V2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.74004 -0.33827  0.04062  0.44064  1.22737 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.11405    0.32089   6.588  1.3e-09 ***
V2           0.03883    0.01277   3.040  0.00292 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.6231 on 118 degrees of freedom
Multiple R-squared: 0.07262,    Adjusted R-squared: 0.06476 
F-statistic:  9.24 on 1 and 118 DF,  p-value: 0.002917 

&gt; anova(fit)
Analysis of Variance Table

Response: V1
           Df Sum Sq Mean Sq F value   Pr(&gt;F)   
V2          1  3.588  3.5878  9.2402 0.002917 **
Residuals 118 45.818  0.3883                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>From the above, I guess the most important value is Pr(>F), right? So this Pr, is less than 0.05 (95% level). How should my ""explain"" this? Do I explain it in ""association"", ie, V2 and V1 are associated (or not) ? or in terms of ""significance""? I always felt that I couldn't understand when people say ""This value is significant...."". So what is ""significant""? Is there a more intuitive form of explanation? like ""I am 95% confident that ...."" . </p>

<p>Also, is the Pr value the only important piece of information? or can i also look at residuals and the rest of the output to ""explain"" the result? thanks</p>
"
"0.0829882662886615","0.0846364211331916"," 52516","<p>I have a data set with the following:</p>

<p>N = 60;
x = developmental stage (range 25 to 44);
y = proportion of 10 minute trial performing a behavior (range 0 to 0.81; 30 zeros)</p>

<p>A scatterplot produces a quadratic looking curve where those in mid-development clearly performed the behavior for more time. Most of the zeros are in the youngest and oldest individuals. If I break up the data into 5 groups according to developmental stage, an ANOVA/Tukey strongly supports this pattern. However, I would like to analyze this data continuously without breaking it into groups.</p>

<p>I have considered arcsine square root transformed proportion data in a linear regression, but I am unsure if that can incorporate a quadratic term, and this analysis results in a very small R squared value (less than 0.1). I have also considered arcsine square root transformed proportion data in a GLM containing a quadratic term or a beta regression (zeros??), but am not sure where to go from here.</p>

<p>I am planning to say in the paper that the individuals in mid-development perform the behavior more than those in early or late development, but am struggling to interpret the data in a way that supports that statement.</p>

<p>I appreciate any suggestions, thank you!</p>
"
"0.070417879021953","0.0718163847823682"," 53312","<p>From the documentation for <code>anova()</code>: </p>

<blockquote>
  <p>When given a sequence of objects, â€˜anovaâ€™ tests the models against one another in the order specified...</p>
</blockquote>

<p>What does it mean to test the models against one another? And why does the order matter?</p>

<p>Here is an example from the <a href=""http://www.genabel.org/sites/default/files/pdfs/GenABEL-tutorial.pdf"">GenABEL tutorial</a>:</p>

<pre><code>    &gt;  modelAdd = lm(qt~as.numeric(snp1))
    &gt;  modelDom = lm(qt~I(as.numeric(snp1)&gt;=2))
    &gt;  modelRec = lm(qt~I(as.numeric(snp1)&gt;=3))
     anova(modelAdd, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ as.numeric(snp1)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(&gt;Chi)
    1   2372 2320                      
    2   2371 2320  1    0.0489     0.82
     anova(modelDom, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ I(as.numeric(snp1) &gt;= 2)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(&gt;Chi)
    1   2372 2322                      
    2   2371 2320  1      1.77     0.18
     anova(modelRec, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ I(as.numeric(snp1) &gt;= 3)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(&gt;Chi)  
    1   2372 2324                        
    2   2371 2320  1      3.53    0.057 .
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>How do I interpret these output?
Thanks!</p>
"
"0.111340442853781","0.0883179691455209"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.0742269619025206","0.0757011164104465"," 57234","<p>I am evaluating <em>the effect of an intervention (treatment: a lecture) on the ability to spot behavioral clues of emotions using visual stimuli</em> using <strong>pretest-posttest plus control group</strong> experimental design.</p>

<p>The experimental group is given the first set of stimuli, then there is intervention and the second set of stimuli follows. Naturally, the same without the intervention occurs in the case of the control group. <strong>Two groups, two measurements each.</strong></p>

<p><strong>1. How to determine the number of subjects needed in each group?</strong>
(Ideally using <strong>G*Power 3</strong> â€“ my problem is that I literally do not understand which method should I pick [I guess ANOVA: Repeated measures, between factors] and on what basis are the desired <strong>power</strong> and <strong>effect size</strong> determined.)</p>

<p>Suppose that I have got data from 30 experimental and 20 control subjects. The most appropriate method to use seems to be the <strong>rANOVA</strong> (<em>GLM -> Repeated Measures</em> in SPSS).</p>

<p>Data example:</p>

<pre><code>    group    pre    post
1     exp     10      15
2     exp      8       9
3     con      8       7
4     con      5       6
</code></pre>

<p><strong>2. How to perform the same analysis in R?</strong></p>

<p><strong>3. After the analysis, which data are best to describe the significance of the intervention?</strong></p>

<p>Apparently, I am a total newbie and indeed a confused one because for the last two days, I have been searching and found many contradictory claims, suggesting <strong>two paired t-tests</strong> or <strong>Mann-Whitney U</strong>, without discussing the procedure of choosing appropriately sized groups in a practical way.</p>

<p>Thanks for any help, improving suggestions are welcome.</p>
"
"0.0909090909090909","0.092714554082312"," 58225","<p>I am conducting a psycholinguistic experiment. Each trial consists of the subject responding to a word by pressing a button.The design of my experiment is as follows:</p>

<p>5 blocks of a 100 trials each (each trial is a response). In each block, 50 trials are <code>Regular</code> and 50 are <code>Random</code>. Response time (RT) is the dependent variable. This experiment is partly a learning experiment. I expect RTs to be faster in the <code>Regular</code> condition because there is learning which allows for faster responses.     </p>

<p>I therefore want to test the difference in mean response time (RT) between the two conditions of my experiment.I know that the model for the ANOVA would be something like </p>

<pre><code>RTs ~ Type of Trial (`Regular` and `Random`) + ...
</code></pre>

<p>I have two related questions</p>

<ol>
<li><p>Should I use <code>Trial</code> or <code>Block</code> as the other factor? (or both?) Normally I would compare mean RT for each condition across blocks, but it is possible that within blocks, there is already changes in RT between trials. So, is <code>Block</code>, <code>Trial</code> or both my factor? (besides the condition factor). </p></li>
<li><p>Since I expect variation across participants, should I include <code>Subject</code> as an Error variable in the model?</p></li>
</ol>

<p>I am doing my analysis in R, in case anyone wants/can provide some advice in that format, but also general statistical advise is much appreciated.</p>
"
"0.165976532577323","0.160809200153064"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.157459164324443","0.160586318271657"," 58745","<p>EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.</p>

<p>The experiments I need to analyze look like this: </p>

<ul>
<li>Subjects were assigned to one of several treatment groups</li>
<li>Measurements of each subject were taken on multiple days</li>
<li>So:
<ul>
<li>Subject is nested within treatment</li>
<li>Treatment is crossed with day</li>
</ul></li>
</ul>

<p>(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)</p>

<p>My dataset contains the following information:</p>

<ul>
<li>Subject = blocking factor (random factor)</li>
<li>Day = within subject or repeated measures factor (fixed factor)</li>
<li>Treatment = between subject factor (fixed factor)</li>
<li>Obs = measured (dependent) variable</li>
</ul>

<p><strong>UPDATE</strong>
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:</p>

<p><strong>Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)</strong></p>

<p>Obviously his notation is different from the R syntax, but this model is supposed to account for:</p>

<ul>
<li>Treatment   (fixed)</li>
<li>Day   (fixed)</li>
<li>the Treatment*Day interaction</li>
<li>Subject nested within Treatment  (random)</li>
<li>Day crossed with ""Subject within Treatment""   (random)</li>
</ul>

<p>So, is this the correct syntax to use? </p>

<pre><code>m4 &lt;- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)
</code></pre>

<p>I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?</p>

<p>Here are my previous attempts at building a model and writing syntax (discussed in answers &amp; comments):</p>

<pre><code>m1 &lt;- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)
</code></pre>

<p>How do I deal with the fact that subject is nested within treatment?  How does <code>m1</code> differ from: </p>

<pre><code>m2 &lt;- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 &lt;- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)
</code></pre>

<p>and are <code>m2</code> and <code>m3</code> equivalent (and if not, why)?</p>

<p>Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like <code>correlation = corAR1</code>)?  According to <a href=""http://circ.ahajournals.org/content/117/9/1238.full"">Repeated Measures</a>, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. </p>

<p>When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?</p>

<p>Here's an example of what the data look like:</p>

<pre><code>mydata &lt;- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))
</code></pre>
"
"0.128564869306645","0.131118179812579"," 59367","<p>I have data from a two-factor within-subjects experiment design where the conditions are not orthogonal. Factor one (Location) has three levels; factor two (Stimulus) has three levels, one of which is ""no stimulus"".</p>

<p>When multiplying these factors I get nine conditions, three of which are equivalent. All three Location x ""no stimulus"" conditions are equivalent because of lack of stimulus. As a result I only have observations from 7 conditions (just a single Location x ""no stimulus"" condition, intended to be used as a control condition). I just chose an arbitrary level of Location for that condition for the sake of coding the data.</p>

<p>My question is - how can I analyse my data in R using ANOVA?</p>

<p>I've tried building a model with the lme() function in the nlme package (following a textbook example for orthogonal designs), but I get an error when trying to build the model (presumably because of missing conditions?):</p>

<pre><code>&gt; model &lt;- lme(Y ~ Location * Design, data, random = ~ 1 | Subject / Location / Design, method=""ML"")
Error in MEEM(object, conLin, control$niterEM) : 
  Singularity in backsolve at level 0, block 1
</code></pre>

<p>The lmer() function in the lme4 package also gives an error:</p>

<pre><code>model &lt;- lmer(Y ~ Location * Design + (1|Subject) + (1|Location) + (1|Design) + (1|Location:Design), data, REML=FALSE)
Error in mer_finalize(ans) : Downdated X'X is not positive definite, 8.
</code></pre>

<p>The ezANOVA() function in the ez package also gives an error.</p>

<p>Any advice on how I could approach this analysis would be greatly appreciated! Would it be bad to duplicate the ""no stimulus"" observations for each of the two Location x ""no stimulus"" conditions which don't have observations?</p>
"
"0.128564869306645","0.109265149843816"," 59861","<p><strong>Data structure:</strong>
I have two datasets from two protected areas that differ in protection status. Both areas contain 43 and 37 sites each. </p>

<p><strong>Question:</strong>
I would like to know which test would be the best for testing whether the PA status has had an effect on:  </p>

<ol>
<li>the first axis of a PCoA (principal coordinates analysis) - i.e. species composition turnover (derived by constructing a bray curtis dissimilarity matrix) and </li>
<li>species richness per site (a continuous variable). </li>
</ol>

<p><strong>Problem:</strong>
I understand that there is pseudoreplication present in this as I only have two areas. From what I have read, it seems that I either have to use an ANCOVA / GLM / mixed-effect model, where I define PA status as both a random effect and a fixed effect. I intended to nest sites within PA, but it seems that as there is only one datapoint per site it will not work as a nested object. </p>

<p>For those familiar with R, here are some codes I have tried:</p>

<pre><code>pcoaPAanovadata1 &lt;- read.csv(""PCoA\\data\\
                              combined data PCoA axis 1 with distance variables.csv"", 
                              header=T)

str(pcoaPAanovadata1)
'data.frame': 80 obs. of 7 variables:
PCOA:    num -0.2215 -0.3521 -0.0611 0.3434 -0.3624 ...
PA.stat: Factor w/ 2 levels ""N"",""P"": 1 1 1 1 1 1 1 1 1 1 ...
village: num 33.6 33.7 39.9 37.9 34 ...
road:    num 4.18 3.8 0.89 0.1 3.43 5.49 1.86 5.04 0.79 0.88 ...
track:   num 8.11 6.48 3.11 2.71 4.49 5.35 1.25 4.03 7.62 6.77 ...
site:    Factor w/ 80 levels ""M1_11"",""M1_17"",..: 1 2 3 4 5 6 7 8 9 10 ...
rich:    num 3.27 1.79 7.31 0.82 1.79 1.82 2.45 0.82 5.47 2.79 ...
</code></pre>

<p>compare community composition turnover at different PAs:
below specifies a null model where the slope deviates as a result of the random effect </p>

<pre><code>z0 &lt;- lmer(rich ~ 1, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z0)
z1 &lt;- lme(rich ~ pastat, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z1)
anova(z0,z1)
</code></pre>

<p>impacts of distance variables:</p>

<pre><code>zz &lt;- lme(pcoa ~ road, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(zz)
</code></pre>

<p>The errors I get from the lme(linear mixed effect model):</p>

<pre><code>Warning message:
In pt(-abs(tTable[, ""t-value""]), tTable[, ""DF""]) : NaNs produced
</code></pre>

<p>The error I get from the ANOVA:</p>

<pre><code>Warning message:
In anova.lme(z0, z1) :
fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Firstly, I was hoping to just clarify whether the test I am running is correct. Secondly, it'd be great if someone could tell me what the errors mean. I apologise if my question is poorly phrased, I am relatively new to R and the statistics I am using. </p>
"
"0.111901355435757","0.125536099672233"," 60833","<p>I have a dependent variable which is measured in two different areas (DV1,DV2).  I want to see if the relationship my independent variable (IV) has with each DV is transferable to the other DV i.e. if we know the relationship between DV1 and IV, can this be used to successfully predict DV2, and vice versa.</p>

<p>People have advised me to carry out a MANOVA in order to do this, with Y containing DV1 and DV2 and x containing IV.  I am a little confused with MANOVA however, as I am not sure if it is telling me what I want to know.  Can someone please explain exactly what a MANOVA analysis can tell me about my data?  I have posted my two outputs below.  </p>

<p>Some code I have used:</p>

<pre><code>Y &lt;- cbind(DV1,DV2)
fit &lt;- manova(Y ~ E)

summary(fit)
</code></pre>

<p>gives me the output:</p>

<pre><code>                   Df  Pillai approx F num Df den Df    Pr(&gt;F)    
E                  1 0.40252    13.81      2     41 2.598e-05 ***
Residuals          42                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p><code>summary.aov(fit)</code> gives me:</p>

<pre><code>Response 1 :
                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    
E                  1  1.123 1.12299  27.826 4.341e-06 ***
Residuals          42  1.695 0.04036                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Response 2 :
                   Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
E                  1 0.41248 0.41248  7.5361 0.008862 **
Residuals          42 2.29884 0.05473                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>For reference, I am actually using a pgls based MANOVA using code from ""Outomuro, D., D.C. Adams, and F. Johansson. 2013. Evolution of wing shape in ornamented-winged damselflies. Evolutionary Biology.""  But I thought for illustratve purposes it would be easier to stick to the normal non-phylogenetic MANOVA.</p>
"
"0.104972776216296","0.0802931591358284"," 62012","<h3>Weighting in sequence analysis</h3>

<p>So far, I have scarcely found papers that address the issue of weighting for sequence analysis (using for example the optimal matching algorithm). Sequence analysis normally involves several steps:  </p>

<ol>
<li>setting or calculation of substitution and insertion/deletion costs,</li>
<li>computation of distance matrices and </li>
<li>following cluster analyses or discrepancy analyses[1].</li>
</ol>

<p>At least, the R-package <a href=""http://mephisto.unige.ch/traminer"" rel=""nofollow"">TraMineR</a> (see Gabadinho et al. 2010 and Gabadinho et al. 2011, p. 11) and the Stata-ado <a href=""http://laurent.lesnard.free.fr/article.php3?id_article=8"" rel=""nofollow"">SEQCOMP</a> by Laurent Lesnard makes it possible to include weights at step 1 and 3.<br>
Furthermore, Lesnard explicitly recommends the usage of sample weights for steps 1 and 3:</p>

<blockquote>
  <p>""Sample weights should only be used to calculate transition matrices, and consequently
  substitution costs. Instead of counting the number of transitions, it is simply
  the weighted number of transitions that should be taken into account. The
  matching procedure in itself, namely, the comparison of pair of sequences, does
  not require any weights; it is by definition a one to one procedure. However, sample
  weights should be turned on to interpret results, for instance, if cluster analysis
  is used, the size of the clusters obtained must be weighted.""<br>
  Lesnard (2010: 415, endnote 12)</p>
</blockquote>

<h3>Open questions</h3>

<p>Nonetheless, there does not seem to be a consensus in the literature when and which weights are needed or useful.  </p>

<ul>
<li>What do you think is the best rationale for applying weights in sequence analysis?  </li>
<li>When should sequences be weighted?  </li>
<li>Do you use cross-sectional sampling weights or longitudinal weights accounting for sampling probabilities as well as panel attrition?  </li>
<li>How do you apply weights if you have unbalanced panel data?  </li>
<li>The usage of weights in TraMineR is well documented; but do you have examples for the usage of weights with a Stata-ado?  </li>
</ul>

<h3>References</h3>

<ul>
<li>Gabadinho, Alexis, Gilbert Ritschard, Matthias Studer and Nicolas S. MÃ¼ller (2010): Mining sequence data in R with the TraMineR package: A user's guide,
University of Geneva.  </li>
<li>Gabadinho, Alexis, Gilbert Ritschard, Nicolas S. MÃ¼ller and Matthias Studer(2011): Analyzing and visualizing state sequences in R with TraMineR, in: Journal of Statistical Software, Vol. 40, No. 4, pp. 1-37.   </li>
<li>Lesnard, Laurent (2010): Setting Cost in Optimal Matching to Uncover Contemporaneous Socio-Temporal Patterns, in: Sociological Methods and Research, Vol. 38, No. 3, pp. 389-419.  </li>
<li>Studer, Matthias, Gilbert Ritschard, Alexis Gabadinho and Nicolas S. MÃ¼ller (2011): Discrepancy Analysis of State Sequences, in: Sociological Methods and Research. Vol. 40, No. 3, pp. 471-510.  </li>
</ul>

<p><sub>[1] See Studer et al. (2011) for a presentation of discrepancy analysis that is an ANOVA like approach for distance matrices.</sub></p>
"
"0.0829882662886615","0.0846364211331916"," 63649","<p>I have used <code>lme</code> and <code>ezAnova</code> to analyse data from a 2$\times$3 repeated-measures experiment. Theoretically those are two different ways to perform the same analysis. However, the resulting $F$-statisics and DF differ and I am lost in why.</p>

<p>Here is the exact data and output:
I have a data set with 2 independent variables (<code>marker_lang</code> and <code>congruency</code>) and the dependent variable <code>RT</code>: Both IV are repeated and completely crossed (thus 6 conditions overall). The data are not collapsed to cell means, meaning that per condition and subject I have several data points.</p>

<p>Here is what I did with ezAnova: </p>

<pre><code>ezANOVA(subset(data.mark.afc, !is.na(afc.RT)), dv=afc.RT, wid=subjectID, 
within=.(marker_lang,congruency), within_full=.(marker_lang,congruency), detailed=1, type=3)
</code></pre>

<p>And the output: </p>

<pre><code>$Anova
              Effect DFn DFd          SSn        SSd            F            p   p&lt;.05          ges
1            (Intercept)   1  24 84879098.819 1892110.06 1076.6278430 1.881814e-21     * 0.9762134497
2            marker_lang   1  24    36392.804   80595.30   10.8371986 3.071336e-03     * 0.0172922873
3             congruency   2  48    25426.393   47319.45   12.8960382 3.292730e-05     * 0.0121448066
4 marker_lang:congruency   2  48     1160.152   48150.91    0.5782581 5.647333e-01       0.0005606399
</code></pre>

<p>Here is what I did with lme:</p>

<pre><code>basemodel &lt;- lme(data=subset(cdata, !is.na(afc.RT)), afc.RT~1,
random=~1|subjectID/congruency/marker_lang, method=""ML"")

langmodel &lt;- update(basemodel, .~. + marker_lang)

angcongmodel &lt;- update(langmodel, .~. + congruency)

fulmodel &lt;- update(langcongmodel, .~. +marker_lang:congruency)
</code></pre>

<p>And the anova-tables for the lme analysis: </p>

<pre><code>anova(fulmodel)
                   numDF denDF   F-value p-value
(Intercept)                1  4715 1112.3468  &lt;.0001
marker_lang                1    72   24.8917  &lt;.0001
congruency                 2    48    8.3902  0.0008
marker_lang:congruency     2    72    0.4475  0.6410

anova(basemodel, langmodel, langcongmodel, fulmodel)
          Model df      AIC      BIC    logLik   Test   L.Ratio p-value
basemodel         1  5 64203.43 64235.88 -32096.72                         
langmodel         2  6 64185.06 64224.00 -32086.53 1 vs 2 20.366313  &lt;.0001
langcongmodel     3  8 64173.27 64225.19 -32078.64 2 vs 3 15.790082  0.0004
fulmodel          4 10 64176.38 64241.28 -32078.19 3 vs 4  0.892535  0.6400
</code></pre>

<p>I would expect the $F$, DF, and $p$-values for corresponding effects to be the same, which is not the case. This seems not an issue of different anova-types, as I tried out different types for <code>ezAnova</code>. None yield the same result as anova of <code>fulmodel</code>.</p>

<p>Any help/ideas will be greatly appreciated!</p>
"
"0.0524863881081478","0.0535287727572189"," 63669","<p>I currently am conducting a study where I have three variables: one that is binary, and two numerical ratio variables. Each of the subjects in my study has values for each of the three variables. </p>

<p>Variables:  </p>

<ul>
<li>Condition (binary): Values 0 and 1</li>
<li>Pre (ratio)</li>
<li>Post (ratio)</li>
</ul>

<p>I want to test if there is a significant difference between the the pre and post variables of the <code>0</code> control group and the <code>1</code> experimental group. Both groups have 103 subjects. The data meet all typical ANOVA assumptions such as normality and the like. I was thinking of nesting the variables as follows and then running a two-way ANOVA. </p>

<p>Variable 1 is <code>exposed</code> / <code>not</code> and variable 2 is <code>pre</code> / <code>post</code>. People are nested in Variable 1 (meaning that each person gives both pre and post information for either exposed or not exposed conditions)</p>

<p>Would this be the correct way to approach this problem? Also how would I implement this statistical analysis, preferably in R or SPSS?</p>
"
"0.0918511791892586","0.107057545514438"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.181963811142144","0.170731419324719"," 64010","<p>I am wondering what the exact relationship between partial $R^2$ and coefficients in a linear model is and whether I should use only one or both to illustrate the importance and influence of factors.</p>

<p>As far as I know, with <code>summary</code> I get estimates of the coefficients, and with <code>anova</code> the sum of squares for each factor - the proportion of the sum of squares of one factor divided by the sum of the sum of squares plus residuals is partial $R^2$ (the following code is in <code>R</code>).</p>

<pre><code>library(car)
mod&lt;-lm(education~income+young+urban,data=Anscombe)
    summary(mod)

Call:
lm(formula = education ~ income + young + urban, data = Anscombe)

Residuals:
    Min      1Q  Median      3Q     Max 
-60.240 -15.738  -1.156  15.883  51.380 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -2.868e+02  6.492e+01  -4.418 5.82e-05 ***
income       8.065e-02  9.299e-03   8.674 2.56e-11 ***
young        8.173e-01  1.598e-01   5.115 5.69e-06 ***
urban       -1.058e-01  3.428e-02  -3.086  0.00339 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 26.69 on 47 degrees of freedom
Multiple R-squared:  0.6896,    Adjusted R-squared:  0.6698 
F-statistic: 34.81 on 3 and 47 DF,  p-value: 5.337e-12

anova(mod)
Analysis of Variance Table

Response: education
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
income     1  48087   48087 67.4869 1.219e-10 ***
young      1  19537   19537 27.4192 3.767e-06 ***
urban      1   6787    6787  9.5255  0.003393 ** 
Residuals 47  33489     713                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The size of the coefficients for 'young' (0.8) and 'urban' (-0.1, about 1/8 of the former, ignoring '-') does not match the explained variance ('young' ~19500 and 'urban' ~6790, i.e. around 1/3).</p>

<p>So I thought I would need to scale my data because I assumed that if a factor's range is much wider than another factor's range their coefficients would be hard to compare:</p>

<pre><code>Anscombe.sc&lt;-data.frame(scale(Anscombe))
mod&lt;-lm(education~income+young+urban,data=Anscombe.sc)
summary(mod)

Call:
lm(formula = education ~ income + young + urban, data = Anscombe.sc)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.29675 -0.33879 -0.02489  0.34191  1.10602 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.084e-16  8.046e-02   0.000  1.00000    
income       9.723e-01  1.121e-01   8.674 2.56e-11 ***
young        4.216e-01  8.242e-02   5.115 5.69e-06 ***
urban       -3.447e-01  1.117e-01  -3.086  0.00339 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5746 on 47 degrees of freedom
Multiple R-squared:  0.6896,    Adjusted R-squared:  0.6698 
F-statistic: 34.81 on 3 and 47 DF,  p-value: 5.337e-12

anova(mod)
Analysis of Variance Table

Response: education
          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
income     1 22.2830 22.2830 67.4869 1.219e-10 ***
young      1  9.0533  9.0533 27.4192 3.767e-06 ***
urban      1  3.1451  3.1451  9.5255  0.003393 ** 
Residuals 47 15.5186  0.3302                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1    
</code></pre>

<p>But that doesn't really make a difference, partial $R^2$ and the size of the coefficients (these are now <em>standardized coefficients</em>) still do not match:</p>

<pre><code>22.3/(22.3+9.1+3.1+15.5)
# income: partial R2 0.446, Coeff 0.97
9.1/(22.3+9.1+3.1+15.5)
# young:  partial R2 0.182, Coeff 0.42
3.1/(22.3+9.1+3.1+15.5)
# urban:  partial R2 0.062, Coeff -0.34
</code></pre>

<p><strong>So is it fair to say that 'young' explains three times as much variance as 'urban' because partial $R^2$ for 'young' is three times that of 'urban'?</strong> Why is the coefficient of 'young' then not three times that of 'urban' (ignoring the sign)?</p>

<p>I suppose the answer for this question will then also tell me the answer to my initial query: Should I use partial $R^2$ or coefficients to illustrate the relative importance of factors? (Ignoring direction of influence - sign - for the time being.)</p>

<p><strong>Edit:</strong></p>

<p>Partial eta-squared appears to be another name for what I called partial $R^2$. <a href=""http://www.inside-r.org/packages/cran/heplots/docs/etasq"">etasq {heplots}</a> is a useful function that produces similar results:</p>

<pre><code>etasq(mod)
          Partial eta^2
income        0.6154918
young         0.3576083
urban         0.1685162
Residuals            NA
</code></pre>
"
"0.133814558580725","0.115476416350195"," 64513","<p>I'm wondering if you could quickly advise on the best statistical analysis for my data.</p>

<p>I have a designed experiment with 12 plots within one larger area.  This area was blocked into 6 to overcome issues of natural gradients present there, so there are 2 plots in each block.</p>

<p>Treatment 1 is a two level treatment split between these 12 plots i.e. 1 plot in every block receives one level of the treatment (""High"") and the other receives another level (""Low"").  Nested within this, each plot is split into two (24 subplots), and given a second two level Treatment 2 (levels ""A"" and ""B"").  Every single plot receiving Treatment 1 has a Treatment Two Subplot receiving level ""A"", and a subplot receiving level ""B"".</p>

<p>As such, I have a two-way factorial design, with 4 types of treatment combination: High/A, High/B, Low/A and Low/B.</p>

<p>This experiment was run for three years, and species richness measured as a response (a simple count of the number of species present in each Treatment 2 subplot) each year.</p>

<p>I am interested to see what the effects of the two levels of Treatment Two are on Richness change with time, in the context of the level of Treatment 1 received.</p>

<p>I am not sure if I need:</p>

<ul>
<li>A two-way mixed ANOVA, with within subjects effects (Year?) and between subjects effects (Treatment 1 and Treatment 2?)</li>
<li>A mixed effect model,  with Treatment 1 and Treatment 2 as fixed effects, and Year and Plot as random effects.  (Is Year a random effect?  I am unsure...).  Should subplot also be a random effect here?</li>
<li>A mixed effect model with a time series.  </li>
<li>I am also unsure as to what to do with block.  Block doesn't have levels of treatment, it was just an attempt to reduce the incidence of similarity between plots as a result of the natural gradients in the study area...</li>
<li>In addition, because the richness data is a count of species, i'm not sure if this affects my choice of analysis...</li>
</ul>

<p>If you have any thoughts on any of these points, I would be glad to hear them.</p>

<p>Thanks!</p>
"
"0.134157234067749","0.146594581573484"," 67643","<p>I have an experiment where several subjects (subjects $= S_1,S_2,...,S_m $) were asked to perform a set of tasks (tasks $= T_1, T_2, T_3,...,T_n$) using both their left ($L$) and right ($R$) arms. Each task for each arm was repeated $r$ times. The response is measured in a variable called 'measure'. Unfortunately, there were some tasks and repetitions missing. </p>

<p>I tried using the aov function in R to perform a repeated measures ANOVA analysis, but later found out that this is not appropriate for unbalanced designs. Here is the sample of what I did:</p>

<pre><code>&gt; summary(aov(measure ~ arm*task + Error(subject/(arm*task)), data=all_data))

Error: subject
          Df Sum Sq Mean Sq F value Pr(&gt;F)
arm        1 0.3240  0.3240   0.398  0.573
task       4 0.1426  0.0357   0.044  0.994
Residuals  3 2.4397  0.8132               

Error: subject:arm
          Df Sum Sq Mean Sq F value Pr(&gt;F)  
arm        1 0.0023 0.00234   0.074 0.8027  
task       4 0.9972 0.24931   7.941 0.0601 .
arm:task   1 0.0112 0.01117   0.356 0.5928  
Residuals  3 0.0942 0.03139                 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Error: subject:task
          Df Sum Sq Mean Sq F value Pr(&gt;F)
task       5 0.3898 0.07795   1.652  0.172
arm:task   4 0.1482 0.03706   0.785  0.542
Residuals 35 1.6511 0.04718               

Error: subject:arm:task
          Df Sum Sq  Mean Sq F value Pr(&gt;F)
arm:task   5 0.0352 0.007032   0.351  0.878
Residuals 35 0.7013 0.020036               

Error: Within
           Df Sum Sq  Mean Sq F value Pr(&gt;F)
Residuals 203  1.288 0.006345               
Warning message:
In aov(measure ~ arm * task + Error(subject/(arm * task)),  :
  Error() model is singular
</code></pre>

<p>Why am I getting the warning message: ""Error() model is singular""?</p>

<p>I also found that for unbalanced design it better to use the function anova() from the ""car"" package. I tried to search for the documentation for anova(), but I am very confused about how to use this function. Which model must I use for testing the following hypothesis:
1. Within-subjects there is no difference between the left and right arms.
2. Within-subjects there is no difference between the different tasks.
3. There is no interaction effect between arm and task within a subject.</p>

<p>It would also be very helpful if someone could point me to suitable books that can help me learn about these concepts.</p>
"
"0.0981930408849676","0.100143163995996"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.148453923805041","0.151402232820893"," 70158","<p>I am emailing for a sanity-check please!</p>

<p>I am analysing some marine wildlife monitoring data from an offshore construction site.    The response data are counts of animals (corrected for detection and survey effort), and the model has 2 covariates, both of which are factors:  </p>

<ol>
<li><code>Season</code> (with 4 levels related to the animals' seasonal migrations),  </li>
<li><code>Period of Construction</code> (with 3 levels: 'Before', 'During' and 'After' construction).  </li>
</ol>

<p>I am using a negative binomial GLM model structure because the Poisson was overdispersed.  I am working in R, using the glm.nb function in the MASS package.</p>

<p>When I model the count data as a function of the 2 factor variables, but without any interactions between the two factors, the model indicates that there was a significantly negative impact on animal abundance 'During' construction (i.e. the coefficient estimate for animal counts was significantly lower 'During' construction when compared to 'Before' construction, which is the base level for the 'Period of Construction' factor variable).</p>

<p>However, when I include an interaction between 'Season' and 'Construction Period', the coefficient estimate for 'During' construction changes to be positive (although non-significant).  I know that by including interactions in the model, I am changing the model structure and I would expect some changes to coefficient estimates; however I am surprised by the  magnitude of the change that occurs by adding the interaction between 'Season' and 'Period of Construction'.  </p>

<p>I am obviously keen to make sure I haven't misunderstood/made some mistake! Below I have copied my Model summary tables and the Anova table for the model containing interactions (to show the covariate main effects).</p>

<p>P.S. The data are not balanced (i.e. not every pairwise combination of 'Season' and 'Construction Period' were sampled).  I know that this should preclude assessing interactions, but I have been instructed to assess them anyway! I am wondering whether this could be causing the unexpected results??</p>

<ul>
<li><p><em>NO INTERACTION MODEL SUMMARY TABLE:</em></p>

<pre><code>Coefficients:
                              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                    1.78284    0.06203  28.741  &lt; 2e-16 
as.factor(Season)Migration     0.15741    0.05078   3.100 0.001935  
as.factor(Season)Pre-Breeding  0.78840    0.07332  10.753  &lt; 2e-16 
as.factor(Season)Winter        0.57884    0.13065   4.430 9.41e-06 
as.factor(Period)During       -0.37198    0.07126  -5.220 1.79e-07 
as.factor(Period)After         0.19159    0.05621   3.409 0.000653 
</code></pre></li>
<li><p><em>INTERACTION MODEL SUMMARY TABLE:</em></p>

<pre><code>Coefficients: (3 not defined because of singularities)
                                           Estimate Std. Error z value Pr(&gt;|z|)    
   (Intercept)                               1.3648     0.1255  10.878  &lt; 2e-16 
   as.factor(Season)Migration                0.4515     0.1373   3.288  0.00101 
   as.factor(Season)Pre-Breeding             1.3620     0.1354  10.058  &lt; 2e-16 
   as.factor(Season)Winter                   0.9969     0.1683   5.924 3.15e-09 
   as.factor(Period)During                   0.3294     0.1691   1.948  0.05139  
   as.factor(Period)After                    0.6119     0.1332   4.593 4.36e-06 
   as.factor(Season)Migration:
            as.factor(Period)During           -0.1849     0.2066  -0.895  0.37079    
   as.factor(Season)Pre-Breeding:
            as.factor(Period)During           -1.5185     0.2004  -7.578 3.50e-14 
   as.factor(Season)Winter:
            as.factor(Period)During             NA         NA      NA       NA    
   as.factor(Season)Migration:
            as.factor(Period)After            -0.2978     0.1488  -2.001  0.04535   
   as.factor(Season)Pre-Breeding:
            as.factor(Period)After              NA         NA      NA       NA    
   as.factor(Season)Winter:
            as.factor(Period)After              NA         NA      NA       NA    
</code></pre></li>
<li><p><em>ANOVA TABLE FOR MODEL WITH INTERACTION:</em></p>

<pre><code>Analysis of Deviance Table (Type II tests)

 Response: RU_density
                                           LR   Chisq Df  Pr(&gt;Chisq)    
   as.factor(Season)                    121.927    3     &lt; 2.2e-16
   as.factor(Period)                     64.713    2     8.865e-15
   as.factor(Season):as.factor(Period)   90.975    3     &lt; 2.2e-16    
</code></pre></li>
</ul>
"
"0.0829882662886615","0.0677091369065533"," 71087","<p>I have a dataset which I am not sure how to analyse. </p>

<p>The dataset came from the following experiment: I grew plants (2 different types) and measured their height at different time point (each plant was followed individually). I had in total, 3 boxes in which I grew the plants and, in each box, 3 plants of each type. I took the measurement at 4 different time points.</p>

<p>So, if I am not wrong, the plant type is a fixed factor and the box a random one.</p>

<p>Here is how it is structured:</p>

<pre><code>time, box, type, height
1, 1, 1, 1.2
1, 1, 1, 1.3
1, 1, 1, 1.1
1, 1, 2, 1.4 
1, 1, 2, 1.5
1, 1, 2, 1.6
...
2, 1, 1, 1.2
2, 1, 1, 1.3
...
1, 2, 1, 1.2
1, 2, 1, 1.3    
</code></pre>

<p>I would like to know what is the correct way to check if there is a <strong>difference between the different plant types</strong> using R</p>

<p>So far, what I have done is this:</p>

<pre><code>lme1 &lt;- lme(height ~ type, random= ~ 1|box, data=mydata)
anova(lme1)
</code></pre>

<p>but I do not know how to include the time variable in the analysis..</p>

<p>Here is the plot of the height evolution with time, for the different plant type. Each line is a plant.</p>

<p><img src=""http://i.stack.imgur.com/5R27I.png"" alt=""enter image description here""></p>
"
"0.139175553567226","0.151402232820893"," 71914","<p>Hopefully this is a question that someone here can answer for me on the nature of decomposing sums of squares from a mixed-effects model fit with <code>lmer</code> (from the <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> R package).</p>

<p>First off I should say that I am aware of the controversy with using this approach, and in practise I would be more likely to use a bootstrapped LRT to compare models (as suggested by Faraway, 2006). However, I am puzzled at how to replicate the results, and so for my own sanity I thought I would ask here.</p>

<p>Basically, I am getting to grips with using mixed-effects models fit by the <code>lme4</code> package. I know that you can use the <code>anova()</code> command to give a summary of sequentially testing the fixed-effects in the model. As far as I know this is what Faraway (2006) refers to as the 'Expected mean squares' approach. What I want to know is how are the sums of squares calculated?</p>

<p>I know that I could take the estimated values from a particular model (using <code>coef()</code>), assume that they are fixed, and then make tests using the sums of squares of model residuals with and without the factors of interest. This is fine for a model containing a single within-subject factor. However, when implementing a split-plot design the sums of squares value I get is equivalent to the value produced by R using <code>aov()</code> with an appropriate <code>Error()</code> designation. However, this is <em>not</em> the same as the sums of squares produced by the <code>anova()</code> command on the model object, despite the fact that the F-ratios are the same. </p>

<p>Of course this makes complete sense as there is no need for the <code>Error()</code> strata in a mixed-model. However, this must mean that the sums of squares are penalised somehow in a mixed-model in order to provide appropriate F-ratios. How is this achieved? And how does the model somehow correct the between-plot sum of squares but not correct the within-plot sum of squares. Evidently this is something that is necessary for a classical split-plot ANOVA that was achieved by designating different error values for the different effects, so how does a mixed-effect model allow for this?</p>

<p>Basically, I want to be able to replicate the results from the <code>anova()</code> command applied to a lmer model object myself to verify the results and my understanding, however, at present I can achieve this for a normal within-subject design but not for the split-plot design and I can't seem to find out why this is the case.   </p>

<p>As an example:</p>

<pre><code>library(faraway)
library(lme4)
data(irrigation)

anova(lmer(yield ~ irrigation + variety + (1|field), data = irrigation))

Analysis of Variance Table
           Df Sum Sq Mean Sq F value
irrigation  3 1.6605  0.5535  0.3882
variety     1 2.2500  2.2500  1.5782

summary(aov(yield ~ irrigation + variety + Error(field/irrigation), data = irrigation))

Error: field
           Df Sum Sq Mean Sq F value Pr(&gt;F)
irrigation  3  40.19   13.40   0.388  0.769
Residuals   4 138.03   34.51               

Error: Within
          Df Sum Sq Mean Sq F value Pr(&gt;F)
variety    1   2.25   2.250   1.578  0.249
Residuals  7   9.98   1.426               
</code></pre>

<p>As can be seen above all the F-ratios agree. The sums of squares for variety also agree. However, the sums of squares for irrigation do not agree, however it appears the lmer output is scaled. So what does the anova() command actually do?</p>
"
"0.166165035230439","0.16946508860008"," 72453","<p>I have a problem like the following:</p>

<p>1) There are six measurements for each individual with large within-subject variance </p>

<p>2) There are two groups (Treatment and Control)</p>

<p>3) Each group consists of 5 individuals</p>

<p>4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.</p>

<p>The data looks like this:
<img src=""http://i.stack.imgur.com/55V9J.png"" alt=""http://s10.postimg.org/p9krg6f3t/examp.png""></p>

<p>And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. <strong>This ignores within-subject variability</strong>:</p>

<pre><code> n.simulations&lt;-10000
    pvals=matrix(nrow=n.simulations,ncol=1)
    for(k in 1:n.simulations){
      subject=NULL
      for(i in 1:10){
        subject&lt;-rbind(subject,as.matrix(rep(i,6)))
      }
      #set.seed(42)

      #Sample Subject Means
      subject.means&lt;-rnorm(10,100,2)

      #Sample Individual Measurements
      values=NULL
      for(sm in subject.means){
        values&lt;-rbind(values,as.matrix(rnorm(6,sm,20)))
      }

      out&lt;-cbind(subject,values)

      #Split into GroupA and GroupB
      GroupA&lt;-out[1:30,]
      GroupB&lt;-out[31:60,]

      #Add effect size to GroupA
      GroupA[,2]&lt;-GroupA[,2]+0

      colnames(GroupA)&lt;-c(""Subject"", ""Value"")
      colnames(GroupB)&lt;-c(""Subject"", ""Value"")

      #Calculate Individual Means and SDS
      GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
      for(i in 1:length(unique(GroupA[,1]))){
        GroupA.summary[i,1]&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
        GroupA.summary[i,2]&lt;-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      }
      colnames(GroupA.summary)&lt;-c(""Mean"",""SD"")


      GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
      for(i in 1:length(unique(GroupB[,1]))){
        GroupB.summary[i,1]&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
        GroupB.summary[i,2]&lt;-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      }
      colnames(GroupB.summary)&lt;-c(""Mean"",""SD"")

      Summary&lt;-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
      colnames(Summary)[1]&lt;-""Group""

      pvals[k]&lt;-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
    }
</code></pre>

<p>And here is code for plots:</p>

<pre><code>#Plots
par(mfrow=c(2,2))
boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupA[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupA[,1]))){
  m&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
  ci&lt;-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupB[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupB[,1]))){
  m&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
  ci&lt;-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
        ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
        main=""Individual Averages"")
stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)

points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(.9,
         t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)

points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(1.9,
         t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)
legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
     main=c(paste(""# sims="", n.simulations),
            paste(""% Sig p-values="",100*length(which(pvals&lt;0.05))/length(pvals)))
)
</code></pre>

<p>Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.</p>

<p>So what is the correct way to analyze this data?</p>

<p><strong>Bonus:</strong></p>

<p>The example above is a simplification. For the actual data: </p>

<p>1) The within-subject variance is positively correlated with the mean. </p>

<p>2) Values can only be multiples of two. </p>

<p>3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. </p>

<p>4) Number of Subjects in each group are not necessarily equal. </p>

<p>Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.</p>

<p><strong>EDIT:</strong></p>

<p>Ok, here is what <em>actual</em> data looks like. There is also three groups rather than two:</p>

<p><img src=""http://i.stack.imgur.com/k1xWd.png"" alt=""enter image description here""></p>

<p>dput() of data:</p>

<pre><code>structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
    NULL, c(""Group"", ""Subject"", ""Value"")))
</code></pre>

<p><strong>EDIT 2:</strong></p>

<p>In response to Henrik's answer:
So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? </p>

<p>My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those ""stars of authority"" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM.</p>

<pre><code>#Get Invidual Means
summary=NULL
for(i in unique(dat[,2])){
sub&lt;-which(dat[,2]==i)
summary&lt;-rbind(summary,cbind(
dat[sub,1][3],
dat[sub,2][4],
mean(dat[sub,3]),
sd(dat[sub,3])
)
)
}
colnames(summary)&lt;-c(""Group"",""Subject"",""Mean"",""SD"")

TukeyHSD(aov(summary[,3]~as.factor(summary[,1])+ (1|summary[,2])))

#      Tukey multiple comparisons of means
#        95% family-wise confidence level
#    
#    Fit: aov(formula = summary[, 3] ~ as.factor(summary[, 1]) + (1 | summary[, 2]))
#    
#    $`as.factor(summary[, 1])`
#             diff       lwr       upr     p adj
#    2-1 -0.672619 -4.943205  3.597967 0.9124024
#    3-1  7.507937  1.813822 13.202051 0.0098935
#    3-2  8.180556  2.594226 13.766885 0.0046312
</code></pre>

<p><strong>EDIT 3:</strong>
I think we are getting close to my understanding. Here is the simulation described in the comments to @Stephane:</p>

<pre><code>#Get Subject Means
means&lt;-aggregate(Value~Group+Subject, data=dat, FUN=mean)

#Initialize ""dat2"" dataframe
dat2&lt;-dat

#Initialize within-Subject sd
s&lt;-.001
pvals=matrix(nrow=10000,ncol=2)

for(j in 1:10000){
#Sample individual measurements for each subject
temp=NULL
for(i in 1:nrow(means)){
temp&lt;-c(temp,rnorm(6,means[i,3], s))
}

#Set new values
dat2[,3]&lt;-temp

#Take means of sampled values and fit to model
dd2 &lt;- aggregate(Value~Group+Subject, data=dat2, FUN=mean)
fit2 &lt;- lm(Value~Group, data=dd2)

#Save sd and pvalue
pvals[j,]&lt;-cbind(s,anova(fit2)[[5]][5])

#Update sd
s&lt;-s+.001
}

plot(pvals[,1],pvals[,2], xlab=""Within-Subject SD"", ylab=""P-value"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMMDY.png"" alt=""enter image description here""></p>
"
"0.0841654636156865","0.100143163995996"," 72513","<p>I'm fairly new to statistics and I'm still trying to figure out the best way to analyse the data I have. The experiment has 2 groups of participants who perform 2 repetitions of a task that consists of 5 stages. All participants completed both repetitions for all stages, but one group had 8 participants while the other group only had 6. I have a about 100 dependent variables that I wish to examine, so my data looks a bit like this:</p>

<pre><code>ID   Group    Repetition    Stage   DV1    DV2     ...
1    A        1             1       212.9  179.9   ...
1    A        2             1       144.8  134.7   ...
2    B        1             1       146.3  156.8   ...
2    B        2             1       128.6  178.2   ...
</code></pre>

<p>Group is a between-subjects factor while Repetition and Stage are within-subjects factors. I would like to determine whether Group and Repetition have a significant effect on each dependent variable within each stage (I am not interested in the effect of stage itself). I'm doing the analysis in R so I have the following code:</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mydata            = read.csv(""data.csv"",header=TRUE)
mydata$Group      = factor(mydata$Group)
mydata$Repetition = factor(mydata$Repetition)
mydata$Stage      = factor(mydata$Stage)
# for each stage
mydata = mydata[mydata$Stage==1,]
for (i in 5:(ncol(mydata))) 
{
   fit = aov(formula=as.formula(paste(names(mydata)[i], 
                                ""~ Group * Repetition + Error(ID/Repetition)"")), 
             data=mydata)
}
</code></pre>

<p>My questions are:</p>

<ol>
<li>Is mixed measures ANOVA a valid test for this data? What's the correct way to test whether my data fits the assumptions of ANOVA in R? If this is not a reliable test, what's a possible alternative?</li>
<li>Have I defined the mixed measures ANOVA in R correctly? The various tutorials I've read define it in different ways so I'm a bit confused.</li>
</ol>
"
"0.0742269619025206","0.0567758373078348"," 73096","<p>This is a cross-post (<a href=""http://stackoverflow.com/questions/19432964/anova-error-in-levelsxx"">http://stackoverflow.com/questions/19432964/anova-error-in-levelsxx</a>) about an error I received in R while trying to run an ANOVA on my data. But error aside, I need help understanding why an ANOVA can't deal with my data and what other statistical models could be applied instead.</p>

<p>So here's my objective: I have 3 people (speaker) who recorded a bunch of words that I analyzed. The analysis yielded 3 continuous variables: skewness, kurtosis and Center of Gravity (CoG)*. I need to find out what combinations of these 3 variables best model the difference between each speaker. For example, are skewness and CoG together more significant than just CoG in finding the difference between speakers?</p>

<p>I have a basic knowledge of stats, but erring on the side of assuming I'm an idiot might be better for any complex explanations.</p>

<p>Thanks in advance!</p>

<ul>
<li>The skewness is a measure for how much the shape of the spectrum below the center of gravity is different from the shape above the mean frequency.</li>
<li>The kurtosis is a measure for how much the shape of the spectrum around the center of gravity is different from a Gaussian shape.</li>
<li>The center of gravity is a measure for how high the frequencies in a spectrum are on average weighted by their energy.</li>
</ul>
"
"0.111340442853781","0.11355167461567"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.107137391088871","0.109265149843816"," 74923","<p>I want to analyse the visual performance, operationalized via contrast threshold, depending on adaptation luminance and spectrum, gathered for 29 subjects. I'm currently kind of confused of how to do that. Some of my data:</p>

<pre><code>    ID C_measured Subject  LB Spectrum SpectrumLB
  1  1 0.1339795   AHI11 0.1       HS     HS.0.1
  2  2 0.1040440   AIC19 0.1       HS     HS.0.1
  3  3 0.1363313   AUO13 0.1       HS     HS.0.1
  4  4 0.1134103   BAR01 0.1       HS     HS.0.1
  5  5 0.1117670   BAR02 0.1       HS     HS.0.1
  6  6 0.1166350   BCL10 0.1       HS     HS.0.1
</code></pre>

<p>LB can be 0.1, 0.21, 0.3 and 1.0, Spectrum HS and LED</p>

<p>I know that I want to do 8 planned comparisons of Spectrum at each LB (4x) and the effect of reducing LB from 0.3 to 0.21 for both spectra with and without exchanging the spectrum (4x).</p>

<p>Some said I should do a two-way ANOVA, e.g. like</p>

<pre><code>   aov(C_measured ~ LB * Spectrum + Error(Subject / (LB * Spectrum)), data = anovaFrameWithGlareFoveal)
</code></pre>

<p>and then a post-hoc test on the interaction variable SpectrumLB, e.g. like</p>

<pre><code>anovaFrameWithGlareFoveal.lme &lt;- lme(C_measured ~ LB, random = ~1 | Subject / LB, data = anovaFrameWithGlareFoveal)
anovaFrameWithGlareFoveal.glht &lt;- glht(am2.subject, linfct = mcp(LB = ""Tukey""))
summary(anovaFrameWithGlareFoveal.glht, test = adjusted(type = ""none""))
</code></pre>

<p>then manually bonferroni-adjusting the p-value to the number of my planned comparisons.</p>

<p>I do that for a couple of other parameters (with glare, without glare, old reference group, young group), which I don't want to include in the statistical analysis, because it is common knowledge that this influences visual performance, I just want to analyse whether the planned comparisons differ for those parameters.</p>

<p>Till here the question: is this how to do it? DO I need the two-way ANOVA at all?</p>

<p>Then I observed some things:
for some of the parameters I observed significant values in the two-way ANOVA for the main effect of Spectrum p&lt;.05, but none of the uncorrected multiple comparisons between the two spectra at all four LBs was significant (ok one was &lt;.1, but I'm testing against .05), which some people commented with ""impossible"".
Is this possible?</p>

<p>Then people said: ""ok paired t.tests should come up with the same results"" so I did this:</p>

<pre><code>df &lt;- anovaFrameWithGlareFoveal
df.led &lt;- subset(df, Spectrum == ""LED"")
df.hs &lt;- subset(df, Spectrum == ""HS"")
t.test(df.led$C_measured[df.led$LB==.1], df.hs$C_measured[df.hs$LB==.1], paired=T)
t.test(df.led$C_measured[df.led$LB==.21], df.hs$C_measured[df.hs$LB==.21], paired=T)
t.test(df.led$C_measured[df.led$LB==.3], df.hs$C_measured[df.hs$LB==.3], paired=T)
t.test(df.led$C_measured[df.led$LB==1], df.hs$C_measured[df.hs$LB==1], paired=T)
</code></pre>

<p>then all of the t.tests were significant (&lt;.05) but only one of the uncorrected multiple comparison was significant (&lt;.05).</p>

<p>I'm not really that much into statistics that I can definitively argue for or against one method or combined methods or whether the lme + glht alone is sufficient. Had a tough time on that the last week and am really curiously looking forward to your comments!</p>
"
"0.128564869306645","0.131118179812579"," 74971","<p>When you collect data from participants in an experiment, sometimes you can collect repeated responses for <em>the same condition</em>, e.g., in R:</p>

<pre><code>set.seed(2012) # keep the example the same each time.

data.full &lt;- data.frame(id=gl(10, 4),
                        condition=gl(2, 40),
                        response=c(rnorm(40), rnorm(40, 1)))
head(data.full)

# Output:
#   id condition    response
# 1  1         1 -0.77791825
# 2  1         1 -0.57787590
# 3  1         1  0.66325605
# 4  1         1  0.08802235
# 5  2         1  1.25707865
# 6  2         1 -0.62977450
</code></pre>

<p>To analyse this (i.e. does condition predict response) I would normally take the mean response for each participant, for each condition. I would do this on the basis that we are supposed to be generalizing from a sample to a population, i.e. there should be one 'estimate' response from each participant for each condition, and the collection of these single responses (for each condition) is our sample, then we do an analysis which generalizes to the population.</p>

<p>I would transform the data e.g. like this:</p>

<pre><code>library(plyr)
data.means &lt;- ddply(data.full, .(id, condition),
                    summarize,
                    mean.response=mean(response))
head(data.means)

# Output:
#   id condition mean.response
# 1  1         1    -0.1511289
# 2  1         2     0.8658770
# 3  2         1     0.1510842
# 4  2         2     0.0129323
# 5  3         1     0.1857577
# 6  3         2     0.9859697
</code></pre>

<p>And then proceed with the within-subjects analysis (note the same process would apply if there were more conditions or a 2x2 design etc.), e.g.:</p>

<pre><code>aov1 &lt;- aov(mean.response ~ condition + Error(id/condition), data=data.means)
summary(aov1) # F = 4.2, p = .07, not significant
</code></pre>

<p>However, I've been told that with linear mixed-effects models, you can include all the underlying data on the basis that the lme models can include correlated data. My understanding was that they could include correlated data meant they could include responses from the same participants (within-subjects effects modelled as random effects), not that you could include the underlying data that gives the participant response estimate.</p>

<p>My question is, can you include the underlying data collected from the multiple responses of each participant in the <em>same condition</em>, i.e. can you do this:</p>

<pre><code>library(nlme)
lme1 &lt;- lme(response ~ 1, random= ~ 1|id/condition, data=data.full, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 3.19, p = .07, not significant
</code></pre>

<p>Or should you do this:</p>

<pre><code>lme1 &lt;- lme(mean.response ~ 1, random= ~ 1|id/condition, data=data.means, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 5.25, p = .02, significant
</code></pre>

<p>Which is the correct approach?</p>
"
"NaN","NaN"," 76059","<p>I am learning R and have been experimenting with analysis of variance.  I have been running both</p>

<pre><code>kruskal.test(depVar ~ indepVar, data=df)
</code></pre>

<p>and </p>

<pre><code>anova(lm(depVar ~ indepVar, data=dF))
</code></pre>

<p>Is there a practical difference between these two tests?  My understanding is that they both evaluate the null hypothesis that the populations have the same mean.  Thanks in advance</p>
"
"0.12894693513945","0.141623820702091"," 76250","<p>I am new to statistics and I am trying to understand the difference between ANOVA and linear regression. I am using R to explore this. I read various articles about why ANOVA and regression are different but still the same and how the can be visualised etc. I think I am pretty there but one bit is still missing.</p>

<p>I understand that ANOVA compares the variance within groups with the variance between groups to determine whether there is or is not a difference between any of the groups tested. (<a href=""https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA"" rel=""nofollow"">https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA</a>)</p>

<p>For linear regression, I found a post in this forum which says that the same can be tested when we test whether b (slope) = 0.
(<a href=""http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared"">Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?</a>)</p>

<p>For more than two groups I found a website stating:</p>

<p>The null hypothesis is: $\text{H}_0: Âµ_1 = Âµ_2 = Âµ_3$</p>

<p>The linear regression model is: $y = b_0 + b_1X_1 + b_2X_2 + e$</p>

<p>The output of the linear regression is, however, then the intercept for one group and the difference to this intercept for the other two groups. 
(<a href=""http://www.real-statistics.com/multiple-regression/anova-using-regression/"" rel=""nofollow"">http://www.real-statistics.com/multiple-regression/anova-using-regression/</a>)</p>

<p>for me, this looks like that actually the intercepts are compared and not the slopes?</p>

<p>Another example where they compare intercepts rather than the slopes can be found here:
(<a href=""http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/"" rel=""nofollow"">http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/</a>)</p>

<p>I am now struggling to understand what is actually compared in the linear regression? the slopes, the intercepts or both? </p>
"
"0.0981930408849676","0.0858369977108537"," 76885","<p>I re-edited this question for clarity:</p>

<ol>
<li>I have coordinates obtained by tracking birds with GPS (all points in plot). </li>
<li>I've used these points to perform <strong>kernel density estimation</strong> to reveal the areas with most dense bird activity (areas ""1"" and ""2"" in plot).</li>
<li>I also know coordinates of nesting trees where each nest was protected by female and male (big blue points with labels (""A"" ...""J"").</li>
<li>On each nest were done nest-defense recordings (10 min each recording, on three various intruders, twice per the same intruder)= 60 min total for each nest.</li>
<li>Each bird (female and male) exhibit 4 behaviours: ""attack"", ""threat"", ""jump"",""check"". I have all these values (duration).     </li>
</ol>

<p><img src=""http://i.stack.imgur.com/ZWRYP.png"" alt=""enter image description here""></p>

<p>Thanks to David Robinson for his time. I've done mistake in refer! I want to refer to <strong>Table 2</strong>. in this article:</p>

<p>onlinelibrary.wiley.com/doi/10.1111/jbi.12048/full</p>

<p>In article is used analysis of molecular variance (AMOVA). May I use something similar to my data in order to explain:
<strong>""source of variation""</strong></p>

<ul>
<li>among areas</li>
<li>among nesting trees within clusters</li>
<li>among individuals (female vs. male) within nesting tree</li>
</ul>

<p>Is it a problem that this AMOVA is (probably) ""designed"" on genetic data?
Which procedure in appropriate for my data?
Or should I use nested ANOVA? How?</p>

<p>Thanks.</p>
"
"0.148453923805041","0.151402232820893"," 76918","<p>I have a question about how to do analysis of an experiment that has already been done, I hope you can help me with some advice!</p>

<p>I will try to keep it as simple as possible, but will give some detail so you know what I'm talking about!</p>

<p>What has been done is a ""screening trial"" to look at the activity of about 50 subjects (fungi) as antagonists (against a pest), the 50 individuals are members of groups (species), but some groups have many more members than others</p>

<p>I have results of several types of screening tests for each of the 50 individuals, with reps of each.  The screening tests look at different aspects, like growth rate, direct effects, and indirect effects.  </p>

<p>I can rank the isolates by their results in each screening test, and there looks like a lot of variability.</p>

<p>I want to be able to report the findings of screens, for each screening test and also to see if some individuals are in top ranks in different screening tests (and also the opposite, if some are great at some tests but not at others). I think what I want is to know if the results of the tests correlate for each individual....? </p>

<p>I am not sure how to say - this individual is the best - how can I tell if it is different than the next in the rank?
If I list the top ten from each screening test, I would like to know that they are statistically different from those I excluded from the list.  I would also like to compare them as groups, to be able to say, this species was the best, but with different numbers of representatives within the species, I dont think I could do this (please advise)</p>

<p>This seems like it would be a common research experiment, for example, for testing drugs in medical experiments, so I am looking for examples of what others have done to present this type of result.</p>

<p>I have seen a similar experiment to what I have to analyse but that had been done on a small scale, and the researchers used ANOVA to test differences among individuals and among groups, and some posthoc test to give each group little letters designating their means different than other groups.  </p>

<p>This seems to be unwieldy for 50 subjects, and I'm not sure about this.... I think I need some kind of mixed model regression to put all the test results in a model to test for correlation/covariance, but my understanding is weak!</p>

<p>I have been learning R and would like to do this analysis using R.</p>

<p>Can you give me advice/suggestions?  I would appreciate any help in understanding and clarifying this problem and solutions!  </p>
"
"0.157677705948457","0.160809200153064"," 76980","<p>I'm trying to analyse some data I've recently gotten my hands on, but I'm not entirely sure which model to use. One suggestion has been a Mixed Model, Repeated Measurements ANOVA, but I'm not sure if that such kind of model can answer the questions of interest.</p>

<p><strong>The data</strong>: 
Two individual persons (A and B) have had a lot of different values (V1, V2, V3, ..., Vn) measured four times (At T0, T1, T2 and T3) - The spacing between times differs.</p>

<p>The different values have been grouped into categories (C1, C2, C3, ..., Cn). One value may belong to none, one or multiple categories. Each of the categories have a continuos value (Response_C1,Response_C1, ..., Response_Cn), which is the sum of the measured values belonging to that category. </p>

<p>In addition to this, person B was given a drug at T1.</p>

<p>What I would like to investigate now, is:</p>

<ol>
<li>Is there any observable effect after administering the drug</li>
<li>On which categories did the drug have an effect</li>
<li>If there is an effct on a category, what is the effect size</li>
<li>How does the effect vary over time</li>
<li>If there is an effect, is the effect observed from the drug at T1 still persistant at T3</li>
</ol>

<p>I realise one of the major pitfalls is the lack of both time points and samples, but it would be appreciated if you could suggest any articles/methods for this type of analysis.</p>

<p><strong>What I have tried so far</strong> is just Repeated Measurements ANOVA, using R:</p>

<pre><code>test.aov &lt;- aov(Response_C ~ Category * Timepoint * Treatment + Error(Sample), data=df)
</code></pre>

<p>But I am not sure that the model is correct, neither am I sure that it actually answers my questions, even if I try to model it as a mixed model. </p>

<p>Any help is much appreciated. Please let me know if any additional information is needed</p>

<p><strong>Edit 1:</strong> After doing some more reading, it seems a Generalised Linear Model with a negative binomial distribution (since this kind of data is usually over-dispersed) might be better suited for this kind of data, but I'm still not sure if such a model would answer the questions. Potentially I could fit a model to each individual category, but that would inflate the Type-I error I guess, and so we would need to correct for multiple testing.</p>

<p><strong>Edit 2:</strong> Some more reading, and I thought the <code>lme4</code> R package would be a good way to fit a Linear mixed model to my data, and just do individual comparisons of each category. Here's the model I tried to fit:</p>

<pre><code>lm1 &lt;- lmer(Response ~ Treatment * Timepoint + (1|Subject), data=my_data)
</code></pre>

<p>First off, I'm not sure whether Timepoint should be a factorial or a numerical value. As I mentioned, timepoints are not evenly distributed (To be precise, I have for time 0, 2days, 14 days, 90days), however, the design is balanced. If I enter the Timepoints as a numerical value, I don't get any estimate of what the value is at any given Timepoint, but just some numbers for Correlation of fixed effects, which I can't really use for anything. On the other hand, if I enter the Timepoints as factors, I do get an estimated value for the effect at each timepoint, but I'm not too sure how certain or reliable this value is.</p>
"
"0.0981930408849676","0.100143163995996"," 78042","<p>If I had a <code>glm</code> using on count data I may do the following:</p>

<pre><code>glm(response ~ exp1 * exp2, family = poisson, data =data)
</code></pre>

<p>The first thing I would do here is check for overdispersion with the <code>residual deviance/df</code>. If there was overdispersion I may choose to use <code>family =  quasipossion</code></p>

<pre><code>glm(response ~ exp1 * exp2, family = quaispoisson, data =data)
</code></pre>

<p>I would then simplify my model to find the optimal model using analysis of deviance based on log likelihoods (likelihood ratio tests)e.g. </p>

<pre><code>m1 &lt;- glm(response ~ exp1 * exp2, family = quaipoissn, data =data)
m2 &lt;- glm(response ~ exp1 + exp2, family = quaipoissn, data =data) #remove interaction
anova(m1, m2, test = ""chi"") #if it was still poisson
# or
anova(m1, m2, test = ""F"") #for quasipoisson
# using p-values to assess the significance of the removed interaction
</code></pre>

<p>Finally then I would then validate my model by plotting deviance residuals against fitted values, explanatory values e.g. <code>plot(m2)</code>. If all is ok, there is independence and no patterns, I don't have to add in extra explanatory variables etc.</p>

<p>My question is, what are the key differences to this process using <code>glmer</code> e.g.</p>

<pre><code>glmer(response ~ exp1 * exp2 + (1|random1/random2), family = poisson, data =data)
</code></pre>
"
"0.0371134809512603","0.0378505582052232"," 78365","<p>Hi I am trying to find the non-parametric equivalent of a two-way ANOVA (3x4 design) which is capable of including interactions. From my reading in Zar 1984 ""Biostatistical analysis"" this is possible using a method put forth in Scheirer, Ray, and Hare (1976), however, according to other posts online it was inferred that this method is no longer appropriate (if it ever was).</p>

<p>Does anyone know what method would be appropriate for doing so, and if so the corresponding functions in R or Stata? </p>
"
"0.153022802096395","0.156061849468725"," 78539","<p>I would like to determine the variance explained by random factors and slopes in a mixed model but am unsure if the analysis I use and my interpretation are correct. Furthermore, comparing models and analysing a mixed model with random slopes seem to give opposite conclusions, therefore I would like to know when to include random slopes? Below I give an overview of the analysis.</p>

<p>I tested three groups of 10 individuals twice in the same task. As I expect individuals to differ in their response across the two tasks, I also include random slopes. The analysis I run is:</p>

<pre><code>lme(behaviour ~ stage * group, random = ~ stage|ID, data=data)
</code></pre>

<p>Part of the output I get is the following:</p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
Data: data 
    AIC       BIC   logLik
   -72.07494 -48.50785 46.03747

 Random effects:
  Formula: ~stage | ID
  Structure: General positive-definite, Log-Cholesky parametrization
             StdDev     Corr  
 (Intercept) 0.12646601 (Intr)
 stage2      0.12662159 -0.455
 Residual    0.05714907       
</code></pre>

<p>To calculate the variance I extract the SD of ID, slopes, and the residual variance as follows:</p>

<pre><code>SD.ID &lt;- (fm2$sigma * attr(corMatrix(fm2$modelStruct[[1]])[[1]],""stdDev""))[[1]]
SD.slope &lt;- (fm2$sigma * attr(corMatrix(fm2$modelStruct[[1]])[[1]],""stdDev""))[[2]]
SD.residual &lt;- fm2$sigma
</code></pre>

<p>And then calculate the percentage of variance explained:</p>

<pre><code>(SD.ID/(SD.ID+SD.slope+SD.residual))*100
(SD.slope/(SD.ID+SD.slope+SD.residual))*100
</code></pre>

<p>In this case this seems to suggest: ""individual ID and random slopes explained 40.8% and 40.8% repectively of the variance of behaviour"".</p>

<p>Although this seems to suggest the random slopes explain a large part of the variance, it seems perhaps a more simple model without slopes is more appropriate:</p>

<pre><code>fm1 &lt;- lme(behaviour ~ stage * group, random = ~ stage|ID, data=data, method=""ML"")
fm0 &lt;- lme(behaviour ~ stage * group, random = ~ 1|ID, data=data, method=""ML"")
anova(fm0,fm1)
</code></pre>

<p>since I get the following output:</p>

<pre><code>    Model df       AIC       BIC   logLik   Test    L.Ratio p-value
fm0     1  8 -76.00947 -57.15580 46.00473                          
fm1     2 10 -72.07494 -48.50785 46.03747 1 vs 2 0.06547599  0.9678
</code></pre>

<p>Which to me seems to suggest the model with the random slope does not significantly better fit the data. This seems contrasting to the 40% of the variance that it seems to explain, as shown with the data above.</p>

<p>Furthermore, if I correlate the coefficients from model fm1, thus the intercept with the slope:</p>

<pre><code>cor.test(fm1$coefficients[[2]][[1]][,1],fm1$coefficients[[2]][[1]][,2]) 
</code></pre>

<p>I get the following output:</p>

<pre><code>Pearson's product-moment correlation

data:  fm1$coefficients[[2]][[1]][, 1] and fm1$coefficients[[2]][[1]][, 2]
t = -2.6802, df = 37, p-value = 0.01092
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.6376166 -0.1004855
sample estimates:
       cor 
-0.4032186 
</code></pre>

<p>Which thus seems to suggest that individuals with lower initial behaviour scores change more in their behaviour over time. Therefore again I would think running the model with the slopes would make the most sense. </p>

<p>Thus, to repeat my question: how can I determine the variance explained by random factors and slopes in a mixed model and when do I know when to include random slopes?</p>
"
"0.0829882662886615","0.0846364211331916"," 78673","<p>I have 2 x 2 x 2 mixed design with two between subject factors (sex, organizer) and one within subjects factor (task). The group sizes of the 'sex'-factor is unequal. When I perform a factorial repeated measures ANOVA, I get the following warning message: </p>

<blockquote>
  <p>Warning: Data is unbalanced (unequal N per group). Make sure you
  specified a well-considered value for the type argument to ezANOVA().</p>
</blockquote>

<p>I used the following model in R:</p>

<pre><code>model &lt;- ezANOVA(data=df, dv=.(top_start), wid=.(id), between=.(sex, org),
                         within=.(task), type = 3, detailed = TRUE)
</code></pre>

<p>I used <code>type = 3</code> for the Anova because, as I understood, it is suited for unbalanced group sizes.</p>

<p>I have the following questions:</p>

<ul>
<li>Do I need to code contrasts when all my factors have only two levels?</li>
<li>Did I use the right type of Anova?</li>
<li>Are there other ways to do this analysis?</li>
</ul>
"
"0.0371134809512603","0.0378505582052232"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.111340442853781","0.11355167461567"," 80881","<p>I have a randomized experimental dataset with six treatments with each approx. N=60. The outcome is a time-series, namely deforestation in a land-use simulation game over 40 rounds.</p>

<p>I managed to show that the impact of the treatments on the state of the land (i.e. number of cummulative cells deforested) is highly significant in a single year, but I have a hard time finding the right method to show that the impact on the ENTIRE TIME SERIES is significant. I'm afraid testing in a single year is overestimating significance, as I can choose any year, adding ""researchers degrees of freedom"". I could (and successfully did) test every year seperatly, but that seems to be a very unelegant solution.</p>

<p>In more general terms:</p>

<p>I have a single independent variable from one 1 to 6, and my dependent variables is time series for every observation. What I want to do is basically an ANOVA, but feeding it with a whole time series as dependent variable instead of single values for each observation.</p>

<p>If possible, it would be cool if the method also allows for controling for other independent fixed factors, such as player age, occupation etc., and ideally for more than one time series as dependent variable, as I also have data for intensification, savings, cows sold and some other values for every year in the game.</p>

<p>Any expert insights?</p>

<p>My data is a SQL database with a single entry for every round of the time series for every subject, linked to the subject-properties via a unique ID => I can bring it into any shape needed for the analysis. My problem is not to shape it but to find the right test. I'm using mySQL &amp; R.</p>
"
"0.0524863881081478","0.0267643863786095"," 81291","<p>I want to test whether the breeding periods of several closely related birds is significantly separated in time. What statistical test should I use?</p>

<p>My data consists of several observations per species with the date of breeding initiation (i.e. egg laying dates).</p>

<p>Could I just simply do an ANOVA on the Julian dates of breeding initiation?</p>

<hr>

<p>I only have data of breeding birds, none of non-breeding ones. We only recorded breeding initiation of birds on nests. Each bird was recorded only once. I don't think survival analysis would fit this type of data. Am I wrong? (I'm not familiar with survival analysis).</p>

<p>Breeding time of different species do not deviate from normality. Variances are equal among species.</p>

<p>Here is what the data looks like. Each type and color of line represents a different species.</p>

<p><img src=""http://i.stack.imgur.com/GWDoj.png"" alt=""Here is what the data looks like.""></p>
"
"0.0909090909090909","0.092714554082312"," 81368","<p>I'm reproducing an example from <a href=""http://rads.stackoverflow.com/amzn/click/0470073713"">Generalized, Linear, and Mixed Models</a>. My MWE is below:</p>

<pre><code>Dilution &lt;- c(1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4)
NoofPlates &lt;- rep(x=5, times=10)
NoPositive &lt;- c(0, 0, 2, 2, 3, 4, 5, 5, 5, 5)
Data &lt;- data.frame(Dilution,  NoofPlates, NoPositive)

fm1 &lt;- glm(formula=NoPositive/NoofPlates~log(Dilution), family=binomial(""logit""), data=Data)
summary(object=fm1)
</code></pre>

<hr>

<p><strong>Output</strong></p>

<hr>

<pre><code>Call:
glm(formula = NoPositive/NoofPlates ~ log(Dilution), family = binomial(""logit""), 
    data = Data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.38326  -0.20019   0.00871   0.15607   0.48505  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      4.174      2.800   1.491    0.136
log(Dilution)    1.623      1.022   1.587    0.112

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.24241  on 9  degrees of freedom
Residual deviance: 0.64658  on 8  degrees of freedom
AIC: 6.8563

Number of Fisher Scoring iterations: 6
</code></pre>

<hr>

<p><strong>Code</strong></p>

<hr>

<pre><code>anova(object=fm1, test=""Chisq"")
</code></pre>

<hr>

<p><strong>Output</strong></p>

<hr>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: NoPositive/NoofPlates

Terms added sequentially (first to last)


              Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                              9     8.2424            
log(Dilution)  1   7.5958         8     0.6466  0.00585 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<hr>

<p><strong>Code</strong></p>

<hr>

<pre><code>library(aod)
wald.test(b=coef(object=fm1), Sigma=vcov(object=fm1), Terms=2)
</code></pre>

<hr>

<p><strong>Output</strong></p>

<hr>

<pre><code>Wald test:
----------

Chi-squared test:
X2 = 2.5, df = 1, P(&gt; X2) = 0.11
</code></pre>

<p>Estimated coefficients are perfectly matching with the results given in the book but SE's are far apart. Based on LRT test the slope  is significant but based on Wald and Z-test slope coefficient is insignificant. I wonder if I miss something basic. Thanks in advance for your help.</p>
"
"0.267676767676768","0.267842045126679"," 82102","<p>I hope this is an appropriate forum to post this question. I recently upgraded my R software from 2.15.0 to 3.0.2. I also upgraded the lme4 package from .999999-0 to 1.1-2. After doing so, the results from one of my linear mixed models analyses have changed a bit unexpectedly. In some respects, I was expecting some change, as the lme4 developers very clearly stated that they had made some significant changes to some fundamental components in the package. However, the changes that I am seeing (described below) make me think that something else is awry. I will start by explaining the experimental design, which is quite simple and then the issue at hand.</p>

<p>My experiment is a basic repeated measures design. I used 24 ""Items"" that each appeared in three different ""Conditions"" (SmallClause_Som, NoSmallClause, SmallClause_NoSom). Levels of Condition were rotated across three presentation lists such that each Subject (45 total, each assigned to a particular list) only saw one level of each item.</p>

<p>I used lmer() for the analysis. Condition was entered in as a Fixed effect and ""Subject"" and ""Item"" were entered as Random effects.</p>

<p>The problem:
Using the current version of R 3.0.2 and lme4 1.1-2 with NoSmallClause as the reference level (and no weighting on any of the contrasts), the ConditionSmallClause_Som/NoSmallClause contrast produces a t value of 1.680. </p>

<p>But, when I change reference level to SmallClause_Som (to observe the one remaining contrast) I get not only a change in the polarity of the effect (plus to minus, as expected), but the values change as well.</p>

<p>When I use R 2.15.0 and lme4 .999999-0 (on another computer), I do not experience this issue. I get slightly different values, but they do not change (apart from the polarity) when I change reference level.</p>

<p>My colleague also tried my analysis for me using R 3.0.2 and a version of lme4 (pre version 1.0) (I don't know exactly which version, but it was before the major changes) and he also does not experience the issue.</p>

<p>R 2.15.0 lme4 1.1-2 (older) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
 AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance   Std.Dev. Corr          
 Subject  (Intercept)                0.98998765 0.994981               
          ConditionSmallClause_Som   0.00203374 0.045097 -1.000        
          ConditionSmallClause_NoSom 0.00019873 0.014097  1.000 -1.000 
 Item     (Intercept)                0.96231875 0.980978               
          ConditionSmallClause_Som   0.89924400 0.948285 -0.020        
          ConditionSmallClause_NoSom 0.62128577 0.788217 -0.256  0.361 
 Residual                            1.68810777 1.299272               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                       Estimate Std. Error t value
(Intercept)                  2.9583     0.2584  11.447
ConditionSmallClause_Som     0.3639     0.2165   1.680
ConditionSmallClause_NoSom   0.1472     0.1878   0.784

Correlation of Fixed Effects:
            (Intr) CnSC_S
CndtnSmlC_S -0.116       
CndtnSmC_NS -0.260  0.392

&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
  AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance  Std.Dev. Corr          
 Subject  (Intercept)                0.9023239 0.949907               
          ConditionNoSmallClause     0.0020340 0.045099 1.000         
          ConditionSmallClause_NoSom 0.0035039 0.059194 1.000  1.000  
 Item     (Intercept)                1.8238288 1.350492               
          ConditionNoSmallClause     0.8992237 0.948274 -0.687        
          ConditionSmallClause_NoSom 0.9804329 0.990168 -0.604  0.670 
 Residual                            1.6881050 1.299271               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3174  10.468
ConditionNoSmallClause      -0.3639     0.2165  -1.680
ConditionSmallClause_NoSom  -0.2167     0.2243  -0.966

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.588       
CndtnSmC_NS -0.521  0.638
</code></pre>

<p>R 3.0.2 and lme4 1.1-2 (newer) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3942.557  4022.312 -1955.278  3910.557 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.9522   0.9758              
          ConditionSmallClause_NoSom 0.1767   0.4204    0.03      
          ConditionSmallClause_Som   0.1760   0.4196   -0.15  0.92
 Item     (Intercept)                1.2830   1.1327              
          ConditionSmallClause_NoSom 0.7782   0.8822   -0.41      
          ConditionSmallClause_Som   1.4901   1.2207    0.09  0.41
 Residual                            1.6466   1.2832              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  2.9583     0.2814  10.512
ConditionSmallClause_NoSom   0.1472     0.2133   0.690
ConditionSmallClause_Som     0.3639     0.2741   1.327

Correlation of Fixed Effects:
            (Intr) CSC_NS
CndtnSmC_NS -0.357       
CndtnSmlC_S -0.007  0.451
&gt; #anova (test.lmer3, test.lmer4)
&gt; 
&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)
Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3951.357  4031.113 -1959.679  3919.357 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.88980  0.9433              
          ConditionNoSmallClause     0.04299  0.2073   0.83       
          ConditionSmallClause_NoSom 0.01562  0.1250   0.90  0.67 
 Item     (Intercept)                2.39736  1.5483              
          ConditionNoSmallClause     0.72053  0.8488   -0.04      
          ConditionSmallClause_NoSom 1.87804  1.3704   -0.16  0.53
 Residual                            1.65166  1.2852              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3525   9.425
ConditionNoSmallClause      -0.3639     0.2004  -1.816
ConditionSmallClause_NoSom  -0.2167     0.2963  -0.731

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.045       
CndtnSmC_NS -0.160  0.514
</code></pre>

<p>My question:
What is going on here? Why is changing the reference level producing a shift from 1.327 to -1.816 in the t scores for the new version of lme4 whereas it produces the same (disregarding sign) value of 1.680/-1.680 in the old version's t scores? Only the older version seems to make sense to me.</p>

<p>1) Am I specifying my model incorrectly for the new version of lme4?</p>

<p>2) Am I missing some basic fundamental fact about how contrasts work? That is, is it possible to get different values just from changing the reference level? (the correlation values look a bit odd in the newer output).</p>

<p>3) Is this a bug in lme4?</p>

<p>4) Some other explanation...?</p>

<p>I have had some other odd issues as well with this same analysis using lme4 1.1-2. For example, if I don't clear the workspace and re-run an analysis, the values also will change between analyses (and also within the analysis as I change the reference level). This never happened to me on the earlier version (and it still does not happen when I run it on the earlier version now).</p>

<p>I hope someone can help with this. I found two other similar questions online (after much searching) but neither had any informative responses.</p>

<p>Thanks DT</p>
"
"0.0524863881081478","0.0535287727572189"," 82156","<p>I am trying to test the effect of each single predictor as well as their combination in predicting an outcome, and I am confusing how to explain these p values:</p>

<pre><code>set.seed(123)
Y&lt;-rnorm(100,10,3)
X1&lt;-rbinom(100,1,0.5)
X2&lt;-Y+runif(100,1,50)
X3&lt;-rbinom(100,50,0.3)
fit&lt;-glm(Y~X1+X2+X3)
#p.value(1)
round(summary(fit)$coef,4)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   8.9131     1.3292  6.7056   0.0000
X1            0.2781     0.5336  0.5211   0.6035
X2            0.0541     0.0182  2.9662   0.0038
X3           -0.0462     0.0829 -0.5577   0.5784
#p.value(2)
Anova(fit,type=2)
Analysis of Deviance Table (Type II tests)

Response: Y
   LR Chisq Df Pr(&gt;Chisq)   
X1   0.2716  1   0.602278   
X2   8.7983  1   0.003015 **
X3   0.3110  1   0.577077 
#p.value(3)
Anova(fit,type=3)
Analysis of Deviance Table (Type III tests)

Response: Y
   LR Chisq Df Pr(&gt;Chisq)   
X1   0.2716  1   0.602278   
X2   8.7983  1   0.003015 **
X3   0.3110  1   0.577077  
#p.value(4)
single.p.value&lt;-NULL
for (var in c(""X1"",""X2"",""X3"")){
    single.p.value[var]&lt;-summary(glm(as.formula(paste(""Y"", var, sep=""~""))))$coef[2,4]
}
single.p.value
         X1          X2          X3 
0.433781616 0.003440499 0.995918785 
</code></pre>

<p>When are the <code>p.values</code> different between <code>car::Anova</code> type-II and type-III? Any body can give a clear explanation of these 4 sets of pvalues?</p>
"
"0.170722012375797","0.189252791026116"," 82387","<p>Working in R, how can I specify a mixed ANOVA with multiple between- and within-subjects factors in such a way that it's amenable to adding a covariate in a subsequent analysis? Also, ideally, I would like to use type 3 SS because that's what I'm used to.</p>

<p>I originally did my analysis (without the covariate) using ezANOVA, and found a predicted 3-way interaction of two between-subjects factors (pretrain and training, below) and one within-subjects factor (section, below). A possible explanation is that the two b-s factors affect another quantity (study, below), which in turn causes the above interaction. I am hoping this explanation is NOT correct, but in order to eliminate it, I'd like to add the other quantity ('study') to my model as a covariate and see whether the original 3-way interaction is still significant. Is this an acceptable approach?</p>

<p>Assuming yes, my issue is that I can't find a good way to add the covariate. (A) it can be added using ""between_covariates"" in ezANOVA, but there's a warning that this function is under development, and the results look strange, so I don't trust them without other verification. (B) Using aov, I couldn't exactly replicate my ezANOVA results <em>without</em> the covariate, and also couldn't figure out how to add a covariate. (C) Using lm, I couldn't figure out how to get the repeated measures working. (D) Using lme from nlme, I did get a working model with the repeated measures (shown below), but the results <em>without</em> the covariate were different from those of ezANOVA, so I don't think it's an equivalent model. I'd like to create a model equivalent to the original one, and then see whether my effect stays significant when the covariate is added. How can I do this?</p>

<p>Reproducible code:</p>

<pre><code># This data comes from an experiment designed to investigate effects of different types of instruction on learning. Instruction is manipulated between subjects using factorial combinations of ""pretraining"" (3 levels) and ""training"" (2 levels). Learning is assessed as change in performance from a pretest, administered before pretraining &amp; training, to a posttest, administered after. Test accuracy is my dv and test section (pre vs post) is treated as a within-subjects factor. I also have an additional within-subjects factor called ""problem type"", which is used to classify different types of problems on the pretest and posttest. Some problem types have more problems than others, but the number of problems of a given type is the same for pretest and posttest.

# Sample data:
nsubj = 215; nsec  = 2; nprob = 6
D = data.frame(
    subjid   = rep( 1:nsubj, each=nsec*nprob ),
    pretrain = rep( sample( c('a','b','c'), nsubj, replace=TRUE ), each=nsec*nprob ),
    training = rep( sample( c('j','k'), nsubj, replace=TRUE ), each=nsec*nprob ),
    study    = rep( sample( 1:6, nsubj, replace=TRUE ), each=nsec*nprob ),
    section  = rep( rep( c( 'pretest', 'posttest' ), each=nprob ), nsubj ),
    probtype = rep( c( 'v', 'w', 'x', 'y', 'z', 'z' ), nsec*nsubj ),
    accuracy = sample( c( 0.0, 0.5, 1.0 ), nsubj * nsec * nprob, replace=TRUE ) )

# Model:
library( ez )
options( contrasts=c(""contr.sum"",""contr.poly"") )
ezANOVA( data=D, wid=.(subjid), dv=.(accuracy), within=.(section,probtype), between=.(pretrain,training), type=3 )

# nlme version:
library( nlme )
fit = lme( accuracy ~ section*probtype*pretrain*training, random=~1|subjid, method='REML', data=D )
anova.lme( fit, type='marginal' )
# results in similar but not identical F and p values to the original model. Denominator dfs are quite different.
</code></pre>
"
"0.0371134809512603","0.0378505582052232"," 82438","<p>I have the data:</p>

<pre><code>numbers &lt;- c(0.176, 0.005, 0.022, 0.016, 0.036, 0.095, 0.069 )
Inds &lt;- as.factor(c(""P06"", ""P07"", ""P08"", ""P09"", ""P10"", ""P12"", ""P13"") )
</code></pre>

<p>and am trying to test for differences in <code>numbers</code> as a function of <code>Inds</code>.  The numbers are proportions of an events success for each individual.  With <code>Inds</code> specified as a factor, I am trying conduct an ANOVA using <code>aov()</code> (below)  </p>

<pre><code>anova(aov(numbers ~ Inds))
</code></pre>

<p>which results in the warning (below)</p>

<pre><code>Analysis of Variance Table
Response: numbers
          Df   Sum Sq   Mean Sq F value Pr(&gt;F)
Inds       6 0.021743 0.0036238               
Residuals  0 0.000000                         
Warning message:
In anova.lm(aov(numbers ~ Inds)) :
  ANOVA F-tests on an essentially perfect fit are unreliable
</code></pre>

<p>Any suggestions (changes in code or theoretical mistakes) would be appreciated.</p>
"
"0.0981930408849676","0.100143163995996"," 82698","<p>I've run a simulation study in order to determine type I error rate of a statistic.My simulation design includes threes factors as sample size (4 levels), test length or number of items (3 levels) and estimator (3 levels). The statistic is developed to measure person fit with test data in educational testing situation.I've replicated the analysis in each cell (i.e. the design is fully-crossed) 100 times.</p>

<p>Now, I have the results and type I error rates range from 0.005 to 0.105 (i.e. across the whole analysis). I want to analyze how factors affect type I error rate using something similar to ANOVA. I tried Beta Regression in R using <code>betareg</code> package but I received this error message:</p>

<blockquote>
  <p>invalid dependent variable, all observations must be in (0, 1)</p>
</blockquote>

<p>Any idea on how to determine the effect of design factors on type I error rate?</p>
"
"0.111340442853781","0.100934821880595"," 83157","<p>I am attempting to find a way to perform a power analysis for a higher order repeated measures ANOVA where all factors are within-subjects (i.e., there are no between-subjects factors). I have looked extensively for a way to do this using existing tools like SPSS, or G*Power and other packages but have not found anything that seems appropriate. G*Power does not seem to support power analysis for repeated measures designs where there are multiple within-subjects factors. </p>

<p>I am wondering if R is the right path to take to perform a power analysis of this type. I am just starting with R and was wondering if anyone had a script built for this? I found a script at the link below that seems promising but wanted to confirm that it is indeed for a power analysis of a design with ONLY within-subjects factors. The design I am interested in analyzing would have 2 factors, typically one with 2 levels and another with 3 or more. However, I also need to assess a much larger design with 4 factors. If the script in the thread linked below is for a totally within-subjects design, can it be expanded to include more factors? If so how?</p>

<p><a href=""http://stats.stackexchange.com/questions/80190/power-analysis-of-repeated-measures-anova-using-simulation-in-r?newreg=183b499633d240a6a3a4a9c2e1890eb7"">Power analysis of repeated measures ANOVA using simulation in R?</a></p>
"
"0.161773912909565","0.164986758173172"," 83458","<p>My question is about the best way to estimate the effect of a predictor on a dependent variable, while accounting for several other predictors that may correlate with the predictor of interest. I'm using a linear mixed-effects model, using the <code>lmer</code> function from the R <code>lme4</code> package. (Warning: I'm fairly new at this, so their may be some misunderstandings woven through my question.)</p>

<h2>The problem</h2>

<p>To make things a bit more specific, I'll just explain the actual data that I'm working with. I have eye-movement data of participants freely viewing natural scenes. I want to determine whether pupil size predicts the 'visual saliency' (i.e. the conspicuity) of the locations in the image that participants are looking at. But there are many other things that correlate with pupil size, such as luminosity, and this makes the analysis tricky (or does it?).</p>

<h2>Option 1 (simple): Looking at fixed effects</h2>

<p>One option would be to simply create a linear mixed-effects model that has all predictors of saliency that I can think of, including the predictor of interest (<code>pupil_size</code>), as fixed effects and <code>subject</code> and <code>scene</code> as random effects. (To keep things manageable, I'm using a purely additive model, although I suppose that this is a whole topic in itself.)</p>

<pre><code>my_lmer = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>This will give me a t-value for the fixed effect <code>pupil_size</code>. From what I understand, this fixed effect will already be partial, so it's the unique predictive power of pupil size, with any correlations between fixed effects already taken into account. Is my understanding correct?</p>

<h2>Option 2 (complex): Using model comparison</h2>

<p>An alternative approach, which I have from <a href=""http://www.sciencedirect.com/science/article/pii/S0749596X07001398"">Baayen et al. (2008)</a>, is to compare a model without pupil size as fixed effect (<code>simple_model</code>) to a model with pupil size as fixed effect (<code>complex_model</code>).</p>

<pre><code>simple_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + (1|subject) + (1|scene))
complex_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>Now I can use the <code>anova</code> function to compare these two models (see Baayen's paper for an example). This will give me a <code>Chisq Chi</code> value, and I can use this to determine whether adding <code>pupil_size</code> as fixed effect is a justified addition to the model.</p>

<p>Clearly, this model comparison approach is more complex than simply looking at the t-values for fixed effects in a single model. And it seems to me that if <code>pupil_size</code> is a significant predictor (per Option 1), then it must also be a significant addition to the model (per Option 2).</p>

<p>In sum, my question is: <em>Is there any reason to do a model comparison (Option 2), or am I better off just creating a single linear mixed-effects model and seeing whether the t-value associated with <code>pupil_size</code> as fixed effect is sufficiently high (Option 1)?</em></p>
"
"0.0371134809512603","0.0378505582052232"," 85798","<p>If I do a multiple regression such as:</p>

<pre><code>df&lt;-data.frame(y1=rnorm(100,2,3),
y2=rnorm(100,3,2),
x1=rbinom(100,1,0.5),
x2=rnorm(100,100,10))

fit&lt;-lm(cbind(y1,y2)~x1+x2,data=df)
&gt; anova(fit)
Analysis of Variance Table

            Df  Pillai approx F num Df den Df Pr(&gt;F)    
(Intercept)  1 0.75423  147.306      2     96 &lt;2e-16 ***
x1           1 0.00720    0.348      2     96 0.7069    
x2           1 0.00928    0.450      2     96 0.6391    
Residuals   97                                          
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I am wondering how to explain this ANOVA object where two models have different responses and the same set of predictors.</p>
"
"0.12894693513945","0.121391846316078"," 86154","<p><strong>The question</strong></p>

<p>I have a dataset which I think requires a multivariate multilevel analysis. I am unsure both of the appropriate model and of how to fit it with <code>R</code>. I have come up with a tentative model, but my understanding of the math is so superficial that I cannot tell whether my analysis is ""right"" or whether it includes blatant errors. I would appreciate any insight on the model design or the model specification in <code>R</code>.</p>

<p><strong>The study design</strong></p>

<p>The question is whether the architectural design of a clinic will influence the outcome of a pathology for permanent residents in this clinic.
We have collected data on 13 symptoms for 8 patients per clinic in 21 clinics.</p>

<p>There is a patient-level IV <code>medication</code> and a clinic-level IV <code>architecture</code>. All variables are continuous-ish.</p>

<p>The 13 symptoms are correlated +.20 on average, which I think indicates a multivariate multilevel analysis is appropriate.</p>

<p><strong>The data</strong></p>

<p>To run the multivariate analysis with <code>nlme</code> I have standardized my DVs, stacked these 13 DVs in a single column, and added a categorical dummy variable to flag which row corresponds to which symptom.  </p>

<p>It looks like this:  </p>

<pre><code> Clinic Patient Symptom    Score    Medication  Architecture
 1            1   EP1      0.12         1               3.2  
 1            1   EP2      0.11         1               3.2  
 1            1   EP3      0.13         1               3.2  
 1            2   EP1      0.56         4               3.2  
 1            2   EP2      0.67         4               3.2  
 1            2   EP3      0.23         4               3.2  
 2            3   EP1      0.22         3               5.1  
 2            3   EP2      0.25         3               5.1  
 2            3   EP3      0.14         3               5.1  
 2            4   EP1      0.78         6               5.1  
 2            4   EP2      0.89         6               5.1  
 2            4   EP3      0.11         6               5.1  
</code></pre>

<p><strong>The model design</strong></p>

<ul>
<li>To run the analysis as multivariate, I use both <code>symptom</code> and <code>symptom:architecture</code> as IVs and I remove the intercept in both the fixed and random parts of the model. I do not include the main effect of <code>architecture</code> as an IV.  </li>
<li>The effect of <code>medication</code> should be the same within all clinics, so there is no random effect for this variable.  </li>
<li>I do not want to constrain equality between the effect of <code>architecture</code> on the different <code>symptoms</code>.  </li>
<li>Due to the multivariate nature of the analysis, I expect the residuals to be correlated, with different correlations between the 13 different symptoms; therefore I specify the covariance structure of residuals as <code>corSymm</code> (non-zero but unstructured, if I get this correctly).  </li>
<li>I also expect heteroscedasticity between the different symptoms (there should be more variance on certain symptoms), so I add the option <code>weights</code> as <code>(~ 1|symptoms)</code>.  </li>
</ul>

<p><strong>The end result</strong></p>

<p>This is the model I come up with:</p>

<pre><code>model1 = lme(fixed = Score ~ symptom + medication:symptom + architecture:symptom + medication:architecture:symptom - 1,  
+ random = ~ symptom - 1 | patient/clinic,  
+ correlation = corSymm,  
+ weights=varIdent(form= ~ 1|symptoms)  
+ method = ""ML"")  
</code></pre>

<p>In order to test the effect of the architectural variables, I would then compare this model to the following constrained model, dropping all the terms related to architecture:</p>

<pre><code>model2 = lme(fixed = Score ~ symptom + medication:symptom - 1,
+ random = ~ symptom - 1 | patient/clinic,
+ correlation = corSymm,
+ weights=varIdent(form= ~ 1|symptoms)
+ method = ""ML"")
</code></pre>

<p>I would then run this comparison with the command <code>anova(model1, model2)</code> and compare the log-likelihood of the two models.</p>

<p>Overall, do these model design and r specification look correct to you?
Thank you so much for your help!</p>
"
"0.0909090909090909","0.092714554082312"," 89021","<p>I have some experimental data that I'm trying to analyze.
I have 1 response variable and 3 explanatory variables (these are factor variables).
The explanatory variables are the presence of a disease(positive and negative),
a genetic profile (X and Y), and whether or not an MRI contrast agent was given
(YES and NO).  </p>

<p>Structure of data looks like this:</p>

<pre><code>     measurement   profile  disease contrast
1    -1.76269      X        NEG       YES
2    -0.34492      X        NEG       NO
3     0.57455      X        POS       YES
4     2.16539      X        POS       NO
            .      .          .        .
            .      .          .        .
            .      .          .        .
77   -1.76269      Y        NEG       YES
78   -0.34492      Y        NEG       NO
79    0.57455      Y        POS       YES
80    2.16539      Y        POS       NO
</code></pre>

<p>I looked into using ANOVA for this analysis but the post hoc <strong>Tukey HSD
looks at all possible combinations</strong> of the explanatory variables so <strong>it
makes far more comparisons than I actually care about.</strong> </p>

<p>We have some specific hypotheses that, e.g.:
X.NEG.NO will differ from Y.NEG.NO,
X.NEG.NO will differ form X.NEG.YES,
X.NEG.NO will differ from X.POS.NO, etc.</p>

<p><em><strong>(notice that each compared group ""consist"" from interaction of all three variables)</em></strong></p>

<p>How to get only some specific comparisons from TukeyHSD?
Is appropriate to make 
Is there a better approach?</p>

<pre><code>Reproducible example:

my.data &lt;- data.frame(measurement = rnorm(80),
                      my.profile = rep(c(""X"",""Y""), each = 40),
                      my.disease = rep(c(""NEG"",""NEG"",""POS"",""POS""), times=20),
                      my.contrast = rep(c(""NO"",""YES""), times = 40))
</code></pre>
"
"0.117363131703255","0.11969397463728"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.123091490979333","0.114123726974758"," 90668","<p>I'm working through the examples in Kruschke's <a href=""http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/"">Doing Bayesian Data Analysis</a>, specifically the Poisson exponential ANOVA in ch. 22, which he presents as an alternative to frequentist chi-square tests of independence for contingency tables.</p>

<p>I can see how we get information about about interactions that occur more or less frequently than would be expected if the variables were independent (ie. when the HDI excludes zero).</p>

<p>My question is how can I compute or interpret an <em>effect size</em> in this framework? For example, Kruschke writes ""the combination of blue eyes with black hair happens less frequently than would be expected if eye color and hair color were independent"", but how can we describe the strength of that association? How can I tell which interactions are more extreme than others? If we did a chi-square test of these data we might compute the  CramÃ©r's V as a measure of the overall effect size. How do I express effect size in this Bayesian context?</p>

<p>Here's the self-contained example from the book (coded in <code>R</code>), just in case the answer is hidden from me in plain sight...</p>

<pre><code>df &lt;- structure(c(20, 94, 84, 17, 68, 7, 119, 26, 5, 16, 29, 14, 15, 
10, 54, 14), .Dim = c(4L, 4L), .Dimnames = list(c(""Black"", ""Blond"", 
""Brunette"", ""Red""), c(""Blue"", ""Brown"", ""Green"", ""Hazel"")))

df

         Blue Brown Green Hazel
Black      20    68     5    15
Blond      94     7    16    10
Brunette   84   119    29    54
Red        17    26    14    14
</code></pre>

<p>Here's the frequentist output, with effect size measures (not in the book):</p>

<pre><code>vcd::assocstats(df)
                    X^2 df P(&gt; X^2)
Likelihood Ratio 146.44  9        0
Pearson          138.29  9        0

Phi-Coefficient   : 0.483 
Contingency Coeff.: 0.435 
Cramer's V        : 0.279
</code></pre>

<p>Here's the Bayesian output, with HDIs and cell probabilities (directly from the book):</p>

<pre><code># prepare to get Krushkes' R codes from his web site
Krushkes_codes &lt;- c(
  ""http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/Programs/openGraphSaveGraph.R"", 
  ""http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/Programs/PoissonExponentialJagsSTZ.R"")

# download Krushkes' scripts to working directory
lapply(Krushkes_codes, function(i) download.file(i, destfile = basename(i)))

# run the code to analyse the data and generate output
lapply(Krushkes_codes, function(i) source(basename(i)))
</code></pre>

<p>And here are plots of the posterior of Poisson exponential model applied to the data:</p>

<p><img src=""http://i.stack.imgur.com/lbjiW.png"" alt=""enter image description here""></p>

<p>And plots of the posterior distribution on estimated cell probabilities:</p>

<p><img src=""http://i.stack.imgur.com/saWCP.png"" alt=""enter image description here""></p>
"
"0.117363131703255","0.0957551797098243"," 91134","<p>consider the following example data:</p>

<pre><code>df1 &lt;- data.frame(customer=c(rep(""customer1"",5),rep(""customer2"",10),rep(""customer3"",7)),
                  money_spent=sample(22))

df2 &lt;- data.frame(customer=c(""customer1"",""customer2"",""customer3""),
                  origin=c(""US"",""US"",""UK""),
                  industry_sector=c(""IS1"",""IS2"",""IS3""),
                  currency=c(""USD"",""USD"",""GBP""))
</code></pre>

<p>My actual data consists of about 200000 rows and I would like to examine it in terms of, for instance, do customers from the US spent more money compared to customers from other countries. I would also like to see whether the amount of money spent depends on the industry sector and so on. I have some more explanatory variables apart from origin, industry sector and currency which I would like to look into. Also the number of records for the customers differ so that it might make sense to average the money spent for each customer.</p>

<p>I am not sure about how to best analyse this data. I first thought of cluster analysis, in particular, hierarchical clustering but am not sure whether it can be applied to such data and, in particular, how to structure the data to be put into the function. The R function <code>hclust</code> takes a matrix as the input but how would I structure such a matrix in terms of my data? Could k-means clustering be a better alternative? </p>

<p>Another approach would probably to analyse this data using boxplots and an one-way ANOVA approach to see whether ""money spent"" differs between different countries or industry sectors. However, this approach does not test whether the variables are dependend on each other. To look into this, I have been advised to apply a decision tree first and then do some statistical significance analysis. However, from what I have read so far I cannot see how decision trees can help me to detect variable dependencies.</p>

<p>So, I am wondering whether there any other/better techniques/functions out there which are more suitable for such data? Maybe a time series analysis is more appropriate since we have also recorded the dates when customers spent money.</p>
"
"0.148453923805041","0.151402232820893"," 91445","<p>I like to keep analyses all in SAS or all in R when I can help it and lately have been using R more and more, but there's one analysis that I do somewhat routinely that has given me trouble in R.</p>

<p>I have repeated measures data where I would like to fit the following model: $$Delta = Day + Group + Day\times Group$$ where $Delta$ is the change from baseline, $Day$ is the number of days from the beginning of the study, and $Group$ is the experimental group.  I fit a variance-covariance matrix to account for repeated measures (for this example I'm using compound symmetry, but the difference is the same using other variance-covariance matrices).  I have the data at the end of the post.</p>

<p>If I don't include the interaction, I can get the analysis to run as I want it to in both SAS and R.  In SAS:</p>

<pre><code>proc mixed data=df;
  class group day id;
  model delta = day group;
  repeated day / subject=id type=cs;
  lsmeans group / diff=all;
run;
</code></pre>

<p>In R:</p>

<pre><code>library(nlme)
library(lsmeans)
fit.cs &lt;- gls(Delta~Day+Group,
              data=df,
              corr=corCompSymm(,form=~1|ID))
anova(fit.cs,type=""marginal"")
lsmeans(fit.cs,pairwise~Group)
</code></pre>

<p>Obviously the results differ in terms of denominator DF, but I don't intend to start that discussion (unless that difference is causing the problem).  When I add interaction in SAS, everything is fine:</p>

<pre><code>proc mixed data=df;
  class group day id;
  model delta = day | group;
  repeated day / subject=id type=cs;
run;
</code></pre>

<p>But when I do the same from R...</p>

<pre><code>fit.cs &lt;- gls(Delta~Day*Group,
              data=df,
              corr=corCompSymm(,form=~1|ID))
# Error in glsEstimate(object, control = control) : 
#   computed ""gls"" fit is singular, rank 19
</code></pre>

<p>Why does R complain about the fit being singular but SAS doesn't? </p>

<p>Here are some fake data that are representative of data that I work with (from R):</p>

<pre><code>df &lt;- structure(list(Delta = c(-1.27, -0.34, 1.92, 0.45, 1.21, 0.43, -0.41, 0.16, -0.35,
1.49, -0.85, -0.86, 1.04, 0.49, 2.32, 0.13, -0.32, 0.5, 0.48, 1.21, -0.82, 0.93,
-0.58, 2.3, -0.9, 0.21, -0.72, 0.11, -0.28, -0.33, -0.7, -1.16, -0.23, -0.88, 0.97,
0.25, 0.8, 0.16, 0.63, -0.49, -0.63, -0.9, 1.1, -1.45, 0.38, -0.93, 0.4, 0.45, 0.48,
0.14, 1.02, -0.01, -1.98, 2.19, -1.53, -0.49, -1.57, -1.02, 1.09, 1.74, 0.54, -1.57,
-1.5, -0.48, 0.26, 0.2, -0.36, -1.05, -1.73, -0.77, -0.65, -1.07, -0.45, -0.14,
-0.56, 0.84, -2.66, -0.52, 1.44, 0.45, 0.24, -0.92), Day = structure(c(1L, 2L, 3L,
1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L,
1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L,
3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 1L, 2L, 3L, 6L,
7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L), .Label = c(""4"",
""7"", ""10"", ""12"", ""14"", ""16"", ""28""), class = ""factor""), Group = structure(c(1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""1"", ""2"",
""3"", ""4""), class = ""factor""), ID = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L,
4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 8L,
8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L,
12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 15L, 15L,
15L, 15L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L),
.Label = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", ""14"",
""15"", ""16"", ""17"", ""18""), class = ""factor"")), .Names = c(""Delta"", ""Day"", ""Group"", ""ID""),
class = ""data.frame"", row.names = c(NA, -82L))
</code></pre>

<p>Here's the same data for SAS:</p>

<pre><code>DATA  df;
INPUT Delta Day Group $ ID $ @@;
CARDS;
-1.27 4 1 1 -0.34 7 1 1 1.92 10 1 1 0.45 4 1 2 1.21 7 1 2 0.43 10 1 2
-0.41 4 1 3 0.16 7 1 3 -0.35 10 1 3 1.49 4 2 4 -0.85 7 2 4 -0.86 10 2
4 1.04 16 2 4 0.49 28 2 4 2.32 4 2 5 0.13 7 2 5 -0.32 10 2 5 0.5 16 2 5
0.48 28 2 5 1.21 4 2 6 -0.82 7 2 6 0.93 10 2 6 -0.58 16 2 6 2.3 28 2 6
-0.9 4 2 7 0.21 7 2 7 -0.72 10 2 7 0.11 16 2 7 -0.28 28 2 7 -0.33 4 2 8
-0.7 7 2 8 -1.16 10 2 8 -0.23 16 2 8 -0.88 4 3 9 0.97 7 3 9 0.25 10 3 9
0.8 16 3 9 0.16 28 3 9 0.63 4 3 10 -0.49 7 3 10 -0.63 10 3 10 -0.9 16 3 10
1.1 28 3 10 -1.45 4 3 11 0.38 7 3 11 -0.93 10 3 11 0.4 16 3 11 0.45 28 3 11
0.48 4 3 12 0.14 7 3 12 1.02 10 3 12 -0.01 16 3 12 -1.98 28 3 12 2.19 4 3 13
-1.53 7 3 13 -0.49 10 3 13 -1.57 16 3 13 -1.02 28 3 13 1.09 4 4 14 1.74 7 4 14
0.54 10 4 14 -1.57 16 4 14 -1.5 4 4 15 -0.48 7 4 15 0.26 10 4 15 0.2 16 4 15
-0.36 28 4 15 -1.05 4 4 16 -1.73 7 4 16 -0.77 10 4 16 -0.65 16 4 16 -1.07 28 4 16
-0.45 4 4 17 -0.14 7 4 17 -0.56 10 4 17 0.84 16 4 17 -2.66 28 4 17 -0.52 4 4 18
1.44 7 4 18 0.45 10 4 18 0.24 16 4 18 -0.92 28 4 18
;
RUN;
</code></pre>
"
"0.0371134809512603","0.0378505582052232"," 92616","<p>What's going on here?</p>

<pre><code>data.2
         subj phon f.amp
    1     1    V   100
    2     2    V    60
    3     3    V   124
    4     4    V    42
    5     5    V   210
    6     6    V   104
    7     7    V   150
    8     1    Ê”    92
    9     2    Ê”    33
    10    3    Ê”    92
    11    4    Ê”    32
    12    5    Ê”    90
    13    6    Ê”    65
    14    7    Ê”   105
    15    1    h   142
    16    2    h    72
    17    3    h   141
    18    4    h    60
    19    5    h   268
    20    6    h   134
    21    7    h   145
</code></pre>

<p>Pairwise comparison of levels PHON<sub>V</sub> and PHON<sub>h</sub> by running ANOVA on the pertinent subset of data:</p>

<pre><code>library(lme4)
anova(lmer(f.amp~phon+(1|subj),data.2[which(data.2[,2]!=""Ê”""),]))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  9.8144
</code></pre>

<p>Pairwise comparison of the same levels by direct definition of contrast coefficients; different resulting <em>F</em>-ratio:</p>

<pre><code>contrasts(data.2[,2],1)=matrix(c(-1,0,1),nrow=3)
contrasts(data.2[,2])
    [,1]
  V   -1
  Ê”    0
  h    1
anova(lmer(f.amp~phon+(1|subj),data.2))
   Analysis of Variance Table
        Df Sum Sq Mean Sq F value
   phon  1 2113.1  2113.1  1.2566
</code></pre>

<p>Since <em>df</em><sub>PHON</sub>, <em>SS</em><sub>PHON</sub> and <em>MS</em><sub>PHON</sub> are the same for both analyses; and since <em>F</em><sub>PHON</sub> = <em>MS</em><sub>PHON</sub> / <em>MS</em><sub>PHON x <em>S</em></sub>, I deduce that the analyses differ regarding the handling of <em>S</em>.</p>

<p>Any ideas as to how and why precisely?</p>
"
"0.0642824346533225","0.0655590899062897"," 93238","<p>I did model test of in anova. once mod1 vs mod2 , other mod2 vs mod1. Does it matter? CAn I compare two models of different df and different parameters? </p>

<pre><code>anova(reg5,reg4, test=""Chisq"")

Analysis of Variance Table
Model 1: Gas ~ CODload
Model 2: Gas ~ Flow * CODin
  Res.Df   RSS Df Sum of Sq  Pr(&gt;Chi)    
1     65 27786                           
2     63 12835  2     14952 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt; anova(reg4,reg5, test=""Chisq"")

Analysis of Variance Table
Model 1: Gas ~ Flow * CODin
Model 2: Gas ~ CODload

  Res.Df   RSS Df Sum of Sq  Pr(&gt;Chi)    
1     63 12835                           
2     65 27786 -2    -14952 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0371134809512603","0.0378505582052232"," 93378","<p>I got the following outcome. I am confused with the interpretation. When I consider RSS then model 2 is better than model 1. What does that high p value mean? Does in influence my conclusion?? </p>

<blockquote>
  <p>anova(reg4.3,reg4.4, test=""Chisq"")</p>
</blockquote>

<pre><code>Analysis of Variance Table

Model 1: Gas ~ Flow * CODin + A + B + C
Model 2: Gas ~ Flow * CODin + A + B + C + Blue + Green

  Res.Df     RSS Df Sum of Sq Pr(&gt;Chi)
1     60 10034.5     
2     58  9851.3  2    183.21   0.5831
</code></pre>
"
"0.192970411092812","0.196802821771055"," 94057","<p>I have an agricultural field experiment (testing a plant protection agent):</p>

<p><strong>Split plot design</strong> with: </p>

<pre><code>2 whole plot treatments ""infestation"": ""high"" &amp; ""low"" 
8 split-plot treatments (""treat""): 

1. Untreated Control (""Ctrl1"")
2. Reference Product (""Ctrl2"")
3. 1 x Test-Product 1
4. 2 x Test-Product 1
5. 3 x Test-Product 1
6. 1 x Test-Product 2
7. 2 x Test-Product 2
8. 3 x Test-Product 2

and 4 replicates (""block""):
</code></pre>

<p>The parameter of interest in this example is grain (<strong>yield</strong>):</p>

<p>First, I could model this:</p>

<pre><code>lme(yield ~ infestation * treat, random = ~ 1 | block/infestation, data)
</code></pre>

<p>or</p>

<pre><code> lmer(yield ~ infestation * treat + (1 | block/infestation), data)
</code></pre>

<p>But as can be seen treatments 3-8 can and have to be recoded as 2 products (""prod"") being tested 1-3 times (""times""), so I have a 2x3 subdesign.</p>

<p>One possibility would be subsetting the data: </p>

<pre><code>  data2 &lt;- subset(data, !data$treat == ""Ctrl1"" &amp;  !data$treat == ""Ctrl2"")
</code></pre>

<p>and recode the resting treatments to ""prod"" = 1,2 and ""times"" = 1:3
then run:</p>

<pre><code>lme(yield ~ infestation * form * times, random = ~ 1 | block/infestation, data)
</code></pre>

<p>Afterwards I could still do contrasts to compare the control treatments with the treated ones. </p>

<p>But (here my actual problem starts): I read an article of</p>

<p><strong>H.P. Piepho</strong>: ""<em>A Note on the Analysis of Designed Experiments with Complex Treatment Structure</em>"", 
HortScience 41(2):446--452. 2006</p>

<p>The author wants to show ""<em>how a meaningful analysis can be obtained based on a linear model by appropiate coidng of factors. (...) Our main objective is to demonstrate that the introduction of dummy variables can conveniently solve a wide variety of inferential problems that would otherwise either require ... multiple linear contrasts... or not make fully eficient use of the data, e.g when only data from orthogonal subdesigns are analysed.</em>""  </p>

<p>A very similar example (Example 1 in the article) is discussed within, and an alternative analysis in SAS is proposed - which I wanted to try to realise in R. </p>

<p>The author adds a dummy variable (<strong>ctrl_vs_trt</strong>) to the data and codes it: ""control"", ""trt"" (in my case <strong>trt</strong>, <strong>Ctrl1</strong>, <strong>Ctrl2</strong>"". </p>

<p>The he uses: 
(in his case <strong>prod</strong> is <strong>form</strong> ulation, and <strong>times</strong> is <strong>conc</strong> entration)</p>

<pre><code>PROC GLM;
CLASS block contr_vs_trt form conc;   ## 
MODEL set = block contr_vs_trt
        contr_vs_trt * form
    contr_vs_trt * conc
    contr_vs_trt * form * conc;
RUN.                    
</code></pre>

<p>I cite a further paragraph: 
""<em>Of course, a test for contr_vs_trt is not produced with this model, and one cannot compute simple means or marginal means. Also, the Type I SS for <strong>form</strong>, <strong>conc</strong>, and <strong>form x conc</strong> are not the same as with Type III SS. With Type III SS, the test for form is adjusted for <strong>conc</strong>, as fitting <strong>conc</strong> takes out the control when coding factors as in Table ""xy"" (as I did here). Similarly, the test for <strong>conc</strong> is adjusted for <strong>form</strong>, because fitting of <strong>form</strong> takes out the control. As a result, the Type III ANOVA for the model <strong>form x conc</strong> turns out to be that for the 3x2 factorial subdesign. (...)
It seems much more stringent and transparent to use the nested model <strong>contr_vs_trt/(form x conc)</strong>, as this properly reflects all nesting and crossing features of the design.</em>""</p>

<p>Now, how to do that in <code>lme</code> or <code>lmer</code>?</p>

<p>lme does not run at all, even if I simplify to: </p>

<pre><code> lme(yield ~ prod * times, random = ~1|block, data), 
 I get
 Error in MEEM(object, conLin, control$niterEM) : 
 Singularity in backsolve at level 0, block 1
</code></pre>

<p>The term <strong>prod * times</strong> cannot be run (<strong>prod + times</strong> logically can). Eliminating both controls from the data set resolves this problem. </p>

<p><code>lmer</code> runs with <strong>prod * times</strong>, but always given the message:</p>

<pre><code> fixed-effect model matrix is rank deficient so dropping ""x"" columns / coefficients
</code></pre>

<p>I understand that the subdesign is not orthogonal and therefor dropping is occuring, but I cannot say if the analysis after dropping can still be right. </p>

<p>Also, I do not know how to specify the full model (leaving out the ""infestation"" whole plot for a second):</p>

<pre><code>lmer(yield ~ prod * times + (1|block/ctr_vs_trt), data)
</code></pre>

<p><strong>prod * times</strong> is nested inside <strong>ctr_vs_trt</strong> but both are nested inside the same block (or whole plot).
Is nesting of fixed effects possible in <code>lme</code> or <code>lmer</code> - does it work as I proposed?
Does it even make sense to run the full model?</p>

<p>With <code>aov()</code> I get the model running, even the partitioning of Df's is right. But due to strong non-orthogonality it is not possible to assume that the results are right.</p>

<p>I can get meaningful results subsetting and using contrasts, but I found the authors approach interesting and it would help in the analysis of some of my other trials. 
Thanks in advance for any help; I hope this question is not too long...</p>
"
"NaN","NaN"," 94173","<p>Quite a simple question, looking to see if anybody has experience using both. I have no knowledge of PRIMER, but wondering if R (currently) has the same capacity for data analysis specifically for ecological data using things like perMANOVAs and NMDS plotting. </p>

<p>I know it can be done in R (what can't!?) but I'm more concerned if it's a much longer route to the same conclusion.</p>
"
"0.135020119496819","0.146881740676447"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.0829882662886615","0.0846364211331916"," 97165","<p>I have the following data set:</p>

<pre><code>                 |       Scenario 1       |     Scenario 2         |
                 |Trial 1|Trial 2| Trial 3|Trial 1|Trial 2| Trial 3|
 -------------------------------------------------------------------
              S1 | ...
 Condition 1  S2 | ...
              S3 |
 -------------------------------------------------------------------
              S5 |
 Condition 2  S6 |
              S7 |
</code></pre>

<p>Thus the <code>Trials</code> are nested in the <code>Scenarios</code> and all of them are within subject. I am trying to run an ANOVA on this data set. Here is the model without defining that <code>Scenarios</code> (and <code>Trials</code>) are within subject.</p>

<pre><code> my_data.aov &lt;- aov(value~Condition*Trial%in%Scenario,data=my_data) #works fine
</code></pre>

<p>But when I specify that these are within subject:</p>

<pre><code>my_data.aov &lt;- aov(value~Condition*Trial%in%Scenario+Error(Player/(Trial%in%Scenario)),data=my_data) 
</code></pre>

<p>I get the following error </p>

<pre><code>In aov(value ~ Condition * Trial % in % Scenario + Error(Player/(Trial %in%  :
Error() model is singular
</code></pre>

<p>The closest set-up I could find was <a href=""http://stats.stackexchange.com/questions/13788/split-plot-in-r"">Split plot in R</a> but there the subjects are nested inside each <code>Trial</code> not in each <code>Condition</code>.</p>

<p><strong>EXAMPLE FILE</strong></p>

<p>Here is an example <a href=""http://www23.zippyshare.com/v/88275827/file.html"" rel=""nofollow"">file</a> in long format.</p>

<p><strong>What about this approach?</strong></p>

<p>If I treat each <code>Trial</code> as a sample, then I can collapse across <code>Scenarios</code> by averaging them, so I will have a simpler model, where each <code>Subject</code>'s behavior is described per <code>Scenario</code>. And since I need to analyze the relationship of <code>value~Condition*Scenario</code> I can do so by defining the <code>Error</code> like <code>Error(Subject/Scenario)</code>. </p>

<p>Will this approach invalidate my analysis?</p>
"
"0.0829882662886615","0.0846364211331916"," 97234","<p>I want to compare 6 designs of spoons (D1,D2,D3,D4,D5,D6) in 20 children (blocks).I also want to see whether holding the spoon in right or left hand affects food-pinching response(number of M&amp;M's picked and placed in cup). 
I have dataset with response % noted for each design type on all of the 20 children, and the information about which hand is used is represented as zero or one.</p>

<p>I am using Randomized Complete Block Design (with children as blocks) to perform the test.
I can use ANOVA in R to check the effect:</p>

<pre><code>summary(aov(response~blocks+designs))
</code></pre>

<p>Should children and  hand data be included in the model. Would the information about hand impact the outcome.</p>

<p><strong>EDIT</strong>:
I am confused about how to compare whether holding spoon in right or left hand affects the number of M&amp;M's picked. Should this information change the above mentioned ANOVA analysis.</p>

<p>Below is the sample of data set which includes data for only 10 children for two design types.</p>

<pre><code>Response  Design Children Hand
20.11   D1  1   0
30.26   D1  2   1
28.56   D1  3   1
23.19   D1  4   1
23.29   D1  5   0
27.62   D1  6   0
29.0    D1  7   1
30.5    D1  8   1
28.23   D1  9   0
29.98   D1  10  1
25.7    D2  1   0
26.2    D2  2   1
19.2    D2  3   1
24.32   D2  4   1
25.3    D2  5   0
30.45   D2  6   0
28.98   D2  7   1
18.85   D2  8   1
32.7    D2  9   0
29.8    D2  10  1
</code></pre>

<p>Any pointers will be helpful.</p>
"
"0.0989692825366941","0.11355167461567"," 99765","<p>I'm trying to run a tree-way repeated measures ANOVA on the following data: I have a completely balanced design with three within-subject factors (type, form and ch - channel) and one dependent variable amp (amplitude).</p>

<p>I'm inclined to believe the results I got using <code>aov</code> function:</p>

<pre><code>res &lt;- aov(amp ~ type*form*ch + Error(sbj/(type*form*ch)), data = p3vals)
</code></pre>

<p>Here is the anova table I have:</p>

<pre><code>Error: Within
               Df Sum Sq Mean Sq F value   Pr(&gt;F)    
type            1   25.0  24.950  12.315 0.000462 ***
form            1   12.9  12.910   6.372 0.011693 *  
ch              1    0.9   0.875   0.432 0.511113    
type:form       1    3.1   3.123   1.542 0.214581    
type:ch         1    0.9   0.938   0.463 0.496404    
form:ch         1    1.3   1.256   0.620 0.431239    
type:form:ch    1    3.0   2.960   1.461 0.226974    
Residuals    1514 3067.3   2.026                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, after reading several pages of similar examples(including <a href=""http://stats.stackexchange.com/questions/14088/why-do-lme-and-aov-return-different-results-for-repeated-measures-anova-in-r"">Why do lme and aov return different results for repeated measures ANOVA in R?</a> ) I decided to try <code>lme</code> and <code>lmer</code> functions from name and lme4 packages for further pairwise multiple comparisons using <code>glht</code> from multcomp package.
In the example mentioned above F values are at least closet those obtained using <code>aov</code>, but I cannot figure out how to get any meaningful results.</p>

<pre><code>lme_p3amp = lmer(amp ~ type*form*ch + (1|sbj) + (1|type:sbj) + (1|form:sbj) + (1|ch:sbj), data = p3vals)
&gt; anova(lme_p3amp)
Analysis of Variance Table
             Df  Sum Sq Mean Sq F value
type          1  0.0266  0.0266  0.0433
form          1  2.0782  2.0782  3.3863
ch            1 28.5513 28.5513 46.5227
type:form     1  2.1980  2.1980  3.5815
type:ch       1  2.9789  2.9789  4.8539
form:ch       1  0.9278  0.9278  1.5118
type:form:ch  1  6.6072  6.6072 10.7661
</code></pre>

<p>and lme produces the following result:</p>

<pre><code> anova(lme(amp ~ type*form*ch, random=list(sbj=pdBlocked(list(~1, pdIdent(~type-1), pdIdent(~form-1), pdIdent(~ch-1)))), data=p3vals))
             numDF denDF  F-value p-value
(Intercept)      1  1508 32.56485  &lt;.0001
type             1  1508  0.02920  0.8643
form             1  1508  3.54422  0.0599
ch               1  1508  8.05747  0.0046
type:form        1  1508  2.79623  0.0947
type:ch          1  1508  3.78969  0.0518
form:ch          1  1508  1.18037  0.2775
type:form:ch     1  1508  8.40553  0.0038
</code></pre>

<p>I'd appreciate if you tell me what is wrong with my code and how can I perform a valid post hoc analysis.</p>
"
"0.189726979909073","0.165852836345304"," 99952","<p><strong>Please read edit 3 first</strong></p>

<p>I am trying to find out the significant factors in a dataset of percentages, a sample of which are below. The difficulty is that the data violates the assumptions of ANOVA, and most of the data are fairly close to 100%.</p>

<p>Please note that glm + binary would not work: the samples used to calculate each proportion are not independent. I do have access to the denominator, if that helps.</p>

<p>Any direction where to start? I've read quite a few things here and elsewhere (notably trying to use some transformation such as arcsin, etc...) and also some other methods I never heard of (""contingency table approaches""). As in a ""textbook ANOVA"" I would like to know which factors are significant, and how much of the variability they explain.</p>

<pre><code>data = c(0.79,0.98,0.95,0.95,1,0.98,0.99,0.97,0.99,0.99,0.98,0.99,0.99,0.94,0.94,  
0.86,0.84,0.86,0.97,0.96,0.53,0.87,0.97,0.81,0.99,1,0.99,0.87,0.98,0.97,0.93,0.8,  
0.7,0.94,0.89,0.98,0.89,0.98,0.96,0.98)
</code></pre>

<p><strong>Edit:</strong> sorry for the lack of clarity. Here's how my percentages are calculated: I basically throw a number of particles (known only a posteriori) in a funnel and count how many make it through/how many remain stuck in the funnel. The percentage is the ratio of the particles having made it through divided by the total number of particles.If a particle which comes at the beginning of the trial remains stuck, the odds of a subsequent particle to remain stuck are higher. As such, I don't think I can apply a generalized linear model, specifying binomial as familly (in R I mean). But again, my statistical insights may be utterly wrong.</p>

<p><strong>Edit2:</strong> regarding independance, I guess my comment was misleading. Each sample in the vector above is independant from the others. However, as I explain in the edit above, the samples used to calculate each percentage are not. </p>

<p><strong>Edit3:</strong>
Well, I reckon I probably made a mess of my question, think it may help if I rephrase the problem and show some data (a fraction of my whole dataset). Besides, I have progressed a bit, hopefully in the right direction. I do not know if I should do a full edit of my original question, or even abandon and start anew, I'll add this as an edit for the interest of history (let me know if I should do differently).</p>

<p>My response variable is the percentage of particles having made it through a funnel (the total number of particles is different for each percentage). If a particle at the beginning of the trial remains stuck, the odds of a subsequent particle to remain stuck are higher.</p>

<p>The (potential) explanatory variables are 1) the type of particles, 2) the funnel type, 3) the funnel position and 4) the total number of particles. In a first stage, I want to find which of these actually impact the response variable and by how much.
In a second stage, I would like to use the current dataset as a reference for the analysis of future samples.  Precisely, I would like to know if the percentage of particles having made through the funnel is  significantly different from the reference dataset and by how much. </p>

<p>Plotting the data indicates me that each population may have a different mean and a different variance.</p>

<pre><code># Libraries ####
library(ggplot2)
library(betareg)
library(lmtest)

#Create data####
df5 = structure(list(type = c(""Type1"", ""Type1"", ""Type1"", ""Type2"", ""Type2"", 
                              ""Type2"", ""Type2"", ""Type2"", ""Type2"", ""Type3"", ""Type3"", ""Type3"", 
                              ""Type1"", ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", 
                              ""Type2"", ""Type2"", ""Type1"", ""Type1"", ""Type1"", ""Type1"", ""Type1"", 
                              ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", ""Type2"", 
                              ""Type1"", ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type1"", 
                              ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", 
                              ""Type2"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", ""Type2""), 
                     funnelType = c(""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", ""fType2"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2""), 
                     position = c(""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", 
                                  ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", 
                                  ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", 
                                  ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", 
                                  ""a"", ""b"", ""c"", ""a"", ""b"", ""c""), 
                     total = c(420L, 293L, 324L, 549L, 
                               527L, 603L, 533L, 571L, 438L, 496L, 534L, 604L, 489L, 360L, 383L, 
                               524L, 560L, 606L, 493L, 513L, 572L, 530L, 527L, 543L, 616L, 471L, 
                               554L, 392L, 530L, 443L, 561L, 529L, 599L, 529L, 481L, 521L, 621L, 
                               567L, 609L, 447L, 398L, 462L, 528L, 574L, 522L, 654L, 531L, 556L, 
                               642L, 569L, 684L, 372L, 540L, 345L), 
                     percentage = c(0.98808, 0.95569, 0.9784, 0.57741, 0.81017, 0.82919, 0.93809, 0.94041, 
                                    0.94744, 0.93352, 0.98129, 0.99006, 0.97339, 0.94728, 0.98695, 
                                    0.9542, 0.84989, 0.92574, 0.79116, 0.92782, 0.98077, 0.96605, 
                                    0.97155, 0.96503, 0.97076, 0.95756, 0.99097, 0.96429, 0.9434, 
                                    0.99097, 0.78253, 0.94518, 0.97664, 0.99056, 0.96261, 0.98273, 
                                    0.88402, 0.96824, 0.92118, 0.95524, 0.97991, 0.9762, 0.83144, 
                                    0.95643, 0.98085, 0.95107, 0.94162, 0.98741, 0.83635, 0.94372, 
                                    0.98099, 0.88447, 0.94817, 0.95365)), 
                .Names = c(""type"", ""funnelType"",""position"", ""total"", ""percentage""), class = ""data.frame"", 
                row.names = c(2L, 
                              3L, 4L, 18L, 19L, 20L, 50L, 51L, 52L, 66L, 67L, 68L, 98L, 99L, 
                              100L, 114L, 115L, 116L, 130L, 131L, 132L, 146L, 147L, 148L, 162L, 
                              163L, 164L, 194L, 195L, 196L, 210L, 211L, 212L, 226L, 227L, 228L, 
                              258L, 259L, 260L, 306L, 307L, 308L, 322L, 323L, 324L, 354L, 355L, 
                              356L, 370L, 371L, 372L, 386L, 387L, 388L))

ggplot(df5,aes(x=total,y=percentage))+geom_boxplot(outlier.shape = NA)+geom_jitter(aes(col=position))+facet_grid(type~funnelType)
</code></pre>

<p>It does seem that I am in a situation comparable the dyslexic example in <a href=""http://psycnet.apa.org/journals/met/11/1/54/"" rel=""nofollow"">http://psycnet.apa.org/journals/met/11/1/54/</a>. I consequently think that I should be able to apply the same method to analyse my data. To do so I use the package betareg in R (no interactions for the sake of the example).</p>

<pre><code>full &lt;- betareg(percentage ~ type+funnelType+position+total|type+funnelType+position+total,data = df5)
summary(full)
</code></pre>

<p>This yields the results that all explanatory variables but ""total"" have a significant effect on the mean. From what I know explanatory variables are likely to be significant in large data sets (I have a relatively large number of observastions in my full dataset). How do I judge the magnitude of the effect of each factor? What should I do if two of the explanatory variables are correlated?</p>

<p>In my second stage I want to compare a new sample to the subpopulation in my dataset that has the same predictor values. How should I perform this? Unequal variance t-test?</p>
"
"0.128564869306645","0.120191664828198","100284","<p>I'm hoping one of the more experienced R programmers/statisticians out there can help me with something, or explain why what I'm trying to do is stupid :-)</p>

<p>In the past, I've done analysis of survey data in SPSS.  I'll often create contingency tables that uses a computed segmentation variable to see if members of the segments exhibit different behavior along an outcome variable.  Often the outcome variable is a response along a 5 or 7 point scale.  Sometimes it might be a binary yes or no.  </p>

<p>Here is what my analysis would typically look like in SPSS:
<img src=""http://i.stack.imgur.com/oXaKF.png"" alt=""enter image description here""></p>

<p>In this case, I grouped respondents into one of 3 segments.  I then re-coded a 5-point response to q39_1 into a binary grouping of 4/5 and 1-3.  I did this because my n's were small in a few of the boxes.  You can see that the response does seem to be dependent on the respondent's segment.  The Chi-square test confirms this.</p>

<p>I've managed to recreate this in R using the survey package with the following code:</p>

<pre><code>&gt; data &lt;- read.spss(""sample_data.sav"", to.data.frame=T)
&gt; design &lt;- svydesign(ids=~1, weights=~Weight, data=data)
&gt; svytable(~q39_1_recode2+alt_segment, design) -&gt; table2
&gt; prop.table(table2,2) -&gt; table2prop
&gt; table2
             alt_segment
q39_1_recode2         1         2         3
            1  68.05988  88.23581  40.40518
            2  31.97007 115.05503 168.27403
&gt; table2prop
             alt_segment
q39_1_recode2         1         2         3
            1 0.6803950 0.4340373 0.1936234
            2 0.3196050 0.5659627 0.8063766
&gt; chisq.test(round(table2,digits=0))

    Pearson's Chi-squared test

data:  round(table2, digits = 0)
X-squared = 71.4848, df = 2, p-value = 3.001e-16
</code></pre>

<p>Now comes the snag.  In the SPSS output, you'll notice that in the contingency table there are little subscripts that indicate where the column proportions are different from each other at an alpha of .05.  I can toggle this column proportion on or off through a checkbox when I create the table.  You can set the alpha level and then toggle a setting to adjust the p-values for multiple comparisons using the Bonferroni method.</p>

<p>I'm obviously not a statistician, but my assumption was that this is essentially doing the z-test equivalent of pairwise t-tests that you might see from an ANOVA analysis.  I find this type of analysis useful because not only do I now know that there is a relationship between the variables, I can now describe what that relationship is.</p>

<p><strong>So, the question is:  How do I replicate these pairwise z-tests in R while correcting for the multiple comparisons?  Is this in fact the correct ""next-step"" after a chi-squared test of independence?</strong>  I've had trouble finding material that discusses what the next steps in an analysis should be after rejecting the null hypothesis in chi-sq test.  I feel like I'm either missing something that's pretty easy or I'm trying to do something that I shouldn't be.  Any help would be appreciated.</p>
"
"0.0757575757575758","0.092714554082312","100509","<p>I have a function for performing one-way ANOVA in R:</p>

<pre><code>cond&lt;-gl(4,5,20,label=c(""a"",""b"",""c"",""d""))

aof&lt;-function(x){
m&lt;-data.frame(cond,x);
anova(aov(x~cond,m))
}

anova.results&lt;-apply(x,1,aof)
</code></pre>

<p>I use it to perform ANOVA (testing four conditions, each with 5 samples) on several hundred rows of <code>x</code> (genes). However, I need to drop some samples such that two of my conditions have 5 samples and two have 4. I'm having trouble reworking my function to allow for ANOVA with unequal sample sizes.</p>

<p>Any help is greatly appreciated!</p>

<p>Edit for clarification: I am using R, but no special package, just the standard <code>stats</code> package.</p>

<p>From what I understand, the <code>gl</code> function specifies the factor level pattern. <code>gl(n,k,n*k,labels)</code> such that <code>n</code> is the number of levels (four conditions, in my case), <code>k</code> is the number of replications (5 for my original analysis), and <code>n*k</code> is the total number of observations. <code>x</code> is a matrix with 20 columns, such that the five samples for condition <code>a</code> are first, then condition <code>b</code>, etc. The rows have values for each sample, and the <code>anova</code> function is done to each row individually.</p>

<p>I have tried changing <code>k</code> to <code>c(5,4,4,5)</code> for unequal sample sizes, but R says it is an <code>invalid 'times' value</code>. So, I'm trying to figure out another way to rework this function, or write another ANOVA function.</p>
"
"0.148453923805041","0.151402232820893","101265","<p>I was trying to use the nlme package in r to do a multilevel linear model.</p>

<p>I have yield as response variable and rainfall as predictor variable for 60 years for 6 different locations (State). I am trying to see whether rainfall has same level of effect on yield in all locations or different effects. In principle, I am trying to see if slope of yield vs rainfall significantly varies between locations. Therefore rainfall is my random effect. I built my model like this: </p>

<pre><code> # baseline model which only includes intercept
mdl1&lt;-gls(yield ~ 1,data = data, method=""ML"")

#intercept as random effect
mdl2&lt;-lme(yield ~ 1,data=data,random = ~1|state,method=""ML"")  

 # slope as random effect
mdl3&lt;-lme(yield ~ rain, data = data, random = ~rain|state,method=""ML"")

##compare the three model
anova(mdl1,mdl2,mdl3)
##this shows me when I add slope as random effect, my model shows better fit compared to baseline model (mdl1)
</code></pre>

<p>this is all working fine. The problem starts when I do the same analysis using an another predictor variable (a count data)</p>

<pre><code> # baseline model which only includes intercept: Works fine
mdl4&lt;-gls(yield ~ 1,data = data, method=""ML"")

#intercept as random effect - works fine
mdl5&lt;-lme(yield ~ 1,data=data,random = ~1|state,method=""ML"")  

 # include different predictor (break) this time instead of rain
mdl6&lt;-lme(yield ~ break, data = data, random = ~break|state,method=""ML"")
</code></pre>

<p>when i run the mdl 6, this gives me the error</p>

<pre><code>Error in lme.formula(res_yld ~ brk, data = data, random = ~brk | state,  : 
nlminb problem, convergence error code = 1
message = iteration limit reached without convergence (10)
</code></pre>

<p>I have absolutely no clue why is this happening. Everything worked fine for my first predictor but this does not work on another predictor. What am I doing wrong here? I tried reading about this online but the posts are not very clear to me. I would really appreciate of anyone could me out on this.
Thanks</p>
"
"0.0742269619025206","0.0757011164104465","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.0371134809512603","0.0378505582052232","102703","<p>I want to perform an ANOVA using R. I have three populations, represented by their respective means and SD:
Pop.1: 5.5 +- 0.4 (n=100)
Pop.2: 5.9 +- 0.3 (n=150)
Pop.3: 6.2 +- 0.5 (n=200)</p>

<p>Which is the exact code using R to perform the ANOVA using exclusively these data?</p>

<p>Moreover, how can I perfom subsequently a post-hoc analysis?</p>

<p>Thanks in advance</p>
"
"0.181818181818182","0.185429108164624","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.123819131746727","0.126278191454325","108537","<p>I'm trying to understand the reason why anova(f1, f2, f3) and anova(f1, f2) gives me different result while anova(f1, f2, f3) and anova(f2, f3) gives me the same.</p>

<p>Here's the code:</p>

<pre><code>&gt; data(swiss)
&gt; fit1 &lt;- lm(Fertility ~ Agriculture, data=swiss)
&gt; fit3 &lt;- lm(Fertility ~ Agriculture + Examination + Education, data=swiss)
&gt; fit5 &lt;- lm(Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality, data=swiss)

&gt; class(fit1)
[1] ""lm""
&gt; class(fit3)
[1] ""lm""
&gt; class(fit5)
[1] ""lm""

&gt; anova(fit1, fit3, fit5)
Analysis of Variance Table

Model 1: Fertility ~ Agriculture
Model 2: Fertility ~ Agriculture + Examination + Education
Model 3: Fertility ~ Agriculture + Examination + Education + Catholic + 
    Infant.Mortality
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     45 6283.1                                  
2     43 3180.9  2    3102.2 30.211 8.638e-09 ***
3     41 2105.0  2    1075.9 10.477 0.0002111 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt; anova(fit1, fit3)
Analysis of Variance Table

Model 1: Fertility ~ Agriculture
Model 2: Fertility ~ Agriculture + Examination + Education
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     45 6283.1                                  
2     43 3180.9  2    3102.2 20.968 4.407e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt; anova(fit3, fit5)
Analysis of Variance Table

Model 1: Fertility ~ Agriculture + Examination + Education
Model 2: Fertility ~ Agriculture + Examination + Education + Catholic + 
    Infant.Mortality
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     43 3180.9                                  
2     41 2105.0  2    1075.9 10.477 0.0002111 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>To me, this is read like this:</p>

<ul>
<li><code>anova(fit1, fit3, fit5)</code> shows me P value 8.638e-09 for the comparison of f1
and f3. </li>
<li><code>anova(fit1, fit3)</code> shows me 4.407e-7 for the same comparison. And this doesn't make sense!</li>
<li><code>anova(fit1, fit3, fit5)</code> and <code>anova(fit3, fit5)</code> shows the same P value for the comparison of fit3 and fit5 and it's 0.0002111.</li>
</ul>

<p>Probably I missed something. What is it?</p>
"
"0.104972776216296","0.107057545514438","108677","<p>I have used lme4 for mixed effects models of reaction times and accuracy rates. I could not use lmerTest because the type of model I was using are not yet implemented there (problem with predictors that are factors). I was able to get p-values for the models ran on accuracy rates (based on Wald z-values) but not for the models built on reaction times.
In order to get p-values for all models, I used Anova in the car package which gives me Wald chisquare values and probability of significance based on those chisquares. 
My concern is that sometimes for the accuracy rates, the effects indicated as significant in the analysis of deviance table (with the Wald chisquare values) are not significant in the mixed effect models. That is even for main effects of a factor with only 2-levels.
Does anyone know why this could be the case?</p>
"
"0.0524863881081478","0.0535287727572189","108899","<p>Can anyone explain the theory (or the formula) about computing Sum Sq (bold highligh below) related to regression items?  The Wikipedia <a href=""http://en.wikipedia.org/wiki/Partition_of_sums_of_squares"" rel=""nofollow"">link</a> gives an introduction on how to calculate the total, model, and regression sum of squares. Is it similar to the Sum Sq computation? Is the regression sum of squares equal to (0.000437+ 0.002545+ 0.060984+ 0.062330+ 0.060480)?</p>

<pre><code>TraingData &lt;- data.frame(x1 = c(3.532,2.868,2.868,3.532,2.868,2.536,3.864),
                         x2 = c(1.992,1.992,1.328,1.328,1.328,1.66,1.66),
                         y  = c(9.040330254,8.900894412,8.701929163,9.057944749,
                                8.701929163,8.74317832,9.10859913)
                         )
lm.sol &lt;- lm(y~1+x1+x2+I(x1^2)+I(x2^2)+I(x1*x2), data=TraingData)
anova(lm.sol)

Analysis of Variance Table

Response: y
            Df   **Sum Sq**     Mean       Sq F    value Pr(&gt;F)
x1          1   0.000437  0.000437    0.1055    0.8001
x2          1   0.002545  0.002545    0.6141    0.5768
I(x1^2)     1   0.060984  0.060984   14.7162    0.1623
I(x2^2)     1   0.062330  0.062330   15.0409    0.1607
I(x1 * x2)  1   0.060480  0.060480   14.5945    0.1630
Residuals   1   0.004144  0.004144  
</code></pre>
"
"0.111340442853781","0.11355167461567","110917","<p>Suppose I create a dummy scenario as such:</p>

<pre><code>&gt; A &lt;- rnorm(10000) 
&gt; B &lt;- rnorm(10000) 
&gt; C &lt;- rnorm(10000) 
&gt; Y &lt;- A*B + rnorm(10000,sd=0.1)
</code></pre>

<p>Doing a simple ANOVA correctly identifies that none of the variables are significantly predictive of the outcome:</p>

<pre><code>&gt; anova(lm(Y~A+B+C))
Analysis of Variance Table

Response: Y
            Df  Sum Sq Mean Sq F value Pr(&gt;F)
A            1     1.5 1.54411  1.4209 0.2333
B            1     0.3 0.28909  0.2660 0.6060
C            1     1.6 1.62425  1.4946 0.2215
Residuals 9996 10862.8 1.08672    
</code></pre>

<p>But not let's say I decide to include the interaction terms:</p>

<pre><code>&gt; anova(lm(Y~A*B*C))
Analysis of Variance Table

Response: Y
           Df  Sum Sq Mean Sq    F value    Pr(&gt;F)    
A            1     1.5     1.5 1.5281e+02 &lt; 2.2e-16 ***
B            1     0.3     0.3 2.8610e+01  9.05e-08 ***
C            1     1.6     1.6 1.6074e+02 &lt; 2.2e-16 ***
A:B          1 10761.8 10761.8 1.0650e+06 &lt; 2.2e-16 ***
A:C          1     0.0     0.0 9.8700e-02    0.7534    
B:C          1     0.0     0.0 1.5062e+00    0.2197    
A:B:C        1     0.0     0.0 1.6790e-01    0.6820    
Residuals 9992   101.0     0.0                         
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>It has correctly identified the interaction between A and B as being the most significant, but now for some reason the individual terms A and B have also gained significance... and C which had nothing at all to do with creating the model is significant as well?  Either I have not written the test correctly or I am completely misunderstanding how a Two-Way ANOVA with interaction terms works</p>

<p>Using a simple linear model gives expected results:</p>

<pre><code>&gt; summary(lm(Y~A*B*C))

Call:
lm(formula = Y ~ A * B * C)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.29566 -0.06667 -0.00092  0.06665  0.33620 

Coefficients:
              Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) -0.0003212  0.0009707   -0.331    0.741    
A            0.0003483  0.0009613    0.362    0.717    
B            0.0003184  0.0009619    0.331    0.741    
C           -0.0003213  0.0009702   -0.331    0.741    
A:B          1.0008711  0.0009370 1068.214   &lt;2e-16 ***
A:C         -0.0014855  0.0009588   -1.549    0.121    
B:C          0.0008860  0.0009561    0.927    0.354    
A:B:C       -0.0002489  0.0009085   -0.274    0.784    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.09705 on 9992 degrees of freedom
Multiple R-squared:  0.9913,    Adjusted R-squared:  0.9913 
F-statistic: 1.634e+05 on 7 and 9992 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.0524863881081478","0.0535287727572189","113690","<p>I have used ANOVA function so that I can get the overall p value of significant factors:</p>

<pre><code>&gt; anova(lmer81)
Analysis of Variance Table
                    Df Sum Sq Mean Sq F value
sex                  1   0.12   0.118  0.0195
vowel3               2 399.96 199.982 33.0859
Language             2 120.41  60.204  9.9604
sex:vowel3           2  89.73  44.865  7.4227
sex:Language         2 166.93  83.463 13.8084
vowel3:Language      4  48.27  12.067  1.9964
sex:vowel3:Language  4  52.76  13.189  2.1821
</code></pre>

<p>However, this does not give me p value. Can I ask how I can get p value from this?</p>
"
"0.213292991069455","0.211131098052559","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.123521130997592","0.136472128413867","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.0742269619025206","0.0567758373078348","115914","<p>I have a set of 12 subjects: 6 administred with drug A and 6 with drug B. For each subject two time-points have been collected (before and after drug treatment) for each analyte (over 6000).
I had a read around and it seems that mixed ANOVA suits my needs. Therefore I performed it using R as recommended on <a href=""http://www.cookbook-r.com/Statistical_analysis/ANOVA/#mixed-design-anova"" rel=""nofollow"">http://www.cookbook-r.com/Statistical_analysis/ANOVA/#mixed-design-anova</a> 
Which is to say: </p>

<pre><code>funAV &lt;- function(x) aov(x ~ Class*time+ Error(Subjects/time),data=X)

aovOb &lt;- apply(X[,-(1:3)],2,funAV)
</code></pre>

<p>Once I get the results I have for each analyte a significant (or not significant) difference concerning Class (A or B), time (before or after) or a combination of both. On <a href=""https://statistics.laerd.com/spss-tutorials/mixed-anova-using-spss-statistics.php"" rel=""nofollow"">https://statistics.laerd.com/spss-tutorials/mixed-anova-using-spss-statistics.php</a> it is reported that there is no need for a post hoc analysis with a 2 level factor between subjects. If this is true and I find a Class effect how can I know if the difference lies at time before or after? The same is true on the other way round i.e. if I find a time effect how can I know if the difference lies at drug A or B? 
I looked for post hoc analysis methods anywhere but all I could get was a bunch of methods for repeated measures ANOVA which is not exactly my case since I have an additional factor (the drug). Many thanks.</p>
"
"0.0989692825366941","0.11355167461567","116761","<p>This is my first question (previously the search function has been enough), so please bear with me.  I have a very simple experimental design with one outcome variable and 5 groups.  My typical strategy in this case would be to run a simple ANOVA and then use something like a Tukey's test to calculate significance between groups.  </p>

<p>In this case, one of the groups has a mean that is way above the other 4.  If I exclude the group with the very high mean there are many significant pairs in the data.  Including the very high group gives significance only in comparisons with that group.  The groups don't have equal variance which I know is a problem, but I'm not sure how to deal with it.  I've tried a ""robust"" anova using the package robustbase, but I haven't been able to figure out a suitable post-hoc test.  Any help you can offer on how to analyze something like this would be greatly appreciated.</p>

<p>Here's a simplified version of the code:</p>

<pre><code>#Baseline condition (this is what the Test conditions need to be compared with)
Baseline = c(450,400,200,250)

#Negative control
Control = c(13,22,17,20)

#Test conditions
Test1 = c(200,400,450,300) 
Test2 = c(120,140,90,80) 
VeryHighTest = c(2700,2500,1800,1750)

#Constructing a data frame for ANOVA including all data########
Labels.all =     c(rep('Baseline',4),rep('Control',4),rep('Test1',4),rep('Test2',4),rep('VeryHighTest',4))
data.all = c(Baseline,Control,Test1,Test2,VeryHighTest)
df.allValues = data.frame(Labels=Labels.all, Values=data.all)

#Constructing data frame for ANOVA excluding the VeryHigh group#######
Labels.low = Labels.all[1:16]
data.low = data.all[1:16]
df.lowValues = data.frame(Labels=Labels.low, Values=data.low)

############ANOVAs##############
anova.all = aov(Values ~ Labels, data = df.allValues)
summary(anova.all) #P value on the order of 10^-9
anova.low = aov(Values ~ Labels, data = df.lowValues)
summary(anova.low) #P value &lt; 0.0001

##########Post-hocs##############
phoc.low = TukeyHSD(anova.low)         #Many comparisons are significant  
phoc.low
phoc.all = TukeyHSD(anova.all)         #!!!!Only comparisons with VERYHIGHTEST are significant!!!!#
phoc.all
</code></pre>

<p>Would I be justified in excluding the VeryHighGroup from my analysis because the variance is so high and then maybe do a single T-test between VeryHighGroup and all the other groups combined?  Clearly, I'm out of my statistical depth.</p>

<p>Here's the residual plot for each group.</p>

<p><img src=""http://i.stack.imgur.com/pzADY.jpg"" alt=""Residual Plot""></p>
"
"0.149378879319591","0.143881915926426","117332","<p>I want to check whether the addition of the new predictor <code>x3</code> improves the predictive information of a Cox model significantly or not. </p>

<p>So far I have:</p>

<pre><code>&gt; m1 &lt;- coxph(Surv(time, y) ~  x1+x2,    data=a)
&gt; m2 &lt;- coxph(Surv(time, y) ~  x1+x2+x3, data=a)
&gt; anova(m1, m2)
Analysis of Deviance Table
 Cox model: response is  Surv(time, y)
 Model 1: ~ x1 + x2
 Model 2: ~ x1 + x2 + x3
   loglik  Chisq Df P(&gt;|Chi|)  
1 -319.85                      
2 -317.17 5.3526  1   0.02069 *
</code></pre>

<p>What about c-statistics? How can I calculate and compare it? Somewhere I read ""C-statistic results were compared by the nonparametric method described by DeLong"". What about other methods? What are the pros and cons of each method? And how do I all this using R?</p>

<p><strong>UPDATE #1</strong></p>

<p>Even after reading the manual, I do not understand how to use <code>rcorrp.cens</code> in my example. The following code</p>

<pre><code>m1 &lt;- coxph(Surv(time, y) ~  x1+x2,    data=a)
m2 &lt;- coxph(Surv(time, y) ~  x1+x2+x3, data=a)
rcorrp.cens(m1, m2, Surv(a$time, a$y)) 
</code></pre>

<p>does not work. Which I have not seriously thought after reading the manual. However, I have no idea what to do. And which method-value in <code>rcorrp.cens</code> would be the best for me?</p>

<ol>
<li><p>Would you/or someone else please give me a straightforward example; as possible derived from my example code?</p></li>
<li><p>Do you know ad hoc a published medical paper which used this statistical method in its analysis?</p></li>
</ol>

<p><strong>UPDATE #2</strong></p>

<p>I am sorry, but unfortunately, I do not get it.  I wrote a executable example.</p>

<pre><code>&gt; library(survival)
&gt; library(Hmisc)
&gt; data(colon)
&gt; d &lt;- colon
&gt; surv &lt;- y &lt;- Surv(d$time, 1-(d$status))
&gt; m1 &lt;- coxph(surv ~ rx+sex, data=d)
&gt; m2 &lt;- coxph(surv ~ rx+sex+age, data=d)
&gt; anova(m1, m2)
Analysis of Deviance Table
 Cox model: response is  surv
 Model 1: ~ rx + sex
 Model 2: ~ rx + sex + age
   loglik  Chisq Df P(&gt;|Chi|)  
1 -5522.6                      
2 -5519.3 6.4838  1   0.01089 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; rcorrp.cens(fitted(m1), fitted(m2), surv)
Error in rcorrp.cens(fitted(m1), fitted(m2), surv) : 
  y must have same length as x
&gt; fitted(m1)
NULL
&gt; fitted(m2)
NULL
</code></pre>

<p>The manual on fitted says ""an object for which the extraction of model fitted values is meaningful"". But what is that? Apparently <code>coxph</code> is not such a model.</p>

<p><strong>UPDATE #3</strong></p>

<pre><code>Following I show a working example incorporating the help from Harrell:

&gt; library(survival)
&gt; library(Hmisc)
&gt; data(colon)
&gt; d &lt;- colon
&gt; surv &lt;- y &lt;- Surv(d$time, 1-(d$status))
&gt; m1 &lt;- coxph(surv ~ rx+sex, data=d)
&gt; m2 &lt;- coxph(surv ~ rx+sex+age, data=d)
&gt; anova(m1, m2)
Analysis of Deviance Table
 Cox model: response is  surv
 Model 1: ~ rx + sex
 Model 2: ~ rx + sex + age
   loglik  Chisq Df P(&gt;|Chi|)  
1 -5522.6                      
2 -5519.3 6.4838  1   0.01089 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; (r &lt;- rcorrp.cens(predict(m1), predict(m2), surv))
               Dxy               S.D. x1 more concordant x2 more concordant                  n 
      6.069800e-02       2.345218e-02       5.282221e-01       4.675241e-01       1.858000e+03 
           missing         uncensored     Relevant Pairs          Uncertain               C X1 
      0.000000e+00       9.380000e+02       9.379880e+05       2.510688e+06       4.733685e-01 
              C X2             Dxy X1             Dxy X2 
      4.614846e-01      -5.326294e-02      -7.703084e-02 
&gt; 
&gt; (conc.m1 &lt;- round((1 - r[['x1 more concordant']])*100, digits=1)) # smaller model with 2 predictors
[1] 47.2
&gt; (conc.m2 &lt;- round((1 - r[['x2 more concordant']])*100, digits=1)) # larger model with 3 predictors
[1] 53.2
&gt; 
&gt; (p.value &lt;- round(2*(1 - pnorm(r[['Dxy']] / r[['S.D.']])), digits=4))
[1] 0.0096
</code></pre>
"
"0.0371134809512603","0.0378505582052232","117489","<p>From my limited statistical knowledge, I could use MANOVA if I had multiple independent variables (x1, x2...xn). What can I do (specifically in R) with one ""x"" variable and multiple ""y"" groups? I'm trying to see if there is any relationship between the y's with respect to their regression with x. I've already set up a loop that computes bivariate, piecewise linear regressions between each pair (x-y1, x-y2, ... x-yn), but that does not include any analysis of variation between the y variables. Does anybody know how I might do this (in a statistically sound manner, of course) in R? My data looks like this:</p>

<pre><code>x         y1       y2      y3      y4      y5
4.19    5.51    19.76   50.00   19.36   54.07
8.60    10.16   33.01   82.99   38.48   44.95
8.03    7.82    31.29   79.05   40.12   59.18
6.64    8.99    27.13   69.13   30.44   59.02
7.03    8.22    25.29   74.45   36.02   50.88
1.50    5.90    10.69   22.88   10.34   34.50
4.36    7.61    19.27   44.47   20.06   24.62
7.17    8.30    26.72   68.68   31.61   20.16
2.68    5.61    14.25   37.07   15.20   67.75
7.91    7.75    30.93   82.01   38.62   65.36
3.74    5.24    16.42   40.17   17.54   15.19
</code></pre>
"
"0.0909090909090909","0.092714554082312","117637","<p>I have problem with the diagnostic of the one way analysis of variance model (fitted in R).
I've checked all the assumptions of the analysis of variance</p>

<p>1) ""For each level of the within-subjects factor, the dependent variable must have a normal distribution."" (shapiro-wilk test) <a href=""http://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA"" rel=""nofollow"">http://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA</a>
Ok, so after there shapiro-wilk failure to reject, I bury my head in the sand and silently assume that this assumption is met.</p>

<p>2) there is homogeneity of the variance of the dependent variable in groups, there are only 2 groups (bartlett test)</p>

<p>3) all observations are independent</p>

<p>Then I fitted model with <code>aov()</code> function and after checking whether the p-value is greater or less than 0.05 (doesn't really matter now) I would like to check the <strong>quality of the fit</strong>.</p>

<p>So after I check if residuals have normal distribution and the answer is no, is there any solution? Because that means the quality is weak. For example box-cox transformation for dependent variable?</p>

<p>Should I also check the homogeneity of the variance for residuals ( motivation for this question is that ANOVA is some kind of a linear model )? Can I perform bartlett test again? And what if there is heterogeneity? Should then I use the weighted least squares method <strong>WLSM</strong> (the same as I would be a linear model)?</p>

<p>Are diagnostic plots for <code>aov()</code> function, which is similar to <code>lm()</code> (as I would perform <code>plot(lm(  formula ) )</code> are valid? Or only they are proper for linear model where explanatory variable are continuous?</p>

<p>Thanks for help! </p>
"
"0.0981930408849676","0.100143163995996","117676","<p>I want to perform an ANOVA on data with a single factor with three levels. The difficulty for me is that two of the levels are repeated measures (within subjects) and the third level is independent measurements (between subjects). How can I setup the anova in R? Is there a non-parametric test as well similar to the Friedman? </p>

<p>Here is some example data that demonstrates my values. Note that id values 1 to 5 are repeated across conditions A and B. I want to know the main effect of condition.</p>

<pre>
 id condition       value
  1         A  2.02007736
  2         A  1.89103975
  3         A  0.14934483
  4         A -0.06426685
  5         A  2.29443309
  1         B  1.03682968
  2         B  3.61084808
  3         B  2.61471544
  4         B -1.05105853
  5         B  3.36151584
  6         C  0.91556132
  7         C  2.33281852
  8         C  4.24242955
  9         C  2.21116219
 10         C  4.48222818
</pre>

<pre><code># R code for created data with n observations per condition
n &lt;- 30
data &lt;- data.frame(
    id = factor(c(1:n, 1:(n*2))),
    condition=factor(rep(c('A', 'B', 'C'), each=n)),
    value = c(rnorm(n, 1), rnorm(n, 2), rnorm(n, 3)) + # observation variability
        c(rep(rnorm(n, 0, 0.5), times=2), rnorm(n, 0, 0.5)) # subject variability
)
# Can I just do a repeated measures analysis? (probably not)
summary(aov(value ~ condition + Error(id/condition), data=data))
</code></pre>
"
"0.240522846460417","0.245299653018046","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.0909090909090909","0.0772621284019266","119790","<p>Is there a difference between chi-squared (from <code>coxph</code> -> <code>anova</code>) and Wald chi-squared (from <code>cph</code> -> <code>anova</code>)?</p>

<p>And how do I have to interpret these chi-squared values? What does a P&lt;0.05 mean in this case? Why does the sum of chi-squared values of each variable not equal TOTAL? My idea was that each chi-squared indicates the predictive information of each variable and TOTAL that of the entire model.</p>

<pre><code>&gt; library(survival)
&gt; library(rms)
&gt;
&gt; data(colon)
&gt; d &lt;- colon
&gt; m1 &lt;- cph(Surv(time, status) ~ age + sex + nodes, data=d)
&gt; anova(m1)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 age          0.03     1    0.8612
 sex          0.93     1    0.3349
 nodes      189.79     1    &lt;.0001
 TOTAL      192.01     3    &lt;.0001
&gt; 0.03+0.93+189.79 # = 190.75
[1] 190.75
&gt; m2 &lt;- coxph(Surv(time, status) ~ age + sex + nodes, data=d)
&gt; anova(m2)
Analysis of Deviance Table
 Cox model: response is Surv(time, status)
Terms added sequentially (first to last)

       loglik    Chisq Df Pr(&gt;|Chi|)    
NULL  -6424.0                           
age   -6423.6   0.7147  1     0.3979    
sex   -6423.4   0.5019  1     0.4787    
nodes -6356.9 132.8685  1     &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.139296523215068","0.142062965386116","120549","<p>It is a basic question but I could not find clear answer on my reading. I am trying to find independent predictors of Infant.Mortality in data frame 'swiss' in R. </p>

<pre><code>&gt; head(swiss)
             Fertility Agriculture Examination Education Catholic Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96             22.2
Delemont          83.1        45.1           6         9    84.84             22.2
Franches-Mnt      92.5        39.7           5         5    93.40             20.2
Moutier           85.8        36.5          12         7    33.77             20.3
Neuveville        76.9        43.5          17        15     5.16             20.6
Porrentruy        76.1        35.3           9         7    90.57             26.6
</code></pre>

<p>Following are the results using lm and I find only Fertility to be a significant predictor: </p>

<pre><code>&gt; fit = lm(Infant.Mortality~., data=swiss)
&gt; summary(fit)

Call:
lm(formula = Infant.Mortality ~ ., data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.2512 -1.2860  0.1821  1.6914  6.0937 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.667e+00  5.435e+00   1.595  0.11850
Fertility    1.510e-01  5.351e-02   2.822  0.00734    #  &lt;&lt;&lt;&lt; NOTE P VALUE HERE
Agriculture -1.175e-02  2.812e-02  -0.418  0.67827
Examination  3.695e-02  9.607e-02   0.385  0.70250
Education    6.099e-02  8.484e-02   0.719  0.47631
Catholic     6.711e-05  1.454e-02   0.005  0.99634

Residual standard error: 2.683 on 41 degrees of freedom
Multiple R-squared:  0.2439,    Adjusted R-squared:  0.1517 
F-statistic: 2.645 on 5 and 41 DF,  p-value: 0.03665
</code></pre>

<p>Following are the graphs:</p>

<pre><code>plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/lopHb.png"" alt=""enter image description here""></p>

<p>On performing stepwise regression, following are the results: </p>

<pre><code>&gt; step &lt;- stepAIC(fit, direction=""both""); 
Start:  AIC=98.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

              Df Sum of Sq    RSS     AIC
- Catholic     1     0.000 295.07  96.341
- Examination  1     1.065 296.13  96.511
- Agriculture  1     1.256 296.32  96.541
- Education    1     3.719 298.79  96.930
&lt;none&gt;                     295.07  98.341
- Fertility    1    57.295 352.36 104.682

Step:  AIC=96.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education

              Df Sum of Sq    RSS     AIC
- Examination  1     1.320 296.39  94.551
- Agriculture  1     1.395 296.46  94.563
- Education    1     5.774 300.84  95.252
&lt;none&gt;                     295.07  96.341
+ Catholic     1     0.000 295.07  98.341
- Fertility    1    72.609 367.68 104.681

Step:  AIC=94.55
Infant.Mortality ~ Fertility + Agriculture + Education

              Df Sum of Sq    RSS     AIC
- Agriculture  1     4.250 300.64  93.220
- Education    1     6.875 303.26  93.629
&lt;none&gt;                     296.39  94.551
+ Examination  1     1.320 295.07  96.341
+ Catholic     1     0.255 296.13  96.511
- Fertility    1    79.804 376.19 103.758

Step:  AIC=93.22
Infant.Mortality ~ Fertility + Education

              Df Sum of Sq    RSS     AIC
&lt;none&gt;                     300.64  93.220
- Education    1    21.902 322.54  94.525
+ Agriculture  1     4.250 296.39  94.551
+ Examination  1     4.175 296.46  94.563
+ Catholic     1     2.318 298.32  94.857
- Fertility    1    85.769 386.41 103.017
&gt; 
&gt; 
&gt; step$anova
Stepwise Model Path 
Analysis of Deviance Table

Initial Model:
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

Final Model:
Infant.Mortality ~ Fertility + Education


           Step Df     Deviance Resid. Df Resid. Dev      AIC
1                                      41   295.0662 98.34145
2    - Catholic  1 0.0001533995        42   295.0663 96.34147
3 - Examination  1 1.3199421028        43   296.3863 94.55125
4 - Agriculture  1 4.2499886025        44   300.6363 93.22041
&gt; 
&gt; 
</code></pre>

<p>Summary shows Education also has trend towards significant association: </p>

<pre><code>summary(step)

Call:
lm(formula = Infant.Mortality ~ Fertility + Education, data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6927 -1.4049  0.2218  1.7751  6.1685 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.63758    3.33524   2.590 0.012973
Fertility    0.14615    0.04125   3.543 0.000951
Education    0.09595    0.05359   1.790 0.080273

Residual standard error: 2.614 on 44 degrees of freedom
Multiple R-squared:  0.2296,    Adjusted R-squared:  0.1946 
F-statistic: 6.558 on 2 and 44 DF,  p-value: 0.003215
</code></pre>

<p>What do I conclude? Is Education an important predictor or not?</p>

<p>Also, do the graphs using plot(fit) add any significant information?</p>

<p>Thanks for your help.</p>

<hr>

<p>Edit: 
I ran shapiro test on all columns and found 2 are not normally distributed: </p>

<pre><code>Fertility : P= 0.3449466 (Normally distributed) 
Agriculture : P= 0.1930223 (Normally distributed) 
Examination : P= 0.2562701 (Normally distributed) 
Education : P= 1.31202e-07 (--- NOT Normally distributed! ---) 
Catholic : P= 1.20461e-07 (--- NOT Normally distributed! ---) 
Infant.Mortality : P= 0.4978056 (Normally distributed) 
</code></pre>

<p>Does that make a difference? </p>
"
"0.135020119496819","0.156061849468725","120849","<p>I need some help verifying a simple analysis.  I have completed a simple two factor experiment of more or less equal sample sizes.  Let's assume for the case of this question that the sample sizes are equal.  In each factor, there are two treatments thus there are 4 total treatments.  I have measured four response variables.  However, the four response variables show high correlation in pairs.  That is response variables 1 and 2 are highly correlated with each other and response variables 3 and 4 are highly correlated with each other.  Looking at the correlation of response variable {1,3}, {1,4}, and so on shows low correlation.  That is because the first response variable will be a length of a branch and the second response variable is the number of leaves on the same branch and the same pattern follows for response variables 3 and 4.</p>

<p>I want to do two tests. I want to look at the two factors and interaction effects on each response variable and I want to do two MANOVA's on the two pairs of response variables.  My questions are as follows:</p>

<p>1) Do I have to do a Bonferonni correction to control for Type 1 error here?  My feeling is that I do.  Because I am doing two MANOVA tests and four ANOVA tests, I will have an increased likelihood of making a type 1 error.  However, I'm not sure if my number of tests is 6 or 2 and 4 respectively (are the MANOVA tests different/separate from the ANOVA tests?).</p>

<p>2) I would first look at the MANOVA test results for what are the main effects and then afterwards do the individual ANOVA tests right?  By doing the MANOVA first, I can see what effects have on the pairs and then the individual ANOVA tests can show which specific response variables contribute to the pair effects?</p>

<p>I have a hard time keeping all of the assumptions in line when doing these sort of things.  Additionally, my ANOVA class never covered MANOVA / multiple response variables for the same experiment.  Thanks for your help and please let me know if there is any more information I can provide to help.</p>
"
"0.117851130197758","0.109265149843816","121517","<p>I have two models:</p>

<pre><code>frm.mE &lt;- glm(frm ~ age + education + socialrole + countedmembers +
            offset(log(words)), family=quasipoisson, data=daten.alle.kom)
frm.oE &lt;- glm(frm ~ age + socialrole + countedmembers +
                 offset(log(words)), family=quasipoisson, data=daten.alle.kom)
</code></pre>

<p>now I want to know which model is the better one, but because of quasipoisson, AIC don't work</p>

<pre><code>summary(frm.mE)
Call:
glm(formula = frm ~ age + education + socialrole + countedmembers + 
offset(log(words)), family = quasipoisson, data = daten.alle.kom)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-6.7040  -1.6727  -0.2329   1.0003   7.4897  

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    -3.95362    0.21432 -18.448  &lt; 2e-16 ***
age             0.01293    0.07041   0.184  0.85454    
education1      0.11532    0.11647   0.990  0.32367    
socialrole1    -0.28367    0.23685  -1.198  0.23287    
socialrole2    -0.80474    0.29054  -2.770  0.00629 ** 
countedmembers -0.03716    0.06120  -0.607  0.54461    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 5.792638)

Null deviance: 909.51  on 160  degrees of freedom
Residual deviance: 841.35  on 155  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>and the second model:</p>

<pre><code>Call:
glm(formula = frm ~ age + socialrole + countedmembers + offset(log(words)), 
family = quasipoisson, data = daten.alle.kom)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-6.4844  -1.6613  -0.3583   1.1036   7.1557  

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    -3.89079    0.20350 -19.119  &lt; 2e-16 ***
age             0.00540    0.06966   0.078  0.93832    
socialrole1    -0.33991    0.22947  -1.481  0.14054    
socialrole2    -0.75470    0.28553  -2.643  0.00905 ** 
countedmembers -0.02634    0.05996  -0.439  0.66104    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 5.761264)

Null deviance: 909.51  on 160  degrees of freedom
Residual deviance: 847.08  on 156  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>is there another way to compare them? or to know if I should keep the variable ""education""?
thanks for any help!</p>

<p>I tried a F test, but not sure if it makes sense:</p>

<pre><code> anova(frm.mE, frm.oE, test=""F"")
Analysis of Deviance Table

Model 1: frm ~ age + education + socialrole + countedmembers + offset(log(words))
Model 2: frm ~ age + socialrole + countedmembers + offset(log(words))
Resid. Df Resid. Dev Df Deviance      F Pr(&gt;F)
1       155     841.35                          
2       156     847.08 -1  -5.7368 0.9904 0.3212
</code></pre>

<p>but I'm not sure how to understand it, does it mean that I should keep ""education"" because model 2 has a too big p-value?</p>
"
"0.167976321977148","0.171312347673479","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.0524863881081478","0.0535287727572189","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.104972776216296","0.0936753523251331","123999","<p>I'm analysing count data with a generalised linear model in R. I started with a Poisson family distribution, but then realized that data was clearly overdispersed. I then took the option of applying a glm with negative binomial distribution (I'm using the function <code>glm.nb()</code> from MASS package). Interestingly, I get the same best-selected model with a forward and a backward stepwise selection approach, which is: </p>

<pre><code>m.step2 &lt;- glm.nb(round(N.FLOWERS) ~ Hs_obs+RELATEDNESS+CLONALITY+PRODUCTION, data = flower[c(-12, -17), ])
</code></pre>

<p>Then to test for fixed effects I use the anova() function, which gives:</p>

<pre><code>anova(m.step2, test = ""Chi"")
Analysis of Deviance Table
Model: Negative Binomial(1.143), link: log
Response: round(N.FLOWERS)
Terms added sequentially (first to last)
              Df Deviance Resid. Df  Resid. Dev   Pr(&gt;F)   

 NULL                           15     40.674                   
 Hs_obs       1   9.5978        14     31.076    0.001948 **
 RELATEDNESS  1   9.4956        13     21.581    0.002060 **
 CLONALITY    1   3.0411        12     18.540    0.081181 . 
 PRODUCTION   1   3.7857        11     14.754    0.051693 .
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
 Warning messages: 
 1: In anova.negbin(m.step2, test = ""F"") : tests made without re-estimating 'theta'
</code></pre>

<p>However, if there were overdispersion (even with the negative binomial) these p-values should be corrected, shouldn't they? In my case, the residual deviance (obtained from the <code>summary(m.step2)</code>) is 14.754 and residual degrees of freedom 11. Thus, overdispersion is 14.754/11 = 1.34. </p>

<p>How do I correct the p-values to account for the small amount of overdispersion detected in this negative binomial model?</p>
"
"0.131215970270369","0.151664856145454","124581","<p>I was doing some log-linear models to test for interactions/associations in multiway contingency tables (based on the tutorial here, <a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html"" rel=""nofollow"">http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html</a>). I was doing this using a Poisson <code>glm</code> on the observed frequencies as well as with <code>MASS</code>'s <code>loglm</code>. I was just wondering though what type of hypothesis test would make most sense here, sequential type I using <code>anova()</code> (not good since p values there depend on the order of the factors in the model), type III test using <code>Anova()</code> in <code>car</code> (independent of the order of the factors in the model) or using <code>drop1</code> starting from the most complex model? </p>

<p>E.g. using the Titanic passenger survival data</p>

<pre><code>library(COUNT)
data(titanic)
titanic=droplevels(titanic)
head(titanic)
mytable=xtabs(~class+age+sex+survived, data=titanic)
ftable(mytable)
                       survived  no yes
class     age    sex                   
1st class child  women            0   1
                 man              0   5
          adults women            4 140
                 man            118  57
2nd class child  women            0  13
                 man              0  11
          adults women           13  80
                 man            154  14
3rd class child  women           17  14
                 man             35  13
          adults women           89  76
                 man            387  75
freqdata=data.frame(mytable)
fullmodel=glm(Freq~SITE*SEX*MORTALITY,family=poisson,data=freqdata)
</code></pre>

<p>Would the most sensible test for interactions between the different categorical factors then be given by type I SS as in</p>

<pre><code>anova(fullmodel, test=""Chisq"")
Analysis of Deviance Table

Model: poisson, link: log

Response: Freq

Terms added sequentially (first to last)


                       Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                      23    2173.33              
class                   2   231.18        21    1942.15 &lt; 2.2e-16 ***
age                     1  1072.61        20     869.54 &lt; 2.2e-16 ***
sex                     1   137.74        19     731.80 &lt; 2.2e-16 ***
survived                1    77.61        18     654.19 &lt; 2.2e-16 ***
class:age               2    32.41        16     621.78 9.178e-08 ***
class:sex               2    29.61        14     592.17 3.719e-07 ***
age:sex                 1     6.09        13     586.09   0.01363 *  
class:survived          2   132.69        11     453.40 &lt; 2.2e-16 ***
age:survived            1    25.58        10     427.81 4.237e-07 ***
sex:survived            1   312.93         9     114.89 &lt; 2.2e-16 ***
class:age:sex           2     4.04         7     110.84   0.13250    
class:age:survived      2    35.45         5      75.39 2.002e-08 ***
class:sex:survived      2    73.71         3       1.69 &lt; 2.2e-16 ***
age:sex:survived        1     1.69         2       0.00   0.19421    
class:age:sex:survived  2     0.00         0       0.00   1.00000 
</code></pre>

<p>or using type III SS using <code>car</code>'s <code>Anova</code> :</p>

<pre><code>library(car)
library(afex)
set_sum_contrasts()
Anova(fullmodel, test=""LR"", type=""III"")
Analysis of Deviance Table (Type III tests)

Response: Freq
                       LR Chisq Df Pr(&gt;Chisq)    
class                    37.353  2  7.744e-09 ***
age                       5.545  1  0.0185317 *  
sex                       0.000  1  0.9999999    
survived                  1.386  1  0.2390319    
class:age                 5.476  2  0.0646851 .  
class:sex                 0.000  2  1.0000000    
age:sex                   0.000  1  0.9999888    
class:survived           16.983  2  0.0002052 ***
age:survived              0.056  1  0.8126973    
sex:survived              0.000  1  0.9999953    
class:age:sex             0.000  2  1.0000000    
class:age:survived        3.461  2  0.1771673    
class:sex:survived        0.000  2  1.0000000    
age:sex:survived          0.000  1  0.9999905    
class:age:sex:survived    0.000  2  1.0000000    
</code></pre>

<p>or using single term deletions and LRTs with <code>drop1</code> :</p>

<pre><code>fullmodel=glm(Freq~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, family=poisson, data=freqdata)
drop1(fullmodel,test=""Chisq"")
Single term deletions

Model:
Freq ~ class + age + sex + survived + class:age + class:sex + 
    class:survived + age:sex + age:survived + sex:survived
               Df Deviance    AIC     LRT  Pr(&gt;Chi)    
&lt;none&gt;              114.89 249.01                      
class:age       2   162.76 292.89  47.877 4.016e-11 ***
class:sex       2   115.74 245.86   0.850    0.6537    
class:survived  2   230.95 361.08 116.067 &lt; 2.2e-16 ***
age:sex         1   114.89 247.02   0.008    0.9294    
age:survived    1   134.39 266.52  19.505 1.003e-05 ***
sex:survived    1   427.81 559.94 312.927 &lt; 2.2e-16 ***
</code></pre>

<p>?</p>

<p>[This last result appears to match that of <code>MASS</code>'s <code>loglm</code>, as should be the case :</p>

<pre><code>fullmodel=loglm(~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, mytable)
stepAIC(fullmodel) 
drop1(fullmodel,test=""Chisq"")
Single term deletions

Model:
~class + age + sex + survived + class:age + class:sex + class:survived + 
    age:sex + age:survived + sex:survived
               Df    AIC     LRT  Pr(&gt;Chi)    
&lt;none&gt;            144.89                      
class:age       2 188.76  47.877 4.016e-11 ***
class:sex       2 141.74   0.850    0.6537    
class:survived  2 256.95 116.067 &lt; 2.2e-16 ***
age:sex         1 142.89   0.008    0.9294    
age:survived    1 162.39  19.505 1.003e-05 ***
sex:survived    1 455.81 312.927 &lt; 2.2e-16 ***
</code></pre>

<p>]</p>

<p>(Any other more elegant ways btw to specify a model with main effects + all first order interaction effects?)</p>

<p>Any thoughts what would be the best way to analyse such multiway contingency tables, and adequately test for associations for unbalanced data sets?</p>

<p>EDIT: based on the answer below I went for the <code>drop1</code> solution :</p>

<pre><code>fullmodel=glm(Freq~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, family=poisson, data=freqdata)
drop1(fullmodel,test=""Chisq"")
</code></pre>

<p>which is equivalent to the log-linear model in <code>MASS</code> :</p>

<pre><code>fullmodel=loglm(~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, mytable)
stepAIC(fullmodel) 
drop1(fullmodel,test=""Chisq"")
</code></pre>
"
"0.0642824346533225","0.0655590899062897","125344","<p>Dear all: I need to test which effects I should include in my model for genetic evaluation of cows. I was using the following code in R: </p>

<pre><code>model1 = lm(milk ~ factor(year) + factor(herd) + factor(season) + age + I(age^2), data=paula1)
anova(model1)
</code></pre>

<p>However, all my effects were highly significant (&lt; 2.2e-16 ***). I treid using <code>step(model1)</code> to choose the best model, I tried to include other effects that I would never expect to be significant and they were. So I was thinking that I was doing something wrongly. Then I tried proc glm in SAS using the following code:</p>

<p>data paula1; set paula0;
proc glm;
class year herd season;
model milk= year herd season age age*age;
run;</p>

<p>And my results were very similar. I decided then to exclude some data and shuffle some variables and it is still signifficant. Now I have no doubts that my analysis are completely wrong, I just cant figure out what my mistake is. For some factors (e.g. herd I have more than 200 levels) and I have missing data as well (coded as NA in R and blank in SAS). The outputs look fine (sum of squares, degree of freedom, etc)
Any help would be very much appreciated. Thanks. Paula</p>
"
"0.0524863881081478","0.0535287727572189","125455","<p>I want to meta-analyze the interaction effect of a 2x2 ANOVA.</p>

<p>(I am <em>not</em> talking about an interaction in the meta-regression, as in <a href=""http://stats.stackexchange.com/questions/71404/main-effects-and-interaction-in-multivariate-meta-analysis-network-meta-analysi"">this question</a> but about an interaction as the focal effect that should be meta-analytically summarized).</p>

<p><strong>What is the best way to code the interaction effect size for a subsequent meta-analysis?</strong>
(preferably in the <code>metafor</code> package)</p>
"
"0.0829882662886615","0.0677091369065533","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.111340442853781","0.11355167461567","125511","<p>Still new and learning how to use R, but I'd like to get some help with figuring out how to block my data by location. I found <a href=""http://www.r-tutor.com/elementary-statistics/analysis-variance/randomized-block-design"" rel=""nofollow"">this tutorial</a>, but I don't understand where to put my data into the code.</p>

<p>I have 14 locations, 2 treatments (yes/no), and count data of the number of calls per species. My data looks basically like this:  </p>

<pre><code>location treatment species1  
BC       yes        20   
BC       no         11  
BC       yes       880   
DW       no          3  
DW       yes       230  
DW       yes         0  
YZ       yes       390   
YZ       no          0  
YZ       yes       540  
(etc...)
</code></pre>

<p>(only in a .csv file) </p>

<p>Because it's count data I'm trying to use <code>glm</code>, and <code>family=poisson</code> (or <code>quasipoisson</code>) so my basic formula without the blocked locations looks like this:</p>

<pre><code>sp1calls = glm(species1~treatment, family=poisson)
</code></pre>

<p>and I know that the number of treatment levels is 2, and the number of control blocks is 14, but in the website's directions I don't understand how to get my data into the code they give.  </p>

<p>Rephrased: I get <code>k=2</code> and <code>n=14</code>, but does <code>f=c(""yes"", ""no"")</code>?  What is the <code>1</code> for? Why is the function <code>gl</code>?  Since it's an example for randomized block design in ANOVA, is <code>glm</code> different?</p>

<p>Once I have <code>blk=(something)</code>, the final code should be: </p>

<pre><code>sp1callswithblock = glm(sp1~treatment+block, family=poisson)   
</code></pre>

<p>right? (or something like that?)</p>
"
"0.12894693513945","0.131507833509084","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.0262431940540739","0.0535287727572189","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.0642824346533225","0.0655590899062897","127356","<p>I'm following an example from the book ""R by Example"", where they talk about two-way ANOVA.</p>

<p>The database used in <code>poison</code>. The analysis is:</p>

<pre><code>L &lt;- aov(Time ~ Poison * Treatment, data = poison)
</code></pre>

<p>Further on, the book says:</p>

<blockquote>
  <p>The residual plots suggest a reciprocal transformation of the response
  (poison survival time) (The dependent variable<code>Time</code>)...</p>
</blockquote>

<p>That is, a more appropriate variable would be <code>1/Time</code>.</p>

<p>Here is the residual plot, using <code>plot(L)</code>:</p>

<p><img src=""http://i.stack.imgur.com/WWeUq.png"" alt=""enter image description here""></p>

<p>I guess that the reciprocal relationship is evident by the gradually increasing residuals. Why is this correct?</p>

<p>When making the reciprocal model using <code>L &lt;- aov(1/Time ~ Poison * Treatment, data = poison)</code>, the residual plot no longer has this property:</p>

<p><img src=""http://i.stack.imgur.com/R46Zl.png"" alt=""enter image description here""></p>

<p>So my questions is <strong>how could I've known that the special pattern in the first residual plot suggests a reciprocal relationship?</strong></p>
"
"0.176941141829152","0.192900396574317","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.123091490979333","0.125536099672233","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.0524863881081478","0.0535287727572189","134179","<p>I'm struggling to find out which ANOVA test to use when you have a non-normal data set with repeated measures AND replication.  </p>

<p>I'm doing my data analysis in R and the friedman.test() method says it is only suitable for un-replicated data. Is there a way I can twist my data to fit the test?</p>

<p>To be specific:
I have put each subject through 4 different treatments (repeated measure) three times each (replication). I want to look for significant differences between the treatments.  My empty data table is structured like this:</p>

<p>SubjectID | Treatment1 | Treatment2 | Treatment3 | Treatment4 <br>
1 <br>
1 <br>
1 <br>
2 <br>
2 <br>
2 <br>
etc....</p>

<p>Any pointers to the right test would be really appriciated! </p>
"
"0.219887787882206","0.224254780218092","136495","<p><b>Background:</b><br>
I am using linear mixed-effects models (LMMs) in order to determine how the interaction between two fixed effects influences measures of a response variable.  Since I am working with a dataset in which there are multiple samples from multiple individuals that could violate the assumption of independence of data points, I am treating ""individual"" as a random effect.  Thus, the generic model I am working with is:  </p>

<pre><code>lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind), dataset, REML=T)
</code></pre>

<p>Note: for my actual dataset, I used a likelihood ratio test to determine whether I needed to also nest the multiple trials within individual [i.e., lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind/Trial), dataset)], and failed to reject the null hypothesis that this ""fuller"" model contributed significantly to accounting for additional variation in the data.   </p>

<p><b>Problem to solve:</b><br>
Determine whether the results from my Tukey's post-hoc comparisons are reliable, given the interactions included in my LMM model.</p>

<p><b>Loading data and libraries:</b><br>
library(car) # for Soils dataset<br>
data(Soils)<br>
library(lme4) # for lmer()<br>
library(lsmeans) # for remaining functions  </p>

<p><b>Example code:</b><br>
     ## Create the LMM<br>
     ## ""Na"" is a numeric continuous response variable<br>
     ## ""Contour"" is a factor, with character categories, and is treated as a fixed effect<br>
     ## ""P"" is an integer variable, is treated as a fixed effect, and differs across the Contour groups<br>
     ## ""Group"" is a a numerical factor and is treated as a random effect  </p>

<pre><code>Na.LMER &lt;- lmer(Na ~ Contour*P + (1|Group), Soils, REML=T)
Na.LMER  

Linear mixed model fit by REML ['lmerMod']
Formula: Na ~ Contour * P + (1 | Group)
   Data: Soils
REML criterion at convergence: 190.4919
Random effects:
 Groups   Name        Std.Dev.
 Group    (Intercept) 2.514   
 Residual             1.063   
Number of obs: 48, groups:  Group, 12
Fixed Effects:
   (Intercept)    ContourSlope      ContourTop               P  ContourSlope:P    ContourTop:P  
    7.104951        4.381251       -0.260527       -0.006811       -0.026952       -0.006258  

### Conduct Tukey's post-hoc comparisons
Na.Tukey &lt;- lsmeans(Na.LMER, pairwise~Contour, adjust=""tukey"")
</code></pre>

<blockquote>
  <p>NOTE: Results may be misleading due to involvement in interactions  </p>
</blockquote>

<pre><code>Na.Tukey  

$lsmeans
 Contour      lsmean       SE   df lower.CL upper.CL
 Depression 5.973118 1.289466 8.15 3.008857 8.937380
 Slope      5.875929 1.286895 8.08 2.913697 8.838160
 Top        4.672639 1.294933 8.24 1.701416 7.643863

Confidence level used: 0.95 

$contrasts
 contrast             estimate       SE   df t.ratio p.value
 Depression - Slope 0.09718976 1.821763 8.11   0.053  0.9984
 Depression - Top   1.30047917 1.827450 8.19   0.712  0.7635
 Slope - Top        1.20328941 1.825636 8.16   0.659  0.7925

P value adjustment: tukey method for a family of 3 means 
</code></pre>

<p><b>So this is where the question comes in.</b><br>
Since I received the warning message (""NOTE: Results may be misleading due to involvement in interactions""), I want to verify whether I can reliably use the p-values output from lsmeans() to determine which contrasts were different from each other.  So how can I tell whether the interactions from this particular dataset could be problematic for interpreting the results from the Tukey's post-hoc comparisons.  </p>

<p><b>Here is what I have tried to investigate this issue.</b><br>
Based on the recommendations by Professor Russell Lenth (developer of the lsmeans R package), I used additional functions from the lsmeans R package to investigate what's going on with the data.</p>

<pre><code>### First, here are the F-tests of the fixed effects of the LMM.
anova(Na.LMER)   

Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
Contour    2  0.5696  0.2848  0.2520
P          1 10.4083 10.4083  9.2093
Contour:P  2  6.7070  3.3535  2.9672  
</code></pre>

<p>Does the Contour:P interaction seem relatively strong?  </p>

<p>Next, I'm going to evaluate whether this interaction is important by determining to what extent the values of P varies across the Contour groups, using lsmip().    </p>

<pre><code>Na.lsm &lt;- lsmeans(Na.LMER, ~Contour|P, at=list(P = c(75, 100, 200, 300, 400)))  
Na.lsm    

P =  75:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.594094 1.399580 10.70  3.5029413  9.685246
 Slope       8.953983 1.562754 13.53  5.5913341 12.316631
 Top         5.864180 1.511863 12.76  2.5917619  9.136598

P = 100:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.423808 1.355688  9.64  3.3876590  9.459957
 Slope       8.109909 1.429365 10.79  4.9562943 11.263524
 Top         5.537432 1.391548 10.16  2.4433848  8.631479

P = 200:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.742665 1.286244  8.08  2.7814923  8.703838
 Slope       4.733616 1.354120  9.32  1.6863856  7.780847
 Top         4.230440 1.384598 10.01  1.1459415  7.314939

P = 300:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.061522 1.396923 10.63  1.9738402  8.149204
 Slope       1.357323 1.960472 21.77 -2.7109112  5.425557
 Top         2.923449 2.025495 24.22 -1.2549312  7.101829

P = 400:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  4.380379 1.651907 17.57  0.9037052  7.857053
 Slope      -2.018970 2.841216 33.67 -7.7950921  3.757152
 Top         1.616457 2.914268 36.01 -4.2938885  7.526803

Confidence level used: 0.95  
</code></pre>

<blockquote>
  <h3>Plotting the interactions</h3>
  
  <p>Na.lsmip &lt;- lsmip(Na.lsm, Contour~P)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/9uDul.jpg"" alt=""Interaction of Contour and P""></p>

<blockquote>
  <h3>It seems like the levels of Contour vary at different values of P (especially for Slope), but I'm going to use pairs() to verify this using pairwise comparison at each value of P.</h3>
</blockquote>

<pre><code>pairs(Na.lsm)  
P =  75:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -2.3598888 2.097862 12.15  -1.125  0.5175
 Depression - Top    0.7299139 2.060232 11.74   0.354  0.9335
 Slope - Top         3.0898026 2.174381 13.15   1.421  0.3589

P = 100:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -1.6861012 1.970019 10.22  -0.856  0.6784
 Depression - Top    0.8863760 1.942755  9.90   0.456  0.8928
 Slope - Top         2.5724773 1.994865 10.47   1.290  0.4308

P = 200:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  1.0090489 1.867637  8.70   0.540  0.8539
 Depression - Top    1.5122246 1.889851  9.04   0.800  0.7122
 Slope - Top         0.5031757 1.936686  9.67   0.260  0.9636

P = 300:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  3.7041990 2.407248 16.78   1.539  0.2988
 Depression - Top    2.1380732 2.460493 18.21   0.869  0.6660
 Slope - Top        -1.5661258 2.818879 23.01  -0.556  0.8447

P = 400:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  6.3993492 3.286534 28.79   1.947  0.1439
 Depression - Top    2.7639218 3.349889 30.81   0.825  0.6905
 Slope - Top        -3.6354273 4.070070 34.89  -0.893  0.6481

P value adjustment: tukey method for a family of 3 means  
</code></pre>

<blockquote>
  <h3>Based on the pairs() output, it doesn't seem like Contour groups vary at these incremental values of P.</h3>
  
  <p><b>Since the Contour groups do not seem to vary at different levels of P, does that mean that the interaction strength is not that strong?  and thus, I am okay to ignore the warning message that ""NOTE: Results may be misleading due to involvement in interactions""?</b>  </p>
</blockquote>

<p>I would appreciate any feedback about interpreting these results, and whether there are additional analyses that I should be conducting in order to address my concern.  If there is any additional information that would be helpful in tackling this problem, please let me know.  </p>

<p>Thank you for your time!</p>

<p>UPDATE (2/6/15): I had a minor typo at the beginning, in which the first line of code read ""Dens.LMER &lt;- lmer(...)"".  The lmer product should have been named ""Na.LMER"", which was used in the remaining code.  Thus, the Dens.LMER product that rvl mentions is equivalent to Na.LMER.  I apologize for the inconvenience.  </p>
"
"0.0909090909090909","0.0772621284019266","136833","<p>I am trying to best analyse a set of foraging ecology data with >10 behaviour categories (DVs) and 3 levels of IV (season, sex, age). The time which an animal spent engaged in a behaviour was recorded and then divided by the total time spent in sight of the observer, so my data are proportional. As is typical, not all animals engaged in all behaviours and there are a large number of zeros in my dataset which is severely over-dispersed. I had initially analysed all the data in R using the <code>glm</code> function with <code>family = quasibinomial</code>, followed by anova. The intention was then to use the false discovery rate alpha to account for the large number of analyses. However, it has since been suggested that a multivariate approach might be better so I have been trying to figure out (a) if it's possible to run a quasi-binomial multivariate analysis of proportion data (b) how to go about it.</p>

<p>In the R documentation <a href=""http://www.inside-r.org/packages/cran/VGAM/docs/quasibinomialff"" rel=""nofollow"">quasi-binomial family function page</a> (<strong>from the VGAM</strong> package, the function mentioned above is <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html"" rel=""nofollow""><code>quasibinomial()</code> from base R</a>) it is stated that if multivariate response = TRUE the response matrix should be binary. This seems a pretty straightforward indictment of my idea to run this analysis on my proportion data, but I am wondering why - is this just not possible and why not; or is there a particular package that could help? </p>
"
"0.0742269619025206","0.0757011164104465","136983","<p>I am trying to figure out what is the estimated variance (i.e. the estimated ""error"") of residuals around a fitted line. </p>

<pre><code>&gt; summary(model)

Call:
lm(formula = fecundity ~ Organic)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.2909 -1.6439 -0.4606  1.5121  3.7273 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  47.6667     1.4907   31.98 9.97e-10 ***
Organic      -8.6788     0.4805  -18.06 9.06e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.182 on 8 degrees of freedom
Multiple R-squared:  0.9761,    Adjusted R-squared:  0.9731 
F-statistic: 326.2 on 1 and 8 DF,  p-value: 9.063e-08

&gt; anova(model)
Analysis of Variance Table

Response: fecundity
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Organic    1 1553.5 1553.50  326.22 9.063e-08 ***
Residuals  8   38.1    4.76                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0371134809512603","0.0378505582052232","138076","<p>I have a very big data and I would like to perform multi comparison test. However, I must firstly perform ANOVA to be able to calculate p-value etc</p>

<p>There are a lot of posts but none of them applicable for my question.</p>

<p>Lets say I have a matrix as follow:</p>

<pre><code>y &lt;- outer(1:20000,1:1500)
</code></pre>

<p>How can I perform this simple statistical analysis on it in R ?</p>
"
"0.148453923805041","0.151402232820893","139222","<p>Although I know there are several post in this forum that are about this topic, none of them was useful in my case.</p>

<p>I have the next data:</p>

<pre><code>     V1     V2
1.62790698   1
 1.62790698  1
 7.95006570  1
 8.60709593  1
 7.82945736  2
14.18604651  2
 4.65116279  2
 3.87596899  2
 3.90930414  2
 0.39093041  2
 6.18421053  2
 2.82894737  2
15.55929352  2
 6.98065601  2
 0.07751938  3
 4.03100775  3
 4.65116279  3
 7.82945736  3
 9.18686474  3
 8.36591087  3
12.74433151  3
 1.60281470  3
 5.78947368  3
13.81578947  3
 1.57894737  3
 8.48684211  3
 6.98065601  3
 5.88730025  3
12.86795627  3
16.31623213  3
</code></pre>

<p>The column on the left represents the measured variable and the column on the right represents the groups. So, there are 3 different groups.</p>

<p>When I introduce this data into R commander, I performed Shapiro-Wilk tests and Bartlett test. Due to all the requisites that are necessary to perform an ANOVA are not accomplished, I decided to perform instead a Kruskal-Wallis test.</p>

<pre><code>&gt; kruskal.test(V1 ~ V2, data=Datos)

    Kruskal-Wallis rank sum test

data:  V1 by V2
Kruskal-Wallis chi-squared = 6.5558, df = 2, p-value = 0.03771
</code></pre>

<p>As you can see, there are statistical differences.</p>

<p>On the other hand, I thought about performing a post-hoc analysis in order to know how my three groups are grouped according to their differences. According to this, I install and charged the PMCMR library. I introduced the next code:</p>

<pre><code>posthoc.kruskal.nemenyi.test(x=V1, g=V2, method=""Tukey"")
</code></pre>

<p>With the next results:</p>

<pre><code>&gt; posthoc.kruskal.nemenyi.test(x=V1, g=V2, method=""Tukey"")

    Pairwise comparisons using Tukey and Kramer (Nemenyi) test  
                   with Tukey-Dist approximation for independent samples 

data:  V1 and V2 

  1     2    
2 0.211 -    
3 1.000 0.098

P value adjustment method: none
</code></pre>

<p>However, a warning also appeared:</p>

<pre><code>[50] NOTA: Aviso en posthoc.kruskal.nemenyi.test(x = V1, g = V2, method = ""Tukey"") :
Ties are present, p-values are not corrected.
</code></pre>

<p>On the other hand, when I execute:</p>

<pre><code>posthoc.kruskal.nemenyi.test(x=V1, g=V2, method=""Chisq"")
</code></pre>

<p>I get the next results:</p>

<pre><code>&gt; posthoc.kruskal.nemenyi.test(x=V1, g=V2, method=""Chisq"")

    Pairwise comparisons using Nemenyi-test with Chi-squared    
                       approximation for independent samples 

data:  V1 and V2 

  1    2   
2 0.24 -   
3 1.00 0.12

P value adjustment method: none
</code></pre>

<p>This one also have a warning:</p>

<pre><code>[51] NOTA: Aviso en posthoc.kruskal.nemenyi.test(x = V1, g = V2, method = ""Chisq"") :
Ties are present. Chi-sq was corrected for ties.
</code></pre>

<p>So, my questions are:</p>

<ol>
<li>If I get a Kruskal Wallis p value lower than 0.05, I would expect to have any statistical differences when obtaining pairwise comparisons, which is not the case.</li>
<li>Is it right the way I proceed?</li>
<li>Is there any other possibility or code (implemented in different libraries) to get to what I wanted?</li>
</ol>
"
"NaN","NaN","139289","<p>I'm new to statistics, and so far I'm not grasping the significance of the concept of ""degrees of freedom"". So far, I've only had tangential exposure to it by way of calculating Student's t.</p>

<p>When we run anova() in R, in this example, I don't understand why the degrees of freedom are returned (the ""Df"" column) in this context, and what it contributes:</p>

<pre><code>&gt; tomato &lt;-data.frame(
+ weight=c(1.5, 1.9, 1.3, 1.5, 2.4, 1.5, # water
+  1.5, 1.2, 1.2, 2.1, 2.9, 1.6, # Nutrient 
+ 1.9, 1.6, 0.8, 1.15, 0.9, 1.6), # Nutrient+24D
+ trt = rep(c(""water"", ""Nutrient"", ""Nutrient+24D""),
+ c(6, 6, 6)))
&gt; tomato.aov &lt;- aov(weight ~ trt, data=tomato)
&gt; anova(tomato.aov)
Analysis of Variance Table

Response: weight
          Df Sum Sq Mean Sq F value Pr(&gt;F)
trt        2 0.6269 0.31347  1.2019  0.328
Residuals 15 3.9121 0.26081   
</code></pre>

<p>Some clarification would be greatly appreciated.</p>
"
"0.104972776216296","0.0802931591358284","140055","<p>I have a dataset with thousands of observations pre-assigned to 18 groups and with measures for 8 different variables. I am using canonical discriminant analysis to see how separable my 18 groups are. What I am actually most interested in is which individual variable separates the groups most (and least). </p>

<p>I have tried running canonical discriminant analysis in R using the ldm() function from the MASS library. </p>

<pre><code>mydata.lda &lt;- lda(group ~ x1 + x2 + x3 .... + x8, data=mydata)
</code></pre>

<p>If I understand correctly, the output has coefficients of linear discriminant which indicates how strongly each variable is associated with each individual discriminant function, and I could standardize the coefficients to help interpret the meanings of the resultant discriminant functions. </p>

<p>I think what I want however is the partial F-square of each individual variable, or the relative ability of each variable to separate groups across all discriminant functions, not one at a time. 
In SPSS, the discriminant analysis function allows one to ask for ""univariate ANOVAs"" which seem to produce what I want: a table showing the Wilks' Lambda statistic and F statistics for each of my 8 variables. How would I get this kind of output in R? Do I need to run a (M)ANOVA based on the output of my lda()? </p>
"
"0.0497929597731969","0.0846364211331916","140140","<p>This paper (<a href=""http://psycnet.apa.org/journals/med/6/4/147/"" rel=""nofollow"">http://psycnet.apa.org/journals/med/6/4/147/</a>) states that departures from normality can be tolerated for one-way ANOVA. </p>

<blockquote>
  <p>""The results give strong support for the robustness of the ANOVA under
  application of non-normally distributed data.""</p>
</blockquote>

<p>I have a factorial design and some of my data is not normal distributed. Although it is only a minor fraction of the total data set (28 out of 340 samples) I wonder if it is legitimate to proceed with a factorial design ANOVA.</p>

<p>My dependant variable is the relative absorption in an IR range (defined by DRIFT analysis) and my independent variables are timepoint, treatment, exposition, depth. Per sampling condition (example: timepoint=0, treatment=x, exposition=north, depth= 0-5cm) I have 3 replicates.</p>

<p>I have 4 treatment, 3 timepoints, 2 exposition and 2 depth. As some replicates are missing, my design is unbalanced and I used type III Anova (from car package) in R . I assumed a linear model with interactions. (linear Model = Absorption ~ Treatment * Timepoint * Depth * Exposition)</p>

<p><img src=""http://i.stack.imgur.com/BkO0p.png"" alt=""Type 3 Anova for unbalanced design""></p>

<p><img src=""http://i.stack.imgur.com/ogoIl.png"" alt=""Type 2 Anova for unbalanced design""></p>
"
"0.0991899501072693","0.101159871930065","140991","<p>I've been having some trouble in attempting to compare sets of data. I can't seem to analyse whether two models describe the same set of data, or if they describe different sets.</p>

<p>Here is my a portion of my basic data:</p>

<pre><code> ZT     WT_PAL Line_37_PAL  WT_PhPRR5 Line_37_PhPRR5    WT_EOBI Line_37_EOBI   WT_EOBII Line_37_EOBII     WT_CM1 Line_37_CM1     WT_ADT
1   0 0.08017366 0.000959987 0.26035363     0.03264146 1.46476869  0.009786237 4.16477772   0.000742414 0.07395887 0.000456353 0.06000000
2   0 0.05930462 0.021197691 0.26147552     0.22926780 1.57837816  0.926847383 1.15031587   0.461807744 0.03682062 0.101097795 0.05322561
3   0 0.14389513 0.756356081 0.63035752     0.72129878 1.76452175  0.640368308 2.42348584   1.364089162 0.12954215 0.892205209 0.13821109
4   4 0.12194367 0.297290671 0.13444482     0.14225469 0.99144104  1.131902963 0.91522009   0.910081812 0.29664680 0.505630813 0.51706760
5   4 0.06025697 0.164053161 0.15448683     0.26627386 1.31917230  1.519721821 0.62925084   2.483566296 0.12296628 0.364813045 0.35061055
6   4 0.20896743 0.249435523 1.23052341     0.61818565 1.77819303  1.284683192 1.41398975   1.523446689 0.30023862 0.282538740 0.56811626
7   8 2.38864472 0.042225180 1.54472331     0.04236890 1.04169534  0.860432687 0.26977645   2.001020769 2.93724542 1.340914776 3.00230489
8   8 2.27484249 0.108464160 1.27963226     0.21218338 0.92997042  0.999347054 0.24756421   0.878011535 2.36280758 0.564269963 2.05923549
9   8 1.72728498 0.284489142 1.17311707     0.63301025 0.73380469  0.863829602 0.20109633   0.831139775 2.37338677 1.046991612 2.24797092
10 12 1.13821434 0.462596491 2.22919520     0.15287139 0.34310114  0.817010999 0.29965738   0.236064056 1.18592546 0.725928756 1.01932917
11 12 1.10145755 0.368458720 2.13568842     0.39531534 0.33147292  1.107039633 0.32343745   0.888220142 0.98362898 0.663785645 0.93808648
12 12 1.91985246 0.219754262 1.44412345     0.66775319 0.22753689  0.513590231 0.07657606   1.100251286 1.75011191 0.251849690 1.61130028
13 16 0.68005324 0.396014538 0.31868826     0.14759449 0.38865638  0.778205100 1.09767555   0.627603654 0.55060102 0.784160371 0.60319061
14 16 0.83616544 0.514261850 0.21921500     0.19384070 0.22801491  1.029590354 0.12193953   0.494258870 0.62367453 0.868126888 0.59068953
15 16 0.59058070 0.758966630 0.56687274     0.80844039 0.12417071  0.698339222 0.12503996   1.321782313 0.50518054 1.127351763 0.90570233
16 20 0.30896858 0.376021422 0.18652112     0.16757942 0.50239187  0.823056297 0.30242397   0.549940528 0.32069459 0.464616256 0.33701357
17 20 0.04854291 0.231663315 0.07268395     0.10814706 0.07590502  0.620767904 0.03008203   0.491554754 0.04180077 0.374756383 0.04942141
18 20 0.81359279 0.833815983 0.58218634     0.32892256 0.35501741  0.381413660 0.34660498   0.558786138 0.43100429 0.645363500 0.99771479
</code></pre>

<p>What I would like to do, is to see if the expression profile over time of Line_37_PAL is significantly different to that of WT_PAL</p>

<p>First thing I did was try to fit the model:</p>

<pre><code>fitWT_PAL_1 &lt;- lm(data1$WT_PAL ~ data1$ZT)
fitWT_PAL_2 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2))
    fitWT_PAL_3 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3))
    fitWT_PAL_4 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4))
fitWT_PAL_5 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4) + I(data1$ZT^5))
</code></pre>

<p>Which determined that fitWT_PAL_4 fit the data best.</p>

<p>I then did the same for the Line_37_PAL, fit37_PAL_5 proved to be the best fit.</p>

<p>I wanted to see here whether or not the two models adequately described the same data, or if the data they described were different (and that the models were in fact describing different expression profiles).</p>

<p>But when entering the anova I get:</p>

<pre><code>&gt; anova(fit37_PAL_4, fitWT_PAL_1)
Analysis of Variance Table

Response: data1$Line_37_PAL
                  Df  Sum Sq  Mean Sq F value Pr(&gt;F)
    data1$ZT       1 0.10797 0.107974  1.7944 0.1901
I(data1$ZT^2)  1 0.00342 0.003422  0.0569 0.8131
    I(data1$ZT^3)  1 0.12717 0.127171  2.1134 0.1561
I(data1$ZT^4)  1 0.04095 0.040949  0.6805 0.4157
    Residuals     31 1.86536 0.060173               
    Warning message:
    In anova.lmlist(object, ...) :
      models with response â€˜""data1$WT_PAL""â€™ removed because response differs from model 1
</code></pre>

<p>I'm assuming this is because my Y-values come from two different sets of data? Please correct me if I'm wrong, and I would be thankful for any advice you might be able to give.</p>

<p>I ran the predicted values of a model against the actual values using t.test(x,y, paired = TRUE), but that only describes the differences in means of the two populations, not the possible differences in expression patterns. Advice on how to proceed?</p>
"
"0.105409255338946","0.107502693153888","141011","<p>I've been having some trouble in attempting to compare sets of data. I can't seem to analyse whether two models describe the same set of data, or if they describe different sets. </p>

<p>Here is my a portion of my basic data:</p>

<pre><code>   ZT     WT_PAL Line_37_PAL  WT_PhPRR5 Line_37_PhPRR5    WT_EOBI Line_37_EOBI   WT_EOBII Line_37_EOBII     WT_CM1 Line_37_CM1     WT_ADT
1   0 0.08017366 0.000959987 0.26035363     0.03264146 1.46476869  0.009786237 4.16477772   0.000742414 0.07395887 0.000456353 0.06000000
2   0 0.05930462 0.021197691 0.26147552     0.22926780 1.57837816  0.926847383 1.15031587   0.461807744 0.03682062 0.101097795 0.05322561
3   0 0.14389513 0.756356081 0.63035752     0.72129878 1.76452175  0.640368308 2.42348584   1.364089162 0.12954215 0.892205209 0.13821109
4   4 0.12194367 0.297290671 0.13444482     0.14225469 0.99144104  1.131902963 0.91522009   0.910081812 0.29664680 0.505630813 0.51706760
5   4 0.06025697 0.164053161 0.15448683     0.26627386 1.31917230  1.519721821 0.62925084   2.483566296 0.12296628 0.364813045 0.35061055
6   4 0.20896743 0.249435523 1.23052341     0.61818565 1.77819303  1.284683192 1.41398975   1.523446689 0.30023862 0.282538740 0.56811626
7   8 2.38864472 0.042225180 1.54472331     0.04236890 1.04169534  0.860432687 0.26977645   2.001020769 2.93724542 1.340914776 3.00230489
8   8 2.27484249 0.108464160 1.27963226     0.21218338 0.92997042  0.999347054 0.24756421   0.878011535 2.36280758 0.564269963 2.05923549
9   8 1.72728498 0.284489142 1.17311707     0.63301025 0.73380469  0.863829602 0.20109633   0.831139775 2.37338677 1.046991612 2.24797092
10 12 1.13821434 0.462596491 2.22919520     0.15287139 0.34310114  0.817010999 0.29965738   0.236064056 1.18592546 0.725928756 1.01932917
11 12 1.10145755 0.368458720 2.13568842     0.39531534 0.33147292  1.107039633 0.32343745   0.888220142 0.98362898 0.663785645 0.93808648
12 12 1.91985246 0.219754262 1.44412345     0.66775319 0.22753689  0.513590231 0.07657606   1.100251286 1.75011191 0.251849690 1.61130028
13 16 0.68005324 0.396014538 0.31868826     0.14759449 0.38865638  0.778205100 1.09767555   0.627603654 0.55060102 0.784160371 0.60319061
14 16 0.83616544 0.514261850 0.21921500     0.19384070 0.22801491  1.029590354 0.12193953   0.494258870 0.62367453 0.868126888 0.59068953
15 16 0.59058070 0.758966630 0.56687274     0.80844039 0.12417071  0.698339222 0.12503996   1.321782313 0.50518054 1.127351763 0.90570233
16 20 0.30896858 0.376021422 0.18652112     0.16757942 0.50239187  0.823056297 0.30242397   0.549940528 0.32069459 0.464616256 0.33701357
17 20 0.04854291 0.231663315 0.07268395     0.10814706 0.07590502  0.620767904 0.03008203   0.491554754 0.04180077 0.374756383 0.04942141
18 20 0.81359279 0.833815983 0.58218634     0.32892256 0.35501741  0.381413660 0.34660498   0.558786138 0.43100429 0.645363500 0.99771479
</code></pre>

<p>What I would like to do, is to see if the expression profile over time of Line_37_PAL is significantly different to that of WT_PAL</p>

<p>First thing I did was try to fit the model: </p>

<pre><code>fitWT_PAL_1 &lt;- lm(data1$WT_PAL ~ data1$ZT)
fitWT_PAL_2 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2))
fitWT_PAL_3 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3))
fitWT_PAL_4 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4))
fitWT_PAL_5 &lt;- lm(data1$WT_PAL ~ data1$ZT + I(data1$ZT^2) + I(data1$ZT^3) + I(data1$ZT^4) + I(data1$ZT^5))
</code></pre>

<p>Which determined that fitWT_PAL_4 fit the data best. </p>

<p>I then did the same for the Line_37_PAL, fit37_PAL_5 proved to be the best fit. </p>

<p>I wanted to see here whether or not the two models adequately described the same data, or if the data they described were different (and that the models were in fact describing different expression profiles). </p>

<p>But when entering the anova I get: </p>

<pre><code>&gt; anova(fit37_PAL_4, fitWT_PAL_1)
Analysis of Variance Table

Response: data1$Line_37_PAL
              Df  Sum Sq  Mean Sq F value Pr(&gt;F)
data1$ZT       1 0.10797 0.107974  1.7944 0.1901
I(data1$ZT^2)  1 0.00342 0.003422  0.0569 0.8131
I(data1$ZT^3)  1 0.12717 0.127171  2.1134 0.1561
I(data1$ZT^4)  1 0.04095 0.040949  0.6805 0.4157
Residuals     31 1.86536 0.060173               
Warning message:
In anova.lmlist(object, ...) :
  models with response â€˜""data1$WT_PAL""â€™ removed because response differs from model 1
</code></pre>

<p>I'm assuming this is because my Y-values come from two different sets of data? Please correct me if I'm wrong, and I would be thankful for any advice you might be able to give. </p>

<p>I ran the predicted values of a model against the actual values using t.test(x,y, paired = TRUE), but that only describes the differences in means of the two populations, not the possible differences in expression patterns. Advice on how to proceed? </p>
"
"0.148711432973085","0.133821931893047","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.206740088587259","0.204257029918379","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.111340442853781","0.100934821880595","143556","<p>I would like to model a treatment effect in two different groups, controlled for some co-variates (like age and education), and I assume that a two-way repeated-measure Anova would be the right approach - if yes, I have some questions on how to model this design.</p>

<p>I'm a bit confused on how to do this with R (and the  <code>lme4</code> package), because I found different approaches for the same design. Let's say, I have following variables:</p>

<ul>
<li>subject</li>
<li>group (control vs treatment group)</li>
<li>time (t0 vs t1, i.e. two measures for each subject)</li>
<li>age (co-variate)</li>
<li>education (co-variate)</li>
</ul>

<p>Am I right, that, according to <a href=""http://stats.stackexchange.com/questions/58745/using-lmer-for-repeated-measures-linear-mixed-effect-model"">this posting on Cross Validated</a>, my model would look like this?</p>

<ol>
<li>model: <code>lmer(DV ~ group * time + age + education + (1+time|subject), mydata)</code> </li>
</ol>

<p>Then I found <a href=""http://www.uni-kiel.de/psychologie/rexrepos/posts/anovaMixed.html#mixed-effects-analysis-1"">this tutorial</a>. Following these instructions, my model would look like this?</p>

<ol start=""2"">
<li>model: <code>lmer(DV ~ group * time + age + education + (1|subject) + (1|group:subject) + (1|time:subject), data=mydata)</code></li>
</ol>

<p>Now I have two questions:</p>

<p>a) which of the two above models is correct? or do both work?</p>

<p>b) my data is in long format, how should my variable <code>subject</code> look like? the same value for each measured person, i.e. a value appears twice in this variable (for <em>person A in group X</em> at <strong>t0</strong> and <em>person A in group X</em> at <strong>t1</strong> the same value), or should each row/observation be indicated by a new, unique ID?</p>
"
"0.123521130997592","0.136472128413867","144349","<p>I'm trying to replicate SPSS output in R for a mixed ANOVA with a polynomial contrast to test a linear trend.</p>

<p>I fitted a mixed ANOVA in R (see code below), but I can't figure out how to get the results of the polynomial contrast for the within subjects variable and how to produce type III Sums of Squares (since that is the type SPSS uses).</p>

<p>I found several posts related to this question (see e.g., <a href=""http://stats.stackexchange.com/questions/4544/how-does-one-do-a-type-iii-ss-anova-in-r-with-contrast-codes"">How does one do a Type-III SS ANOVA in R with contrast codes?</a> and <a href=""http://stats.stackexchange.com/questions/140183/mixed-model-type-iii-sums-of-squares-r-vs-spss"">Mixed Model Type-III Sums of Squares- R vs SPSS</a>), but I couldn't find the answer to my question.</p>

<p>In the code below, the data set is downloaded directly from Open Science Framework. </p>

<p>My dependent variable is perc_causal_words, my between subjects variable is AffCoh, and my within subjects variable is story.</p>

<p>The SPSS result of the polynomial contrast for story*AffCoh is  F(1, 111) = .99, p=.322.</p>

<p>Can someone help me in getting a mixed ANOVA with type III Sums of Squares and a polynomial contrast?</p>

<p>Here is the SPSS syntax I am trying to replicate:</p>

<pre><code>GLM cause.1.00 cause.2.00 cause.3.00 BY AffCoh
/WSFACTOR=story 3 Polynomial 
/METHOD=SSTYPE(3)
/CRITERIA=ALPHA(.05)
/WSDESIGN=story 
/DESIGN=AffCoh.
</code></pre>

<p>The data and the code in R:</p>

<pre><code>library(""httr"")
library(""RCurl"")
source(""http://sachaepskamp.com/files/OSF/getOSFfile.R"") # the getOSFfile function
library(""foreign"")
library(""tidyr"")

##@@ DATA LOADING @@##
file &lt;- getOSFfile(""https://osf.io/nwbpd/"")
data &lt;- read.spss(file)

##@@ DATA MANIPULATION @@##
# select only the variables relevant for the main analysis
# that is: participant number, number of causal words in story 1, 2, and 3, 
# and whether participants were in the coherent or incoherent condition
data_mod &lt;- data.frame(data$Participant,
                           data$cause.1.00,
                           data$cause.2.00,
                           data$cause.3.00,
                           data$AffCoh)

colnames(data_mod) &lt;- c(""subject"",""cause1"",""cause2"",""cause3"",""AffCoh"")
data_mod$subject &lt;- as.factor(data_mod$subject)

# gather data into a long format
data_long &lt;- gather(data=data_mod, 
                    key=story, 
                    perc_causal_words, 
                    cause1:cause3)

# fit mixed ANOVA
aov &lt;- aov(perc_causal_words ~ AffCoh * story + Error(subject/story), data=data_long)
</code></pre>
"
"0.0642824346533225","0.0655590899062897","144837","<p>What is the difference between <code>Anova(data)</code>and <code>anova(data)</code>?</p>

<pre><code>&gt; anova(res.full)
Analysis of Variance Table

Response: Sales
                     Df Sum Sq Mean Sq   F value    Pr(&gt;F)    
Promotional.Accounts  1   1070    1070   35.0800 0.0001465 ***
Active.Accounts       1  44337   44337 1453.2067 3.680e-12 ***
Competing.Brands      1  43331   43331 1420.2314 4.124e-12 ***
Potential             1     16      16    0.5146 0.4895772    
Residuals            10    305      31                        
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; Anova(res.full)
Anova Table (Type II tests)

Response: Sales
                     Sum Sq Df   F value    Pr(&gt;F)    
Promotional.Accounts    275  1    9.0193   0.01327 *  
Active.Accounts       27371  1  897.1238 4.025e-11 ***
Competing.Brands      42877  1 1405.3398 4.346e-12 ***
Potential                16  1    0.5146   0.48958    
Residuals               305 10                        
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ enter code here
</code></pre>

<p>I don't understand why in each table the F statistic for each factor is different.</p>
"
"0.10562681853293","0.107724577173552","145657","<p>Basically I'm attempting to recreate the results of an example from class in R. What I'm trying to do is decide whether it's best to use a single regression line for an entire data set or two lines based on a categorical variable. The teacher indicates there are three steps to this:</p>

<ol>
<li>Determine if two different lines are required</li>
<li>If yes, determine if they differ in slope</li>
<li>If yes, determine if they differ in intercept</li>
</ol>

<p>Here is my data:</p>

<pre><code>&gt; example
   Predictor Response Group
1         21       11     A
2         24       21     A
3         26       23     A
4         29       29     A
5         35       34     A
6         45       51     A
7         51       59     A
8         68       73     A
9         72       83     A
10        76       95     A
11        17       11     B
12        21       55     B
13        26       34     B
14        28       44     B
15        32       26     B
16        36       34     B
17        40       15     B
18        45       21     B
19        51       16     B
20        68       21     B
</code></pre>

<p>I've realized that if I add the interaction and group terms to the model:</p>

<pre><code>ex_mod &lt;- lm(Response ~ Predictor,data = example)
ex_mod2 &lt;- lm(Response ~ Predictor + Group + Predictor:Group,data = example)
</code></pre>

<p>And then perform ANOVA on this. I get the right answer for step 1:</p>

<pre><code>&gt; anova(ex_mod,ex_mod2)
Analysis of Variance Table

Model 1: Response ~ Predictor
Model 2: Response ~ Predictor + Group + Predictor:Group
  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    
1     18 6616.4                                 
2     16 1583.8  2    5032.6 25.42 1.078e-05 ***   
</code></pre>

<p>Which means I need different lines, but now I need to know if they differ in slope or y-intercept or both. And here is where I'm stuck. I cant seem to get the right answer (F = 293.17 for slope, and F = 170.77 for intercept). </p>

<p>The teacher indicates that the next steps are: 1) to generate RSS in which the slope is fixed,but the y-intercepts are allowed to vary; and 2) generate RSS in which the y-intercept is fixed, but the slopes are allowed to vary.</p>

<p>I apologize if the question is confusing or simplistic, but I dont know how to proceed from here.</p>

<p>Thanks</p>
"
"0.0829882662886615","0.0846364211331916","145790","<p>I'm trying to figure out how to produce an ANOVA Table in R for a multiple regression model. So far I can only produce it for each regressor, and the Mean Square is calculating as the same as Sum Of Squares.</p>

<pre><code>&gt; anova(nflwin.lm)
Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
pass_yard     1  76.193  76.193  26.172 3.100e-05 ***
percent_rush  1 139.501 139.501  47.918 3.698e-07 ***
oppo_rush     1  41.400  41.400  14.221 0.0009378 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I'm trying to produce something like</p>

<pre><code>Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
Model         3  76.193  76.193  26.172 3.100e-05 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0909090909090909","0.092714554082312","148808","<p>I'm trying to evaluate the value of an object, depending on his characteristics. In order to do this, I'm building the following regression model <code>price ~ .</code>, using similar objects and for each variable I got min 20 observations</p>

<p>I encountered following problem: none of the regression models worked for all my data, so I decided to use all of the followings methods:</p>

<pre><code>model.lm &lt;- lm(price ~ .)
model.lmLog &lt;- lm(log(price) ~ .)
model.ltsReg &lt;- ltsReg(price ~ .)
model.ltsRegLog &lt;- lts(log(price) ~ .)
model.lmrob &lt;- lmrob(price ~ .)
model.lmrobLog &lt;- lmrob(log(price) ~ .)
model.lmRob &lt;- lmRob(price ~ .)
model.lmRobLog &lt;- lmRob(log(price) ~ .)
model.glm &lt;- glm(price ~ .)
model.glmLog &lt;- glm(price ~ ., family=gaussian(link=""log""))
</code></pre>

<p>My question is: how can I decide which of this models fits best for the current data, without plotting the results?</p>

<p>As far as I know, the <code>r-squared</code> aren't trusty, because I the data is corrupted, so will be the <code>r-squared</code>.</p>

<p>Any ideas?</p>

<p>Thank you!</p>

<p><strong>[UPDATE]</strong></p>

<p>what do you think about using <code>BIC</code> or <code>AIC</code> and choosing the one with the lowest value?</p>

<p>what do you think about choosing the variables for the regression upon the analysis of <code>anova</code>?</p>

<p>I have 17 variables from which 10 are dummy variables, is that a problem?</p>
"
"0.104972776216296","0.0936753523251331","150960","<p>I am studying the effects of a sales program on the weekly unit sales at ~1,000 retail locations. I am having trouble figuring out which statistical test is appropriate to run for this scenario. Here is the background of my problem.</p>

<p>I have ~1,000 stores which are divided into 3 groups:</p>

<ul>
<li>Program Group (~600 locations) - had a sales program applied</li>
<li>Control Group (~400 locations) - did not have any program applied</li>
<li>Online Group (1 location) - online sales without physical location</li>
</ul>

<p>Each store has 17 weeks of data. Each week had one of these three possible conditions applied. These conditions did not run consecutive and they not overlapping.</p>

<ul>
<li>Program A: 5 out of 10 weeks</li>
<li>Program B: 2 weeks out of 10 weeks</li>
<li>Null Program (no program): Remaining 10 weeks</li>
</ul>

<p>Only stores in the program group were subjected to one of the conditions and all ~600 stores were subjected for the same program for that given week. There is no reason to believe that a program would impact sales at Control stores and should be considered isolated.</p>

<p>The dependent variable being measured here is sales units for that week. <strong>The specific question is whether Program A or Program B or both had a measurable impact on sales against the control group.</strong> Additionally, it is possible that the program had an indirect effect on online sales which is why it is included separately.</p>

<p>I think an ANOVA test is required for this. However I got confused because I have repeated measures with multiple conditions. Should I be considering all 17 measures for each location (17 x 1,000)? Or do I aggregate numbers? I'm not sure how to get started. If someone can point me in the right direction it would be helpful. I have R and Excel I can use to run this analysis.</p>
"
"0.138865930150177","0.141623820702091","151200","<p>I have been trying to figure out how to do a fairly basic repeated measures analysis using linear mixed effects in R, and then analysing it using post-hoc tests. The problem is that I'm not sure whether the output I get is statistically sound?</p>

<p>The response variable: <code>weighted</code>- an index of habitat preference (prop. individuals on habitatA / prop. of total habitat that is A). A value above 1 indicates the habitat is being used more than what you would expect from its availability. this was repeatedly  measured on the same colony through time over several weeks</p>

<p>Fixed variables: <code>Type</code> - habitat type (live/dead), <code>weeks</code> - the time variable</p>

<p>Random variables: <code>colony</code> - because each measurement of colony violates independence assumption.</p>

<p>Here's what the data loss like plotted over time (orange=live habitat, blue=dead habitat):</p>

<p><img src=""http://i.stack.imgur.com/atUVm.jpg"" alt=""enter image description here""> </p>

<p>i run the analysis using the <code>lmer()</code> function from the <code>lme4</code> package:</p>

<pre><code>results_full=lmer(weighted~type*weeks+(weeks|colony), data=Pos, REML=F)
</code></pre>

<p>My reasoning is that i have no reason to expect a random intercept, they should all start on 1 at time 0, and then individuals will start avoiding the dead habitat and favouring the live habitat. The <code>(weeks|colony)</code> term allows the slope of each colony to be random across time?</p>

<p>So to my question:</p>

<p>I compare the likelihood of two models with each other, in a likelihood ratio test to get p-values of the fixed effects using a reduced model:</p>

<pre><code>results_null=lmer(weighted~type+weeks+ (weeks|colony), data=Pos, REML=F)
anova(results_null, results_full)
</code></pre>

<p>But what I'm really interested in is at what time point (week) do the individuals start avoiding the dead habitat. as you can see from the figure this happens at week 1 so comparing live-dead habitat week by week ""should"" generate a n/s result at week 0 and sig result from then on (I'm not trying to force a statistically significant result, but the fig is pretty clear...)</p>

<p>I tried converting the weeks into a factor, and then performing </p>

<pre><code>lsmeans(results_full, pairwise~type+weeks)
</code></pre>

<p>But it didn't generate anything that seemed meaningful, the output didn't make sense in relation to the data. </p>

<p>Does anyone have any thoughts on A) whether my model and test is appropriate to this data, and B) how I can perform a post hoc test to compare habitat type over time?</p>

<p>Would it be appropriate to use a Dunetts post hoc test to compare preferences to a reference value (=1) rather than to each other?</p>

<p>Grateful for any ideas or pointers!</p>
"
"0.0371134809512603","0.0378505582052232","152514","<p>How should I understand the <code>anova</code> result when comparing two models?</p>

<p>Example:</p>

<pre><code>  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1      9 54.032                                  
2      7  4.632  2      49.4 37.329 0.0001844 ***
</code></pre>

<p>The manpage states: ""Compute analysis of variance (or deviance) tables for one or more fitted model objects."" However, out professor mentioned that it may be employed for model comparison - that's what I intend to do.</p>

<p>Hence I assume I could use <code>anova(model1, model2)</code> and obtain a p-value which tells me whether I should reject the null hypothesis: ""the models are the same"".</p>

<p>May I state that if the p-value is less then (let's say) 0.05, the models differ significantly?</p>
"
"0.0981930408849676","0.100143163995996","152774","<p>I have read ""Design and Analysis of Experiments"" 8th Edition by D.C. Montagomery. In Chapter 14, there is a nested experiment with two factor A and B. Both factors are random. Then the ANOVA test should be performed as: </p>

<p>For A, MS_A / MS_B(A)</p>

<p>For B, MS_B(A) / MS_E</p>

<p>I want to know is it possible to perform this kind of test using classical ANOVA by <code>aov()</code> in R.</p>

<p>I try to include Error term in the formula, but for this simple question, I have to run <code>aov()</code> twice. Something like:</p>

<pre><code>&gt; summary(aov(water ~ A + Error(B), abc))

Error: B
          Df Sum Sq Mean Sq F value Pr(&gt;F)
A          2  416.8  208.39   22.68 0.0155 *
Residuals  3   27.6    9.19
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: Within
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  6   0.31 0.05167
&gt; summary(aov(water ~ B + Error(A), abc))

Error: A
  Df Sum Sq Mean Sq
B  2  416.8   208.4

Error: Within
          Df Sum Sq Mean Sq F value   Pr(&gt;F)
B          3  27.57   9.190   177.9 2.99e-06 ***
Residuals  6   0.31   0.052
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt;
</code></pre>

<p>My question is whether it is possible to combine the above two steps into on formula in R?</p>
"
"0.0371134809512603","0.0378505582052232","152892","<p>I have a following problem:</p>

<p>I have some research data that need to be analysed. There are three randomized independent variables (binary) and one dependent variable (reaction time). The data consists of several thousand observations (repeated measures for about 30 subjects, with full randomization): </p>

<p><code>[distractor, IV1, IV2, RT]</code></p>

<p>I need to test this data for interactions. AFAIK, I should use some version of ANOVA test, but I don't know which one exactly is appropriate here. I'd like to use R, or perhaps Scipy; I read some tutorials and articles on this topic, also here on SE. Unfortunately, my understanding of statistical analysis is very limited, and it seems I don't have enough theoretical knowledge to decide how it should be done. I would be really grateful for a possibly detailed explanation and tips on solving this with R.</p>
"
"0.111340442853781","0.11355167461567","153122","<p>As we know, if we are doing many tests or multiple comparison, we don't use the same $\alpha$ value and use some $\alpha$ correction methods like Bonferroni. This is done because when we do multiple tests, we have higher chance of getting something as significant compared to doing for fewer numbers of tests. </p>

<p>But my main question is this: </p>

<p>1) It is said if you are comparing multiple sample means using ANOVA and once you find there is some significant difference then you can do a <em>post hoc</em> analysis by doing pairwise comparison. But now you don't have to actually do a Bonferroni correction. Why is that? Isn't this <em>post hoc</em> analysis same as other pairwise t test where we use Bonferroni correction?</p>

<p>2) If Bonferroni correction is required because more tests leads to more chances of getting something significant then why we don't use the same thing, where we are doing something like regression where we are testing significance of $\beta$ estimates, or whether a variable is significant or not for feature selection using p value/F score? In that case also we are doing multiple comparison in checking whether each variable is significant or not. Then why don't we use Bonferroni correction on critical $\alpha$ there?</p>

<p>Please advise.</p>
"
"0.0989692825366941","0.11355167461567","153698","<p>I just encountered a problem while analyzing experimental data using lme4 and lmertest. In the experiment, 67 subjects gave 3 ratings for 50 stimuli shown for 3 different durations (a total of 10050 responses). I used the same nested model for each of the 3 ratings (the only difference being the response variable), but the denominator dfs are different for each of the 3 lmertest anovas (1808, 9848, 1807). How is this possible?</p>

<p>A similar question was asked fro mixed effects in SPSS, but remained unanswered. <a href=""http://stats.stackexchange.com/questions/82997/different-degrees-of-freedom-when-using-the-same-mixed-effect-model-spss"">Different degrees of freedom when using the same mixed effect model (SPSS)</a></p>

<p>Edit: Looking more closely at the model using summary(), I found that one group of the analysis that yielded much higher dfs (9848) has a variance of 0. Might this be the reason (the factor cd is nested within the duration factor named dur)?</p>

<p>Low df model:</p>

<pre><code>Random effects:
Groups        Name        Variance Std.Dev.
cd:(dur:subj) (Intercept) 0.13794  0.3714  
dur:subj      (Intercept) 0.04408  0.2099  
subj          (Intercept) 0.41425  0.6436  
Residual                  1.58692  1.2597  
Number of obs: 10050, groups:  cd:(dur:subj), 2010; dur:subj, 201; subj, 67
</code></pre>

<p>High df model:</p>

<pre><code>Random effects:
Groups        Name        Variance Std.Dev.
cd:(dur:subj) (Intercept) 0.00000  0.0000  
dur:subj      (Intercept) 0.06821  0.2612  
subj          (Intercept) 0.39013  0.6246  
Residual                  1.74595  1.3213  
Number of obs: 10050, groups:  cd:(dur:subj), 2010; dur:subj, 201; subj, 67
</code></pre>
"
"0.0742269619025206","0.0757011164104465","154352","<p>I've needed to learn some DOE for work and I've been going through Montgomery's book using R to help with the calculations. The fixed-effect models have been straightforward, but I have some trouble with getting R to the analysis for a simple (one-factor) random effects model.</p>

<p>As a simple example, one of the exercises gives a uniformity data for semiconductor film depositions in a reactor. The reactor has many slots available and wafers from only a few slots are measured.  The R data frame is given below.</p>

<pre><code>Positions &lt;- c(1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4)
Uniformity &lt;- c(2.76, 5.67, 4.49, 1.43, 1.70, 2.19, 2.34, 1.97, 1.47, 0.94, 1.36, 1.65)
d &lt;- data.frame(Positions, Uniformity)
</code></pre>

<p>For the basic ANOVA, I have</p>

<pre><code>summary(aov(Uniformity ~ as.factor(Positions), d))
</code></pre>

<p>which gives the ANOVA table.  It looks fine and agrees with the output that Minitab gives for the same data.</p>

<p>How do I now get the components of variance in R?  Minitab gives the following output:</p>

<pre><code>Source    Variance  % of Total    StDev  % of Total
Position   1.58481      70.85%  1.25889      84.17%
Error     0.652183      29.15%  0.80758      53.99%
Total      2.23699              1.49566
</code></pre>

<p>I can reproduce this by hand, but I can't figure out any way to get the same results in R. I've tried the varComp, lme4 and ICC packages without luck, but I'm sure it's just my lack of understanding.</p>
"
"0.153022802096395","0.146881740676447","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.10562681853293","0.0957551797098243","155023","<p>I'm studying the prevalence of one parasite on a host from different localities. I do it by assigning presence (""1"") or absence (""0"") to each host sampled.</p>

<p>After the sampling, I got something like this:</p>

<pre><code>site      &lt;- c(""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C"",""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C"", 
               ""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C"",""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C"")
infection &lt;- c(""1"", ""1"", ""0"", ""0"", ""0"", ""1"", ""0"", ""1"", ""1"", ""0"", ""0"", ""0"", ""1"", ""1"", 
               ""1"", ""1"", ""0"", ""0"", ""1"", ""1"", ""1"", ""1"", ""0"", ""1"", ""0"", ""0"", ""0"", ""1"")
table1    &lt;- data.frame (site, infection)
</code></pre>

<p>I created a <code>data.frame</code> with the data to automatize the process:</p>

<pre><code>library(dplyr)
table.by.site &lt;- ddply(table1, 
                       ""site"", 
                       summarise,
                       Infected = length(which(infection==""1"")),
                       NonInfected = length(which(infection==""0"")))
table.by.site
</code></pre>

<p>In total, 3 sites and 2 states of infection.  Number of hosts from each site: A= 180; B= 160; C=160.</p>

<p>How can I see if the prevalence of the infections depends on the site of sampling? I have done a <code>fisher.test</code>, finding significant differences:</p>

<pre><code>table.simple &lt;- table.by.site [,-1] #I remove the ""site"" column.
fisher.test(table.simple)
</code></pre>

<p>But how I perform a pairwise t-test? How can I introduce my grouping factor (site)? Should I use a different approach?</p>

<p><strong>EDIT</strong></p>

<p>I will edit the questions instead open a new one.</p>

<p>Let's say I've been collecting samples from 3 different sites (i.e., A, B, and C) in different moments (i.e., week of the year, from 1 to 52). From each of this sampling trips I could obtain a certain number of hosts (e.g. 20 from each). I examined these hosts and I calculate the prevalence (%) of infected hosts per site and trip.</p>

<p>So, the code to generate the table would be something like this:</p>

<pre><code>week      &lt;- c(""1"",""1"",""2"",""2"",""3"",""3"",""4"",""5"",""5"",""6"",""6"",""6"",""6"",""7"",""8"",""9"",
               ""10"",""11"",""12"",""13"",""14"",""14"",""14"",""15"",""15"",""15"",""16"",""16"",""16"",
               ""16"",""16"",""17"",""17"",""18"",""18"",""18"",""18"",""18"")
site      &lt;- c(""A"",""A"",""C"",""C"",""B"",""B"",""C"",""B"",""B"",""A"",""A"",""A"",""A"",""B"",""C"",""B"",""C"",""B"",
               ""C"",""A"",""C"",""B"",""B"",""B"",""A"",""A"",""A"",""A"",""C"",""C"",""C"",""C"",""C"",""A"",""A"",""B"",
               ""B"",""B"")
infection &lt;- c(1,0,0,0,1,1,1,0,1,0,1,0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,1,1,1,0,0,0,0,
               1,1,0,0,0)
table (week)
raw.table &lt;- data.frame (week, site, infection)
</code></pre>

<p>Then I calculate the Prevalence of infection for each site and site:</p>

<pre><code>library(plyr)
table.summary &lt;- ddply(raw.table,
                       .(week, site),
                       summarize,
                       Prevalence = ( (sum (infection)*100) / length(infection)))
table.summary
</code></pre>

<p>Now, can I use an <code>ANOVA</code> to test the differences in prevalence among sites? Is there any problem because they are percentages?</p>

<pre><code>anova.table &lt;- aov(Prevalence ~ site, data=table.summary)
summary(anova.table)
</code></pre>

<p>In the example there is no significant differences among site, but if I want to do a post hoc test, I'd use a pairwise comparison, correcting the p-value with Bonferroni</p>

<pre><code>pairwise.t.test(table.summary$Prevalence, table.summary$site, adj.meth=""bonferroni"")
</code></pre>

<p>Is the whole process correct? Should I take additional steps? Are the percentages a problem for this analysis?</p>
"
"0.117363131703255","0.107724577173552","155559","<p>I have run a discriminant analysis and would like to test the significance of each resulting discriminant function - i.e., does each discriminant function contribute to the separability of the groups? I can't find a definitive answer on how to do this. Most of the reading I have done has shown examples of testing for significance among the groups using MANOVA with respect to the individual, original values, not to the discriminant functions. My approach is to use the predict() function after running the lda, then using the scores that result in individual anovas. Can someone please indicate whether this is valid? </p>

<pre><code>#run manova to test for differences among classes based on original input variables 
#optional
mydata.manova &lt;- manova(var1 + var2 + var3 + var4 + var5 ~ CLASS,  data=mydata)
summary(mydata.manova, test=""Wilks"")


#run lda 
mydata.lda &lt;- lda(CLASS ~ var1 + var2 + var3 + var4 +  var5, data=mydata,na.action=""na.omit"")

#view coefficients and variance explained
mydata.lda


#test significance of each discriminant function in separating the groups
#first predict back to original data to get discimriminant scores? 
mydata.lda.predict &lt;- predict(mydata.lda,data=mydata)

#join predictions to original dataframe
mydata2 &lt;- cbind(mydata, mydata.lda.predict)

#test significance 
test1 &lt;- aov(mydata2$x.LD1 ~d$CLASS)
summary(test1)

test2 &lt;- aov(mydata2$x.LD2 ~d$CLASS)
summary(test2)

test3 &lt;- aov(mydata2$x.LD3 ~d$CLASS)
summary(test3)

test4 &lt;- aov(mydata2$x.LD4 ~d$CLASS)
summary(test4)
</code></pre>

<p>When I run the above on my data, all the tests come back significant. </p>
"
"0.124574574491482","0.146594581573484","155572","<p>I have a statistical analysis / data analysis problem: </p>

<p>I am analyzing data using a factorial three-way ANOVA with a-priori contrasts and type III sums of squares.  (Please don't speak about type I SS vs. type III SS.  That's not the point of my question.)  I get the contrasts like I need using <code>summary.aov()</code>, however that uses type I SS.  When I use the <code>Anova()</code> function from <code>library(car)</code> to get type III SS, I don't get the contrasts.  Why don't I get contrasts?</p>

<p>I have also tried using <code>drop1()</code> with the <code>lm()</code> model, but I get the same results as <code>Anova()</code> (without the contrasts).  I have also tried the following, all without a resolution to my issue: <code>ezANOVA()</code> from <code>library(ez)</code>, <code>glht()</code> from <code>library(multcomp)</code> which returned the error <em>Error in glht.matrix(EpiLM, linfct = con) :   â€˜ncol(linfct)â€™ is not equal to â€˜length(coef(model))â€™</em>, and <code>C()</code>.   Why do I get this error?  How do I resolve this error?  I have searched extensively online for answers to this error message as well as this issue generally, but without finding a solution.  </p>

<p>How do I get the results of a factorial ANOVA with a-priori contrasts <strong>and</strong> type III SS as shown in my example code below?  </p>

<p>Sample data:</p>

<pre><code>DF &lt;- structure(list(Code = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L,  
3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 
9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L), .Label = c(""A"", 
""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""I"", ""J"", ""K"", ""L""), class = 
""factor""), GzrTreat = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), contrasts = structure(c(1, 
-2, 1, 1, 0, -1), .Dim = c(3L, 2L), .Dimnames = list(c(""I"", 
""N"", ""R""), NULL)), .Label = c(""I"", ""N"", ""R""), class = ""factor""), 
BugTreat = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = 
c(""Immigration"", ""Initial"", ""None""), class = ""factor""), TempTreat =   
structure(c(2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L), .Label = c(""Not Warm"", ""Warmed""), class = 
""factor""), ShadeTreat = structure(c(2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 
2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 
1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L), .Label = c(""Light"", 
""Shaded""), class = ""factor""), EpiChla = c(0.268482353, 0.423119608, 
0.579507843, 0.738839216, 0.727856863, 0.523960784, 0.405801961, 
0.335964706, 0.584441176, 0.557543137, 0.436456863, 0.563909804, 
0.432398039, 0.344956863, 0.340309804, 0.992884314, 0.938390196, 
0.663270588, 0.239833333, 0.62875098, 0.466011765, 0.536182353, 
0.340309804, 0.721172549, 0.752082353, 0.269372549, 0.198180392, 
1.298882353, 0.298354902, 0.913139216, 0.846129412, 0.922317647, 
0.727033333, 1.187662745, 0.35622549, 0.073547059), log_EpiChla = 
c(0.10328443, 0.153241402, 0.198521787, 0.240259426, 0.237507762, 
0.182973791, 0.147924145, 0.125794985, 0.19987612, 0.192440084, 
0.157292589, 0.194211702, 0.156063718, 0.128708355, 0.127205194, 
0.299482089, 0.287441205, 0.220962908, 0.093363308, 0.21185469, 
0.166137456, 0.186442772, 0.127205194, 0.235824411, 0.243554515, 
0.103589102, 0.078522208, 0.361516746, 0.113393422, 0.281746574, 
0.266262141, 0.283825153, 0.23730072, 0.339980371, 0.132331903, 
0.030821087), MeanZGrowthAFDM_g = c(0.00665, 0.003966667, 0.004466667, 
0.01705, 0.0139, 0.0129, 0.0081, 0.003833333, 0.00575, 0.011266667, 
0.0103, 0.009, 0.0052, 0.00595, 0.0105, 0.0091, 0.00905, 0.0045, 0.0031, 
0.006466667, 0.0053, 0.009766667, 0.0181, 0.00725, 0, 0.0012, 5e-04, 
0.0076, 0.00615, 0.0814, NA, 0.0038, 0.00165, 0.0046, 0, 0.0015)), 
.Names = c(""Code"", ""GzrTreat"", ""BugTreat"", ""TempTreat"", ""ShadeTreat"", 
""EpiChla"", ""log_EpiChla"", ""MeanZGrowthAFDM_g""), class = ""data.frame"", 
row.names = c(NA, -36L))
</code></pre>

<p>Code:</p>

<pre><code>## a-priori contrasts
library(stats)
contrasts(DF$GzrTreat) &lt;- cbind(c(1,-2,1), c(1,0,-1))
    round(crossprod(contrasts(DF$GzrTreat)))
c_labels &lt;- list(GzrTreat=list('presence'=1, 'immigration'=2))

## model  
library(car)
EpiLM &lt;- lm(log_EpiChla~TempTreat*GzrTreat*ShadeTreat, DF)
summary.aov(EpiLM, split=c_labels) ### MUST USE summary.aov(), to get 
#contrast results, but sadly this uses Type I SS
Anova(EpiLM, split=c_labels, type=""III"") # Uses Type III SS, but NO     
#CONTRASTS!!!!!

# I need contrast results like from summary.aov(), AND Type III SS 
# like from Anova()
</code></pre>
"
"0.0524863881081478","0.0267643863786095","156170","<p>This is a subset of my dataset:</p>

<pre><code>&gt; head(db,20)
   YEAR RING  CO2       Nup
1  1998    1 elev  6.441205
2  1998    2 elev  6.939212
3  1998    3  amb  6.370073
4  1998    4  amb  6.816244
5  1998    5  amb  4.839825
6  1999    1 elev  7.032590
7  1999    2 elev  7.473761
8  1999    3  amb  5.791581
9  1999    4  amb  8.209857
10 1999    5  amb  5.607706
11 2000    1 elev  9.457697
12 2000    2 elev  7.509605
13 2000    3  amb  5.938662
14 2000    4  amb  7.868018
15 2000    5  amb  8.397162
16 2001    1 elev 12.085675
17 2001    2 elev  7.136464
18 2001    3  amb  5.624912
19 2001    4  amb  7.483332
20 2001    5  amb  9.395876
</code></pre>

<p>The aim of my analysis is to find out if there is an effect of the <code>CO2</code> treatment on Nup, but furthermore, I need to know in which <code>YEARS</code> the effect of <code>CO2</code> is significant. My model would hence be:</p>

<pre><code>mod &lt;- lm(Nup~CO2*YEAR)
anova(mod)
</code></pre>

<p>Which renders a significant interaction. In the past I have used Tukey inside the function <code>glht {multcomp}</code>, but only with one factor. How can I do Tukey post-hoc multiple comparisons to find out what are the years in which the effect is significant?</p>
"
"0.0428549564355483","0.0655590899062897","157426","<p>I would like to perform an ANOVA analysis to determine which factors can explain most of the variance for a response variable</p>

<pre><code>y ~ A + B + A:B + C 
</code></pre>

<p>My data are not balanced and I am totally confused on which type of anova should I use.</p>

<p>1) If I use Type I (aov) I get different results depending on the order (as expected)</p>

<p>2) I have been reading that Type II do not include the interaction.. However, I used the Anova (car) function with the type II specification and one of the analysed factor is A:B. What then does ""not including the interaction"" mean?</p>

<p>3) I have been reading that Type III shouldn't be used but I haven't really understood why.. </p>

<p>Many thanks</p>
"
"0.0371134809512603","0.0378505582052232","157938","<p>I used the <code>anova</code> function in R to get an ANOVA table for my model.</p>

<pre><code>fit &lt;- lm(open_time ~ sent_time + email_id + day, data=mydata)
anova(fit)
</code></pre>

<p>I got the following output:</p>

<pre><code>Analysis of Variance Table

Response: open_time 
             Df    Sum Sq Mean Sq F value Pr(&gt;F)
sent_time    1    222321  222321  2.2673 0.1323
email_id     1     15229   15229  0.1553 0.6936
day          1       798     798  0.0081 0.9281
Residuals 1996 195721653   98057  
</code></pre>

<p>Can someone explain in basic terms as to how can I use this output to get useful information about my model (as in which independent variables are more significant in the prediction model, etc)?</p>
"
"0.17799000188592","0.165740126283802","158051","<p>I am analyzing a multiply imputed dataset produced from the MICE package in R. To assess the overall significance of my linear model, I am using pool.compare() to compare my ""full"" model to an intercept only ""restricted"" model. However, the degrees of freedom (residual) returned by pool.compare() seem very highly inflated (I have set m = 50 imputations). I'm aware that 50 imputations is high, but it's needed for my dataset. I've given an example below of the same issue using the nhanes2 dataset from the MICE package. I have two questions:</p>

<p>1) Why are the degrees of freedom returned by pool.compare() so high?  </p>

<p>2) Is it appropriate to use the adjustment to the degrees of freedom suggested by Barnard and Rubin (1999) and described in section 2.3.6 of Stef van Burren's <em>Flexible Imputation of Missing Data</em> textbook?</p>

<p>The R code below shows the issue I'm asking about using the nhanes2 dataset. This dataset has 25 observations and the example fits a linear model with one categorical predictor (age) with three levels and one continuous predictor (chl).</p>

<pre><code># load package and data  
library(""mice"")  
data(nhanes2)  

# impute missing values, m = 50
imp &lt;- mice(nhanes2, m = 50, seed = 1, print = FALSE)

# produce the models to compare, a full model and
# an intercept only restricted model  
fit.imputed.full &lt;- with(imp, lm(bmi ~ age + chl))
fit.imputed.res &lt;- with(imp, lm(bmi ~ 1))  

# compare models using pool.compare()
pooled.comparison &lt;- pool.compare(fit.imputed.full, fit.imputed.res)

# given that the original dataset had 25 observations, and we have a 
# linear model with three predictors (age is a factor with three levels)
# I'd expect the degrees of freedom (residual) for the comparison to be at  
# most 24. The df for the numerator comes as expected:

pooled.comparison$df1
[1] 3

# the df for the denominator comes out a much larger than the 
# maximum of 24:

pooled.comparison$df2
[1] 1374.457

# by way of comparison, the same analysis conducted on a single
# hypothetically complete dataset gives the expected degrees of freedom

nhanes2CCA &lt;- complete(imp, 1)
attach(nhanes2CCA)
fit.CCA.full &lt;- lm(bmi ~ age + chl)
fit.CCA.res &lt;- lm(bmi ~ 1)
detach(nhanes2CCA)
anova(fit.CCA.full, fit.CCA.res)

Model 1: bmi ~ age + chl
Model 2: bmi ~ 1
  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1     21 293.60                              
2     24 477.23 -3   -183.62 4.3778 0.01525 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In the end, it seems strange that an analysis conducted on a hypothetically complete dataset returns 24 degrees of freedom, while an analysis conducted on 50 multiply imputed datasets returns over 1000 degrees of freedom. Why is there this large difference in degrees of freedom?</p>

<p>My second question relates to the correction proposed by Barnard and Rubin(1999). Is it appropriate to use that correction here? Because this is a multi-parameter test, doing so requires, I guess, an estimate of lambda which is averaged across the parameters being estimated. </p>

<p>The figures I've used in this example are:<br>
v_old = 1374.457<br>
v_com = 25-1 = 24<br>
average lambda = 0.329<br>
v_obs = 14.91<br>
v (adjusted degrees of freedom) = 14.75  </p>

<p>Applying this correction in this instance returns a corrected degrees of freedom of 14.75, which is more than the df that would be returned by analyzing only complete cases (12) and less than the df that would be returned by analyzing a hypothetically complete dataset (24). Which seems reasonable. </p>

<p>Thank you all for your assistance. </p>

<p>Matt. </p>
"
"0.138865930150177","0.131507833509084","158319","<p>I have used a repeated-measures ANOVA in SPSS to analyse some of my data. It's the typical approach in my area, but I think it might be more appropriate to use a mixed effect model. However, I struggle with both building the model as well as interpreting it.</p>

<p><strong>Experimental design</strong></p>

<p>300+ participants from two different samples have rated on a continuous scale a stimulus at seven different manipulation levels. I want to test whether individual differences in the participants (recorded as ordinal or binary variables) interact with that rating score. In particular, I'm interested in whether the rating score changes as a function of stimulus level differently in people that, for example, feel mainly attracted to men or women.</p>

<p>Thus,  I have a within-subjects factor (stimulus level), a between-subjects factor (such as being attracted to men or women), and a random effect of participant nested in sample.</p>

<p>I've been using <code>lmer()</code> from the lme4 package and lmerTest and have come up with the following model</p>

<pre><code>model &lt;- lmer (rating.score ~ stim.level + factor + stim.level*factor +
                                                     (1|participant) + (1|sample), mydata)
</code></pre>

<p><strong>Analysis</strong></p>

<ol>
<li>Is lmer() the right package to work with?</li>
<li>Am I appropriately accounting for the random effects of participant and sample, or do I need something like <code>(1|sample/participant)</code>? I followed the <a href=""http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf"" rel=""nofollow"">Pastes data example</a>, but am not sure that's the right thing to do in this context.</li>
<li>Based on previous literature, I expect the relationship of <code>rating.score</code> and <code>stim.level</code> to be quadratic - should/could I enter <code>stim.level</code> as squared term?</li>
</ol>

<p><strong>Interpretation</strong></p>

<p>In SPSS, I find a significant interaction of <code>stim.level</code> x <code>factor</code>. By visualizing the interaction and running post-hoc tests I can then interpret the nature of that interaction. In R, I get estimates of the interaction at each level of <code>stim.level</code>, some of which are significant, some of which are not. Can I still make the conclusion that <code>factor</code> affects the relationship of <code>rating.score</code> and <code>stim.level</code> (even though not necessarily to the same extent at each level)?</p>

<p><strong>EDIT:</strong> I just realized I had entered <code>stim.level</code> as a factor. I think it is appropriate to enter it as a linear variable - the different levels correspond to the same manipulation applied with increasing extent (the steps between each level are the same). This also resolves one of my earlier questions regarding an error message when trying to model random slopes which I have thus now removed.</p>
"
"0.0909090909090909","0.092714554082312","158598","<p>I have a data table in R where I summarized the dollar value of prescribed medicines for each doctor-patient combination. In all, I have about half a million observations, with about 3000 doctors and 250 thousand patients (3 column data table: <code>DoctorID</code>, <code>PatientID</code>, <code>totalValue</code>).</p>

<p>I would like to find, based on data, if there is any indication of any of the doctors prescribing unusually (from the perspective of their own prescription habits) high value formulas to any specific patient (most likely any given doctor would have prescribed drugs to <em>more than one</em> patient. Similarily, any given patient would have been prescribed by more than one doctor), and thus find ""improper"" associations.</p>

<p>I thought ANOVA could be a tool to perform this analysis, but I don't know if this assumption is right, and if it's so, I don't know how to interpret the results (I ran <code>aov(totalValue ~ DoctorID * PatientID,DataTable)</code> and got <code>Pr&gt;F</code> smaller than 1E-17 for both variables, and about 0.01 for the interaction, yet I don't have a clue as to how I can use those results to find Doctor-Patient pairs who could be misbehaving).</p>

<p>I appreciate if you can guide me as to what tool to use, and then how to interpret the produced results.</p>

<p>I'll also appreciate if someone more experienced could help me adding appropriate tags to this question. I'm so lost I can't even think of adequate tags.</p>

<p>============== <strong>EDIT TO ADD</strong> ===============</p>

<p>Please find a scaled-down, modified version of my data <a href=""https://www.dropbox.com/s/30iebxnch48aj0z/exDisp.RData?dl=0"" rel=""nofollow"">here</a>. Each observation is <em>one medicine</em> from a formula.</p>

<p>I can obtain the sum of <code>Value</code> grouped by <code>FormulaNumber</code> for each formula, along with the prescribing Doctor and the Patient:</p>

<pre><code>ex_doc_pat&lt;-exDisp[,.(DoctorID=unique(DoctorID),PatientID=unique(PatientID),Disease=unique(Disease),totalValue=sum(Value)),by=FormulaNumber]
</code></pre>

<p>I can produce a 2way table from there (which, as @kjetil metioned, is fairly sparse --actual data is less sparse than this scaled-down version--):</p>

<pre><code>2wayTab&lt;-reshape(ex_doc_pat[,.(DoctorID,PatientID,totalValue)],idvar = ""DoctorID"",timevar = ""PatientID"",direction=""wide"")
</code></pre>

<p>Responding to @Scortchi, I dropped the rest of data, as it wasn't particularily interesting (place of the farmacy, code of the farmacy, etc); and yes, it could be that a high-tag prescription is well deserved by patient's condition. I just need a starting place to begin looking for unusually high priced doctor-patient relationships.</p>
"
"0.0909090909090909","0.0618097027215413","158713","<p>I have a question regarding multiple regression with an unbalanced grouping factor. Essentially what I am doing is an ANCOVA, but the interaction term ends up significant (which is interesting!) so I've chosen not to call it a true ANCOVA.</p>

<h1>The Data</h1>

<p>The dataset is comprised of 72 individuals who responded to many different measures for the purposes of conducting a cluster analysis to uncover relatively heterogenous subgroups within the dataset. Three clusters resulted form this analysis, where the resulting cluster sizes were n=30, n=32, and n=10. These clusters were interpreted for the purpose of a descriptive analysis. </p>

<p>An independent dataset describes these same 72 individuals on two separate continuous measures: <strong>score</strong>, and <strong>dv</strong>. The hope for my current project is to asses the effect of <strong>group</strong> (cluster membership, unbalanced) and <strong>score</strong> (and the interaction) on the <strong>dv</strong>. </p>

<h1>The Data (Example)</h1>

<pre><code>g1    &lt;-rep(1,30)
g2    &lt;-rep(2,32) 
g3    &lt;-rep(3,10)
group &lt;-as.factor(c(g1,g2,g3))
score &lt;-as.numeric(sample(1:10,72,replace=T))
dv    &lt;-as.numeric(sample(1:7,72,replace=T))
data  &lt;-data.frame(cbind(group, score, dv))
head(data)

head(data)
      group     score       dv
1     1          9          5
2     1          3          6
3     1         10          6
4     1         10          6
5     1         10          6
6     1          4          5
</code></pre>

<h1>My Question</h1>

<p>1) Can I run an analysis despite my groups being so unbalanced? If I understand correctly, by using type III SS, all groups will be weighted equally but I'm not sure if this solves my issue so simply.</p>

<p>For example:</p>

<pre><code>lm&lt;-lm(dv~1+score*group,data=data)
library(car)
Anova(lm,type=""III)
</code></pre>

<p>2) If not, am I unable to proceed in some other way? </p>

<p>I am looking for any suggestions / guidance as I try to sort this out.</p>

<p>Thanks!</p>
"
"0.0556702214268904","0.0757011164104465","159711","<p>I'm trying to figure out why the <code>anova</code> function in R gives me the same results (for the p-value) regardless of the order of the models.</p>

<pre><code>&gt; anova(lm.fit ,lm.fit2)
Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt; anova(lm.fit2,lm.fit)
Analysis of Variance Table

Model 1: medv ~ lstat + I(lstat^2)
Model 2: medv ~ lstat
  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
1    503 15347                                 
2    504 19472 -1   -4125.1 135.2 &lt; 2.2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I can't understand why the p-value is so low in both cases. The way I'm understanding I should interpret the result of the <code>anova</code> is that the model 2 is better than model 1 if the p-value is very low, but in this case I'm getting exactly the same no matter the order.</p>

<p>I'm trying to read <code>?anova</code> to check what this all means, but the help page is very succinct, is there another help where it states what the <code>Df</code> parameter means for instance?</p>
"
"NaN","NaN","159895","<p>The my dataset is not normal distributed. In order to apply a non parametric analysis, exist only the Kruskal Wallis test or others?
I need to find a test similar to ANOVA to apply a dataset not normal distributed.
Thanks so much.</p>
"
"0.0989692825366941","0.100934821880595","160087","<p>My colleagues and I conducted a study of the effects of an experimental translocation on the movement and activity patterns of common brushtail possums in New Zealand. This involved first capturing 12 individuals (6 males and 6 females), fitting them with GPS collars, and releasing each study animal within its home range. After seven days, we re-captured the animals, fitted them with new GPS collars, and then moved them to a common release site that was well outside of all of their home ranges. For both deployments the GPS collars recorded location data at 5-min intervals for an 11-h period during the night when possums are active. Our response variables were duration of nightly active periods, total distance moved per night, mean nightly speed and several other metrics that were descriptive of variation in movement behaviour. 
Our data are a bit messy (unbalanced) because we didnâ€™t get observations from all animals each night of the two 7-d sampling periods (for various reasons), but basically look like this for each animal, with multiple response variables:</p>

<ol>
<li>Up to seven nights of movement data prior to translocation;</li>
<li>Translocation event; </li>
<li>Up to seven nights of movement data after translocation.</li>
</ol>

<p>Our major research questions were:
1.  Do males and females differ in the responses to translocation?
2.  How do responses to translocation (as measured by activity, movement, etc) change with respect to the time (day) since the translocation event? 
3.  The interaction between 1 and 2 above.  </p>

<p>As far as I can tell, these data are best analysed with a mixed-effects model with REML, because of their repeated-measures nature (both before and after translocation), missing values, and combination of fixed (sex) and random factors. From what Iâ€™ve read (Zuurâ€™s book), classical repeated-measures ANOVA is inappropriate for several reasons.</p>

<p>My question is thus: what is an appropriate mixed-effects model formulation for these data in R using the â€˜lme4â€™ package? There is a before/after effect (with respect to translocation), a repeated-measures effect (seven sequential days of data for each GPS collar deployment), and again the fixed effect of sex. I am confused on how to nest the data properly to incorporate the two different levels of temporal autocorrelation (i.e., before/after translocation and then the time series of each GPS deployment). </p>

<p>Any help with what is the correct model code for analysis in R would be hugely appreciated!!</p>
"
"0.0841654636156865","0.100143163995996","160265","<p>I am doing a 3x2x2 between subject study. To make thing simple, I name my variables as A, B and C here.  Using the <code>WRS 2</code> package in R, I have gotten results for the 3 way robust ANOVA (<code>t3way</code>). One of the two-way interactions and the three way interaction were significant. The t3way anova result showed a significant result between AB and ABC with p-value of .031 and .051 respectively.</p>

<p>When come to the post hoc test, I was made understood that there is no post hoc test available for 3 way robust ANOVA in R and hence, I have to split the file by the third IV for the two way ANOVA and post hoc tests. I split the data with variable C (i.e. Ci and Cii) and the t2way anova result of Ci is:</p>

<p>A   .394(p-value)</p>

<p>B   .618</p>

<p>AB  .019</p>

<p>t2way anova for Cii din't show any significant.</p>

<p>When I performed <code>mcp2atm</code> (post hoc test) on the separate robust 2 way ANOVAs, I didn't get a single contrast that is significant.  Post hoc test (mcp2atm) result for Ci showed:</p>

<pre><code>    V1       ci.lower ci.upper p-value
</code></pre>

<p>A1                1.00000 -1.11291  3.11291 0.22754</p>

<p>A2                1.33333 -0.87160  3.53827 0.12861</p>

<p>A3                0.33333 -2.56160  3.22827 0.77417</p>

<p>B1               -0.33333 -2.68024  2.01358 0.77417</p>

<p>A1:B1             0.33333 -1.77957  2.44624 0.68124</p>

<p>A2:B1             0.00000 -2.20493  2.20493 1.00000</p>

<p>A3:B1            -0.33333 -3.22827  2.56160 0.77417</p>

<p>I am puzzled and do not know how to continue with the analysis.</p>
"
"0.0371134809512603","0.0378505582052232","160314","<p>I am running an ecosystem model and would like to give the proper statistics for a sensitivity analysis I'm performing on the model. The model provides a vector x containing all the flows in the model (i.e. x = (1200, 98.3, 12, 108.9, 0.4, 23...)) along with a vector of the standard deviation for each of those flows.</p>

<p>I am looking for a rigorous way to compare two of these vectors of distributions and to quantify the difference. I can make up a formula to do this easily enough, eg sum of square residuals, but doing it right and having a statical basis would be much better. Is there a test for this sort of data? I've looked a bit into MANOVAs and related but I havent seen anything that deals well with distributions rather than observations.</p>

<p>Hope this is somewhat clear and thanks! (I use R for all my coding btw)</p>
"
"0.12894693513945","0.101159871930065","162369","<p>I collected muscle activity levels from 4 different leg muscles on each lower limb over a 20 jump test in 2 groups of athletes.  One group has ACL injury (n=11) and there is a control group (n=11).  Additionally, there are 4 separate jump phases for each individual jump.  Finally, I break the 20 jumps down into 4 clusters of 5 jumps to evaluate the fatigue effects (cluster 1 = rested, cluster 4 = fatigued).  You can think of the 4 clusters of jumps as a time series.</p>

<p>My first analysis plan was to subset my data down to each level of interest.  I planned to filter out a single muscle first, and then a single jump phase.  I then planned to assess equality of variance and normality and to transform my data as needed to compare the left limb to the right limb for each group separately using a paired t-test.  Then, I planned to make group comparisons by comparing the injured limb of the ACL subjects to a limb average for the controls, and the uninjured limb of the ACL subjects to the limb average for the controls using a one-way anova.  I would repeat this process for each muscle and each jump phase.</p>

<p>My second plan was to delve into something more complex like a multilevel model.  Note that I would plan to build this model up to include interaction terms and the appropriate contrasts to get at my 2 primary research questions: do muscle activity levels differ between limbs and between groups w/ fatigue for ACL subjects and controls as measured over the four different jump clusters.  </p>

<p>Here is my first attempt at this model without the interaction terms included (I wanted to save space). </p>

<pre><code>Muscle.Activity.lme = lme(muscle.activity~group+limb+muscle+jump.cluster+jump.phase, random = ~1 | subject/limb/muscle/jump.cluster/jump.phase, data = EMG, method = ""ML"")
</code></pre>

<p>My questions:</p>

<ol>
<li><p>Is it wrong to separate out the various levels to perform between group and within group comparisons as indicated in my first analysis plan?</p></li>
<li><p>Is a multilevel model feasible for my data set and is this approach reasonable? I think my sample size might be too small but I'm wondering if this is a significant limitation or one that needs to be managed.</p></li>
<li><p>Are there any other analysis approaches that might work better?</p></li>
</ol>

<p>Thank you for any insight you can provide.</p>

<p>Matt</p>
"
"0.254492440808642","0.25413946223507","162804","<p>When searching for correlations between between a dependent variable and a factor or a combination of factors in a repeated measure design with lme() I noticed that I can encounter two types of results, and I am wondering which is the best way to report each of them in a journal publication. It is not clear to me when I should report the values of the beta coefficient together with the t-test value and p-value, or the beta coefficient with F value and p-value.</p>

<p>Letâ€™s have as a reference the following two models:</p>

<p>MODEL TYPE 1: fixed effects only </p>

<pre><code>lme_Weigth &lt;- lme(Sound_Feature ~ Weight, data = My_Data, random = ~1 | Subject)
summary(lme_Weigth)

lme_Height &lt;- lme(Sound_Feature ~ Height, data = My_Data, random = ~1 | Subject)
summary(lme_Height)
</code></pre>

<p>MODEL TYPE 2: Fixed and interaction effects together</p>

<pre><code>lme_Interaction &lt;- lme(Sound_Feature ~ Weight*Height, data = My_Data, random = ~1 | Subject)

summary(lme_Interaction)  
anova.lme(lme_Interaction, type = ""marginal"").
</code></pre>

<p>RESULTS CASE 1: Applying model type 2 I do not get any significant p-value so there is no interaction effect. Therefore I check
the simplified model type 1, and I get for both Height and Weight significant p-values.</p>

<p>RESULTS CASE 2: Applying model type 2 I get a significant p-value so there is an interaction effect. Therefore I do not check
the simplified model type 1 for the two factors separately. Moreover, in the results of model type 2 I can also see that the fixed effects of both factors are significant.</p>

<p>I am not sure if in presence of an interaction it is correct to report the significant interactions of the separate factors, since I read somewhere that it does not make too much sense. Am I wrong?</p>

<p>My attempt in reporting the results for the two cases is the following. Can you please tell me it I am right?</p>

<p>â€œWe performed a linear mixed effects analysis of the relationship between Sound_Feature and Height and Weight. As fixed effects, we entered Height and Weight (without interaction term) into a first model, and we included the interaction effect into a second model. As random effects, we had intercepts for subjects.â€</p>

<p>RESULTS CASE 1: â€œResults showed that Sound_Feature was linearly related to Height (beta = value, t(df)= value, p &lt; 0.05) and Weight (beta = value, t(df)= value, p &lt; 0.05), but no to their interaction effect.â€</p>

<p>RESULTS CASE 2:  â€œResults showed that Sound_Feature was linearly related to Height (beta = value, F(df)= value, p &lt; 0.05) and Weight (beta = value, F(df)= value, p &lt; 0.05), and to their interaction effect (beta = value, F(df)= value, p &lt; 0.05).â€</p>

<p>Basically I used for reporting the beta value in the 2 cases I use the output of summary(). In the case 1, I report the value of the t-test, still taken from summary. But for case 2 I do not report the t-test, I report the F value as result of anova.lme(lme_Interaction, type = ""marginal"").</p>

<p>Is this the correct way of proceeding in the results reporting?</p>

<p>I give an example of the outputs I get using the two models for the three cases:</p>

<p>RESULTS CASE 1:</p>

<pre><code>&gt; ############### Sound_Level_Peak vs Weight*Height ###############
&gt; 
&gt;
&gt; 
&gt; library(nlme)
&gt; lme_Sound_Level_Peak &lt;- lme(Sound_Level_Peak ~ Weight*Height, data = My_Data1, random = ~1 | Subject)
&gt; 
&gt; summary(lme_Sound_Level_Peak)
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  716.2123 732.4152 -352.1061

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.470027 4.246533

Fixed effects: Sound_Level_Peak ~ Weight * Height 
                  Value Std.Error DF    t-value p-value
(Intercept)   -7.185833  97.56924 95 -0.0736485  0.9414
Weight         0.993543   1.63151 15  0.6089715  0.5517
Height        -0.076300   0.55955 15 -0.1363592  0.8934
Weight:Height -0.005403   0.00898 15 -0.6017421  0.5563
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.95289464 -0.51041805 -0.06414148  0.48562230  2.95415889 

Number of Observations: 114
Number of Groups: 19 


&gt; anova.lme(lme_Sound_Level_Peak,type = ""marginal"")
              numDF denDF   F-value p-value
(Intercept)       1    95 0.0054241  0.9414
Weight            1    15 0.3708463  0.5517
Height            1    15 0.0185938  0.8934
Weight:Height     1    15 0.3620936  0.5563
&gt; 
&gt; 





&gt; ############### Sound_Level_Peak vs Weight ###############
&gt; 
&gt; library(nlme)
&gt; summary(lme(Sound_Level_Peak ~ Weight, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  706.8101 717.6841 -349.4051

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.717712 4.246533

Fixed effects: Sound_Level_Peak ~ Weight 
                Value Std.Error DF    t-value p-value
(Intercept) -3.393843  6.291036 95 -0.5394728  0.5908
Weight      -0.196214  0.087647 17 -2.2386822  0.0388
 Correlation: 
       (Intr)
Weight -0.976

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.90606493 -0.51419643 -0.05659565  0.56770327  3.00098859 

Number of Observations: 114
Number of Groups: 19 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; ############### Sound_Level_Peak vs Height ###############
&gt; 
&gt; library(nlme)
&gt; summary(lme(Sound_Level_Peak ~ Height, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC   logLik
  702.9241 713.7981 -347.462

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.174077 4.246533

Fixed effects: Sound_Level_Peak ~ Height 
               Value Std.Error DF   t-value p-value
(Intercept) 46.36896 20.764187 95  2.233122  0.0279
Height      -0.36643  0.119588 17 -3.064113  0.0070
 Correlation: 
       (Intr)
Height -0.998

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.93697776 -0.50963502 -0.06774953  0.50428597  2.97007576 

Number of Observations: 114
Number of Groups: 19 
&gt; 
&gt; 
</code></pre>

<p>So, I will report the results in this way: â€œResults showed that Sound_Level_Peak was linearly related to Height (beta = -0.36643, t(17)= -3.064113, p = 0.007) and Weight (beta = -0.196214, t(17)= -2.2386822, p &lt; 0.0388), but no to their interaction effect.â€</p>

<p>RESULTS CASE 2:</p>

<pre><code>&gt; ############### Centroid vs Weight*Height ###############
&gt; 
&gt; 
&gt; 
&gt; library(nlme)
&gt; lme_Centroid &lt;- lme(Centroid ~ Weight*Height, data = My_Data2, random = ~1 | Subject)
&gt; 
&gt; summary(lme_Centroid)
Linear mixed-effects model fit by REML
 Data: My_Data2 
       AIC      BIC    logLik
  1904.563 1920.766 -946.2817

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1180.301 945.3498

Fixed effects: Centroid ~ Weight * Height 
                  Value Std.Error DF   t-value p-value
(Intercept)   -45019.39 21114.912 95 -2.132113  0.0356
Weight           710.53   353.074 15  2.012414  0.0625
Height           330.61   121.092 15  2.730246  0.0155
Weight:Height     -4.34     1.943 15 -2.233779  0.0411
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.16255520 -0.60084449 -0.02651629  0.54377042  1.92638924 

Number of Observations: 114
Number of Groups: 19 


&gt; anova.lme(lme_Centroid,type = ""marginal"")
              numDF denDF  F-value p-value
(Intercept)       1    95 4.545908  0.0356
Weight            1    15 4.049810  0.0625
Height            1    15 7.454243  0.0155
Weight:Height     1    15 4.989769  0.0411
&gt; 
&gt; 
&gt; 
</code></pre>

<p>So, I will report the results in this way:  â€œResults showed that Centroid was linearly related to the interaction effect of Weight and Height (beta = -4.34, F(1,15)= 4.989769, p = 0.0411), and to Height (beta = 330.61, F(1,15)= 7.454243, p = 0.0155). </p>
"
"0.194473021269209","0.198335273928161","163161","<p>I try to figure out how to describe my continuous variable. Unfortunately, I did not understand all of the statistics. I would really appreciate I you guys would help me out here. To better illustrate my problem, I wrote the following example.</p>

<pre><code>library(rms)
library(survival)

data(pbc)
d &lt;- pbc
rm(pbc, pbcseq)
d$status &lt;- ifelse(d$status != 0, 1, 0)

dd = datadist(d)
options(datadist='dd')

# linear model
f1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
p1 &lt;- Predict(f1, fun=exp)
(a1 &lt;- anova(f1))
Function(f1)
plot(p1, anova=a1, pval=TRUE, ylab=""Hazard Ratio"")

# rcs model
f2 &lt;- cph(Surv(time, status) ~  rcs(albumin, 4), data=d)
p2 &lt;- Predict(f2, fun=exp)
(a2 &lt;- anova(f2))
Function(f2)
plot(p2, anova=a2, pval=TRUE, ylab=""Hazard Ratio"")

# minimal CI width
p1$diff &lt;- p1$upper-p1$lower
    min(p1$diff) # = 0.002321521
p1[which(p1$diff==min(p1$diff)),]$albumin # = 3.494002
    describe(d$albumin) # mean = 3.497

p2$diff &lt;- p2$upper-p2$lower
    min(p2$diff) # = 0.2039817
p2[which(p2$diff==min(p2$diff)),]$albumin # = 3.502447
    describe(d$albumin) # mean = 3.497

# both models in a single figure
p &lt;- rbind(linear.model=p1, rcs.model=p2)
library(ggplot2)
df &lt;- data.frame(albumin=p$albumin, yhat=p$yhat, lower=p$lower, upper=p$upper, predictor=p$.set.)
(g &lt;- ggplot(data=df, aes(x=albumin, y=yhat, group=predictor, color=predictor)) + geom_line(size=1))
(g &lt;- g + geom_ribbon(data=df, aes(ymin=lower, ymax=upper), alpha=0.2, linetype=0))
(g &lt;- g + theme_bw())
(g &lt;- g + xlab(""Albumin""))
(g &lt;- g + ylab(""Hazard Ratio""))
(g &lt;- g + theme(axis.line = element_line(color='black', size=1)))
(g &lt;- g + theme(axis.ticks = element_line(color='black', size=1)))
(g &lt;- g + theme( plot.background = element_blank() ))
(g &lt;- g + theme( panel.grid.minor = element_blank() ))
(g &lt;- g + theme( panel.border = element_blank() ))
</code></pre>

<ol>
<li>Why shows the plot of the linear model (p1) not a straight line? </li>
<li>How can I plot the models f1 and f2 in the same figure? </li>
<li>How can I compare the models f1 and f2 to investigate which models fits the data better? ... like anova() for coxph in the survival package</li>
<li>Why is the minimal CI width near the mean of albumin more pronounce in the
linear (f1) model? </li>
<li>What does the P value in the plots mean? How do I have to interpret the output of anova(...)</li>
</ol>

<p><a href=""http://i.stack.imgur.com/kbGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kbGuv.png"" alt=""pkot of f1""></a></p>

<p><a href=""http://i.stack.imgur.com/1UVJd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1UVJd.png"" alt=""plot of f2""></a></p>

<p><a href=""http://i.stack.imgur.com/WG3ui.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WG3ui.png"" alt=""combined plot""></a>  </p>

<p><strong>Update #1</strong>
Following the answer from Harrell, I updated the code above showing how to combine spline plots of two predictors in a single figure. One last question: How can I compare the two rms models like <code>anova(m1, m2)</code> of the survival package as shown below?</p>

<pre><code>&gt; m1 &lt;- coxph(Surv(time, status) ~ albumin, data=d)
&gt; m2 &lt;- coxph(Surv(time, status) ~ pspline(albumin), data=d)
&gt; anova(m1, m2) # compare models
Analysis of Deviance Table
 Cox model: response is  Surv(time, status)
 Model 1: ~ albumin
 Model 2: ~ pspline(albumin)
   loglik  Chisq Df P(&gt;|Chi|)
1 -975.61                    
2 -973.26 4.6983 11    0.9449
&gt; summary(m1)
Call:
coxph(formula = Surv(time, status) ~ albumin, data = d)

  n= 418, number of events= 186 

           coef exp(coef) se(coef)      z Pr(&gt;|z|)    
albumin -1.4695    0.2300   0.1714 -8.574   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

        exp(coef) exp(-coef) lower .95 upper .95
albumin      0.23      4.347    0.1644    0.3219

Concordance= 0.688  (se = 0.023 )
Rsquare= 0.147   (max possible= 0.992 )
Likelihood ratio test= 66.6  on 1 df,   p=3.331e-16
Wald test            = 73.51  on 1 df,   p=0
Score (logrank) test = 72.38  on 1 df,   p=0
</code></pre>

<p><strong>UPDATE #2</strong>
I think I just answered my ""one last question"" by myself (see below). I hope this does not show correct accidentally. I would think that I can compare models from <code>cph</code> and <code>coxph</code> that way, can't I? Is the way of calculating the degrees of freedom <code>df</code> correct?</p>

<pre><code>&gt; # using coxph from survival
&gt; m1 &lt;- coxph(Surv(time, status) ~  albumin, data=d)
&gt; m2 &lt;- coxph(Surv(time, status) ~  albumin + age, data=d)
&gt; # loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
&gt; m1$loglik[2]
    [1] -975.6126
    &gt; m2$loglik[2]
[1] -973.2272
&gt; (df &lt;- abs(length(m1$coefficients) - length(m2$coefficients)))
[1] 1
&gt; (LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2]))
[1] 4.770787
&gt; pchisq(LR, df, lower=FALSE)
[1] 0.02894659
&gt; anova(m2, m1)
Analysis of Deviance Table
 Cox model: response is  Surv(time, status)
 Model 1: ~ albumin + age
 Model 2: ~ albumin
   loglik  Chisq Df P(&gt;|Chi|)  
1 -973.23                      
2 -975.61 4.7708  1   0.02895 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; m1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
&gt; m2 &lt;- cph(Surv(time, status) ~  albumin + age, data=d)
&gt; # loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
&gt; m1$loglik[2]
    [1] -975.6126
    &gt; m2$loglik[2]
[1] -973.2272
&gt; (df &lt;- abs(length(m1$coefficients) - length(m2$coefficients)))
[1] 1
&gt; (LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2]))
[1] 4.770787
&gt; pchisq(LR, df, lower=FALSE)
[1] 0.02894659
</code></pre>

<p><strong>UPDATE #3</strong>
I changed the example following the kind answer from DWin as follows. This way the degrees of freedom should be calculated properly:</p>

<pre><code>library(Hmisc)
library(rms)
library(ggplot2)
library(gridExtra)

data(pbc)
d &lt;- pbc
rm(pbc, pbcseq)
d$status &lt;- ifelse(d$status != 0, 1, 0)

### log likelihood test using a coxph model
m1 &lt;- coxph(Surv(time, status) ~  albumin, data=d)
m2 &lt;- coxph(Surv(time, status) ~  albumin + age, data=d)
# loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
m1$loglik[2]
    m2$loglik[2]
(df &lt;- abs(sum(anova(m1)$Df, na.rm=TRUE) - sum(anova(m2)$Df, na.rm=TRUE)))
(LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2])) # the most parsimonious models have to be first
pchisq(LR, df, lower=FALSE)
anova(m2, m1)

### log likelihood test using a cph model
dd = datadist(d)
options(datadist='dd')
m3 &lt;- cph(Surv(time, status) ~  albumin, data=d)
m4 &lt;- cph(Surv(time, status) ~  albumin + age, data=d)
# loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
m3$loglik[2]
    m4$loglik[2]
(df &lt;- abs(print(anova(m3)[, ""d.f.""])[['TOTAL']] - print(anova(m4)[, ""d.f.""])[['TOTAL']]))
(LR &lt;- 2 * (m4$loglik[2] - m3$loglik[2])) # the most parsimonious models have to be first
pchisq(LR, df, lower=FALSE)
</code></pre>
"
"0.138865930150177","0.131507833509084","164228","<p>GLM (family=binomial) is foucusd on when the response is dichotomous(yes/no, male/female, etc..). I'm wondering how to judge if the model we built is good eough? As we know, in OLS regression some criterion like R^2 and adjusted R^2 can tell us how much variations are explained but not for GLM. See example I performed:</p>

<pre><code>    &gt; summary(fit.full)
    Call:
    glm(formula = ynaffair ~ gender + age + yearsmarried + children + 
    +religiousness + education + occupation + rating, family = binomial(), 
    data = Affairs)

    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
    -1.6575  -0.7459  -0.5714  -0.2552   2.5099  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    0.71792    0.96165   0.747 0.455336    
    gendermale     0.28665    0.23973   1.196 0.231811    
    age           -0.04494    0.01831  -2.454 0.014142 *  
    yearsmarried   0.09686    0.03236   2.993 0.002758 ** 
    childrenyes    0.37088    0.29466   1.259 0.208147    
    religiousness -0.32230    0.09003  -3.580 0.000344 ***
    education      0.01795    0.05088   0.353 0.724329    
    occupation     0.03210    0.07194   0.446 0.655444    
    rating2       -0.02312    0.58177  -0.040 0.968303    
    rating3       -0.84532    0.57619  -1.467 0.142354    
    rating4       -1.13916    0.55740  -2.044 0.040981 *  
    rating5       -1.61050    0.56649  -2.843 0.004470 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 608.22  on 589  degrees of freedom
    AIC: 632.22
</code></pre>

<p>After removed the insignificant variables, the reduced model look like below,although the AIC decreasd, we still do not know if this is the model with the lowest AIC we can achieved:</p>

<pre><code>    &gt; summary(fit.reduced)
    Call:
    glm(formula = ynaffair ~ age + yearsmarried + religiousness + 
        +rating, family = binomial(), data = Affairs)

    Deviance Residuals: 
    Min        1Q      Median      3Q      Max  
   -1.5117  -0.7541  -0.5722  -0.2592   2.4123  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    1.10220    0.71849   1.534 0.125014    
    age           -0.03588    0.01740  -2.062 0.039224 *  
    yearsmarried   0.10113    0.02933   3.448 0.000565 ***
    religiousness -0.32571    0.08971  -3.631 0.000282 ***
    rating2        0.11848    0.57258   0.207 0.836068    
    rating3       -0.70168    0.56671  -1.238 0.215658    
    rating4       -0.96190    0.54230  -1.774 0.076109 .  
    rating5       -1.49502    0.55550  -2.691 0.007118 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 613.63  on 593  degrees of freedom
    AIC: 629.63
</code></pre>

<p>And we perform the ANOVA, suggesting that the reduced model with
four predictors fits as well as the full model:</p>

<pre><code>    &gt; anova(fit.reduced, fit.full, test=""Chisq"")
    Analysis of Deviance Table

    Model 1: ynaffair ~ age + yearsmarried + religiousness + +rating
    Model 2: ynaffair ~ gender + age + yearsmarried + children + 
             +religiousness + education + occupation + rating
    Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
     1       593     613.63                     
     2       589     608.22  4   5.4124   0.2475
</code></pre>
"
"0.117363131703255","0.11969397463728","164705","<p>I'm working on analyzing a time series of physical variables in many lakes in Florida for an associate, and I've run into an issue. I'm attempting to run a regression for each time series of physical variables in each lake. I can get regression results in R easily, but they don't match up with my coworker's JMP results. Anyway, here's a sample from the data:</p>

<pre><code>Year = seq(1987,2015)
TP = c(14, 12, 14, 14, 17, 16, 15, 12, 18, 14, 15, 18, 18, 21, 21, 17, 17, 20, 19, 17, 18, 18, 26, 20, 18, 21, 21, 20, 18)
summary(lm(TP~Year))
</code></pre>

<p>gives </p>

<pre><code>Call:
lm(formula = TP ~ Year)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7310 -1.3724 -0.4305  0.9685  6.3675 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -502.90542   98.13981  -5.124 2.18e-05 ***
Year           0.26010    0.04904   5.303 1.35e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.21 on 27 degrees of freedom
Multiple R-squared:  0.5102,    Adjusted R-squared:  0.4921 
F-statistic: 28.12 on 1 and 27 DF,  p-value: 1.35e-05
</code></pre>

<p>His JMP analysis spits out the following:</p>

<pre><code>Parameter Estimates
Term        Estimate    Std Error   t Ratio Prob&gt;|t|
Intercept   -500.4634   96.74332    -5.17   &lt;.0001*
Year        0.2588707   0.048347    5.35    &lt;.0001*
</code></pre>

<p>For all lakes and all parameters of interest, the SS, slope estimates, etc. are all slightly off. I have looked into different types of Sum of Squares for ANOVA, but changing to different types (e.g. Type III using Anova()) still doesn't get the results to match up. What am I missing? Any assistance would be appreciated.</p>

<p>Edit: Thanks for y'all's help. Sorry for the belated response, I had to meet up with my colleague. To address the questions:</p>

<ul>
<li>I have hardcoded the data in my question, but it's merely a subset of a much larger dataset from Excel. We are using the same data and the remainder of my code is working properly. <a href=""https://www.dropbox.com/s/k21v38sfdbm0ola/LWFormatted.csv?dl=0"" rel=""nofollow"">Here's what the actual data look like.</a></li>
<li>I know OLS isn't great, but it's being used for some really basic trend descriptions for informing stakeholders. I may pursue a better option in the future.</li>
<li>The JMP model is setup using Y by X with the Bivariate option, then applying a linear regression. Below is a screenshot.</li>
</ul>

<p>Thanks again for your help!</p>

<p><a href=""http://i.stack.imgur.com/ndtBr.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ndtBr.jpg"" alt=""JMP Input""></a></p>
"
"0.123091490979333","0.125536099672233","167946","<p><a href=""http://i.stack.imgur.com/p1woC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1woC.png"" alt=""residue normality and data boxplots""></a>  </p>

<p>I have a data set from 7 groups, with 20 fish in each group. Measurement of a parameter is made on 25 cells from each fish (so each observation in the data-set is completely independent, right?). One of the groups functions as the control group while other 6 are treatment groups. So we have a total of 25*20*7 measurements. This is how the data looks like (a boxplot of all 7 groups is attached):</p>

<pre><code>samples subjects groups response
    1        1      1     4.85
    2        1      1     3.77 ..
    25       1      1     4.71
    26       2      1     4.51 ..
    500      20     1     4.21
    501      1      2     4.11 ..
    3500     20     7     4.19
</code></pre>

<p>I wish to run an ANOVA and the expectation is that a couple of groups should differ from the control group in regards to the parameter under observation. Here are a few questions:</p>

<ol>
<li><p>Is the following R code appropriate? (It shows there is no significant difference between groups.)</p>

<pre><code>n = 20
k = 25
g = 7
subjects = gl(n,   k, n*k*g)
groups   = gl(g, n*k, n*k*g)

study1 = data.frame(c(1:(n*k*g)), subjects, groups, r11)
colnames(study1) = c(""samples"", ""subjects"", ""groups"", ""response"")

fit = lm(response~groups + samples*subjects, data=study1) # or aov?
anova(fit)

Analysis of Variance Table

Response: response
                   Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
groups              6  846.8 141.134 122.2864 &lt; 2.2e-16 ***
samples             1   13.1  13.055  11.3114 0.0007787 ***
subjects           19  119.5   6.289   5.4493 2.078e-13 ***
samples:subjects   19  149.6   7.872   6.8206 &lt; 2.2e-16 ***
Residuals        3454 3986.4   1.154                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre></li>
<li><p>The attached qqplot shows normal residues, however the Shapiro-Wilk test always fails on all groups. (Is my sample size of 3500 too big and problematic?)</p>

<pre><code>shapiro.test(study1$response[study1$groups==1])

data:  study1$response[study1$groups == 1] W = 0.9818, p-value =
6.648e-06
</code></pre>

<p>And so does the Levene for equality of variance:</p>

<pre><code>leveneTest(lm(response ~ groups, data=study1))
Levene's Test for Homogeneity of Variance (center = median)
        Df F value    Pr(&gt;F)    
group    6   19.37 &lt; 2.2e-16 ***
      3493    
</code></pre></li>
</ol>

<p>Please guide me as to how should I proceed. Should I keep on using ANOVA and disregard the fact that the normality and equality of variance assumptions are being violated? Should I remove the outliers from my data? Should I transform data somehow to be 'more' normal? Should I switch to non parametric or rank based tests? The end goal is to identify groups that differ significantly from the control group. </p>
"
"0.111340442853781","0.100934821880595","169543","<p>I'm doing a two-factor ANOVA using the <code>lmerTest</code> package. Each factor has multiple levels. When one (or more) of the effects are significant, I would like to do a post-hoc test to determine which of the levels differ from each other. Here, I set up the model as:</p>

<pre><code>library('lmerTest')
model = lmer('measure~factor*experiment+(1|subject_id)', data=data)
print(anova(model))
</code></pre>

<p>The output appears as follows:</p>

<pre><code>Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
                  Df  Sum Sq Mean Sq F value  Denom    Pr(&gt;F)    
factor             3 2388.82  796.27 16.3140  9.999 0.0003527 ***
experiment         3  254.11   84.70  2.7689 30.000 0.0588323 .  
factor:experiment  9 1301.40  144.60  2.9626 30.000 0.0121071 *  
</code></pre>

<p>At this point, I can inspect the factors and see which factors are significant. However, when I look at the summary of the model:</p>

<pre><code>summary(model)
</code></pre>

<p>I get a much more detailed output (truncated to the relevant portion for clarity):</p>

<pre><code>                                t value Pr(&gt;|t|)    
(Intercept)                      -5.600 8.99e-06 ***
factorLevel1                      0.289  0.77522    
factorLevel2                      2.855  0.00871 ** 
factorLevel3                     -6.535 9.00e-07 ***
experimentSession1               -0.747  0.46086    
experimentSession2               -0.825  0.41596    
experimentSession3                0.317  0.75354    
factorLevel1:experimentSession1  -1.297  0.20454    
factorLevel2:experimentSession1  -0.903  0.37376    
factorLevel3:experimentSession1   3.025  0.00506 ** 
factorLevel1:experimentSession2   0.591  0.55917    
factorLevel2:experimentSession2  -0.777  0.44341    
factorLevel3:experimentSession2   3.027  0.00504 ** 
factorLevel1:experimentSession3  -0.123  0.90269    
factorLevel2:experimentSession3  -1.060  0.29770    
</code></pre>

<p>How do I interpret these values? Is this telling me that coefficient for <code>factorLevel2</code> is significantly different from 0? If I then do a Multiple Comparison of Means:</p>

<pre><code>print(summary(glht(m, linfct=mcp(experiment=""Tukey"", factor=""Tukey""))))
</code></pre>

<p>I get the following output:</p>

<pre><code>experiment: Session1 - Session0 == 0   -0.747   0.9829    
experiment: Session2 - Session0 == 0   -0.825   0.9718    
experiment: Session3 - Session0 == 0    0.317   0.9999    
experiment: Session2 - Session1 == 0   -0.078   1.0000    
experiment: Session3 - Session1 == 0    1.064   0.9079    
experiment: Session3 - Session2 == 0    1.142   0.8759    
factor: Level2 - Level1 == 0            0.289   0.9999    
factor: Level3 - Level1 == 0            2.855   0.0425 *  
factor: Level4 - Level1 == 0           -6.535   &lt;0.001 ***
factor: Level3 - Level2 == 0            1.517   0.6542    
factor: Level4 - Level2 == 0           -5.395   &lt;0.001 ***
factor: Level4 - Level3 == 0           -8.341   &lt;0.001 ***
</code></pre>

<p>This is more understandable as it is telling me which pairwise factors are significantly different. But, I'm unsure how to interpret the summary table produced by <code>summary()</code> and how the numbers compare to the table produced by <code>glht()</code>.</p>
"
"0.111901355435757","0.114123726974758","172782","<p>Newbie question using R's mtcars dataset with anova() function. My question is how to use anova() to select the best (nested) model. Here's some example data:</p>

<pre><code>&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+am,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + am
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1     30 317.16                                
2     29 246.68  1    70.476 8.0036 0.008535 **
3     28 246.56  1     0.126 0.0143 0.905548   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+hp,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + hp
  Res.Df    RSS Df Sum of Sq       F   Pr(&gt;F)   
1     30 317.16                                 
2     29 246.68  1    70.476 10.1201 0.003571 **
3     28 194.99  1    51.692  7.4228 0.010971 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My understanding is anova() compares the reduction in the residual sum of squares to report a corresponding p-value for each nested model, where lower p-values means that nested model is more significantly different from the first model. </p>

<p>Question 1: Why is it that changing the 3rd regressor variable effects results from the 2nd nest model? That is, the p-value for <code>disp+wt</code> model changes from 0.008535 to 0.003571 going from the first to the second example. (does anova's model 2 analysis use data from model 3???)</p>

<p>Question 2: Since the 3rd model's <code>Sum of Sq</code> value is much lower in the first example (e.g. 0.126 versus 51.692), I'd expect the p-value to be lower as well, but it in fact increases (e.g. 0.905548 versus 0.010971). Why?</p>

<p>Question 3: Ultimately I'm trying to understand, given a dataset with a lot of regressors, how to use anova() to find the best model. Any general rules of thumb are appreciated. </p>
"
"0.093890505362604","0.0957551797098243","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0556702214268904","0.0567758373078348","173039","<p>I will be performing an experiment of 3 levels across one factor, and analysing the results using a <a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/repeated.html"" rel=""nofollow"">repeated measures one-way ANOVA, using R</a>.</p>

<p>Before starting the experiment, I want to perform a <a href=""http://statmethods.net/stats/power.html"" rel=""nofollow"">power analysis</a> to determine sample size (and feasibility). I have a target effect size (""medium""), significance level (0.05) and power (80%).</p>

<p>The power analysis page (link above) demonstrates how to perform a power analysis for a single-measure one-way ANOVA. How do I modify it for repeated measures?</p>

<p><em>I would also like to consider a two-factor design, which would require repeated measures two-way ANOVA. How can I perform a power analysis on that?</em></p>
"
"0.0787295821622217","0.0936753523251331","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.0742269619025206","0.0757011164104465","173401","<p>Hope everyone is doing fine. I am dealing with GLM model for my data. Below is my model. A= 2 levels, B = 2 levels, and C = 3 levels</p>

<pre><code>model &lt;- glm( y ~ FactorA*FactorB*FactorC, family = poisson, data = data)
Anova(model)`Analysis of Deviance Table (Type II tests)
</code></pre>

<p>I got all of my factors and interactions are significant except FactorB:FactorC. Since this is a GLM model, Can anyone please suggest me how can I do a post-hoc analysis for multiple comparison and how can I plot main effects and significant interactions?</p>

<p>Thank you!</p>
"
"0.0642824346533225","0.0655590899062897","174523","<p>My question is quite straightforward, but I did not find a clear answer anywhere.</p>

<p>I'm computing the Standard Error of the Estimate (SEE) by doing the square root of the Residuals Mean Square output of the anova table:</p>

<pre><code>anovatable&lt;-anova(lm(carb~hp,data=mtcars))

anovatable

Analysis of Variance Table

Response: carb

          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
hp         1 45.469  45.469  38.527 7.828e-07 ***
Residuals 30 35.406   1.180                      

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEE&lt;-sqrt(anovatable$`Mean Sq`[2])
SEE
[1] 1.086363    
</code></pre>

<p>Is it the correct way of doing it?</p>

<p>Is there any already implemented way in R to obtain the SEE? If so, It will be better than accessing the <code>Mean Sq</code> term for Residuals, since its position depends upon the number of predictors.</p>
"
"0.10562681853293","0.0957551797098243","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.0524863881081478","0.0535287727572189","175111","<p>I've understood that relative importance of predictors is a tricky question. Suggested methods range from very complex models to very simple variable transformations. I've understood that the brightest still debate which way to go on this matter. I'm looking for an easy but still appealing method to approach this in survival analysis (Cox regression).</p>

<p>My aim is to answer the question: which predictor is the most important one (in terms of predicting the outcome). The reason is simple: clinicians want to know which risk factor to adress first. I understand that ""important"" in clinical setting is not equal to ""important"" in the regression-world, but there is a link.</p>

<p>Should I compute the proportion of explainable log-likelihood that is explained by each variable (see Frank Harrell <a href=""http://stats.stackexchange.com/questions/155246/which-variable-relative-importance-method-to-use"">post</a>), by using:</p>

<pre><code>library(survival); library(rms)
data(lung)
S &lt;- Surv(lung$time, lung$status)
f &lt;- cph(S ~ rcs(age,4) + sex, x=TRUE, y=TRUE, data=lung)
plot(anova(f), what='proportion chisq')
</code></pre>

<p>As I understand it, its only possible to use the 'proportion chisq' for Cox models and this should suffice to convey some sense of each variables relative importance. Or should I perhaps use the default plot(anova()), which displays Wald Ï‡2 statistic minus its degrees of freedom for assessing the partial effect of each variable?</p>

<p>I would appreciate some guidance if anyone has any experience on this matter.</p>
"
"0.124574574491482","0.117275665258787","175597","<p>I am using the <code>quantreg</code> package in R to develop quantile estimates at different taus, then using <code>anova</code> to test whether the Beta Estimates at different quantiles are equal ($H_0$) or not ($H_1$). Thus </p>

<pre><code>library(quantreg)
data(Mammals) # sample data in quantreg
</code></pre>

<p>for taus 0.1, 0.25, 0.5, 0.75 and 0.9</p>

<pre><code>fit1 &lt;- rq(weight ~ speed + hoppers + specials, tau = .1, data = Mammals)
fit2 &lt;- rq(weight ~ speed + hoppers + specials, tau = .25, data = Mammals)
fit3 &lt;- rq(weight ~ speed + hoppers + specials, tau = .5, data = Mammals)
fit4 &lt;- rq(weight ~ speed + hoppers + specials, tau = .75, data = Mammals)
fit5 &lt;- rq(weight ~ speed + hoppers + specials, tau = .9, data = Mammals)

anova(fit1, fit2, fit3, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Results in </p>

<pre><code>Quantile Regression Analysis of Deviance Table

Model: weight ~ speed + hoppers + specials
Tests of Equality of Distinct Slopes: tau in {  0.1 0.25 0.5 0.75 0.9  }

             Df Resid Df F value  Pr(&gt;F)  
speed         4      531  1.0952 0.35810  
hoppersTRUE   4      531  2.5898 0.03599 *
specialsTRUE  4      531  1.3774 0.24046  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However changing the order of the models to say;</p>

<pre><code>anova(fit3, fit1, fit2, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Produces the <strong>exact same result!</strong></p>

<h2>My question is basically, what gives?</h2>

<p><strong>(1)</strong> Is <code>anova</code> truly comparing all the models to one another (ie all estimates from different taus, ${_nC_r} = {_5C_2} = 10$ <strong>separate</strong> comparisons) </p>

<p><strong>OR</strong> </p>

<p><strong>(2)</strong> Is <code>anova</code> selecting the model with the lowest tau and comparing the remaining models to that?</p>

<p>I've extracted (and annotated) the relevant segments of the of <code>anova</code> function called in the <code>quantreg</code> environment bellow.</p>

<pre><code>getAnywhere(anova.rqlist)
sum.fit1 &lt;- summary(fit1, covariance=TRUE); sum.fit2 &lt;- summary(fit2, covariance=TRUE); 
sum.fit3 &lt;- summary(fit3, covariance=TRUE); sum.fit4 &lt;- summary(fit4, covariance=TRUE); 
sum.fit5 &lt;- summary(fit5, covariance=TRUE)
objects &lt;- list(); objects[[1]] &lt;- sum.fit1; objects[[2]] &lt;- sum.fit2 
objects[[3]] &lt;- sum.fit3; objects[[4]] &lt;- sum.fit4; objects[[5]] &lt;- sum.fit5
taus &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)
m &lt;- length(taus)
n &lt;- length(fit1$y)
    Omega &lt;- outer(taus, taus, pmin) - outer(taus, taus) ##!!!HERE!!!###
    J &lt;- objects[[1]]$J 
# From help file on summary.rq: J is Unscaled Outer product of gradient matrix returned if cov=TRUE and se != ""iid"". The Huber sandwich is cov = tau (1-tau) Hinv %*% J %*% Hinv. 
p &lt;- dim(J)[1]
H &lt;- array(unlist(lapply(objects, function(x) x$Hinv)), c(p, p, m))
# From help file on summary.rq: Hinv : inverse of the estimated Hessian matrix returned if cov=TRUE and se %in% c(""nid"",""ker"") , note that for se = ""boot"" there is no way to split the estimated covariance matrix into its sandwich constituent parts.    
H &lt;- matrix(aperm(H, c(1, 3, 2)), p * m, p) %*% t(chol(J))
W &lt;- (H %*% t(H)) * (kronecker(Omega, outer(rep(1, p), rep(1, p)))) ##!!!HERE!!!###
coef &lt;- unlist(lapply(objects, function(x) coef(x)[, 1]))
Tn &lt;- pvalue &lt;- rep(0, p - 1)
ndf &lt;- m - 1
ddf &lt;- n * m - (m - 1)
for (i in 2:p) {
  E &lt;- matrix(0, 1, p)
  E[1, i] &lt;- 1
  D &lt;- kronecker(diff(diag(m)), E)
  Tn[i - 1] &lt;- t(D %*% coef) %*% solve(D %*% W %*% 
                                         t(D), D %*% coef)/ndf
  pvalue[i - 1] &lt;- 1 - pf(Tn[i - 1], ndf, ddf)
}
pvalue
</code></pre>

<p>The reason i care is that if explanation <strong>(1)</strong> is being implemented then all the estimates are truly being compared, while if explanation <strong>(2)</strong> is being implemented, then technically the models are only being compared to minimum tau and <strong>NOT</strong> to one another. </p>

<p><strong>Note:</strong> The lines that define <code>Omega</code> and <code>W</code> suggest to me that the latter interpretation <strong>(2)</strong> is being implemented, but I'm not sure.</p>
"
"0.178144708566049","0.174112567744027","175975","<p>Say, I wanted to compare the effect of $t=3$ (8 hours, 12 hours, 16 hours) lengths of exposures to sun on plant growth. I randomly applied these 3 lengths of exposures to $r=3$ pots but let us say that it is too costly or tedious to measure plant growth for all plants in the entire pot, so I randomly selected $s=4$ plants within each pot for my measurement. The variables would be: treatment (a factor of 3 different hours of exposure to sunlight at which each measurement of growth is taken), pot (a factor of three per treatment), plant (a factor of four plants per pot), growth (dependent variable). The following are the null hypotheses to be tested in order (according to our manual): (1) The mean plant growth within pots are the same. (2) There are no differences in mean plant growth among the different treatments.</p>

<p>The linear model for the experiment is</p>

<p>$$Y_{ijk}=\mu+\tau_i+\delta_{ij}+\varepsilon_{ijk}$$</p>

<p>where $Y_{ijk}$ is the $k$th response on the $j$th pot applied with the $i$th treatment, $\tau_i$ is the effect of the length of exposure to sunlight on plant growth, $\delta_{ij}$ is the error associated to the $j$th pot in the $i$th treatment on the growth of the plant, and $\varepsilon_{ijk}$ is the error attributed to the $k$th plant on the $j$th pot applied with treatment $i$.</p>

<p>In the corresponding ANOVA table, I have $SSTot=SSTrt+SSPE+SSSSE$, where SSTot is Total Sum of Squares, SSTrt is Treatment Sum of Square, SSPE is Sum of Squares for the Pots, and SSSSE is the sum of squares for the subsamples,  with degrees of freedom, $t-1$, $t(r-1)$, $tr(s-1)$, respectively, where $t=$ number of treatments, $r=$ number of experimental units and $s=$ number of sampling units,</p>

<p>Our school manual suggests a sequential tests of hypotheses. For tests of variability of the experimental units, it says to test</p>

<p>$$ \frac{MSPE}{MSSSE}\sim F_{(\alpha,t(r-1),tr(s-1))}$$</p>

<p>Then it suggests to consider the following cases for the test of differences among treatment means</p>

<ul>
<li>When $H_0:\sigma^2_\varepsilon=0$ is rejected</li>
</ul>

<p>$$\frac{MSTrt}{MSPE}\sim F_{(t-1,t(r-1))}$$</p>

<ul>
<li>When $H_0: \sigma^2_{\varepsilon}=0$ is accepted</li>
</ul>

<p>$$\frac{MSTrt}{MSE_{pooled}}\sim F_{(t-1,tr(s-1))}$$</p>

<p>where </p>

<p>$$MSE_{pooled}=\frac{SSPE+SSSSE}{t(rs-1)}$$.</p>

<p>Here, MS stands for mean squares for the corresponding sum of squares previously defined.</p>

<p><strong>Question: How do I perform this sequential tests of hypotheses in R</strong>?</p>

<pre><code>model &lt;- lm(response ~ trt/pot, data)
anova(model)
</code></pre>

<p>I can't get it to display the correct $F$ for the treatment differences in both cases. Below is an example when $H_0:\sigma^2_\varepsilon=0$ is rejected.</p>

<p>I actually don't know if all of these even make sense. I did not come from a stats background and I finally decided to start learning it after a while. My school uses SAS and this procedure seems to be built-in. I know of the University Edition of SAS but I couldn't run it on my old laptop. So I am trying to find a way to get the same output in R.</p>

<p><strong>Example</strong></p>

<p><a href=""https://dl.dropboxusercontent.com/u/28713619/crossvalidated/example.csv"" rel=""nofollow"">Here</a> is a the data set for the following.</p>

<pre><code>example &lt;- read.csv(""example.csv"", header=T)
example$pot &lt;- factor(example$pot)
example$hours &lt;- factor(example$hours)
model &lt;- lm(growth ~ hours/pot, example)
anova(model)
</code></pre>

<p>which gives the following output</p>

<pre><code>## Analysis of Variance Table
## 
## Response: growth
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## hours      2 15.0417  7.5208 15.3255 3.569e-05 ***
## hours:pot  6  8.2083  1.3681  2.7877   0.03054 *  
## Residuals 27 13.2500  0.4907                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>In this particular example where there is significant variance among experimental units, our school manual prescribes that the F value for hours should be <code>anova(model)$""Sum Sq""[1]/anova(model)$""Sum Sq""[2]</code>. Is there a way to compute it automatically? In this case, the $F$ for the test of treatment differences should be 5.4973.</p>

<p>This is based on the example given in our manual. I don't know if the manual is even correct in its prescribed procedures. </p>
"
"0.0262431940540739","0.0535287727572189","176398","<p>All,</p>

<p>Say I have a 2 (between) x 4 (repeated measures) mixed factorial design. I was going to analyze it with ANOVA, but normality issues and sphericity concerns have made that not a good idea. </p>

<p>Are there any reasons why sphericity would PREVENT a valid HLM analysis? I was going to do a poisson model in lme4 (addresses the non-normality issue) ... but the sphericity is not an issue in a multilevel model, correct?</p>
"
"0.139175553567226","0.141939593269587","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.167406706250773","0.148462103760625","176712","<p>This question is related to <a href=""http://stats.stackexchange.com/questions/120650/one-observation-per-level-in-mixed-effect-model"">this other question</a>.</p>

<h1>Introduction of the problem</h1>

<p>In Field et al. (2012): Discovering statistics using R, the following implementation of repeated-measures designs using lme() is suggested (p573):</p>

<pre><code>m &lt;- lme(..., random = ~1|Subject/WithinSubjectFactor1/WithinSubjectFactor2, ...)
</code></pre>

<p>An example of this is also mentioned in the <a href=""http://stats.stackexchange.com/questions/120650/one-observation-per-level-in-mixed-effect-model"">other question</a>.</p>

<p>I would like to go over another example in this book. On p. 583, a factorial repeated-measures design with one dependent variable (<code>attitude</code>) and two independent (factor) variables (<code>drink</code> and <code>imagery</code>) is introduced.
Each factor variable has three levels.
All 3x3=9 levels are measured in each subject (<code>participant</code>). For background on the study, see below.</p>

<h1>Suggested implementation by Field et al.</h1>

<pre><code>library(reshape2)
library(multilevel)

#Generating data
attitudeData&lt;-read.delim(""http://studysites.uk.sagepub.com/dsur/study/DSUR%20Data%20Files/Chapter%2013/Attitude.dat"", header = TRUE)
longAttitude &lt;-melt(attitudeData, id = ""participant"", measured = c( ""beerpos"", ""beerneg"", ""beerneut"", ""winepos"", ""wineneg"", ""wineneut"", ""waterpos"", ""waterneg"", ""waterneu"", ""participant""))
names(longAttitude)&lt;-c(""participant"", ""groups"", ""attitude"")
longAttitude$drink&lt;-gl(3, 60, labels = c(""Beer"", ""Wine"", ""Water""))
	longAttitude$imagery&lt;-gl(3,20, 180, labels = c(""Positive"", ""Negative"", ""Neutral""))
longAttitude&lt;-longAttitude[order(longAttitude$participant),]

#Setting contrasts
AlcoholvsWater&lt;-c(1, 1, -2)
BeervsWine&lt;-c(-1, 1, 0)
NegativevsOther&lt;-c(1, -2, 1)
PositivevsNeutral&lt;-c(-1, 0, 1)
contrasts(longAttitude$drink)&lt;-cbind(AlcoholvsWater, BeervsWine)
	contrasts(longAttitude$imagery)&lt;-cbind(NegativevsOther, PositivevsNeutral)

#Using lme
baseline&lt;-lme(attitude ~ 1, random = ~1|participant/drink/imagery, data = longAttitude, method = ""ML"")
# Pay attention to this:    -------------------------------------  
#                                             ^

drinkModel&lt;-update(baseline, .~. + drink)
imageryModel&lt;-update(drinkModel, .~. + imagery)
attitudeModel&lt;-update(imageryModel, .~. + drink:imagery)
anova(baseline, drinkModel, imageryModel, attitudeModel)

summary(attitudeModel)
</code></pre>

<h1>Questions</h1>

<p>I'm wondering whether the <code>random = ~1|participant/drink/imagery</code> specification is correct. This suggests that imagery is nested in drink, and drink is nested in participant, or am I mis-interpreting the code? Wouldn't <code>random = ~1|participant</code> be sufficient in this case?</p>

<p>I strongly have the feeling that the Field et al. implementation of a repeated-measures analysis with lme() is wrong.
Could someone with more statistical background look at this?</p>

<p>Also, when comparing ANOVA-based analysis of the above problem to lme-based analysis, Field et al. state that(p. 576) ""If you use lme() then you can forget about sphericity.""
Shouldn't one still consider different variance structures (homogeneous vs. heterogeneous) when performing lme()?</p>

<h1>Study background</h1>

<blockquote>
  <p>There is evidence from advertising research that attitudes towards stimuli can be changed using positive imagery (e.g., Stuart, Shimp, &amp; Engle, 1987). As part of an initiative to stop binge drinking in teenagers, the government funded some scientists to look at whether negative imagery could be used to make teenagersâ€™ attitudes towards alcohol more negative. The scientists designed a study to address this issue by comparing the effects of negative imagery against positive and neutral imagery for different types of drinks. Table 13.4 illustrates the experimental design and contains the data for this example (each row represents a single participant).</p>
  
  <p>Participants viewed a total of nine mock adverts over three sessions. In one session, they saw three adverts: (1) a brand of beer (Brain Death) presented with a negative image (a dead body with the slogan â€˜drinking Brain Death makes your liver explodeâ€™); (2) a brand of wine (Dangleberry) presented in the context of a positive image (a sexy naked man or woman â€“ depending on the participantâ€™s preference â€“ and the slogan â€˜drinking Dangleberry wine makes you irresistibleâ€™); and (3) a brand of water (Puritan) presented alongside a neutral image (a person watching television accompanied by the slogan â€˜drinking Puritan water makes you behave completely normallyâ€™). In a second session (a week later), the participants saw the same three brands, but this time Brain Death was accompanied by the positive imagery, Dangleberry by the neutral image and Puritan by the negative. In a third session, the participants saw Brain Death accompanied by the neutral image, Dangleberry by the negative image and Puritan by the positive. After each advert participants were asked to rate the drinks on a scale ranging from âˆ’100 (dislike very much) through 0 (neutral) to 100 (like very much). The order of adverts was randomized, as was the order in which people participated in the three sessions. This design is quite complex. There are two independent variables: the type of drink (beer, wine or water) and the type of imagery used (positive, negative or neutral). These two variables completely cross over, producing nine experimental conditions.</p>
</blockquote>
"
"0.0656079851351847","0.0802931591358284","176864","<p>I have a dataframe called ""spf""</p>

<pre><code>str(spf)
'data.frame':   120 obs. of  6 variables:
 $ id     : Factor w/ 30 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 2 2 2 2 3 3 ...  
 $ BTW    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...  
 $ WTH1   : Factor w/ 2 levels ""0"",""1"": 1 1 2 2 1 1 2 2 1 1 ...  
 $ WTH2   : Factor w/ 2 levels ""0"",""1"": 1 2 1 2 1 2 1 2 1 2 ...  
 $ Z_score: num  -1.06 -0.678 1.194 1.94 -1.06...
</code></pre>

<p>And I do three-ways anova analysis:</p>

<pre><code>anova &lt;- aov (Z_score ~ BTW*WTH1*WTH2 + Error (id/(WTH1*WTH2)), data = spf)
</code></pre>

<p>Then</p>

<pre><code>summary (anova)
</code></pre>

<p>I have:</p>

<pre><code>Error: id
          Df Sum Sq Mean Sq F value Pr(&gt;F)
BTW        4   0.00   0.000       0      1
Residuals 25  39.19   1.567               

Error: id:WTH1
           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
 WTH1       1 13.632  13.632  18.769 0.00021 ***
 BTW:WTH1   4  2.181   0.545   0.751 0.56686 
 Residuals 25 18.158   0.726  
...
</code></pre>

<p>My question is:</p>

<ul>
<li><p>why the ""Sum Sq"" of BTW variable is 0, it does not make sense for me.</p></li>
<li><p>Why ""Sum Sq"" and ""Mean Sq"" of WTH1 are the same (both are 13.632)? I am a very newbie in ANOVA, but I guess ""sum of something"" and ""mean of something"" should be different?</p></li>
</ul>

<p>Did I do anything wrong?</p>
"
"0.105409255338946","0.127048637363686","176869","<p>I am quite newbie of ANOVA, and just learnt how to run the function.</p>

<p>I am using the function 'Anova' from the package 'car' in R, and when I summary the analysis, I see the text:</p>

<pre><code>Univariate Type II Repeated-Measures ANOVA Assuming Sphericity
</code></pre>

<p>My professor told me that, ""assuming sphericity"" is not the thing she want, and she want some kind of ""real"" thing. Unfortunately, she does not know R well so she cannot tell me how to do that.</p>

<p>Could you explain me what is ""assuming sphericity"", and how could I satisfy the requirement of my prof?</p>

<p><strong>Updated:</strong></p>

<p>Thanks for Rolan and Peter, I chose to run ""lme"" (in package ""nlme"") and I have the output:</p>

<pre><code>    lme3 &lt;- lme (Z_score ~ WTH1 + WTH2, random = ~1|BTW, data = spf)
    summary (lme3)

&gt; Linear mixed-effects model fit by REML  Data: spf 
&gt;        AIC      BIC    logLik
&gt;   327.9795 341.7903 -158.9897
&gt; 
&gt; Random effects:  Formula: ~1 | BTW
&gt;          (Intercept)  Residual StdDev: 2.310226e-05 0.8962092
&gt; 
&gt; Fixed effects: Z_score ~ WTH1 + WTH2 
&gt;                  Value Std.Error  DF   t-value p-value  
&gt;(Intercept) -0.6584315 0.1417031 113 -4.646557   0e+00  
&gt;WTH11        0.6741030 0.1636247 113  4.119813   1e-04  
&gt;WTH21        0.6427601 0.1636247 113  3.928259   1e-04  
&gt;Correlation: 
&gt;       (Intr) WTH11  
&gt; WTH11 -0.577        
&gt; WTH21 -0.577  0.000
&gt; 
&gt; Standardized Within-Group Residuals:
&gt;        Min         Q1        Med         Q3        Max 
&gt; -2.1274596 -0.7449203 -0.1466509  0.6385587  3.2199382 
&gt; 
&gt; Number of Observations: 120 Number of Groups: 5
</code></pre>

<p>So, you can see, my dataset have 5 columns:</p>

<pre><code>&gt; str (spf)
&gt; 'data.frame': 120 obs. of  5 variables:
&gt;  $ id     : Factor w/ 30 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 2 2 2 2 3 3 ... 
    &gt;  $ BTW    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
&gt;  $ WTH1   : Factor w/ 2 levels ""0"",""1"": 1 1 2 2 1 1 2 2 1 1 ...
    &gt;  $ WTH2   : Factor w/ 2 levels ""0"",""1"": 1 2 1 2 1 2 1 2 1 2 ...
&gt;  $ Z_score: num  -1.06 -0.678 1.194 1.94 -1.06 ...
</code></pre>

<p>BTW (between) is actually a Group ID (we run an experiment 5 times, each time with a different group), and WTH1 and WTH2 (within) are within variables. Z_score is the measurement.</p>

<p>I want to know, is there any effect of BTW on WTH1 and WTH2 (I hope not), but I do not know how to interpret the data of lme?</p>

<p>(Using Anova function I mentioned above, the result is quite straightforward, with the notion '<strong>*' and '</strong>' etc to determine the significant level, but there is a problem related to 'Assuming Sphericity"")</p>
"
"0.0701378863464054","0.0715308314257114","178004","<p>I have a data like below:</p>

<p>My data:</p>

<pre><code>str (df)
'data.frame': 120 obs. of  5 variables:
 $ id     : Factor w/ 30 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 2 2 2 2 3 3 ... 
     $ BTW    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ WTH1   : Factor w/ 2 levels ""0"",""1"": 1 1 2 2 1 1 2 2 1 1 ...
     $ WTH2   : Factor w/ 2 levels ""0"",""1"": 1 2 1 2 1 2 1 2 1 2 ...
 $ score: num  -1.06 -0.678 1.194 1.94 -1.06 ...

df &lt;- read.table(text=""id BTW WTH1 WTH2       score
                1   1    0    0 -1.16055841
                1   1    0    1 -0.78485342
                1   1    1    0  1.05951654
                1   1    1    1  1.79384902
                7   2    0    0 -0.44330343
                7   2    0    1  1.87923652
                7   2    1    0 -0.44330343
                7   2    1    1 -0.42622593
               13   3    0    0 -1.16055841
               13   3    0    1 -0.22129593
               13   3    1    0  0.29102906
               13   3    1    1  0.71796654
               19   4    0    0 -0.40914843
               19   4    0    1  0.49595905
               19   4    1    0  1.69138402
               19   4    1    1  0.35933905
               25   5    0    0 -0.30668343
               25   5    0    1 -0.40914843
               25   5    1    0  1.84508152
               25   5    1    1 -0.13590843
                2   1    0    0 -1.16055841
                2   1    0    1 -1.10932591
                2   1    1    0 -0.93855091
                2   1    1    1 -1.02393841
                8   2    0    0 -1.01629043
                8   2    0    1  0.11668190
                8   2    1    0  0.26631975
                8   2    1    1  0.82211750
               14   3    0    0 -0.18259381
               14   3    0    1  0.95037852
               14   3    1    0  1.20690056
               14   3    1    1  1.31378474
               20   4    0    0 -0.54600003
               20   4    0    1  1.50617627
               20   4    1    0 -1.03766727
               20   4    1    1  1.67719096
               26   5    0    0 -0.73839156
               26   5    0    1 -0.67426105
               26   5    1    0 -1.16592829
               26   5    1    1  0.35182710
                3   1    0    0 -1.27281247
                3   1    0    1  1.50617627
                3   1    1    0  1.10001638
                3   1    1    1 -0.20397065
                9   2    0    0 -1.10179778
                9   2    0    1 -0.97353676
                9   2    1    0 -0.82389890
                9   2    1    1 -1.08042094
               15   3    0    0 -1.37219594
               15   3    0    1 -0.55904279
               15   3    1    0 -0.28257072
               15   3    1    1  0.12400586
               21   4    0    0 -0.38014910
               21   4    0    1  0.25411036
               21   4    1    0 -0.07115090
               21   4    1    1 -0.05488784
               27   5    0    0 -0.67288423
               27   5    0    1 -0.21751847
               27   5    1    0 -0.64035811
               27   5    1    1  0.35168874
                4   1    0    0 -1.22582837
                4   1    0    1  0.15653198
                4   1    1    0  0.62816081
                4   1    1    1 -0.68914730
               10   2    0    0 -1.06319774
               10   2    0    1 -0.39641216
               10   2    1    0 -0.02236171
               10   2    1    1  0.23784730
               16   3    0    0 -0.64035811
               16   3    0    1  1.18110495
               16   3    1    0  2.14062567
               16   3    1    1  3.21398783
               22   4    0    0 -0.59035895
               22   4    0    1  0.04005773
               22   4    1    0 -0.29091103
               22   4    1    1 -0.22786936
               28   5    0    0 -1.34685897
               28   5    0    1 -1.25229646
               28   5    1    0 -1.28381730
               28   5    1    1 -1.03165063
                5   1    0    0 -1.37837980
                5   1    0    1  0.40254732
                5   1    1    0  0.48134940
                5   1    1    1  0.81231816
               11   2    0    0 -1.40990063
               11   2    0    1  1.04872441
               11   2    1    0 -0.27515061
               11   2    1    1  0.35526607
               17   3    0    0 -0.35395270
               17   3    0    1  1.41121400
               17   3    1    0  2.18347443
               17   3    1    1  1.89978693
               23   4    0    0 -0.32243186
               23   4    0    1  0.40254732
               23   4    1    0  0.63895357
               23   4    1    1  0.08733898
               29   5    0    0 -0.76451809
               29   5    0    1  0.15770687
               29   5    1    0 -0.01521031
               29   5    1    1 -0.26497957
                6   1    0    0 -1.30248265
                6   1    0    1  0.50354123
                6   1    1    0 -1.20641755
                6   1    1    1 -1.34090869
               12   2    0    0 -1.53303889
               12   2    0    1 -0.09206239
               12   2    1    0 -0.53396185
               12   2    1    1  0.19613291
               18   3    0    0 -1.53303889
               18   3    0    1  0.33062405
               18   3    1    0  1.42576620
               18   3    1    1  1.00307976
               24   4    0    0 -0.47632279
               24   4    0    1  0.71488446
               24   4    1    0  1.71396150
               24   4    1    1  2.34799116
               30   5    0    0 -0.45710977
               30   5    0    1  0.13849385
               30   5    1    0  0.36905009
               30   5    1    1  0.61881936"", header=TRUE)
</code></pre>

<p>I want to do anova analysis with this, but I want to define that, ""id"" and ""BTW"" are random effects, and ""WTH1"" and ""WTH2"" are fixed effects.</p>

<p>I tried to do:</p>

<pre><code>aov (score ~ BTW*WTH1*WTH2 + Error (id/(WTH1*WTH2)), data = df)
</code></pre>

<p>and</p>

<pre><code>aov (score ~ BTW*WTH1*WTH2 + Error (BTW/(WTH1*WTH2)), data = df)
</code></pre>

<p>I am not sure which method is the correct one, and also, in the 2nd method, the aov does not show the significance test ('<strong><em>','</strong>','</em>' ... in R).</p>

<p>Could you help to tell me how can I do ANOVA analysis with this?</p>

<p>Thanks a lot,</p>
"
"0.12894693513945","0.111275859123071","178384","<p>I've been trying to analyze my data set and I believe that I am on the right track, but need some conformation. I'm trying to analyze the catch rate of fishes along several reaches of the same river and evaluate the effectiveness of the type of gear used over 2 years of study. My data consists of:</p>

<ol>
<li>21 different sites</li>
<li>Sampling techniques categorized as ""Active"" or ""Passive""</li>
<li>2 years of data gathering separated by month. </li>
</ol>

<p>The sites were not sampled uniformly over the course of the study. They were not all sampled every month, not all for the same amount of time, nor with the same sampling techniques. I believe you would not categorize it as repeated measures since almost no two sampling periods are the same. </p>

<p>I believe that the correct way to analyze these data would be to use a 2-way randomized block ANOVA. The months would be the blocking factor in the analysis. I got some results, but was unsure if the code used was correct. </p>

<p>Would anyone be able to proof the code I used and confirm / deny that it is indeed the correct code for a 2-way randomized block design in R? </p>

<pre><code>FishLM &lt;- lm(Caught.Hr ~ Site + Method + Site:Method, Fish)
anova(FishLM)
</code></pre>

<p>Here is some sample data: </p>

<pre><code>Site     Month  Year    Device  Method  Hrs/Month   Caught  Caught/Hr  
Reach 01    5   2014    BS      Active  0.7            0    0  
Reach 01    6   2014    BS      Active  7.92           0    0  
Reach 01    7   2014    BS      Active  5.73           0    0  
Reach 01    8   2014    BS      Active  1.82           0    0  
Reach 01    9   2014    BS      Active  10.08          0    0  
Reach 01    10  2014    BS      Active  10.08          0    0  
Reach 01    11  2014    BS      Active  6.9            0    0  
Reach 02    3   2013    BS      Active  2.5            0    0  
Reach 02    4   2013    BS      Active  2.5            0    0  
Reach 02    5   2013    BS      Active  3.75           0    0  
Reach 02    6   2013    BS      Active  17.3           0    0  
Reach 02    7   2013    BS      Active  2.5            0    0  
Reach 02    8   2013    BS      Active  2.5            0    0  
Reach 02    9   2013    BS      Active  2.5            0    0  
Reach 02    10  2013    BS      Active  2.5            0    0  
Reach 02    11  2013    BS      Active  2.5            0    0  
Reach 03    3   2013    BS      Active  3              0    0  
Reach 03    4   2013    BS      Active  3              0    0  
Reach 03    5   2013    BS     Active   2.5            0    0  
Reach 03    6   2013    BS     Active   3.5            1    0.285714286  
Reach 03    7   2013    BS     Active   3              0    0  
Reach 03    8   2013    BS     Active   3              0    0  
Reach 03    9   2013    BS     Active   3              1    0.333333333  
Reach 03    10  2013    BS     Active   8.75           2    0.228571429  
Reach 03    11  2013    BS      Active  3              0    0  
Reach 04    3   2013    MT      Passive           
Reach 04    4   2013    MT      Passive           
Reach 04    5   2013    MT      Passive           
Reach 04    6   2013    MT      Passive 72             0    0  
Reach 04    7   2013    MT      Passive 120            2    0.016666667  
Reach 04    8   2013    MT      Passive 120            0    0  
Reach 04    9   2013    MT      Passive 72             0    0  
Reach 04    10  2013    MT      Passive           
Reach 04    11  2013    MT      Passive           
Reach 07    3   2014    MF      Passive           
Reach 07    4   2014    MF      Passive 96             7    0.072916667  
Reach 07    5   2014    MF      Passive 96             5    0.052083333  
Reach 07    6   2014    MF      Passive 96             8    0.083333333  
Reach 07    7   2014    MF      Passive 96             1    0.010416667  
Reach 07    8   2014    MF      Passive 96             1    0.010416667  
Reach 07    9   2014    MF      Passive 96             3    0.03125  
Reach 07    10  2014    MF      Passive 96            10    0.104166667  
Reach 07    11  2014    MF      Passive           
Reach 03    3   2013    BP      Active          
Reach 03    4   2013    BP      Active          
Reach 03    5   2013    BP      Active          
Reach 03    6   2013    BP      Active 0.143           1    6.993006993
Reach 03    7   2013    BP      Active          
Reach 03    8   2013    BP      Active          
Reach 03    9   2013    BP      Active          
Reach 03    10  2013    BP      Active          
Reach 03    11  2013    BP      Active          
Reach 09    3   2013    BP      Active          
Reach 09    4   2013    BP      Active          
Reach 09    5   2013    BP      Active          
Reach 09    6   2013    BP      Active          
Reach 09    7   2013    BP      Active          
Reach 09    8   2013    BP      Active          
Reach 09    9   2013    BP      Active          
Reach 09    10  2013    BP      Active  1.097222222    0    0
Reach 09    11  2013    BP      Active          
Reach 09    3   2014    MF      Passive         
Reach 09    4   2014    MF      Passive 96             0    0
Reach 09    5   2014    MF      Passive 96             2    0.020833333
Reach 09    6   2014    MF      Passive 336            0    0
Reach 09    7   2014    MF      Passive 336            0    0
Reach 09    8   2014    MF      Passive 96             0    0
Reach 09    9   2014    MF      Passive 96             0    0
Reach 09    10  2014    MF      Passive 96             0    0
Reach 09    11  2014    MF      Passive 96             0    0
Reach 08    3   2014    MF      Passive         
Reach 08    4   2014    MF      Passive 96             1    0.010416667
Reach 08    5   2014    MF      Passive 96             0    0
Reach 08    6   2014    MF      Passive 96             9    0.09375
Reach 08    7   2014    MF      Passive 96             0    0
Reach 08    8   2014    MF      Passive 96             6    0.0625
Reach 08    9   2014    MF      Passive 96             0    0
Reach 08    10  2014    MF      Passive 96             0    0
Reach 08    11  2014    MF      Passive         
Reach 10    3   2014    MF      Passive         
Reach 10    4   2014    MF      Passive 96             0    0
Reach 10    5   2014    MF      Passive 96             1    0.010416667
Reach 10    6   2014    MF      Passive 336            0    0
Reach 10    7   2014    MF      Passive 720            0    0
Reach 10    8   2014    MF      Passive 480            0    0
Reach 10    9   2014    MF      Passive 96             0    0
Reach 10    10  2014    MF      Passive 96             0    0
Reach 10    11  2014    MF      Passive 96             0    0
Reach 08    3   2014    MT      Passive         
Reach 08    4   2014    MT      Passive         
Reach 08    5   2014    MT      Passive         
Reach 08    6   2014    MT      Passive         
Reach 08    7   2014    MT      Passive 648            2    0.00308642
Reach 08    8   2014    MT      Passive         
Reach 08    9   2014    MT      Passive         
Reach 08    10  2014    MT      Passive         
Reach 08    11  2014    MT      Passive         
Reach 11    3   2014    MT      Passive         
Reach 11    4   2014    MT      Passive         
Reach 11    5   2014    MT      Passive 504            0    0
Reach 11    6   2014    MT      Passive 168            3    0.017857143
Reach 11    7   2014    MT      Passive 1440           0    0
Reach 11    8   2014    MT      Passive         
Reach 11    9   2014    MT      Passive         
Reach 11    10  2014    MT      Passive         
Reach 11    11  2014    MT      Passive         
</code></pre>
"
"0.0821541921922785","0.0957551797098243","178619","<p>I am using 'Anova' function in 'car' package to analyze plot - split experimental data.</p>

<p>I have 1 between group, and 2 within group variables, so it is SPFp.qr analysis.</p>

<p>The problem is, the between group variable is random effect, and 2 within group variables are fixed effects. But it seems to me that, R understand all of them are fixed effects.</p>

<p>How can I tell R that the between group variable is random effect?</p>

<p><strong>Update</strong></p>

<p>Below are Anova code, and my data:</p>

<pre><code>dfW1         &lt;- reshape(df, v.names=""score"", timevar=""WTH1"", idvar=c(""id"",""BTW"",""WTH2""),
                        direction = ""wide"")
dfSPFp.qrW   &lt;- reshape(dfW1, v.names=c(""score.0"", ""score.1""), timevar=""WTH2"", 
                        idvar=c(""id"", ""BTW""), direction=""wide"")
fitSPFp.qr   &lt;- lm(cbind(score.0.0, score.1.0, score.0.1, score.1.1) ~ BTW, 
                   data=dfSPFp.qrW)
inSPFp.qr    &lt;- expand.grid(WTH1=gl(2, 1, labels=c(""0"",""1"")), 
                            WTH2=gl(2, 1, labels=c(""0"",""1"")) )
AnovaSPFp.qr &lt;- Anova(fitSPFp.qr, idata=inSPFp.qr, idesign=~WTH1*WTH2)
print(summary(AnovaSPFp.qr, multivariate=FALSE, univariate=TRUE))
</code></pre>

<p>And the result:</p>

<pre><code>Univariate Type II Repeated-Measures ANOVA Assuming Sphericity

                   SS num Df Error SS den Df       F    Pr(&gt;F)      
(Intercept)    0.0000      1   32.691     25  0.0000   1.00000     
BTW           15.4270      4   32.691     25  2.9494   0.03991 *  
WTH1          13.0875      1   14.809     25 22.0943 8.099e-05 ***
BTW:WTH1       5.0237      4   14.809     25  2.1203   0.10815    
WTH2          11.1769      1    9.657     25 28.9338 1.402e-05 ***
BTW:WTH2       1.4321      4    9.657     25  0.9268   0.46420    
WTH1:WTH2      7.0250      1    7.959     25 22.0653 8.164e-05 ***
BTW:WTH1:WTH2  1.7118      4    7.959     25  1.3442   0.28142
</code></pre>

<p>My data:</p>

<pre><code>str (df)
'data.frame': 120 obs. of  5 variables:
 $ id     : Factor w/ 30 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 2 2 2 2 3 3 ... 
 $ BTW    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ WTH1   : Factor w/ 2 levels ""0"",""1"": 1 1 2 2 1 1 2 2 1 1 ...
 $ WTH2   : Factor w/ 2 levels ""0"",""1"": 1 2 1 2 1 2 1 2 1 2 ...
 $ score: num  -1.06 -0.678 1.194 1.94 -1.06 ...

df &lt;- read.table(text=""id BTW WTH1 WTH2       score
                1   1    0    0 -1.16055841
                1   1    0    1 -0.78485342
                1   1    1    0  1.05951654
                1   1    1    1  1.79384902
                7   2    0    0 -0.44330343
                7   2    0    1  1.87923652
                7   2    1    0 -0.44330343
                7   2    1    1 -0.42622593
               13   3    0    0 -1.16055841
               13   3    0    1 -0.22129593
               13   3    1    0  0.29102906
               13   3    1    1  0.71796654
               19   4    0    0 -0.40914843
               19   4    0    1  0.49595905
               19   4    1    0  1.69138402
               19   4    1    1  0.35933905
               25   5    0    0 -0.30668343
               25   5    0    1 -0.40914843
               25   5    1    0  1.84508152
               25   5    1    1 -0.13590843
                2   1    0    0 -1.16055841
                2   1    0    1 -1.10932591
                2   1    1    0 -0.93855091
                2   1    1    1 -1.02393841
                8   2    0    0 -1.01629043
                8   2    0    1  0.11668190
                8   2    1    0  0.26631975
                8   2    1    1  0.82211750
               14   3    0    0 -0.18259381
               14   3    0    1  0.95037852
               14   3    1    0  1.20690056
               14   3    1    1  1.31378474
               20   4    0    0 -0.54600003
               20   4    0    1  1.50617627
               20   4    1    0 -1.03766727
               20   4    1    1  1.67719096
               26   5    0    0 -0.73839156
               26   5    0    1 -0.67426105
               26   5    1    0 -1.16592829
               26   5    1    1  0.35182710
                3   1    0    0 -1.27281247
                3   1    0    1  1.50617627
                3   1    1    0  1.10001638
                3   1    1    1 -0.20397065
                9   2    0    0 -1.10179778
                9   2    0    1 -0.97353676
                9   2    1    0 -0.82389890
                9   2    1    1 -1.08042094
               15   3    0    0 -1.37219594
               15   3    0    1 -0.55904279
               15   3    1    0 -0.28257072
               15   3    1    1  0.12400586
               21   4    0    0 -0.38014910
               21   4    0    1  0.25411036
               21   4    1    0 -0.07115090
               21   4    1    1 -0.05488784
               27   5    0    0 -0.67288423
               27   5    0    1 -0.21751847
               27   5    1    0 -0.64035811
               27   5    1    1  0.35168874
                4   1    0    0 -1.22582837
                4   1    0    1  0.15653198
                4   1    1    0  0.62816081
                4   1    1    1 -0.68914730
               10   2    0    0 -1.06319774
               10   2    0    1 -0.39641216
               10   2    1    0 -0.02236171
               10   2    1    1  0.23784730
               16   3    0    0 -0.64035811
               16   3    0    1  1.18110495
               16   3    1    0  2.14062567
               16   3    1    1  3.21398783
               22   4    0    0 -0.59035895
               22   4    0    1  0.04005773
               22   4    1    0 -0.29091103
               22   4    1    1 -0.22786936
               28   5    0    0 -1.34685897
               28   5    0    1 -1.25229646
               28   5    1    0 -1.28381730
               28   5    1    1 -1.03165063
                5   1    0    0 -1.37837980
                5   1    0    1  0.40254732
                5   1    1    0  0.48134940
                5   1    1    1  0.81231816
               11   2    0    0 -1.40990063
               11   2    0    1  1.04872441
               11   2    1    0 -0.27515061
               11   2    1    1  0.35526607
               17   3    0    0 -0.35395270
               17   3    0    1  1.41121400
               17   3    1    0  2.18347443
               17   3    1    1  1.89978693
               23   4    0    0 -0.32243186
               23   4    0    1  0.40254732
               23   4    1    0  0.63895357
               23   4    1    1  0.08733898
               29   5    0    0 -0.76451809
               29   5    0    1  0.15770687
               29   5    1    0 -0.01521031
               29   5    1    1 -0.26497957
                6   1    0    0 -1.30248265
                6   1    0    1  0.50354123
                6   1    1    0 -1.20641755
                6   1    1    1 -1.34090869
               12   2    0    0 -1.53303889
               12   2    0    1 -0.09206239
               12   2    1    0 -0.53396185
               12   2    1    1  0.19613291
               18   3    0    0 -1.53303889
               18   3    0    1  0.33062405
               18   3    1    0  1.42576620
               18   3    1    1  1.00307976
               24   4    0    0 -0.47632279
               24   4    0    1  0.71488446
               24   4    1    0  1.71396150
               24   4    1    1  2.34799116
               30   5    0    0 -0.45710977
               30   5    0    1  0.13849385
               30   5    1    0  0.36905009
               30   5    1    1  0.61881936"", header=TRUE)
</code></pre>
"
"NaN","NaN","179992","<p>I have part of the analysis of some data where there are 24 observations all together and I have been given the:</p>

<p>Degrees of freedom, Deviance residuals, Degrees of freedom residuals and deviance</p>

<p>How can I use these to form an ANOVA table ? </p>

<p><a href=""http://i.stack.imgur.com/WZz90.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZz90.png"" alt=""enter image description here""></a></p>
"
"0.127716247033867","0.138936217408987","180288","<p>I am trying to understand the effect of a covariate (COVAR) in a linear mixed effects model with 2 categorical IVs (IV1, IV2). In order to illustrate where I am struggling, I had to paste the rather long <code>dput()</code> here:</p>

<pre><code>df &lt;- structure(list(ID=c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L),
IV1=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L),.Label=c(""412A"",""415D"",""512A"",""515A"",""615A""),class=""factor""),
IV2=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L),.Label=c(""24"",""27"",""2403""),class=""factor""),
DV=c(NA,NA,NA,17,19,27,14,21,21,31,34,NA,22,29,32,16,18,NA,NA,NA,39,33,27.5,28,27,NA,18,NA,24,38,27,15,NA,NA,22,27,17,52,NA,19,35,37,38,30,29,44,74,60,31,54,66,61,60,35,49,NA,52,53,30,36.5,46,57,54,59,NA,41,45,53.5,39,48,43,58.5,50,31,46,23,46,44,25,51,49,32.5,51,37,53,34,52,56,50.5,10,33,31,35,39,27,22,36,21,39,26,35,24,NA,28,39,28,35,21,39,34,30,NA,25,13,NA,31,28,29,32,NA,21,18,32,34,33.5,55,46,26.5,57,29,37,NA,23,52,31,32,41,25,29.5,47,37.5,30,NA,NA,NA,NA,43,43,43,29,42,31,NA,36,16,55,11,30,50,49,38,33,42,45,43.5,35,28,NA,44,36.5,34,41,35,17,38.5,24,49,42,40.5,37.5,15,37.5,32,30,44,25,38,39.5,37.5,43,25,28.5,26,32,43,NA,19,35,19,40.5,33,13,39,39,32,39,44,7,39,40,16,35,52,33,NA,54,24,52,37,31,27,24,31,18,50,16,31,NA,43,NA,42,39,NA,NA,51,36,38,28,NA,30,27,30,31,31,19,NA,38,35,38,21,29,31,8,32,19,23,18,NA,22,30,31,44,31,14,NA,28,25,34,32,39,30,27,33,44,47,16,46.5,12,24,17,40,29,21,47,6,19.5,39,32,28,43,51,42,44,36,48,37,32,37,43,41,10,5,37,28,10,35,45.5,51,22,35,38,39,45,44,46,24,41,37.5,30,NA,33,21,24,NA,25,27,18,NA,22,42,19,30,31,36,19,18,42,25,12,30,32,36.5,27,36,39,37,36,43,35,30.5,11,36,15,43,37,38,23,34,NA,14,39,35,42,38,45,31,41,37,36,37,33,12,44,42,45,39.5,36,44.5,38,14,14,36.5,36,32,43,39,35,38,51,43,48,35,25,49,46,26,46,51.5,35,45.5,NA,53,38.5,45,53,34,51,31,13,36,NA,32,37,43,43,19,35.5,45,41,28,42,44,43,44,34,30,46,43,45,37,33.5,47,23,19,36,38.5,26,41,NA,34,35.5,25,11,38,34,47,9,47,16,20,31,9,9,35,32,NA,34.5,31,NA,32,39,NA,NA,NA,NA,32,26,10,11,NA,37,44,25,15,37,25,10,NA,15,32,NA,24,27,NA,25,31,23,41.5,27,40,31,32,11,NA,14,25,29,36,37,31.5,37,27,21,NA,27,38,NA,NA,25,23,25,40,NA,47,35,33,39,35,38,43,27,35.5,33,28,NA,40,30,48,39,11,35,42.5,42.5,42,42,38,48,46,41,NA,32.5,43.5,34,29,35,NA,38,NA,NA,31,36,31,28.5,15,25,34,30,36,26,35,39,19,NA,NA,31,22,NA,NA,35,35,15,23,38.5,38,NA,36,16,18,26,30,28,NA,25,27,26,25,5,41,29,37,28,34,43,38,29,45,NA,41,32,37,50,31,NA,35,40,41,36,25,34,38,32,38,42,33,34,39,34,39,31,46,8,NA,36,48,25,32,37,NA,40,32,17,37,29,NA,37.5,NA,38,39,NA,44,48,40,NA,20,NA,36.5,20,33,31,41,32.5,28,43,39,29,23,37,32,39,26,36,15,37,31,11,38,29,42,38.5,32,30,37,38,32,33),
COVAR=c(5.2,5.2,5.87,5.68,5.49,7.67,6.3,8.34,7.01,5.51,5.8,4.35,3.95,5.23,6.32,4.01,3.16,3.61,4.67,3.44,5.27,4.59,4.18,4.64,3.97,4.11,3.68,7.57,3.97,5.9,6.02,4.79,5.14,5.84,7.61,4.99,4.18,7.25,3.92,6.3,6.04,5.02,8.01,4.14,8.24,6.21,7.44,5.69,6.31,5.9,6.7,4.96,5.08,4.93,6.4,7.2,7.38,9.59,6.37,8.24,5.6,5.87,4.99,3.64,3.44,5.72,4.52,6.5,4.78,5.18,5.92,8.79,7.65,4.5,4.3,5.76,8.53,4.38,4.46,8.7,8.26,8.89,5.85,6.98,6.65,7.27,8.92,7.43,5.91,5.49,7.64,7.15,6.8,5.74,4.63,4.62,7.02,5.43,9.59,5.42,6.13,8.9,4.66,6.87,6.83,8.38,8.96,5.25,5.54,6.95,8.03,4.33,7.76,6.35,4.99,7.41,6.13,4.67,4.1,4.51,4.6,3.71,6.72,5.37,8.21,6.5,5.46,5.6,7.83,5.08,5.42,3.9,4.88,6.63,4.21,5.3,4.57,8.56,3.84,7.07,4.84,6.19,5.15,3.73,5.32,8.32,7.09,6.06,5.42,7,6.65,5.28,6.08,4.84,4.73,5.15,5.44,6.38,7.4,6.28,4.96,5.14,5.53,8.46,6.93,5.34,5.03,4.4,6.68,7.31,6.17,5.5,9.65,4.36,4.64,6.77,6.95,7.56,8.47,4.68,3.9,4.33,4.77,3.65,5.17,4.44,6.37,4.35,4.55,7.09,4.06,7.78,4.49,6.37,9.03,2.67,3.89,4.38,5.56,6.77,4.48,4.69,4.94,6.17,4.32,4.25,8.11,3.79,5.62,3.99,5.19,4.47,7.07,8.32,8.79,4.27,4.55,4.5,4.15,5.12,10.11,7.68,4.01,6.53,5.66,6.52,5.99,6.62,9.44,5.44,11.1,8.62,5.85,3.82,9.46,8.69,10.36,6.95,6.27,8.37,6.35,7.12,3.71,8.21,5.98,5.49,7.62,6.31,7.98,8.26,6.93,7.03,3.4,3.35,4.74,5.84,7.99,5.07,7.35,7.88,7.44,9.32,7.22,6.47,5.32,5.98,6.61,8.26,7.79,8.19,7.05,3.24,6.5,3.94,7.33,4.4,6.22,5.95,3.56,6.13,6.98,5.2,5.67,5.29,3.6,4.71,5.88,4.27,4.52,5.44,5.39,6.07,6.51,3.24,7.55,4.52,4.19,6.41,5.43,5.48,4.08,5.26,6.99,3.66,5.4,6.13,7.24,10.57,5.92,6.78,6.47,7.78,12.14,8.49,8.77,4.74,8.49,8.03,9.02,5.42,8.22,4.95,5.77,7.49,4.52,4.8,4.62,7,9.01,9.36,4.73,5.14,6.63,7.44,6.91,5.47,7.24,7.46,4.52,6.35,9.13,9.56,8.11,8.97,12.03,8.16,10.79,7.8,6.39,5.8,3.97,7.44,5.03,8.35,6.94,8.44,4.04,6.6,6.04,4.61,5.9,7.72,7.57,6.25,6.96,5.55,9.01,7.44,5.09,5.56,9.17,8.97,7.99,10.16,11.04,6.33,6.96,7,5.08,5.37,4.4,5.49,6.17,6.97,7.65,6.48,5.54,7.79,8.42,7,8.11,5.02,3.9,5.09,4.4,4.63,7.92,9.47,7.05,9.63,4.93,8.36,7.83,10.81,11.58,5.68,11.66,8.01,4.35,5.43,9.3,6.01,5.7,7.64,8.03,7.8,5.9,9.05,6.9,6.36,9.57,6.58,7.66,7.14,5.75,3.58,10.36,6.4,6.09,7.46,7.16,8.78,5.12,4.66,4.61,4.48,4.66,8.11,4.18,5.93,5.97,6.36,6.07,7.4,4.78,8.51,5.21,8.44,5.25,4.68,4.1,3.92,3.57,4.7,5.54,4.5,5.88,5.42,4.45,4.86,6.48,4.71,4.67,4.29,4.71,3.71,5.23,5.64,4.67,3.93,4.79,4.21,4.39,3.4,4.41,4.81,3.85,4.72,4.58,3.09,5.58,4.84,5.19,6.39,3.82,3.89,4.04,4.53,5.8,4.6,4.49,4.35,5.85,4.67,5.44,3.83,5.28,4.33,5.14,3.92,4.37,6.03,6.1,6.38,6.04,5.98,5.26,5.44,3.76,5.37,5.36,6.33,5.52,4.56,4.6,5.58,5.1,4.21,5.03,4.85,4.56,5.79,4.22,3.77,3.34,4.03,6.53,6.97,4.49,6.4,4.49,5.98,5.41,5.03,5.28,4.92,6.92,4.91,4.7,6.6,4.98,6.81,4.8,4.1,4.09,4.87,4.83,4.77,4.4,4.89,4.55,4.55,4.65,5.12,4.85,5.78,5.49,4.58,5.25,5.09,4.93,4.9,5.42,5.33,4.81,4.61,6.67,4.46,5.33,8.05,5.99,4.35,5.06,5.31,4.29,4.29,3.48,4.32,3.86,4.64,4.03,4.18,5.39,4.35,3.54,4.22,3.65,4.63,4.61,4.14,3.4,4.28,5.98,3.48,3.68,5.54,4.22,4.78,3.49,5.84,6.52,6.1,3.9,4.77,4.59,5.31,4.45,4.44,3.97,4.24,3.75,3.84,5.66,4.15,4.35,5.62,5.09,5.65,4.57,4.97,3.53,3.64,3.87,5.49,5.33,4.66,5.85,3.69,6.43,4.73,4.67,4.76,4.7,5.05,8.12,4.53,9.82,3.97,5.24,11.78,5.09,4.94,4.33,5,6.49,7.02,5.1,5.98,4.56,4.06,5.76,4.51,6.56,5.41,4.35,3.76,3.91,3.77,4.69,3.97,4.83,4.78,4.75,4.39,3.46,8.21,3.85,3.48,9.49,3.91,5.19,4.52,4.2,4.7,4.95)),.Names=c(""ID"",""IV1"",""IV2"",""DV"",""COVAR""),class=""data.frame"",row.names=c(NA,675L))
</code></pre>

<p>Model fit with the covariate:</p>

<pre><code>require(lmerTest)
require(car)

m1&lt;-lmer(DV ~ COVAR*IV1*IV2 + (1|IV1:ID), data=df)
</code></pre>

<p>Then I wanted to test whether COVAR is significant and whether an interaction between COVAR and the IVs exists. I used the <code>anova()</code> function provided by <code>lmerTest</code>. Here the covariate is significant as well as the interaction between COVAR and IV2:</p>

<pre><code>anova(m1)
Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
          Sum Sq Mean Sq NumDF  DenDF F.value    Pr(&gt;F)    
COVAR         4589.8  4589.8     1 567.74  56.506 2.197e-13
IV1            610.4   152.6     4 548.49   1.879  0.112731    
IV2           1223.2   611.6     2 562.64   7.529  0.000593
COVAR:IV1      208.7    52.2     4 560.52   0.642  0.632594    
COVAR:IV2      703.4   351.7     2 563.35   4.330  0.013613  
IV1:IV2        776.8    97.1     8 561.48   1.195  0.299305    
COVAR:IV1:IV2  680.6    85.1     8 561.47   1.047  0.399018
</code></pre>

<p>However when I use <code>Anova(m1, type=3)</code>, just to check and compare different outputs, it comes out like this:</p>

<pre><code>Analysis of Deviance Table (Type III Wald chisquare tests)

Response: DV
           Chisq Df Pr(&gt;Chisq)   
(Intercept)   7.7308  1   0.005429
COVAR         1.9850  1   0.158866   
IV1           6.5038  4   0.164549   
IV2           2.0069  2   0.366610   
COVAR:IV1     0.3739  4   0.984554   
COVAR:IV2     1.6527  2   0.437654   
IV1:IV2       9.5635  8   0.297007   
COVAR:IV1:IV2 8.3786  8   0.397383 
</code></pre>

<p>When I run the <code>Anova(m1)</code> it looks again closer to what <code>anova(m1)</code> produced, however, IV1 is now ""highly"" significant (which is what I would have expected a priori given the nature of IV1), plus there is also an interaction between IV1 and IV2. That being said and also given the discussions regarding type 2 and type 3 SS, I would opt for going ahead with type 2 SS:</p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: DV
             Chisq Df Pr(&gt;Chisq)    
COVAR          97.1301  1  &lt; 2.2e-16 
IV1           104.2557  4  &lt; 2.2e-16 
IV2            20.0292  2  4.474e-05 
COVAR:IV1       0.2244  4  0.9941594    
COVAR:IV2       9.1881  2  0.0101119   
IV1:IV2        28.5092  8  0.0003865 
COVAR:IV1:IV2   8.3786  8  0.3973834 
</code></pre>

<p><strong>Question 1:</strong> What is the explanation for these substantial variations between these outputs (especially <code>anova(m1)</code> vs. <code>Anova(m1, type=3)</code> which are both <code>type=3</code> calculations)?</p>

<p>Given the fact that COVAR interacts with IV2 and also that <code>m2 &lt;- lmer(COVAR ~ IV1*IV2 + (1|IV1:ID), data=df); Anova(m2)</code> turns out to be significant (again for IV2), I cannot sell this anlysis as ANCOVA since both additional assumptions for ANCOVA are violated. </p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: COVAR
      Chisq Df Pr(&gt;Chisq)    
IV1       3.093  4     0.5424    
IV2     160.317  2  &lt; 2.2e-16
IV1:IV2  34.734  8  2.989e-05
</code></pre>

<p>However, COVAR seems to play an important role and therefore should be kept in the model nonetheless.</p>

<p><strong>Question 2:</strong> Is this reasonable? And if yes, how do I go on and interpret the output of such a model, especially the interaction between COVAR and IV2?</p>

<p>What I would do is plot the interactions for IV1:IV2 and for COVAR:IV2 first:</p>

<pre><code>with(na.omit(df), interaction.plot(IV1,IV2,DV))
require(ggplot2)
ggplot(df,aes(x=COVAR, y=DV))+geom_point(aes(colour=IV2))+
geom_smooth(aes(colour=IV2), method=lm)
</code></pre>

<p>and then start discussing.</p>

<pre><code>&gt; sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
[1] ggplot2_1.0.1   car_2.1-0       lmerTest_2.0-29
[4] lme4_1.1-10     Matrix_1.2-2
</code></pre>
"
"0.0981930408849676","0.100143163995996","182696","<p>As you may know, R has the problem that it uses the wrong MSE in calculating p values in 2-way ANOVA for split plot experiments. </p>

<p>The whole plot factor is Tree.Name, subplot factor is In.Out, the replicates are Tree.ID, and P is the dependent variable as shown below:</p>

<pre><code>structure(list(Tree.Name = structure(c(5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L), .Label = c(""Akun"", ""Jaror"", ""Ku'ch"", ""Petz-kin"", 
""Puuna"", ""Yax bache""), class = ""factor""), Tree.ID = structure(c(10L, 
10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L), .Label = c(""AK-1"", 
""AK-2"", ""AK-3"", ""AK-4"", ""AK-5"", ""AK-6"", ""AK-7"", ""AK-8"", ""AK-9"", 
""C-1"", ""C-2"", ""C-3"", ""C-4"", ""C-5"", ""C-6"", ""C-6 "", ""C-7"", ""C-8"", 
""C-9"", ""Ce-1"", ""Ce-2"", ""Ce-3"", ""Ce-4"", ""Ce-5"", ""Ce-6"", ""Ce-7"", 
""Ce-8"", ""Ce-9"", ""J-1"", ""J-2"", ""J-3"", ""J-4"", ""J-5"", ""PK-1"", ""PK-3"", 
""PK-4"", ""PK-5"", ""PK-6"", ""PK-7"", ""PK-8"", ""PK-9"", ""Y-1"", ""Y-2"", 
""Y-3"", ""Y-4"", ""Y-5"", ""Y-6"", ""Y-7"", ""Y-8"", ""Y-9""), class = ""factor""), 
    Sample = c(1L, 2L, 3L, 5L, 6L, 7L, 1L, 2L, 3L, 4L), In.Out = structure(c(2L, 
    1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L), .Label = c(""In"", ""Out""
    ), class = ""factor""), P = c(9.18, 10.38, 6.77, 7.37, 7.97, 
    7.37, 7.07, 6.77, 6.47, 9.78), OM = c(14.71, 15.55, 11.19, 
    15.89, 14.21, 18.91, 12.19, 17.9, 9.84, 30.65), N = c(0.73, 
    0.79, 0.57, 0.8, 0.72, 0.96, 0.61, 0.91, 0.5, 1.55)), .Names = c(""Tree.Name"", 
""Tree.ID"", ""Sample"", ""In.Out"", ""P"", ""OM"", ""N""), row.names = c(NA, 
10L), class = ""data.frame"")
</code></pre>

<p>I found this solution while digging online: </p>

<pre><code>split.plot&lt;-aov(P~Tree.Name*In.Out+
                  Error(Tree.ID),
                data=tree.data)
summary(split.plot)
</code></pre>

<p>However, a friend recommended that I try using <code>lmer()</code></p>

<pre><code>library(""lme4"", lib.loc=""~/R/win-library/3.2"")
x &lt;- lmer(
  P~Tree.Name*In.Out+(1|Tree.ID),
  data=tree.data)
anova(x)
</code></pre>

<p>However the (F) values in the results of the analysis using <code>aov()</code></p>

<pre><code>##                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Tree.Name         5 2273.5   454.7  17.935 2.14e-09 ***
## In.Out            1    3.5     3.5   0.137    0.713    
## Tree.Name:In.Out  2  192.0    96.0   3.787    0.031 *  
## Residuals        41 1039.5    25.4   
</code></pre>

<p>are quite different than those from <code>lmer()</code></p>

<pre><code>## Analysis of Variance Table
##                  Df Sum Sq Mean Sq F value
## Tree.Name         5 322.78  64.556 15.7145
## In.Out            1  25.87  25.870  6.2974
## Tree.Name:In.Out  5  26.04   5.208  1.2677
</code></pre>

<p>My question is twofold:</p>

<p>Why aren't I getting the appropriate F values from lmer? and is one method preferred over the other for any particular reason?</p>
"
"0.149378879319591","0.169272842266383","182988","<p>I've run a fully within-subjects repeated-measures ANOVA using the <code>aov()</code> function. My dependent variable is not normally distributed, so I'm very interested in running assumption tests on my analysis. It seems that just calling <code>plot()</code> on the output doesn't work for repeated-measures, so I've manually taken the residuals and the fitted values for a model of interest, and have plotted them against each other. I'm assuming that this is how I would plot to test for the assumption of Homoskedasticity.</p>

<p>The plot comes out with 2 vertical bands (please see the image below). It turns out the fitted values are all centred around 2 values (although according to <code>==</code> they are not exactly equal), where one is the negative of the other.</p>

<p>I have 2 questions:</p>

<p>1) Is this the correct way to manually test the assumption homoskedasticity? If not, how would I go about it from repeated-measures designs (since just calling <code>plot()</code> doesn't work)?</p>

<p>2) If it is correct, what is this plot telling me? Why are the fitted values so clustered? What can I conclude from it?</p>

<p>Thanks heaps for any input here. Also, if you know of better ways to check (preferably plot) for assumptions in rm-ANOVAs, that would be useful information as well.</p>

<p>I've included some mock data here to replicate the scenario:</p>

<pre><code>#Create mock data (there's probably a more efficient way to do this.. would also be nice to know! :) )
p &lt;- sort(rep(1:20,8))
y &lt;- rep(rep(1:2,4),20)
z &lt;- rep(rep(c(1,1,2,2),2),20)
w &lt;- rep(c(1,1,1,1,2,2,2,2),20)
x &lt;- rnorm(160,10,2)

d &lt;- data.frame(x,p=factor(p),y=factor(y),z=factor(z),w=factor(w))

#Run repeated-measures ANOVA
ex.aov &lt;- aov(x ~ y*z*w + Error(p/(y*z*w)), d)

#Try to plot full object (doesn't work)
plot(ex.aov)

#Try to plot section of object (doesn't work)
plot(ex.aov[[""p:y:z""]])

#Plot residuals against fitted (custom ""skedasticity"" plot - works)
plot(residuals(ex.aov[[""p:y:z""]])~fitted(ex.aov[[""p:y:z""]]))
</code></pre>

<p><a href=""http://i.stack.imgur.com/pBexJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pBexJ.png"" alt=""enter image description here""></a></p>

<p><strong>Begin Edit</strong></p>

<p>In light of the information provided by @Stefan , I've added some additional details below, using the improved data structure he proposed:</p>

<pre><code># Set seed to make it reproducible
set.seed(12)

#New variable names and generation
subj &lt;- sort(factor(rep(1:20,8)))
x1 &lt;- rep(c('A','B'),80)
x2 &lt;- rep(c('A','B'),20,each=2)
x3 &lt;- rep(c('A','B'),10, each=4)
outcome &lt;- rnorm(80,10,2)

d3 &lt;- data.frame(outcome,subj,x1,x2,x3)

#Repeated measures ANOVA
ex.aov &lt;- aov(outcome ~ x1*x2*x3 + Error(subj/(x1*x2*x3)), d3)

#proj function
ex.aov.proj &lt;- proj(ex.aov)

# Check for normality by using last error stratum
qqnorm(ex.aov.proj[[9]][, ""Residuals""])
# Check for heteroscedasticity by using last error stratum
plot(ex.aov.proj[[9]][, ""Residuals""])
</code></pre>

<p>The resulting plots are below:</p>

<p><a href=""http://i.stack.imgur.com/wFiYy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wFiYy.png"" alt=""Repeated measures normality check?""></a></p>

<p><a href=""http://i.stack.imgur.com/siHVi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/siHVi.png"" alt=""Repeated measures homoskedasticity check?""></a></p>

<p>Can anyone interpret the images above (especially the last one)? It looks like there is clustering and pattern structure. Can it be used to infer the presence of heteroskedasticity?</p>
"
"0.111340442853781","0.11355167461567","183248","<p>During an experiment I presented participants with some sound stimuli and I asked them to modify two parameters of the sound (Centroid and Sound_Level_Peak) to reach a given experimental goal. The sounds represented hand interactions with different kinds of materials by means of different kinds of objects, and where created thanks to a parametric synthesizer.</p>

<p>The initial sounds (preset sounds) had a value for those two parameters. What I am interested in is whether participants' modifications of the two parameters of the sound stimuli resulted in values actually different from the initial values of the parameters of the preset sounds.</p>

<p>In more details, the experiment involved 19 subjects, who where subjected to 12 kinds of stimuli, each based on 3 presets of centroid and peak level (so in total there where 36 stimuli). Each stimulus was repeated twice for a total of 72 trials per subject. Accordingly, the number of presets was 72.</p>

<p>To give an idea, some rows of my data set are the following:</p>

<pre><code>&gt; head(scrd) 
Stimulus_Type   Centroid        Sound_Level_Peak    Preset
Stimulus_A      1960.2          -20.963             no
Stimulus_A      5317.2          -42.741             no
.....
Stimulus_B      11256.0          -16.480            no
Stimulus_B      9560.3          -19.682             no
.....
.....
Stimulus_A      1900.2          -18.63             yes
Stimulus_A      5617.6          -44.41             yes
Stimulus_B      12056.0          -17.80            yes
Stimulus_B      8960.5          -21.82             yes
</code></pre>

<p>This is the analysis I performed:</p>

<pre><code>&gt; fit &lt;- manova(cbind(Centroid,Sound_Level_Peak)~ Stimulus_Type*Preset, data=scrd)
&gt; summary(fit, test=""Pillai"")
                       Df  Pillai approx F num Df den Df  Pr(&gt;F)    
Stimulus_Type          11 0.91888  106.629     22   2760 &lt; 2e-16 ***
Preset                  1 0.00343    2.371      2   1379 0.09378 .  
Stimulus_Type:Preset   11 0.01155    0.729     22   2760 0.81348    
Residuals            1380                                           
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
&gt; 
</code></pre>

<p>These results should then say that for each stimulus type there is no difference between the patterns of the two parameters in the preset and modified conditions.</p>

<p>Can anyone please tell me if this analysis is correct or suggest how to perform in R an alternative analysis?</p>
"
"0.171852555767074","0.220704784087699","183441","<ul>
<li>Note: This question is heavy on R programming, but it was recommended that I post it here after I posted an almost identical question in StackOverflow</li>
</ul>

<h2>Main Question</h2>

<p>I'm looking for help correctly setting up a one-way within subjects MANOVA in R for a data-set that has no between-subject factors.</p>

<h2>Detailed Question</h2>

<p>I'm trying to figure out how to setup a one-way within-subjects MANOVA in R, where my design has a single within-subjects IV (with 2 levels), and 3 DVs. It has come down to a question of whether or not this is best done with the standard <code>manova()</code> function, or using <code>Anova()</code> from the <code>car</code> package. Using a toy example (replicated below), I have done both but get different results, and these differences seem to be associated with how each function is figuring out the appropriate degrees of freedom for the ultimate F-test.</p>

<h2>Example</h2>

<p>To demonstrate the problem, I'll use a subset of the OBrienKaiser data set, and I'll assume that each of the levels of the <code>Hours</code> within-subjects factor instead represents the measurement of a different dependent variable. I'll then take the <code>pre</code> and <code>post</code> conditions to be the two levels of my single within-subjects independent variable. To keep things concise, I'll only look at the first three levels from <code>Hours</code>. </p>

<p>So what I have for my data set is 16 subjects measured in two different conditions (<code>pre</code> and <code>post</code>) on 3 different dependent variables (<code>1</code>,<code>2</code>, and <code>3</code>).</p>

<pre><code>data &lt;- subset(OBrienKaiser,select=c(pre.1,pre.2,pre.3,post.1,post.2,post.3))
</code></pre>

<p><strong>car::Anova( )</strong></p>

<p>To perform this analysis with <code>Anova()</code>, I have primarily relied on a combination of the documentation provided with <code>car</code>, and the slightly more detailed examples found here...</p>

<p><a href=""http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Multivariate-Linear-Models.pdf</a></p>

<p>First, define the within-subjects factor and create the data structure for the linear model.</p>

<pre><code>condition &lt;- as.factor(rep(c('pre','post'),each=3))
idata &lt;- data.frame(condition)
data.model &lt;- with(data,cbind(pre.1,pre.2,pre.3,post.1,post.2,post.3))
</code></pre>

<p>Next, define the multivariate-linear model.</p>

<pre><code>mod.mlm &lt;- lm(data.model ~ 1)
</code></pre>

<p>Finally, perform the MANOVA using a call to <code>Anova()</code> and print the results</p>

<pre><code>mav.car &lt;- Anova(mod.mlm,idata=idata,idesign=~condition,type=3)
print(mav.car)
</code></pre>

<p>The output is...</p>

<pre><code>Type III Repeated Measures MANOVA Tests: Pillai test statistic
            Df test stat approx F num Df den Df   Pr(&gt;F)    
(Intercept)  1   0.91438  160.189      1     15 2.08e-09 ***
condition    1   0.37062    8.833      1     15 0.009498 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My issue here is that I don't think the DF have been properly calculated. I remember learning something about MANOVAs losing DF for each DV included in the analysis, but the DF here seem to be typical for a univariate-ANOVA of the same design (i.e., if I didn't have multiple DVs). However, in trying to answer this question myself, I came across a pdf of a user manual for STATA (<a href=""http://www.stata.com/manuals13/mvmanova.pdf"" rel=""nofollow"">http://www.stata.com/manuals13/mvmanova.pdf</a>). It presents a problem of measuring 4 DVs for each of 8 trees from 6 different root stocks (i.e., N=48, one between-factor with 6 levels, &amp; DVs=4). They state that for the one-way MANOVA, the DF...</p>

<blockquote>
  <p>are just as they would be for an ANOVA. Because there are
  six rootstocks, we have 5 degrees of freedom for the hypothesis. There > are 42 residual degrees of
  freedom and 47 total degrees of freedom.</p>
</blockquote>

<p><strong>stats::manova( )</strong></p>

<p>This method actually comes from the answer to this posted question...</p>

<p><a href=""http://stats.stackexchange.com/questions/141468/what-is-the-best-approach-for-this-set-up-rm-anova-manova-mixed-models"">What is the best approach for this set-up: RM ANOVA / MANOVA / Mixed-Models?</a></p>

<p>...given by @Chris Novak. For demonstration, I'll use the same dataset, but cast it to a long-format to accommodate the requirements of the <code>stats::manova()</code> function and rename it <code>data2</code>. I'll omit the actual casting, but the result looks like this...</p>

<pre><code>&gt;some(data2,4)
   Subject Condition V1 V2 V3
3        3       pre  5  6  5
16      16       pre  4  5  7
23       7      post  7  7  8
25       9      post  4  5  6
</code></pre>

<p>Setting up the MANOVA using <code>stats::manova()</code> is very similar to setting up a typical repeated-measures anova with that function.</p>

<pre><code>mav.stat &lt;- with(data2,manova(cbind(V1,V2,V3) ~ Condition + Error(Subject/Condition)))
</code></pre>

<p>The output looks like this:</p>

<pre><code>Error: Subject
           Df Pillai approx F num Df den Df Pr(&gt;F)
Residuals 15                                     

Error: Subject:Condition
          Df  Pillai approx F num Df den Df  Pr(&gt;F)  
Condition  1 0.40717   2.9762      3     13 0.07066 .
Residuals 15                                         
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The P-Values are clearly different, as are the numDf and denDf used in the calculations. While I'm inclined to think that this is the correct way of performing the within-subjects MANOVA, I'd like to know what I'm doing wrong in <code>car::Anova()</code> and how to correctly perform the MANOVA with <code>car::Anova()</code>. I'd also like to understand how the DF get treated/calculated in the computation of a within-subjects MANOVA. Thanks so much for the guidance.</p>
"
"0.0909090909090909","0.092714554082312","184248","<p>For my analysis I am currently running a 1-way within-subjects bootstrapped ANOVA with trimmed means (<code>rmanovab</code> from Rand Wilcox's <code>WRS</code> package). This is a very nice robust method, but it does not report effect size.</p>

<p>I really need to get an estimate of effect size like a generalized eta squared or Cohen's D. Since I know that <code>ezANOVA</code> reports a generalized eta squared effect size I tried using it, but it lacks trimming and the results differ quite a lot from <code>rmanovab</code> due to a single outlier. I even tried naively to manually trim the values but that also does not seem to be an option because that would mess up the 'within subject' calculations that <code>ezANOVA</code> does: I got this error:</p>

<pre><code>Warning: You have removed one or more Ss from the analysis. Refactoring ""Participant"" for ANOVA.
Error in ezANOVA_main(data = data, dv = dv, wid = wid, within = within,  : 
One or more cells is missing data. Try using ezDesign() to check your data. 
</code></pre>

<p>Does anyone have an idea for how to compute a generalized eta-squared or Cohen's d using trimmed means (and optionally using a bootstrapped method like <code>rmanovab</code>)? Any pointers are much appreciated.</p>
"
"0.157677705948457","0.160809200153064","185391","<p>I am using the Titanic dataset to understand glm model. These are the two models,</p>

<pre><code>titanic.glm       &lt;- glm(survived ~ pclass,         family=binomial, data=titanic.train)
titanic.glm.title &lt;- glm(survived ~ pclass + title, family=binomial, data=titanic.train)
</code></pre>

<p>I have two summaries. Since it has categorical variables, I am not able to interpret it properly and compare it. Can anybody help me in comparing both the models and find out which one is better? following is the summaries of both the models,</p>

<pre><code>&gt; summary(titanic.glm)

Call:
glm(formula = survived ~ pclass, family = binomial, data = titanic.train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3652  -0.7779  -0.7779   1.0006   1.6388  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.4313     0.1272   3.391 0.000696 ***
pclass2      -0.7086     0.1852  -3.826 0.000130 ***
pclass3      -1.4715     0.1593  -9.237  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1393.6  on 1046  degrees of freedom
Residual deviance: 1301.5  on 1044  degrees of freedom
AIC: 1307.5

Number of Fisher Scoring iterations: 4

&gt; summary(titanic.glm.title)

Call:
glm(formula = survived ~ pclass + title, family = binomial, data = titanic.train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2066  -0.6462  -0.4263   0.6521   2.2106  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    1.5336     0.3356   4.570 4.88e-06 ***
pclass2       -0.9028     0.2259  -3.997 6.41e-05 ***
pclass3       -1.7950     0.2037  -8.814  &lt; 2e-16 ***
titleMiss      0.4701     0.3207   1.466 0.142706    
titleMr.      -2.0911     0.3131  -6.679 2.40e-11 ***
titleMrs.      0.8092     0.3507   2.308 0.021011 *  
titleNothing  -1.7024     0.5157  -3.301 0.000964 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1393.58  on 1046  degrees of freedom
Residual deviance:  990.66  on 1040  degrees of freedom
AIC: 1004.7

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I tried the following,</p>

<pre><code>anova(titanic.glm,titanic.glm.title, test = ""Chisq"")

Analysis of Deviance Table

Model 1: survived ~ pclass
Model 2: survived ~ pclass + title
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1      1044    1301.47                          
2      1040     990.66  4   310.81 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I am not able to compare both the models. I read somewhere AIC is one of the parameter to compare it. Can anybody help in comparing both the models and decide which one is better? Also how can we find whether a particular model is stable or not?</p>
"
"0.0371134809512603","0.0378505582052232","185501","<p>I have measured Copper content in fishing nets. I have 2 independent variables - <code>treatment of the net</code> with 4 levels and <code>type of net</code> with 2 levels. I'm using R to do the ANOVA.</p>

<p>I care about the interaction, so I should do:</p>

<pre><code>mCu = aov(Cu ~ Type * Treatment, data = Ultrasonic)
</code></pre>

<p>But why do I get different values for the difference in type if I do:</p>

<pre><code>mCu = aov(Cu ~ Type + Treatment, data = Ultrasonic)
</code></pre>

<p>I thought using the asterisk instead of the plus sign would just add the interactions as well, not change the analysis of the variables by themselves. (Although, the values are not very far off.)</p>
"
"0.0895210843486056","0.125536099672233","186836","<p>Based on a <a href=""http://stats.stackexchange.com/questions/182988/plotting-to-check-homoskedasticity-assumption-for-repeated-measures-anova-in-r"">previous question</a> that I asked about checking assumptions of repeated-measures ANOVAs in R (which turns out to be not so trivial), I'm wondering about the relationship between a repeated-measures ANOVA and a linear mixed model on the same data.</p>

<p>In an excellent exploration of my data, it was suggested to me that I switch to linear mixed models for my full analysis, which I have already done. However, since for reasons of completeness I still need to run a respeated-measures ANOVA, I'm specifically wondering <strong>whether assumption checks on a linear mixed model can be used to infer assumption violations of assuptions for a repeated-measures ANOVA using the same data</strong>.</p>

<p>For example, using the example data below:</p>

<pre><code>set.seed(12)

#Generate variables and data frame
subj &lt;- sort(factor(rep(1:20,8)))
x1 &lt;- rep(c('A','B'),80)
x2 &lt;- rep(c('A','B'),20,each=2)
x3 &lt;- rep(c('A','B'),10, each=4)
outcome &lt;- rnorm(80,10,2)

d3 &lt;- data.frame(outcome,subj,x1,x2,x3)

#Repeated measures ANOVA
m.aov &lt;- aov(outcome ~ x1*x2*x3 + Error(subj/(x1*x2*x3)), d3)

#Linear mixed model assumption checks
require(lme4)
#`subj` as random term to account for the repeated measurements on subject.
m.lmer&lt;-lmer(outcome ~ x1*x2*x3 + (1|subj), data = d3)

# Check for heteroscedasticity
plot(m.lmer)
# or
boxplot(residuals(m.lmer) ~ d3$x1 + d3$x2 + d3$x3)
# Check for normality
qqnorm(residuals(m.lmer))
</code></pre>

<p><strong>Can the assumption plots on m.lmer be used to test assumption violations for m.aov?</strong> For example, if m.lmer displays heteroskedasticity, would that suggest that m.aov is afflicted with heteroskedasticity as well?</p>

<p>Thanks for any insight!</p>
"
"0.111340442853781","0.11355167461567","187426","<p>in my experiment I have 3 Pretests: PreA, PreB and PreC done at the same day; also I have 3 PostTests: PostA, PostB and PostC done 3 weeks later.
During the 3 weeks between pre tests and the posts tests there was a training  followed by 40 participants. The control group was 33 participants. Group: Trained (n=40) or Control (n=33).</p>

<p>To analyse data, I divided the sample in three levels (low, middle, upper) according to the performance in preA and PreB, I called it the ABLevel. </p>

<p>These are the results for the Percent Change (improvement) in test C (analysed in R): <a href=""http://i.stack.imgur.com/5tRGK.png"" rel=""nofollow"">anova and t.test only for low level</a></p>

<p><strong>There is a big difference between Control and Trained in the Low level. If I run a t.test only for low level, it is significant the different between control and trained groups. But it is not correct to run a t.test only of a part, isn't?</strong></p>

<p>If I run a t.test of the whole sample, it is not significant the difference between gropus *control vs trained):
<a href=""http://i.stack.imgur.com/Andr1.png"" rel=""nofollow"">t.test and also a lm</a></p>

<p>Please, is there another type of analysis more appropriate to show better the difference between control and trained groups for the low level profile?
Thank you very much!</p>
"
"0.0524863881081478","0.0535287727572189","187509","<p>With a small book-exercise with four metric variables on 10 cases (one dependent/outcome, three independent/predictor) I ran <em>linear regression</em> in <code>SPSS</code> and <code>R</code>, and <em>ANOVA</em> (in <code>SPSS</code> declaring the predictors as ""covariates"").<br>
I found the output of the <em>SSQ</em> (Sum-of-Squares) different - and obviously from this also the F-test statistic and the p-values. Except from the last predictor the displayed values are different (the predictors in <code>R</code>may be reordered and the analysis be rerun to find all <code>SPSS</code>- coefficients).                   </p>

<p>By reengineering the computations in matrix-formulae I could reproduce the SPSS-values as well as the R-values and found, that <code>SPSS</code> uses the (partial) SSQ based on the logic of the ""usefulness""-coefficients for each predictor (which is sort of semipartial coefficient), while <code>R</code> simply uses the (hierarchically) partial SSQ. <em>(Unfortunately I'm not sure how to express that two approaches correctly so this toy-characterizing might be improved)</em> .             </p>

<p><strong><em>Q:</em></strong> Has that property of different focuses/philosophies been discussed anywhere? Is there some advantage of one over the other?   </p>

<p><hr>
Data: (taken from M. Backhaus et al., multivariate Verfahren)             </p>

<pre><code>predictors                   outcome-item
---------------------------+-------------
Preis   VerkFoer  Vertreter  Absatzmenge
12.50      2000      109      2298
10.00       550      107      1814
 9.95      1000       99      1647
11.50       800       70      1496
12.00         0       81       969
10.00      1500      102      1918
 8.00       800      110      1810
 9.00      1200       92      1896
 9.50      1100       87      1715
12.50      1300       79      1699
</code></pre>

<p>The comparision of the output:
<a href=""http://i.stack.imgur.com/zNJDB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zNJDB.png"" alt=""bild""></a></p>
"
"0.0642824346533225","0.0655590899062897","187745","<p>The code is like the following:</p>

<pre><code>&gt; d &lt;- data.frame(a=c(1,1,1,1,2,2,2,2), b=c(1,1,2,2,1,1,2,2),v=1:8)
&gt; anova(lm(v~a*b, data=d))
Analysis of Variance Table

Response: v
          Df Sum Sq Mean Sq F value   Pr(&gt;F)   
a          1     32    32.0      64 0.001324 **
b          1      8     8.0      16 0.016130 * 
a:b        1      0     0.0       0 1.000000   
Residuals  4      2     0.5                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; anova(lm(v~a, data=d))
Analysis of Variance Table

Response: v
          Df Sum Sq Mean Sq F value   Pr(&gt;F)   
a          1     32  32.000    19.2 0.004659 **
Residuals  6     10   1.667                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My question is why ""a"" has different value of F and Pr between lm(v~a<em>b) and lm(v~a)?  As far as I think anova(v~a</em>b) will test avona(v~a), anova(v~b) and anova for a and b together. </p>
"
"0.0663906130309292","0.0846364211331916","188230","<p>I am performing a repeated measures ANOVA in R, where the experimental design is composed of a between-subjects factor (<code>Prodotto</code>, three levels), the within-subjects factor is time (<code>Tempo</code>, four levels&mdahs;an ordered factor) and each <code>Product * Time</code> is repeated on eleven patients (<code>idPaziente</code>).</p>

<p>The <code>aov()</code> syntax would be:  </p>

<pre><code>myAov&lt;-aov(valore ~ Tempo*Prodotto + Error(idPaziente/Tempo),data=mydata)
</code></pre>

<p>whilst the <code>lme()</code> (from the <code>nlme</code> package) syntax would be </p>

<pre><code>myModel &lt;- lme(valore ~  Tempo*Prodotto,data=mydata, random= ~ 1 | idPaziente/Tempo)
</code></pre>

<p>In Finch (2014) <a href=""http://rads.stackoverflow.com/amzn/click/1466515856"" rel=""nofollow""><em>Multilevel Modeling with R</em></a>,  I find a specification of a longitudinal model that would be written as: </p>

<pre><code>myModel &lt;- lme(valore ~  Tempo*Prodotto,data=mydata, random= ~ 1 | idPaziente)
</code></pre>

<p>I guess the equivalent <code>aov</code> version would have <code>Error(idPaziente)</code> instead of <code>Error(idPaziente/Tempo)</code>.  </p>

<p>I would like to ask: </p>

<ol>
<li><p>What is the correct specification for my repeated measures model, both for <code>aov()</code> and <code>lme()</code>? </p></li>
<li><p>Why are <code>(1 | idPaziente/Tempo)</code> and <code>Error(idPaziente/Tempo)</code> more appropriate than <code>(1 | idPaziente)</code> and <code>Error(idPaziente)</code>? </p></li>
<li><p>How can I perform post hoc analysis taking on the Prodotto term? </p></li>
</ol>
"
"0.0989692825366941","0.100934821880595","193752","<p>I have one response variable $Y$ and one predictor $X$. I am trying to fit a polynomial regression model and try to compare different model with different highest power term, the output of ANOVA in R is the following</p>

<pre><code>Analysis of Variance Table
Model 1: Y ~ X
Model 2: Y ~ X + I(X^2)
Model 3: Y ~ X + I(X^2) + I(X^3)
Model 4: Y ~ poly(X, 5)

  Res.Df    RSS   Df  Sum of Sq       F    Pr(&gt;F)    
    504    19472                                   
    503    15347  1    4125.1     151.693 &lt; 2.2e-16 ***
    502    14616  1     731.8     26.909 3.104e-07 ***
    500    13597  2    1018.4     18.726 1.438e-08 ***

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I understand how to derive all these numbers in the table, but there are some contradiction. Here is my question: in this table, the last model is the biggest model in the sense that it contains all the predictors in all previous models, the ""Sum of Sq"" for Model 4 is 1018.4 = 14616-13597, i.e., the difference of the sum of residuals between model 3 and model 4 and the F statistic for Model 4, which is 18.726 is obtained by $\frac{RSS_3-RSS_4}{502-500}\div\frac{13597}{500}$, i.e., the difference in RSS between model 3 and model 4 divide by the difference of degree of freedom and then divide by the MSE of model 4. This makes a lot of sense. However, when I compute the F statistic for model 3, I am so confused. The ""Sum of Sq"" for model 3 is obtained via $731.8=15347-14616$, i.e,. the difference in RSS of model 2 and model 3. But the F statistic for model 3 is obtained via $\frac{RSS_2-RSS_3}{503-502}\div\frac{13597}{500}$, i.e., the difference in RSS of model 2 and model 3 divide by their difference in degree of freedom, BUTTTT then divide by the MSE of model 4. In my mind, it should finally divide the MSE of model 3 rather than model 4, since we are comparing the difference between model 2 and model 3. </p>
"
"0.0742269619025206","0.0757011164104465","193868","<p>I have a measurement that was taken 10 times on a sample.  The measurement has roughly 11K points. If you're curious, I'm looking at mass spectrum data.  I want to determine the repeatability of the spectra.</p>

<p>I am looking at either using ANOVA to do this, or using a PCA to determine how close the PC coordinates (PC1, PC2) are to each other.  Would either of these options give a good measure of repeatability for this application or is there another method that would be better?</p>

<p>I've looked at this <a href=""http://stats.stackexchange.com/questions/30247/how-to-assess-repeatability-of-multivariate-and-method-specific-outcomes"">post</a> and I've eliminated co-inertia analysis since it is used to compare only two multivariate data sets.  Not sure how to use option 2 in the accepted answer and option 3 seems like using PCA.</p>
"
"0.0524863881081478","0.0535287727572189","194909","<p>Stats newb here, I have to determine if two time series are really different instead of being part of the same population with noise in the samples.</p>

<p>The data is a comparison between two algorithms ctr by day, a control one (a) and an experimental (b)</p>

<p><a href=""http://i.stack.imgur.com/H2SXn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H2SXn.png"" alt=""Plot of algorithms comparison""></a></p>

<pre><code>a      b
1 3.6162 3.6808
2 3.8967 4.0155
3 4.0669 4.2945
4 4.3680 4.4321
5 4.0558 4.2071
6 3.9234 3.9131
7 3.7467 3.9533
</code></pre>

<p>We can see the mean is bigger in b (4.070914 vs 3.953386)
But are they statistically significant?
To see that im doing ANOVA to get the p-value and compare to the alpha of 0.05 as far as i know, if it is smaller then the null hypothesis H0 of being equals is false.</p>

<p>The problem is when i do the oneway.test i get a p-value really big, am i doing something wrong?</p>

<pre><code>data
        x name
1  3.6162    a
2  3.6808    b
3  3.7467    a
4  3.8967    a
5  3.9131    b
6  3.9234    a
7  3.9533    b
8  4.0155    b
9  4.0558    a
10 4.0669    a
11 4.2071    b
12 4.2945    b
13 4.3680    a
14 4.4321    b

 oneway.test(x~name, data = data)

    One-way analysis of means (not assuming equal variances)

data:  x and name
F = 0.77477, num df = 1.00, denom df = 11.97, p-value = 0.3961 
</code></pre>

<p>Thanks a lot!</p>
"
"NaN","NaN","198035","<p>I took few courses in stats so go easy on me. I used R on a dataset of the influence of pests on sugar cane production (<a href=""http://www.stat.ufl.edu/~winner/datasets.html"" rel=""nofollow"">http://www.stat.ufl.edu/~winner/datasets.html</a>).</p>

<p>What I found with the regular ANOVA test is a high F value (40.79) and low probability value (2.27*10^-10) - which are both good.</p>

<p>But TukeyHSD analysis shows that much higher p-values when comparing variables against each other(excluding the control).</p>

<p>I think I should reject the means.Should I? What conclusions can I draw from this? What are the next steps for the study? Thanks, Matthew Mano</p>
"
"0.18924236358783","0.193000734888813","198124","<p>I am fitting some generalized additive models using the <code>mgcv</code> package in R, and I am wanting to test between two
models; whether I can remove a term or not. I am, however, getting conflicting results (as far as I can tell).</p>

<p>A model, <code>m1</code>, with a smooth term for <code>x</code> added, appears to give a better fit in terms of $R^{2}_{adj}$, AIC,
deviance explained, and when comparing the models using an F-test. However, the significance of the 
smooth term is not significant (nor is it when I added to the model as a linear covariate, instead of a spline). </p>

<p>Is my interpretation of the smooth terms tests in correct? As much as I could understand the help page, was that the tests are approximate, but there is quite a large difference here.</p>

<p>The model outputs</p>

<pre><code>m1 &lt;- gam(out ~ s(x) + s(y) + s(z), data=dat)
&gt; summary(m1)
# 
# Family: gaussian 
# Link function: identity 
# 
# Formula:
# out ~ s(x) + s(y) + s(z)
# 
# Parametric coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)
# (Intercept) -7.502e-16  1.209e-01       0        1
# 
# Approximate significance of smooth terms:
#        edf Ref.df     F  p-value    
# s(x) 4.005  4.716 1.810    0.136    
# s(y) 8.799  8.951 4.032 4.01e-05 ***
# s(z) 7.612  8.526 5.649 4.83e-07 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# R-sq.(adj) =  0.213   Deviance explained = 24.8%
# GCV = 6.9741  Scale est. = 6.6459    n = 455

&gt; AIC(m1)
#[1] 2175.898

&gt; m2 &lt;- gam(out ~ s(y) + s(z), data=dat)
&gt; summary(m2)
# 
# Family: gaussian 
# Link function: identity 
# 
# Formula:
# out ~ s(y) + s(z)
# 
# Parametric coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|)
# (Intercept) 1.705e-15  1.228e-01       0        1
# 
# Approximate significance of smooth terms:
#        edf Ref.df     F  p-value    
# s(y) 8.726  8.968 5.137 6.78e-07 ***
# s(z) 8.110  8.793 5.827 1.55e-07 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# R-sq.(adj) =  0.187   Deviance explained = 21.7%
# GCV =  7.144  Scale est. = 6.8639    n = 455

&gt; AIC(m2)
#[1] 2187.168

&gt; anova(m1, m2, test=""F"")
# Analysis of Deviance Table
# 
# Model 1: out ~ s(x) + s(y) + s(z)
# Model 2: out ~ s(y) + s(z)
#   Resid. Df Resid. Dev      Df Deviance      F    Pr(&gt;F)    
# 1    433.58     2881.6                                      
# 2    437.16     3000.7 -3.5791   -119.1 5.0073 0.0009864 ***
</code></pre>

<hr>

<p><strong>EDIT</strong>: added model from comments</p>

<pre><code>&gt; summary(m3 &lt;- gam(out ~ s(x) + s(y) + s(z), data=dat, select=TRUE))

#Family: gaussian 
#Link function: identity 

#Formula:
#out ~ s(x) + s(y) + s(z)

#Parametric coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|)
#(Intercept) -1.588e-14  1.209e-01       0        1

#Approximate significance of smooth terms:
#       edf Ref.df     F  p-value    
#s(x) 4.424      9 1.750  0.00161 ** 
#s(y) 8.260      9 3.623 5.56e-06 ***
#s(z) 7.150      9 5.329 4.19e-09 ***
#---
#Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

#R-sq.(adj) =  0.212   Deviance explained = 24.7%
#GCV = 6.9694  Scale est. = 6.6502    n = 455
</code></pre>
"
"0.139175553567226","0.151402232820893","198364","<p>My aim is to run repeated measures ANOVA. I am using AR1 covariance structure to model within-subject correlations in dependent variable as AR1 covariance structure based model fits my data better than Compound symmetry based model and equally compared to unconstrained covariance structure based model. I am using R package nlme for my analyses with following syntax:</p>

<pre><code>model &lt;- formula(Sah_1 ~ 1 + Pre_CVS.sahharoosi.eelistus + Gender + Grupp + HC.LC + Time.f + Gender:Grupp +
                     Gender:HC.LC + Gender:Time.f + Grupp:HC.LC + Grupp:Time.f + HC.LC:Time.f +
                     Gender:Grupp:HC.LC +
                     Gender:Grupp:Time.f +
                     Gender:HC.LC:Time.f +
                     Grupp:HC.LC:Time.f +
                     Gender:Grupp:HC.LC:Time.f
)


lme_data &lt;- lme(model, random = ~1|grupimajutuse.ID, correlation=corAR1(form=~as.numeric(Time.f)|grupimajutuse.ID,fixed = FALSE), 
                na.action = (na.omit),
                data = SAH,
                method= ""REML"")
anova(lme_data)
</code></pre>

<p><strong>Time.f</strong> is repeated factor (my measures are separated by constant time intervals);<br>
<strong>Gender; HC.LC; Grupp</strong> are between-subjects treatment factors<br>
<strong>Pre_CVS.sahharoosi.eelistus</strong> is between subjects co-variate. </p>

<p><strong>My problem is non-convergence of my results obtained in R with results I obtain fitting identically specified model in InVivoStat.</strong> InVivoStat is free GUI based statistical package that runs R as its computational engine and InVivoStat documentations points out that repeated measures ANOVA is performed by using nlme packageâ€™s lme() function. 
In my case main effects and lower level interactions differ in F-statistics and p-values between R and InVivoStat, whereas higher order interactions return identical results. 
Here are the results I obtain myself from R: </p>

<pre><code>                            numDF denDF  F-value p-value
(Intercept)                     1   120 413392.7  &lt;.0001
Pre_CVS.sahharoosi.eelistus     1    39      4.9  0.0331
Gender                          1    39      0.0  0.9391
Grupp                           1    39      6.6  0.0140
HC.LC                           1    39      0.9  0.3422
Time.f                          3   120     16.2  &lt;.0001
Gender:Grupp                    1    39      1.3  0.2683
Gender:HC.LC                    1    39      3.8  0.0582
Gender:Time.f                   3   120      0.6  0.6150
Grupp:HC.LC                     1    39      0.0  0.9358
Grupp:Time.f                    3   120      2.7  0.0516
HC.LC:Time.f                    3   120      2.6  0.0560
Gender:Grupp:HC.LC              1    39      0.0  0.9484
Gender:Grupp:Time.f             3   120      0.2  0.9155
Gender:HC.LC:Time.f             3   120      0.4  0.7464
Grupp:HC.LC:Time.f              3   120      0.4  0.7281
Gender:Grupp:HC.LC:Time.f       3   120      0.0  0.9869
</code></pre>

<p>Here are the results from InVivoStat:</p>

<p><a href=""http://i.stack.imgur.com/NMZe0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NMZe0.png"" alt=""enter image description here""></a></p>

<p><strong>Here is the outline of steps I have tried myself without success:</strong></p>

<p>Changing the anova type from sequential to marginal and/or changing the REML estimator to ML estimator does not work as I lose the convergence even in higher order interaction terms in respect to F-statistics and p-values. In detail, changing the <code>anova(lme_data)</code> to <code>anova(update(lme_data,method=""ML""))</code> does not help. Neither do <code>anova(update(lme_data,method=""ML""), type = ""marginal"")</code> or <code>anova(update(lme_data,method=""REML""), type = ""marginal"")</code> work. </p>

<p>Neither does forcing the variability of responses at different time periods to be equal help:</p>

<pre><code>lme_data &lt;- lme(model, random = ~1|grupimajutuse.ID, correlation=corAR1(form=~as.numeric(Time.f)|grupimajutuse.ID,fixed = FALSE), 
                na.action = (na.omit),
                data = SAH,
                method= ""REML"",
                weights = varFixed(~as.numeric(Time.f))
                ) 
</code></pre>

<p>Does anybody have an idea or suggestions based on my outlined syntax how to achieve the convergence of results between R and InVivoStats in repeated measures analysis. I.e., I am trying to guess the model specification used in InVivoStat to carry out previously described analysis. Unfortunately the source code of InVivoStat is not available so I canâ€™t directly compare my syntax with it.       </p>

<p>Thanks in advance,</p>
"
"0.149378879319591","0.152345558039745","198484","<p>Consider this example:</p>

<pre><code>foo &lt;-data.frame(x=c(0.010355057,0.013228936,0.016313905,0.019261687,0.021710159,0.023973474,0.025968176,0.027767232,0.029459730,0.030213807,0.023582566,0.008689883,0.006558429,0.005144958),
                 y=c(971.3800,1025.2271,1104.1505,1034.2607,902.6324,713.9053,621.4824,521.7672,428.9838,381.4685,741.7900, 979.7046,1065.5245,1118.0616))
Model3 &lt;- lm(y~poly(x,3),data=foo)
Model4 &lt;- lm(y~poly(x,4),data=foo)
</code></pre>

<p>For <code>Model3</code>, the <code>poly(x,3)</code> term is not significant:</p>

<pre><code>&gt; summary(Model3)

Call:
lm(formula = y ~ poly(x, 3), data = foo)

Residuals:
   Min     1Q Median     3Q    Max 
-76.47 -51.61  -0.55  38.22 100.57 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   829.31      17.85  46.463 5.14e-13 ***
poly(x, 3)1  -819.37      66.78 -12.269 2.37e-07 ***
poly(x, 3)2  -373.05      66.78  -5.586 0.000232 ***
poly(x, 3)3   -87.85      66.78  -1.315 0.217740    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 66.78 on 10 degrees of freedom
Multiple R-squared:  0.9483,    Adjusted R-squared:  0.9328 
F-statistic: 61.15 on 3 and 10 DF,  p-value: 9.771e-07
</code></pre>

<p>However, for <code>Model4</code> it is:</p>

<pre><code>&gt; summary(Model4)

Call:
lm(formula = y ~ poly(x, 4), data = foo)

Residuals:
    Min      1Q  Median      3Q     Max 
-34.344 -19.982   1.229  18.499  33.116 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  829.310      7.924 104.655 3.37e-15 ***
poly(x, 4)1 -819.372     29.650 -27.635 5.16e-10 ***
poly(x, 4)2 -373.052     29.650 -12.582 5.14e-07 ***
poly(x, 4)3  -87.846     29.650  -2.963 0.015887 *  
poly(x, 4)4  191.543     29.650   6.460 0.000117 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 29.65 on 9 degrees of freedom
Multiple R-squared:  0.9908,    Adjusted R-squared:  0.9868 
F-statistic: 243.1 on 4 and 9 DF,  p-value: 3.695e-09
</code></pre>

<p>Why does this happen? Note that the estimate of all coefficients is the same in both cases, since the polynomials are orthogonal. However, the significance is not. This seems to me difficult to understand: if I performed a degree 3 regression, it looks like I could drop the <code>poly(x, 4)3</code> term, thus reverting to a degree 2 orthogonal regression. However, if I performed a degree 4 regression, I shouldn't, even though the coefficients of the common terms have exactly the same estimate. What do I conclude? Probably that one should never trust subset selection :) An <code>anova</code> analysis says that the difference among the degree 2, degree 3 and degree 4 models is significant:</p>

<pre><code>&gt; Model2 &lt;- lm(y~poly(x,2),data=foo)     
&gt; anova(Model2,Model3,Model4)
Analysis of Variance Table

Model 1: y ~ poly(x, 2)
Model 2: y ~ poly(x, 3)
Model 3: y ~ poly(x, 4)
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1     11 52318                                   
2     10 44601  1      7717  8.7782 0.0158868 *  
3      9  7912  1     36689 41.7341 0.0001167 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>EDIT: following a suggestion in comments, I add the residual vs fitted plots for <code>Model2</code>, <code>Model3</code> and <code>Model4</code></p>

<p><a href=""http://i.stack.imgur.com/9ZU8h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ZU8h.png"" alt=""enter image description here""></a>` </p>

<p>It's true that the maximum residual error is more or less the same for <code>Model2</code> and <code>Model3</code>, and it becomes nearly one third going from <code>Model3</code> to <code>Model4</code>. There seems to be still some kind of trend in the residuals, though it is less evident than for <code>Model2</code> and <code>Model3</code>. However, why does this invalidate the <em>p</em>-values? Which hypothesis of the linear model paradigm is violated here? I seem to remember that the residuals only had to be uncorrelated with the predictor. However, if they also have to uncorrelated among themselves, then clearly this assumption is violated and the <em>p</em>-values based on the t-test are invalid.</p>
"
"0.0371134809512603","0.0378505582052232","198879","<p>I'm a little stuck with my analysis in R.</p>

<p>I conducted an experiment with 2 IV's (2 x 3) and 36 subjects (Every Subject running each combination of the two conditions). </p>

<p>First, I used this code (""ez"" package)</p>

<pre><code>ezANOVA(data=longDataset,dv=.(DV),wid=.(subject),within=.(IV1,IV2),type=3,detailed=T)         
</code></pre>

<p>I got highly significant results for both IV's (4.4*10^-3 and 1.6*10^-6 respectively). 
Now I came across the aov function, applied it and got totally different results. IV1 did not even become significant. Code used was: </p>

<pre><code>m1 &lt;- aov(DV~ IV1*IV2 + Error(subject/(IV1*IV2)), longDataset)     
</code></pre>

<p>Am I doing something wrong or how can that happen? I'm a little confused because this seems a little inconsistent to me. </p>

<p>An other question: I wanted to test for normality and I read that this should rather be done by checking normality of residuals. How exactly is that done? My guess was to use the residuals of the above mentioned aov model but since the results are so different, I'm not so sure about that.</p>

<p>Thanks in advance.</p>

<p>My dataset as csv (sorry, I don't really know how to do it otherwise)</p>

<p>subject;Var;IV1;IV2
1;73.93795;IV1-A;IV2-A
2;67.16974;IV1-A;IV2-A
3;74.71601;IV1-A;IV2-A
4;73.11427;IV1-A;IV2-A
5;76.19306;IV1-A;IV2-A
6;74.92958;IV1-A;IV2-A
7;67.12077;IV1-A;IV2-A
8;78.15253;IV1-A;IV2-A
9;68.11335;IV1-A;IV2-A
10;80.70079;IV1-A;IV2-A
11;73.59406;IV1-A;IV2-A
12;65.78347;IV1-A;IV2-A
13;80.15322;IV1-A;IV2-A
14;70.38403;IV1-A;IV2-A
15;63.33345;IV1-A;IV2-A
16;62.98373;IV1-A;IV2-A
17;63.79794;IV1-A;IV2-A
18;52.40538;IV1-A;IV2-A
19;75.7191;IV1-A;IV2-A
20;95.91965;IV1-A;IV2-A
21;65.51091;IV1-A;IV2-A
22;70.55601;IV1-A;IV2-A
23;75.49884;IV1-A;IV2-A
24;71.57908;IV1-A;IV2-A
25;82.98507;IV1-A;IV2-A
26;69.22869;IV1-A;IV2-A
27;73.73734;IV1-A;IV2-A
28;69.6452;IV1-A;IV2-A
29;63.92717;IV1-A;IV2-A
30;65.8353;IV1-A;IV2-A
31;73.25384;IV1-A;IV2-A
32;64.38599;IV1-A;IV2-A
33;76.98158;IV1-A;IV2-A
34;72.95371;IV1-A;IV2-A
35;83.84995;IV1-A;IV2-A
36;95.53878;IV1-A;IV2-A
1;76.01903;IV1-A;IV2-B
2;68.75768;IV1-A;IV2-B
3;77.25573;IV1-A;IV2-B
4;75.23084;IV1-A;IV2-B
5;78.35189;IV1-A;IV2-B
6;79.04529;IV1-A;IV2-B
7;72.31582;IV1-A;IV2-B
8;90.57213;IV1-A;IV2-B
9;77.88892;IV1-A;IV2-B
10;81.97801;IV1-A;IV2-B
11;75.51124;IV1-A;IV2-B
12;72.89464;IV1-A;IV2-B
13;84.65149;IV1-A;IV2-B
14;71.88893;IV1-A;IV2-B
15;64.4815;IV1-A;IV2-B
16;66.04601;IV1-A;IV2-B
17;64.2564;IV1-A;IV2-B
18;52.40004;IV1-A;IV2-B
19;74.51742;IV1-A;IV2-B
20;98.29295;IV1-A;IV2-B
21;69.10086;IV1-A;IV2-B
22;72.89067;IV1-A;IV2-B
23;78.25941;IV1-A;IV2-B
24;72.29076;IV1-A;IV2-B
25;84.42784;IV1-A;IV2-B
26;76.65195;IV1-A;IV2-B
27;70.61015;IV1-A;IV2-B
28;70.35392;IV1-A;IV2-B
29;65.58365;IV1-A;IV2-B
30;66.51723;IV1-A;IV2-B
31;75.67548;IV1-A;IV2-B
32;70.68385;IV1-A;IV2-B
33;76.46937;IV1-A;IV2-B
34;80.02998;IV1-A;IV2-B
35;87.84725;IV1-A;IV2-B
36;104.4789;IV1-A;IV2-B
1;75.97461;IV1-B;IV2-A
2;71.14298;IV1-B;IV2-A
3;83.13754;IV1-B;IV2-A
4;76.12074;IV1-B;IV2-A
5;81.87975;IV1-B;IV2-A
6;76.6732;IV1-B;IV2-A
7;67.66522;IV1-B;IV2-A
8;79.42232;IV1-B;IV2-A
9;68.7449;IV1-B;IV2-A
10;83.17275;IV1-B;IV2-A
11;73.05918;IV1-B;IV2-A
12;70.95263;IV1-B;IV2-A
13;80.85802;IV1-B;IV2-A
14;70.09309;IV1-B;IV2-A
15;62.91889;IV1-B;IV2-A
16;65.9611;IV1-B;IV2-A
17;64.54493;IV1-B;IV2-A
18;54.5939;IV1-B;IV2-A
19;76.50805;IV1-B;IV2-A
20;96.86421;IV1-B;IV2-A
21;68.85271;IV1-B;IV2-A
22;70.98612;IV1-B;IV2-A
23;76.95681;IV1-B;IV2-A
24;70.94803;IV1-B;IV2-A
25;80.33619;IV1-B;IV2-A
26;66.91155;IV1-B;IV2-A
27;72.48396;IV1-B;IV2-A
28;75.28927;IV1-B;IV2-A
29;63.93246;IV1-B;IV2-A
30;66.58893;IV1-B;IV2-A
31;73.01255;IV1-B;IV2-A
32;63.37467;IV1-B;IV2-A
33;80.31099;IV1-B;IV2-A
34;79.65555;IV1-B;IV2-A
35;88.78948;IV1-B;IV2-A
36;94.68608;IV1-B;IV2-A
1;75.46562;IV1-B;IV2-B
2;68.15868;IV1-B;IV2-B
3;82.53435;IV1-B;IV2-B
4;78.03364;IV1-B;IV2-B
5;80.13785;IV1-B;IV2-B
6;79.44407;IV1-B;IV2-B
7;70.88688;IV1-B;IV2-B
8;85.31608;IV1-B;IV2-B
9;86.18613;IV1-B;IV2-B
10;86.42501;IV1-B;IV2-B
11;81.91581;IV1-B;IV2-B
12;72.16431;IV1-B;IV2-B
13;86.91892;IV1-B;IV2-B
14;70.34856;IV1-B;IV2-B
15;65.21148;IV1-B;IV2-B
16;70.43033;IV1-B;IV2-B
17;64.93448;IV1-B;IV2-B
18;55.85067;IV1-B;IV2-B
19;75.11274;IV1-B;IV2-B
20;96.96702;IV1-B;IV2-B
21;72.33325;IV1-B;IV2-B
22;73.93191;IV1-B;IV2-B
23;77.30208;IV1-B;IV2-B
24;71.75197;IV1-B;IV2-B
25;82.33074;IV1-B;IV2-B
26;71.91012;IV1-B;IV2-B
27;75.57676;IV1-B;IV2-B
28;73.60806;IV1-B;IV2-B
29;67.05006;IV1-B;IV2-B
30;67.84696;IV1-B;IV2-B
31;74.15455;IV1-B;IV2-B
32;70.61079;IV1-B;IV2-B
33;77.16185;IV1-B;IV2-B
34;90.41821;IV1-B;IV2-B
35;92.53073;IV1-B;IV2-B
36;101.3155;IV1-B;IV2-B
1;73.0103;IV1-C;IV2-A
2;68.92526;IV1-C;IV2-A
3;76.62519;IV1-C;IV2-A
4;71.85777;IV1-C;IV2-A
5;85.37516;IV1-C;IV2-A
6;79.49771;IV1-C;IV2-A
7;66.574;IV1-C;IV2-A
8;80.83074;IV1-C;IV2-A
9;76.91873;IV1-C;IV2-A
10;82.54743;IV1-C;IV2-A
11;80.00673;IV1-C;IV2-A
12;70.82084;IV1-C;IV2-A
13;78.17903;IV1-C;IV2-A
14;69.12538;IV1-C;IV2-A
15;63.12577;IV1-C;IV2-A
16;64.31039;IV1-C;IV2-A
17;65.86452;IV1-C;IV2-A
18;57.20816;IV1-C;IV2-A
19;75.65835;IV1-C;IV2-A
20;96.8246;IV1-C;IV2-A
21;67.89588;IV1-C;IV2-A
22;71.34121;IV1-C;IV2-A
23;75.6153;IV1-C;IV2-A
24;71.9312;IV1-C;IV2-A
25;77.72873;IV1-C;IV2-A
26;68.60328;IV1-C;IV2-A
27;70.79825;IV1-C;IV2-A
28;73.48208;IV1-C;IV2-A
29;65.5584;IV1-C;IV2-A
30;65.38625;IV1-C;IV2-A
31;72.97655;IV1-C;IV2-A
32;64.04929;IV1-C;IV2-A
33;77.22308;IV1-C;IV2-A
34;71.15641;IV1-C;IV2-A
35;85.69486;IV1-C;IV2-A
36;100.7196;IV1-C;IV2-A
1;75.29735;IV1-C;IV2-B
2;67.89488;IV1-C;IV2-B
3;76.80959;IV1-C;IV2-B
4;76.0126;IV1-C;IV2-B
5;90.19161;IV1-C;IV2-B
6;84.14849;IV1-C;IV2-B
7;66.1515;IV1-C;IV2-B
8;92.13945;IV1-C;IV2-B
9;81.23921;IV1-C;IV2-B
10;83.10084;IV1-C;IV2-B
11;100.0274;IV1-C;IV2-B
12;77.26562;IV1-C;IV2-B
13;82.82025;IV1-C;IV2-B
14;72.16927;IV1-C;IV2-B
15;64.28114;IV1-C;IV2-B
16;68.60584;IV1-C;IV2-B
17;66.36162;IV1-C;IV2-B
18;58.2878;IV1-C;IV2-B
19;75.00374;IV1-C;IV2-B
20;99.60698;IV1-C;IV2-B
21;68.52582;IV1-C;IV2-B
22;74.7569;IV1-C;IV2-B
23;82.94307;IV1-C;IV2-B
24;73.43107;IV1-C;IV2-B</p>
"
"0.128564869306645","0.131118179812579","199422","<p>Following <a href=""http://www.stat.columbia.edu/~martin/W2024/R8.pdf"" rel=""nofollow"">this</a> and other sources of information on how to perform ANOVA and ANCOVA in R, I got very confused on the difference between the two on how to compute this difference. Please consider the following two examples</p>

<p><strong>ANCOVA</strong></p>

<pre><code>require(ggplot2)


&gt; anova(lm(price~table+depth, data = diamonds))  
Response: price
             Df     Sum Sq    Mean Sq  F value  Pr(&gt;F)    
depth         1 9.7323e+07 9.7323e+07   6.2202 0.01263 *  
table         1 1.4462e+10 1.4462e+10 924.2957 &lt; 2e-16 ***
Residuals 53937 8.4391e+11 1.5646e+07                     
</code></pre>

<p>and </p>

<pre><code>&gt; anova(lm(price~depth+table, data = diamonds))  
Response: price
             Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    
table         1 1.3876e+10 1.3876e+10 886.825 &lt; 2.2e-16 ***
depth         1 6.8360e+08 6.8360e+08  43.691 3.882e-11 ***
Residuals 53937 8.4391e+11 1.5646e+07                      
</code></pre>

<p>The sum of squares, pvalues and other values all changed depending on the order. This lead me to think that I just performed an ANCOVA. </p>

<p><strong>ANOVA</strong></p>

<p>The example comes form <a href=""http://www.r-bloggers.com/two-way-analysis-of-variance-anova/"" rel=""nofollow"">here</a></p>

<pre><code>delivery.df = data.frame(
  Service = c(rep(""Carrier 1"", 15), rep(""Carrier 2"", 15),
    rep(""Carrier 3"", 15)),
  Destination = c(rep(c(""Office 1"", ""Office 2"", ""Office 3"",
    ""Office 4"", ""Office 5""), 9)),
  Time = c(15.23, 14.32, 14.77, 15.12, 14.05,
  15.48, 14.13, 14.46, 15.62, 14.23, 15.19, 14.67, 14.48, 15.34, 14.22,
  16.66, 16.27, 16.35, 16.93, 15.05, 16.98, 16.43, 15.95, 16.73, 15.62,
  16.53, 16.26, 15.69, 16.97, 15.37, 17.12, 16.65, 15.73, 17.77, 15.52,
  16.15, 16.86, 15.18, 17.96, 15.26, 16.36, 16.44, 14.82, 17.62, 15.04)
)


&gt; anova(lm(Time ~ Service*Destination, data = delivery.df))
Response: Time
                    Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
Service              2 23.1706 11.5853 161.5599 &lt; 2.2e-16 ***
Destination          4 17.5415  4.3854  61.1553 5.408e-14 ***
Service:Destination  8  4.1888  0.5236   7.3018 2.360e-05 ***
Residuals           30  2.1513  0.0717                       
</code></pre>

<p>and </p>

<pre><code>&gt; anova(lm(Time ~Destination*Service, data = delivery.df))
Response: Time
                    Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
Destination          4 17.5415  4.3854  61.1553 5.408e-14 ***
Service              2 23.1706 11.5853 161.5599 &lt; 2.2e-16 ***
Destination:Service  8  4.1888  0.5236   7.3018 2.360e-05 ***
Residuals           30  2.1513  0.0717                       
</code></pre>

<p>Here the values do not depend on the order suggesting that I did an ANOVA</p>

<p><strong>Questions</strong></p>

<ul>
<li>Am I right to think that I first did an ANCOVA and then an ANOVA?</li>
<li>Where did the code differ to cause one analysis to be an ANOVA and the other an ANCOVA?</li>
<li>Because, the order matters in ANCOVA, I would have thought I would be able to compute the interaction before the main effect if I desire. I tried <code>anova(lm(Time ~ Destination:Service+Destination+Service, data = delivery.df))</code> but the interaction remains at the end.</li>
</ul>
"
"0.0742269619025206","0.0757011164104465","200547","<p>So I have 1 IV that consists of 2 categorical groups. Both groups complete the same four tasks, the 4 DV's in my study which are continious.</p>

<p>I planned a MANOVA, to see whether the DV's are different within the group itself. I think the best approach is post-hoc t-tests between the four variables to see which one is different than which one.</p>

<p>After doing that, I want to see whether the 4 variables also differ between groups. I had in mind to test variable 1 with variable 1 in the other group, variable 2 with variable 2 in the other group and so on. To do this, I planned on doing a MANOVA between the two groups and 4 variables and doing post-hoc t-tests again between variable 1 and variable 1 in the other group, variable 2 and variable 2 in the other group and so on.</p>

<p>Do you have other recommendations for the analyses?</p>

<p>If it matters, I'd like to do my analysis in R.</p>
"
"0.139175553567226","0.123014314166975","201105","<p>I'm about to have data from intercept surveys conducted in parks. The goal of the survey is to determine which characteristics of parks users find most important to park quality (do they care a lot about safety, a little about the facilities, and not at all about who else is there?).</p>

<p>We've designed a survey with open-ended questions to answer this question. The current plan is to take down the responses, and then, once we have them, group them into categories (safety, facilities, social environment, accessibility, etc). </p>

<p>For example, one question on the survey asks the user why they came to park. </p>

<p>Each user's response (we're allowing them to list as many reasons as they like, but are asking for primary reasons first, then secondary reasons and so on) will then be associated with some field coding. For one user it might be, say, facilities and park aesthetics, for another it might be easy access. We'll also have some demographic data (age, sex, ethnicity, activity at the park) for each user.</p>

<p><strong>Question 1:</strong> We want to determine which of the categories is most important to users, and if possible, by how much. I've never done any categorical data analysis, and I have no idea what to do here. For some questions we're just going to have counts: 16 people came for facilities, 10 for open spaces, etc.</p>

<p><strong>Question 2:</strong> A separate series of questions asks users to categorize park quality on a Likert-like scale (low to high quality), and also to rate sub-components of park quality in the same way (quality of facilities, from low to high, and so on). We want to determine which predictors have the largest effect on perceived park quality here as well.</p>

<p><strong>I want to know what type of models to fit to our data, and why.</strong></p>

<p>I'm presuming we want some categorical analogue of regression. I want to pick up theoretical underpinnings, learn how to fit models in R, and also how to perform diagnostics on them. </p>

<p>Once I've decided on the appropriate analysis and have picked up the necessary background, I'd like to pre-register my data analysis plan. I've never done this before and am curious what the convention is for this.</p>

<p>Some details about the sample of parks: the city Parks and Recreation department has selected 10 parks for us to visit. Their park selection criteria is not entirely known, but I think they want to visit some well developed and some under developed parks. There are five pairs of parks that the Parks department thinks are comparable. In each pair of parks, one has recently undergone renovation, and the other hasn't.</p>

<p>My background:</p>

<p>I have taken a first course in math-stat, a course on linear regression, and am halfway through a course on experimental design/ANOVA/EM/Bootstrap. I have some pure math, multi, lin-alg and optimization background as well. I have some limited experience in R as well.</p>
"
"0.0742269619025206","0.0757011164104465","202032","<p>I have the following data that I wish to analyze using R:</p>

<pre><code>     Resilience     PartsA       PartsB
1   4.805032           1           1
2   4.657384           1           2
3   4.703198           1           3
4   3.993497           1           4
5   4.645764           1           5
6   4.603158           1           1
7   4.811521           1           2
8   4.682717           1           3
9   4.728485           1           4
10  4.734114           1           5
11  4.532497           1           1
12  4.885308           1           2
13  4.702712           1           3
14  4.692207           1           4
15  4.740994           1           5
16  4.572724           1           1
17  4.919445           1           2
18  4.650043           1           3
19  4.761368           1           4
20  4.790507           1           5
21  4.653509           2           1
22  4.720434           2           2
23  4.833647           2           3
24  4.997706           2           4
25  4.630829           2           5
26  4.690605           2           1
27  4.681007           2           2
28  4.784369           2           3
29  4.704247           2           4
30  4.575493           2           5
31  4.553369           2           1
32  4.758170           2           2
33  4.855304           2           3
34  4.903961           2           4
35  5.002031           2           5
36  4.769658           2           1
37  4.651714           2           2
38  4.929959           2           3
39  4.648468           2           4
40  4.788978           2           5
41  4.812591           3           1
42  4.877903           3           2
43  4.928751           3           3
44  4.925799           3           4
45  4.005860           3           5
46  4.662776           3           1
47  4.896822           3           2
48  4.904109           3           3
49  4.971777           3           4
50  4.832897           3           5
</code></pre>

<p>I want to perform some kind of analysis in order to understand which Parts from A and B (which combination, such as 1 from PartsA and 3 from PartsB) cause the most deviation from the mean in the final result (material resilience).</p>

<p>From PartsA, 3 same parts are used, but from different sources (in the construction of a material), and PartsB that's used in the construction is brought in from 5 different sources.</p>

<p>Basically I want to test to see whether or not using parts from different sources creates a significant difference on the results or if all parts render same results (null hypothesis). Essentially a test for the significance that PartsA and PartsB play in the final outcome.</p>

<p>I've thought about using ANOVA, in order to analyze the variance but I am rather unsure about how to interpret the results. 
Any help would be greatly appreciated. Many thanks </p>
"
"0.0371134809512603","0.0378505582052232","203291","<p>I have a time series for 24 individuals over the course of one month measured on day 0,2,4,6,8,11,14 and 30. Each individual was infected by a viral pathogen (on day 0) and virus titer, acute phase proteins and cytokines were measured at on each day. </p>

<p>I would like to answer the following questions with a univariate analysis for each variable:</p>

<p>1) When do biomarkers increase after exposure (day 0)?
2) How long do they stay increased for (i.e. when do they start to decrease)?
3) What are the mean values for each biomarker at each time point?</p>

<p>I think I may be able to answer this question by looking at multiple change points in a panel model but am not sure if R code has been developed for this purpose. I am also open to suggestions for other analyses (I have also already looked at using ANOVA).  </p>
"
"0.0642824346533225","0.0437060599375265","204119","<p>There is a package in R called <code>pwr</code>. This is useful to make power analysis when designing the sampling of a project. here are few examples: </p>

<pre><code>library(pwr)
pwr.anova.test(k = 4, f = 0.5, sig.level = 0.05, power = .9)
pwr.2p.test(h = 0.5, sig.level =0.05, power = .9)
pwr.f2.test(u = 4, f2 = .5, sig.level = 0.05, power = .8)
</code></pre>

<p>However, is it possible to run a power analysis for a spline regression (or a generalized additive model (GAM))? I want to know how may organisms I would have to sample to detect an effect of selection, that is a shift in morphology of the beak of birds of only 0.5Â mm, given that my sig.level = 0.05 and that I have 4 species. </p>

<p>Also, Iâ€™m recapturing birds in a population each year since 2003. Is there a power calculation to estimate how many birds should I sample to get a probability of recapture of 25%? Iâ€™m running a recapture model in Bayesian statistics, so there is not a function in the package <code>pwr</code> that can do this. </p>
"
"0.0742269619025206","0.0630842636753721","204314","<p>Sorry, I am not very good in posting code and data frames on the internet, so probably I could write this post in a better way.</p>

<p>I am trying to perform a 2 way anova with repeated measurements in the R environment.</p>

<p>I have 16 subjects, 2 different conditions for every subject (HDBR and HOWI) and 6 time phases for every condition (phasepre, phase1, phase2, phase3, phase4, phase5)</p>

<p>I found two different scripts online :</p>

<p>1) <a href=""http://www.r-bloggers.com/two-way-anova-with-repeated-measures/"" rel=""nofollow"">http://www.r-bloggers.com/two-way-anova-with-repeated-measures/</a></p>

<p>2) <a href=""http://rtutorialseries.blogspot.de/2011/02/r-tutorial-series-two-way-repeated.html"" rel=""nofollow"">http://rtutorialseries.blogspot.de/2011/02/r-tutorial-series-two-way-repeated.html</a></p>

<p>The first needs a dataframe long, this is what I did:</p>

<pre><code>Alpha1 &lt;- read.csv(""Alpha1_trasponed.csv"")
Alpha1 &lt;- Alpha1[order(Alpha1$subject), ]
    head(Alpha1)
    Alpha1.mean &lt;- aggregate(Alpha1$value,
                         by = list(Alpha1$subject, Alpha1$condition,
                                   Alpha1$phase),
                         FUN = 'mean')

colnames(Alpha1.mean) &lt;- c(""subject"",""condition"",""phase"",""value"")

Alpha1.mean &lt;- Alpha1.mean[order(Alpha1.mean$subject), ]
head(Alpha1.mean)
value.aov &lt;- with(Alpha1.mean,
                   aov(value ~ condition * phase +
                         Error(subject / (condition * phase)))
)

summary(value.aov)
</code></pre>

<p>The second one needs a dataframe wide and this is what I did:</p>

<pre><code>Alpha1 &lt;- read.csv(""Alpha1.csv"")
idata &lt;- read.csv(""idata.csv"")
idata
interestBind &lt;- cbind(Alpha1$HOWIphasepre, Alpha1$HOWIphase1, Alpha1$HOWIphase2, Alpha1$HOWIphase3, Alpha1$HOWIphase4, Alpha1$HOWIphase5, Alpha1$HDBRphasepre, Alpha1$HDBRphase1, Alpha1$HDBRphase2, Alpha1$HDBRphase3, Alpha1$HDBRphase4, Alpha1$HDBRphase5)
interestModel &lt;- lm(interestBind ~ 1)
library(car)
analysis &lt;- Anova(interestModel, idata = idata, idesign = ~Condition * Phase)
#summary(analysis)
summary(analysis, multivariate=FALSE)
</code></pre>

<p>These are my .csv files (I used x instead of real values)</p>

<p>Idata:</p>

<pre><code>Condition   Phase
HOWI    phasepre
HOWI    phase1
HOWI    phase2
HOWI    phase3
HOWI    phase4
HOWI    phase5
HDBR    phasepre
HDBR    phase1
HDBR    phase2
HDBR    phase3
HDBR    phase4
HDBR    phase5
</code></pre>

<p>Alpha1:</p>

<pre><code>Subject HOWIphasepre    HOWIphase1  HOWIphase2  HOWIphase3  HOWIphase4  

HOWIphase5  HDBRphasepre
1   x   x   x   x   x   x   x
2   x   x   x   x   x   x   x
3   x   x   x   x   x   x   x
4   x   x   x   x   x   x   x
5   x   x   x   x   x   x   x
6   x   x   x   x   x   x   x
7   x   x   x   x   x   x   x
8   x   x   x   x   x   x   x
9   x   x   x   x   x   x   x
10  x   x   x   x   x   x   x
11  x   x   x   x   x   x   x
12  x   x   x   x   x   x   x
13  x   x   x   x   x   x   x
14  x   x   x   x   x   x   x
15  x   x   x   x   x   x   x
16  x   x   x   x   x   x   x
</code></pre>

<p>Alpha1_trasponed:</p>

<pre><code>condition   phase   value   subject
    HOWI    phasepre    x   1
    HOWI    phase1  x   1
    HOWI    phase2  x   1
    HOWI    phase3  x   1
    HOWI    phase4  x   1
    HOWI    phase5  x   1
    HDBR    phasepre    x   1
    HDBR    phase1  x   1
    HDBR    phase2  x   1
    HDBR    phase3  x   1
    HDBR    phase4  x   1
    HDBR    phase5  x   1
    HOWI    phasepre    x   2
    HOWI    phase1  x   2
    HOWI    phase2  x   2
    HOWI    phase3  x   2
    HOWI    phase4  x   2
    HOWI    phase5  x   2
    HDBR    phasepre    x   2
    HDBR    phase1  x   2
    HDBR    phase2  x   2
    HDBR    phase3  x   2
    HDBR    phase4  x   2
    HDBR    phase5  x   2
    HOWI    phasepre    x   3
    HOWI    phase1  x   3
    HOWI    phase2  x   3
    HOWI    phase3  x   3
    HOWI    phase4  x   3
    HOWI    phase5  x   3
    HDBR    phasepre    x   3
    HDBR    phase1  x   3
    HDBR    phase2  x   3
    HDBR    phase3  x   3
    HDBR    phase4  x   3
    HDBR    phase5  x   3
    HOWI    phasepre    x   4
    HOWI    phase1  x   4
    HOWI    phase2  x   4
    HOWI    phase3  x   4
    HOWI    phase4  x   4
    HOWI    phase5  x   4
    HDBR    phasepre    x   4
    HDBR    phase1  x   4
    HDBR    phase2  x   4
    HDBR    phase3  x   4
    HDBR    phase4  x   4
    HDBR    phase5  x   4
    HOWI    phasepre    x   5
    HOWI    phase1  x   5
    HOWI    phase2  x   5
    HOWI    phase3  x   5
    HOWI    phase4  x   5
    HOWI    phase5  x   5
    HDBR    phasepre    x   5
    HDBR    phase1  x   5
    HDBR    phase2  x   5
    HDBR    phase3  x   5
    HDBR    phase4  x   5
    HDBR    phase5  x   5
    HOWI    phasepre    x   6
    HOWI    phase1  x   6
    HOWI    phase2  x   6
    HOWI    phase3  x   6
    HOWI    phase4  x   6
    HOWI    phase5  x   6
    HDBR    phasepre    x   6
    HDBR    phase1  x   6
    HDBR    phase2  x   6
    HDBR    phase3  x   6
    HDBR    phase4  x   6
    HDBR    phase5  x   6
    HOWI    phasepre    x   7
    HOWI    phase1  x   7
    HOWI    phase2  x   7
    HOWI    phase3  x   7
    HOWI    phase4  x   7
    HOWI    phase5  x   7
    HDBR    phasepre    x   7
    HDBR    phase1  x   7
    HDBR    phase2  x   7
    HDBR    phase3  x   7
    HDBR    phase4  x   7
    HDBR    phase5  x   7
    HOWI    phasepre    x   8
    HOWI    phase1  x   8
    HOWI    phase2  x   8
    HOWI    phase3  x   8
    HOWI    phase4  x   8
    HOWI    phase5  x   8
    HDBR    phasepre    x   8
    HDBR    phase1  x   8
    HDBR    phase2  x   8
    HDBR    phase3  x   8
    HDBR    phase4  x   8
    HDBR    phase5  x   8
    HOWI    phasepre    x   9
    HOWI    phase1  x   9
    HOWI    phase2  x   9
    HOWI    phase3  x   9
    HOWI    phase4  x   9
    HOWI    phase5  x   9
    HDBR    phasepre    x   9
    HDBR    phase1  x   9
    HDBR    phase2  x   9
    HDBR    phase3  x   9
    HDBR    phase4  x   9
    HDBR    phase5  x   9
    HOWI    phasepre    x   10
    HOWI    phase1  x   10
    HOWI    phase2  x   10
    HOWI    phase3  x   10
    HOWI    phase4  x   10
    HOWI    phase5  x   10
    HDBR    phasepre    x   10
    HDBR    phase1  x   10
    HDBR    phase2  x   10
    HDBR    phase3  x   10
    HDBR    phase4  x   10
    HDBR    phase5  x   10
    HOWI    phasepre    x   11
    HOWI    phase1  x   11
    HOWI    phase2  x   11
    HOWI    phase3  x   11
    HOWI    phase4  x   11
    HOWI    phase5  x   11
    HDBR    phasepre    x   11
    HDBR    phase1  x   11
    HDBR    phase2  x   11
    HDBR    phase3  x   11
    HDBR    phase4  x   11
    HDBR    phase5  x   11
    HOWI    phasepre    x   12
    HOWI    phase1  x   12
    HOWI    phase2  x   12
    HOWI    phase3  x   12
    HOWI    phase4  x   12
    HOWI    phase5  x   12
    HDBR    phasepre    x   12
    HDBR    phase1  x   12
    HDBR    phase2  x   12
    HDBR    phase3  x   12
    HDBR    phase4  x   12
    HDBR    phase5  x   12
    HOWI    phasepre    x   13
    HOWI    phase1  x   13
    HOWI    phase2  x   13
    HOWI    phase3  x   13
    HOWI    phase4  x   13
    HOWI    phase5  x   13
    HDBR    phasepre    x   13
    HDBR    phase1  x   13
    HDBR    phase2  x   13
    HDBR    phase3  x   13
    HDBR    phase4  x   13
    HDBR    phase5  x   13
    HOWI    phasepre    x   14
    HOWI    phase1  x   14
    HOWI    phase2  x   14
    HOWI    phase3  x   14
    HOWI    phase4  x   14
    HOWI    phase5  x   14
    HDBR    phasepre    x   14
    HDBR    phase1  x   14
    HDBR    phase2  x   14
    HDBR    phase3  x   14
    HDBR    phase4  x   14
    HDBR    phase5  x   14
    HOWI    phasepre    x   15
    HOWI    phase1  x   15
    HOWI    phase2  x   15
    HOWI    phase3  x   15
    HOWI    phase4  x   15
    HOWI    phase5  x   15
    HDBR    phasepre    x   15
    HDBR    phase1  x   15
    HDBR    phase2  x   15
    HDBR    phase3  x   15
    HDBR    phase4  x   15
    HDBR    phase5  x   15
    HOWI    phasepre    x   16
    HOWI    phase1  x   16
    HOWI    phase2  x   16
    HOWI    phase3  x   16
    HOWI    phase4  x   16
    HOWI    phase5  x   16
    HDBR    phasepre    x   16
    HDBR    phase1  x   16
    HDBR    phase2  x   16
    HDBR    phase3  x   16
    HDBR    phase4  x   16
    HDBR    phase5  x   16
</code></pre>

<p>The problem is that I got different p values!</p>

<p>I suppose that is my fault... Which script is correct?</p>

<p>Where did I made mistakes?</p>

<p>Do anybody have a good ""two way anova with repeated measurements"" script?</p>

<p>Thank you in advance, I hope my doubt is clear!</p>

<p>Dorian</p>
"
"0.0909090909090909","0.092714554082312","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.0524863881081478","0.0535287727572189","204848","<p>I am working with infection and mortality numbers in plants. I have the raw numbers and calculated probabilities based on the sample size for both.</p>

<p>Initially I thought I needed to use the raw data in a GLM model and the predict function in R to graph and analyze the data. However, a member of my lab group thought this was incorrect and that a simple analysis of calculated probabilities with ANOVA would be more appropriate, and more powerful.</p>

<p>Is there any reason why it would not be appropriate to utilize ANOVA to analyze calculated probabilities?</p>

<p>I have attached a screenshot of some similar data to give an idea</p>

<p>Thanks in advance for any input you can provide
<a href=""http://i.stack.imgur.com/eUWtW.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eUWtW.jpg"" alt=""Sample data from exel""></a></p>
"
"0.0642824346533225","0.0655590899062897","205626","<p>I want to tune the parameters of an optimization algorithm. Since there are $4$ parameters: <code>A</code>, <code>B</code>, <code>C</code>, and <code>D</code>, I used the $2^4$ design. </p>

<p>Each combination of the parameters was run on <code>18</code> problem instances (<code>6</code> functions with <code>3</code> levels of dimensionality) <code>51</code> times. Optimization result, let's call it <code>Y</code>, was recorded for each run.</p>

<p>Now, I want to see which parameters have significant effect on <code>Y</code>, i.e. to determine which main effects and interactions are significant. I read that ANOVA could be used to do it. However, I'm not sure how to structure the data in SPSS to be able to begin the analysis in the first place.</p>

<p>How should I do it:</p>

<p>1)</p>

<pre><code>A B C D function dimensionality Y1 Y2 Y3 ... Y51
</code></pre>

<p>or</p>

<p>2) </p>

<pre><code>A B C D function dimensionality run_number Y
</code></pre>

<p>and then  have <code>run_number</code> go from 1 to 51?</p>

<p>SPSS is my primary choice, but any help for how to do this in R will be welcome. :)</p>
"
"0.0642824346533225","0.0655590899062897","205786","<p>I have conducted an experimental study, with 1 within-variable (time: T1 and T2) and 1 between-variable (group: control and treatment), measuring just one dependent variable.  </p>

<p>I understand that this is a design which would require a mixed ANOVA analysis. Of course two distinct t-tests would be much easier: one dependent and one independent two-sample-t-tests. </p>

<p>What is problematic about using two distinct t-tests in comparison to mixed ANOVA (despite ignoring the interaction effect, which I assume to not exist)? </p>

<p>Thanks for advice.</p>

<p><strong>Update</strong>: What I've done so far (in R) is:</p>

<pre><code>t.test(Con$DELTA, mu=0, alternative = c(""greater""))

t.test(Exp$DELTA, mu=0, alternative = c(""greater""))
</code></pre>

<p>Two single dependent/paired t.tests for each group, one-sided, because I'm just interested in each groups behavior change success.  Afterwards I can compare both groups (with independent two-sample-t-test):</p>

<pre><code>t.test(Con$DELTA, Exp$DELTA)
</code></pre>

<p>two-sided test, because I'm not sure which group is better than the other. Each t-test represents one stand-alone hypothesis. </p>
"
"0.138865930150177","0.141623820702091","206894","<p>Background on what I am doing...</p>

<p>I have 31 years of Landsat satellite data, and have extracted spectral reluctance and calculated 13 unique spectral based vegetation metrics for a series of 16 field plots have, which were then split into two groups, deciduous dominant and spruce dominant.  My goal is to identify the change in spectral seperability between the two groups over time, following a fire in 1994.  </p>

<p>To identify seperability, I have used both ANOVA anova(lm()) and t.test() in R, with the goal of identifying statistically significant differences in the mean spectral reflectance of the two groups. </p>

<p>For one iteration of my analysis, I end up with 31 years years of t tests for 113 metrics, or 403 t tests.  I have run several iterations, using different classification criteria for vegetation type dominance.  So a TON of t tests.  I have all of the T statistics organized in a tidy .csv.   </p>

<p>My t test is set up as follows: </p>

<p>t.test(data1_1984, data2_1984, paired=FALSE, var.equal=FALSE)</p>

<p>where data1_1984 is a list of spectral reflectance values for the deciduous group, and data2 is spectral reflectance for the spruce group. </p>

<p>I fundamentally understand that I should look at each resulting t statistic, to determine whether t statistic is > critical t value, as determined by the df of the sample.  Given that the same set of samples is used for each test, I assumed that there would be one common df, with one common critical value used to interpret all tests.  But when I look at the df of the resulting tests, they are highly variable, non-integer numbers.   </p>

<p>An example of my t_test t statistic results : </p>

<p><a href=""http://i.stack.imgur.com/w55sM.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/w55sM.jpg"" alt=""enter image description here""></a></p>

<p>as well as an example of the df for each respective record in the t_test table:</p>

<p><a href=""http://i.stack.imgur.com/55z1c.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/55z1c.jpg"" alt=""enter image description here""></a></p>

<p>So my question is: </p>

<p>1) Is there some optional argument that I am not setting correctly which is causing the df to be calculated for each individual test</p>

<p>or</p>

<p>2) If the variability in df is to be expected, a) could someone explain how these values are calculated, and b) can anyone suggest an automated way to analyze some 1200 t test results?? My original method was to use conditional formatting in Excel, to simply highlight any cells that were >= the critical value... </p>
"
"0.17799000188592","0.173632513249697","208273","<p>I want to analyse the effect of different treatment types (<code>control, treatment1, ..., treatment4</code>) on the surface of specimens made of certain materials (<code>plastic, metal</code>). The undamaged area of the surface is measured <code>before</code> and <code>after</code> the treatment.</p>

<p>According to this design I specified a mixed model using lme4 as follows:</p>

<pre><code>require(""lme4"")
data &lt;- read.csv(""http://pastebin.com/raw/G4D8dh1f"")
mm1  &lt;- lmer(undamaged_area ~ time*material*treatment + (1|specimen_id), data)
</code></pre>

<p><strong>Questions:</strong> </p>

<ol>
<li><p>Is the mixed model the optimal choice in this case? I found some hints that an ANCOVA (something like <code>lm(undamaged_area_after ~ material*treatment + undamaged_area_before, data)</code>) might be an alternative approach.</p>

<p>A closer look on the diagnostic plots of the mixed model makes me very suspicious:</p>

<pre><code>plot(mm1); require(""lattice""); qqmath(mm1)  
</code></pre>

<p><a href=""http://i.stack.imgur.com/LDxEY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LDxEY.png"" alt=""diagnostic plots""></a></p></li>
<li><p>Does the plots actually indicate a violation of model assumptions? Does the strange pattern come from a misspecification of the model? </p></li>
</ol>

<p><strong>Progress after <a href=""http://stats.stackexchange.com/a/208463/112794"">donlelek's answer</a> (=mixed model not required):</strong></p>

<p>Just to be clear: The treatments were measured in different pieces of metal/plastic. So every piece is exactly measured twice - before and after the treatment. Thus, we are aiming for the ANOVA on damage, I guess. I had the impression to lose informations by just substracting the pre-post values. I did further research in the literature (with my limited knowledge in statistics). But according to <a href=""https://pdfs.semanticscholar.org/b764/e331525ec9ba814b51ee890aea7f663e175d.pdf"" rel=""nofollow"">""Pretest-posttest designs and measurement of
change""</a> the use of such gain scores seems to be ok:</p>

<blockquote>
  <p>""First, contrary to the
  traditional misconception, the reliability of gain scores is high in many practical situations, particularly when the pre- and posttest scores do not have equal variance and equal reliability.""</p>
</blockquote>

<p>so we have the following model:</p>

<pre><code>library(tidyr)
library(dplyr)

data &lt;- read.csv(""http://pastebin.com/raw/G4D8dh1f"")    
data_wide  &lt;- data %&gt;% 
  spread(time, undamaged_area) %&gt;% 
  separate(specimen_id, c(""mat"", ""id"", ""tx"")) %&gt;% 
  mutate(damage = before - after, 
         unique_id = paste(mat, id, sep = ""_"")) %&gt;% 
  select(-mat, -tx, -id)

# model for full factorial with replications
mm2  &lt;- lm(damage ~ material * treatment , data = data_wide)
</code></pre>

<p>The variance problem still remains. Confirmed by Levene's test:</p>

<pre><code>library(car)
leveneTest(damage ~ material * treatment, data_wide)

# Levene's Test for Homogeneity of Variance (center = median)
#       Df F value    Pr(&gt;F)    
# group  9  4.8619 2.646e-05 ***
#       90                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1    
</code></pre>

<p>Following the link suggested by donlelek I found different approaches for <a href=""http://stats.stackexchange.com/questions/91872/alternatives-to-one-way-anova-for-heteroskedastic-data/91881#91881"">anovas with heteroskedastic data</a>. I tried to stabilize the variance by using log-transformation. Then Levene's test says that heterogeneity of the variance diappears:</p>

<pre><code>data_wide &lt;- within(data_wide, log_damage &lt;- log(damage+1))
leveneTest(log_damage ~ material * treatment, data_wide)

# Levene's Test for Homogeneity of Variance (center = median)
#       Df F value Pr(&gt;F)
# group  9  0.6916 0.7147
#       90
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1   
</code></pre>

<p>The diagnostic plots seems not to be as weird as the previous ones (see <a href=""http://stats.stackexchange.com/a/208463/112794"">donlelek's answer</a>):</p>

<pre><code>mm3 &lt;- lm(log_damage ~ material * treatment, data_wide)
plot(fitted(mm3), residuals(mm3, type = ""pearson""))
qqnorm(residuals(mm3, type = ""pearson""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/NDTAj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NDTAj.png"" alt=""enter image description here""></a></p>

<p>The anova table gives the following output:</p>

<pre><code>anova(mm3)

# Analysis of Variance Table
#
# Response: log_damage
#                    Df Sum Sq Mean Sq F value    Pr(&gt;F)    
# material            1  0.436  0.4362  0.7462      0.39    
# treatment           4 83.652 20.9129 35.7786 &lt; 2.2e-16 ***
# material:treatment  4 20.213  5.0532  8.6452 5.966e-06 ***
# Residuals          90 52.606  0.5845                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Just for double checking:</p>

<p>The <code>Anova</code> function from <code>car</code> package offers an option for heteroscedasticity correction. Interestingly, this function generates a roughly similar table for the ""non-transformed"" <code>mm2</code>:</p>

<pre><code>Anova(mm2, white.adjust=TRUE)

# Analysis of Deviance Table (Type II tests)
# 
# Response: damage
#                    Df       F    Pr(&gt;F)    
# material            1  1.4251    0.2357    
# treatment           4 28.2422 3.329e-15 ***
# material:treatment  4  9.5739 1.701e-06 ***
# Residuals          90                      
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><strong>New questions:</strong> </p>

<p>This double check gives me more confidence in the results. But do you think that the log-transformation is a reasonable approach? Can I trust the model now?</p>
"
"0.10562681853293","0.107724577173552","208913","<p>I have a repeated data on which Iâ€™d like to apply <code>lmer</code> from package <code>lme4</code>.</p>

<p>My data: every quarter of a certain year, the dependent and a independent continuous variables (<strong><em>y</em></strong> and <strong><em>x</em></strong>) of my subjects are measured. Some of those subjects receive a treatment starting in a random period (the treatment goes on for the following periods). For instance, if subject <strong><em>i</em></strong> receive the treatment on the 2 quarter, heâ€™ll also receive it on quarters 3 and 4. A comparable example of my data can be generated with the following code:</p>

<pre><code>require(""dplyr"")

set.seed(321)

nsubj = 100
subject = paste0(""subj"",1:nsubj)
time  = 1:4
trtmt.time = data.frame(subject = subject, time = sample(time,nsubj,replace = TRUE), trtmt = sample(0:1,nsubj,replace = TRUE))
df.tmp = expand.grid(subject = subject,time = time)
df.tmp = merge(x = df.tmp, y = trtmt.time,by=c(""subject"",""time""), all.x = TRUE)
df.tmp[is.na(df.tmp)] = 0

df = data.frame(mutate(group_by(df.tmp,subject), trtmt=cumsum(trtmt)))
df$x = rnorm(nrow(df),10,2)
    df$y = rnorm(nrow(df),150,30)

### A little extract from the data:
# Subject3 didn't receive the treatement
# Subject4 started receiving the treatement from period 2
&gt; print(df[9:16,])

   subject time trtmt         x        y
9    subj3    1     0  9.572928 185.1341
10   subj3    2     0  9.314505 200.2399
11   subj3    3     0  6.872656 114.9252
12   subj3    4     0 11.039379  89.4818
13   subj4    1     0 10.499021 127.7390
14   subj4    2     1 10.849371 110.1334
15   subj4    3     1 13.086679 173.4492
16   subj4    4     1 10.675099 161.6651
</code></pre>

<p>My final goal is to test if the treatment is effective.  Hereâ€™s how I proceeded: first I test if <strong><em>time</em></strong> and <strong><em>x</em></strong> are significant on the model:</p>

<pre><code>require(lme4)
# Baseline model
baseline   &lt;- lmer(y ~ (1|subject), df, REML=FALSE)

# Compact model
model.fit1  &lt;- lmer(y ~ x+time+(time|subject), df, REML=FALSE)
anova(baseline,model.fit1)
</code></pre>

<p>Of course, with the example data I provided here, the comparison between the models says that the model with <strong><em>time</em></strong> and <strong><em>x</em></strong> is not better. However, this is not the case for my real data (p-value&lt;0.001). Next, I compare model.fit1 with a model including the treatment:</p>

<pre><code># Augmented model
model.fit2 &lt;- lmer(y ~ trtmt+x+time+(time|subject), df, REML=FALSE)
anova(model.fit1,model.fit2)
</code></pre>

<p>Since Iâ€™m new on repeated analysis, I was wondering if what Iâ€™m doing is a good approach. Should I include <strong><em>x</em></strong> on the random part of the model?</p>
"
"0.0865981222196073","0.0630842636753721","209939","<p>I would be extremely grateful for some advice on how to correctly fit linear mixed effects models with my repeated measures design!</p>

<p>In my experiment, subjects completed a task with 3 difficulty conditions: easy, medium, and hard. In addition, I have assessed subjects' depressive symptoms on a continuous scale. The outcome measure is accuracy.</p>

<p>""Medium"" here serves as a comparison condition. I hypothesized that depressive symptom severity would moderate the impact of difficulty on accuracy, such that for individuals who are low in depressive symptoms, difficulty would have little impact on accuracy. By contrast, I hypothesized that individuals who are high in depressive symptoms would perform worse during hard rounds and better during easy rounds. Thus, I planned to examine the interaction between difficulty and depressive symptoms.</p>

<p>To accomplish this analysis, my thought was to fit linear mixed effects models using lme4 package in R -- a full model and a reduced model. Then I would implement a likelihood ratio test. I planned to model both random slopes and random intercepts for subjects.</p>

<p>Here's how I would have thought to examine the interaction of a 2-level within-subjects factor (dummy coded) and a centered continuous predictor:</p>

<pre><code>full.model &lt;- lmer(accuracy ~ dummy_difficulty * depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
reduced.model &lt;- lmer(accuracy ~ dummy_difficulty + depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
anova(reduced.model, full.model)
</code></pre>

<p>However, my difficulty factor actually has 3 levels.  Since the ""medium"" condition is the comparison condition, I created two dummy variables as follows:</p>

<blockquote>
  <p>dummy_1: easy = 1, medium = -1, hard = 0</p>
  
  <p>dummy_2: easy = 0, medium = -1, hard = 1</p>
</blockquote>

<p>But now, with the two dummy variables, I'm at a loss as to how to model random slopes and random intercepts.  Can anyone help me out?  I would really, really appreciate any advice you might have to offer!</p>
"
"0.117363131703255","0.107724577173552","210001","<p>I have two data sets (<code>base</code> and <code>to_match</code>), each with 10 individuals, grouped in 2 classes. Each individual is described by a set of 4 variables. </p>

<p>What I want to do is: </p>

<ol>
<li>test wether the groups in the first dataset (<code>base</code>) are identical, based on all the describing variables</li>
<li>for each group in the second dataset (<code>to_match</code>) find the best matching group in the first dataset (<code>base</code>). </li>
</ol>

<p>So far, what I did, was to perform a MANOVA within the first dataset, to determine if the groups were identical or not. </p>

<p>Then, I did a Linear Discriminant Analysis on the first group and used the resulting model to predict, for each individual of the second dataset, what was the closest group in the first dataset. </p>

<p>The <code>base</code> dataset looks like this</p>

<pre><code>Group   A   B   C   D
1   0.457713143 -0.961504141    0.569530865 -0.467462304
1   -0.636764605    -0.695107438    0.210832138 -0.602475976
1   -1.216053575    0.647085589 0.42723523  0.024371887
1   -1.143872508    -0.771171997    1.610054266 0.862983524
1   -0.947740051    -0.96552701 -0.528481972    -0.157774001
2   -0.446452415    -0.555949371    -0.392508973    -0.465565853
2   1.143621911 -0.083821489    -1.174028532    -0.307616562
2   -0.118523439    2.250822002 -0.423022806    -0.342627702
2   -0.119453796    2.251860651 -0.460992853    -0.412183789
2   -1.119923882    0.945486343 1.269202026 -1.005019157
</code></pre>

<p>The <code>to_match</code>dataset looks like this:</p>

<pre><code>Group   A   B   C   D
1   0.778450123 -1.245864489    -0.688726943    -0.365538752
1   -1.177318015    0.059801545 0.259885094 0.453012798
1   -1.442516109    -0.422214798    -0.563490254    0.12831251
1   -0.639054602    -0.290063747    -1.249974299    1.473130636
1   -0.334179518    -1.006135106    0.30382184  2.093512163
2   -0.441086171    -0.494222266    -0.346210044    -0.394250031
2   -0.426666213    -0.444327313    -0.350570961    -0.437023047
2   0.382495524 -1.716667725    -1.040363139    0.544599656
2   -0.51828116 -0.757302352    -0.2163689  0.776728601
2   -0.497205151    -0.364979901    -0.632382926    0.222393228
</code></pre>

<p>What I have done so far:</p>

<pre><code>library(MASS)

# Load Datasets
base  &lt;- read.table(""base.txt"", header = T)
to_match  &lt;- read.table(""to_match.txt"", header = T)

# MANOVA analysis
fit &lt;- manova(cbind(A, B, C, D) ~ Group,  base) # Stele was removed
summary(fit)

# LDA analysis
fit &lt;- lda(Group ~ ., data=base, CV=F)

# Predict the class of each individual from the to_match dataset
fit.p &lt;- predict(fit, newdata = to_match[,-1])
</code></pre>

<p>So what I have been able to do so far is to match individuals from the <code>to_match</code>dataset to group in the <code>base</code> dataset using the <code>lda</code> and <code>predict</code> functions from the <code>MASS</code>package. </p>

<p>What I would like to do is <strong>not the match individuals, but the groups directly</strong>. Is this possible? </p>
"
"0.0909090909090909","0.092714554082312","211642","<p>I have a question according the following example:</p>

<p>What I want to find out is whether two fertilizers (A and B) have different effects on the biomass of my plants. My explanatory variable is 'fertilizer' (categorial) with the levels 'A' and 'B'. My response variable is 'biomass' (continuous). Besides that, I have two other continuous variables 'seed mass' of the plants (measured before they germinated) and 'growth duration' (=harvesting date minus germination date). These two variables are expected explain some variance in my data and therefore I want to include them as covariates. (For each of the two levels I have 30 plants without any missing values.)</p>

<p>I read that my factor is a fixed effect and the covariates are random effects, as they represent a random sample out of the natural population. So I would do an ANCOVA in R using the lmer function (package lmerTest) like this:</p>

<pre><code>model &lt;- lmer(biomass~fertilizer+(1|seed_mass)+(1|growth_duration), data=dataset)
anova(model)
</code></pre>

<p>My question - are my considerations and the way I'm performing the analysis correct? Or is this analysis not appropriate for my question? My special concern is about the covariates, if I should include them in a different way in the model.</p>
"
"0.199973338665482","0.197146667211603","212533","<p>Third Update: Output from suggested code:</p>

<pre><code>&gt; fit1&lt;- lm(cbind(Risk_Pct, PCT_Stocks_MF_1) ~ US_Born, regdata)
&gt; summary(fit1)
Response Risk_Pct :
</code></pre>

<p>Call:
lm(formula = Risk_Pct ~ US_Born, data = regdata)</p>

<p>Residuals:
   Min     1Q Median     3Q    Max 
-6.527 -1.319  0.681  1.681 91.681 </p>

<p>Coefficients:
            Estimate Std. Error t value Pr(>|t|)<br>
(Intercept)  6.26699    0.05146 121.777   &lt;2e-16 ***</p>

<h2>US_Born      0.05210    0.03113   1.673   0.0943 .</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Residual standard error: 2.289 on 5041 degrees of freedom</p>

<p>(10957 observations deleted due to missingness)
Multiple R-squared:  0.0005551, Adjusted R-squared:  0.0003569 
F-statistic:   2.8 on 1 and 5041 DF,  p-value: 0.09432</p>

<p>Response PCT_Stocks_MF_1 :</p>

<p>Call:
lm(formula = PCT_Stocks_MF_1 ~ US_Born, data = regdata)</p>

<p>Residuals:
   Min     1Q Median     3Q    Max 
-229.2 -155.2 -130.2 -130.2  812.7 </p>

<p>Coefficients:
            Estimate Std. Error t value Pr(>|t|)<br>
(Intercept)  241.195      7.577  31.831   &lt;2e-16 ***</p>

<h2>US_Born      -10.976      4.584  -2.394   0.0167 *</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Residual standard error: 337 on 5041 degrees of freedom
  (10957 observations deleted due to missingness)
Multiple R-squared:  0.001136,  Adjusted R-squared:  0.0009379 
F-statistic: 5.733 on 1 and 5041 DF,  p-value: 0.01668</p>

<p><strong>I think a problem with this may be that in the raw data for ""PCT_Stocks_MF_1 there codes for ""don't know"" and ""refused to answer"" are given values of 998/999 which brings up the mean and messes with the results since no one actually has 998% of their assets invested in stocks</strong></p>

<hr>

<p>Second Update: Sample of 100 rows of the data</p>

<pre><code> regdata &lt;- data.frame(HHID, US_Born, Born_In_US, Risk_Pct, PCT_Stocks_MF_1, Stocks_Pct, age, gender, Own_Home, Marital_Status, current_job_status,Total_Wealth,stock_market_expectations )
</code></pre>

<blockquote>
<pre><code>head(regdata, n = 100)
</code></pre>
</blockquote>

<pre><code>      HHID US_Born Born_In_US Risk_Pct PCT_Stocks_MF_1 Stocks_Pct
1   010004       1         NA        7              50         NA
2   010013       1         NA       10              NA         50
3   010038       1         NA        8              40         65
4   010038       1         NA        8              40         NA
5   010038       1         NA        8              40         85
6   010050       1         NA        5             998        998
7   010050       1         NA        5             998         NA
8   010325       1         NA        2             998         NA
9   010397       1         NA        3              75         NA
10  010397       1         NA        3              75        100
11  010433       1         NA        5              NA         NA
12  010451       5         NA        6              NA         50
13  010451       5         NA        6              NA         50
14  010451       5         NA        2              NA         NA
15  010481       1         NA        5             998         NA
16  010481       1         NA        5             998         NA
17  010481       1         NA        5             998         NA
18  010565       1         NA        7              NA         NA
19  010565       1         NA        7              NA         NA
20  010565       1         NA        7              NA         NA
21  010577       1         NA        5              NA         NA
22  010592       5         NA        8              NA         NA
23  010592       5         NA        8              NA         NA
24  010611       1         NA        0              NA         NA
25  010645       1         NA        4              NA         NA
26  010645       1         NA        4              NA         NA
27  010648       1         NA        3              NA         NA
28  010648       1         NA        5              NA         NA
29  010696       5         NA       NA              NA         NA
30  010769       1         NA        4              60         NA
31  010769       1         NA        4              60         50
32  010773       5         NA        2              50         NA
33  010773       5         NA        7              50          0
34  010773       5         NA        7              50         NA
35  010893       1         NA        6              NA         30
36  010893       1         NA        6              NA        100
37  010893       1         NA        6              NA         NA
38  010893       1         NA        6              NA         NA
39  010893       1         NA        6              NA         NA
40  010962       1         NA        5              NA         NA
41  010989       1         NA        4              NA         NA
42  010989       1         NA        4              NA         NA
43  011067       1         NA        8             998         NA
44  011256       1         NA        5              NA         NA
45  011332       1         NA        2              NA         NA
46  011341       1         NA        5              80        998
47  011341       1         NA        5              80         NA
48  011377       1         NA        5              NA        998
49  011377       1         NA        5              NA         NA
50  011377       1         NA        5              NA         NA
51  011377       1         NA        5              NA         NA
52  011378       5         NA        5              NA         NA
53  011466       1         NA        6              NA         NA
54  011620       1         NA        8             100        100
55  011620       1         NA        8             100         NA
56  011620       1         NA        8             100         NA
57  011620       1         NA        8             100         60
58  011620       1         NA        8             100         60
59  011626       5         NA        3              NA         NA
60  011626       5         NA        3              NA        998
61  011802       1         NA       10              NA         NA
62  011802       1         NA       10              NA         NA
63  011802       1         NA        8              NA        100
64  011802       1         NA        8              NA         NA
65  011802       1         NA        8              NA         NA
66  011810       1         NA       10              NA        999
67  011810       1         NA       10              NA         NA
68  011841       1         NA       10              NA        998
69  011881       5         NA        5              NA        100
70  011881       5         NA        5              NA        998
71  011902       1         NA        0              NA        999
72  011902       1         NA        0              NA         NA
73  011902       1         NA        0              NA         NA
74  011911       1         NA        7              NA        998
75  011911       1         NA        7              NA         NA
76  011911       1         NA        7              NA        998
77  011911       1         NA        7              NA         NA
78  011936       1         NA        6              NA          0
79  011936       1         NA        6              NA          0
80  011936       1         NA        6              NA         NA
81  011983       1         NA        8              NA         25
82  011983       1         NA        8              NA         NA
83  011999       1         NA        7              NA         NA
84  012005       1         NA        5              NA         NA
85  012005       1         NA        5              NA        998
86  012009       1         NA        3             998         50
87  012009       5         NA        6             998        998
88  012009       5         NA        6             998         NA
89  012033       1         NA        5             998         NA
90  012033       1         NA        0              NA          0
91  012104       1         NA        5              NA        998
92  012104       1         NA        5              NA          0
93  012112       1         NA        6              NA          0
94  012112       1         NA        6              NA        998
95  012161       1         NA        2              NA         NA
96  012161       1         NA        2              NA         NA
97  012161       1         NA        2              NA         NA
98  012166       1         NA       NA              NA         NA
99  012166       1         NA       NA              NA         NA
100 012166       1         NA        6              NA         NA
age gender Own_Home Marital_Status current_job_status
1    68      2        1              5                  5
2    76      1        2              4                  5
3    71      2        1              1                  1
4    71      2        1              1                  1
5    71      2        1              1                  1
6    73      2        1              5                  1
7    73      2        1              5                  1
8    75      2        2              5                  5
9    73      1        1              5                  1
10   73      1        1              5                  1
11   80      2        3              5                  5
12   76      2        1              1                  5
13   76      2        1              1                  5
14   74      1        1              1                  5
15   74      2        2              1                  5
16   74      2        2              1                  5
17   74      2        2              1                  5
18   82      1        7              5                  7
19   82      1        7              5                  7
20   82      1        7              5                  7
21   75      2        1              5                  5
22   77      2        1              6                  4
23   77      2        1              6                  4
24   73      2        2              6                  5
25   67      2        2              5                  5
26   67      2        2              5                  5
27   73      1        1              1                  5
28   74      2        1              1                  5
29   73      2        1              4                  5
30   58      2        1              5                  6
31   58      2        1              5                  6
32   77      2        1              1                  5
33   86      1        1              1                  5
34   86      1        1              1                  5
35   74      2        1              4                  1
36   74      2        1              4                  1
37   74      2        1              4                  1
38   74      2        1              4                  1
39   74      2        1              4                  1
40   74      2        1              5                  1
41   73      2        1              1                  5
42   73      2        1              1                  5
43   72      1        1              1                  5
44   73      1        2              1                  1
45   74      2        1              4                  5
46   75      2        1              5                  2
47   75      2        1              5                  2
48   68      2        1              1                  3
49   68      2        1              1                  3
50   68      2        1              1                  3
51   68      2        1              1                  3
52   77      1       NA              3                  5
53   62      2        2              6                  4
54   73      1       NA              1                  5
55   73      1       NA              1                  5
56   55      2       NA              1                  1
57   55      2       NA              1                  1
58   55      2       NA              1                  1
59   65      2        1              1                  6
60   65      2        1              1                  6
61   80      1        1              1                  5
62   80      1        1              1                  5
63   58      2        1              1                  1
64   58      2        1              1                  1
65   58      2        1              1                  1
66   76      1        1              1                  5
67   76      1        1              1                  5
68   78      1        1              1                  5
69   69      2        1              4                  6
70   69      2        1              4                  6
71   66      2        2              6                  5
72   66      2        2              6                  5
73   66      2        2              6                  5
74   75      1        1              1                  1
75   75      1        1              1                  1
76   75      1        1              1                  1
77   75      1        1              1                  1
78   75      2       NA              4                  5
79   75      2       NA              4                  5
80   75      2       NA              4                  5
81   71      1        1              1                  5
82   71      1        1              1                  5
83   77      1        1              1                  5
84   81      2        2              5                  1
85   81      2        2              5                  1
86   74      2        1              1                  6
87   87      1        1              1                  5
88   87      1        1              1                  5
89   73      1       NA              3                  5
90   73      2       NA              3                  5
91   79      2        1              5                  6
92   79      2        1              5                  6
93   66      2        2              5                  5
94   66      2        2              5                  5
95   76      2        7              5                  5
96   76      2        7              5                  5
97   76      2        7              5                  5
98   76      1        1              1                  5
99   76      1        1              1                  5
100  69      2        1              1                  5
    Total_Wealth stock_market_expectations
1         901001                        NA
2           2000                        NA
3             NA                        NA
4             NA                        NA
5             NA                        NA
6        1224150                        75
7        1224150                        75
8          20000                        NA
9        1390000                        30
10       1390000                        30
11        196000                        50
12        194000                         5
13        194000                         5
14            NA                        80
15            NA                        10
16            NA                        10
17            NA                        10
18         51500                        NA
19         51500                        NA
20         51500                        NA
21         -2955                        NA
22        372000                        NA
23        372000                        NA
24          -925                        NA
25          4400                       100
26          4400                       100
27        303000                        20
28            NA                        50
29            NA                        NA
30            NA                        NA
31            NA                        NA
32            NA                        NA
33        304000                        NA
34        304000                        NA
35       1701000                        NA
36       1701000                        NA
37       1701000                        NA
38       1701000                        NA
39       1701000                        NA
40        -80500                         0
41            NA                        50
42            NA                        50
43       1072000                        60
44         22050                        NA
45            NA                        NA
46        135740                        NA
47        135740                        NA
48            NA                        NA
49            NA                        NA
50            NA                        NA
51            NA                        NA
52        112500                        50
53             0                        NA
54        400012                       100
55        400012                       100
56            NA                        NA
57            NA                        NA
58            NA                        NA
59        153700                        NA
60        153700                        NA
61        106000                        40
62        106000                        40
63            NA                       100
64            NA                       100
65            NA                       100
66            NA                        80
67            NA                        80
68         20000                        60
69            NA                        NA
70            NA                        NA
71            NA                        NA
72            NA                        NA
73            NA                        NA
74        353000                        50
75        353000                        50
76        353000                        50
77        353000                        50
78            NA                        30
79            NA                        30
80            NA                        30
81        771500                        NA
82        771500                        NA
83        100000                        75
84         42200                        40
85         42200                        40
86       1760500                        50
87            NA                        NA
88            NA                        NA
89         49000                        NA
90        -10500                        NA
91         17500                        NA
92         17500                        NA
93         54000                        NA
94         54000                        NA
95        -31000                        10
96        -31000                        10
97        -31000                        10
98            NA                        NA
99            NA                        NA
100           NA                        50    
</code></pre>

<hr>

<p>Update:I tried to run a ""MANOVA"" but am not entirely sure if I did this correct. 
Risk_Pct is the raw data risk measure 1 and PCT_Stocks_MF_1 is the raw data risk measure 2 </p>

<pre><code>&gt; y&lt;-cbind(Risk_Pct, PCT_Stocks_MF_1)

&gt; fit.manova&lt;-manova(y ~ US_Born)
 summary(fit.manova, test = ""Pillai"")
        Df    Pillai approx F num Df
US_Born      1 0.0016159   4.0788      2
Residuals 5041                          
      den Df  Pr(&gt;F)  
US_Born     5040 0.01698 *
Residuals                 
---
Signif. codes:  
  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1
  â€˜ â€™ 1
&gt; summary(fit.manova, test = ""Roy"")
            Df       Roy approx F num Df
US_Born      1 0.0016186   4.0788      2
Residuals 5041                          
          den Df  Pr(&gt;F)  
US_Born     5040 0.01698 *
Residuals                 
---
Signif. codes:  
  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1
</code></pre>

<h2>Also, if I have some variables I want to hold constant (such as ""wealth"") is it possible to do that in MANOVA? If so, how?</h2>

<p>Original post:
I'm trying to determine if immigration status is a significant determinant of risk preferences. In order to do this, I'm using two different measures of risk.</p>

<p>The first measure of risk has participants rate their level of risk on a scale from 0-10. I then divide that scale into 5 categories of risk: ""no"", ""low"", ""some"", ""high"", and ""substantial"" risk tolerance. 
  The second measure of risk is based on the percentage(0-100%) of assets participants invest in stocks. They are also divided into the same 5 categories of risk: ""no"", ""low"", ""some"", ""high"", and ""substantial"" risk tolerance.
In my preliminary analysis I found that using the first measure of risk I found that for both immigrants and natives the category with the most respondents was the ""high"" risk tolerance. Meanwhile, with the second measure of risk I found that for both immigrants and natives the category with the most respondents was ""substantial"" risk tolerance.
 How can I determine if there is a significant difference between these two measures of risk? I knew typically to find a significant difference you would use a t-test, but since they variables are categorical I can't find the mean. Even if I used the original data, not my groupings into risk tolerance data, the first variable is on  a scale from 0-10 and the second variable is on a scale of 0-100 so the means are totally different. I am using ""R"" to do my analysis.</p>

<p>If it helps, here is a table of the the breakdown of risk measure 1, by immigration status and risk tolerance group:</p>

<pre><code>             No     Low     Some    High    Substantial     Total
Native      4.5     3.54    31.74   52.11   8.11            100
Immigrant   9.34    3.67    23.19   47.71   16.08           99.99
</code></pre>

<p>And the same table, for risk measure 2:</p>

<pre><code>            No      Low     Some     High   Substantial DK/RF   Total
Native      0.08%   1.81%   6.51%   17.38%  57.64%      16.58%  100.00%
Immigrant   0.00%   4.32%   14.59%  20.27%  48.65%      12.16%  99.99%
</code></pre>

<ul>
<li>I don't think the tables show correctly when printed like this, so the attached image is of these two tables <a href=""http://i.stack.imgur.com/HsK74.png"" rel=""nofollow"">enter image description here</a>
Thanks so much for any help on this situation</li>
</ul>
"
"0.0829882662886615","0.0846364211331916","212592","<p>I am trying to find out whether it is true that variation in expenditure is greater, for more narrow subsets. e.g. is it more likely that an individual buys an orange instead of an apple, than it is for him to buy a potato instead of an apple.</p>

<p>So far i have used var() and f-tests(while i believe f-tests aren't really applicable to my problem either). I would however like to use more applicable models if that is possible. Every test or class of models i can find however, are meant to compare two samples' variances. While I want to test for each observation, whether they are more likely to go for alternatives, the more constrained the subset is.</p>

<p>I have a dataframe, with variables on multiple levels (drinks, soda, types of coke), and two time points of observation. As far as I am aware, two time points are not enough for a time-series analysis, and a repeated measures anova is not really applicable either.</p>

<p>Could anyone point me to a resource or name of a test / type of model that could help me solve the problem? or, are var() and var.test() the only tools that are applicable?</p>
"
"0.0642824346533225","0.0655590899062897","212886","<p>I'm trying to analyse a dataset in R, looking like this:</p>

<pre><code>str(data1m): 
'data.frame':   3360 obs. of  6 variables:
 $ Concentration: Factor w/ 7 levels ""0 ppb"",""50 ppb"",..: 2 7 7 1 7 1 2 1 7 1 ...
 $ Sex          : Factor w/ 2 levels ""f"",""m"": 1 2 2 2 1 2 2 1 1 2 ...
 $ Line         : Factor w/ 4 levels ""20"",""23"",""40"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Temp         : Factor w/ 2 levels ""23"",""29"": 1 1 1 1 1 1 1 1 1 1 ...
 $ Time     : Factor w/ 6 levels ""0,5 sec"",""1 sec"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ value        : num  107.2 46.2 58.1 75.9 87.8 ...
</code></pre>

<p>So as you can see the dataset consist of 5 factors and 1 response value, i have made an ANOVA model in R</p>

<pre><code>aov1 &lt;- aov(value ~ Concentration*Sex*Line*Temp*Time, data=data1m)
</code></pre>

<p>What i would like to know is, is there a similar analysis to PCA that i can apply to my dataset to somehow show the effect of the 5 factors, i have multiple interactions so to me it doesn't really make a lot of sense to look at main effects.
EDIT: added the results of the anova</p>

<p><a href=""http://i.stack.imgur.com/QNVPP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QNVPP.png"" alt=""enter image description here""></a></p>
"
"0.104972776216296","0.0802931591358284","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.145778859224959","0.15693371016017","213592","<p>I prepared a mixed 2x2 ANOVA design analysis both in SPSS and in R. The SPSS script is correct, but in R script there is a mistake somewhere. To test that I generated artificial data from a normal distribution to simulate the interaction between two independent variables. There were no difference between the results in main effects, but results of simple effects analysis do not match when comparing between levels of variable which introduced repeated measures (GROUP A: PRE vs POST ; GROUP B: PRE vs POST).</p>

<p>I would be very thankful if you can help me.
The code below will do everything for you.</p>

<p><strong>Here is the code in R which:</strong>
- generates the data
- calculates mixed ANOVA
- prepares data to csv format to import to SPSS
- performs simple effect analysis (there is probably a mistake)</p>

<pre><code>N &lt;- 100
absMean &lt;- 1
sdCustom &lt;- 5

grA_pre &lt;- data.frame(ID = seq(N), lvl=rnorm(N, mean=absMean, sd=sdCustom), group=factor('A'), stage = factor('pre'))
grA_post &lt;- data.frame(ID = seq(N), lvl=rnorm(N, mean=-absMean, sd=sdCustom), group=factor('A'), stage = factor('post'))
grB_pre &lt;- data.frame(ID = seq(N+1,2*N), lvl=rnorm(N, mean=-absMean, sd=sdCustom), group=factor('B'), stage = factor('pre'))
grB_post &lt;- data.frame(ID = seq(N+1,2*N), lvl=rnorm(N, mean=absMean, sd=sdCustom), group=factor('B'), stage = factor('post'))

gr &lt;- rbind(grA_pre, grA_post, grB_pre, grB_post)
names(gr)
head(gr)

# save set to .csv to import to SPSS 
grSPSS &lt;- reshape(data = gr, timevar = ""stage"", idvar = c(""ID"", ""group""), direction = ""wide"")

write.csv2(grSPSS, file = ""sample2.csv"")

library(ggplot2)
library(plyr)
library(ez)

print(""Omnibus mixed ANOVA - main effects and interactions"")
ezPlot(data = gr, wid = ID, dv = lvl, between = group, within = stage, type = ""III"", x = group, split = stage, x_lab = ""Group"", y_lab = ""Level of experience"")
ezANOVA(data = gr, wid = ID, dv = lvl, between = group, within = stage, detailed = TRUE, type = ""III"")
#ezStats(data = gr, wid = ID, dv = lvl, between = group, within = stage, type = ""III"")


print(""Simple main effects analysis"")
dataA &lt;- subset(gr, group == ""A"" )
dataB &lt;- subset(gr, group == ""B"" )
dataPRE &lt;- subset(gr, stage == ""pre"" )
dataPOST &lt;- subset(gr, stage == ""post"" )

print(""GROUP = A: PRE vs POST"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataA, dv = lvl, wid = ID, within = stage, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)

print(""GROUP = B: PRE vs POST"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataB, dv = lvl, wid = ID, within = stage, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)

print(""STAGE = PRE: A vs B"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataPRE, dv = lvl, wid = ID, between = group, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)

print(""STAGE = POST: A vs B"")
simpleEffControlANOVA &lt;- ezANOVA(data = dataPOST, dv = lvl, wid = ID, between = group, detailed = TRUE, type = ""III"" )
print(simpleEffControlANOVA)
</code></pre>

<p><strong>Here is the code for SPSS Syntax which:</strong>
- calculates everything on imported data, generated by R</p>

<pre><code>DATASET ACTIVATE DataSet1.
GLM lvl.pre lvl.post BY group
  /WSFACTOR=stage 2 Polynomial 
  /METHOD=SSTYPE(3)
  /POSTHOC=group(TUKEY T3) 
  /EMMEANS=TABLES(group) COMPARE ADJ(BONFERRONI)
  /EMMEANS=TABLES(stage) COMPARE ADJ(BONFERRONI)
  /EMMEANS=TABLES(group*stage) COMPARE(group)
  /EMMEANS=TABLES(group*stage) COMPARE(stage)
  /PLOT=PROFILE(group*stage)
  /PRINT=DESCRIPTIVE ETASQ OPOWER HOMOGENEITY 
  /CRITERIA=ALPHA(.05)
  /WSDESIGN=stage 
  /DESIGN=group.
</code></pre>
"
"0.0757575757575758","0.092714554082312","213804","<p>I am running some linear regressions in R. I am dealing with a linear dependent and linear as well as categorical independent variables using <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html"" rel=""nofollow"">lm</a>. So far, I have looked at the output that <code>summary(model)</code> gives me. </p>

<p>Other studies instead run <a href=""http://www.inside-r.org/packages/cran/car/docs/Anova"" rel=""nofollow"">Anova()</a> from the <a href=""https://cran.r-project.org/web/packages/car/index.html"" rel=""nofollow"">car</a> package on their linear model, which returns a similar table. The docs for <code>Anova()</code> state that it</p>

<blockquote>
  <p>Calculates type-II or type-III analysis-of-variance tables for model objects. </p>
</blockquote>

<p>I am under the impression that this <code>Anova()</code> returns an F instead of the t-statistic but is ~ equivalent in what its tell me. (sample output below). So I was wondering</p>

<ul>
<li><p>Are standard R <code>summary(lm)</code> and car <code>Anova(lm)</code> indeed doing pretty much the same calculations here? If not, what is the difference?</p></li>
<li><p>They both report the same p-value, however the F-statistic at the bottom of the standard output is different from the <code>Anova()</code> one. Why is that?</p></li>
<li><p>What are applications where one would choose one over the other?</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>Sample output:</p>

<p>Standard R</p>

<pre><code>summary(linreg)
...
         Estimate    t value    Pr(&gt;|t|)
Age      -18.016     -3.917     0.000107
Gender   -45.4912    -4.916     1.35e-06
---
Residual standard error: 85.81 on 359 degrees of freedom
F-statistic: 16.71 on 2 and 359 DF, p-value: 1.147e-07
</code></pre>

<p>Anova() output</p>

<pre><code>Anova(linreg)

Anova Table (Type II tests)

           Sum Sq    F value   Pr (&gt;F)
Age        112997    15.345    0.0001072
Gender     1777936   24.164    1.348e-06
</code></pre>
"
"0.0454545454545455","0.092714554082312","214213","<p><strong>Question</strong>: </p>
<strong>Is there an equivalent of Nested -ANOVA for data which fails ANOVA requirement? </p>
Is it possible to perform in R?</strong></p>

<p>Many thanks, Savani</p>

<hr>

<p>I am doing an analysis of large data set.</p>

<ul>
<li>I have three groups (Genotypes: Wild type, Heterozygous , Homozygous mutant).</li>
<li>Each group has data from >10 animals.</li>
<li>For each animal, I have 500+ <em>independent</em> observations (example: Volume of 500 different retinal cells).</li>
</ul>

<p>Based on Levene's test between the three groups, I see I cannot do ANOVA. I have decided to do a Welch ANOVA and I do see significant difference. (I used the <code>oneway.test()</code> function.)</p>

<p>As my 500+ observations are nested inside individual animals, I think I should perform <em>nested</em> Welch ANOVA for my data set.</p>

<p>How can I do nested Welch ANOVA in R?</p>

<p>Could you please show me an example with a similar data?</p>

<pre><code>mydata &lt;- read.table(header=TRUE,
text=
""Tech Rat Protein
Janet 1   1.119 
Janet 1   1.2996 
Janet 1   1.5407 
Janet 1   1.5084 
Janet 1   1.6181 
Janet 1   1.5962 
Janet 1   1.2617 
Janet 1   1.2288 
Janet 1   1.3471 
Janet 1   1.0206 
Janet 2   1.045 
Janet 2   1.1418 
Janet 2   1.2569 
Janet 2   0.6191 
Janet 2   1.4823 
Janet 2   0.8991 
Janet 2   0.8365 
Janet 2   1.2898 
Janet 2   1.1821 
Janet 2   0.9177 
Janet 3   0.9873 
Janet 3   0.9873 
Janet 3   0.8714 
Janet 3   0.9452 
Janet 3   1.1186 
Janet 3   1.2909 
Janet 3   1.1502 
Janet 3   1.1635 
Janet 3   1.151 
Janet 3   0.9367 
Brad  5   1.3883 
Brad  5   1.104 
Brad  5   1.1581 
Brad  5   1.319 
Brad  5   1.1803 
Brad  5   0.8738 
Brad  5   1.387 
Brad  5   1.301 
Brad  5   1.3925 
Brad  5   1.0832 
Brad  6   1.3952 
Brad  6   0.9714 
Brad  6   1.3972 
Brad  6   1.5369 
Brad  6   1.3727 
Brad  6   1.2909 
Brad  6   1.1874 
Brad  6   1.1374 
Brad  6   1.0647 
Brad  6   0.9486 
Brad  7   1.2574 
Brad  7   1.0295 
Brad  7   1.1941 
Brad  7   1.0759 
Brad  7   1.3249 
Brad  7   0.9494 
Brad  7   1.1041 
Brad  7   1.1575 
Brad  7   1.294 
Brad  7   1.4543 
"")
</code></pre>
"
"0.153022802096395","0.137701631884169","214449","<p>I am trying to determine if immigration status is a determinant of risk preferences. To do this, I am using the 2014 Health and Retirement Study data which has approximately ~20,000 participants and is representative of residents in the US over age 50. I have two different measures of risk preferences that I am using in this analysis.</p>

<p>The first risk measure is based on a 0-10 rating that participants gave themselves for how risky they are in general situations. The regression using this risk measure looks like this:</p>

<pre><code>fit1_usesRaw &lt;-vglm(Risk_Pct ~ is.native + is.male + oh + cjs + ms + age2 + tw,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"", ""is.male"", and ""ms""(marital status). Is.native has a positive coefficient and is.male and ms each have negative coefficients. </p>

<p>The second risk measure is based on the percentage of the participant's retirement accounts kept in stocks. The regression for this risk measure looks like this:</p>

<pre><code>fit2_usesRaw &lt;-vglm(PCT_Stocks_MF_1 ~ is.native + is.male + oh + cjs + ms + age2 + tw + sme,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"" (negative coefficient), ""is.male"" (positive coefficient), ""age2"" (positive coefficient), ""tw"" (total wealth, negative coefficient), and ""cjs"" (current job status, negative coefficient). </p>

<p>How can I test to see if the differences between the two risk measures are significant? Is there any way to determine which risk measure is ""right""? I tried a ANOVA test, but I'm unsure if that would be correct. The output for the Anova was:</p>

<pre><code> Anova(w1.mod, test = ""Roy"")

Type II MANOVA Tests: Roy test statistic
          Df test stat approx F num Df den Df    Pr(&gt;F)    
is.native  1  0.008895    7.441      2   1673 0.0006063 ***
is.male    1  0.048695   40.733      2   1673 &lt; 2.2e-16 ***
age2       1  0.020496   17.145      2   1673  4.26e-08 ***
oh         1  0.000329    0.275      2   1673 0.7596626    
ms         1  0.002546    2.130      2   1673 0.1191937    
tw         1  0.002674    2.236      2   1673 0.1071594    
sme        1  0.000688    0.576      2   1673 0.5624582    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Does this tell me there are significant differences between ""is.native"", ""is.male"" and ""age2"" using the two different risk measures? Or am I reading this output completely incorrectly?</p>

<p>Thanks for any help!</p>
"
"0.0642824346533225","0.0655590899062897","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.0829882662886615","0.0677091369065533","215250","<p>I computed Bayes Factors for a repeated measures ANOVA in JASP and the R package BayesFactor. There were two between-subjects groups (factor ""group"") with multiple measurements. However, the results were different:</p>

<p>JASP output:</p>

<pre><code>Model comparison - dependent
Models      P(M)        P(M|data)       BF M            BF 10       % error
group       0.200       5.618eâ€‰-56      2.247eâ€‰-55      0.174       0.944

Analysis of Effects - dependent
Effects     P(incl)     P(incl|data)    BF Inclusion 
group       0.600       0.169           0.135
</code></pre>

<p>vs.</p>

<p>BayesFactor model:</p>

<pre><code>bf = anovaBF(val ~ group + subj, data = dat, + whichRandom=""subj"")
</code></pre>

<p>BayesFactor output:</p>

<pre><code>Bayes factor analysis
--------------
[1] group + subj : 0.1693252 Â±0.69%
</code></pre>

<p>This is my first Bayesian analysis, so I may be missing some obvious points. The analyses were performed according to the manuals though.</p>

<p><strong>Update:</strong> I now noticed that the BayesFactor output is identical to the P(incl|data) value provided by JASP. I am trying to find evidence for the null hypothesis. I want to report the BF 10, but I do not know how it relates to the BayesFactor output. I would be very thankful for a clarification which portions of the output are relevant.</p>
"
"0.148453923805041","0.141939593269587","215847","<p>I have no idea how to analyze this dataset.</p>

<p>I am asking if two genotypes, T and M, respond differently to a treatment, E2 (I also have a control, CON). All 36 animals were given both E2 and CON in a counterbalanced order and then their behavior was measured. The behavior was scored once every 30 seconds for 10 minutes as ""yes"" or ""no"" (coded ""1"" or ""0"").</p>

<p>I am interested in knowing if the treatment affects the genotypes differently and whether this effect changes over time.</p>

<p>I have tried running the ANOVA (see below) as a non-parametric test on my count data, but the data are not normally distributed and the results do not make sense.</p>

<pre><code>model&lt;-aov(behavior~genotype*treatment*time+Error(animal/(time*treatment)), data=dataset)
</code></pre>

<p>Therefore, I think that a mixed effects model is right. In this analysis, I think my fixed effects should be genotype and treatment. The random effects should be animal, treatment, and time. I also noted sex and age, but I'm not sure that matters for this analysis. The datafile is set up such that each line is a separate observation for each animal, every 30 seconds. See below:</p>

<pre><code>&gt;animal genotype sex age    treatment   time    behavior                                                                                
&gt;1403   T   F   AHY CON 0   1                                                                               
&gt;1404   T   F   AHY CON 0   1                                                                               
&gt;1406   T   F   HY  CON 0   1                                                                               
&gt;1407   T   F   AHY CON 0   1                                                                               
&gt;1423   T   F   AHY CON 0   1                                                                               
&gt;1425   T   F   HY  CON 0   1                                                                               
&gt;1428   T   F   AHY CON 0   1                                                                               
&gt;1431   T   F   AHY CON 0   1   
</code></pre>

<p>I have tried modeling the data using lme in R but I am not sure that I am nesting the random factors properly because the df for ""genotype"" is 28, but I only have 2 genotypes (so it should be df=1). This is my model:</p>

<pre><code>mixed.model1 &lt;- lme(fixed=behavior~genotype * treatment * time, 
random= ~ 1|animal/time/treatment, data=dataset)
summary(mixed.model1)

Linear mixed-effects model fit by REML
 Data: dataset 
       AIC      BIC    logLik
  2727.064 2779.666 -1351.532

Random effects:
 Formula: ~1 | animal
        (Intercept)
StdDev:    1.345537

 Formula: ~1 | time %in% animal
        (Intercept)
StdDev:   0.5004338

 Formula: ~1 | treatment %in% time %in% animal
        (Intercept)  Residual
StdDev:    2.030743 0.5704242

Fixed effects: behavior ~ genotype * treatment * time 
                           Value Std.Error  DF   t-value p-value
(Intercept)            2.1647059 0.4852943 296  4.460604  0.0000
genotypeT              0.3737557 0.7372150  28  0.506983  0.6161
treatmentE2           -0.3098039 0.4942422 296 -0.626826  0.5313
time                  -0.1465241 0.0578876 268 -2.531184  0.0119
genotypeT:treatmentE2  0.7969834 0.7508078 296  1.061501  0.2893
genotypeT:time        -0.1541752 0.0879375 268 -1.753236  0.0807
treatmentE2:time       0.0124777 0.0796543 296  0.156648  0.8756
genotypeT:treatmentE2:time -0.0367201 0.1210036 296 -0.303463  0.7618
 Correlation: 
                      (Intr) genoT treatE2 time gnW:E2 genoW: trtE2:
genoT                 -0.658                                          
treatE2               -0.509  0.335                                   
time                  -0.656  0.432  0.610                            
genoT:treatE2          0.335 -0.509 -0.658 -0.401                     
genoT:time             0.432 -0.656 -0.401 -0.658  0.610              
treatE2:time           0.451 -0.297 -0.886 -0.688  0.584  0.453       
genoT:treatE2:time    -0.297  0.451  0.584  0.453 -0.886 -0.688 -0.658

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-0.53883607 -0.11769447 -0.03528952  0.04858721  1.83383169 

Number of Observations: 600
Number of Groups: 
 animal        time %in% animal treat %in% time %in% animal
 30                         300                         600 
</code></pre>

<p>I am also concerned that the count data are not normally distributed. Should I use a GLM instead? If so, does that change the random effects? Please help. I have not seen any examples like this in any of the R books I have seen or on this blog.</p>

<p>Thanks in advance!!!</p>
"
"0.143739893644017","0.146594581573484","217511","<p>I want to compare several models built using the codes I have written in R for a mixed-effects model. I already knew that <code>anova()</code> function in car package provides <code>AIC</code>, which is a factor that we can use to compare models in a mixed-effects modelling analysis. However, I realised that <code>model.sel</code> function in <code>MuMIn</code> package seems to do the same thing and I need help as I am confused and the R help did not quite help. How are these functions different? Which one should I use? (I had 100 participants, 50 from each language group, from which each 25 participants in a language group received a different either list A or list B of the items.)</p>

<p>[packages <code>lme4</code> and <code>lmertest</code> were used to build the models]
Following are the codes I used to build the models:</p>

<pre><code>m1.1.1&lt;-lmer (RT~ Language*Col + (1+Col|Subject) + (1+Language|Item), data=RQ1.lmm.data.1) 
m1.1.2&lt;-lmer(RT~ Language*Col + (1|Subject) + (1+Language|Item), data=RQ1.lmm.data.1) 
m1.1.3&lt;-lmer(RT~ Language*Col + (1+Col|Subject) + (1|Item), data=RQ1.lmm.data.1)   
m1.1.4&lt;-lmer(RT~ Language*Col + (1|Subject) + (1|Item), data=RQ1.lmm.data.1) 
</code></pre>

<p>This is the function I used for comparing the models in order to find the most optimal model:</p>

<pre><code>anova(m1.1.1, m1.1.2, m1.1.3, m1.1.4)
</code></pre>

<p>[as this function shows the AIC for each model]</p>

<p>Resulting table:</p>

<pre><code>Data: RQ1.lmm.data.1 Models:

..3: RT ~ Language*Col + (1|Subject) + (1|Item)
..1: RT ~ Language*Col + (1|Subject) + (1+Language|Item)
..2: RT ~ Language*Col + (1+Col|Subject) + (1|Item)
object: RT ~ Language * Col + (1 + Col|Subject)+(1 + Language | Item)

   Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
..3     7 15912 15947 -7949.2    15898                             
..1     9 15874 15919 -7928.1    15856 42.322      2  6.456e-10 ***
..2     9 15902 15947 -7941.9    15884  0.000      0          1    
object 11 15865 15920 -7921.6    15843 40.634      2  1.501e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>As I have also seen the following function to be used for what seems like the same type of comparison, I used it and the following table was the result</p>

<pre><code>model.sel(m1.1.1, m1.1.2, m1.1.3, m1.1.4)

Model selection table 

       (Int) Cll Lng Cll:Lng      random df    logLik    AICc delta weight

m1.1.1 820.3   +   +       + 1+C|S+1+L|I 11 -7901.947 15826.1  0.00 0.988
m1.1.2 819.2   +   +       +     S+1+L|I  9 -7908.437 15835.0  8.90 0.012
m1.1.3 851.4   +   +       +     1+C|S+I  9 -7922.365 15862.9 36.76 0.000
m1.1.4 850.5   +   +       +         S+I  7 -7929.688 15873.5 47.34 0.000
Models ranked by AICc(x) 
Random terms: 
1+C|S = â€˜1 + Col|Subjectâ€™
1+L|I = â€˜1 + Language|Itemâ€™
S = â€˜1|Subjectâ€™
I = â€˜1|Itemâ€™
</code></pre>

<p>I'm guessing the difference between the two is only the <code>AIC</code> being the corrected version in the <code>model.sel</code> function (which seems to be more appropriate for small sample sizes, but not exactly sure of it.
And am I correct to interpret that, based on the results, my core model (model m1.1.1) is the most optimal model in this case?</p>

<p>(any comments on the interpretation of the model would be much appreciated as I am  quite new to this type of analysis technique.)</p>
"
"0.0524863881081478","0.0535287727572189","217834","<p>I am analyzing this MISeq microbial community data with Phyloseq and vegan R-packages from coral hosts and I would much appreciate some help and ideas as this is my first analysis of this kind. 
We have 10 samples of paired bleached and unbleached coral colonies (10 bleached corals and 10 unbleached corals for each spp) from 3 different spp., in 3 different sites, and those corals were tagged and re-sampled from the climax of the bleaching event and afterwards during the bleaching recovery in total in 4 time-points. So we have the variables:  </p>

<ul>
<li>Species: 3 coral spp.</li>
<li>Status: 2 levels: 10 bleached and 10 unbleached corals per spp.</li>
<li>TimePoints: 4 time-points when these same corals were re-sampled</li>
<li>Reef_Location: 3 sites</li>
</ul>

<p>I have 2 big questions, around the largest question that would be: ""what is the best way to analyze these data"":</p>

<ol>
<li><p>I was planning on running a PERMANOVA on with vegan package, but I am not sure whether to consider some of the variables nested or not. I have tried several combinations, after creating a distance phyloseq object with ""Bray"" distance called 'physeq.bray.dist', such as:</p>

<pre><code>permanova.total &lt;- adonis(physeq.bray.dist~(Status*Species*TimePoint)+(1|Reef_location), 
                          data=as(sample_data(physeq.f.t), ""data.frame""), 
                          permutations=9999)
</code></pre>

<p>I suspect there should be other ways to run more effectively this PERMANOVA, if whether to consider the sites as nested, and I am not sure as how to treat the repeated sampling on the same corals. Any suggestions?... Should I run other tests as well?</p></li>
<li><p>Further we have 3 data sets, from bacteria, fungi and microalgae (so 16S, ITS1, TTS2). I am starting analyzing all separately, but would like to compare and see interactions among the 3 microbial compartments. What analyses should I be running for this?...</p></li>
</ol>
"
"0.0909090909090909","0.092714554082312","218180","<p>0
down vote
favorite</p>

<p>I have a large multivariate abundance data and I am interested in comparing multiple models that fit different combinations of three categorical predictor variables to my species matrix response variable. I have been using anova() to compare my different models, but I am having difficulty interpreting the output. Below, I have given my code as well as the corresponding R output.</p>

<pre><code>invert.mvabund &lt;- mvabund(mva.dat)
null&lt;-manyglm(mva.dat~1, family='negative.binomial')
m1 &lt;- manyglm(mva.dat~Habitat+Detritus, family='negative.binomial')
m2 &lt;- manyglm(mva.dat~Habitat*Detritus, family='negative.binomial')
m3 &lt;- manyglm(mva.dat~Habitat*Detritus+Block, family='negative.binomial')
anova(null,m1,m2,m3)

Analysis of Deviance Table

null: mva.dat ~ 1
m1: mva.dat ~ Habitat + Detritus
m2: mva.dat ~ Habitat * Detritus 
m3: mva.dat ~ Habitat * Detritus + Block

Multivariate test:
     Res.Df Df.diff   Dev Pr(&gt;Dev)       
null     99                           
m1       94       5 257.2    0.001 ***
m2       90       4  87.7    0.003 ** 
m3       81       9 173.5    0.003 ** 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>How do I interpret these results? Is m2 the best-fitting model because it has the lowest deviance, even though it has a higher p-value than m1? Is this because the p-value is suggesting that there is a significant level of deviance, so the optimal model will have a higher p-value? Any suggestions on how to interpret these results would be much appreciated- I haven't been able to find a clear answer in my Google searches. Thanks!</p>
"
"0.0524863881081478","0.0535287727572189","219415","<p>I am performing a MANOVA with a large multivariate dataset representing total body shape (dependent variable). My two independent variables are gender (sex) and cluster (from a previously computed cluster analysis). </p>

<pre><code>body.shape=cbind(unix,uniy,pw1x,pw1y,pw2x,pw2y)
bshape.manova=manova(body.shape~sex*cluster,data=data)
summary(bshape.manova,test=""Wilks"")
</code></pre>

<p>The output works however, no matter what I do I cannot get the interaction term between the two independent variables (sex*cluster) which is really important. 
I have run 5 MANOVA's on other populations and it all worked great. This population specifically will not given the interaction between the independent variables and their effect on the dependent variable. </p>
"
"0.23472626340651","0.233403250542697","220603","<p>I have some measurements (concentration) made in 4 groups (W, X, Y, Z) and time is my covariate. I make a linear model:</p>

<pre><code>fit &lt;- lm(concentration~group*year, data=data)
</code></pre>

<p>The results are as follows: ANOVA table:</p>

<pre><code>anova(fit)

Analysis of Variance Table

Response: concentration
           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
group       3 3600.7 1200.22 32.6132 4.081e-10 *** #!
year        1  559.7  559.71 15.2087 0.0004311 ***
group:year  3   97.3   32.42  0.8809 0.4607155    
Residuals  34 1251.3   36.80                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and pairwise comparison:</p>

<pre><code>summary(fit)
Call:
lm(formula = concentration ~ group * year, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -433.0108   828.4293  -0.523    0.605
groupX      -1574.0090  1170.3741  -1.345    0.188 #!
groupY      -1666.3673  1170.3741  -1.424    0.164 #!
groupZ      -1201.2766  1170.3891  -1.026    0.312 #!
year            0.2418     0.4128   0.586    0.562
groupX:year     0.7937     0.5831   1.361    0.182
groupY:year     0.8409     0.5831   1.442    0.158
groupZ:year     0.6104     0.5831   1.047    0.303

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09
</code></pre>

<p>Now I have a problem in the interpretation of this data. As far as I understand, since the interaction in the ANOVA table is nonsignificant, I can check the group effect, and it is significant. This means that the intercept in different groups should be [significantly] different. But when I look to the summary table, there is no significant difference, at least â€“ between group W and others (groupX, groupY and groupZ are nonsignificant). If I change the compared group from W to X or Y or Z the comparison results are still nonsignificant:</p>

<pre><code>data2 &lt;- data
data2$group[data2$group==""X""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -2.007e+03  8.267e+02  -2.428   0.0206 *
groupW       1.574e+03  1.170e+03   1.345   0.1876 #! 
groupY      -9.236e+01  1.169e+03  -0.079   0.9375 #! 
groupZ       3.727e+02  1.169e+03   0.319   0.7518 #!
year         1.035e+00  4.119e-01   2.514   0.0168 *
groupW:year -7.937e-01  5.831e-01  -1.361   0.1824  
groupY:year  4.717e-02  5.825e-01   0.081   0.9359  
groupZ:year -1.834e-01  5.825e-01  -0.315   0.7549  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09

data2 &lt;- data
data2$group[data2$group==""Y""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -2.099e+03  8.267e+02  -2.539   0.0158 *
groupW       1.666e+03  1.170e+03   1.424   0.1636 #! 
groupX       9.236e+01  1.169e+03   0.079   0.9375 #! 
groupZ       4.651e+02  1.169e+03   0.398   0.6933 #! 
year         1.083e+00  4.119e-01   2.628   0.0128 *
groupW:year -8.409e-01  5.831e-01  -1.442   0.1584  
groupX:year -4.717e-02  5.825e-01  -0.081   0.9359  
groupZ:year -2.305e-01  5.825e-01  -0.396   0.6948  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09

data2 &lt;- data
data2$group[data2$group==""Z""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -433.0108   828.4293  -0.523    0.605
groupX      -1574.0090  1170.3741  -1.345    0.188 #!
groupY      -1666.3673  1170.3741  -1.424    0.164 #!
groupZ      -1201.2766  1170.3891  -1.026    0.312 #!
year            0.2418     0.4128   0.586    0.562
groupX:year     0.7937     0.5831   1.361    0.182
groupY:year     0.8409     0.5831   1.442    0.158
groupZ:year     0.6104     0.5831   1.047    0.303

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09
</code></pre>

<p>How is it possible that there are no significant difference between any two groups when there is a significant group effect? Apparently my interpretation that significant group effect means that at least one group differ significantly from other in the intercept value is incorrect. So what is the correct interpretation of the significant group effect?  </p>
"
"0.111340442853781","0.11355167461567","221023","<p>I have a dilema of the suitability of the analysis with my design.
I have 3 fixed factors:
- Photoperiod (2 levels: 16L8D; 10L14D)
- Temperature (2 levels: 6ÂºC; 25ÂºC)
- Time (4 levels: 50;70;90;150 days) </p>

<p>Photoperiod and Temperature are crossed, and Time is nested within the crossed factors. See this image:</p>

<p><img src=""http://i.stack.imgur.com/gEoAo.jpg"" alt=""enter image description here""></p>

<p>I have tried the following ANOVA nested model: (Y: dependent variable; df: dataframe)</p>

<pre><code>aov(Y ~ (Photoperiod * Temperature) + Error((Photoperiod * Temperature)/Time), data=df)
</code></pre>

<p>And I get that results:</p>

<pre><code>Call:
aov(formula = Y ~ (Photoperiod * Temperature) + 
    Error((Photoperiod * Temperature)/Time), data=df)

Grand Mean: 4.492955

Stratum 1: Photoperiod

Terms:
                 Photoperiod
Sum of Squares  197.7843
Deg. of Freedom        1

1 out of 2 effects not estimable
Estimated effects are balanced

Stratum 2: Temperature

Terms:
                Temperature
Sum of Squares   3795.089
Deg. of Freedom         1

1 out of 2 effects not estimable
Estimated effects are balanced

Stratum 3: Photoperiod:Temperature

Terms:
                Photoperiod:Temperature
Sum of Squares           197.7843
Deg. of Freedom                 1

Estimated effects are balanced

Stratum 4: Photoperiod:Temperature:Time

Terms:
                Residuals
Sum of Squares   626.4977
Deg. of Freedom         2

Residual standard error: 17.69884

Stratum 5: Within

Terms:
                Residuals
Sum of Squares   30658.85
Deg. of Freedom       182

Residual standard error: 12.97903
</code></pre>

<p>I don't know if this approach is right, and how can I get p-values from those results. </p>
"
"0.0909090909090909","0.092714554082312","221182","<p>I've got a dataset with two measures in a group of people, before (pre) and after (post) an intervention. Second measure is always greater than first one.</p>

<p>My assumption is that the initial value determines the magnitude of the post-intervention measure, so, the lower the initial value, the higher the increase, the higher the initial value, the lower the increase.</p>

<p>The target of my analysis is not the change itself, but the ratio post/pre intervention. My main outcome should be a pvalue of the change (it it is significant or not) and a graph pre Vs predicted(post/pre).</p>

<p>Iâ€™ve tried a one-way repeated measures anova, but this way I only analyse the variance, I cant predict the outcome.</p>

<pre><code>library(car)
options(contrasts=c(""contr.sum"",""contr.poly""))
measure1&lt;-runif(1000,0,1000)
measure2&lt;-measure1*(200-20*log(measure1))
lmmodel &lt;- lm(cbind(measure1,measure2) ~ 1)
measureFactor&lt;-factor(c(""measure1"",""measure2""), ordered=F)
finalmodel&lt;-Anova(lmmodel,idata=data.frame(measureFactor),idesign=~ measureFactor,type=3)
summary(finalmodel)
</code></pre>

<p>I have the impression that this is way more easy than I think, but Iâ€™m blocked, any clues?</p>

<p>SECOND: what if there are two groups, the first one is a â€œcontrolâ€ group and the other one is an â€œinterventionâ€ group, both with measures pre and post intervention, how would this modify my model? How could I get the pvalue of differences between the two groups.</p>
"
"0.0829882662886615","0.0846364211331916","221321","<p>I am new to statistics and I am trying to conduct an analysis in <code>R</code> on data containing read count information for 45 samples. I carried out an <code>ANOVA</code> (using the <code>aov()</code> function), comparing the means of each sample, and have also run a <code>post-hoc</code> <code>Tukey</code> test (using the <code>TukeyHSD(</code>) function). </p>

<p>I have used the <code>multcompLetters()</code> function from the '<code>multcompView</code>' package to cluster the samples into groups that differ significantly based on the result of the <code>Tukey</code> test. It has produced a list of groups named 'a', 'ab' and 'b'. </p>

<p>I understand that the samples assigned to group 'a' are significantly different from those assigned to group 'b', but I was hoping someone might be able to help me understand what it means when a sample is assigned the 'ab' group?</p>

<p>Thank you!</p>
"
"0.0829882662886615","0.0846364211331916","221406","<p>I am using R to perform an anova analysis on model with a single factor (7 levels).</p>

<p>I am interested in finding the table of means and standard errors for an balanced design. </p>

<pre><code>print(model.tables(anovaname,""means"",se=TRUE))
</code></pre>

<p>When I do this, I obtain a standard error for the estimate of difference in means. For a balanced design, R returns me a single value for my factor. However, this is the standard error of the estimate of difference in means. Is there a way to find the standard error of estimate of a single mean? </p>
"
"0.157459164324443","0.151664856145454","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.117363131703255","0.107724577173552","223612","<p>I have a data set containing root, shoot, and seedling growth measurements from 5 treatment groups (at different concentrations, 3 replicates) including control. There were no separate control groups for each treatment group. Each treatment had different concentration ranges. Thus the dataset  looks like the following:</p>

<p>Treatment:  5 treatment (each with different concentrations which varied for each treatment)</p>

<p>Variable:  three variables (root, shoot and seedling -considered as factor) </p>

<p>length: measurement of variable response</p>

<p>Actually, I was trying to see how certain chemical treatments affect the growth of root, shoot, and seedling and whether there is any interaction in the outcome. Assuming that the â€œconcentration X treatmentâ€  interaction term in did not make sense (as the levels- concentrations- were different for different treatments), I modified the analysis by splitting the data by each treatment. Thus each dataset  included the respose variables (root, shoot, and seedling- responses used as factor called variable) at different concentrations for ONE treatment. Thus the ANOVA reduced to two-way and required log transformation .</p>

<p>I have the following doubts (The code for analysis is given below, the data set is attached):</p>

<p>Questions:  </p>

<ol>
<li>would it be meaningful to use the concentration X variable term (since the concentrations differ in each treatment group)? </li>
<li>Should I use the same transformed data (used for two-way analysis) for the one â€“way model required  during multiple comparisons even though the untransformed one-way model meets the assumptions? </li>
<li>What type of transformation should I use for the data (continuous) which could not be fitted even after log transformation?</li>
</ol>
"
"0.207017507556151","0.239659826646991","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.117363131703255","0.107724577173552","223648","<p>I am new in statistical analysis field.</p>

<p>I have a dataset which is divided into five groups each with four columns(common in all five groups). All these five groups have a baseline group to be compared with. Each row in these groups is considered to be different sample such Sample1, Sample2, Sample 3 and so on. There are total 30 samples for each group.</p>

<p>For example:</p>

<p>Baseline ( Column A, Column B, Column C, Column D)
Group A(Column A, Column B, Column C, Column D)
Group B(Column A, Column B, Column C, Column D)
Group C(Column A, Column B, Column C, Column D)
Group D(Column A, Column B, Column C, Column D)
Group E(Column A, Column B, Column C, Column D)
The test has been conducted on different groups, and columns values have been recorded for different groups. I have to decide which group will be best to select that has values closer to the baseline value by recorded column values.</p>

<p>I would like to know what kind of statistical analysis would be best. Should I do the t-test for each column of the different group with baseline and compare their p-value and the best p-value would be the group to select. Should I do Anova test for each column at one go and decide based on these results.</p>

<p>Or, should I do perform some other analysis apart from which I mentioned here to select the best group.</p>

<p>Please suggest.</p>
"
"0.100711219892181","0.125536099672233","223978","<p>I've encountered several times the situation where ANOVA gives drastically different results depending on whether the variable is entered as a factor or as a numeric variable. Why is this the case?</p>

<p>I understand somewhat that this classification evokes different analyses in the case where the variable has more than two levels - for example, say the variable is ""grade level"" and has levels 5, 6, 7. But, I sometimes get quite different results even when the variable has only two levels, say ""grade level"" with levels 5 and 6. Should I ever get different results in this case? Why? And how should I decide which result to use?</p>

<p>If it matters, I'm using ezANOVA in R with type 3 SS and the variable in question is between-subject, but the analysis also includes other between- and within-subjects variables. I'm not sure if this phenomenon reflects normal behavior of ANOVA or potentially a problem with ezANOVA.</p>

<p>In this reproducible example, ANOVA finds a significant effect of the within-subjects factor when the between-subjects variable is treated as a factor, but not when it is treated as numeric.</p>

<pre><code>library(ez)
set.seed(1)
N       = 60
subjid  = factor( rep(1:N,each=2) )
between = c( rep(7,N), rep(8,N) )
within  = factor( rep(c(""A"",""B""),N) )
x       = as.numeric( between )
y       = as.numeric( within==""A"" )
dv      = x + 0.5*y + runif(length(between))
D       = data.frame( subjid=subjid, between=between, within=within, dv=dv )
D$betweenF = factor( D$between )

ezANOVA( data=D, wid=subjid, dv=dv, between=between, within=within, type=3 )
## Warning: ""between"" will be treated as numeric.
## $ANOVA
##           Effect DFn DFd            F            p p&lt;.05          ges
## 2        between   1  58 380.47083320 3.725618e-27     * 0.7765827418
## 3         within   1  58   0.85506909 3.589537e-01       0.0068830702
## 4 between:within   1  58   0.02700153 8.700494e-01       0.0002188134

ezANOVA( data=D, wid=subjid, dv=dv, between=betweenF, within=within, type=3 )
## $ANOVA
##            Effect DFn DFd            F            p p&lt;.05          ges
## 2        betweenF   1  58 380.47083320 3.725618e-27     * 0.7765827418
## 3          within   1  58 130.79260911 1.701170e-16     * 0.5145964337
## 4 betweenF:within   1  58   0.02700153 8.700494e-01       0.0002188134
</code></pre>
"
"0.0829882662886615","0.0846364211331916","224434","<p>Our experimental design is as follows:</p>

<p>For each of two genotypes (wt and ko), we perform two different gene expression assays (Assay1 and Assay2), and do 4 replicates of each assay. We are interested in knowing if the true proportion of geneA.ko/geneA.wt significantly deviates from 1. However, the catch is that the true abundance we are seeking requires us to normalize the ratio of <em>geneA.ko.Assay1/geneA.wt.Assay1</em> by the ratio <em>geneA.ko.Assay2/geneA.wt.Assay2</em>. In other words, what we want to know is if the following ratio
deviates significantly from 1: <em>(geneA.ko.Assay1/geneA.wt.Assay1) / (geneA.ko.Assay2/geneA.wt.Assay2)</em>. </p>

<p>Here are three sample datasets in R that I hope illustrate the structure of the data:</p>

<pre><code>set.seed(555)
#Not significant, because the ratio of ratios is ~ (500/50)/(10000/1000) = 1
NS = data.frame(geneID=rep(c(""geneA.Rep1"",""geneA.Rep2"",""geneA.Rep3"",""geneA.Rep4""),4),
geno = c(rep(""wt"",4),rep(""ko"",4),rep(""wt"",4),rep(""ko"",4)),
assay = c(rep(""Assay1"",8),rep(""Assay2"",8)),
 intensity = 
   c(rnorm(4,50,5),
    rnorm(4,500,5),
    rnorm(4,1000,5),
    rnorm(4,10000,5)
  )
)

#Significant, because the ratio of ratios is ~ 6
S = data.frame(geneID=rep(c(""geneA.Rep1"",""geneA.Rep2"",""geneA.Rep3"",""geneA.Rep4""),4),
 geno = c(rep(""wt"",4),rep(""ko"",4),rep(""wt"",4),rep(""ko"",4)),
 assay = c(rep(""Assay1"",8),rep(""Assay2"",8)),
 intensity = 
   c(rnorm(4,25,5),
    rnorm(4,150,5),
    rnorm(4,1000,5),
    rnorm(4,1000,5)
  )
)
#Also significant, because the ratio of ratios is ~ 60
S2 = data.frame(geneID=rep(c(""geneA.Rep1"",""geneA.Rep2"",""geneA.Rep3"",""geneA.Rep4""),4),
 geno = c(rep(""wt"",4),rep(""ko"",4),rep(""wt"",4),rep(""ko"",4)),
 assay = c(rep(""Assay1"",8),rep(""Assay2"",8)),
 intensity = 
   c(rnorm(4,25,5),
    rnorm(4,150,5),
    rnorm(4,1000,5),
    rnorm(4,100,5)
  )
)
</code></pre>

<p>Our collaborator recommended using log-linear modeling to determine if the fit to the data is improved by incorporating the Assay2 terms. Unfortunately, I have very little experience with linear modeling beyond basic differential expression analysis. So far I have tried doing the following:</p>

<pre><code>model1 = lm(intensity ~ geno, data = NS, na.action = na.omit)
model2 = lm(intensity ~ geno * assay, data = NS, na.action = na.omit)
anova(model1, model2)
</code></pre>

<p>But the fact that the p-value returned from the anova here is still &lt;&lt;&lt;&lt;&lt; .05 suggests that my model is asking the wrong question. Any insight would be hugely appreciated. I'd be happy to go into more detail on other approaches I've tried if it's useful.</p>
"
"0.0371134809512603","0.0378505582052232","224437","<p>Hi I'm running mixed design ANOVA analysis for a set of data. I have one between-subject variable and two within-subject variables. I used aov() command to do the analysis but Tukey() did not work for the post-hoc tests, which I guess was because there's within-subject variables. I was suggested to try t-test for the post-hoc comparisons. I'd like to know if there're other ways? Thanks.</p>
"
"0.117363131703255","0.11969397463728","224509","<p>I'm conducting a meta-analysis on standardised mean difference scores. Some studies provide multiple effect sizes, thereby violating the assumption of independence. An example is given below (all effect sizes were calculated with regard to a pre-test). In study A, all participants received the same treatment (watching a video), and were tested repeatedly. In study B, there were two different treatment groups (one group watched a video, the other group listened to an audio book), and everyone was tested once. Study C provided only one effect size.</p>

<pre><code>study        treatment          testing_moment         effect_size

A            video              immediately            0.6
A            video              delayed                0.5
B            video              immediately            0.9
B            audio_book         immediately            0.7
C            audio_book         delayed                0.4
</code></pre>

<p>I'm using the <em>metafor</em> package in <em>R</em>, in which you can fit a multilevel model to account for non-independent sampling errors. </p>

<p>What I've done:</p>

<pre><code>rma.mv(effect_size_vector, variance_vector, mods = ~ testing_moment, 
  random = ~ 1 | treatment/study, data = rev)
</code></pre>

<p>Could anyone please have a look whether this approach is correct? I'm especially unsure about whether I've correctly indicated the clustering using slash (/) (this decision was based on <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this page</a>), and whether the model as a result indeed takes into account the non-independence of effect sizes. </p>

<p>I'm also wondering whether somehow it should be corrected that the samples in study A are dependent and in study B they are independent. Or is that already accounted for by virtue of the treatment variable being the same for both samples in study A?</p>
"
"0.10562681853293","0.107724577173552","224955","<p>I am trying to perform a mixed ANOVA analysis with one between subject and two within subject factors.</p>

<p><strong>Some Experiment information</strong></p>

<ul>
<li>Two groups of 17 subjects (old and young participant group)</li>
<li>Three conditions (C1,C2 and C3)</li>
<li>Two speaking styles in each condition (conversational and adapted)</li>
<li>I measure the participant response to each speaking style in each condition for the two age groups. There are no missing values.</li>
<li>Data sample:</li>
</ul>

<p><code>&gt; head(new)
  Participant condition   measure     Age          style
1           1        C1 1.971400 Young adults Conversational
2           2        C1 3.961505 Young adults Conversational
3           3        C1 3.445986 Young adults Conversational
4           4        C1 1.604906 Young adults Conversational
5           5        C1 1.545867 Young adults Conversational
6           6        C1 1.934843 Young adults Conversational</code></p>

<p>I haven't really done this before so I'm using a specification I found in a few related questions on this website:</p>

<p><code>summary(aov(measure ~ Age * condition * style + Error(Participant/(condition*style)), data=new))</code></p>

<p>So <em>Age</em> is the between subject factor characterizing the groups and <em>condition</em> and <em>style</em> are the within subject factors. However, while searching for a solution I also found an approach using <code>ezANOVA</code> :</p>

<p><code>testmodel&lt;-ezANOVA(data=new,dv=.(measure),wid=.(Participant),between = .(Age),within = .(condition,style),type=3,detailed=TRUE)</code></p>

<p>This produces different results...It also gives me a warning that it's converting all variables to factors but I guess that shouldn't cause problems.
This has also been suggested
<code>anova(lme(measure ~ Age * condition * style, random=~1 | Participant, method=""ML"", data=new))</code>.  </p>

<p>I have a few questions: <strong>What's the difference between the three calls? What's the correct (or easiest I guess) method to use for my type of data set? How can I know which approach to choose (or what would have to change in my data set for the analysis to require a different approach)?</strong></p>
"
"0.166666666666667","0.169976682484239","225241","<p>Consider a mixed model as follows.</p>

<pre><code>library(lme4)
# Load data
data &lt;- structure(list(blk = c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3L),
                       gent = c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10L),
                       yld = c(83, 77, 78, 78, 70, 75, 74, 79, 81, 81, 91, 79, 78, 92, 79, 87, 81, 96, 89, 82L),
                       syld = c(250, 240, 268, 287, 226, 395, 450, 260, 220, 237, 227, 281, 311, 258, 224, 238, 278, 347, 300, 289L)),
                  .Names = c(""blk"", ""gent"", ""yld"", ""syld""), class = ""data.frame"", row.names = c(NA, -20L))
data$blk &lt;- as.factor(data$blk)
data$gent &lt;- as.factor(data$gent)
</code></pre>

<p>The data is unbalanced.</p>

<pre><code># Mixed effect model
frmla &lt;- ""syld ~ 1 + gent + (1|blk)""
library(lme4)
model &lt;- lmer(formula(frmla), data = data)

model
Linear mixed model fit by REML ['merModLmerTest']
Formula: syld ~ 1 + gent + (1 | blk)
   Data: data
REML criterion at convergence: 73.9572
Random effects:
 Groups   Name        Std.Dev.
 blk      (Intercept)  9.385  
 Residual             16.919  
Number of obs: 20, groups:  blk, 3
Fixed Effects:
(Intercept)        gent2        gent3        gent4        gent5        gent6        gent7        gent8        gent9  
    256.000      -28.000       -8.333        8.000       32.127       43.678      -36.805       90.678       62.127  
     gent10       gent11       gent12  
     32.678      132.195      187.195  
</code></pre>

<p>Primarily I want to compare the <code>gent</code> levels by LS means.</p>

<pre><code>library(""lmerTest"")
lsmeans(model)
Least Squares Means table:
         gent Estimate Standard Error   DF t-value Lower CI Upper CI p-value    
gent  1   1.0    256.0           11.2  6.9    22.9      229      283  &lt;2e-16 ***
gent  2   5.0    228.0           11.2  6.9    20.4      201      255  &lt;2e-16 ***
gent  3   6.0    247.7           11.2  6.9    22.2      221      274  &lt;2e-16 ***
gent  4   7.0    264.0           11.2  6.9    23.6      237      291  &lt;2e-16 ***
gent  5   8.0    288.1           18.5  8.0    15.6      245      331  &lt;2e-16 ***
gent  6   9.0    299.7           18.5  8.0    16.2      257      342  &lt;2e-16 ***
gent  7  10.0    219.2           18.5  8.0    11.8      177      262  &lt;2e-16 ***
gent  8  11.0    346.7           18.5  8.0    18.8      304      389  &lt;2e-16 ***
gent  9  12.0    318.1           18.5  8.0    17.2      275      361  &lt;2e-16 ***
gent  10  2.0    288.7           18.5  8.0    15.6      246      331  &lt;2e-16 ***
gent  11  3.0    388.2           18.5  8.0    21.0      346      431  &lt;2e-16 ***
gent  12  4.0    443.2           18.5  8.0    24.0      401      486  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In addition I am interested in variance partitioning.</p>

<p>The variance component due to random effect and residual can be estimated as follows.</p>

<pre><code>VCrandom &lt;- VarCorr(model)
print(VCrandom, comp = ""Variance"")
 Groups   Name        Variance
 blk      (Intercept)  88.083 
 Residual             286.250
</code></pre>

<p>How to partition the total variance into components due to each of the factors <code>gent</code> and <code>blk</code> along with the residual ? Something similar to the output given by <code>PROC MIXED</code> of <code>SAS</code>, where MSE is computed even when estimation is by ML or REML instead of least squares.</p>

<p>Should I treat the fixed effect as random just for the purpouse of getting variance component ?</p>

<pre><code>frmla2 &lt;- ""syld ~ 1 + (1|gent) + (1|blk)""
model2 &lt;- lmer(formula(frmla2), data = data)
model2

VCrandom2 &lt;- VarCorr(model2)
print(VCrandom2, comp = ""Variance"")
 Groups   Name        Variance
 gent     (Intercept) 4152.08 
 blk      (Intercept)  116.11 
 Residual              274.92 
</code></pre>

<p>If there is no random effect, variance components can be estimated using the least squares approach (ANOVA, Sum of squares, MSE).</p>

<p>The package <code>mixlm</code> has provision for variance partitioning using SS in case of mixed models.</p>

<pre><code>library(mixlm)

mixlm &lt;- lm(syld ~ 1 + r(gent) + r(blk), data)

Anova(mixlm, type=""III"")

Analysis of variance (unrestricted model)
Response: syld
          Mean Sq   Sum Sq Df F value Pr(&gt;F)
gent      5360.49 58965.36 11   18.73 0.0009
blk        638.58  1277.17  2    2.23 0.1886
Residuals  286.25  1717.50  6       -      -

            Err.term(s) Err.df VC(SS)
1 gent              (3)      6 3044.5
2 blk               (3)      6   52.8
3 Residuals           -      -  286.3
(VC = variance component)

               Expected mean squares
gent      (3) + 1.66666666666667 (1)
blk       (3) + 6.66666666666667 (2)
Residuals (3)                       

WARNING: Unbalanced data may lead to poor estimates
</code></pre>

<p>The estimates are different</p>

<pre><code># Total variance
var(data$syld)

|source   |  model1|  model2|  mixlm|
|:--------|-------:|-------:|------:|
|gent     |      NA| 4152.08| 3044.5|
|blk      |  88.083|  116.11|   52.8|
|Residual | 286.250|  274.92|  286.3|
</code></pre>

<p>Can fixed effect variance be extracted using <code>predict</code> function as suggested here <a href=""https://sites.google.com/site/alexandrecourtiol/what-did-i-learn-today/inrhowtoextractthedifferentcomponentsofvarianceinalinearmixedmodel"" rel=""nofollow"">In R: How to extract the different components of variance in a linear mixed model!</a> ?</p>

<pre><code>var(predict(model))
</code></pre>

<p>Which is the most appropriate method compatible with <code>(RE)ML</code> estimates in lme4 ?</p>
"
"0.123091490979333","0.114123726974758","225848","<p>I am new in statistical analysis field.</p>

<p>I have a dataset which is divided into five groups each with four columns(common in all five groups). All these five groups have a baseline group to be compared with. Each row in these groups are considered to be different sample such Sample1, Sample2, Sample 3 and so on. There are total 30 samples for each group.</p>

<p>For example:</p>

<pre><code>Baseline ( Column A, Column B, Column C, Column D)
Group A(Column A, Column B, Column C, Column D)
Group B(Column A, Column B, Column C, Column D)
Group C(Column A, Column B, Column C, Column D)
Group D(Column A, Column B, Column C, Column D)
Group E(Column A, Column B, Column C, Column D)
</code></pre>

<p>The test have been conducted on different groups and columns values have been recorded for different groups. I have to decide which group will be best to select that has values closer to the baseline value on the basis of recorded column values.</p>

<p>I would like to know what kind of statistical analysis would be best. Should I do t test for each column of different group with baseline and compare their p-value and the best p value would be the group to select.
Should I Anova test for each column at one go and decide based on these results.</p>

<p>Or, should I do perform some other analysis apart from which I mentioned here to select the best group. </p>

<p>This analysis is not done over the period of time. i believe this is done at one go for different groups</p>

<p>Please suggest. </p>
"
"0.0524863881081478","0.0535287727572189","229343","<p>I've got a dataset with patients (n=50) with 10 readings each (so overall, n=500) who suffered syncope (1 or 0), and 2 continuous predictors (rate and doppler).</p>

<p>I'm trying to see if 1 predictor is more effective than another. I'm currently synthesising data, then creating a model for each using glm(syncope~predictor,family=""binomial""), and then using an ANOVA on these. The code is as follows.</p>

<pre><code>n_pts &lt;- 50
n_reads_per_pt &lt;- 10
intercept = log(0.2)
gradient = 2.5
x &lt;- rnorm(n_pts*n_reads_per_pt,mean=0,sd=1)

x_doppler &lt;- x
x_rate &lt;- x + (rnorm(n_pts*n_reads_per_pt)) #Add a second random factor to make rate a less good predictor

y &lt;- intercept + gradient*x
p &lt;- exp(y)/(1+exp(y))
tmp &lt;- runif(n_pts*n_reads_per_pt)
syncope &lt;- (tmp &lt; p)

glm_rate &lt;- glm(syncope~x_rate,family=""binomial"")
glm_doppler &lt;- glm(syncope~x_doppler,family=""binomial"")

anova(glm_rate, glm_doppler,test=""Chisq"")
</code></pre>

<p>The problem is the output of the ANOVA is:</p>

<pre><code>&gt; anova(glm_rate, glm_doppler,test=""Chisq"")
Analysis of Deviance Table

Model 1: syncope ~ x_rate
Model 2: syncope ~ x_doppler
  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
1       498     585.72                     
2       498     386.71  0   199.01  
</code></pre>

<p>Note no p value. I assume this is because I have 0 Df left?</p>

<p>How would you recommend I then compare such a dataset, where 2 continuous predictors are being compared to assess one binary outcome?</p>
"
"0.12894693513945","0.141623820702091","229722","<p>Thank anyone who look my question. I'm doing a linguistic experiment. I let people in two second language proficiency levels (inter and advanced) and living in two places (city A and B) do a same rating test. The rating test have 6 types of questions, each type have 6 tokens, in total 36 test items for each subject. I also have a native speaker group as control (L1). The picture shows how I code data.
<a href=""http://i.stack.imgur.com/1a2sM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1a2sM.png"" alt=""enter image description here""></a></p>

<p>The followings are my code.</p>

<pre><code>library(plyr)

# Read data
data = read.csv(""Ba_rang_bei"", header = TRUE)
# Summarise data for table
sum = ddply(.data=data, c(""type"", ""level""), summarise, mean =mean(rating,na.rm=TRUE), sd = sd(rating, na.rm=TRUE))
sum
# Summarise data for analysis
agg = ddply(.data=data, c(""ID"", ""type"", ""level""), summarise, mean = mean(rating, na.rm=TRUE))
# Run anova with 'rating' as the dependent factor, 'type'as a with-subject factor and 'level'as a between-subject factor.
# Include interaction
anova1= aov(mean ~ type*level+Error(ID/type), data = agg)
summary(anova1)
</code></pre>

<p>The result shows:
<a href=""http://i.stack.imgur.com/6kGKI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6kGKI.png"" alt=""enter image description here""></a><br>
I want to do multiple ANOVA comparisons on subjects' mean rating scores between types and between levels. Like I want to know ""whether A-inter group behave significantly different with the native group on Type A"", ""whether inter group (both in city A and city B) behave significantly different from advaned group (both in city A and city B), ""whether A-inter group's ratings on Type A significantly different on Type B, C, D, E and F.The Tukey HSD test doesn't work in my case, so I have to find an appropriate linear model to do multiple ANOVA comparisons. Can anybody give me any suggestions on building linear models and do multiple ANOVA comparisons? Please help me.</p>
"
"0.17799000188592","0.173632513249697","230734","<p>I've been running GLMMs in the R package ""glmmadmb"" looking at the effects of different sizes of pan trap on the abundance of their catch, using the following code: <code>glmm5 &lt;- glmmadmb(ab$Totalpolls ~ ab$Pan_size+ab$Treatment+log(ab$Nectar+1)+log(ab$Mean.nectar+1)+ab$Max_temp+ab$Season+(1|Year)+(1|Transect), data = ab, zeroInflation = FALSE, family = ""nbinom"")</code></p>

<p>The basic output look like this:</p>

<pre><code>Anova(glmm5)

Analysis of Deviance Table (Type II tests)
Response: ab$Total_polls  Df Chisq Pr(&gt;Chisq)    
ab$Pan_size               3 41.6487  4.763e-09 ***
ab$Treatment              2 14.8347  0.0006007 ***
log(ab$Nectar + 1)        1  8.0988  0.0044295 ** 
log(ab$Mean.nectar + 1)   1  5.0591  0.0244971 *  
ab$Max_temp               1  8.5233  0.0035062 ** 
ab$Season                 4 46.4576  1.978e-09 ***
Residuals               212                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and...</p>

<blockquote>
  <p>summary(glmm5)</p>
</blockquote>

<pre><code>Call:
glmmadmb(formula = ab$Total_polls ~ ab$Pan_size + ab$Treatment + 
log(ab$Nectar + 1) + log(ab$Mean.nectar + 1) + ab$Max_temp + 
ab$Season + (1 | ab$Year) + (1 | ab$Transect), data = ab, 
family = ""nbinom"", zeroInflation = FALSE)

AIC: 855.6 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)              -1.1797     0.5312   -2.22  0.02638 *  
ab$Pan_size2              0.5272     0.1587    3.32  0.00089 ***
ab$Pan_size5.5            0.0926     0.1668    0.56  0.57882    
ab$Pan_size12             0.9026     0.1538    5.87  4.4e-09 ***
ab$Treatment24            0.0540     0.1368    0.39  0.69286    
ab$Treatment48            0.4973     0.1291    3.85  0.00012 ***
log(ab$Nectar + 1)        0.0739     0.0260    2.85  0.00443 ** 
log(ab$Mean.nectar + 1)   0.0934     0.0415    2.25  0.02450 *  
ab$Max_temp              -0.0513     0.0176   -2.92  0.00351 ** 
ab$Season5                0.6176     0.2051    3.01  0.00260 ** 
ab$Season6                1.2434     0.2229    5.58  2.4e-08 ***
ab$Season7                0.7909     0.2338    3.38  0.00072 ***
ab$Season8                0.6476     0.3554    1.82  0.06840 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=228, ab$Year=2, ab$Transect=19 
Random effect variance(s):
Group=ab$Year
             Variance   StdDev
(Intercept) 1.142e-07 0.000338
Group=ab$Transect
            Variance StdDev
(Intercept)  0.01727 0.1314

Negative binomial dispersion parameter: 5.6367 (std. err.: 2.0044)

Log-likelihood: -411.816
</code></pre>

<p>Which shows that pan sizes 12 and 2 are significantly different to size 1, and size 5.5 isn't significantly different at all. However, when I put this model through post hoc tests using the following code: <code>summary(glht(glmm5, lsm(pairwise ~ ab$Pan_size)))</code> (using the glht interface in R package ""Lsmeans"") it gives me this:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Fit: glmmadmb(formula = ab$Total_polls ~ ab$Treatment + ab$Pan_size + 
log(ab$Nectar + 1) + log(ab$Mean.nectar + 1) + ab$Max_temp + 
ab$Season + (1 | ab$Year) + (1 | ab$Transect), data = ab, 
family = ""nbinom"", zeroInflation = FALSE)

Linear Hypotheses:
              Estimate Std. Error z value Pr(&gt;|z|)    
1 - 2 == 0     -0.1465     0.1239  -1.182  0.62507    
1 - 5.5 == 0   -0.3086     0.1252  -2.464  0.06256 .  
1 - 12 == 0    -0.6975     0.1421  -4.907  &lt; 0.001 ***
2 - 5.5 == 0   -0.1621     0.1063  -1.525  0.40911    
2 - 12 == 0    -0.5510     0.1666  -3.308  0.00498 ** 
5.5 - 12 == 0  -0.3889     0.1327  -2.931  0.01685 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>Which suggests that size 12 is not better than size 1 and that size two is also not better than size 1; it also indicates that size 5.5 is better than size 1. This all seems to contradict what's in the model summary, which has got me slightly puzzled. All of the other post hoc tests I've run for the other categorical variables in the model have run fine and present results as expected. I've tried changing the position of <code>ab$pan_size</code> in the model, but that doesn't improve things.</p>

<p>Here's a list of the basic code that I'm using:</p>

<pre><code>#pollinator abundance vs. pan trap size and time left active

ab&lt;-read.csv(""Total_polls.csv"")

ab
names(ab)
str(ab)
summary(ab)

# create factors from numerical variables

ab$Year&lt;-as.factor(ab$Year)
ab$Transect&lt;-as.factor(ab$Transect)
ab$Treatment&lt;-as.factor(ab$Treatment)
ab$Pan_size&lt;-as.factor(ab$Pan_size)
ab$Season&lt;-as.factor(ab$Season)

library(glmmADMB)
library(RVAideMemoire)
library(car)

glmm5 &lt;- glmmadmb(ab$Total_polls ~ ab$Treatment+ab$Pan_size+log(ab$Nectar+1)+log(ab$Mean_nectar+1)+ab$Max_temp+ab$Season+(1|ab$Year)+(1|ab$Transect), data = ab, zeroInflation = FALSE, family = ""nbinom"")

Anova(glmm5) #(Package: car)
summary(glmm5) 

# pairwise multiple comparisons tests between multi-level fixed effects (pan_size, treatment (time), and season)

library(multcomp)
library(lsmeans)

glht1 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Pan_size)))
glht2 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Treatment)))
glht3 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Season)))

summary(glht1)
</code></pre>

<p>I'm using R version 3.2.3 (2015-12-10) -- ""Wooden Christmas-Tree"". Could anyone help me to sort this out? I'm not exactly stats savvy, so you may have to be kind with the mathematical language.</p>

<p>Many thanks,
Tom</p>
"
"0.128564869306645","0.120191664828198","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.123091490979333","0.125536099672233","231258","<p>After using weight of evidence &amp; Information value mechanism, of the 40 odd   variables I am left with 8 variables which are highly or moderately significant.<br>
One of the independent variable which is categorical has 60+ categories. This is a very highly predictable variable hence please suggest as to how should I<br>
use this variable in the model.<br>
When I add this variable in the model my null deviance and AIC decreases   and makes other predictors loose their predictive power.<br>
Then another model without this variable my null deviance and AIC improves.<br>
What could be the reason. Is this variable collinear with some other predictor.   </p>

<p><em>Please see the syntax: &lt; Without that Categorical Var></em>  </p>

<pre><code>m1.logit&lt;- glm(survey ~ region+ know + repS+ und+ case_status, family = binomial(logit), data = a1 )
m1.logit  
summary(m1.logit)

Call:  
glm(formula = survey ~ region + know + repS + und + case_status, 
    family = binomial(logit), data = a1)  

Deviance Residuals:   
     ` Min       1Q   Median    3Q     Max`
    -2.579    0.271   0.290   0.336   2.895    

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 2553.5  on 2540  degrees of freedom
Residual deviance: 1287.7  on 2526  degrees of freedom
AIC: 1318    
Number of Fisher Scoring iterations: 13
</code></pre>

<p>Also ran an anova test to analyze the table of deviance  </p>

<pre><code>anova(m1.logit, test=""Chisq"")   
Analysis of Deviance Table  

Model: binomial, link: logit  
Response: survey  

Terms added sequentially (first to last)  

             Df Deviance Resid. Df Resid. Dev             Pr(&gt;Chi)     
 NULL                         2540       2554                           
 region       5       13      2535       2540                0.022 *    
 know         1      507      2534       2033 &lt; 0.0000000000000002 ***  
 repS         1      715      2533       1319 &lt; 0.0000000000000002 ***  
 und          1        3      2532       1316                0.109        
 case_status  6       28      2526       1288             0.000078 ***    

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1  
</code></pre>

<p>Please suggest as to how to deal with this predictor variable with 50+ categories  </p>
"
"0.0909090909090909","0.092714554082312","231377","<p>I am trying to carry out an lmerTest on two separate datasets, and for some reason I am getting the following error for one of the datasets.</p>

<blockquote>
  <p>In pf(F.stat, qr(Lc)$rank, nu.F) : NaNs produced</p>
</blockquote>

<p><a href=""https://drive.google.com/file/d/0B9jz9CiotnoER1dzUzRrVllVcm8/view?usp=sharing"" rel=""nofollow"">This dataset</a> gives me the p-value of the interaction term between <code>habitat</code> and <code>soil</code> without issue.</p>

<blockquote>
  <p>anova(lmer(sqrt(abs) ~ habitat*soil + (1|species), data=frl_light,
  REML=T))</p>
</blockquote>

<pre><code>Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq  Mean Sq NumDF  DenDF F.value  Pr(&gt;F)  
habitat      0.057617 0.028809     2 8.8434  1.0880 0.37805  
soil         0.232708 0.232708     1 2.6732  8.7888 0.06848 .
habitat:soil 0.308003 0.154001     2 2.7134  5.8163 0.10443  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><a href=""https://drive.google.com/file/d/0B9jz9CiotnoEdWkzbGhHM0RSVnM/view?usp=sharing"" rel=""nofollow"">This dataset</a> which has a similar structure however throws the error, and fails to give the p-value for the interaction between <code>habitat</code> and <code>light</code>. The density degree of freedom measurement is also 0, which is probably the problem.</p>

<blockquote>
  <p>anova(lmer(sqrt(abs) ~ habitat*light + (1|species), data=frl_soil,
  REML=T))</p>
</blockquote>

<pre><code>Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq  Mean Sq NumDF  DenDF F.value Pr(&gt;F)
habitat       0.00845 0.004223     2 7.9751  0.3494 0.7154
light         0.01634 0.016336     1 1.9241  1.3517 0.3689
habitat:light 0.42813 0.214067     2 0.0000 17.7124       
Warning message:
In pf(F.stat, qr(Lc)$rank, nu.F) : NaNs produced
</code></pre>

<p>I have no idea why lmerTest works for one dataset but not the other as both datasets appear to me at least, to be virtually indistinguishable from one another. If there is anyone who can shed light on the matter, please help.</p>
"
"0.0663906130309292","0.0677091369065533","232170","<p>Suppose I performed an experiment for two groups, did ANOVA test and f for this experiment was 0.48 between these two groups, having n=18, power = 0.8 and significance (alpha) = 0.05</p>

<p>Now if I want to perform same experiment but within 4 groups of samples with same variance within groups, and then figure out which group have effect (perform Tukey Post Hoc test).</p>

<p>quesiton is - how to estimate size of groups needed.</p>

<p>I used power test to determine size of the groups:</p>

<pre><code>pwr.anova.test(k=4,f=0.43,sig.level=0.05,power=0.8)

 Balanced one-way analysis of variance power calculation 

          k = 4
          n = 15.75307
          f = 0.43
  sig.level = 0.05
      power = 0.8
</code></pre>

<p>but since I am doing pairwise post-hoc analysis after between each group, probability of Type I error is increasing (since 0-hypothesis is there's no difference between all groups).</p>

<p>Therefore question is - how to correctly estimate n for 4 groups.</p>
"
"0.0642824346533225","0.0655590899062897","234652","<p>I have a dataframe which has <code>SUBJECT_ID</code>, <code>VISIT_NAME</code> and <code>BIOMARKER</code> measurement levels and <code>ARM_GROUP</code> as the four columns. The visit names has three levels - visit 1, visit 2 and visit 3. Subjects fall into two groups - cases and controls. </p>

<p>The study is unbalanced. </p>

<p>The objective is to find if there are any differences in the biomarker measurements between the 3 visits and the 2 groups. This calls for a repeated measures ANOVA. </p>

<p>The R code I use for this:</p>

<pre><code>anova = aov(BIOMARKER~VISIT_NAME*ARM_GROUP + Error(SUBJECT_ID) , data = studyData)
</code></pre>

<p>Now, the output from R looks like this:</p>

<pre><code>Error: SUBJECT_ID
                                      Df Sum Sq Mean Sq F value Pr(&gt;F)
ARM_GROUP                             1     29    29.1   0.074  0.786
VISIT_NAME                            2   1220   610.0   1.551  0.214
ARM_GROUP:VISIT_NAME                  2    945   472.7   1.202  0.302
Residuals                             269 105790   393.3               

Error: Within
                                      Df Sum Sq Mean Sq F value   Pr(&gt;F)    
VISIT_NAME                            2   2182  1091.2  13.759 2.54e-06 ***
PLANNED_ARM_DESC:VISIT_NAME           2    175    87.4   1.102    0.334    
Residuals                             198  15703    79.3                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>This is confusing because <code>Error (SUBJECT_ID)</code> contains variables from within subjects analysis also. Is this because the study is unbalanced? What will be the correct way to approach this problem?</p>
"
"0.174077655955698","0.177534854723893","235168","<p>I'm studying Design and Analysis of Experiments, 8th Edition. Douglas C. Montgomery is the author. I'm trying to replicate the first example he gives in Chapter 13, Experiments with Random Factors.</p>

<p>In this example, there are measurements in a critical dimension on a part. 20 parts are randomly selected and measured by 3 operators, also selected at random. I want to fit two models to this data. The first one I call full model and it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + (\tau\beta)_{ij} + \varepsilon_{ijk}$$</p>

<p>The other model I call reduced model ant it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}$$</p>

<p>Both $\tau_i, i=1, \cdots, 20$ and $\beta_j, j=1, 2, 3$ are random effects. The code I'm using to analyze my problem is below:</p>

<pre><code>gauge &lt;- structure(list(part = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 
18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 
19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 
20L), operator = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), replication = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L), measurement = c(21L, 24L, 20L, 27L, 
19L, 23L, 22L, 19L, 24L, 25L, 21L, 18L, 23L, 24L, 29L, 26L, 20L, 
19L, 25L, 19L, 20L, 23L, 21L, 27L, 18L, 21L, 21L, 17L, 23L, 23L, 
20L, 19L, 25L, 24L, 30L, 26L, 20L, 21L, 26L, 19L, 20L, 24L, 19L, 
28L, 19L, 24L, 22L, 18L, 25L, 26L, 20L, 17L, 25L, 23L, 30L, 25L, 
19L, 19L, 25L, 18L, 20L, 24L, 21L, 26L, 18L, 21L, 24L, 20L, 23L, 
25L, 20L, 19L, 25L, 25L, 28L, 26L, 20L, 19L, 24L, 17L, 19L, 23L, 
20L, 27L, 18L, 23L, 22L, 19L, 24L, 24L, 21L, 18L, 25L, 24L, 31L, 
25L, 20L, 21L, 25L, 19L, 21L, 24L, 22L, 28L, 21L, 22L, 20L, 18L, 
24L, 25L, 20L, 19L, 25L, 25L, 30L, 27L, 20L, 23L, 25L, 17L)), .Names = c(""part"", 
""operator"", ""replication"", ""measurement""), class = ""data.frame"", row.names = c(NA, 
-120L))

###############
# full model
fit.full &lt;- lmer(measurement ~ (1|part) + (1|operator) + (1|part:operator), data=montgomery)
summary(fit.full)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator) + (1 | part:operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups        Name        Variance Std.Dev.
 part:operator (Intercept)  0.00000 0.0000  
 part          (Intercept) 10.25127 3.2018  
 operator      (Intercept)  0.01063 0.1031  
 Residual                   0.88316 0.9398  
Number of obs: 120, groups:  part:operator, 60; part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95

###############
# reduced model
fit.reduced &lt;- lmer(measurement ~ (1|part) + (1|operator), data=montgomery)
summary(fit.reduced)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups   Name        Variance Std.Dev.
 part     (Intercept) 10.25127 3.2018  
 operator (Intercept)  0.01063 0.1031  
 Residual              0.88316 0.9398  
Number of obs: 120, groups:  part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95    
</code></pre>

<p>However, I'm getting different estimates from the ones in the book. Montgomery used Minitab to fit its model and here are his results for the full model:</p>

<p><a href=""http://i.stack.imgur.com/1aeGL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1aeGL.png"" alt=""Anova Table for the Full Model""></a></p>

<p>They are different from mine. Notice how his <code>part*operator</code> has a negative estimation, while mine is zero. However, his estimates for the reduced model are the same as mine:</p>

<p><a href=""http://i.stack.imgur.com/SaGVu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SaGVu.png"" alt=""Anova Table for the Reduced Model""></a></p>

<p>So, my question about his problem are:</p>

<ol>
<li><p>Why our estimates differ for the full model? I understand that I can't have a negative variance like the one he got, but why does Minitab doesn't set it to zero? </p></li>
<li><p>Using R, where (or how) can I get an ANOVA table like the one Minitab presents? I couldn't test my hypothesis in this problem because I can't find the p-values associated with the parameters I'm testing.</p></li>
</ol>
"
