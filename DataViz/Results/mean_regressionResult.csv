"V1","V2","V3","V4"
"0.051172824211752","0.0642293744423385","  1432","<p>In answering <a href=""http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur"">this</a> question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?</p>
"
"0.0948769553749019","0.0952675579132743","  1571","<p>I am trying to recreate (in R) a frequentist hypothesis testing in Bayesian from, by calculating Bayes factors of the null (H0) and alternative (H1) models.</p>

<p>The model is simply a simple linear regression that tries to detect a trend in global temp. data from 1995 to 2009 (<a href=""http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt"" rel=""nofollow"">here</a>). Therefore, H0 is no trend (i.e. slope = 0), or similary, the H0 model is a linear model with only the intercept. </p>

<p>So I calculated the <code>lm()</code> of both models to arrive at negative log likelihood values that are significantly different. The p-value for the H1 lm() model is 0.0877.</p>

<p>I also calculated this in a Bayesian way by using <a href=""http://cran.r-project.org/web/packages/MCMCpack/index.html"" rel=""nofollow"">MCMCpack</a>, and I get negative log likelihood values that are <strong>super duper uber</strong> different. Log likelihood values of 13.7 and 4.3 are about a 10000 fold difference in their likelihood ratios (where <a href=""http://en.wikipedia.org/wiki/Bayes_factor"" rel=""nofollow"">>100 is considered to be ""decisive""</a>).</p>

<p>The means and sds of the estimates are very similar, so why am I getting such different likelihood values? (particularly for the Bayesian H0 model) I feel like there is a gap in my understanding on marginal likelihoods, but I can't pinpoint the problem.</p>

<p>Thanks</p>

<pre><code>library(MCMCpack)

## data: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt

head(hadcru, 2)
##  Year      1      2      3      4      5      6      7      8      9     10
## 1 1850 -0.691 -0.357 -0.816 -0.586 -0.385 -0.311 -0.237 -0.340 -0.510 -0.504
## 2 1851 -0.345 -0.394 -0.503 -0.480 -0.391 -0.264 -0.279 -0.175 -0.211 -0.123
##       11     12    Avg
## 1 -0.259 -0.318 -0.443
## 2 -0.141 -0.151 -0.288

hadcru.lm &lt;- lm(Avg ~ 1 + Year, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)))
hadcru.lm.zero &lt;- lm(Avg ~ 1, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)))

hadcru.mcmc &lt;- MCMCregress(Avg ~ 1 + Year, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)), thin = 100, mcmc = 100000, b0 = c(-20, 0), B0 = c(.00001, .00001), marginal = ""Laplace"")
hadcru.mcmc.zero &lt;- MCMCregress(Avg ~ 1, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)), thin = 100, mcmc = 100000, b0 = c(0), B0 = c(.00001), marginal = ""Laplace"")

-logLik(hadcru.lm)
## 'log Lik.' -14.55338 (df=3)
-logLik(hadcru.lm.zero)
## 'log Lik.' -12.80723 (df=2)

attr(hadcru.mcmc, ""logmarglike"")
##           [,1]
## [1,] -13.65188
attr(hadcru.mcmc.zero, ""logmarglike"")
##           [,1]
## [1,] -4.310564
</code></pre>

<p><img src=""http://www.skepticalscience.com/images/HadCRUT_1995_2009.gif"" alt=""alt text""></p>
"
"0.0814154647783432","0.0817506471951855","  3497","<p>I have a fairly larege file 100M rows and 30 columns or so on which I would like to run multiple regressions. I have specialized code to run the regressions on the entire file, but what I would like to do is draw random samples from the file and run them in R.
The strategy is:
              randomly sample N rows from the file without replacement
              run a regression and save the coefficients of interest
              repeat this process M times with different samples 
              for each coefficient calculate the means and standard errors  of 
                the coefficeints over M runs.</p>

<p>I would like to interpret the mean computed over M runs as an estimate of the values of the coefficients computed on the whole data set, and the stadard errors of the means as estimates of the standard errors of the coefficients computed on the entire data set.</p>

<p>Experiments show this to be a promising strategy, but I am not sure about the underlying theory. Are my estimators consistent efficient and unbiased? If they are consistent how quickly should they converge? What tradeoffs of M and N are best?</p>

<p>I would very much appreciate it if someone could point me to the papers, books etc. with the relevanth theory.</p>

<p>Best regards and many thanks,</p>

<p>Joe Rickert</p>
"
"0.103142124625879","0.0956000809414361","  5135","<p>the help pages in R assume I know what those numbers mean. I don't :)
I'm trying to really intuitively understand every number here. I will just post the output and comment on what I found out. There might (will) be mistakes, as I'll just write what I assume. Please correct me, and I will edit the wrong parts.<br>
Mainly I'd like to know what the t-value in the coefficients mean, and why they print the residual standard error. I hope someone can clarify that.</p>

<pre><code>Call:
lm(formula = iris$Sepal.Width ~ iris$Petal.Width)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.09907 -0.23626 -0.01064  0.23345  1.17532 
</code></pre>

<p>A 5-point-summary of the residuals (Their mean is always 0, right?). The numbers can be used  (I'm guessing here) to quickly see if there are any big outliers. Also you can already see it here if the residuals are far from normally distributed (they should be normally distributed).</p>

<pre><code>Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       3.30843    0.06210  53.278  &lt; 2e-16 ***
iris$Petal.Width -0.20936    0.04374  -4.786 4.07e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Estimates $\hat{\beta_i}$ , computed by least squares regression. Also, the standard error $\sigma_{\beta_i}$ . I'd like to know how this is calculated.<br>
Also, no idea where the t value and the corresponding p come from. I know $\hat{\beta}$ should be normal distributed, but how is the t value calculated?</p>

<pre><code>Residual standard error: 0.407 on 148 degrees of freedom
</code></pre>

<p>$\sqrt{ \frac{1}{n-p} \epsilon^T\epsilon }$ , I guess. But why do we calculate that, and what does it say us?</p>

<pre><code>Multiple R-squared: 0.134,  Adjusted R-squared: 0.1282 
</code></pre>

<p>$ R^2 = \frac{s_\hat{y}^2}{s_y^2} $ , which is $ \frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} $ . The ratio is close to 1 if the points lie on a straight line, and 0 if they are random.<br>
What is the adjusted R-squared?</p>

<pre><code>F-statistic: 22.91 on 1 and 148 DF,  p-value: 4.073e-06 
</code></pre>

<p>F and p for the <strong>whole</strong> model, not only for single $\beta_i$s as previous. The F value is $ \frac{s^2_{\hat{y}}}{\sum\epsilon_i} $ . The bigger it grows, the more unlikely it is that the $\beta$'s do not have any effect at all.</p>
"
"0.06396603026469","0.0642293744423385","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.0404556697031367","0.0406222231851194","  5434","<p>I am doing multiple regression with some data (5 predictors, 1 response). Since the response is discrete and non-negative, I thought I would try Poisson regression. However, the data are significantly overdispersed (variance > mean), so I am now trying negative binomial regression.</p>

<p>I was able to fit the model with this code.</p>

<pre><code>library(MASS)
model.nb &lt;- glm.nb(Response ~ Pred1 + Pred2 + Pred3 + Pred4 + Pred5 - 1, data=d)
</code></pre>

<p>Now I would like to see if I can get a better fit by including interactions between the predictors. However, when I try to do so, I get the following error.</p>

<pre><code>&gt; model.nb.intr &lt;- glm.nb(Response ~ Pred1 * Pred2 * Pred3 * Pred4 * Pred5 - 1, data=d)
Error: no valid set of coefficients has been found: please supply starting values
</code></pre>

<p>Any ideas what may be causing this?</p>
"
"0.0404556697031367","0.0406222231851194","  5952","<p>Is there a way of plotting the regression line of a piecewise model like this, other than using <code>lines</code> to plot each segment separately, or using <code>geom_smooth(aes(group=Ind), method=""lm"", fill=FALSE)</code> ?</p>

<pre><code>m.sqft &lt;- mean(sqft)
model &lt;- lm(price~sqft+I((sqft-m.sqft)*Ind))
# sqft, price: continuous variables, Ind: if sqft&gt;mean(sqft) then 1 else 0

plot(sqft,price)
abline(reg = model)
Warning message:
In abline(reg = model) :
  only using the first two of 3regression coefficients
</code></pre>

<p>Thank you.</p>
"
"0.0495478739876288","0.0497518595104995","  6268","<p>I'm searching how to (visually) explain simple linear correlation to first year students.</p>

<p>The classical way to visualize would be to give an Y~X scatter plot with a straight regression line.</p>

<p>Recently, I came by the idea of extending this type of graphics by adding to the plot 3 more images, leaving me with: the scatter plot of y~1, then of y~x, resid(y~x)~x and lastly of residuals(y~x)~1 (centered to the mean)</p>

<p>Here is an example of such a visualization:
<img src=""http://i.stack.imgur.com/Pe2ul.png"" alt=""alt text""></p>

<p>And the R code to produce it:</p>

<pre><code>set.seed(345)
x &lt;- runif(50) * 10
y &lt;- x +rnorm(50)


layout(matrix(c(1,2,2,2,2,3 ,3,3,3,4), 1,10))
plot(y~rep(1, length(y)), axes = F, xlab = """", ylim = range(y))
points(1,mean(y), col = 2, pch = 19, cex = 2)
plot(y~x, ylab = """", )
abline(lm(y~x), col = 2, lwd = 2)

plot(c(residuals(lm(y~x)) + mean(y))~x, ylab = """", ylim = range(y))
abline(h =mean(y), col = 2, lwd = 2)

plot(c(residuals(lm(y~x)) + mean(y))~rep(1, length(y)), axes = F, xlab = """", ylab = """", ylim = range(y))
points(1,mean(y), col = 2, pch = 19, cex = 2)
</code></pre>

<p>Which leads me to my question: I would appreciate any <strong>suggestions on how this graph can be enhanced</strong> (either with text, marks, or any other type of relevant visualizations). Adding relevant R code will also be nice.</p>

<p>One direction is to add some information of the R^2 (either by text, or by somehow adding lines presenting the magnitude of the variance before and after the introduction of x)
Another option is to highlight one point and showing how it is ""better explained"" thanks to the regression line.  Any input will be appreciated.</p>
"
"0.0948769553749019","0.0952675579132743","  6404","<p>Anyone that follows baseball has likely heard about the out-of-nowhere MVP-type performance of Toronto's Jose Bautista. In the four years previous, he hit roughly 15 home runs per season. Last year he hit 54, a number surpassed by only 12 players in baseball history. </p>

<p>In 2010 he was paid 2.4 million and he's asking the team for 10.5 million for 2011. They're offering 7.6 million. If he can repeat that in 2011, he'll be easily worth either amount. But what are the odds of him repeating? How hard can we expect him to regress to the mean? How much of his performance can we expect was due to chance? What can we expect his regression-to-the-mean adjusted 2010 totals to be? How do I work it out?</p>

<p>I've been playing around with the Lahman Baseball Database and squeezed out a query that returns home run totals for all players in the previous five seasons who've had at least 50 at-bats per season.</p>

<p>The table looks like this (notice Jose Bautista in row 10)</p>

<pre><code>     first     last hr_2006 hr_2007 hr_2008 hr_2009 hr_2010
1    Bobby    Abreu      15      16      20      15      20
2   Garret Anderson      17      16      15      13       2
3  Bronson   Arroyo       2       1       1       0       1
4  Garrett   Atkins      29      25      21       9       1
5     Brad   Ausmus       2       3       3       1       0
6     Jeff    Baker       5       4      12       4       4
7      Rod  Barajas      11       4      11      19      17
8     Josh     Bard       9       5       1       6       3
9    Jason Bartlett       2       5       1      14       4
10    Jose Bautista      16      15      15      13      54
</code></pre>

<p>and the full result (232 rows) is available <a href=""http://datalove.org/files/regress_hr.csv"">here</a>.</p>

<p>I really don't know where to start. Can anyone point me in the right direction? Some relevant theory, and R commands would be especially helpful. </p>

<p>Thanks kindly</p>

<p>Tommy</p>

<p>Note: The example is a little contrived. Home runs definitely aren't the best indicator of a player's worth, and home run totals don't consider the varying number of chances per season that a batter has the chance to hit home runs (plate appearances). Nor does it reflect that some players play in more favourable stadiums, and that league average home runs change year over year. Etc. Etc. If I can grasp the theory behind accounting for regression to the mean, I can use it on more suitable measures than HRs.</p>
"
"0.0495478739876288","0.0497518595104995","  6562","<p>I have read an <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">article</a> from Christopher Manning, and saw an interesting code for collapsing categorical variable in an logistic regression model:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class == 1), family=binomial(""logit""))
</code></pre>

<p>Does the <code>I(class == 1)</code> means that the <code>class</code> variable has been recoded into either it is <code>1</code> or it is not <code>1</code>?</p>

<p>After that, I am thinking of modifying it a bit:</p>

<pre><code>glm(ced.del ~ cat + follows + I(class %in% C(1,2)), family=binomial(""logit""))
</code></pre>

<p>I am planning to merge the variable <code>class</code> from <code>c(1,2,3,4)</code> into two groups, one group contains <code>c(1,2)</code>, another group contains <code>c(3,4)</code>, can the code above give me the result I want?</p>

<p>Thanks.</p>
"
"0.0495478739876288","0.0497518595104995","  6776","<p>I would appreciate some help getting some EM stuff straight. So, say I generate data in R as follows:</p>

<pre><code>N       &lt;- 100
epsilon &lt;- rnorm(N)
X       &lt;-  10*runif(N)
beta.0  &lt;- 10
beta.1  &lt;-  3
sigma   &lt;- 2
Y       &lt;-  beta.0 + beta.1 * X + sigma * epsilon
epsilon2 &lt;- rnorm(N)
X2 &lt;- 10*runif(N)
Y2 &lt;-  3 - X2 + 0.25 * epsilon2
Y.mix &lt;- c(Y, Y2)
X.mix &lt;- c(X, X2)
</code></pre>

<p>Now, in expectation maximization, in the first step, I have some prior probability, say 0.5, of the data being from either one or the other distribution. So, using EM I know I can estimate the mean and variance of the two mixtures. From looking at a density plot, it seems like the means are at about -2 and 30 for the data I simulated. But, at what stage in EM do I back out the betas? I want to recover the slope, intercept, and sd deviation parameters for the 2 regression-type equations.</p>

<p>Thanks for an clarification.</p>
"
"0.0572129567690623","0.0574484989621426","  7899","<p>I need to draw a complex graphics for visual data analysis.
I have 2 variables and a big number of cases (>1000). For example (number is 100 if to make dispersion less ""normal""):</p>

<pre><code>x &lt;- rnorm(100,mean=95,sd=50)
y &lt;- rnorm(100,mean=35,sd=20)
d &lt;- data.frame(x=x,y=y)
</code></pre>

<p>1) I need to plot raw data with point size, corresponding the relative frequency of coincidences, so <code>plot(x,y)</code> is not an option - I need point sizes. What should be done to achieve this?</p>

<p>2) On the same plot I need to plot 95% confidence interval ellipse and line representing change of correlation (do not know how to name it correctly) - something like this:</p>

<pre><code>library(corrgram)
corrgram(d, order=TRUE, lower.panel=panel.ellipse, upper.panel=panel.pts)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R561Q.png"" alt=""correlogramm""></p>

<p>but with both graphs at one plot.</p>

<p>3) Finally, I need to draw a resulting linar regression model on top of this all:</p>

<pre><code>r&lt;-lm(y~x, data=d)
abline(r,col=2,lwd=2)
</code></pre>

<p>but with error range... something like on QQ-plot:</p>

<p><img src=""http://i.stack.imgur.com/vgvRr.png"" alt=""QQ-plot""></p>

<p>but for fitting errors, if it is possible.</p>

<p>So the question is: </p>

<p><strong>How to achieve all of this at one graph?</strong></p>
"
"0.0948769553749019","0.0952675579132743","  7996","<p>I am evaluating a scenario's output parameter's dependence on three parameters: A, B and C. For this, I am conducting the following experiments:</p>

<ul>
<li>Fix A+B, Vary C - Total four sets of (A+B) each having 4 variations of C</li>
<li>Fix B+C, Vary A - Total four sets of (B+C) each having 3 variations of C</li>
<li>Fix C+A, Vary B - Total four sets of (C+A) each having 6 variations of C</li>
</ul>

<p>The output of any simulation is the value of a variable over time. For instance, A could be the area, B could be the velocity and C could be the number of vehicles. The output variable I am observing is the number of car crashes over time. </p>

<p>I am trying to determine which parameter(s) dominate the outcome of the experiment. By dominate, I mean that sometimes, the outcomes just does not change when one of the parameters change but when some other parameter is changed even by a small amount, a large change in the output is observed. I need to capture this effect and output some analysis from which I can understand the dependence of the output on the input parameters. A friend suggested Sensitivity Analysis but am not sure if there are simpler ways of doing it. Can someone please help me with a good (possibly easy because I don't have a Stats background) technique? It would be great if all this can be done in R.</p>

<p><strong>Update:</strong> 
I used linear regression to obtain the following:</p>

<pre><code>lm(formula = T ~ A + S + V)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.01606    0.16437  -0.098 0.923391    
A            0.80199    0.15792   5.078 0.000112 ***
S           -0.27440    0.13160  -2.085 0.053441 .  
V           -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>Does this mean that the output depends mostly on A and less on V?</p>
"
"0.134176277047853","0.134728672455117","  8340","<p><strong>Update: I wanted to clarify that this is a simulation. Sorry if I confused everyone. I have also used meaningful names for my variables.</strong></p>

<p>I am not a statistician so please correct me if I make a blunder in explaining what I want. In regard to my <a href=""http://stats.stackexchange.com/questions/7996/what-is-a-good-way-of-estimating-the-dependence-of-an-output-variable-on-the-inpu"">previous question</a>, I have reproduced parts of my question here for reference.</p>

<blockquote>
  <p>I am evaluating a scenario's output
  dependence on three
  variables: Area, Speed and NumOfVehicles. For this, I am
  conducting the following experiments:</p>
  
  <ul>
  <li>Fix Area+Speed, Vary NumOfVehicles - Total four sets of (Area+Speed) each having 4 variations of NumOfVehicles</li>
  <li>Fix Speed+NumOfVehicles, Vary Area - Total four sets of (Speed+NumOfVehicles) each having 3 variations of Area</li>
  <li>Fix NumOfVehicles+Area, Vary Speed - Total four sets of (NumOfVehicles+Area) each having 6 variations of Speed</li>
  </ul>
  
  <p>The output of any simulation is the
  value of a variable over time. The output
  variable I am observing is the time at which 80% of the cars crashe.</p>
  
  <p>I am trying to determine which
  parameter(s) dominate the outcome of
  the experiment. By dominate, I mean
  that sometimes, the outcomes just does
  not change when one of the parameters
  change but when some other parameter
  is changed even by a small amount, a
  large change in the output is
  observed. I need to capture this
  effect and output some analysis from
  which I can understand the dependence
  of the output on the input parameters.
  A friend suggested Sensitivity
  Analysis but am not sure if there are
  simpler ways of doing it. Can someone
  please help me with a good (possibly
  easy because I don't have a Stats
  background) technique? It would be
  great if all this can be done in R.</p>
</blockquote>

<p>My previous result was not very satisfactory looking at the regression results. So what I did was that I went ahead and repeated all my experiments 20 times each with different variations of each variable (so for instance, instead of 4 variations of Area, I now have 8 and so on). Following is the summary I obtained out of R after using linear regression:</p>

<pre><code>Call:
lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.13315 -0.06332 -0.01346  0.04484  0.29676 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      0.04285    0.02953   1.451    0.148    
Area             0.70285    0.02390  29.406  &lt; 2e-16 ***
Speed           -0.15560    0.02080  -7.479 2.12e-12 ***
NumOfVehicles   -0.27447    0.02927  -9.376  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.08659 on 206 degrees of freedom
Multiple R-squared: 0.8304, Adjusted R-squared: 0.8279 
F-statistic: 336.2 on 3 and 206 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>as opposed to my previous result:</p>

<pre><code>lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   -0.01606    0.16437  -0.098 0.923391    
Area           0.80199    0.15792   5.078 0.000112 ***
Speed         -0.27440    0.13160  -2.085 0.053441 .  
NumOfVehicles -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>From my understanding, my current results have a lower standard error so that is good. In addition the Pr value also seems quite low which tells me that this result is better than my previous result. So can I go ahead and say that A has the maximum effect on the output and then come S and V in that order? Can I make any other deductions from this result?</p>

<p>Also, I was suggested that I look into adding additional variates like $A^2$ etc. but if $A$ is the area, what does saying ""time"" depends on $A^2$ actually mean? </p>
"
"0.0707974219804893","0.0812444463702388","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0","0.0287242494810713","  8891","<p>I am trying to predict real estate sales prices. </p>

<ul>
<li>In my dataset there are independent variables that are both nominal and numeric (square meters, prices etc.) </li>
<li>Before feeding the data to any regression algorithm I'd like to preprocess it correctly (binning, normalizing mean / std deviation, discretization etc.)</li>
<li>I am overwhelmed by the many methods listed in various textbooks and try to find out what works well in practice</li>
</ul>

<p>Although the most reasonable answer to this question is probably 'it depends', could you maybe give me some rules of thumb / war stories / general advice?</p>

<ul>
<li>How do you usually preprocess data for regression? </li>
<li>What methods do you usually apply?</li>
<li>Which regression algorithms need a special treatment?</li>
</ul>

<p>As my tools I am using weka and R.</p>

<p>Many Thanks!</p>
"
"0.0286064783845312","0.0287242494810713","  9027","<p>I have two logistic regression models in R made with <code>glm()</code>.  They both use the same variables, but were made using different subsets of a matrix.  Is there an easy way to get an average model which gives the means of the coefficients and then use this with the predict() function?</p>

<p>[ sorry if this type of question should be posted on a programming site let me know and I'll post it there ]</p>

<p>Thanks</p>
"
"0.0904616275314925","0.0817506471951855","  9506","<p>I am new to R and to time series analysis. I am trying to find the trend of a long (40 years) daily temperature time series and tried to different approximations. First one is just a simple linear regression and second one is Seasonal Decomposition of Time Series by Loess.</p>

<p>In the latter it appears that the seasonal component is greater than the trend. But, how do I quantify the trend? I would like just a number telling how strong is that trend.</p>

<pre><code>     Call:  stl(x = tsdata, s.window = ""periodic"")
     Time.series components:
        seasonal                trend            remainder               
Min.   :-8.482470191   Min.   :20.76670   Min.   :-11.863290365      
1st Qu.:-5.799037090   1st Qu.:22.17939   1st Qu.: -1.661246674 
Median :-0.756729578   Median :22.56694   Median :  0.026579468      
Mean   :-0.005442784   Mean   :22.53063   Mean   : -0.003716813 
3rd Qu.:5.695720249    3rd Qu.:22.91756   3rd Qu.:  1.700826647    
Max.   :9.919315613    Max.   :24.98834   Max.   : 12.305103891   

 IQR:
         STL.seasonal STL.trend STL.remainder data   
         11.4948       0.7382    3.3621       10.8051
       % 106.4          6.8      31.1         100.0  
     Weights: all == 1
     Other components: List of 5   
$ win  : Named num [1:3] 153411 549 365  
    $ deg  : Named int [1:3] 0 1 1   
$ jump : Named num [1:3] 15342 55 37  
    $ inner: int 2  
$ outer: int 0
</code></pre>

<p><img src=""http://i.stack.imgur.com/jwCSr.png"" alt=""enter image description here""></p>
"
"0.134303277811777","0.140719508946058"," 10017","<p>I am trying to understand standard error ""clustering"" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either <code>plm</code> or writing my own function. I'll use the <code>diamonds</code> data from the <code>ggplot2</code> package.</p>

<p>I can do fixed effects with either dummy variables</p>

<pre><code>&gt; library(plyr)
&gt; library(ggplot2)
&gt; library(lmtest)
&gt; library(sandwich)
&gt; # with dummies to create fixed effects
&gt; fe.lsdv &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; ct.lsdv &lt;- coeftest(fe.lsdv, vcov. = vcovHC)
&gt; ct.lsdv

t test of coefficients:

                      Estimate Std. Error  t value  Pr(&gt;|t|)    
carat                 7871.082     24.892  316.207 &lt; 2.2e-16 ***
factor(cut)Fair      -3875.470     51.190  -75.707 &lt; 2.2e-16 ***
factor(cut)Good      -2755.138     26.570 -103.692 &lt; 2.2e-16 ***
factor(cut)Very Good -2365.334     20.548 -115.111 &lt; 2.2e-16 ***
factor(cut)Premium   -2436.393     21.172 -115.075 &lt; 2.2e-16 ***
factor(cut)Ideal     -2074.546     16.092 -128.920 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.</p>

<pre><code>&gt; # by demeaning with degrees of freedom correction
&gt; diamonds &lt;- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] 
&gt; fe.dm &lt;- lm(price.dm ~ carat.dm + 0, data = diamonds)
&gt; ct.dm &lt;- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)
&gt; ct.dm

t test of coefficients:

         Estimate Std. Error t value  Pr(&gt;|t|)    
carat.dm 7871.082     24.888  316.26 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I can't replicate these results with <code>plm</code>, because I don't have a ""time"" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).</p>

<pre><code>&gt; plm.temp &lt;- plm(price ~ carat, data = diamonds, index = ""cut"")
duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]]) : 
</code></pre>

<p>I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their <code>cluster</code> option (<a href=""http://www.stata.com/support/faqs/stat/cluster.html"">explained here</a>), which is to solve $$\hat V_{cluster} = (X&#39;X)^{-1} \left( \sum_{j=1}^{n_c} u_j&#39;u_j \right) (X&#39;X)^{-1}$$ where $u_j = \sum_{cluster~j} e_i * x_i$, $n_c$ si the number of clusters, $e_i$ is the residual for the $i^{th}$ observation and $x_i$ is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's <em>Cross Section and Panel Data</em>). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get <code>plm</code> to do clusters on one factor, I'm not sure how to benchmark my code.</p>

<pre><code>&gt; # with cluster robust se
&gt; lm.temp &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; 
&gt; # using the model that Stata uses
&gt; stata.clustering &lt;- function(x, clu, res) {
+     x &lt;- as.matrix(x)
+     clu &lt;- as.vector(clu)
+     res &lt;- as.vector(res)
+     fac &lt;- unique(clu)
+     num.fac &lt;- length(fac)
+     num.reg &lt;- ncol(x)
+     u &lt;- matrix(NA, nrow = num.fac, ncol = num.reg)
+     meat &lt;- matrix(NA, nrow = num.reg, ncol = num.reg)
+     
+     # outer terms (X'X)^-1
+     outer &lt;- solve(t(x) %*% x)
+ 
+     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i
+     for (i in seq(num.fac)) {
+         index.loop &lt;- clu == fac[i]
+         res.loop &lt;- res[index.loop]
+         x.loop &lt;- x[clu == fac[i], ]
+         u[i, ] &lt;- as.vector(colSums(res.loop * x.loop))
+     }
+     inner &lt;- t(u) %*% u
+ 
+     # 
+     V &lt;- outer %*% inner %*% outer
+     return(V)
+ }
&gt; x.temp &lt;- data.frame(const = 1, diamonds[, ""carat""])
&gt; summary(lm.temp)

Call:
lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)

Residuals:
     Min       1Q   Median       3Q      Max 
-17540.7   -791.6    -37.6    522.1  12721.4 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
carat                 7871.08      13.98   563.0   &lt;2e-16 ***
factor(cut)Fair      -3875.47      40.41   -95.9   &lt;2e-16 ***
factor(cut)Good      -2755.14      24.63  -111.9   &lt;2e-16 ***
factor(cut)Very Good -2365.33      17.78  -133.0   &lt;2e-16 ***
factor(cut)Premium   -2436.39      17.92  -136.0   &lt;2e-16 ***
factor(cut)Ideal     -2074.55      14.23  -145.8   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1511 on 53934 degrees of freedom
Multiple R-squared: 0.9272, Adjusted R-squared: 0.9272 
F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: &lt; 2.2e-16 

&gt; stata.clustering(x = x.temp, clu = diamonds$cut, res = lm.temp$residuals)
                        const diamonds....carat..
const                11352.64           -14227.44
diamonds....carat.. -14227.44            17830.22
</code></pre>

<p>Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in <a href=""http://sekhon.berkeley.edu/causalinf/sp2010/section/week7.pdf"">this lecture</a>), but I can't figure it out in R. Thanks!</p>
"
"0.0814154647783432","0.090834052439095"," 10036","<p>I am using R to replicate a study and obtain mostly the same results the author reported. At one point, however, I calculate marginal effects that seem to be unrealistically small. I would greatly appreciate if you could have a look at my reasoning and the code below and see if I am mistaken at one point or another.</p>

<p>My sample contains 24535 observations, the dependent variable <code>x028bin</code> is a binary variable taking on the values 0 and 1, and there are furthermore 10 explaining variables. Nine of those independent variables have numeric levels, the independent variable <code>f025grouped</code> is a factor consisting of different religious denominations.</p>

<p>I would like to run a probit regression including dummies for religious denomination and then compute marginal effects. In order to do so, I first eliminate missing values and use cross-tabs between the dependent and independent variables to verify that there are no small or 0 cells. Then I run the probit model which works fine and I also obtain reasonable results:</p>

<pre><code>probit4AKIE &lt;- glm(x028bin ~ x003 + x003squ + x025secv2 + x025terv2 + x007bin + x04chief + x011rec + a009bin + x045mod + c001bin + f025grouped, family=binomial(link=""probit""), data=wvshm5red2delna, na.action=na.pass)

summary(probit4AKIE)
</code></pre>

<p>However, when calculating marginal effects with all variables at their means from the probit coefficients and a scale factor, the marginal effects I obtain are much too small (e.g. 2.6042e-78). The code looks like this:</p>

<pre><code>ttt &lt;- cbind(wvshm5red2delna$x003,
    wvshm5red2delna$x003squ, wvshm5red2delna$x025secv2, wvshm5red2delna$x025terv2,
wvshm5red2delna$x007bin, wvshm5red2delna$x04chief, wvshm5red2delna$x011rec,
    wvshm5red2delna$a009bin, wvshm5red2delna$x045mod, wvshm5red2delna$c001bin,
wvshm5red2delna$f025grouped, wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,
    wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,
wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,
wvshm5red2delna$f025grouped) #I put variable ""f025grouped"" 9 times because this variable consists of 9 levels

ttt &lt;- as.data.frame(ttt)

xbar &lt;- as.matrix(mean(cbind(1,ttt[1:19]))) #1:19 position of variables in dataframe ttt

betaprobit4AKIE &lt;- probit4AKIE$coefficients

zxbar &lt;- t(xbar) %*% betaprobit4AKIE

scalefactor &lt;- dnorm(zxbar)

marginprobit4AKIE &lt;- scalefactor * betaprobit4AKIE[2:20] 

#(2:20 are the positions of variables in the output of the probit model 'probit4AKIE' 
#(variables need to be in the same ordering as in data.frame ttt), the constant in   
#the model occupies the first position)

marginprobit4AKIE #in this step I obtain values that are much too small
</code></pre>
"
"0.06396603026469","0.0642293744423385"," 10316","<p>I'm working on a multiple logistic regression in R using <code>glm</code>. The predictor variables are continuous and categorical. An extract of the summary of the model shows the following:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   2.451e+00  2.439e+00   1.005   0.3150
Age           5.747e-02  3.466e-02   1.658   0.0973 .
BMI          -7.750e-02  7.090e-02  -1.093   0.2743
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Confidence intervals:</p>

<pre><code>                  2.5 %       97.5 %
(Intercept)  0.10969506 1.863217e+03
Age          0.99565783 1.142627e+00
BMI          0.80089276 1.064256e+00
...
</code></pre>

<p>Odd ratios:</p>

<pre><code>                 Estimate Std. Error   z value Pr(&gt;|z|)
(Intercept)  1.159642e+01  11.464683 2.7310435 1.370327
Age          1.059155e+00   1.035269 5.2491658 1.102195
B            9.254228e-01   1.073477 0.3351730 1.315670
...
</code></pre>

<p>The first output shows that $Age$ is significant. However, the confidence interval for $Age$ includes the value 1 and the odds ratio for $Age$ is very close to 1. What does the significant p-value from the first output mean? Is $Age$ a predictor of the outcome or not?</p>
"
"0.0707974219804893","0.0812444463702388"," 10444","<p>In general, I standardize my independent variables in regressions, in order to properly compare the coefficients (this way they have the same units: standard deviations). However, with panel/longitudinal data, I'm not sure how I should standardize my data, especially if I estimate a hierarchical model.</p>

<p>To see why it can be a potential problem, assume you have $i = 1, \ldots, n$ individuals measured along $t=1,\ldots, T$ periods and you measured a dependent variable, $y_{i,t}$ and one independent variable $x_{i,t}$. If you run a complete pooling regression, then it's ok to standardize your data in this way: $x.z = (x- \text{mean}(x))/\text{sd}(x)$, since it will not change t-statistic. On the other hand, if you fit an unpooled regression, i.e., one regression for each individual, then you should standardize your data by individual only, not the whole dataset (in R code): </p>

<pre><code>for (i in 1:n) {
  for ( t in 1:T) x.z[i] =  (x[i,t] - mean(x[i,]))/sd(x[i,]) 
}
</code></pre>

<p>However, if you fit a simple hierarchical model with a varying intercept by individuals, then you are using a shrinkage estimator, i.e, you are estimating a model between pooled and unpooled regression. How should I standardize my data? Using the whole data like a pooled regression? Using only individuals, like in the unpooled case?</p>
"
"0.051172824211752","0.0642293744423385"," 11236","<p>I've been using R's <code>lm</code> to do some linear regression, but decided to give <code>MCMCregress</code> a try to get a feel for how it works. As expected, I got basically the same coefficients, but the extra <code>sigma2</code> value puzzles me.</p>

<p>When I do a <code>qqmath</code> plot of the coefficients, I get the following graph, and I'm puzzled by the sigma2 plot. It's obviously not linear, but I'm not sure if that's meaningful in this context. I assume it's sigma <em>squared</em>, and when I took the square root and plotted it, the line was straighter, but still curved.</p>

<p>I guess my question boils down to: what is sigma2 telling me about the MCMC regression fit, and is a graph of it useful or should I ignore the graph and focus on something else? (All of the diagnostics and graphs I've done on my original <code>lm</code> fit seem to indicate that the fit is good, so I'm also wondering if the MCMC regression gives me more information or not.)</p>

<p><img src=""http://i.stack.imgur.com/wayi4.png"" alt=""qqmath plot of MCMCregress results""></p>

<p>(If I need to provide the actual data, I can. I'm hoping that an answer depends more on what sigma2 is rather than on specific values.)</p>
"
"NaN","NaN"," 12605","<p>I've been playing around with random forests for regression and am having difficulty working out exactly what the two measures of importance mean, and how they should be interpreted.</p>

<p>The <code>importance()</code> function gives two values for each variable: <code>%IncMSE</code> and <code>IncNodePurity</code>.
Is there simple interpretations for these 2 values?</p>

<p>For <code>IncNodePurity</code> in particular, is this simply the amount the RSS increase following the removal of that variable?</p>

<p>I greatly appreciate any enlightment :)</p>
"
"0.0495478739876288","0.0497518595104995"," 13152","<p>I always use <code>lm()</code> in R to perform linear regression of $y$ on $x$. That function returns a coefficient $\beta$ such that $$y = \beta x.$$</p>

<p>Today I learned about <strong>total least squares</strong> and that <code>princomp()</code> function (principal component analysis, PCA) can be used to perform it. It should be good for me (more accurate). I have done some tests using <code>princomp()</code>, like:</p>

<pre><code>r &lt;- princomp(Â ~Â xÂ +Â y)
</code></pre>

<p>My problem is: how to interpret its results? How can I get the regression coefficient? By ""coefficient"" I mean the number $\beta$ that I have to use to multiply the $x$ value to give a number close to $y$.</p>
"
"0.064873395163555","0.0759972207238908"," 13478","<p>Using R, I have developed three models:  </p>

<ul>
<li>linear regression using <code>lm()</code>;</li>
<li>decision tree using <code>rpart()</code>;</li>
<li>k-nearest neighbor using <code>kknn()</code>. </li>
</ul>

<p>I would like to conduct leave-one-out cross-validation tests and compare these models. However, which error metric should I use for better representation? Does mean absolute percentage error (MAPE) or sMAPE (symmetric MAPE) look fine? Please suggest me a metric. </p>

<p>For example, when I conducted leave-one-out CV tests on linear regression (LR) and decision tree (DT) models, the sMAPE error values are 0.16 and 0.20. However, the R-squared values of LR and DT are 0.85 and 0.92 respectively. Where sMAPE computed as <code>[sum (abs(predicted - actual)/((predicted + actual)/2))] / (number of data points)</code>. Here DT is pruned regression tree. These R^2 values are computed on full data set. There are a total of 60 data points in the set.</p>

<pre><code>Model  R^2   sMAPE
 LR    0.85   0.16
 DT    0.92   0.20
</code></pre>
"
"0.06396603026469","0.0642293744423385"," 13617","<p>I've been implementing the GLMNET version of elastic net for linear regression with another software than R. I compared my results with the R function glmnet in lasso mode on <a href=""http://www.stanford.edu/~hastie/Papers/LARS/diabetes.data"">diabetes data</a>.</p>

<p>The variable selection is ok when varying the value of the parameter (lambda) but I obtain slightly different values of coefficients. For this and other reasons I think it comes from the intercept in the update loop, when I compute the current fit, because I don't vary the intercept (which I take as the mean of the target variable) in the whole algorithm : as explained in Trevor Hastie's article ( <a href=""http://www.jstatsoft.org/v33/i01/paper"">Regularization Paths for Generalized Linear Models via Coordinate Descent</a>, Page 7, section 2.6):</p>

<blockquote>
  <p>the intercept is not regularized, [...] for all values of [...] lambda [the L1-constraint parameter]</p>
</blockquote>

<p>But despite the article, the R function glmnet does provide different values for the intercept along the regularization path (the lambda different values). Does anyone has a clue about how the values of the Intercept are computed?</p>
"
"0.0809113394062735","0.0609333347776791"," 13778","<ol>
<li><p>for the general case : data is normal-gamma (mean normal and sd is gamma) and I want to estimate the $b$ and $a$ distribution ( they assumed to be normal) in $y=bx+a$ using Bayesian regression. I know I need assumption for my priors but I think I have to be OK with most simplistic case. not very good in reading statistics literature to derive the code from heavy literature.</p></li>
<li><p>{trying to be more clear} piece of code is helpful as well as pseudo-code of how to do it. Here I explain the case I did for single parameter as an example. there are 2 ways actually by conditioning on ro/p (precision) or unconditional. Keep these pre-calculation: <code>Sxx&lt;-sum((x-xbar)^2) , Syy&lt;-sum((y-ybar)^2) , Sxy&lt;-sum((x-xbar)(y-ybar)) , See&lt;-sum( ( y-ybar-b(x-xbar))^2)</code> }</p></li>
</ol>

<p>Posterior distribution for $\rho$ is Gamma with shape $(n-2)/2$ and scale $2/S_{ee}$.</p>

<p>Conditional on $\rho$, $\eta$ and $\beta$ are independent and normally distributed, $\eta$ has mean $y$ and precision $n\rho$, $\beta$  has mean $b$ and precision $S_{xx}\rho$.</p>

<p>Uncoditional: Marginal posterior distribution for $\eta$ and $\beta$. $\eta$ has nonstandard t distribution with: Center $\bar y$,  spread $(n(n-2)/S_{ee})-1/2$,  degrees of freedom $n-2$.</p>

<p>$\beta$ has nonstandard t distribution with:  Center $\bar y$, spread $(S_{xx}(n-2)/S_{ee})-1/2$,  degrees of freedom $n-2$. </p>

<p>$\eta$ and $\beta$ are uncorrelated but not independent when we marginalize out $\rho$.</p>

<p>I dont know how this could be for MultiVariate Case. I know I can use Winbugs but I need to write it in Matlab.</p>
"
"0.06396603026469","0.0642293744423385"," 14005","<p>I'm trying to understand how exactly factors work in R. Let's say I want to run a regression using some sample data in R:</p>

<pre><code>&gt; data(CO2)
&gt; colnames(CO2)
[1] ""Plant""     ""Type""      ""Treatment"" ""conc""      ""uptake""   
&gt; levels(CO2$Type)
[1] ""Quebec""      ""Mississippi""
&gt; levels(CO2$Treatment)
[1] ""nonchilled"" ""chilled""   
&gt; lm(uptake ~ Type + Treatment, data = CO2)

Call:
lm(formula = uptake ~ Type + Treatment, data = CO2)

Coefficients:
 (Intercept)   TypeMississippi  Treatmentchilled  
       36.97            -12.66             -6.86  
</code></pre>

<p>I understand that <code>TypeMississippi</code> and <code>Treatmentchilled</code> are treated as booleans: For each row, the initial uptake is <code>36.97</code>, and we subtract <code>12.66</code> if it's of type Mississippi and <code>6.86</code> if it was chilled. I'm having trouble understanding something like this:</p>

<pre><code> &gt; lm(uptake ~ Type * Treatment, data = CO2)

 Call:
 lm(formula = uptake ~ Type * Treatment, data = CO2)

 Coefficients:
                 (Intercept)                   TypeMississippi  
                      35.333                            -9.381  
            Treatmentchilled  TypeMississippi:Treatmentchilled  
                      -3.581                            -6.557  
</code></pre>

<p>What does it mean to multiply two factors together in an <code>lm</code>?</p>
"
"0.0904616275314925","0.090834052439095"," 14399","<p>For my microsimulation, I want to use R to predict values and draw a random sample based on this prediction.</p>

<p>To clarify my point: I want to simulate the number of chronic conditions people suffer from ($y_t$) at a certain point in time. I have a few waves of panel data available to estimate a relation between age, sex, number of chronic conditions in the previous observation period (plus some others that I might include in later stages).</p>

<p>Suppose my regression model is $y_t = Î²_0 + Î²_1 age + Î²_2 sex + Î²_3y_{t-1} +u $.   </p>

<p>Since R provides me with coefficients for the betas, it is easy to predict $y_t$ given the independent variables. However, this is not what I want to do. Instead, I want my population of about 1300 individuals to resemble the variance in the possible outcomes of $y_t$ (otherwise after a few steps my simulated population wonâ€™t include those unlucky ones with much more chronic conditions than the average).   </p>

<p>I believe what I have to do is to draw a random sample from the distribution of the predicted value $y_t$, conditional on the independent variables. I further believe this can be done by drawing random numbers with mean $Î²$ and variance $var(Î²)$, multiplied by the actual values of the independent variables.  </p>

<p>So my question is: Is this the correct approach? Will this produce reliable values? Or do I need to take possible covariation of the independent variables etc. into consideration?</p>

<p><strong>Edit</strong>: Another point came to my mind. Does it make a difference whether $y_t$ or $y_t - y_{t-1}$ is my left hand side variable?</p>

<p>Thank you for your ideas.</p>
"
"0.114624397492221","0.115096299024505"," 14694","<p>I have a dataset of genomic information which I'm going to be comparing with various biochemical markers. Unfortunately a lot of the biochemical markers have limited ranges in their assays, so I have a lot of data that looks like ""40"", "">45"", ""35"", "">45"" for tests that have a threshold at 45 (for example).
My intended analysis for most of this data is linear regression in R. So what is the statistically correct way to deal with this data?</p>

<ol>
<li><p>Ignore it, let R cast the values with "">"" to <code>NA</code> and potentially lose information about important associations</p></li>
<li><p>Make the over threshold values equal to the threshold. This has similar problems to 1)</p></li>
<li><p>It depends. Sigh. Could you please give me some pointers as to what other considerations I should be thinking about or information you might need to answer my question? </p></li>
</ol>

<p>Edit: Based on the comments I've given more information about my datasets. The values which are out of range (GFR and Fol) are independent variables which I'll use in linear regression like so:</p>

<pre><code>lm(H~allele+Age+Sex+as.double(GFR)+as.double(Fol))
</code></pre>

<p>GFR looks like: </p>

<pre><code>summary(as.double(GFR)) 
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
31.00   70.00   77.00   75.66   83.00  100.00  105.00
</code></pre>

<p>and appears to be normally distributed:</p>

<pre><code>V = qqnorm(na.omit(as.double(GFR))
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 105 values coded as "">90"" (not sure why the summary said Max is 100) out of 434.</p>

<p>Fol is distributed like so:</p>

<pre><code>summary(as.double(Fol))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
6.10   23.20   29.80   29.14   35.70   45.30    8.00
</code></pre>

<p>and also appears to be normally distributed:</p>

<pre><code>V  = qqnorm(na.omit(as.double(Fol)))   
cor(V$x, V$y)
[1] 0.9911351
</code></pre>

<p>There are 8 out of 434 variables in Fol coded are "">45.3"". I took my cue for calling these normally distributed from <a href=""http://www.math.utah.edu/~davar/ps-pdf-files/Assessing_Normality.pdf"" rel=""nofollow"">this assessment of normality guide</a> ).</p>

<p>I also have another variable CRP which is a dependent variable, which I'd like to do linear regression on similarly to the above. CRP has 11 out of 434 coded as ""&lt;0.2"". Its distribution is:</p>

<pre><code>summary(as.double(CRP))
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
0.200   0.600   1.300   2.674   2.650 112.400  11.000
</code></pre>

<p>The data graphed is clearly not normaly and it has a correlation with qqnorm of 0.5153663. The value of 112 is a clear outlier. </p>

<p>I hope that makes it more clear. Please let me know if you need more information. Thanks for your help.</p>
"
"0.131226987728912","0.137756661144821"," 15160","<p>I have a large dataset with patients and I'm studying a rare outcome (~ 2%) and death is a competing risk (mean age ~69 years). I've used the R ""cmprsk"" package for my statistics and it seems that competing risks and the Cox regression are performing similarly although the competing risk analysis is more conservative giving hazard ratios closer to 1.</p>

<p>I've been suggested to do a Poisson regression on the data but the results don't make any sense and I would be really grateful to get some input on the benefits of doing this kind of analysis on survival data. I've created this simulation for creating a dataset with similar risk factors:</p>

<pre><code>library(""cmprsk"")
# The time for the study
accrual_time &lt;- 10
followup_time &lt;- 1

base_risk &lt;- list(""event"" = .015, ""cmprsk"" = .1)

risk_factors &lt;- list(list(""frequency""=.1, 
                ""event"" = base_risk$event*.5, 
                ""cmprsk"" = base_risk$cmprsk*2),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*1, 
                ""cmprsk"" = base_risk$cmprsk*1),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*-.5, 
                ""cmprsk"" = base_risk$cmprsk*0))

# Number of subjects
n &lt;- 5000

# Create base time, sequential inclusion
time_in_study &lt;- rep(c(1:n)/n*accrual_time + followup_time, 1)

set.seed(100)

# Create empty sets
x &lt;- matrix(0, ncol=length(risk_factors), nrow=n)
time_2_event &lt;- rep(0, n)
time_2_comprsk &lt;- rep(0, n)

# Create each studied observation and outcome
for(i in 1:n){
    # Set base risk
    event_risk &lt;- base_risk$event 
    comp_risk &lt;- base_risk$cmprsk

    for(j in 1:length(risk_factors)){
        x[i, j] &lt;- rbinom(1, 1, risk_factors[[j]]$frequency)[1]

        # If there is a risk factor defined
        if (x[i, j] &gt; 0){
            event_risk &lt;- event_risk +
                    risk_factors[[j]]$event
            comp_risk &lt;- comp_risk + 
                    risk_factors[[j]]$cmprsk
        }
    }

    # Time 2 event/risk is 1/rate meaning that higher number -&gt; shorter time
    time_2_event[i] &lt;- rexp(1, rate=event_risk)[1]
    time_2_comprsk[i] &lt;- rexp(1, rate=comp_risk)[1]
}

cn &lt;- c()
for(i in 1:length(risk_factors)){
    ev_rsk &lt;- risk_factors[[i]]$event/base_risk$event+1
    cmp_rsk &lt;- risk_factors[[i]]$cmprsk/base_risk$cmprsk+1
    name &lt;- paste(""Risk factor no: "", i, ""\n * ev="", ev_rsk, "" cr="", cmp_rsk, "" *"", sep="""")
    cn &lt;- c(cn, name)
}
colnames(x) &lt;- cn

# Select the event that happens first: study ends, evenent occurs, a competing event occurs
time &lt;- apply(cbind(time_in_study, time_2_event, time_2_comprsk), 1, min)

# Outcome identifiers
event &lt;- (time_2_event == time) + 0
comprsk &lt;- (time_2_comprsk == time) + 0
cens &lt;- event+2*(event==0 &amp; comprsk==1)

out.cox_ev &lt;- coxph(Surv(time, event)~x)
summary(out.cox_ev)

out.crr_ev &lt;- crr(time, cens, x, failcode=1)
summary(out.crr_ev)

out.cox_cmprsk &lt;- coxph(Surv(time, comprsk)~x)
summary(out.cox_cmprsk)

out.crr_cmprsk &lt;- crr(time, cens, x, failcode=2)
summary(out.crr_cmprsk)
</code></pre>

<p>The output makes sense but when I do a:</p>

<pre><code>out.glm_pr &lt;- glm(event ~ x, family=""poisson"")
summary(out.glm_pr)
</code></pre>

<p>It gives estimates of:</p>

<ul>
<li>RF 1 ~ .14 </li>
<li>RF 2 ~ .41 </li>
<li>RF 3 ~ -.23</li>
</ul>

<p>My questions: </p>

<ul>
<li>Is the glm() code correct or should I somehow transform my data?</li>
<li>Does the Poisson output make any sense and how should if so interpret it?</li>
<li>What are the benefits/pitfalls in using Poisson regression for survival data?</li>
</ul>

<p>Thanks!</p>

<hr>

<h2>UPDATE</h2>

<p>After adding exp(out.glm_pr$coefficients) the results are almost identical to the competing risk regression, here's a forest plot that compares the three:</p>

<p><img src=""http://i.stack.imgur.com/14Zt0.png"" alt=""A forestplot comparing the different methods - Poisson: 1.152  1.509  0.794, CRR: 1.151 1.524 0.812, Cox PH: 1.897 1.931 0.798""></p>

<p>The x-axis is perhaps not entirely valid (should be ""incident rate ratios"" for the Poisson regression) but why are the outcomes for CRR &amp; poisson almost identical?</p>

<p>As for testing over-dispersion I've found these two methods:</p>

<pre><code>&gt; library(qcc)
&gt; qcc.overdispersion.test(event)

Overdispersion test Obs.Var/Theor.Var Statistic p-value
       poisson data         0.9391878      4695 0.99902
&gt; 
&gt; library(pscl)
&gt; out.glm_nb &lt;- glm.nb(event ~ x)
Warning messages:
1: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
2: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
&gt; odTest(out.glm_nb)
Likelihood ratio test of H0: Poisson, as restricted NB model:
n.b., the distribution of the test-statistic under H0 is non-standard
e.g., see help(odTest) for details/references

Critical value of test statistic at the alpha= 0.05 level: 2.7055 
Chi-Square Test Statistic =  -0.0139 p-value = 0.5 
</code></pre>

<p>I conclude that there isn't any evidence of over-dispersion or are there other methods better suited for testing over-dispersion in this kind of survival data?</p>

<p>The quasipoisson analysis gives similar values:</p>

<pre><code>&gt; out.glm_quasi_pr &lt;- glm(event ~ x, family=quasipoisson(link=""log""))
&gt; round(exp(out.glm_quasi_pr$coefficients), 3)
(Intercept)       xRF 1       xRF 2       xRF 3 
      0.059       1.152       1.509       0.794 
</code></pre>
"
"0.0495478739876288","0.0497518595104995"," 15469","<p>Recently we discussed on SO how to update a standard linear regression summary with NeweyWest standard errors. I used <code>coeftest</code>from the <code>sandwich</code> package. It was told to use unclass to update my already existing summary like this: </p>

<pre><code>library(sandwich)
library(lmtest)
temp.lm &lt;- lm(runif(100) ~ rnorm(100))
temp.summ &lt;- summary(temp.lm)
temp.summ$coefficients &lt;- unclass(coeftest(temp.lm, vcov. = NeweyWest)
</code></pre>

<p>Now I wonder whether the joint parameters shown in the summary aren't affected at all when using a NeweyWest VC matrix? I mean with this code they are not affected obviously â€“Â but is this correct? Note this is not a syntax but a stats question :) Stuff like</p>

<pre><code>Residual standard error: 1.177 on 83 degrees of freedom  
Multiple R-squared: 0.7265, Adjusted R-squared:  0.71 
F-statistic:  44.1 on 5 and 83 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>remains the same. Are there any cases that need adjustment as well?</p>
"
"0.06396603026469","0.0642293744423385"," 15597","<p>As my previous questions I'm trying to solve a problem with my stocks tests.
I tried Breusch-Pagan test for heteroscedasticity but some residuals still pass these tests.</p>

<p>My procedure is:</p>

<ul>
<li><p>Get two stocks prices (I have a matrix with two columns that represent the price lists)</p></li>
<li><p>I do a linear regression, like: <code>lm(prices[,1] ~ prices[,2])</code></p></li>
<li><p>Then I test the residuals of the linear regression with Unit Root tests (PP and KPSS)</p></li>
<li><p>After these tests I do Breusch-Pagan test because I only need to work with stocks that have constant variance during all the period. I do: <code>bptest(prices[,1] ~ prices[,2])</code></p></li>
</ul>

<p>Ok, now I still get strange results, with strange i mean that when i plot the residuals i see that the variance is not constant (take a look at the chart below). So now, I need to understand how better test is the variance is constant. </p>

<p>I read someone use GARCH (1,1) but I never used it, could someone exmplain it or maybe give me other tests to try?</p>

<p><img src=""http://i.stack.imgur.com/OYqKq.png"" alt=""enter image description here""></p>
"
"0.0495478739876288","0.0497518595104995"," 15900","<p>I plan to do a simulation study where I compare the performance of several robust correlation techniques with different distributions (skewed, with outliers, etc.). With <em>robust</em>, I mean the ideal case of being robust against a) skewed distributions, b) outliers, and c) heavy tails.</p>

<p>Along with the Pearson correlation as a baseline, I was thinking to include following more robust measures:</p>

<ul>
<li>Spearman's $\rho$</li>
<li>Percentage bend correlation (Wilcox, 1994, [1])</li>
<li>Minimum volume ellipsoid, minimum covariance determinant (<code>cov.mve</code>/ <code>cov.mcd</code> with the <code>cor=TRUE</code> option)</li>
<li>Probably, the winsorized correlation</li>
</ul>

<p>Of course there are many more options (especially if you include robust regression techniques as well), but I want to restrict myself to the mostly used/ mostly promising approaches.</p>

<p><strong>Now I have three questions (feel free to answer only single ones):</strong></p>

<ol>
<li><strong>Are there other robust correlational methods I could/ should include?</strong></li>
<li><strong>Which robust correlation techniques are</strong> <em><strong>actually</em></strong>  <strong>used in your field?</strong>
<sub>(Speaking for psychological research: Except Spearman's $\rho$, I have never seen any robust correlation technique outside of a technical paper. Bootstrapping is getting more and more popular, but other robust statistics are more or less non-existent so far).</sub></li>
<li><strong>Are there already systematical comparisons of multiple correlation techniques that you know of?</strong></li>
</ol>

<p>Also feel free to comment the list of methods given above.</p>

<hr>

<p>[1] Wilcox, R. R. (1994). The percentage bend correlation coefficient. <em>Psychometrika</em>, 59, 601-616.</p>
"
"0.0286064783845312","0.0287242494810713"," 18208","<p>I'm wondering how to interpret the coefficient standard errors of a regression when using the display function in R.</p>

<p>For example in the following output:</p>

<pre><code>lm(formula = y ~ x1 + x2, data = sub.pyth)
        coef.est coef.se
(Intercept) 1.32     0.39   
x1          0.51     0.05   
x2          0.81     0.02   

n = 40, k = 3
residual sd = 0.90, R-Squared = 0.97
</code></pre>

<p>Does a higher standard error imply greater significance? </p>

<p>Also for the residual standard deviation, a higher value means greater spread, but the R squared shows a very close fit, isn't this a contradiction?</p>
"
"0.0286064783845312","0.0287242494810713"," 18404","<p>I want to regress two series (one big series divided in half) with the mean of the big series. 
I do that because I would like to ""investigate"" the relationship between those two subseries and the mean.</p>

<p>Does this make any sense for you?</p>

<p>When running the code below, I don't understand why I don't get p-value:</p>

<pre><code>&gt; x  = rnorm(200)
&gt; m  = mean(x) 
&gt; anova(lm(rep(m, 100) ~ x[1:100]), lm(rep(m, 100) ~ x[101:200])) 
Analysis of Variance Table

Model 1: rep(m, 100) ~ x[1:100]
Model 2: rep(m, 100) ~ x[101:200]
  Res.Df RSS Df Sum of Sq F Pr(&gt;F)
1     98   0                      
2     98   0  0         0   
</code></pre>
"
"0.0404556697031367","0.0203111115925597"," 18738","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/12398/how-to-interpret-f-and-p-value-in-anova"">How to interpret F- and p-value in ANOVA?</a>  </p>
</blockquote>



<p>I found that I can use ANOVA also for ONE Model, doing something like:</p>

<pre><code>&gt; anova(lm(a~b))
Analysis of Variance Table

Response: a
           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    
b           1 0.002679 0.0026791  11.191 0.0009001 ***
Residuals 398 0.095282 0.0002394                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I know that ANOVA check the means BUT what test is that if I use only ONE model?
If the p.value is above 0.05 it means that the regression fit good?</p>
"
"0.0404556697031367","0.0406222231851194"," 20052","<p>I am working in a genomics project and I ended up having a huge table with around 800 measurements (cases/rows), around 200 channels (columns/continuous variables) and 5 categories (one categorical column)</p>

<p>I would like to do two things: </p>

<ul>
<li>Try to find sub-groups in the different levels of the categorical variable that I already have</li>
<li>create a new classification of these 800 measurements based only in the information </li>
</ul>

<p>I have been doing my homework and read about using different strategies like (k-means or PCA) but I have found that it is very useful to get rid of redundant variables. How can I choose these properly? </p>

<p>Someone recommended me to use multinomial regression, any good resource you recommend to have a bite?</p>

<p>I am using R.
Many thanks</p>
"
"0.134176277047853","0.134728672455117"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.0583927294833815","0.0703597544730292"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.0700712753800578","0.0703597544730292"," 20672","<p>I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S.</p>

<p>If one calls <code>gls(Y ~ X, correlation = S)</code>, what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being <code>S^(-1)*Y = S^(-1) * X</code>.</p>

<p>How is this transformation actually performed? From the literature I've consulted, I've seen everything from:</p>

<pre><code>X.transformed &lt;- solve(chol(S)) %*% X 
#The inverse of the Choleski decomposition of S times the vertical vector X, 
#which in my case does nothing to the data
</code></pre>

<p>to</p>

<pre><code>X.transformed &lt;- chol(solve(S)) %*% X 
# which has negative values and gives meaningless values of X
</code></pre>

<p>Another method I've seen is transforming the dependent variable by </p>

<pre><code>chol(solve(S)) %*% Y 
</code></pre>

<p>and the independent variable by </p>

<pre><code>chol(solve(S)) %*% cbind(1,X) 
</code></pre>

<p>and doing the linear model using the transformed intercept terms in the first column of the X matrix: </p>

<pre><code>lm(Y ~ X - 1)
</code></pre>

<p>On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) I suppose it would follow from this that an $R^2$ statistic on transformed variables is also meaningless?</p>
"
"0.0404556697031367","0.0406222231851194"," 20725","<p>I have a model that looks like </p>

<pre><code>lm(y ~ lag(x, -1) + lag(z, -1))
</code></pre>

<p>So basically, this is a time series regression with exogenous variables, and I want to carry out a rolling analysis of sample forecasts, meaning that:
I first used a subsample (e.g., 1990-1995) for estimation, then I performed a one step ahead forecast, then I added one observation and made another one step ahead forecast, and so on.</p>

<p>I have tried to work with <code>rollapply</code>, defining the model as <code>arima(0,0,0)</code> with <code>xreg=lags</code> of the other variables, but that doesn't work. </p>

<p>Your help would be much appreciated!</p>
"
"0.051172824211752","0.0642293744423385"," 21043","<p>R linear regression seems to fail if my predictor variance is very small, but nonzero:</p>

<pre><code>&gt; reg = lm(V1~V2,data)
&gt; summary(reg)

Call:
lm(formula = V1 ~ V2, data = data)

Residuals:
Min      1Q  Median      3Q     Max
-15.968  -4.898   1.627   5.218   8.468

Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  11.7963     0.6036   19.54   &lt;2e-16 ***
V2                NA         NA      NA       NA
</code></pre>

<p>Here is an excerpt from my data... (it comes out this way after boxcox...)</p>

<pre><code>&gt; print(data$V2,digits=15)
 [1] -0.640196668095416 -0.640196668115515 -0.640196668075674 -0.640196668083867
 [5] -0.640196668103316 -0.640196668073982 -0.640196668094188 -0.640196668081038
etc
</code></pre>

<p>And here is the regression working if I manually remove the mean value:</p>

<pre><code>&gt; shifted = data$V2+0.640196668
&gt; shifted
 [1] -9.541601e-11 -1.155150e-10 -7.567402e-11 -8.386702e-11 -1.033160e-10
 [6] -7.398204e-11 -9.418799e-11 -8.103807e-11 -8.288503e-11 -9.411305e-11
etc

&gt; reg = lm(data$V1~shifted)
&gt; reg

Coefficients:
(Intercept)      shifted
  6.308e+00   -5.771e+10
</code></pre>

<p>Can anyone tell me if I'm using <code>lm</code> wrong?  Thank you...</p>
"
"0.10703564115707","0.107476300250388"," 23042","<p>Can someone explain my Cox model to me in plain English? </p>

<p>I fitted the following Cox regression model to <strong>all</strong> of my data using the <code>cph</code> function. My data are saved in an object called <code>Data</code>. The variables <code>w</code>, <code>x</code>, and <code>y</code> are continuous; <code>z</code> is a factor of two levels. Time is measured in months. Some of my patients are missing data for variable <code>z</code> (<em>NB</em>: I have duly noted Dr. Harrell's suggestion, below, that I impute these values so as to avoid biasing my model, and will do so in the future).</p>

<pre><code>&gt; fit &lt;- cph(formula = Surv(time, event) ~ w + x + y + z, data = Data, x = T, y = T, surv = T, time.inc = 12)

Cox Proportional Hazards Model
Frequencies of Missing Values Due to Each Variable
Surv(time, event)    w    x    y    z 
                0    0    0    0   14 

                Model Tests          Discrimination 
                                            Indexes        
Obs       152   LR chi2      8.33    R2       0.054    
Events     64   d.f.            4    g        0.437    
Center 0.7261   Pr(&gt; chi2) 0.0803    gr       1.548    
                Score chi2   8.07                      
                Pr(&gt; chi2) 0.0891                      

                   Coef    S.E.   Wald Z   Pr(&gt;|Z|)
         w      -0.0133  0.0503    -0.26     0.7914  
         x      -0.0388  0.0351    -1.11     0.2679  
         y      -0.0363  0.0491    -0.74     0.4600  
         z=1     0.3208  0.2540     1.26     0.2067
</code></pre>

<p>I also tried to test the assumption of proportional hazards by using the <code>cox.zph</code> command, below, but do not know how to interpret its results. Putting <code>plot()</code> around the command gives an error message.</p>

<pre><code> cox.zph(fit, transform=""km"", global=TRUE)
            rho chisq      p
 w      -0.1125 1.312 0.2520
 x       0.0402 0.179 0.6725
 y       0.2349 4.527 0.0334
 z=1     0.0906 0.512 0.4742
 GLOBAL      NA 5.558 0.2347
</code></pre>

<hr>

<h3>First Problem</h3>

<ul>
<li>Can someone explain the results of the above output to me in plain English? I have a medical background and no formal training in statistics.</li>
</ul>

<h3>Second Problem</h3>

<ul>
<li><p>As suggested by Dr. Harrell, I would like to internally validate my model by performing 100 iterations of 10-fold cross-validation using the <code>rms</code> package (from what I understand, this would entail building <code>100 * 10 = 1000</code> different models and then asking them to predict the survival times of patients that they had never seen).</p>

<p>I tried using the <code>validate</code> function, as shown.</p>

<pre><code>&gt; v1 &lt;- validate(fit, method=""crossvalidation"", B = 10, dxy=T)
&gt; v1
      index.orig training    test optimism index.corrected  n
Dxy      -0.2542  -0.2578 -0.1356  -0.1223         -0.1320 10
R2        0.0543   0.0565  0.1372  -0.0806          0.1350 10
Slope     1.0000   1.0000  0.9107   0.0893          0.9107 10
D         0.0122   0.0128  0.0404  -0.0276          0.0397 10
U        -0.0033  -0.0038  0.0873  -0.0911          0.0878 10
Q         0.0155   0.0166 -0.0470   0.0636         -0.0481 10
g         0.4369   0.4424  0.6754  -0.2331          0.6700 10
</code></pre>

<p>How do you perform the 100x resampling? I think my above code only performs the cross-validation once.</p></li>
<li><p>I then wanted to know how good my model was at prediction. I tried the following:</p>

<pre><code>&gt; c_index &lt;- abs(v1[1,5])/2 + 0.5
&gt; c_index
[1] 0.565984
</code></pre>

<p>Does this mean that my model is only very slightly better than flipping a coin?</p></li>
</ul>

<h3>Third Problem</h3>

<p>Dr. Harrell points out that I have assumed linearity for the covariate effects, and that the number of events in my sample is just barely large enough to fit a reliable model if all covariate effects happen to be linear.</p>

<ul>
<li>Does this mean that I should include some sort of interaction term in my model? If so, any advice as to what to put?</li>
</ul>
"
"0.110792414376932","0.103831970547663"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.0814154647783432","0.090834052439095"," 24072","<p>I am running the following unit root test (Dickey-Fuller) on a time series using the <code>ur.df()</code> function in the <code>urca</code> package.</p>

<p>The command is:</p>

<pre><code>summary(ur.df(d.Aus, type = ""drift"", 6))
</code></pre>

<p>The output is:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.266372 -0.036882 -0.002716  0.036644  0.230738 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  0.001114   0.003238   0.344  0.73089   
z.lag.1     -0.010656   0.006080  -1.753  0.08031 . 
z.diff.lag1  0.071471   0.044908   1.592  0.11214   
z.diff.lag2  0.086806   0.044714   1.941  0.05279 . 
z.diff.lag3  0.029537   0.044781   0.660  0.50983   
z.diff.lag4  0.056348   0.044792   1.258  0.20899   
z.diff.lag5  0.119487   0.044949   2.658  0.00811 **
z.diff.lag6 -0.082519   0.045237  -1.824  0.06874 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.06636 on 491 degrees of freedom
Multiple R-squared: 0.04211,    Adjusted R-squared: 0.02845 
F-statistic: 3.083 on 7 and 491 DF,  p-value: 0.003445 


Value of test-statistic is: -1.7525 1.6091 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.43 -2.86 -2.57
phi1  6.43  4.59  3.78
</code></pre>

<ol>
<li><p>What do the significance codes (Signif. codes) mean? I noticed that some of them where written against: z.lag.1, z.diff.lag.2, z.diff.lag.3 (the ""."" significance code) and z.diff.lag.5 (the ""**"" significance code).</p></li>
<li><p>The output gives me two (2) values of test statistic: -1.7525 and 1.6091. I know that the ADF test statistic is the first one (i.e. -1.7525). What is the second one then?</p></li>
<li><p>Finally, in order to test the hypothesis for unit root at the 95% significance level, I need to compare my ADF test statistic (i.e. -1.7525) to a critical value, which I normally get from a table. The output here seems to give me the critical values through. However, the question is: which critical value between ""tau2"" and ""phi1"" should I use.</p></li>
</ol>

<p>Thank you for your response.</p>
"
"0.11794753195637","0.11843311462705"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.0404556697031367","0.0203111115925597"," 24572","<p>In my previous <a href=""http://stats.stackexchange.com/questions/24380/how-to-get-ellipse-region-from-bivariate-normal-distributed-data"">question</a> I needed to help with ellipse region extraction and determine if point lies in that region or not.
I ended up with this code:</p>

<pre><code>library(ellipse)
library(mvtnorm)
require(spatstat)

netflow &lt;- read.csv(file=""data.csv"",head=FALSE,sep="" "")
#add headers
names(netflow)&lt;-c('timestamps','flows','flows_tcp','flows_udp','flows_icmp','flows_other','packe ts','packets_tcp','packets_udp','packets_icmp','packets_other','octets','octets_tcp','octets_udp','octets_icmp','octets_other')
attach(netflow)

#load library
library(sfsmisc)
#plot
plot(packets,flows,type='p',xlim=c(0,500000),ylim=c(0,50000),main=""Dependence number of flows on number of packets"",xlab=""packets"",ylab=""flows"",pch = 16, cex = .3,col=""#0000ff22"",xaxt=""n"")
#Complete the x axis
eaxis(1, padj=-0.5, cex.axis=0.8)

pktsFlows=subset(na.omit(netflow),select=c(packets,flows))
head(pktsFlows)
#plot(pktsFlows,pch = 16, cex = .3,col=""#0000ff22"")

cPktsFlows &lt;- apply(pktsFlows, 2, mean)
elpPktsFlows=ellipse::ellipse(var(pktsFlows),centre=cPktsFlows,level=0.8)

png(file=""graph.png"")
plot(elpPktsFlows,type='l',xlim=c(0,500000), ylim=c(0,50000))
points(pktsFlows,pch = 19, cex = 0.5,col=""#0000FF82"")
grid(ny=10,nx=10)
dev.off()

W &lt;- owin(poly=elpPktsFlows)
inside.owin(100000,18000,W)
</code></pre>

<p>This produces this <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png"" rel=""nofollow"">graph</a>.</p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png"" alt=""graph ellipse""></p>

<p>Here is the same data with the <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png"" rel=""nofollow"">regression line plotted</a></p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png"" alt=""Plot all with linear regression line"">.</p>

<p>Can you explain me, why the ellipse has this shape? I expected that main axe of ellipse will have the same direction with linear regression line, but it hasn't.</p>

<p>Btw. <a href=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png"" rel=""nofollow"">kernel density estimation</a> also points to 100000 althought there are no points...</p>

<p><img src=""https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png"" alt=""kernel density estimation""></p>
"
"NaN","NaN"," 25068","<p>I am trying to fit data with a GLM (poisson regression) in R.  When I plotted the residuals vs the fitted values, the plot created multiple (almost linear with a slight concave curve) ""lines"".  What does this mean? </p>

<pre><code>library(faraway)
modl &lt;- glm(doctorco ~ sex + age + agesq + income + levyplus + freepoor + 
            freerepa + illness + actdays + hscore + chcond1 + chcond2,
            family=poisson, data=dvisits)
plot(modl)
</code></pre>

<p><img src=""http://i.stack.imgur.com/7gwmX.png"" alt=""enter image description here""></p>
"
"NaN","NaN"," 25284","<p>This is almost dumb and a little embarassing, but I can't figure out how to (or even whether it is possible) to use the <a href=""http://cran.r-project.org/web/packages/plm/index.html"" rel=""nofollow"">plm</a> package in R to run a regression including fixed-effects that do not correspond to the individual observation unit. For instance, I have observations on firms' outcomes, but I need to include sector-specific dummy variables. Of course I would have to ""demean"" these out (because there are so many of them). Is that even possible?</p>
"
"0.0756856276908142","0.0759972207238908"," 25702","<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>

<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>

<p>(i) With an intercept only</p>

<p>$$\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\overline{Y}_{t-1} + d_i\Delta\overline{Y}_{t-1}+ e_i\Delta\overline{Y}_{t-2}+ f_i\Delta\overline{Y}_{i,t-1}+ g_i\Delta\overline{Y}_{i,t-2} + \varepsilon_{i,t}$$</p>

<p>where $\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>

<p>I constructed each of these variables in the following way:</p>

<p>$\Delta Y_t$</p>

<pre><code>dif.yt = diff(yt) 
## yt is the object containing all the observations for a specific country 
## (e.g. Australia)
</code></pre>

<p>$Y_{t-1}$</p>

<pre><code>yt.lag.1 = lag(yt, -1)
</code></pre>

<p>$\overline{Y}_{t-1}$</p>

<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) 
## x is the object containing my entire data frame
</code></pre>

<p>$\Delta \overline{Y}_{t-1}$</p>

<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)
</code></pre>

<p>$\Delta \overline{Y}_{t-2}$</p>

<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))
</code></pre>

<p>$\Delta Y_{t-1}$</p>

<pre><code>dif.yt.lag.1 = diff(yt.lag.1)
</code></pre>

<p>$\Delta Y_{t-2}$</p>

<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)
</code></pre>

<p>After constructing each variable individually, I then run the linear regression</p>

<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + 
                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)
summary(reg)
</code></pre>

<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>

<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>
"
"0.0948769553749019","0.0952675579132743"," 25912","<p>I want to estimate a multivariate variance function in R.  That is, I want to allow the variance (as well as the mean) to vary according to some set of independent variables.  </p>

<p>In this particular case, I want to estimate the effects of a set of typical demographic covariates (age, race, education) on the variance of logged wages.  </p>

<p>What is a good way to implement this in R?  Is there a package that simplifies this?</p>

<p>It may be that this is only a search away - but having searched on the R help pages, Google, Rseek, and StackOverflow, I can't find anything relevant under ""variance function"" or similar. </p>

<p>Any suggestions gratefully received. </p>

<hr>

<p>Thanks for your responses -- I will try to clarify my question.</p>

<p>I am working in a maximum likelihood framework.  I can code this by hand from the log-likelihood, but the real data set has a <em>lot</em> of variables and ""optim"" is very slow, so I would like to find a package in R that makes this more computationally efficient.</p>

<p>I start with the log-likelihood for a basic OLS regression:
$$
\text{ln }L = \sum (-\frac{1}{2} (\text{ln }\sigma^2 - \frac{(y - xB)^2}{\sigma^2}))
$$
Then I relax the assumption of constant variance (homoskedasticity) and redefine the variance as:
$$
\sigma^2 = exp(Z*\gamma)
$$
where $Z$ is the matrix of variables affecting $\sigma^2$.  (Exponentiate so that you don't end up with $\sigma^2$ less than zero.)  When I substitute the reparameterization of $\sigma^2$ into the original log-likelihood and code the new log-likelihood function in R, I get this: </p>

<pre><code>ll.normal.vary &lt;- function (par, X, Y, Z) {
  beta  &lt;-par[1:ncol(X)]
  gamma &lt;- par[(ncol(X)+1):(ncol(X)+ncol(Z))]   
  -1/2* sum((Z %*% gamma) + ((Y - X %*% beta)^2)/exp(Z %*% gamma))
}
</code></pre>

<p>Then I optimize:</p>

<pre><code>v.optim1 &lt;- optim (par = start1, fn=ll.normal.vary, X=x.mat, Y=y.vec, Z=z.mat, 
                   method = ""BFGS"", hessian = F, control = list(fnscale = -1))
v.optim1$par
v.optim1$value
</code></pre>

<p>Here are some sample data if you want to test it:</p>

<pre><code>var1   &lt;- c(0,0,0,1,1,0)
var2   &lt;- c(.28, .07, -.05, .38, .08, -.1)
var3   &lt;- c(-.11, -.17, -.17, -.05, .1, -.01)
x.mat  &lt;- cbind(var1, var2, var3)
y.vec  &lt;- c(.46, .77, .49, .59, .60, .44)
z.mat  &lt;- cbind(var1, var2) 
start1 &lt;- rep(0.1, ncol(x.mat)+ncol(z.mat))
</code></pre>

<p>Thanks again for any tips.</p>
"
"0.110792414376932","0.111248539872496"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.0858194351535935","0.0861727484432139"," 26065","<p>I have compared two regression models using ANCOVA by following <a href=""http://r-eco-evo.blogspot.ca/2011/08/comparing-two-regression-slopes-by.html"" rel=""nofollow"">this tutorial</a> but using my own data. 
Here is a snippet of my data:</p>

<pre><code>             X         MC         CC         RC           FM      AT HS
1     0.874375         NA         NA         NA           NA    grab  1
2     0.451250 0.41948802 0.44885230 0.45113473  0.408781437 release  1
3     0.099375         NA         NA         NA           NA    grab  1
4     0.608125 0.50268263 0.52253593 0.51664870           NA release  1
5     0.151875 0.29038872 0.30763473 0.30972255           NA    grab  2
6     0.948750 0.68792116 0.69742615 0.68139072           NA release  2
7     0.452500 0.42250699 0.45583832 0.47252445           NA    grab  2
8     0.894375 0.62101946 0.62549900 0.62338872           NA release  2
</code></pre>

<p>MC, CC, RC and FM are four variables that can model X. There are two categorical variables each with two levels: Action Type (AT) and Hand Side (HS). Hand side 1 means right and 2 is left hand.</p>

<p>I want to know if hand side is a covariate in any of the models. Consider the case for RC, for this model I perform ANCOVA using the following R command:</p>

<pre><code> aov(X ~ RC * HS, data = X.models)
</code></pre>

<p>here are the results:</p>

<pre><code>             Df Sum Sq Mean Sq   F value    Pr(&gt;F)    
RC            1 58.427  58.427 10148.368 &lt; 2.2e-16 ***
HS            1 10.732  10.732  1864.025 &lt; 2.2e-16 ***
RC:HS         1  0.069   0.069    11.927 0.0005803 ***
Residuals   859  4.946   0.006   
</code></pre>

<p>My understanding is that there is significant interaction between HS and RC, and therefore the difference between the slops of the two models is significant. However, in this case the difference between the intercepts is much larger and more interesting. Is there a complementary measurement that would represent the effect size of the interaction? My current solution is to report the difference in slope and intercept alongside the ANCOVA P value, but I am not sure if that is the best way to do it. </p>
"
"0.10340625341847","0.103831970547663"," 26500","<p>Hello after struggling with using R for the last couple of days I was hoping someone could help me with a statistical analysis I am completing for an environmental science honours project. Using R statistics is not something we have been taught and I am worried that I may have bitten of more then I can chew, however my whole project is based around the <strong>hierarchical partitioning method and the exhaustive search multiple regression analysis method.</strong></p>

<p>The <a href=""http://cran.r-project.org/web/packages/hier.part/index.html"" rel=""nofollow"">hier.part</a> package was installed along with <a href=""http://cran.r-project.org/web/packages/gtools/index.html"" rel=""nofollow"">gtools</a>.</p>

<p>I have converted my dataset to a .csv file with seven independent variables and one dependant variable with around 400 replicates (my intention is to do this analysis on eight datasets in total with different amounts of replicates and another dependant variable, but I am starting with this one). The dependant variable is GPP, the independent variables are, NDVI, Temperature, Precipitation, Solar Radiation, Nutrient Availability and Soil Available Water Capacity.</p>

<p>Secondly I imported the .csv file into R using the script</p>

<pre><code>GPPANDDRIVER &lt;- read.table(""C:\\etc, header=T, sep="","")
</code></pre>

<p>This works fine and I can edit the table using </p>

<pre><code>edit(GPPANDDRIVER)
</code></pre>

<p>After looking at the <code>hier.part</code> package documentation available <a href=""http://cran.r-project.org/web/packages/hier.part/hier.part.pdf"" rel=""nofollow"">here</a> it seems like I need to define Y which in the script below is the dependent variable and define <code>scan</code> which is the independent variables (mentioned before).</p>

<pre><code>hier.part(y, xcan, family = ""gaussian"", gof = ""RMSPE"", barplot = TRUE)
</code></pre>

<p>I was defining the dependant <code>y</code> vector as </p>

<pre><code>y &lt;- as.vector(GPPANDDRIVER[""GPP""])
</code></pre>

<p>This also works fine and I have my y vector. However I am not sure how to load independent variables onto the xcan dataframe part of the script. I have tried typing in two scripts but they have not worked.</p>

<pre><code>xcan &lt;- as.vector(GPPANDDRIVER[-GPP])
## AND
xcan &lt;- data.frame(GPPANDDRIVER[-GPP])
</code></pre>

<p>If anyone could help me find the right script for representing my independant variables as xcan that would be greatly appreciated. Also once defined if I entered in the hier.part script mentioned above would R then show me results of the analysis after processing? I will be moving onto to the regression analysis after this if anyone can shed some light on this first problem.</p>

<pre><code>*information on hier.part arguments.*

**Arguments**

y a vector containing the dependent variables

xcan a dataframe containing the n independent variables

family family argument of glm

gof Goodness-of-fit measure. Currently ""RMSPE"", Root-mean-square â€™predictionâ€™

error, ""logLik"", Log-Likelihood or ""Rsqu"", R-squared

print.vars if FALSE, the function returns a vector of goodness-of-fit measures. If TRUE, a data frame is returned with first column listing variable combinations and the
second column listing goodness-of-fit measures.
</code></pre>
"
"0.0762839423587497","0.0765979986161901"," 26927","<p>I have two sets of data from the FRED database: real GDP (y) and GDP deflator (p) and I want to be able to use R in order to estimate a VAR(p) (p determined by AIC) process and generate the sets of impulse-response functions with the short-run assumptions (Sims, 1980) which utilizes the Cholesky decomposition.</p>

<p>Since this is a website for learning, this is a detailed explanation of the process so that this post could actually ""teach"" something to some of you. If you can help me and you already know how to do it, the first paragraph is actually what I am looking for.</p>

<p>Impulse-response analysis is the analysis of the dynamic response of an economic variable of interest (e.g. real GDP) to shocks in other economic variables such as demand shocks (e.g. inflation) or supply shocks ( e.g. technology). In order to do that, we may want to use a reduced form vector autoregressive process (RVAR):</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt%7D%20%3D%20B_%7B1%7DY_%7Bt-1%7D%20&plus;%20B_%7Bt-2%7D%20&plus;%20...%20&plus;%20B_%7Bt-p%7D%20&plus;%20%5Cvarepsilon%20_%7Bt%7D"" alt=""RVAR(1)""></p>

<p>Where:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt%7D%20%3D%20%5Cbegin%7Bbmatrix%7D%20Y_%7B1%2Ct%7D%5C%5C%20...%5C%5C%20Y_%7Bk%2Ct%7D%5C%5C%20%5Cend%7Bbmatrix%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?B%20%3D%20%5Cbegin%7Bbmatrix%7D%20b_%7B11%7D%20%26%20...%20%26%20b_%7B1k%7D%5C%5C%20...%20%26%20...%20%26%20...%20%5C%5C%20b_%7Bk1%7D%20%26%20...%20%26%20b_%7Bkk%7D%20%5Cend%7Bbmatrix%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cvarepsilon%20_%7Bt%7D%20%3D%20%5Cbegin%7Bbmatrix%7D%20%5Cvarepsilon%20_%7B1%2Ct%7D%5C%5C%20...%5C%5C%20%5Cvarepsilon%20_%7Bk%2Ct%7D%5C%5C%20%5Cend%7Bbmatrix%7D"" alt=""""></p>

<p>The variance-covariance matrix of this process is as follows:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?E%5Cvarepsilon%20_%7Bt%7D%5Cvarepsilon%20_%7Bt%7D%5E%7B%27%7D%20%3D%20%5Csum"" alt=""Variance-covariance matrix""></p>

<p>And it is a symmetric, positive definite matrix whose off-diagonal values are non zero, which means that the error terms are mutually correlated. Consequently, our attempt to trace the dynamic responses of our variable of interest will be hindered. One solution to this problem is the use of a structural vector autoregressive process (SVAR):</p>

<p><img src=""http://latex.codecogs.com/gif.latex?A_%7B0%7DY_%7Bt%7D%20%3D%20A_%7B1%7DY_%7Bt-1%7D%20&plus;%20A_%7B2%7DY_%7Bt-2%7D%20&plus;%20...%20&plus;%20A_%7Bp%7DY_%7Bt-p%7D%20&plus;%20u_%7Bt%7D"" alt=""Variance-covariance matrix""></p>

<p>Where A0 is the contemporaneous relations between the k variables.</p>

<p>We can multiply both sides of this equation by the inverse of the contemporaneous effect:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt%7D%20%3D%20A_%7B0%7D%5E%7B-1%7DA_%7B1%7DY_%7Bt-1%7D%20&plus;%20A_%7B0%7D%5E%7B-1%7DA_%7B2%7DY_%7Bt-2%7D%20&plus;%20...%20&plus;%20A_%7B0%7D%5E%7B-1%7DA_%7Bp%7DY_%7Bt-p%7D%20&plus;%20A_%7B0%7D%5E%7B-1%7Du_%7Bt%7D"" alt=""Structural form VAR""></p>

<p>Where:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cvarepsilon%20_%7Bt%7D%20%3D%20A_%7B0%7D%5E%7B-1%7Du_%7Bt%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?A_%7Bj%7D%20%3D%20A_%7B0%7DB_%7Bj%7D"" alt=""""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?E%5Cvarepsilon%20_%7Bt%7D%5Cvarepsilon%20_%7Bt%7D%5E%7B%27%7D%20%3D%20I"" alt=""""></p>

<p>So here, we need to estimate A0 (which is assumed to be a lower triangular matrix) in order to fully describe the SVAR.</p>

<p>One popular method was proposed by Sims (1980) and involves short-run assumptions using the Cholesky decomposition of the variance-covariance matrix such that:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Csum%20%3D%20PP%5E%7B%27%7D"" alt=""""></p>

<p>Where:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?P%20%3D%20A_%7B0%7D%5E%7B-1%7D"" alt=""""></p>

<p>By recursive substitution of the VAR(1) process:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?Y_%7Bt&plus;j%7D%20%3D%20B_%7B1%7D%5E%7Bj&plus;1%7D%20&plus;%20Pu_%7Bt&plus;j%7D%20&plus;%20B_%7B1%7DPu_%7Bt&plus;j-1%7D%20&plus;%20...%20&plus;%20B_%7B1%7D%5E%7Bj%7DPu_%7Bt%7D"" alt=""""></p>

<p>And finally, the impulse-response function of Y_t+j is:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cpsi%20_%7Bj%7D%20%3D%20B_%7B1%7D%5E%7Bj%7DP"" alt=""""></p>

<hr>

<p>As you can see, I understand the process completely and I would like to be able to do it using R. So what I'm trying to do is:
(i): Estimate the final VAR(p) process (p determined by AIC)
(ii): Generate the impulse-response function</p>

<p>Thank you very much.</p>
"
"0.0707974219804893","0.0609333347776791"," 27351","<p>I want to compare three models, one linear-regression-model, one regression-tree-model (from <code>rpart</code>) and one MARS-model (from <code>mda</code> package).</p>

<p>I want to compare the models using a <em>leave one out cross validation</em> using the mean square error and MAPE. I have the following implementation in R:</p>

<pre><code>library(data.table)
library(rpart)
library(mda)

#Load Sample-Data
data(trees)

#The following models should be compared:
# lm(Volume~Girth+Height, data=trees)
# rpart(Volume~Girth+Height, data=trees)
# mars(trees[,-3], trees[3])

LOOCV&lt;-function(modelCall) {
  unlist(sapply(seq(1,nrow(trees)), function(i) {         
    training=trees[-i,]
    test=trees[i,]

    fit=eval(modelCall)
    testValue = predict(fit, test[1:2])

    test[3]-testValue
  }))
}

LOOCV_MSE&lt;-function(modelCall) {
   sum(LOOCV(modelCall)^2)/nrow(trees)
}

LOOCV_RMSE&lt;-function(modelCall) {
   sqrt(LOOCV_MSE(modelCall))
}

LOOCV_MAPE&lt;-function(modelCall) {
  sum(abs(LOOCV(modelCall)/sapply(seq(1, nrow(trees)), function(i) {trees[i,3]})))/nrow(trees)*100                                    
}


cat(""Cross-Validation Metrics:\n"")
cat(""-------------------------\n"")
cat(""LOOCV MSE for LM:"", LOOCV_MSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for CART:"", LOOCV_MSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for MARS:"", LOOCV_MSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV RMSE for LM:"", LOOCV_RMSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for CART:"", LOOCV_RMSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for MARS:"", LOOCV_RMSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV MAPE for LM:"", LOOCV_MAPE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for CART:"", LOOCV_MAPE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for MARS:"", LOOCV_MAPE(quote(mars(training[,-3], training[3]))),""\n"")
</code></pre>

<p>Outputs:</p>

<pre><code>Cross-Validation Metrics:
-------------------------
LOOCV MSE for LM: 18.15783 
LOOCV MSE for CART: 69.83769 
LOOCV MSE for MARS: 13.72282 

LOOCV RMSE for LM: 4.2612 
LOOCV RMSE for CART: 8.356895 
LOOCV RMSE for MARS: 3.704432 

LOOCV MAPE for LM: 14.6114 
LOOCV MAPE for CART: 23.51401 
LOOCV MAPE for MARS: 10.00316 
</code></pre>

<p>Does this implementation make sense? When whould using MSE on the errors make sense? When would I use MAPE/SMAPE instead? I already read ""<a href=""http://stats.stackexchange.com/questions/13478/metric-to-compare-models"">Metric to compare models?</a>"" and the conclusion there was <em>it depends</em>, can someone explain this further. On what does it depend?</p>

<p>My data is not a time series, it is more like the <code>tree</code> example data. </p>
"
"0.0495478739876288","0.0497518595104995"," 27621","<p>I used R <a href=""http://cran.r-project.org/web/packages/mvpart/index.html"" rel=""nofollow"">mvpart</a> package to create a multivariate regression tree. This is part of the output:</p>

<pre><code>          CP               nsplit         rel error    xerror       xstd
1       0.02717093      0         1.0000000 1.0005358 0.03481409
2       0.01302184      2         0.9456581 0.9521266 0.03306820

Node number 1: 3479 observations,    complexity param=0.02717093
Means=12.94,0.5749,9.375,0.72,1.611,0.973,2.153,0.6209,3.307,3.702,2.422,0.3837,1.499,     Summed MSE=1305.19 
left son=2 (992 obs) right son=3 (2487 obs)
Primary splits:
  Dag           splits as  RRLLRRR, improve=0.02478172, (0 missing)
  Hoofdberoep    splits as  RLRLLRRLLLLRLL, improve=0.02313676, (0 missing)
  Ploegenstelsel splits as  RRLRRRL, improve=0.02191660, (0 missing)
  Werksituatie   splits as  LRLR, improve=0.02179270, (0 missing)
  Werkuren       splits as  RLRRRR, improve=0.02130351, (0 missing)
</code></pre>

<p>How do I have to interpret the different 'improve values' (0.02478172,0.02313676,...) and how are they related to the complexity parameter (0.02717093)?</p>
"
"0.140559022854369","0.146566068538932"," 27830","<p>In a previous post Iâ€™ve wondered how to <a href=""http://stats.stackexchange.com/questions/22494/is-using-a-questionnaire-score-euroqols-eq-5d-with-a-bimodal-distribution-as"">deal with EQ-5D scores</a>. Recently I stumbled upon logistic quantile regression suggested by <a href=""http://www.ncbi.nlm.nih.gov.proxy.kib.ki.se/pubmed/19941281"">Bottai and McKeown</a> that introduces an elegant way to deal with bounded outcomes.
The formula is simple:</p>

<p>$logit(y)=log(\frac{y-y_{min}}{y_{max}-y})$</p>

<p>To avoid log(0) and division by 0 you extend the range by a small value, $\epsilon$. This gives an environment that respects the boundaries of the score. </p>

<p>The problem is that any $\beta$ will be in the logit scale and that makes doesnâ€™t make any sense unless transformed back into the regular scale but that means that the $\beta$ will be non-linear. For graphing purposes this doesnâ€™t matter but not with more $\beta$:s this will be very inconvenient. </p>

<p>My question:</p>

<p><strong>How do you suggest to report a logit $\beta$ without reporting the full span?</strong></p>

<hr>

<h2>Implementation example</h2>

<p>For testing the implementation Iâ€™ve written a simulation based on this basic function:</p>

<p>$outcome=\beta_0+\beta_1* xtest^3+\beta_2*sex$</p>

<p>Where $\beta_0 = 0$, $\beta_1 = 0.5$ and $\beta_2 = 1$. Since there is a ceiling in scores Iâ€™ve set any outcome value above 4 and any below -1 to the max value.</p>

<h3>Simulate the data</h3>

<pre><code>set.seed(10)
intercept &lt;- 0
beta1 &lt;- 0.5
beta2 &lt;- 1
n = 1000
xtest &lt;- rnorm(n,1,1)
gender &lt;- factor(rbinom(n, 1, .4), labels=c(""Male"", ""Female""))
random_noise  &lt;- runif(n, -1,1)

# Add a ceiling and a floor to simulate a bound score
fake_ceiling &lt;- 4
fake_floor &lt;- -1

# Just to give the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)

# Simulate the predictor
linpred &lt;- intercept + beta1*xtest^3 + beta2*(gender == ""Female"") + random_noise
# Remove some extremes
linpred[linpred &gt; fake_ceiling + abs(diff(range(linpred)))/2 |
    linpred &lt; fake_floor - abs(diff(range(linpred)))/2 ] &lt;- NA
#limit the interval and give a ceiling and a floor effect similar to scores
linpred[linpred &gt; fake_ceiling] &lt;- fake_ceiling
linpred[linpred &lt; fake_floor] &lt;- fake_floor
</code></pre>

<p>To plot the above:</p>

<pre><code>library(ggplot2)
# Just to give all the graphs the same look
my_ylim &lt;- c(fake_floor - abs(fake_floor)*.25, 
             fake_ceiling + abs(fake_ceiling)*.25)
my_xlim &lt;- c(-1.5, 3.5)
qplot(y=linpred, x=xtest, col=gender, ylab=""Outcome"")
</code></pre>

<p>Gives this image:</p>

<p><img src=""http://i.stack.imgur.com/luZGu.png"" alt=""Scatterplot from simulation""></p>

<h3>The regressions</h3>

<p>In this section I create the regular linear regression, quantile regression (using the median) and logistic quantile regression. All estimates are based on bootstrapped values using the bootcov() function.</p>

<pre><code>library(rms)

# Regular linear regression
fit_lm &lt;- Glm(linpred~rcs(xtest, 5)+gender, x=T, y=T)
boot_fit_lm &lt;- bootcov(fit_lm, B=500)
p &lt;- Predict(boot_fit_lm, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
lm_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# Quantile regression regular
fit_rq &lt;- Rq(formula(fit_lm), x=T, y=T)
boot_rq &lt;- bootcov(fit_rq, B=500)
# A little disturbing warning:
# In rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique

p &lt;- Predict(boot_rq, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))
rq_plot &lt;- plot.Predict(p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)

# The logit transformations
logit_fn &lt;- function(y, y_min, y_max, epsilon)
    log((y-(y_min-epsilon))/(y_max+epsilon-y))


antilogit_fn &lt;- function(antiy, y_min, y_max, epsilon)
    (exp(antiy)*(y_max+epsilon)+y_min-epsilon)/
        (1+exp(antiy))


epsilon &lt;- .0001
y_min &lt;- min(linpred, na.rm=T)
y_max &lt;- max(linpred, na.rm=T)
logit_linpred &lt;- logit_fn(linpred, 
                          y_min=y_min,
                          y_max=y_max,
                          epsilon=epsilon)

fit_rq_logit &lt;- update(fit_rq, logit_linpred ~ .)
boot_rq_logit &lt;- bootcov(fit_rq_logit, B=500)


p &lt;- Predict(boot_rq_logit, xtest=seq(-2.5, 3.5, by=.001), gender=c(""Male"", ""Female""))

# Change back to org. scale
transformed_p &lt;- p
transformed_p$yhat &lt;- antilogit_fn(p$yhat,
                                    y_min=y_min,
                                    y_max=y_max,
                                    epsilon=epsilon)
transformed_p$lower &lt;- antilogit_fn(p$lower, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)
transformed_p$upper &lt;- antilogit_fn(p$upper, 
                                     y_min=y_min,
                                     y_max=y_max,
                                     epsilon=epsilon)

logit_rq_plot &lt;- plot.Predict(transformed_p, 
             se=T, 
             col.fill=c(""#9999FF"", ""#BBBBFF""), 
             xlim=my_xlim, ylim=my_ylim)
</code></pre>

<h3>The plots</h3>

<p>To compare with the base function Iâ€™ve added this code:</p>

<pre><code>library(lattice)
# Calculate the true lines
x &lt;- seq(min(xtest), max(xtest), by=.1)
y &lt;- beta1*x^3+intercept
y_female &lt;- y + beta2
y[y &gt; fake_ceiling] &lt;- fake_ceiling
y[y &lt; fake_floor] &lt;- fake_floor
y_female[y_female &gt; fake_ceiling] &lt;- fake_ceiling
y_female[y_female &lt; fake_floor] &lt;- fake_floor

tr_df &lt;- data.frame(x=x, y=y, y_female=y_female)
true_line_plot &lt;- xyplot(y  + y_female ~ x, 
                         data=tr_df,
                         type=""l"", 
                         xlim=my_xlim, 
                         ylim=my_ylim, 
                         ylab=""Outcome"", 
                         auto.key = list(
                           text = c(""Male"","" Female""),
                           columns=2))


# Just for making pretty graphs with the comparison plot
compareplot &lt;- function(regr_plot, regr_title, true_plot){
  print(regr_plot, position=c(0,0.5,1,1), more=T)
  trellis.focus(""toplevel"")
  panel.text(0.3, .8, regr_title, cex = 1.2, font = 2)
  trellis.unfocus()
  print(true_plot, position=c(0,0,1,.5), more=F)
  trellis.focus(""toplevel"")
  panel.text(0.3, .65, ""True line"", cex = 1.2, font = 2)
  trellis.unfocus()
}

compareplot(lm_plot, ""Linear regression"", true_line_plot)
compareplot(rq_plot, ""Quantile regression"", true_line_plot)
compareplot(logit_rq_plot, ""Logit - Quantile regression"", true_line_plot)
</code></pre>

<p><img src=""http://i.stack.imgur.com/74Uid.png"" alt=""Linear regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/xHRtF.png"" alt=""Quantile regression for bounded outcome""></p>

<p><img src=""http://i.stack.imgur.com/XfLy8.png"" alt=""Logistic quantile regression for bounded outcome""></p>

<h3>The contrast output</h3>

<p>Now I've tried to get the contrast and it's almost ""right"" but it varies along the span as expected:</p>

<pre><code>&gt; contrast(boot_rq_logit, list(gender=levels(gender), 
+                              xtest=c(-1:1)), 
+          FUN=function(x)antilogit_fn(x, epsilon))
   gender xtest Contrast   S.E.       Lower      Upper       Z      Pr(&gt;|z|)
   Male   -1    -2.5001505 0.33677523 -3.1602179 -1.84008320  -7.42 0.0000  
   Female -1    -1.3020162 0.29623080 -1.8826179 -0.72141450  -4.40 0.0000  
   Male    0    -1.3384751 0.09748767 -1.5295474 -1.14740279 -13.73 0.0000  
*  Female  0    -0.1403408 0.09887240 -0.3341271  0.05344555  -1.42 0.1558  
   Male    1    -1.3308691 0.10810012 -1.5427414 -1.11899674 -12.31 0.0000  
*  Female  1    -0.1327348 0.07605115 -0.2817923  0.01632277  -1.75 0.0809  

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
</code></pre>
"
"0.0756856276908142","0.0651404749061921"," 27945","<p>What is the meaning and effect of %in% in a model formula?</p>

<p>It is apparently used for nesting of one variable into another in a variety of analysis (manova, anova, regressions) in a few published articles.</p>

<p>From ?formula, b%in%a is a:b, so why use %in%?<br>
How is a:b nesting?</p>

<p>I am probably mistaken, but my understanding is that nesting b in a should not lead to the same mean square as the interaction of a and b denoted by a:b?</p>

<pre><code>library(lme4)  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>with(sleepstudy, Days%in%Subject)
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ...  
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fit&lt;-aov(data=sleepstudy, Reaction~Days + Days%in%Subject)
anova(fit)


               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
 Days           1 162703  162703  193.23 &lt; 2.2e-16 ***
 Days:Subject  17 269685   15864   18.84 &lt; 2.2e-16 ***
 Residuals    161 135567     842
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
anova(fm1)


      Df Sum Sq Mean Sq F value
 Days  1  29986   29986  45.785
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<pre><code>fm1 &lt;- lmer(Reaction~Days + Days%in%Subject + (1|Subject), sleepstudy)
anova(fm1)

Analysis of Variance Table
             Df Sum Sq Mean Sq  F value
Days          1 162703  162703 248.4233
Days:Subject 17  73391    4317   6.5916
</code></pre>
"
"0.0404556697031367","0.0406222231851194"," 28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.11100944184129","0.111466460825459"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"0.12136700910941","0.121866669555358"," 28688","<p>I ran <code>lm()</code> on my data with models selected by individual <code>lm</code>'s of each characteristic and then combined the top $R^2$ based on $p$-value. For instance, the first few characteristics are taken, then the rest are evaluated if they have $p&lt;.005$. My characteristics contain some duplication: for instance, I have a characteristic and its normalized variant in test P. My $p$-values are all very small but my diagrams do not look correct for R and T. (Referring to this blog post: <a href=""http://www.findnwrite.com/musings/evaluating-linear-regression-model-in-r/"" rel=""nofollow"">Evaluating Linear Regression Model in R</a>.)</p>

<p>In test P (and T) there is one outlier according to Cooks Distance. How do I find and eliminate that instance?</p>

<p>According to this tutorial on <a href=""http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf"" rel=""nofollow"">Using R for Linear Regression</a>,</p>

<blockquote>
  <p>The plot in the upper left shows the residual errors plotted versus
  their fitted values.  The residuals should be randomly distributed
  around the horizontal line representing a residual error of zero; that
  is, there should not be a distinct trend in the distribution of
  points.</p>
</blockquote>

<p>Test P looks ok in the residual error but test R and T have a grouping what does that mean and how do I account for it?</p>

<blockquote>
  <p>The plot in the lower left is a standard Q-Q plot, which should
  suggest that the  residual errors are normally distributed.  The
  scale-location plot in the upper right shows the square root of the
  standardized residuals (sort of a square root of relative error) as a
  function of the fitted values.  Again, there should be no obvious
  trend in this plot.</p>
</blockquote>

<p>Again Test P looks ok in the standard Q-Q plot but test R and T have a grouping what does that mean and how do I account for it?</p>

<p>Also what is the coefficients on the output. I notice it lists the characteristics and a p value but i don't understand what it means.</p>

<p>And finally how do I make predictions using the model I created? </p>

<p><strong>Test P</strong>
F-statistic: 2.684 on 280 and 2221 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/10A8r.png"" alt=""enter image description here""></p>

<p><strong>Test R</strong>
F-statistic: 3.691 on 258 and 2243 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/jy6IR.png"" alt=""enter image description here""></p>

<p><strong>Test T</strong>
F-statistic: 4.029 on 268 and 2233 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/bs69P.png"" alt=""enter image description here""></p>

<p>edit after running gls my p looks like this</p>

<p><img src=""http://i.stack.imgur.com/cUBGB.png"" alt=""""></p>
"
"0.0904616275314925","0.090834052439095"," 28732","<p>I am using the randomForest package in R (R version 2.13.1, randomForest version 4.6-2) for regression and noticed a significant bias in my results: the prediction error is dependent on the value of the response variable. High values are under-predicted and low values are over-predicted. At first I suspected this was a consequence of my data but the following simple example suggests that this is inherent to the random forest algorithm:</p>

<pre><code>n = 1000; 
x1 = rnorm(n, mean = 0, sd = 1)
response = x1
predictors = data.frame(x1=x1) 
rf = randomForest(x=predictors, y=response)
error = response-predict(rf, predictors)
plot(x1, error)
</code></pre>

<p>I suspect the bias is dependent on the distribution of the response, for example, if <code>x1</code> is uniformly-distributed, there is no bias; if <code>x1</code> is exponentially distributed, the bias is one-sided. Essentially, the values of the response at the tails of a normal distribution are outliers. It is no surprise that a model would have difficulty predicting outliers. In the case of randomForest, a response value of extreme magnitude from the tail of a distribution is less likely to end up in a terminal leaf and its effect will be washed out in the ensemble average.</p>

<p>Note that I tried to capture this effect in a previous example, ""RandomForest in R linear regression tails mtry"". This was a bad example. If the bias in the above example is truly inherent to the algorithm, it follows that a bias correction could be formulated given the response distribution one is trying to predict, resulting in more accurate predictions.  </p>

<p>Are tree-based methods, such as random forest, subject to response distribution bias? If so, is this previously known to the statistics community and how is it usually corrected (e.g. a second model that uses the residuals of the biased model as input)?</p>

<p>Correction of a response-dependent bias is difficult because, by nature, the response is not known. Unfortunately, the estimate/predicted response does not often share the same relationship to the bias.</p>
"
"0.0756856276908142","0.0759972207238908"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.06396603026469","0.0642293744423385"," 28957","<p>I am interested in understanding the graph plots we get after running <code>lm()</code> command (for linear regression) in R like, for example</p>

<pre><code>lm.mod1 = lm(y ~ x1 + x2)
</code></pre>

<p>I then get the do the summary by:</p>

<pre><code>summary(lm.mod1)
</code></pre>

<p>I get the result as: </p>

<pre><code> Residuals:
  Min      1Q  Median      3Q     Max
 -750.32 -160.54  -49.83  115.83 2923.74

 Coefficients:
                           Estimate Std. Error     t value Pr(&gt;|t|)    
(Intercept)               -345.1552      37.0393   -9.319   &lt;2e-16 ***
         x1                52.9091       2.4929    21.224   &lt;2e-16 ***
         x2                8.9669        0.5395    16.620   &lt;2e-16 ***

Residual standard error: 274.4 on 1985 degrees of freedom
Multiple R-squared: 0.2059, Adjusted R-squared: 0.2051 
F-statistic: 257.3 on 2 and 1985 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I then do the plotting by </p>

<pre><code>par(mfrow = c(2,2))
plot(lm.mod1)
</code></pre>

<p>I get 4 graphs (I can't post the graphs since I am a new user and my experience level is below 10. :/)</p>

<p>My questions are : </p>

<ol>
<li><p>How do they calculate F-statistics and t-value?</p></li>
<li><p>Could someone explain me the what do we interpret with the last two graphs i.e. $\text{Scale-Location vs. (Standardized residuals)}^{1/2}$ and $\text{Residuals vs. Leverage}$. What do you mean by Leverage?</p></li>
<li><p>What do you mean by Cook's Distance? I saw it on wikipedia but I didnt get it. </p></li>
<li><p>How could we suggest if our model is a good model or not?</p></li>
</ol>
"
"0.0286064783845312","0.0287242494810713"," 29329","<p>I was wondering, what is the meaning of operators in anova or regression formulas in R</p>

<p>For example</p>

<ul>
<li>""<strong>+</strong>"" aov &lt;- aov(x~time+sample, data=data) -> repeated mesures anova?</li>
<li>""<strong>*</strong>"" aov &lt;- aov(x~time*sample, data=data) -> two way anova?</li>
<li>""<strong>/</strong>"" aov &lt;- aov(x~time/sample, data=data) -> ?</li>
<li>""<strong>:</strong>"" aov &lt;- aov(x~time:sample, data=data) -> ?</li>
</ul>

<p>And also are there more operators for this kind of formulas?</p>
"
"0.151461330519253","0.157329193881888"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.0858194351535935","0.0861727484432139"," 29981","<p>Let's have some linear model, for example just simple ANOVA:</p>

<pre><code># data generation
set.seed(1.234)                      
Ng &lt;- c(41, 37, 42)                    
data &lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      
fact &lt;- as.factor(rep(LETTERS[1:3], Ng)) 

m1 = lm(data ~ 0 + fact)
summary(m1)
</code></pre>

<p>Result is as follows:</p>

<pre><code>Call:
lm(formula = data ~ 0 + fact)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30047 -0.60414 -0.04078  0.54316  2.25323 

Coefficients:
      Estimate Std. Error t value Pr(&gt;|t|)    
factA  -0.9142     0.1388  -6.588 1.34e-09 ***
factB   0.1484     0.1461   1.016    0.312    
factC   1.0990     0.1371   8.015 9.25e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8886 on 117 degrees of freedom
Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 
F-statistic: 36.23 on 3 and 117 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Now I try two different methods to estimate confidence interval of these parameters</p>

<pre><code>c = coef(summary(m1))

# 1st method: CI limits from SE, assuming normal distribution
cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], 
    high = c[,1] + qnorm(p = 0.975) * c[,2])

# 2nd method
confint(m1)
</code></pre>

<h2>Questions:</h2>

<ol>
<li>What is the distribution of estimated linear regression coefficients? Normal or $t$?</li>
<li>Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.</li>
</ol>

<p>Thank you very much!</p>

<p>data ~ 0 + fact</p>

<p><strong>EDIT after an answer</strong>:</p>

<p>The answer is exact, this will give exactly the same result as <code>confint(m1)</code>!</p>

<pre><code># 3rd method
cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], 
    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])
</code></pre>
"
"0.0286064783845312","0.0287242494810713"," 31724","<p>Let's say I have a logistic regression model which predicts whether a consumer will buy an item based on about 10 consumer characteristics. </p>

<p>$$\begin{array}{rcl}Buy &amp;=&amp; B_0 + B_1\times Gender + B_2\times CreditType + B_3\times Education + B_4\times OwnsHome \\\phantom{Buy} &amp;&amp; + B_5\times CarMake + B_6\times CarYear + B_7\times State + B_8\times Income + B_9\times Insurance \\ \phantom{Buy} &amp;&amp;+ B_{10}\times CarAccidents\end{array} $$</p>

<ol>
<li><p>Is there ever an issue with including too many predictors in a logistic regression model? I'm not talking about insignificant variables or ones that may be related, but just the sheer number of variables included in a model. </p></li>
<li><p>With a larger number of predictors, how should one present the regression results in a meaningful manner? Is it just a matter of plotting the probability curve for $Y=1$, or are there ""better"" ways of doing this. I'd be doing this in R, so any help on that end would be appreciated.</p></li>
</ol>
"
"0.0495478739876288","0.0497518595104995"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.0572129567690623","0.0574484989621426"," 32616","<p>In my data, some individuals have missing data on the central predictor (father missed the intake assessment). Comparing the DVs' means for those with a missing/non-missing predictor yielded some sizeable effects.</p>

<p>Now I want to find out whether the systematic missings may have led me to underestimate the size of the OLS regression coefficients. What's a good way to do this?</p>

<p>Simply comparing the variances of the DVs in the group without missings to the group with missings is easy to do?<br>
But conceptually I want to know whether the whole sample has significantly less variability when I leave the group with missings (and significantly-lower-than-average-scores) out, not whether the two groups (with missings and without missings) have different variances.<br>
All that I found so far was about independent samples, not about subsets.</p>

<p>Also, just to bring me up to speed: heteroscedasticity is usually used in the context of residual variance, right? What's a good term for constricted variance that would give me better luck with google?</p>
"
"0.06396603026469","0.0642293744423385"," 32694","<p>I'm using R together with the <code>forecast</code> package to set up a ARIMA model, that will be used to predict a energy related variable. I used <code>auto.arima()</code> to fit different models (according to geographic region), and I need to put the model coefficients in our database, so that the IT folks can automate things. That's exactly the problem: I simply don't know how set up the equations by looking at the model:</p>

<pre><code>ARIMA(1,0,1)(2,0,1)[12] with non-zero mean 

Coefficients:

       ar1     ma1    sar1    sar2     sma1   intercept    prec0    prec1
     0.3561  0.3290  0.6857  0.2855  -0.7079  11333.240   15.5291  28.0817

s.e. 0.2079  0.1845  0.2764  0.2251   0.3887   2211.302    6.2147   6.0906
</code></pre>

<p>I have 2 regressor variables (prec0 and prec1). Given the residuals, the ARIMA vector <code>ARIMA(1,0,1)(2,0,1)[12]</code>, the time series up to period $t$, the number $h$ of forecasting periods and the regressor matrix reg, how can I set a function to return the forecast values? I.e:</p>

<pre><code>do.forecast = function(residuals, ARIMA, timeSeries, h, regMatrix)
{
  p = ARIMA[1]
  q = ARIMA[3]

  ## arima equations here...
}
</code></pre>

<p>Thanks!  </p>

<p>PS: I know this is a possible duplicate of <a href=""http://stats.stackexchange.com/questions/23881/reproducing-arima-model-outside-r"">Reproducing ARIMA model outside R</a>, but my model seems very different, and I really don't know how to start with.</p>
"
"0.0756856276908142","0.0651404749061921"," 33174","<p>I'm failing to understand the value of the intercept value in a multiple linear regression with categorical values. Taking the ""warpbreaks"" data set as an example, when I do:</p>

<pre><code>&gt; lm(breaks ~ wool, data=warpbreaks)

Call:
lm(formula = breaks ~ wool, data = warpbreaks)

Coefficients:
(Intercept)        woolB
     31.037       -5.778
</code></pre>

<p>I'm able to understand that the value of intercept is the mean value of breaks when wool equals ""A"", and that adding up the ""woolB"" coefficient to the intercept value I get the mean value of breaks when wool equals ""B"". However, if I also consider the tension variable in the model, I'm unable to figure out the meaning of the intercept value:</p>

<pre><code>&gt; lm(breaks ~ wool + tension, data=warpbreaks)

Call:
lm(formula = breaks ~ wool + tension, data = warpbreaks)

Coefficients:
(Intercept)        woolB     tensionM     tensionH
     39.278       -5.778      -10.000      -14.722
</code></pre>

<p>I thought it would be the mean value of breaks when either wool equals ""A"" or tension equals ""L"", but that isn't true for this dataset.</p>

<p>Any clues on interpreting the value of intercept?</p>
"
"0.121978433679867","0.116356580756692"," 33712","<p>I have a question about which prediction variance to use to calculate prediction intervals from a fitted <code>lm</code> object in R. </p>

<p>For a certain multiple linear regression model I have obtained an error variance with leave-one-out-cross-validation (LOOCV) by taking the mean of the squared difference between observed and predicted values (i.e., mean squared prediction error). I am aware of some of the drawbacks of LOOCV (e.g., <a href=""http://stats.stackexchange.com/questions/2352/when-are-shaos-results-on-leave-one-out-cross-validation-applicable"">When are Shao&#39;s results on leave-one-out cross-validation applicable?</a>), but for my specific application this was the easiest (and probably the only realistically) implementable CV method. The final fitted linear model (<code>fitted_lm</code>) is fitted with all observations and with this model I would like to make predictions for new observations (<code>new_observations</code>). For this I am using the <code>predict.lm</code>  function in R.</p>

<pre><code>predict(fitted_lm, new_observations, interval = ""prediction"", pred.var = ???)
</code></pre>

<p>My questions are:  </p>

<ul>
<li>What value do I use for <code>pred.var</code> (i.e., â€œthe variance(s) for future observations to be assumed for prediction intervalsâ€) in order to obtain realistic prediction intervals for my new_observations?  </li>
<li>Do I use the error variance obtained from the LOOCV, or do I use the functionâ€™s default (i.e., â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€)?  </li>
<li>Is the mean squared prediction error not appropriate in this case?</li>
</ul>

<p>Following up on Michael Chernick's answer hereunder, I had a look in the Draper &amp; Smith (1998) book  (â€œApplied regression analysis. 3rd Editionâ€). In this book <em>s<sup>2</sup></em> is defined as â€œvariance about the regressionâ€ (p 32). This is, I presume, what we describe below as the model estimate of residual variance. Furthermore, this book mentions: </p>

<blockquote>
  <p>â€œSince the actual observed value of <em>Y</em> varies about the true mean value <em>Ïƒ<sup>2</sup></em> [independent of the <em>V(Å¶)</em>], a predicted value of an individual observation will still be given with <em>Å¶</em> but will have variance</p>
  
  <p><img src=""http://i.stack.imgur.com/uUPXs.jpg"" alt=""formula""></p>
  
  <p>With corresponding estimated value obtained by inserting <em>s<sup>2</sup></em> for <em>Ïƒ<sup>2</sup></em>â€ (pp 82-81).</p>
</blockquote>

<p>Thus, as far as I understand, in the D &amp; S book they only use the model estimate of residual variance to calculate confidence intervals. This would be the default setting in the <code>predict</code> function (function help: â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€). However, as fosgen states below, â€œalthough LOOCV mean squared prediction error is not equal to the real mean squared prediction error, it is much more close to real than error variance of fitted modelâ€.</p>

<p>To make this more concrete; in my dataset I get a model estimate of residual variance of <code>0.005998</code> and a LOOCV mean squared prediction error of <code>0.007293</code>. What should I then fill in as <code>pred.var</code> in the <code>predict.lm</code> function:</p>

<ul>
<li>Nothing (i.e. use the default, which would equal to the model estimate of residual variance)</li>
<li><code>0.007293</code> (i.e. the LOOCV mean squared prediction error) </li>
<li><code>0.005998 + 0.007293</code> (Michael Chernick: â€œThe model estimate of residual variance gets added to the error variance due to estimating the parameters to get the prediction error variance for a new observationâ€).</li>
</ul>
"
"0.0858194351535935","0.0861727484432139"," 33862","<p>I have some models built with the <code>auto.arima</code> function from the <code>forecast</code> package. I'm modeling a variable called 'natural efluent energy' (ena), which is how much energy you can extract from some Hydrography region. There are 2 regressor variables (rainfall precipitation from period $t$ and $t-1$.)</p>

<p>Each region has it's own model - some series show positive trend, some shows negative trend, and some seems stationary. The problem is that some forecasts 'from <code>auto.arima</code>' are giving values higher/lower than usual (some forecasts give me negative values, which are not possible).</p>

<p>My original call is below:</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars)
</code></pre>

<p>For the data on the link, I changed it to</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars, max.P = 0, max.Q = 0, stationary = TRUE)
</code></pre>

<p>Then I get good forecasts in this case. My question is, what these parameters(<code>max.P</code>, <code>max.Q</code>) actually control, and how they relate to the trend show by my model variable?</p>

<p>Here is a link for the historic data:
<a href=""http://www.datafilehost.com/download-7718b3fc.html"" rel=""nofollow"">http://www.datafilehost.com/download-7718b3fc.html</a></p>

<p>And here a link for the forecast regressors:
<a href=""http://www.datafilehost.com/download-ca44dfa4.html"" rel=""nofollow"">http://www.datafilehost.com/download-ca44dfa4.html</a></p>

<p>And here a link of mean historic values, the forecast must fall between these values:
<a href=""http://www.datafilehost.com/download-e1e265b7.html"" rel=""nofollow"">http://www.datafilehost.com/download-e1e265b7.html</a></p>

<p>My data starts at 2001/Jun, so the serie is:</p>

<pre><code>  y = ts(dframe$ena, freq = 12, start = c(2001, 6))
</code></pre>
"
"0.0917448352774886","0.0997994216610746"," 34080","<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the 'true model'. I have written a blog post with a more detailed analysis <a href=""http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/"" rel=""nofollow"">here</a>.</p>

<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>

<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression Ã  la Rivers and Vuong (1988). </p>

<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=""http://www.cemfi.es/~arellano/binary-endogeneity.pdf"" rel=""nofollow"">these lecture notes</a>) the ASF can be calculated as follows:</p>

<p>$P(y|\bar{\textbf{X}},v)=\widehat{ASF}=\frac{1}{N}\sum^{N}_{i} \Phi(\bar{\textbf{X}}\boldsymbol{\hat{\beta}}+\rho \hat{\textbf{v}_{i}}) $</p>

<p>where the $\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>

<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>

<p>A picture of my attempt is:</p>

<p><img src=""http://i.stack.imgur.com/OZBA8.jpg"" alt=""enter image description here""></p>

<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>

<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>

<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>

<pre><code>rm(list=ls())
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
y1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)
y2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)

# true
r1 &lt;- glm(y2~y1+x1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))
names(data) &lt;- c(""y1"",""x1"")
asf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))
plot(asf1,type=""l"",col=""blue"",xlab=""y1"",ylab=""P(y2)"")

# no endog correction
r2 &lt;- glm(y2~y1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2)))
names(data) &lt;- c(""y1"")
asf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))
lines(asf2,type=""l"",col=""green"")

# control function approach
v1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))
r3 &lt;- glm(y2~y1+v1,binomial(link=""probit""))
# proceedure to get asf
asf3 &lt;- cbind(seq(-4,6,0.2),NA)
for(i in 1:dim(asf3)[1]){
    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))
    names(dat2) &lt;- c(""y1"",""v1"")
    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))
}
lines(asf3,type=""l"",col=""red"")
</code></pre>
"
"0.064873395163555","0.0759972207238908"," 34493","<p>I am using both R and SAS for the time series modeling. There is an option in SAS that I could not find so far in any packages developed in R for the time series modeling such as TSA or forecast package, at least to the best of my knowledge! To explain more, if we use the windowing environment in SAS to fit an ARIMA model with a regressor, we basically choose:  </p>

<p>Solution->Analysis->Time series Forecasting System->Develop Models<br>
Then Fit ARIMA model -> Predictors->Dynamic Regressors</p>

<p>If we ask to forecast this model, SAS says â€œThe following regressor(s) do not have any forecasting models. The system will automatically select forecasting models for these regressorsâ€. This means that we have not provided the values of the regressors over the forecasting period, and the system tries to find a model for that.</p>

<p>My questions:</p>

<ol>
<li>Is there any package in R with the same capability (explained above) as in SAS to forecast an ARIMA model?  </li>
<li>How can SAS automatically forecast the regressor(s) and based on what models?</li>
</ol>
"
"0.0495478739876288","0.0497518595104995"," 34549","<p>I understand that there is a function in R called <code>poly()</code> that can generate orthogonal polynomials--useful for applying on input variables before running a predictive model.</p>

<p>My question is that what is the role of categorical variables when we generate polynomials? Are they to be excluded?</p>

<h2>Update:</h2>

<p>Dan, Thank you for your kind response. I'm not sure I understand it completely - let me explain the query in more detail. I'm trying to run logistic regression using glmnet on <a href=""http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls"" rel=""nofollow"">Titanic dataset</a>. <BR/> Let us assume shortened set of columns:<ul>* class(factor with three levels 1, 2 ,3), <br/>* sex(factor: male, female), <br/>* Age (integer), <br/>*survived(factor &amp; target variable 0 or 1).</ul> The questions is it meaningful to create polynomial features based on these factors? e.g. class. If yes could you pls explain what it means? <BR> I've seen examples with numeric input variables, where one can pass the entire input set to the poly() function and get polynomial features as output. <br/> Your response is highly appreciated.</p>
"
"0.0495478739876288","0.0497518595104995"," 35778","<p>I have a 2 level repeated measures DV of accuracy, and a covariate of response bias. As response bias increases, accuracy level 1 increases while level 2 decreases.  I want to see if there is a difference in the means of the groups after controlling for response bias. </p>

<p>I can't do it with an ANCOVA.  Can I just manually calculate expected values based on a regression equation, and run an ANOVA?  </p>

<p>I'd like to do it in R or SPSS, so specifics for either would be welcome, but not necessary.</p>
"
"0.0286064783845312","0.0287242494810713"," 35936","<p>I am new to R and trying to practice with some exercises. Given a data set with 40  observations and 5 variables. Spending is the the response and there are 4 predictors. I started with a linear model Residuals:</p>

<pre><code>    Min      1Q  Median      3Q     Max 
-51.082 -11.320  -1.451   9.452  94.252 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  22.55565   17.19680   1.312   0.1968    
sex         -22.11833    8.21111  -2.694   0.0101 *  
status        0.05223    0.28111   0.186   0.8535    
income        4.96198    1.02539   4.839 1.79e-05 ***
verbal       -2.95949    2.17215  -1.362   0.1803    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 22.69 on 42 degrees of freedom
Multiple R-squared: 0.5267, Adjusted R-squared: 0.4816 
F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06 
</code></pre>

<p>First, is this what they mean by fit regression model and Secondly, how do I compute the correlation of the residuals with the fitted values? </p>
"
"0.0286064783845312","0.0287242494810713"," 36064","<p>I am looking at some simple regression models using both R and the <code>statsmodels</code> package of Python. I've found that, when computing the coefficient of determination, <code>statmodels</code> uses the following formula for $R^2$:
$$
R^2 = 1 - \frac{SSR}{TSS}~~~~~~(\text{centered})
$$
where $SSR$ is the sum of squared residuals, and $TSS$ is the total sum of squares of the model. (""Centered"" means that the mean has been removed from the series.) However, the same calculation in R yields a different result for $R^2$. The reason is that R seems to be calculating $R^2$ as:
$$
R^2 = 1 - \frac{SSR}{TSS}~~~(\text{uncentered})
$$
So, what gives? Presumably there's some reason to prefer one over the other in certain situations. I haven't been able to find any information online about the cases where one of the above formulae should be preferred. </p>

<p>Can someone please explain why one is better than the other?</p>
"
"0.0429097175767967","0.0574484989621426"," 37395","<p>What is the canonical example which show situation when robust linear regression has advantage over least square linear regression ? I was trying to simulate situation when some errors (20% of them) are generated from t-student distribution and 80% are from normal - both distribution with the same variance ! on datasets with 50 observation, and I cant see clearly that robust regression is better, here is my R code for this experiment :</p>

<pre><code>library(MASS)
n=50 # size of datasets
N=1000 # number of regressions
wynik=matrix(0,N,2) # matrix with estimated coefficients
v=5  # parametr of t-student distribution
Sd=(v/(v-2))^.5 # standard deviation of gaussian distribution
a=1 # coefficient 

for(i in 1:N){

x=rnorm(n,mean=1,sd=Sd)

e_norm&lt;-rnorm(n,sd=Sd)
e_t&lt;-rt(10, df=v )

y_norm=a*x+e_norm
y_t=a*x+c(e_t,rnorm(40,sd=Sd)) # wariant 2 czÄ™Å›Ä‡ to outliery

Zm1=lm(y_t~x)$coef[2]
    Zm2=rlm(y_t~x)$coef[2]

wynik[i,1]=Zm1
wynik[i,2]=Zm2
plot(1,1,main=paste(i))
}

plot(density(wynik[,1]),main=""density of LS estimator(black) and robust estimator (green)"")
lines(density(wynik[,2]),col=""green"")
# average values of LS and robust estimator
colMeans(wynik)
</code></pre>
"
"0.0572129567690623","0.0574484989621426"," 37466","<p>I am taking a graduate course in Applied Statistics that uses the following textbook (to give you a feel for the level of the material being covered): <a href=""http://amzn.com/0471072044"">Statistical Concepts and Methods</a>, by G. K. Bhattacharyya and R. A. Johnson.</p>

<p>The Professor requires us to use SAS for the homeworks. </p>

<p>My question is that: is there a Java library(ies), that can be used instead of SAS for problems typically seen in such classes.</p>

<p>I am currently trying to make do with <a href=""http://commons.apache.org/math/"">Apache Math Commons</a> and though I am impressed with the library (it's ease of use and understandability) it seems to lack even simple things such as the ability to draw histograms (thinking of combining it with a charting library).</p>

<p>I have looked at Colt, but my initial interest died down pretty quickly. </p>

<p>Would appreciate any input -- and I've looked at similar questions on Stackoverflow but have not found anything compelling.</p>

<p>NOTE: I am aware of R, SciPy and Octave and java libraries that make calls to them -- I am looking for a Java native library or set of libraries that can together provide the features I'm looking for.</p>

<p>NOTE: The topics covered in such a class typically include: one-samle and two-sample tests and confidence intervals for means and medians, descriptive statistics, goodness-of-fit tests, one- and two-way ANOVA, simultaneous inference, testing variances, regression analysis, and categorical data analysis.</p>
"
"0.0908377689773195","0.0912117424359157"," 37840","<p>Okay, so I am trying to understand linear regression. I've got a data set and it looks all quite alright, but I am confused. This is my linear model-summary:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.2068621  0.0247002   8.375 4.13e-09 ***
temp        0.0031074  0.0004779   6.502 4.79e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.04226 on 28 degrees of freedom
Multiple R-squared: 0.6016, Adjusted R-squared: 0.5874 
F-statistic: 42.28 on 1 and 28 DF,  p-value: 4.789e-07 
</code></pre>

<p>so, the p-value is really low, which means it is very unlikely to get the correlation between x,y just by chance.
If I plot it and then draw the regression line it looks like this:
<a href=""http://s14.directupload.net/images/120923/l83eellv.png"" rel=""nofollow"">http://s14.directupload.net/images/120923/l83eellv.png</a>
(Had it in as a picture but I am - as a new user - currently not allowed to post it)
Blue lines = confidence interval
Green lines = prediction interval</p>

<p>Now, a lot of the points do not fall into the confidence interval, why would that happen? I think none of the datapoints falls on the regression line b/c they are just quite far away from each other, but what I am not sure of: Is this a real problem? They still are around the regression line and you can totally see a pattern. But is that enough?
I'm trying to figure it out, but I just keep asking myself the same questions over and over again.</p>

<p>What I thought of so far:
The confidence interval says that if you calculate CI's over and over again, in 95% of the times the true mean falls into the CI.
So: It it is not a problem that the dp do not fall into it, as these are not the means really.
The prediction interval on the other hand says, that if you calculate PI's over and over again, in 95% of the times the true VALUE falls into the interval. So, it is quite important to have the points in it (which I do have).
Then I've read the PI always has to have a wider range than the CI. Why is that?
This is what I have done:</p>

<pre><code>conf&lt;-predict(fm, interval=c(""confidence""))
prd&lt;-predict(fm, interval=c(""prediction""))
</code></pre>

<p>and then I plotted it by:</p>

<pre><code>matlines(temp,conf[,c(""lwr"",""upr"")], col=""red"")
matlines(temp,prd[,c(""lwr"",""upr"")], col=""red"")
</code></pre>

<p>Now, if I calculate CI and PI for additional data, it does not matter how wide I choose the range, I get the exact same lines as above. I cannot understand. What does that mean?
This would then be:</p>

<pre><code>conf&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""confidence""))
prd&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""prediction""))
</code></pre>

<p>for new x I chose different sequences.
If the sequence has a different # of observations than the variables in my regression, I am getting a warning. Why would that be?</p>
"
"0.0429097175767967","0.0574484989621426"," 38118","<p>I have a multiple regression problem, which I tried to solve using simple multiple regression:</p>

<pre><code>model1 &lt;- lm(Y ~ X1 + X2 + X3 + X4 + X5, data=data)
</code></pre>

<p>This seems to be explaining the 85% of variance (according to R-squared) which seems pretty good.</p>

<p>However what worries me is the weird looking Residuals vs Fitted plot, see below:</p>

<p><img src=""http://i.stack.imgur.com/Y2UBn.png"" alt=""enter image description here""></p>

<p>I suspect the reason why we have such parallel lines is because the Y value has only 10 unique values corresponding to about 160 of X values.</p>

<p>Perhaps I should use a different type of regression in this case?</p>

<p><strong>Edit</strong>: I've seen in the <a href=""http://www.tandfonline.com/doi/abs/10.1080/00031305.1988.10475569#preview"" rel=""nofollow"">following paper</a> a similar behavior. Note it's a one-page only paper so when you preview it you can read it all. I think it explains pretty well why I observe this behavior but I'm still not sure if any other regression would work better here?</p>

<p><strong>Edit2:</strong> The closest example to our case I can think of is the change in interest rates. FED announces new interest rate every few months (we don't know when and how often). In the meantime we gather our independent variables on the daily basis (such as daily inflation rate, stock market data, etc.). As a result we will have a situation where we can have many measurements for one interest rate. </p>
"
"0.0756856276908142","0.0759972207238908"," 39000","<p>I assessed the internal reliability of a self-created scale with eight items ($N = 150$) by calculating Cronbachâ€™s $\alpha$. It appears that one item correlates low with the overall score of the scale (item 4 in the example below). The corrected item-total correlation, i.e. the correlation of this item with the scale total excluding that item, is only $r= .046$. </p>

<pre><code>library(psych)
scale&lt;-mydata[,c(24,25,26,27,28,29,30,31)]
alpha(scale)

Reliability analysis   
Call: alpha(x = scale)

          0.62      0.64    0.66      0.18  4.3 0.79

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r
item1      0.56      0.59    0.62      0.17
item2      0.53      0.57    0.58      0.16
item3      0.54      0.56    0.58      0.16
item4      0.66      0.67    0.67      0.23
item5      0.60      0.62    0.63      0.19
item6      0.55      0.59    0.62      0.17
item7      0.58      0.61    0.63      0.18
item8      0.63      0.65    0.67      0.21

 Item statistics 
        n    r r.cor r.drop mean   sd
item1 144 0.60  0.51  0.395  4.5 0.71
item2 145 0.65  0.62  0.499  4.6 0.71
item3 142 0.67  0.64  0.484  4.5 0.72
item4 146 0.33  0.15  0.046  4.6 0.81
item5 147 0.51  0.41  0.298  4.9 0.41
item6 139 0.59  0.50  0.404  4.4 0.82
item7 136 0.53  0.43  0.339  4.2 1.03
item8 135 0.39  0.21  0.190  4.3 0.94

Non missing response frequency for each item
         1    2    3    4    5 miss
item1 0.01 0.01 0.04 0.34 0.60 0.04
item2 0.01 0.01 0.03 0.24 0.71 0.03
item3 0.00 0.01 0.11 0.28 0.60 0.05
item4 0.01 0.03 0.05 0.14 0.77 0.03
item5 0.00 0.00 0.02 0.11 0.87 0.02
item6 0.01 0.02 0.10 0.29 0.58 0.07
item7 0.04 0.02 0.15 0.25 0.54 0.09
item8 0.02 0.03 0.11 0.32 0.52 0.10
</code></pre>

<p><strong>PROBLEM</strong>: I would like to report this low correlation with the degrees of freedom in parentheses and the significance level in the main text. Yet, I am not sure whether I calculated the correct p-value. What I did is a simple regression with item 4 as the dependent variable:</p>

<pre><code>scale &lt;- as.data.frame(scale)
summary(lm(item4 ~ item1+item2+item3+item5+item6+item7+item8, data=scale))

Call:
lm(formula = item4 ~ item1 + item2 + item3 + item5 + item6 + 
    item7 + item8, data = scale)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4256 -0.0465  0.2869  0.3500  1.3405 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.65098    1.00288   1.646 0.102492    
item1        0.12916    0.11560   1.117 0.266262    
item2        0.02387    0.12921   0.185 0.853760    
item3       -0.07323    0.12718  -0.576 0.565921    
item5        0.64204    0.18636   3.445 0.000802 ***
item6       -0.04596    0.10230  -0.449 0.654120    
item7       -0.13217    0.08030  -1.646 0.102545    
item8        0.05609    0.08758   0.641 0.523136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8235 on 113 degrees of freedom
  (29 observations deleted due to missingness)
Multiple R-squared: 0.1385,     Adjusted R-squared: 0.08518 
F-statistic: 2.596 on 7 and 113 DF,  p-value: 0.01604
</code></pre>

<p><strong>QUESTION:</strong> Is it correct if I report something like ""Item 4 correlates only weakly with the overall score of the scale $(r(113)= .046, p= .02)$"" - or did I make a rather large error in reasoning here? </p>
"
"0.0858194351535935","0.0765979986161901"," 40372","<p>I have a dependent variable $Y$, an independent variable $X$, and a categorical independent variable T (which can take 2 levels).
Now I have 4 models:</p>

<p><strong>MI:</strong> different slopes, different intercepts
$$y_{i1}=\alpha_1+\beta_1x_{i1}, y_{i2}=\alpha_2+\beta_2x_{i2}; \text{or}\; y_{ij}=\beta_0+\beta_1X_{ij}+\beta_2T_i+\beta_3X_{ij}T_j$$</p>

<p><strong>MII:</strong> same slope, different intercepts
$$y_{i1}=\alpha_1+\beta x_{i1}, y_{i2}=\alpha_2+\beta x_{i2}; \text{or}\; y_{ij}=\beta_0+\beta_1X_{ij}+\beta_2T_j$$</p>

<p><strong>MIII:</strong> same intercept, different slopes
$$y_{i1}=\alpha+\beta_1x_{i1}, y_{i2}=\alpha+\beta_2x_{i2}$$</p>

<p><strong>MIV:</strong> one regression line for both
$$y_{ij}=\alpha+\beta x_{ij}; \text{or}\; y_{ij}=\beta_0+\beta_1X_i$$</p>

<p>Now I am trying to compare these models one against the other. 
I am not sure, if I am right with what I think:<br>
Comparing MI to MII tests $H_0:\, \beta_1=\beta_2$ (this I do know), which means that $H_0$: same slope against $H_a$: different slopes (which I am not sure of).
If I compare these in R, I get a p-value. Now what I think is that, if the p-value &lt; alpha level, I can reject $H_0$, which would mean I should stick to the first model. If it is a large value, I cannot reject the null hypotheses, which means I should stick with the second model (as there I have the same slope). I am not sure if I am right with that.<br>
Plus I cannot write down the second equation for the model third model, can someone help out?</p>
"
"0.0993902382172794","0.0997994216610746"," 40385","<p>I would like to test in what regression fits my data best. My dependent variable is a count, and has a lot of zeros. </p>

<p>And I would need some help to determine what model and family to use (poisson or quasipoisson, or zero-inflated poisson regression), and how to test the assumptions.</p>

<ol>
<li>Poisson Regression: as far as I understand, the strong assumption is that dependent variable mean = variance. How do you test this? How close together do they have to be? Are unconditional or conditional mean and variance used for this? What do I do if this assumption does not hold? </li>
<li>I read that if variance is greater than mean we have overdispersion, and a potential way to deal with this is including more independent variables, or family=quasipoisson. Does this distribution have any other requirements or assumptions? What test do I use to see whether (1) or (2) fits better - simply <code>anova(m1,m2)</code>?</li>
<li>I also read that negative-binomial distribution can be used when overdispersion appears. How do I do this in R? What is the difference to quasipoisson?</li>
<li><p>Zero-inflated Poisson Regression: I read that using the vuong test checks what models fits better.  </p>

<p><code>&gt; vuong (model.poisson, model.zero.poisson)</code></p>

<p>Is that correct? What assumptions does a zero-inflated regression have? </p></li>
<li><p><a href=""http://www.ats.ucla.edu/stat/"">UCLA's Academic Technology Services, Statistical Consulting Group</a> has a <a href=""http://www.ats.ucla.edu/stat/R/dae/zipoisson.htm"">section</a> about zero-inflated Poisson Regressions, and test the zeroinflated model (a) against the standard poisson model (b):  </p>

<p><code>&gt; m.a &lt;- zeroinfl(count ~ child + camper | persons, data = zinb)</code><br>
<code>&gt; m.b &lt;- glm(count ~ child + camper, family = poisson, data = zinb)</code><br>
<code>&gt; vuong(m.a, m.b)</code></p></li>
</ol>

<p>I don't understand what the <code>| persons</code> part of the first model does, and why you can compare these models. I had expected the regression to be the same and just use a different family. </p>
"
"0.103142124625879","0.103566754353222"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.134644607536518","0.129565642238372"," 40670","<p>I am familiar with linear regression models but the random section of linear mixed models just melts my mind. I did find an excellent guide that could have helped me but the languageR package is not compatible with newer versions of lme4 so I've been unable to implement it in my work.</p>

<p>For me the fixed effects are very understandable (below lactation and a higher yr2 value both contribute to a higher weight but the lactation effect is more consistent which results in a higher t-value).</p>

<p>The first problem is to understand what I am actually putting in. To a certain extent I understand that <code>(1|P$grupp)</code> means that the mixed model add to the base line (intercept) while <code>(P$grupp|P$lweek)</code> mean that belong to a group is expected to affect the average weight increase (or decrease) while <code>P$lweek</code> adds to the baseline value. But why does all tutorials seem to favor write ups like <code>(1+P$fgrupp|P$lweek)</code> rather than <code>(P$grupp|P$lweek)</code>?</p>

<p>Now on to the actual output (see below for full output). I've used the following models (sorry for the Swenglish but the sample is the weight of cows <code>P$vikt</code> is the weight at certain time points and <code>P$lweek</code> is the time since a calf was born, <code>P$fgrupp</code> is a factor telling if the cow belongs to feed group 1,2 or 3):</p>

<pre><code>Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)
</code></pre>

<p>Where I understand it as the first one being rather useless (essentially it tells us that the average weight of the cows isn't affected of which feed group it belongs too). This is reflected by fgrupp having variance 0 in the first formula below. The second is more interesting as the <code>P$fgrupp|P$lweek</code> as I understand it should show if different feed groups affect the weight increase of cows as function of the time. But I am really not competent enough to understand the input. I understand that variance somehow mean that belonging to group2 or 3 explain some of the variation in the growth curves but I really don't understand how to interpret this.</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr        
 P$lweek  (Intercept)   13.068  3.6149                                     #$
          P$fgrupp2     77.230  8.7881  1.000                              #$
          P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
 Residual             4031.831 63.4967              
Number of obs: 1048, groups: P$lweek, 84
</code></pre>

<p><strong>Full output</strong></p>

<pre><code>#First model#
Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
   AIC   BIC logLik deviance REMLdev
 11703 11732  -5845    11698   11691
Random effects:
 Groups   Name        Variance Std.Dev.
 P$fgrupp (Intercept)    0.0    0.000                                      #$
 Residual             4139.9   64.342  
Number of obs: 1048, groups: P$fgrupp, 3                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept)  509.593      4.683  108.82
P$lweek        1.028      0.105    9.79                                    #$
P$laktation   22.789      1.454   15.67                                    #$
P$yr2         35.294      4.093    8.62                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$
P$lweek     -0.560                                                         #$
P$laktation -0.636  0.030                                                  #$
P$yr2       -0.240 -0.034 -0.141                                           #$


#Second model#

Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)  #$
       AIC   BIC logLik deviance REMLdev
     11707 11761  -5842    11693   11685
    Random effects:
     Groups   Name        Variance Std.Dev. Corr        
     P$lweek  (Intercept)   13.068  3.6149                                     #$
              P$fgrupp2     77.230  8.7881  1.000                              #$
              P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
     Residual             4031.831 63.4967              
    Number of obs: 1048, groups: P$lweek, 84                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept) 508.2291     5.1770   98.17
P$lweek       1.0662     0.1192    8.94                                    #$
P$laktation  22.6525     1.4459   15.67                                    #$
P$yr2        35.6343     4.0848    8.72                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$$
P$lweek     -0.627                                                         #$
P$laktation -0.570  0.025                                                  #$
P$yr2       -0.224 -0.018 -0.136                                           #$
</code></pre>
"
"0.0707974219804893","0.0812444463702388"," 41916","<p>I have a question about <code>plm</code> package. </p>

<p>My code is:</p>

<pre><code>fixedmodel &lt;- plm(formula=Inv_ret~Firm.size+leverage+Risk+Liquidity+Equity,
                  data=datanewp, model=""within"")
</code></pre>

<p>In the <code>plm vignette</code> the authors write: </p>

<blockquote>
  <p>This is called the fixed effects (a.k.a. within or least squares dummy
  variables) model, usually estimated by ols on transformed data, and
  gives consistent estimates for beta.</p>
</blockquote>

<p>I wonder then if <code>plm</code> package assumes that <code>least squares dummy variable</code> model is estimated with <code>model=""within</code> in the <code>plm</code> package? In other words, if <code>wihin</code> is actually <code>least squares dummy variable</code> model in plm?</p>

<p>On the other hand, if I refer to some other site they write:</p>

<blockquote>
  <p>There are several strategies for estimating a fixed effect model. The
  least squares dummy variable model (LSDV) uses dummy variables,
  whereas the â€œwithinâ€ estimation does not. These strategies, of course,
  produce the identical parameter estimates of regressors (nondummy
  independent variables).</p>
</blockquote>

<p>So I guess the authors of <code>plm</code> package mean that they actually use <code>within</code> model but compute the unique group specific intercepts from <code>within</code> transformation later. Am I correct?</p>
"
"0.156765720099655","0.157411114842338"," 43040","<p>I need some guidance related to regression model verification using validation data. 
I am new to R-tool &amp; statistics and trying my best to learn. I did search on internet too but I couldn't get a final answer to my questions. 
Actually I have a lot of questions, I may try my best to explain the problems:
I am experimenting with network packets and R-tool.
I have captured some packets from a network using a custom made packet sniffer in java. The sniffer will capture some packets and save the information of packet header like: tcp window size, tcp sequence numbers, date-time, ip header length, ip time to live etc... in a csv file.</p>

<p>Also the sniffer will add category number to each csv file so that we can know which packet belongs to which category. I created 9 different categories saved in 9 different csv files.
Now I extracted 1000 observation from each of the csv files and created a data set named ""alldata"".</p>

<p>Then I created training data set and validation data set from ""alldata"" data set.</p>

<p>Now I want to perform linear regression, logistic regression, decision tree analysis, cluster analysis etc on this ""alldata"" data set.</p>

<p>So my plan was to use training data set to create models and then later use validation data set to verify my models. </p>

<p>Category will be my target variable in any case. I want to predict the category from other independent variables.</p>

<ol>
<li><p>My first confusion is that after I created scatter plot of category with other independent variables and I don't see any linear relationship between them. Moreover I even don't know what relation exists between category and independent variables. From scatter plots it seems to me that there is no specific relation between category and other independent variables(except date_time it is bit linear to category). Am I doing the correct interpretation ?
Here are some of the plots:
<a href=""http://imageshack.us/photo/my-images/211/tcpdport.png/"" rel=""nofollow"">plot 1</a>
<a href=""http://imageshack.us/photo/my-images/547/tcpchksum.png/"" rel=""nofollow"">plot 2</a></p></li>
<li><p>I think doing linear regression won't make any sense now after having a look at scatter plots. Is this correct assumption?</p></li>
<li><p>Although I tried to do make some regression models with training data set, but the R-square values for all the models is quite low (for example like 0.00019, 0.0035, 0.018 etc. ) 
So can I assume that these models are not good due to very low r-square vales?</p></li>
<li><p>As logistic regression is used when we have target variables having only two values 1 or 0, or some probabilities between 0.0 - 1.0.
This means performing logistic regression is not possible for this type of data set.
Is my assumption true?</p></li>
<li><p>My main question was how to verify a model created with training data set by using validation data set?
Please let me know the commands and the procedure.
Please let me know if I am doing this in wrong way or if you can suggest me a better way to do this whole work. I think if someone could please clear my doubts then I may ask further more questions.</p></li>
</ol>

<p>If you don't understand my problem we can discuss in more detail
I look forward for your replies.
Thank you!</p>

<hr>

<p>@Wayne</p>

<p>Hello thanks for the reply, but the thing is for each category I have almost same range of values of independent variables like(tcpheader, ipttl, iplen). For example iptype is only having two values 6 and 17. So most of the categories are having iptype value of 6 &amp; 17.
So it is also same is for tcpheader, tcp sequence number, tcp acknowledgement number etc. I don't think there is any way to distinguish a particular packet based on these independent variables. Only the independent variable that can be helpful is time.
But when I created a model with time it had good r-squared value but the regression line equation doesn't predict category with any value of date_time.
I don't understand this behaviour.</p>

<p>Thanks.</p>
"
"0.0756856276908142","0.0759972207238908"," 43699","<p>I am R-tool beginner. I have a question regarding how to know the performance of a linear regression model by using validation data.
My approach was</p>

<ol>
<li><p>Create training and validation data sets from original data set.
""train"" is name of my training data set and ""valid"" is name of my validation data set. ""category"" will be my target variable and ""date_time"" is my independent variable.</p></li>
<li><p>Use training data set to create a regression model</p>

<blockquote>
  <p>attach(train)</p>
  
  <p>lreg=lm(category~date_time)</p>
</blockquote></li>
<li><p>Now do predictions for validation data set using model created with training data set</p>

<blockquote>
  <p>p=predict(lreg,valid)</p>
</blockquote></li>
<li><p>Now check the accuracy by finding the values of ACC, AUC.</p>

<blockquote>
  <p>mmetric(valid$category,p,""AUC"")</p>
  
  <p>mmetric(valid$category,p,""ACC"")</p>
</blockquote></li>
</ol>

<p>Now if AUC and ACC have small values then it means that model created by training data set is not good in making predictions.</p>

<p>Is my approach correct ?</p>

<p>Thanks and regards!</p>
"
"0.0700712753800578","0.0703597544730292"," 44281","<p>In R, is there a predefined function that will give me the log hazard ratio and its standard error for a black male (as shown in the example below) given the output of coxph regression?</p>

<pre><code>library(survival)
library(KMsurv)

#Kidney transplant data from Klein and Moeshberger. Massage data to make
#results look like those in book
data(kidtran)
data2 &lt;- kidtran
data2$Gender &lt;- ""male""
    data2[data2$gender==2,7] &lt;- ""female""
data2$Race &lt;- ""white""
    data2[data2$race==2,8] &lt;- ""black""
data2$Gender &lt;- as.factor(data2$Gender)
data2$Race &lt;- as.factor(data2$Race)
data2$Race &lt;- relevel(data2$Race,ref=""white"")

fit2 &lt;- coxph(Surv(time,delta) ~ Gender * Race, data=data2)
summary(fit2)

#Relative log risk for a black male (reference white female) from
#page 252 in Klein and Moeshberger
(coef(fit2)[3] + coef(fit2)[2] + coef(fit2)[1])
sqrt(sum(diag(fit2$var)) + 2*fit2$var[2,1] + 2*fit2$var[3,1] + 2*fit2$var[3,2])

#Let's use predict
black.male &lt;- data.frame(
  Gender=""male"",
  Race=""black""
)

white.female &lt;- data.frame(
  Gender=""female"",
  Race=""white""
)

bm &lt;- predict(fit2,newdata=black.male,se.fit=TRUE)

#bm in terms of original coefficients
coef(fit2)[3]*(1-fit2$means[3]) + coef(fit2)[2]*(1-fit2$means[2]) + coef(fit2)[1]*(1-fit2$means[1])

wf &lt;- predict(fit2,newdata=white.female,se.fit=TRUE)

#Relative log risk and se for a black male (reference white female) 
bm$fit - wf$fit
sqrt(bm$se.fit^2 + wf$se.fit^2)
</code></pre>
"
"0.0700712753800578","0.0703597544730292"," 44359","<p>So I have data from a randomized blind trial of 1mg of nicotine gum on dual n-back working memory scores; I analyzed them as usual with a t-test and found a small increase in means but a large increase in standard deviations on a f-test! Strange. I also have data for each day on mood/productivity that day on a 1-5 scale.</p>

<p>I wondered: is nicotine following an inverse U-curve, where it causes higher scores on the worser days (1-3) and lower scores on the better days (3-5)? I look around and it seems I want a multinomial logistic regression comparing the placebo &amp; active days.</p>

<p>I enter the data &amp; load <code>mlogit</code>:</p>

<pre><code>nicotine &lt;- read.table(stdin(),header=TRUE)
day      active mp score
20120824 1      3  35.2
20120827 0      5  37.2
20120828 0      3  37.6
20120830 1      3  37.75
20120831 1      2  37.75
20120902 0      2  36.0
20120905 0      5  36.0
20120906 1      5  37.25
20120910 0      5  49.2
20120911 1      3  36.8
20120912 0      3  44.6
20120913 0      5  38.4
20120915 0      5  43.8
20120916 0      2  39.6
20120918 0      3  49.6
20120919 0      4  38.4
20120923 0      5  36.2
20120924 0      5  45.4
20120925 1      3  43.8
20120926 0      4  36.4
20120929 1      3  43.8
20120930 1      3  36.0
20121001 1      3  46.0
20121002 0      4  45.0
20121008 0      2  34.6
20121009 1      3  45.2
20121012 0      5  37.8
20121013 0      4  37.2
20121016 0      4  40.2
20121020 1      3  39.0
20121021 0      3  41.2
20121022 0      3  42.2
20121024 0      5  40.4
20121029 1      2  41.4
20121031 1      3  38.4
20121101 1      5  43.8
20121102 0      3  48.2
20121103 1      5  40.6

library(mlogit)
Nicotine &lt;- mlogit.data(nicotine,shape=""wide"", choice=""mp"")
mlogit(score ~ (active + mp)^2, Nicotine)
Error in solve.default(H, g[!fixed]) : 
  Lapack routine dgesv: system is exactly singular
Calls: mlogit ... mlogit.optim -&gt; as.vector -&gt; solve -&gt; solve.default
</code></pre>

<p>The error also happens even with the simplest call I can think of:</p>

<pre><code>mlogit(score ~ active, Nicotine)
Error in solve.default(H, g[!fixed]) : 
  Lapack routine dgesv: system is exactly singular
Calls: mlogit ... mlogit.optim -&gt; as.vector -&gt; solve -&gt; solve.default
</code></pre>

<p>Reading the documentation for <code>mlogit</code> didn't much help, and look at the other questions having the same error, they're different enough I can't tell whether they apply or not.</p>

<p>Thank you for your assistance.</p>
"
"0.0707974219804893","0.0812444463702388"," 44895","<p>I am doing a regression analysis which troubled me. </p>

<p>My independent variable are 4 interplanetary condition components, and the dependent variable is the latitude of auroral oval boundary. 
So far, the specific relationship is still unknown in physical principle, what we want to do is to get a model (function expression) from the massive data which shows how these independent variables affect the dependent variable.</p>

<p>I used the Matlab statistical toolbox to do the regression analysis, but the results were very bad. The p values of the F statistic and t statistic are very small, but the RÂ² is also very low, about 20%. </p>

<p>So how should I improve the RÂ²? Are there good methods? I see that SVM (or LS-SVM) can do regression anaysis, is it a good way to manage the massive data, multiple independent variables regression anaysis?</p>

<p>The following are the results:</p>

<pre><code>mdl = 
Linear regression model:
    y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x2*x3 + x2*x4 + x1^2 + x2^2 + x3^2 + x4^2

Number of observations: 18471, Error degrees of freedom: 18457    
Root Mean Squared Error: 2.44  
R-squared: 0.225,  Adjusted R-Squared 0.225  
F-statistic vs. constant model: 413, p-value = 0  
</code></pre>

<p>when we add another predictor, i.e., the independent variables become 5, the resuts of the regression analysis are:</p>

<p>Linear regression model:</p>

<pre><code>y ~ 1 + x1*x2 + x1*x3 + x1*x4 + x1*x5 + x2*x3 + x2*x4 + x2*x5 + x3*x5 + x2^2 + x3^2 + x4^2 + x5^2
</code></pre>

<p>Number of observations: 18457, Error degrees of freedom: 18439
Root Mean Squared Error: 2.21
R-squared: 0.366,  Adjusted R-Squared 0.366
F-statistic vs. constant model: 627, p-value = 0</p>
"
"0.06396603026469","0.0642293744423385"," 45184","<p><a href=""http://www.nber.org/papers/w14723.pdf"" rel=""nofollow"">Lee and Lemieux</a> (p. 31, 2009) suggest the researcher to present the graphs while doing Regression discontinuity design analysis (RDD). They suggest the following procedure: </p>

<blockquote>
  <p>""...for some bandwidth $h$, and for some number of bins $K_0$ and
  $K_1$ to the left and right of the cutoff value, respectively, the
  idea is to construct bins ($b_k$,$b_{k+1}$], for $k = 1, . . . ,K =
 K_0$+$K_1$, where $b_k = câˆ’(K_0âˆ’k+1) \cdot h.$""</p>
</blockquote>

<pre><code>c=cutoff point or threshold value of assignment variable
h=bandwidth or window width.
</code></pre>

<p>...then compare the mean outcomes just to the left and right of the cutoff point...""</p>

<p>..in all cases, we also show the ï¬tted values from a quartic regression model estimated separately on each side of the cutoff point...(p. 34 of the same paper)</p>

<p>My question is how do we program that procedure in <code>Stata</code> or <code>R</code> for plotting the graphs of outcome variable against assignment variable (with confidence intervals) for the sharp RDD.. A sample example in <code>Stata</code> is mentioned <a href=""http://www.stata.com/statalist/archive/2010-11/msg00131.html"" rel=""nofollow"">here</a> and <a href=""http://www.stata.com/statalist/archive/2011-05/msg01640.html"" rel=""nofollow"">here</a> (replace rd with rd_obs) and a sample example in <code>R</code> is <a href=""http://blog.lib.umn.edu/moor0554/canoemoore/2010/02/regression_discontinuity_gallery_nonparametric.html"" rel=""nofollow"">here</a>. However, I think both of these didn't implement the step 1. Note, that both have the raw data along with the fitted lines in the plots.</p>

<p>Sample graph without confidence variable [Lee and Lemieux,2009] <img src=""http://i.stack.imgur.com/a2KPD.png"" alt=""enter image description here"">
Thank you in advance.  </p>
"
"0.0286064783845312","0.0287242494810713"," 45546","<p>Suppose I have two real-valued PMFs $f(i)$ and $g(i)$ with $1\le i\le N$ and $N$ is about 1 billion.  The functions $f$ and $g$ can be assumed to be continuous on the positive integers, one-sidedly so at the endpoints of course. (Meaning if i and j are close, then $f(i)$ and $f(j)$ are close.)</p>

<p>Lets say I have several samples of more than 10 billion observations each.  I tally each sample to get a series of histograms. The histograms basically give us functions $s_1(i)$, $s_2(i)$, $s_3(i)$ and so on.</p>

<p>For each sample, I know the percentage of the observations that come from $f$ and the percentage that come from the distribution described by $g$.</p>

<p>Can I use the samples to reconstruct the distributions $f$ and $g$.</p>

<p>My first guess was to do linear regression.  Although, I'm unsure if linear regression makes use of all the information.</p>
"
"0.110792414376932","0.103831970547663"," 46075","<p>Is it possible to add standard error or confidence interval to a plot of a predicted vs observed values derived from a multiple regression model? I believe that I have seen such plots as an output in Statistica, but am unsure how to create them in R.</p>

<p>I believe I have a solution (below), but am unsure that I have done this correctly. Basically, I have created a new <code>dataframe</code> with predictor variable in the range of their possible values. My worry with such an approach is that the prediction is based on the rows of data, and does not really address situations where the variables are randomly selected. </p>

<p>Many thanks for your help.</p>

<p><strong>Example:</strong></p>

<pre><code>set.seed(1)
n &lt;- 200
x1 &lt;- rnorm(n, mean=10, sd=3)
x2 &lt;- rnorm(n, mean=20, sd=5)
e &lt;- rnorm(n, mean=10, sd=3)

y &lt;- 5 + 2*x1 + 0.5*x2 + e

fit &lt;- lm(y ~ x1 + x2)
summary(fit)

#plot of predicted vs observed
pred1 &lt;- predict(fit, se.fit=TRUE)
plot(pred1$fit ~ y)
abline(0,1, col=8, lwd=2)

#new dataframe sequence of each predictor variable in their range
df.new &lt;- data.frame(x1=seq(min(x1), max(x1),,100), x2=seq(min(x2), max(x2),,100))
pred2 &lt;- predict(fit, df.new, se.fit=TRUE)

#plot of predicted vs observed w/ standard error interval?
png(""pred_vs_obs.png"", width=6, height=6, units=""in"", res=200)
plot(pred1$fit ~ y)
    abline(0,1, col=8, lwd=2)
    lines(pred2$fit+1.96*pred2$se.fit ~ pred2$fit, col=2, lty=2, lwd=2)
lines(pred2$fit-1.96*pred2$se.fit ~ pred2$fit, col=2, lty=2, lwd=2)
dev.off()
</code></pre>

<p><img src=""http://i.stack.imgur.com/AgUkO.png"" alt=""enter image description here""></p>

<p><strong>Edit:</strong> </p>

<p>The following code elaborates on my hesitation with the method that I used. The relationship between Standard Error (SE) and y is not a precise; i.e. various values of y that are relatively close together, have widely differing SE (black symbols in figure below, <code>pred1</code>), while the above method predicts a single SE for each predicted y (red symbols, <code>pred2</code>). Furthermore, using several different combinations of x1 and x2 that always result in the same y-value, I get a single (but different!) SE (green symbol, <code>pred3</code>). What is going on here? Is there a more correct way of doing this with some sort of permutation method?</p>

<pre><code>#? Do different solutions to a given predicted value always give the same standard error?
y.tmp &lt;- rep(40,20)
x1.tmp &lt;- seq(0,10, length(y.tmp))
x2.tmp &lt;- (y.tmp - fit$coeff[1] - fit$coeff[2]*x1.tmp) / fit$coeff[3]

df3 &lt;- data.frame(x1=x1.tmp, x2=x2.tmp)
pred3 &lt;- predict(fit, df3, se.fit=TRUE)

YLIM &lt;- range(pred1$se.fit, pred2$se.fit, pred3$se.fit)
    png(""fit.se_vs_fit.png"", width=6, height=6, units=""in"", res=200)
    plot(pred1$se.fit ~ pred1$fit, ylim=YLIM, lwd=2)
    points(pred2$se.fit ~ pred2$fit, col=2, lwd=2)
    points(pred3$se.fit ~ pred3$fit, col=3, lwd=2)
legend(""topright"", legend=c(""orig. data"", ""range of x1 &amp; x2"", ""various comb. of x1 &amp; x2 \nto acheive y=40""), col=1:3, pch=1, lwd=2, lty=0)
dev.off()
</code></pre>

<p><img src=""http://i.stack.imgur.com/4TcVs.png"" alt=""enter image description here""></p>
"
"0.118129972176712","0.12520610071704"," 46096","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to compare the relative importance of each of my predictor variables regarding their impact on the response variable (note: the predictors each have quite different scales - sometimes by orders of magnitude).  Unfortunately, the output from R gives me results as unstandardized (<em>b</em>) coefficients (""estimates"").  I'm hoping someone can give me a hint as to how to go about getting standardized (<em>beta</em>) coefficients from the NB regression model... or another 'better' way to determine the relative importance of each of my predictors on my response variable.</p>

<p>I've investigated several potential ways like: </p>

<ol>
<li>using the R package 'relimpo' (as suggested in a comment to <a href=""http://stats.stackexchange.com/a/7118"">http://stats.stackexchange.com/a/7118</a>), but it does not work on a NB regression model, thus completely changing the assumptions I should be accounting for and making the outcomes very different; </li>
<li>mean-centering and scaling my data, which changes the interpretation and makes it so that I can't use NB model due to response variables now having negative values; </li>
<li>scaling-only, so that I can still run a NB model... which I <em>thought</em> would only affect the scale of the coefficients without changing their direction (viz., <a href=""http://stats.stackexchange.com/a/29784"">http://stats.stackexchange.com/a/29784</a> ) - but I do get some positive coefficients that flip to neg. and vice-verse... which seems strange to me and makes me wonder whether I'm making a mistake.</li>
</ol>

<p>I've benefited from looking at <a href=""http://stats.stackexchange.com/q/29781"">When should you center your data &amp; when should you standardize?</a> (and the suggested links from comments on the question such as <a href=""http://andrewgelman.com/2009/07/when_to_standar/"" rel=""nofollow"">http://andrewgelman.com/2009/07/when_to_standar/</a> and <a href=""http://stats.stackexchange.com/q/7112"">When and how to use standardized explanatory variables in linear regression</a> and <a href=""http://stats.stackexchange.com/q/19216"">Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?</a>).  </p>

<p>Bottom line: I have not yet found a way to use a NB model in R (which I have statistically confirmed is more appropriate than lm, glm, or poisson for modeling my data) and still get at the relative importance - or at least to the standardized beta coefficients - for my predictors...</p>

<p>The R scripts is something like this:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
  predictor3, data=myData, control=glm.control(maxit=125))
summary(nb)

scaled_nb = glm.nb(scale(responseCountVar, center = FALSE) ~ scale(predictor1, center = FALSE) + scale(predictor2, center = FALSE) + 
  scale(predictor3, center = FALSE), data=myData, control=glm.control(maxit=125))
summary(scaled_nb)
</code></pre>
"
"0.0572129567690623","0.0574484989621426"," 46205","<p>I have a large amount of vegetation data that has been broken down into 13 habitat classes. I am trying to determine which vegetation tends to fall into or is absent from which habitat with any sort of significance. I have been put onto running a multinomial logistic regression, specifically using glmnet (as I have approximately 200 variables, and only about 260 observations).</p>

<p>Running cv.glmnet using the code:</p>

<pre><code>cv&lt;-cv.glmnet(data,Class,family=""multinomial"",nfolds=50,standardize=FALSE)
</code></pre>

<p>I get a list of numbers that I am struggling to understand, however I found the code:</p>

<pre><code>coef(cv, s=cv$lambda.1se)
</code></pre>

<p>Which returns the coefficients for each variable for each habitat class for the lambda that is 1 SE larger than the minimum Lambda value (which as far as I can tell the generally accepted lambda value).</p>

<pre><code>(Intercept)                                              0.7914263664   
Salix                                                    0.0000000000  
Mash                                                     0.0000000000   
Pin                                                      0.0000000000   
Choke                                                    .          
Betula                                                   0.0025260258   
Ideae                                                    0.0000000000   
Leather                                                  0.0000000000
</code></pre>

<p>What I'm wondering, using these coefficients, is it possible to state that those values with the largest magnitude (either closest to -1 and +1) are the most important in defining that class, which those close to 0 are unimportant, and those with periods were removed during the cv.glmnet. So in this case the plant ""Betula"" would be more influential than all others, and ""Choke"" was so uninfluential that it was removed? Also, no idea what intercept means, but I imagine I can find that one on my own.</p>
"
"0.051172824211752","0.0642293744423385"," 46391","<p>I need to build a model using climate variables (temperature, rainfall) to predict
monthly sales (horizon of 6 months) for certain product. The data has strong seasonality and a standard regression model would works fine, the problem is that the historic data will not be updated, meaning that the observed data points will not be incorporated into the model.</p>

<p>Whats a good way to solve this? What if i split the sales data into levels (say 'WEAK', 'NORMAL', 'HIGH', VERY HIGH') and then use a regression tree? Is there any 'danger' in doing this?</p>

<p>For a standard regression model, how i deal with the seasonality if the new points will not be incorporated?</p>

<p>I'm using R, thanks!</p>
"
"0.0904616275314925","0.090834052439095"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"0.12793206052938","0.128458748884677"," 47258","<p>I have following data stored in a file. I am applying 'glm' in R to find linear regression equation to best predict the 'output'. </p>

<pre><code>&gt; tmpData
   logOfOutput randomSample multiplied part1 part2 randNormalMean100Std20 output
1    0.0000000           33         11     1    19               89.65387      1
2    0.6931472           76         24     2    18              128.23471      2
3    1.0986123           12         39     3    17              103.70930      3
4    1.3862944           68         56     4    16               99.12617      4
5    1.6094379           50         75     5    15               95.68173      5
6    1.7917595            7         96     6    14              129.27551      6
7    1.9459101           70        119     7    13              104.59333      7
8    2.0794415           55        144     8    12              102.15247      8
9    2.1972246           20        171     9    11               72.43795      9
10   2.3025851           24        200    10    10               80.63634     10
11   2.3978953           32        231     9    11              105.03423     11
12   2.4849067           97        264     8    12               78.10613     12
13   2.5649494           28        299     7    13              107.95286     13
14   2.6390573           99        336     6    14               80.07396     14
15   2.7080502           66        375     5    15              102.01156     15
16   2.7725887           95        416     4    16              119.07361     16
17   2.8332133           42        459     3    17               64.19354     17
18   2.8903718           53        504     2    18              106.23402     18
19   2.9444390           85        551     1    19              151.07976     19
20   2.9957323           48        600     0    20               82.78324     20
</code></pre>

<p>I am using the following code to perform the same </p>

<pre><code>fn = ""delnowSample.txt""
tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
cnames = colnames(tmpData)
(fmla &lt;- as.formula(paste(cnames[length(cnames)], "" ~ "", paste(cnames[1:(length(cnames)-1)],collapse= ""+"")))  )
model &lt;- try(glm(formula = fmla, family=binomial(), na.action=na.omit, data=tmpData));
summary(model) 
</code></pre>

<p>The output that I get is as follow: </p>

<pre><code>&gt; summary(model)

Call:
glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse = ""+""))), 
    family = gaussian(), na.action = na.omit)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.37926  -0.11242  -0.03441   0.16087   0.28200  

Coefficients: (1 not defined because of singularities)
                                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                0.2638036  0.3078536   0.857  0.40592    
unlist(tmpData[""logOfOutput""])             0.9202273  0.2727884   3.373  0.00455 ** 
unlist(tmpData[""randomSample""])            0.0026201  0.0018177   1.441  0.17145    
unlist(tmpData[""multiplied""])              0.0288073  0.0012359  23.308 1.34e-12 ***
unlist(tmpData[""part1""])                   0.2106002  0.0403442   5.220  0.00013 ***
unlist(tmpData[""part2""])                          NA         NA      NA       NA    
unlist(tmpData[""randNormalMean100Std20""]) -0.0006214  0.0024922  -0.249  0.80673    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for gaussian family taken to be 0.04403284)

    Null deviance: 665.00000  on 19  degrees of freedom
Residual deviance:   0.61646  on 14  degrees of freedom
AIC: 1.1676

Number of Fisher Scoring iterations: 2
</code></pre>

<p>To a large extent it is predicting the Pr(z) correctly as we can see the probabilities of random variable are not significant. The R-square is also high (1-residual.deviance/null.deviance), close to 1.  </p>

<p>Question 1:
In the above data 'part1+part2' is equal to output variable. Is 'glm' not able to identify such type of relations? </p>

<p>Question 2: 
Why the degree of freedom of  null and residual deviance are different? </p>

<p>Question 3:
I need to convert the output variable into categorical variable (i.e. Everything &lt;=10 is 'no' and more than this is 'yes'). What is the best way to call 'glm', when the response variable is 'categorical'. I tried converting 'no' to '0' and 'yes' to 1, and called glm as follows:</p>

<pre><code> model &lt;- try(glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse= ""+""))), family=binomial(), na.action=na.omit));
</code></pre>

<p>I am getting warning message with this code. Also, I am not sure if this is the correct way to call categorical variable. </p>

<p>Edit:</p>

<p>I have the following categorical data:</p>

<pre><code>&gt; tmpData
           x1  x2 x3  y1
1  0.16294456   1  1  no
2  0.80494934   2  2  no
3  0.28962222   1  3  no
4  0.07177347   2  4  no
5  0.54830544   1  5  no
6  0.67655327   2  6  no
7  0.45189608   1  7  no
8  0.82412502   2  8  no
9  0.09076793   1  9  no
10 0.12221227   2 10  no
11 0.56751754 111 11 yes
12 0.04970992 222 12 yes
13 0.56162037 111 13 yes
14 0.96617891 222 14 yes
15 0.50994534 112 15 yes
16 0.70093692 212 16 yes
17 0.02034940 212 17 yes
18 0.78356903 121 18 yes
19 0.58439662 213 19 yes
20 0.31729282 212 20 yes
</code></pre>

<p>And the following code:</p>

<pre><code>  fn = ""delnowSample.txt""
  tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
  tmpData
  model &lt;- glm(formula = 'y1~x1+x2+x3', family=binomial(), na.action=na.omit, data=tmpData)
  summary(model) 
</code></pre>

<p>This one doesn't seem to be working?? </p>
"
"0.0700712753800578","0.0703597544730292"," 47774","<p>I am carrying a linear regression on some data. One of my variables is a factor (categorical). Using regression with an intercept leads to difficult interpretation, since one of the factor levels is taken as the intercept, and the remaining levels are given relative to that. Removing the intercept give me an effect of each level of the factor, which I what I want.</p>

<p>As far as I know, both models are precisely equivalent. They produce identical predictions (on the training set) - to within ~3e-15. However, their RÂ² scores vary wildly.</p>

<pre><code># MWE
library(car)
int &lt;- lm(fscore ~ 1 + partner.status + conformity + fcategory,
          data = Moore)  #with intercept
nint &lt;- lm(fscore ~ 0 + partner.status + conformity + fcategory,
           data = Moore) #w/o intercept
summary(int)$r.squared
summary(nint)$r.squared  #RÂ² values are not remotely the same
max(predict(int)-predict(nint)) #Predictions are essentially identical
</code></pre>

<p>Why are the models not identical? Is it because RÂ² is a comparision of the model to ""no model"", and that ""no model"" corresponds to ""y=0"" and ""y=mean(fscore)"", for nint and int, respectively?</p>
"
"0.06396603026469","0.0642293744423385"," 47947","<p>I have a dataset containing information about a bunch of wireless devices.  Specifically, the number of other wireless devices each device encounters.  Based on other researcher's prior work in the same area, I have reason to believe my distribution may potentially be well fitted by the not so well known <a href=""ftp://netlib.bell-labs.com/who/nuzman/papers/nssw_TCP.pdf"" rel=""nofollow"">biPareto distribution</a>, whose CCDF is defined as:</p>

<p>$1 - F(x) = (x/k)^{-\alpha}(\frac{x+kb}{k + kb})^{\alpha-\beta}$, for $ x &gt; k$</p>

<p>and simply </p>

<p>$1 - F(x) = 1$, for $x \le k$.</p>

<p>Assume I already know $k = 1$.  This leaves a function to fit with the three parameters $\alpha$,$b$,$\beta$, with the restriction that each parameter is $&gt; 0$.</p>

<p>What I am unsure about is how one would go about determining the most suitable fitting parameter values, ideally in <code>R</code>.  </p>

<p>I am quite the novice when it comes to fitting data, so aside from the syntactical considerations in <code>R</code>, I am also interested in any advice regarding what specific methods of fitting would be most appropriate for this particular function.  For example:</p>

<ol>
<li>Does this function qualify for linear regression?</li>
<li>Is least squares appropriate/applicable here?  What about minimum mean squared error (MMSE)?</li>
<li>How sensitive is any given fitting method to any ""guiding"" initial parameter values I give it (iterative methods?), or is the result determined analytically (always produces the same answer in the end?) </li>
</ol>

<p>If it's at all relevant, I'm ultimately looking to use the Kolmogorov-Smirnov test on the data to see how well it matches the parameterized biPareto distribution.</p>
"
"0.221700207480116","0.219022402293169"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.104071351726209","0.111466460825459"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.0286064783845312","0.0287242494810713"," 48694","<p>I'm working on a regression problem involving multiple independent learning tasks. I'm using conditional random forest as the learner (see Hothorn et al. 2006) as they are quite robust. The problem I'm having is saving the models. Multiple learning tasks means many big models.</p>

<p>How can I save a subset of each model without taking too much disk/memory space without compromising the usability of my models for predictions?</p>

<p>Thanks!</p>

<p>PK
^_^</p>
"
"0.0495478739876288","0.0497518595104995"," 48811","<p>For count data that I have collected, I use Poisson regression to build models. I do this using the <code>glm</code> function in R, where I use <code>family = ""poisson""</code>. To evaluate possible models (I have several predictors) I use the AIC. So far so good. Now I want to perform cross-validation. I already succeeded in doing this using the <code>cv.glm</code> function from the <code>boot</code> package. From <a href=""http://stat.ethz.ch/R-manual/R-patched/library/boot/html/cv.glm.html"">the documentation</a> of <code>cv.glm</code> I see that e.g. for binomial data you need to use a specific cost function to get a meaningful prediction error. However, I have no idea yet what cost function is appropriate for <code>family = poisson</code>, and an extensive Google search did not yield any specific results. My question is anybody has some light to shed on which cost function is appropriate for <code>cv.glm</code> in case of poisson glm's.</p>
"
"0.0404556697031367","0.0406222231851194"," 48854","<p>I have linear regression code written in R and I have to do the same thing in Java. I used <a href=""http://commons.apache.org/math/apidocs/overview-summary.html"" rel=""nofollow"">Apache Commons math</a> library for this. I used the same data in R code and in Java code, but I got different intercept value. I could not figure out what stupid thing I have done in the code.</p>

<p><strong>R Code:</strong></p>

<pre><code>test_trait &lt;- c( -0.48812477 , 0.33458213, -0.52754476, -0.79863471, -0.68544309, -0.12970239,  0.02355622, -0.31890850,0.34725819 , 0.08108851)
geno_A &lt;- c(1, 0, 1, 2, 0, 0, 1, 0, 1, 0)
geno_B &lt;- c(0, 0, 0, 1, 1, 0, 0, 0, 0, 0) 
fit &lt;- lm(test_trait ~ geno_A*geno_B)
fit
</code></pre>

<p><strong>R Output</strong>:</p>

<pre><code>Call:
lm(formula = test_trait ~ geno_A * geno_B)

Coefficients:
  (Intercept)         geno_A         geno_B  geno_A:geno_B  
    -0.008235      -0.152979      -0.677208       0.096383 
</code></pre>

<p><strong>Java Code (includes EDIT1):</strong></p>

<pre><code>package linearregression;
import org.apache.commons.math3.stat.regression.SimpleRegression;
public class LinearRegression {
    public static void main(String[] args) {

        double[][] x = {{1,0},
                        {0,0},
                        {1,0},
                        {2,1},
                        {0,1},
                        {0,0},
                        {1,0},
                        {0,0},
                        {1,0},
                        {0,0}
        };

        double[]y = { -0.48812477,
                       0.33458213,
                      -0.52754476,
                      -0.79863471,
                      -0.68544309,
                      -0.12970239,
                       0.02355622,
                      -0.31890850,
                       0.34725819,
                       0.08108851
        };
        SimpleRegression regression = new SimpleRegression(true);
        regression.addObservations(x,y);

        System.out.println(""Intercept: \t\t""+regression.getIntercept());
// EDIT 1 -----------------------------------------------------------
System.out.println(""InterceptStdErr: \t""+regression.getInterceptStdErr());
System.out.println(""MeanSquareError: \t""+regression.getMeanSquareError());
System.out.println(""N: \t\t\t""+regression.getN());
System.out.println(""R: \t\t\t""+regression.getR());
System.out.println(""RSquare: \t\t""+regression.getRSquare());
System.out.println(""RegressionSumSquares: \t""+regression.getRegressionSumSquares());
System.out.println(""Significance: \t\t""+regression.getSignificance());
System.out.println(""Slope: \t\t\t""+regression.getSlope());
System.out.println(""SlopeConfidenceInterval: ""+regression.getSlopeConfidenceInterval());
System.out.println(""SlopeStdErr: \t\t""+regression.getSlopeStdErr());
System.out.println(""SumOfCrossProducts: \t""+regression.getSumOfCrossProducts());
System.out.println(""SumSquaredErrors: \t""+regression.getSumSquaredErrors());
System.out.println(""XSumSquares: \t\t""+regression.getXSumSquares());
// EDIT1 ends here --------------------------------------------------

    }
}
</code></pre>

<p><strong>Java Output:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
</code></pre>

<p><strong>Java Output of EDIT1:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
InterceptStdErr:    0.17268454347538026
MeanSquareError:    0.16400973355415271
N:          10
R:          -0.3660108396736771
RSquare:        0.13396393475863017
RegressionSumSquares:   0.20296050132281976
Significance:       0.2982630977579106
Slope:          -0.21477287227272726
SlopeConfidenceInterval: 0.4452137360615129
SlopeStdErr:        0.193067188937234
SumOfCrossProducts:     -0.945000638
SumSquaredErrors:   1.3120778684332217
XSumSquares:        4.4
</code></pre>

<p>I will greatly appreciate your help. Thanks !</p>
"
"0.0762839423587497","0.0765979986161901"," 48922","<p>I am trying to estimate a selection model of the form:</p>

<p>$Z_i = 1[\alpha_0 + \alpha_1X_{1,i} + \alpha_2X_{2,i} + \delta_i$ > 0]</p>

<p>$Y_i = \beta_0 + \beta_1X_{1,i} + Z_i + \epsilon_i$</p>

<p>where $1[]$ denotes the indicator function.</p>

<p>The purpose of the model is to calculate the indirect effect of $X_1$ on $Y$ through $Z$, as well as the the direct effect.</p>

<p>My first question is how to go about estimating this type of model, and how this estimation can be achieved in R. As far as I see it I have a few possible approaches:</p>

<p>(1) Use a standard Heckman selection model, using OLS for both the reduced form and structural equations, using ivreg() in R. This will obviously ignore the constraint that $Z$ is bounded between 0 and 1.</p>

<p>(2) Estimate the first stage with a probit model (i.e. $\delta_i \sim N(0,1)$), and the second stage using standard OLS. I understand that I could do this via manual 2SLS, but as far as I am aware the standard errors will be incorrect? Am I right in that this model is feasible, and if so, can you direct me to a method of achieving this in R?</p>

<p>(3) Build a switching regression model (tobit-5) using the selection() function from sampleSelection package in R. I believe this model will estimate two equations for  $Y$, one for where $Z_i=0$ and one where $Z_i=1$, and with a unique intercept and coefficients for each of the regressors in the outcome equations.</p>

<p>The question then is how to get an estimate of the indirect effect of $X_1$ for each of these methods.</p>

<ul>
<li><p>If I use (1) or (2) then I imagine it might be possible to calculate the average marginal effect of $Z$ on $Y$, and the average marginal effect of $X_1$ on $Z$, then approximate the indirect effect by multiplying the two values?</p></li>
<li><p>If (3) then could I take the fitted value under the estimated model for $Y$ where $Z=0$, and compare the mean to the mean of the fitted values under the estimated model for $Y$ where $Z=1$? This would then give me an estimate of the marginal effect of $Z$? Then use the same method as above and multiple this effect by the marginal effect of $X_1$ on $Z$?</p></li>
</ul>

<p>Many thanks in advance!</p>
"
"0.0904616275314925","0.090834052439095"," 49497","<p>I have a dataset I'm working on that has some co-variate shift between the training set and the test set.  I'm trying to build a predictive model to predict an outcome, using the training set.  So far my best model is a random forest.</p>

<p>How can I deal with the shifted distributions in the training vs. test set?  I've come across 2 possible solutions that I've been able to implement myself:</p>

<ol>
<li>Remove the shifted variables.  This is sub-optimal, but helps prevent my model from over fitting the training set.</li>
<li>Use a logistic regression to predict whether a given observation is from the test set (after balancing the classes), predict ""test set probabilities"" for the training set, and then boostrap sample the training set, using the probabilities for sampling.  Then fit the final model on the new training set.</li>
</ol>

<p>Both 1 and 2 are pretty easy to implement, but neither one satisfies me, as #1 omits variables that might be relevant, and #2 uses a logistic regression, when my final model is tree-based.  Furthermore, #2 takes a few paragraphs of custom code, and I worry that my implementation may not be correct.</p>

<p>What are the standard methods for dealing with covariate shift?  Are there any packages in R (or another language) that implement these methods?</p>

<p>/edit: It seems like ""kernel mean matching"" is another approach I could take.  I've found lots of academic papers on the subject, but no one seems to have published any code.  I'm going to try to implement this on my own, and will post the code as an answer to this question when I do.</p>
"
"0.0404556697031367","0.0406222231851194"," 49568","<p>I have a research data with 3 levels of agregation: level1 is the individual, level2 is house area and level3 is school.</p>

<p>This means, the levels are not nested in the usual way (level1 within level2 within level3). In my case, level 1 lays within level 2 and also within level 3, but level 2 and level 3 are not within each other.</p>

<p>Is is still possible to estimate a hlm regression with such configuration with lme funcion in R?</p>
"
"NaN","NaN"," 49939","<p>What is the meaning of <code>t value</code> and <code>Pr(&gt;|t|)</code> when using <code>summary()</code> function on linear regression model in R?</p>

<pre><code>Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    10.1595     1.3603   7.469 1.11e-13 ***
log(var)                        0.3422     0.1597   2.143   0.0322 *
</code></pre>
"
"0.138114331619421","0.138682939753008"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0495478739876288","0.033167906340333"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"0.066748449563906","0.0765979986161901"," 50284","<p>Using R, I'd like to test whether multiple parameters in a regression model are
  equal to specific values (by default, are multiple parameters equal to 0).</p>

<p>For example, in this regression model:</p>

<p>score ~ beta0 + beta1*i1 + beta2*i2 + beta3*age + beta4*i1*age + beta5*i2*age</p>

<p>I want to test H0: (beta2 = 0) and (beta5 = 0)</p>

<p>SAS can do this in PROC REG using the TEST statement.</p>

<p>SAS Program editor contents:</p>

<pre><code>proc reg data=tolerate;
  model score=i1 i2 age i1age i2age;
  test i2=0, i2age=0;    * do assc prof have same reg line as asst? ;
run;
</code></pre>

<p>SAS Output window contents:</p>

<pre><code>       Test 1 Results for Dependent Variable score
                                Mean
Source             DF         Square    F Value    Pr &gt; F
Numerator           2        0.15581       0.38    0.6859
Denominator        24        0.40678
</code></pre>

<p>Here's code in R that starts the analysis:</p>

<pre><code># data description: http://statacumen.com/teach/ADA2/ADA2_HW_07_S13.pdf
tolerate &lt;- read.csv(""http://statacumen.com/teach/ADA2/ADA2_HW_07_tolerate.csv"")
tolerate$rank &lt;- factor(tolerate$rank)
tolerate$rank &lt;- relevel(tolerate$rank, ""3"")
str(tolerate)

tolerate.manual &lt;- data.frame(score = tolerate$score
                                , i1 = (tolerate$rank==1)
                            , i2 = (tolerate$rank==2)
                                , age = tolerate$age
                            , i1age = tolerate$age * (tolerate$rank==1)
                            , i2age = tolerate$age * (tolerate$rank==2)
                            )
lm.man &lt;- lm(score ~ i1 + i2 + age + i1age + i2age, data = tolerate.manual)
summary(lm.man)
</code></pre>

<p>I have been unable to find a solution using library multcomp, contrast, or C().
Ideally, I could do this without creating separate terms in the model, but directly from this lm() statement:</p>

<pre><code>lm.s.a.r.ar &lt;- lm(score ~ age*rank, data = tolerate)
</code></pre>

<p>This gets close using a Wald test, but I'm looking for the same F-test SAS uses.</p>

<pre><code>summary(lm.s.a.r.ar)
library(aod) # for wald.test()
coef.test.values &lt;- rep(0, length(coef(lm.s.a.r.ar))) # typically, this will be all 0s
wald.test(b = coef(lm.s.a.r.ar) - coef.test.values
        , Sigma = vcov(lm.s.a.r.ar)
        , Terms = c(4,6))

Wald test:
----------
Chi-squared test:
X2 = 0.77, df = 2, P(&gt; X2) = 0.68
</code></pre>

<p>Thanks for considering this question.</p>
"
"0.0572129567690623","0.0574484989621426"," 51208","<p>Suppose I want to test whether or not code patches created on weekends have a greater bug rate than those created on weekdays. (We might guess that this is so because people who are at work on weekends are more hurried etc.)</p>

<p>If we follow the standard and model bug creation as a Poisson process, this means (I think) that we want to tell the probability that both data sets came from the same underlying distribution, i.e. $\lambda_1 = \lambda_2$. </p>

<p>How can I do this? One way I thought of is use MLE to find the parameter which best fits one distribution, and then test the likelihood that parameter generates the second distribution. Alternatively, I could do two regressions and then use a likelihood ratio test. The problem with both methods is they are not testing whether one single $\lambda$ underlies both sets, but rather if the best-fit for one is the best-fit for the other.</p>
"
"0.0330319159917525","0.0497518595104995"," 51786","<p>Does anyone know what exact data cleaning steps one need to undertake in order to clean data for a logit regression (not a logistic regression)?</p>

<p>I have only time variables, meaning year and month, as my independent variables, and I am using R.</p>

<p>A logit regression is simply a normal linear regression where the DV have been transformed with the following formula:</p>

<blockquote>
  <p><code>logit(y) = ln(y/(1-y)</code> for </p>
</blockquote>

<p>An example:</p>

<blockquote>
  <p>3 of 12 people gets cured from taking a pill in period 3 ->
  <code>ln(0.25/(1-0.25)</code></p>
  
  <p>5 of 25 people gets cured taking a pill in period 5 ->
  <code>ln(0.20/(1-0.20)</code></p>
</blockquote>

<p>One can use the logit transformation if you have ratios and in many papers and books it is closely related to the logistic regression.</p>
"
"0.06396603026469","0.0642293744423385"," 52132","<p>I am currently working on a regression model where I have only categorical/factor variables as independent variables. My dependent variable is a logit transformed ratio.</p>

<p>It is fairly easy just to run a normal regression in R, as R automatically know how to code dummies as soon as they are of the type ""factor"". However this type of coding also implies that one category from each variable is used as a baseline, making it hard to interpret.</p>

<p>My professor have told me to just use effect coding instead (-1 or 1), as this implies the use of the grand mean for the intercept.</p>

<p>Does anyone know how to handle that?</p>

<p>Until now I have tried:</p>

<pre><code>gm &lt;- mean(tapply(ds$ln.crea, ds$month,  mean))
model &lt;- lm(ln.crea ~ month + month*month + year + year*year, data = ds, contrasts = list(gm = contr.sum))

Call:
lm(formula = ln.crea ~ month + month * month + year + year * 
    year, data = ds, contrasts = list(gm = contr.sum))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.89483 -0.19239 -0.03651  0.14955  0.89671 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.244493   0.204502 -15.865   &lt;2e-16 ***
monthFeb    -0.124035   0.144604  -0.858   0.3928    
monthMar    -0.365223   0.144604  -2.526   0.0129 *  
monthApr    -0.240314   0.144604  -1.662   0.0993 .  
monthMay    -0.109138   0.144604  -0.755   0.4520    
monthJun    -0.350185   0.144604  -2.422   0.0170 *  
monthJul     0.050518   0.144604   0.349   0.7275    
monthAug    -0.206436   0.144604  -1.428   0.1562    
monthSep    -0.134197   0.142327  -0.943   0.3478    
monthOct    -0.178182   0.142327  -1.252   0.2132    
monthNov    -0.119126   0.142327  -0.837   0.4044    
monthDec    -0.147681   0.142327  -1.038   0.3017    
year1999     0.482988   0.200196   2.413   0.0174 *  
year2000    -0.018540   0.200196  -0.093   0.9264    
year2001    -0.166511   0.200196  -0.832   0.4073    
year2002    -0.056698   0.200196  -0.283   0.7775    
year2003    -0.173219   0.200196  -0.865   0.3887    
year2004     0.013831   0.200196   0.069   0.9450    
year2005     0.007362   0.200196   0.037   0.9707    
year2006    -0.281472   0.200196  -1.406   0.1625    
year2007    -0.266659   0.200196  -1.332   0.1855    
year2008    -0.248883   0.200196  -1.243   0.2164    
year2009    -0.153083   0.200196  -0.765   0.4461    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3391 on 113 degrees of freedom
Multiple R-squared: 0.3626, Adjusted R-squared: 0.2385 
F-statistic: 2.922 on 22 and 113 DF,  p-value: 0.0001131 
</code></pre>
"
"NaN","NaN"," 52171","<p>We consider a sparse autoregressive time series of length 1000 obeying the model</p>

<p>$$X(t)=0.2X(t-1)+0.1X(t-3)+0.2X(t-5)+0.3X(t-10)+0.1X(t-15)+Z(t)$$</p>

<p>with nonzero coefficients at lags 1,3,5,10 and 15,where the innovations Z(t) are i.i.d. Gaussians with mean zero and standard deviation 0.1.</p>

<p>The question is how to simulate 1000 time series from the model with R or SAS?</p>
"
"0.143032391922656","0.137876397509142"," 52252","<p>I have a regression model that looks like this: $$Y = \beta_0+\beta_1X_1 + \beta_2X_2 + \beta_3X_3 +\beta_{12}X_1X_2+\beta_{13}X_1X_3+\beta_{123}X_1X_2X_3$$</p>

<p>...or in R notation: <code>y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x1:x2:x3</code></p>

<p>Let's say $X_1$ and $X_2$ are categorical variables and $X_3$ is numeric. The complication is that $X_1$ has three levels $X_{1a}, X_{1b}, X_{1c}$ and instead of standard contrasts, I need to test: </p>

<ul>
<li>Whether the intercept for level $X_{1a}$ significantly differs from the average intercept for levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the response of $X_2$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the slope of $X_3$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
</ul>

<p>Based on <a href=""http://stats.stackexchange.com/questions/32188/how-to-interpret-these-custom-contrasts"">this post</a> it seems like the matrix I want is...</p>

<pre><code> 2
-1
-1
</code></pre>

<p>So I do <code>contrasts(mydata$x1)&lt;-t(ginv(cbind(2,-1,-1)))</code>. The estimate of $\beta_1$ changes, but so do the others. I can reproduce the new estimate of $beta_1$ by subtracting the predicted values of the $X_1b$ and $X_1c$ group means (when $X_3=0$ and $X_2$ is at its reference level) from twice the value of $X_1a$ at those levels. But I can't trust that I specified my contrast matrix correctly unless I can also similarly derive the other coefficients.</p>

<p>Does anybody have any advice for how to wrap my head around the relationship between cell means and contrasts? Thanks. Is there a standard name for this type of contrast?</p>

<hr>

<p>Aha! According to the <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm"">link posted in Glen_b's answer</a>, the bottom line is, you can convert ANY comparison of group means you want into an R-style contrast attribute as follows:</p>

<ol>
<li>Make a square matrix. The rows represent the levels of your factor and the columns represent contrasts. Except the first one, which tells the model what the intercept should represent.</li>
<li>If you want your intercept to be the grand mean, fill the first column with all of the same non-zero value, doesn't matter what. If you want the intercept to be one of the level means, put a number in that row and fill the rest with zeros. If you want the intercept to be a mean of several levels, put numbers in those rows and zeros in the rest. If you want it to be a weighted mean, use different numbers, otherwise use the same number. <em>You can even put in negative values in the intercept column and that probably means something too, but it completely changes the other contrasts, so I have no idea what that's for</em></li>
<li>Fill in the rest of the columns with positive and negative values indicating what levels you want compared to what others. I forget why summing to zero is important, but adjust the values so that the columns do sum to zero.</li>
<li>Transpose the matrix using the <code>t()</code> function.</li>
<li>Use <code>ginv()</code> from the <code>MASS</code> package or <code>solve()</code> to get the inverse of the transposed matrix.</li>
<li>Drop the first column, e.g. <code>mycontrast&lt;-mycontrast[,-1]</code>. You now have a p x p-1 matrix, but the information you put in for your intercept was encoded in the matrix as a whole during step 5.</li>
<li>If you want labels in the summary output more pleasant to read than <code>lm()</code> et al.'s default output, name your matrix's columns accordingly. The intercept will always automatically get named <code>(Intercept)</code> however.</li>
<li>Make your matrix the new contrast for the factor in question, e.g. <code>contrasts(mydata$myfactor)&lt;-mymatrix</code></li>
<li>Run <code>lm()</code> (and probably many other functions that use formulas) as normal in standard R without having to load <code>glht</code>, <code>doBy</code>, or <code>contrasts</code>.</li>
</ol>

<p>Glen_b, thank you, and thank you UCLA Statistical Consulting Group. My applied stats prof spent several days handwaving on this topic, and I was still left with no idea how to actually write my own contrast matrix. And now, an hour of reading and playing with R, and I finally think I get it. Guess I should have applied to UCLA instead. Or University of StackExchange.</p>
"
"0.0404556697031367","0.0406222231851194"," 52352","<p>I have a dataset of counts (responses to a marketing campaign), collected at the zipcode level.  I am trying to use a poisson regression to determine underlying response rate of each zipcode.</p>

<p>How do I account for the fact that some zipcodes have very large populations, while other have very small populations, and my estimate of the response rate in these zip codes is much more uncertain? Furthermore, a count of 1 in a small zipcode is a lot more meaningful than a count of 1 in a large zipcode.</p>

<p>Is Poisson even the right approach to take here?  Some example code in R would be appreciated.</p>
"
"0.0495478739876288","0.0497518595104995"," 52379","<p>My case is that I have one continuous DV variable and two categorical IVs containing 11 and 12 different levels (YEAR &amp; MONTH) form 1998 to 2008.</p>

<p>Until now I have experimented a lot with contrasts() and the deviation coding seems to be the one I want to use, because I would rather compare the individuals levels in each IV to the  mean of YEAR rather than some default baseline.</p>

<p>An example of my output is provided here:</p>

<blockquote>
  <p>model &lt;- lm(LN.IDEA ~ 0 + MONTH + MONTH*MONTH + YEAR + YEAR*YEAR, data
  = ds)</p>
</blockquote>

<pre><code>             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) -3.467431   0.031038 -111.717  &lt; 2e-16 ***
MONTH1       0.207696   0.098558    2.107   0.0375 *  
MONTH2       0.080218   0.098558    0.814   0.4176    
MONTH3      -0.197687   0.098558   -2.006   0.0475 *  
MONTH4      -0.110153   0.098558   -1.118   0.2663    
MONTH5       0.039526   0.098558    0.401   0.6892    
MONTH6      -0.194322   0.098558   -1.972   0.0514 .  
MONTH7       0.174461   0.098558    1.770   0.0797 .  
MONTH8       0.014709   0.098558    0.149   0.8817    
MONTH9      -0.025038   0.094821   -0.264   0.7923    
MONTH10     -0.086207   0.094821   -0.909   0.3654    
MONTH11      0.060783   0.094821    0.641   0.5229    
YEAR1        0.081754   0.155188    0.527   0.5995    
YEAR2        0.545592   0.090489    6.029 2.65e-08 ***
YEAR3        0.044065   0.090489    0.487   0.6273    
YEAR4       -0.103906   0.090489   -1.148   0.2535    
YEAR5        0.005907   0.090489    0.065   0.9481    
YEAR6       -0.110614   0.090489   -1.222   0.2244    
YEAR7        0.076436   0.090489    0.845   0.4003    
YEAR8        0.069966   0.090489    0.773   0.4412    
YEAR9       -0.218867   0.090489   -2.419   0.0173 *  
YEAR10      -0.204054   0.090489   -2.255   0.0263 * 
</code></pre>

<p>It has been claimed that this question is a duplicate of </p>

<p><a href=""http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr/31694#31694"">Claimed answer</a></p>

<p>I realize that this answer touches upon what levels in regression are, but the question is about testing and interpreting p-values in regression. Therefore I argue that this is not a duplicate.</p>
"
"0.0700712753800578","0.0586331287275243"," 52522","<p>I have a quick question: if I plot the diagnostic plots to an R regression, a couple of them have ""Standardized Residuals"" as their y-axis such as in this plot:</p>

<p><img src=""http://i.stack.imgur.com/kZwaM.png"" alt=""enter image description here"">  </p>

<p>My question is this: <strong>what are the residuals standardized over?</strong> That is, let us assume that in my model, there are 100 predicted values hence 100 residuals.</p>

<ol>
<li>Standardized residual $e_i$ is defined as $(e_i - \bar e)/s_e$(realized residual - mean of all 100 realized residuals)/(standard deviation of all 100 realized residuals)?</li>
<li>Since each residual $e_i$ is itself a realized value out of a distribution of possible realizations for this single residual $e_i$, is this residual $e_i$ normalized by its own mean $\bar e_i$ and variance $\text{Var}(e_i)$ (as opposed to the mean and variance from all other values 1 to 100 as described above)?</li>
</ol>

<p>I tried finding documentation clarifying this distinction but could not find any that was beyond doubt. Any help answering this question, and providing me tips on how to find such answers through documentation in the future would be immensely appreciated.</p>
"
"0.0286064783845312","0.0287242494810713"," 52785","<p>I'm only a linguist, so my knowledge of statistics is very basic.</p>

<p>I fitted a logistic regression model with R (with <code>lrm(formula, y=T, x=T)</code>), and when I use the option <code>validate(lrm)</code>, I get some statistics I don't really understand.</p>

<pre><code>index.orig   training   test optimism index.corrected     n
Dxy           0.5984   0.6112  0.5461   0.0651          0.5333 40
R2            0.3258   0.3676  0.2929   0.0747          0.2511 40
Intercept     0.0000   0.0000 -0.0105   0.0105         -0.0105 40
Slope         1.0000   1.0000  0.8427   0.1573          0.8427 40
Emax          0.0000   0.0000  0.0399   0.0399          0.0399 40
D             0.2713   0.3176  0.2394   0.0782          0.1931 40
U            -0.0177  -0.0177  0.0092  -0.0269          0.0092 40
Q             0.2890   0.3353  0.2302   0.1051          0.1839 40
B             0.1864   0.1772  0.1972  -0.0201          0.2064 40
g             1.4632   1.6642  1.3460   0.3182          1.1449 40
gp            0.2840   0.3011  0.2703   0.0308          0.2532 40
</code></pre>

<p>I don't really understand most of that. I think <code>R2</code> and <code>Dxy</code> are supposed to be statistics of how good the predictors are, but I'm not sure how I should interpret the values, does the corrected <code>Dyx = 0.651</code> mean that there is a strong correlation, while the corrected <code>R2 = 0.0747</code> means that the correlation is very weak? I think the model is overfitted, but I'm not sure if I'm right.</p>

<p>Also, the other statistics are totally strange to me. What are <code>Emax, D, U, Q, B, g</code>, and <code>gp</code>?</p>
"
"0.13731109624575","0.132131547612928"," 53375","<p>I'm trying to test whether 4 different slopes from a 3-way interaction in multiple regression are significantly different from zero.  The four lines are plotted at 2 levels of each of the 2 moderators (lo-lo, hi-lo, lo-hi, hi-hi).</p>

<p>Here's how we could test the significance slopes in a 2-way interaction model.  I would like to extend this method to test the significance of slopes in a 3-way interaction model in multiple regression. A 2-way interaction in multiple regression takes the following form:</p>

<pre><code> y = a + b1(X) + b2(Z) + b3(X)(Z)
</code></pre>

<p>With a 2-way interaction, one can examine whether the slopes at various levels of the moderator, Z, are significantly different from zero using the following equations:</p>

<pre><code> b1 at Z = b1 + b3(Z)
</code></pre>

<p>where b1 is the slope of the predicted effects of X on Y at any particular value of Z</p>

<pre><code> SE(b1 at Z) = (var(b1) + (Z^2)(var(b3) + (2Z)(cov(b1,b3))^(1/2)
</code></pre>

<p>where var(b1) is the variance of the b1 regression coefficient, var(b3) is the variance of the b3 regression coefficient, cov(b1,b3) is the covariance between the b1, b3 regression coefficients</p>

<pre><code> t = (b1 at Z)/SE(b1 at Z)
 df = N - k - 1
</code></pre>

<p>where <em>N</em>=sample size and <em>k</em>=number of predictors</p>

<p>One can then test the significance of each slope using a t-test.  My question is, how can I extend this to the significance of slopes in a 3-way interaction model?  See my example R syntax below:</p>

<pre><code>set.seed(123)
predictor &lt;- rnorm(1000, 10, 5)
moderator1 &lt;- rnorm(1000, 100, 25)
moderator2 &lt;- rnorm(1000, 50, 20)
outcome &lt;- predictor*moderator1*moderator2*rnorm(20, 30)/10000
mydata &lt;- data.frame(predictor, moderator1, moderator2, outcome)

model &lt;- lm(outcome ~ predictor + moderator1 + moderator2 + predictor*moderator1 + predictor*moderator2 + moderator1*moderator2 + predictor*moderator1*moderator2, data=mydata)
plotData &lt;- expand.grid(
                    predictor = pretty(qnorm(pnorm(c(-1, 1)), mean = mean(mydata$predictor, na.rm = TRUE), sd = sd(mydata$predictor, na.rm = TRUE))),
                    moderator1 = qnorm(pnorm(c(-1, 1)), mean = mean(mydata$moderator1, na.rm = TRUE), sd = sd(mydata$moderator1, na.rm = TRUE)),
                    moderator2 = qnorm(pnorm(c(-1, 1)), mean = mean(mydata$moderator2, na.rm = TRUE), sd = sd(mydata$moderator2, na.rm = TRUE))
                    )

plotData$outcome &lt;- predict(model, newdata = plotData, level = 0)
    plotData$mod1 &lt;- factor(plotData$moderator1, labels = c(""Lo mod1"", ""Hi mod1""))
    plotData$mod2 &lt;- factor(plotData$moderator2, labels = c(""Lo mod2"", ""Hi mod2""))

mod1Lo_mod2Lo &lt;- plotData[plotData$mod1==""Lo mod1"" &amp; plotData$mod2==""Lo mod2"",]
mod1Hi_mod2Lo &lt;- plotData[plotData$mod1==""Hi mod1"" &amp; plotData$mod2==""Lo mod2"",]
mod1Lo_mod2Hi &lt;- plotData[plotData$mod1==""Lo mod1"" &amp; plotData$mod2==""Hi mod2"",]
mod1Hi_mod2Hi &lt;- plotData[plotData$mod1==""Hi mod1"" &amp; plotData$mod2==""Hi mod2"",]

#Generate Plot
plot(mod1Lo_mod2Lo$predictor, mod1Lo_mod2Lo$outcome, lty=1, lwd=2, type='l', xlab=""predictor"", ylab=""outcome"", ylim=c(min(plotData$outcome), max(plotData$outcome)))
lines(mod1Hi_mod2Lo$predictor, mod1Hi_mod2Lo$outcome, lty=2, lwd=2)
lines(mod1Lo_mod2Hi$predictor, mod1Lo_mod2Hi$outcome, lty=1, lwd=2, col=""gray"")
lines(mod1Hi_mod2Hi$predictor, mod1Hi_mod2Hi$outcome, lty=2, lwd=2, col=""gray"")
legend(""topleft"", legend=c(""lo Mod1, lo Mod2"",""hi Mod1, lo Mod2"",""lo Mod1, hi Mod2"",""hi Mod1, hi Mod2""), lty=c(1,2,1,2), lwd=c(2,2,2,2), col=c(""black"",""black"",""grey"",""grey""))
</code></pre>

<p>How can I test whether the slopes of each of these four lines, separately, is different from zero?</p>

<p>Many thanks in advance!</p>
"
"0.0495478739876288","0.0497518595104995"," 53432","<p>I have 3 categorical variables (CVa, CVb, CVc) all 0 or 1. Two continuous variables (IV1, IV2) are confounding my observational study. The multiple regression </p>

<pre><code>lm(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2)
</code></pre>

<p>is showing great significance for CVa</p>

<pre><code>              Estimate   Std. Error t value Pr(&gt;|t|)
(Intercept)  -1.414684   1.498886  -0.944  0.35233
CVa1         -0.841076   0.256946  -3.273  0.00255 **
CVb1         -0.413594   0.168753  -2.451  0.01990 * 
CVc1         -0.328669   0.183652  -1.790  0.08298 . 
IV1          -0.011768   0.006519  -1.805  0.08049 . 
IV2           0.487658   0.211015   2.311  0.02743 * 
CVa1:CVb1     0.321766   0.238869   1.347  0.18743   
CVa1:CVc1     0.741290   0.259402   2.858  0.00744 **
</code></pre>

<p>I thought that ANCOVA (between factor CVa) must also show significance, but</p>

<pre><code>summary(aov(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2))
</code></pre>

<p>is not showing any significance for CVa</p>

<pre><code>          Df Sum Sq Mean Sq F value  Pr(&gt;F)   
CVa        1  0.368  0.3681   3.093 0.08817 . 
CVb        1  0.427  0.4275   3.593 0.06709 . 
CVc        1  0.015  0.0148   0.125 0.72629   
IV1        1  0.585  0.5849   4.916 0.03384 * 
IV2        1  0.693  0.6935   5.828 0.02166 * 
CVa:CVb    1  0.126  0.1262   1.061 0.31069   
CVa:CVc    1  0.972  0.9716   8.166 0.00744 **
Residuals 32  3.807  0.1190
</code></pre>

<p>Am I doing ANOVA instead of ANCOVA? If yes, how do I control for IV1, IV2 to get that F-value they usually report in papers?</p>

<p>Just in case, <code>lsmeans(m2,pairwise ~ CVa * CVb)</code> reports that main effect of CVa is significant when controlled for IV1, IV2</p>

<pre><code>$`CVa:CVb pairwise differences`
               estimate        SE df  t.ratio p.value
0, 0 - 1, 0  0.47043119 0.1725208 32  2.72681 0.04807
</code></pre>
"
"NaN","NaN"," 54519","<p>Could you please advise whether studentized residuals are meaningful when computed on a robust linear regression model using an M-estimator? </p>

<p>I'd like to use it to detect outliers by doing something like this:</p>

<pre><code>rfit = rlm(y~x, data=d)
pt(rstudent(rfit), df=nrow(d)-3) 
</code></pre>

<p>Is this reasonable? I'd be quite happy with a rather crude measure and I'd rather err on the conservative side.</p>

<p>I'm also wondering whether I should run some kind of diagnostic of the general goodness of fit on this robust model before doing this.</p>
"
"0.0952081150392732","0.103566754353222"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.0495478739876288","0.0497518595104995"," 55462","<p>I am using <code>KFAS</code> package for <code>R</code>.</p>

<p>You can run</p>

<pre><code>install.packages(""KFAS"")
library(KFAS)
?regSSM
</code></pre>

<p>to see how this package allows to build a state space representation of linear regression models and many others.</p>

<p>Now let we have the following state space system:</p>

<p>$S_{t}=\alpha+(1+k_{t})L_{t}+v_{t}$</p>

<p>$k_{t}=\phi k_{t-1}+(1-\phi)\bar{k}+w_{t}$</p>

<p>being $\bar{k}$ a constant, a.k.a. the unconditional mean of the unobservable AR(1) process.</p>

<p>Anyone can tell me how may I set this state space representation in <code>KFAS</code> through <code>regSSM</code> or any other <code>KFAS</code> package's function (like <code>arimaSSM</code>)?</p>
"
"0.0495478739876288","0.0497518595104995"," 55662","<p>I am doing an ANCOVA model in order to explain the gap in a given distance. So I have a control group, and I have several quantitative variables, right now I am trying to evaluate the impact of the quantitative variables individually. But I think the results I get are incoherent, lest see:</p>

<pre><code>                           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
as.factor(groupe)           1  738.8   738.8  21.931 1.03e-05 ***
BASE0008                    1   36.6    36.6   1.087  0.29992    
as.factor(groupe):BASE0008  1  270.0   270.0   8.015  0.00576 ** 
Residuals                  87 2930.9    33.7 
</code></pre>

<p>These are the results from the ANOVA table, we could say that the group has a significant effect so does the interaction, but when I look at the results of the regression model, I find this:</p>

<pre><code>                              Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)                    1.16666    3.38404   0.345  0.73111   
as.factor(groupe)NAV           4.48191    3.79515   1.181  0.24084   
BASE0008                       0.26027    0.08651   3.009  0.00343 **
as.factor(groupe)NAV:BASE0008 -0.26902    0.09503  -2.831  0.00576 **
</code></pre>

<p>Well, the interaction is still relevant, but it looks like there is not effect of the group and the quantitative variable is more important to determinate the output of the experiment. I want to know if my interpretation is accurate: What can I say about the group? </p>
"
"0.0858194351535935","0.0861727484432139"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.148738510974761","0.154684817416817"," 56237","<p>I'm a beginner in R and Im wondering how to interprete my results.....
My question is about the results that I got after I did a regression on the Translog production function for panel data:
$ log(y)=log(A) + \alpha_{K} log(K) + \alpha_{L} log(L) + \beta_{KL} log(K)log(L) + \beta_{L^2} log^2(L) + \beta_{K^2} log^2(K)$</p>

<p>L stands for labour and K for Kapital.</p>

<p>The results I got for the Within, Random and first difference a the following:
Within:</p>

<pre><code>  #Within
    Coefficients :
  Estimate  Std. Error  t-value Pr(&gt;|t|)    
     K   1.0902e-05  1.0654e-06  10.2326   &lt;2e-16 ***
     L  -2.4009e-06  1.5086e-07 -15.9150   &lt;2e-16 ***
     LK  1.9788e-03  3.6069e-03   0.5486   0.5833    
     LL  3.0511e-02  1.3141e-03  23.2173   &lt;2e-16 ***
     KK  5.0333e-02  2.6650e-03  18.8868   &lt;2e-16 ***
     ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

    Total Sum of Squares:    6886.3
        Residual Sum of Squares: 1983.9
  R-Squared      :  0.71191 
  Adj. R-Squared :  0.69692 
    F-statistic: 10729.1 on 5 and 21709 DF, p-value: &lt; 2.22e-16


&gt; #regression random translog
&gt; tl.random&lt;-plm(Y ~ K + L + LK + LL + KK, data=panel, model=""random"")
 &gt; summary(tl.random)
 Oneway (individual) effect Random Effect Model 
(Swamy-Aroras transformation)

 Call:
 plm(formula = Y ~ K + L + LK + LL + KK, data = panel, model = ""random"")

  Balanced Panel: n=462, T=48, N=22176

  Effects:
               var std.dev share
  idiosyncratic 0.09139 0.30230 0.397
   individual    0.13856 0.37224 0.603
  theta:  0.8836  

  Residuals :
Min.  1st Qu.   Median  3rd Qu.     Max. 
 -3.16000 -0.14200  0.00724  0.15400  4.89000 

   Coefficients :
                 Estimate  Std. Error  t-value Pr(&gt;|t|)    
   (Intercept)  1.6266e+00  3.9030e-02  41.6763   &lt;2e-16 ***
    K            9.0932e-06  1.0552e-06   8.6178   &lt;2e-16 ***
    L           -2.5192e-06  1.5023e-07 -16.7684   &lt;2e-16 ***
   LK           2.7566e-03  3.6102e-03   0.7636   0.4451    
   LL           2.9491e-02  1.3138e-03  22.4474   &lt;2e-16 ***
   KK           4.8817e-02  2.6659e-03  18.3117   &lt;2e-16 ***
   ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

  Total Sum of Squares:    7183.6
  Residual Sum of Squares: 2070.2
  R-Squared      :  0.71181 
  Adj. R-Squared :  0.71162 
  F-statistic: 10951.9 on 5 and 22170 DF, p-value: &lt; 2.22e-16

  &gt; #regression first difference translog
   &gt; tl.fd&lt;-plm(Y ~ K + L + LK + LL + KK-1, data=panel, model=""fd"")
   &gt; summary(tl.fd)
    Oneway (individual) effect First-Difference Model


       #First difference regression
     Call:
      plm(formula = Y ~ K + L + LK + LL + KK - 1, data = panel, model = ""fd"")

      Balanced Panel: n=462, T=48, N=22176

      Residuals :
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    -1.4900 -0.0321  0.0199  0.0202  0.0715  0.9860 

          Coefficients :
          Estimate  Std. Error t-value  Pr(&gt;|t|)    
      K   2.3847e-07  2.8965e-06  0.0823 0.9343856    
      L  -8.0238e-07  2.3128e-07 -3.4693 0.0005229 ***
     LK -2.6986e-02  6.7755e-03 -3.9829 6.831e-05 ***
     LL  5.6920e-02  2.3933e-03 23.7830 &lt; 2.2e-16 ***
     KK  3.7811e-02  5.1254e-03  7.3773 1.674e-13 ***
    ---
       Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

        Total Sum of Squares:    426.54
       Residual Sum of Squares: 269.92
        R-Squared      :  0.38799 
          Adj. R-Squared :  0.3879 
</code></pre>

<p>My question are:</p>

<p>1) Is there a reason why the estimation for coefficient for LK is not significant in both within and random? but in first diff?</p>

<p>2) Why give within and random so similar results, and why first difference is different from them?</p>

<p>3)Can I interpret Standard error and R squared?
Is there anything else I can interpret? Which is the best model of the three?</p>

<p>Thank you so much for your help! </p>
"
"0.064873395163555","0.0759972207238908"," 56962","<p>Greedings to everybody.</p>

<p>I have the dataset which you can find <a href=""https://dl.dropboxusercontent.com/u/8546316/Dataset.csv"" rel=""nofollow"" title=""here"">here</a>, containing many different characteristics of different houses, including their types of heating, or the number of adults and children living in the house. In total there are about 500 records. I want to use an algorithm, that can be trained using the dataset above, in order to be able to predict the electricity consumption of a house that is not in the set.</p>

<p>I have tried every possible machine learning algorithm (using weka) (linear regression, SVM etc) . However I had about 350 mean absolute error, which is not good. I tried to make my data to take values from 0 to 1, or to delete some characteristics. I did not managed to find some good results.</p>

<p>I also tried to use R tool, and I did not have good results either...</p>

<p>I would be very grateful, if someone could give me some advice, or if you could examine a little the dataset and run some algorithms on it. What type of preprocessing should I use, and what type of algorithm?</p>
"
"0.12136700910941","0.121866669555358"," 57031","<p>I have data from a survey experiment in which respondents were randomly assigned to one of four groups:</p>

<pre><code>&gt; summary(df$Group)
       Control     Treatment1     Treatment2     Treatment3 
            59             63             62             66 
</code></pre>

<p>While the three treatment groups do vary slightly in the stimulus applied, the main distinction that I care about is between the control and treatment groups. So I defined a dummy variable <code>Control</code>:</p>

<pre><code>&gt; summary(df$Control)
     TRUE FALSE 
       59   191 
</code></pre>

<p>In the survey, respondents were asked (among other things) to choose which of two things they preferred: </p>

<pre><code>&gt; summary(df$Prefer)
      A   B  NA's 
    152  93   5 
</code></pre>

<p>Then, after receiving some stimulus as determined by their treatment group (and none if they were in the control group), respondents were asked to choose between the same two things:</p>

<pre><code>&gt; summary(df$Choice)
  A    B 
149  101 
</code></pre>

<p>I want to know if the being in one of the three treatment groups had an effect on the choice that respondents made in this last question. My hypothesis is that respondents who received a treatment are more likely to choose <code>A</code> than <code>B</code>.     </p>

<p>Given that I am working with categorical data, I have decided to use a logit regression (feel free to chime in if you think that's incorrect). Since respondents were randomly assigned, I am under the impression that I shouldn't necessarily need to control for other variables (e.g. demographics), so I have left those out for this question. My first model was simply the following:</p>

<pre><code>&gt; x0 &lt;- glm(Product ~ Control + Prefer, data=df, family=binomial(link=""logit""))
&gt; summary(x0)

Call:
glm(formula = Choice ~ Control + Prefer, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8366  -0.5850  -0.5850   0.7663   1.9235  

Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           1.4819     0.3829   3.871 0.000109 ***
ControlFALSE         -0.4068     0.3760  -1.082 0.279224    
PreferA              -2.7538     0.3269  -8.424  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 328.95  on 244  degrees of freedom
Residual deviance: 239.69  on 242  degrees of freedom
  (5 observations deleted due to missingness)
AIC: 245.69

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I am under the impression that the intercept being statistically significant is not something that holds interpretable meaning. I thought perhaps that I should include an interaction term as follows:</p>

<pre><code>&gt; x1 &lt;- glm(Choice ~ Control + Prefer + Control:Prefer, data=df, family=binomial(link=""logit""))
&gt; summary(x1)

Call:
glm(formula = Product ~ Control + Prefer + Control:Prefer, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5211  -0.6424  -0.5003   0.8519   2.0688  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                         3.135      1.021   3.070  0.00214 ** 
ControlFALSE                       -2.309      1.054  -2.190  0.02853 *  
PreferA                            -5.150      1.152  -4.472 7.75e-06 ***
ControlFALSE:PreferA                2.850      1.204   2.367  0.01795 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 328.95  on 244  degrees of freedom
Residual deviance: 231.27  on 241  degrees of freedom
  (5 observations deleted due to missingness)
AIC: 239.27

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Now the respondents status as in a treatment group has the expected effect. Was this a valid set of steps? How can I interpret the interaction term <code>ControlFALSE:PreferA</code>? Are the other coefficients still the log odds?</p>
"
"0.0948769553749019","0.0866068708302494"," 57157","<p>This is probably a very naive question... I'd like to estimate ""adjusted"" or ""conditional"" means for a variable (i'm unsure of the correct terminology). My data are on cortisol levels (dependent variable) in rabbits (n=56). I have many measurements at different times of the day, over many months. I'd like to calculate mean weekly values of cortisol for each individual rabbit so these can be used as a predictor in another model for which I only have weekly data. Rather than calculate the means from the raw data, i'd like to control for the time of day the samples were taken (this can influence the measurement). I thought i'd regress time of day (in minutes from 00:00 each day) on cortisol level and then extract the fitted values and calculate the weekly mean for each rabbit from these. Would this give me the estimated mean for cortisol, while controlling for time of day? </p>

<p>I can't share my data, but i've created a similar mock up using the iris data set. Here I fit a model, extract the fitted values and then calculate ""adjusted"" means for each species while controlling for the predictor. Am I right in thinking the difference between these means and the ones for the raw data (below) reflect the adjustment made when controlling for the independent variable?</p>

<pre><code>data(iris)

fit &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)
summary(fit)

with(iris, plot(Sepal.Length ~ Petal.Length, col = as.numeric(Species), asp = 1))
abline(coef(fit))

iris$fitted &lt;- fitted(fit)

with(iris, aggregate(fitted, list(Species), mean))
#      Group.1       x
# 1     setosa  4.9044
# 2 versicolor  6.0486
# 3  virginica  6.5769

with(iris, aggregate(Sepal.Length, list(Species), mean))
#      Group.1      x
# 1     setosa  5.006
# 2 versicolor  5.936
# 3  virginica  6.588
</code></pre>
"
"0.114425913538125","0.107715935554017"," 57227","<p>I am wondering about the distribution of the error term/innovation process in a ARCH/GARCH process and its implementation, I am not sure about some points. The basic assumption is</p>

<p>$r_t=\sigma_t*\epsilon_t$</p>

<p>where the $\sigma_t$ is the volatility, modeled by ARCH/GARCH and the $\epsilon_t$ are mostly assumed to be N(0,1).</p>

<p>Now my questions are:</p>

<ol>
<li><p>More sophisticated models drop this assumption. So I can say, e.g. $\epsilon_t$ follows a generalized hyperbolic distribution. So the mean does not need to be zero and the variance does not need to be equal to 1. This is correct, right?</p></li>
<li><p>If I use the <a href=""http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf"" rel=""nofollow"">rugarch package</a>: It supports different distributional assumptions. But I am not getting the following: So they also drop the assumption of mean zero and variance one? Or are they using something like a ""standardized"" version?</p></li>
<li><p>Suppose I want to fit a GARCH(1,1) assuming, that the $\epsilon_t$ follow a generalized hyperbolic distribution, but the mean does not have to be zero and the variance does not need to be one. Is rugarch doing a jointly parameter estimation? So in my final output, do I get the parameters of the GARCH process and the parameters of my generalized hyperbolic distribution?</p></li>
</ol>

<p>My last question is, how can I implement this?</p>

<p>I guess I have to use the following command:</p>

<pre><code>ugarchspec(variance.model = list(model = ""sGARCH"", garchOrder = c(1, 1), 
submodel = NULL, external.regressors = NULL, variance.targeting = FALSE), 
mean.model = list(armaOrder = c(1, 1), include.mean = TRUE, archm = FALSE, 
archpow = 1, arfima = FALSE, external.regressors = NULL, archex = FALSE), 
distribution.model = ""norm"", start.pars = list(), fixed.pars = list(), ...)
</code></pre>

<p>the distribution.model has to be set to <code>ghyp</code>. Is this assuming a mean of zero and a variance of one? </p>

<p>I think no, right?</p>

<p>How can I use the hyperbolic distribution for distribution.model?</p>
"
"0.0404556697031367","0.0406222231851194"," 58151","<p>I'm using the well-known USC Burns Survival dataset to explore logistic regression in R.</p>

<p>The independent variable is burn area, and the outcome is binary survival (yes/no).</p>

<p>In the documentation, burn areas are grouped as the 'midpoint of set intervals', and taken as a log ie.</p>

<pre><code>log(area + 1)
</code></pre>

<p>for sample datapoints like (highest/lowest):</p>

<pre><code>1.35    yes
2.35    no
</code></pre>

<p>Medically, burns are usually specifed in terms of surface area % of total body eg. 40%, or alternatively, as an estimate of surface area in square meters. My question is: how does this relate to the dataset independent variable ie. what does '1.35' actually mean in terms of % body surface area burnt? What would eg. 30% burns become in the dataset, using the 'midpoint of set interval'?</p>

<p>Thanks guys</p>

<p>Ref: <a href=""http://statmaster.sdu.dk/courses/st111/data/index.html#burns"" rel=""nofollow"">http://statmaster.sdu.dk/courses/st111/data/index.html#burns</a></p>
"
"0.134176277047853","0.134728672455117"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.0286064783845312","0.0287242494810713"," 58504","<p>I am running a negative binomial regression of clinic counts in each county in the entire country (~3k counties).  I'd like to at least partially account for the non-independence of neighboring counties by bootstrapping the confidence intervals in a ""clustered"" fashion--e.g. draw an entire state's (50 states total) worth of data at once.  This has become <a href=""http://www.mitpressjournals.org/doi/abs/10.1162/rest.90.3.414#.Vg7SjnUVhBc"" rel=""nofollow"">standard practice</a>, for better or for worse, in the econometric literature.</p>

<p>I could write the code to do this myself, but the <code>boot</code> package seems like it should have the ability to do this somehow, and in general I prefer tested, general solutions to one-off hacks.  Is there a way to coerce the <code>boot</code> package to do a clustered bootstrap?</p>

<p>I tried the <code>strata</code> argument, but that randomizes <em>within</em> strata rather than randomizing which cluster gets taken, as the following code confirms:</p>

<pre><code>dat &lt;- data.frame( cluster=rep(letters[1:5],each=10), x=runif(5*10), stringsAsFactors=TRUE )
boot.stat &lt;- function(dat,idx) {
    print(dat[idx,]$cluster)
    	print(table(dat[idx,]$cluster))
    mean(dat[idx,]$x)
    }
    boot( 
    	data=dat, 
    	statistic=boot.stat, 
    	strata=dat$cluster, 
    stype=""i"", 
    R=5 
)
</code></pre>
"
"0.0756856276908142","0.0759972207238908"," 59074","<p>I am trying to get a grasp on how to use machine learning to predict financial timeseries 1 or more steps into the future.</p>

<p>I have a financial timeseries with some descriptive data and I would like to form a model and then use the model to predict n-steps ahead.</p>

<p>What I have been doing so far is:</p>

<pre><code>getSymbols(""GOOG"")

GOOG$sma &lt;- SMA(Cl(GOOG))
    GOOG$range &lt;- GOOG$GOOG.High-GOOG$GOOG.Low

tail(GOOG)


           GOOG.Open GOOG.High GOOG.Low GOOG.Close GOOG.Volume GOOG.Adjusted     sma range
2013-05-07    863.01    863.87   850.67     857.23     1959000        857.23 828.214 13.20
2013-05-08    857.00    873.88   852.91     873.63     2468300        873.63 834.232 20.97
2013-05-09    870.84    879.66   868.23     871.48     2200600        871.48 840.470 11.43
2013-05-10    875.31    880.54   872.16     880.23     1897700        880.23 848.351  8.38
2013-05-13    878.89    882.47   873.38     877.53     1448500        877.53 854.198  9.09
2013-05-14    877.50    888.69   877.14     887.10     1579300        887.10 860.451 11.55
</code></pre>

<p>Then I have fitted a randomForest model to this data.</p>

<pre><code>fit &lt;- randomForest(GOOG$GOOG.Close ~ GOOG$sma + GOOG$range, GOOG)
</code></pre>

<p>Which seems to fit surprisingly well:</p>

<pre><code>&gt; fit

Call:
 randomForest(formula = GOOG$GOOG.Close ~ GOOG$sma + GOOG$range,      data = GOOG) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 1

          Mean of squared residuals: 353.9844
                    % Var explained: 97.28
</code></pre>

<p>And tried to use it to predict:</p>

<pre><code>predict(fit, GOOG, n.ahead=2)
</code></pre>

<p>But this prediction ofc did not work.</p>

<p>I try to predict the Close, should I lag the other variables by as many steps as I want the prediction, before fitting the model?</p>

<p>Probably a lot of other stuff I should take into account as well but these are really my first steps trying out machine-learning.</p>

<p>Thankful for any tips!</p>
"
"0.0825797899793814","0.0912117424359157"," 59166","<p>Iâ€™ve simulated some data consisting of one response variable (â€˜yâ€™) and two collinear predictor variables (â€˜Amountâ€™ and â€˜MPSâ€™), where collinearity arises from one of two causes: (1) Amount causes MPS, or (2) Amount and MPS are jointly affected by an unmeasured variable. </p>

<p>What I'm trying to do is figure out whether path analysis can discriminate between these two causes of collinearity. But I'm having trouble specifying a path model for collinearity scenario (2). </p>

<p>My question:
<strong>Is it possible to specify a path model that implies that two exogenous variables are jointly influenced by an unmeasured variable?</strong></p>

<p>I'm working in lavaan, but answers for how to do this conceptually would also be appreciated (if you aren't familiar with lavaan).</p>

<p>Here are my data, simulated in R:</p>

<pre><code># collinearity cause (1)
Amount &lt;- rnorm(n=350, mean=0, sd=1)   
MPS &lt;- rnorm(n=350, mean=0.76*Amount, sd=0.653) 
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72) 

# collinearity cause (2)
Lurking &lt;- rnorm(n=350, mean=0, sd=1) 
Amount &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
MPS &lt;- rnorm(n=350, mean=0.872*Lurking, sd=0.486)  
y &lt;- rnorm(n=350, mean=0.367*Amount + 0.367*MPS, sd=0.72)         
</code></pre>

<p>And this is my path model for (1), specified in lavaan:</p>

<pre><code>model1 &lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  MPS ~ Amount
  '
</code></pre>

<p>And this is a path model I tried for (2):</p>

<pre><code>model2&lt;- '
  #regressions
  y ~ Amount
  y ~ MPS
  #residual correlations
  MPS ~~ Amount
  '
</code></pre>

<p>so for path model (2) my approach was to specify a residual correlation between MPS and Amount. I'm uncertain if this is the correct approach. but even if it is, it doesn't work â€“ to make it work I have to specify that exogenous variables are not fixed, and this uses up my degrees of freedom so I can't test the model.</p>

<p>If anyone has any suggestions for how I can do this â€“ or if it is possible at all - I'd really appreciate it.</p>
"
"0.0904616275314925","0.090834052439095"," 59530","<p>With a colleague, we are working on a dataset containing ~5000 continuous variables for 120 individuals belonging to 8 classes.</p>

<p>We want to <strong>estimate the relative importance of each variable</strong> to explain the classes.
We have used a random forest approach with some success.
Now, we could like to go deeper by considering the fact that <strong>the 8 classes we fit are unequally distant from each other</strong>.
In fact, in our case <strong>we can <em>a priori</em> generate a distance matrix (<em>i.e.</em> cost matrix) for all possible pairs of classes</strong>.</p>

<p>My (very limited) understanding of random forest is that, for regression problems,
the error $E$ is computed by the mean square difference between the OOB sample and the prediction for the same sample:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{{(y_i-\hat{y}_i)}^2}$</p>

<p>Where $y_i$ is the predicted value and $\hat{y}_i$ the real value of an out-of bag-sample $i$.</p>

<p>Ultimately, the calculation of the variable importance depends on how the error is computed (right?).</p>

<p>In our case, I would like to use a modified loss function, for instance:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{M_{y_i,\hat{y}_i}}$</p>

<p>Where $M$ is predefined a distance matrix; so $M_{a,b}$ is the distance between class $a$ and $b$.
In this way the misclassification error would be more important if $y_i$ and $\hat{y}_i$ represent distant classes and, ultimately, <strong>the variable importance should be more relevant</strong>.</p>

<p>My questions are:</p>

<ol>
<li>Does this approach make sense to you, or am I missing something?</li>
<li>Can you think of any study that has used something similar.</li>
<li>We have so far used the <code>randomForest</code> package in R. It does not seem possible to use it in combination with an a priori distance matrix between classes. Do you know if this is already implemented somewhere?</li>
</ol>

<p><strong>EDIT</strong></p>

<p>I believe this is a very frequent problem in my field, biology, because we deal with classes for which relations can be represented and quantified by trees (dendrograms), often because of there lineage.</p>

<p>After some research, it appears that my question is about using a <strong>cost-sensitive</strong> version of random forest. In this respect, it is very similar to <a href=""http://stats.stackexchange.com/questions/46963/how-to-control-the-cost-of-misclassification-in-random-forests"">this question</a>.
I specificity want to use a <strong>cost matrix</strong> rather than a cost vector though.
It there any ontological reason why it is not possible or is it simply not implemented?</p>
"
"NaN","NaN"," 59691","<p>I have a question about the <code>rugarch</code> package. </p>

<p>My sample size is 43 and I have a problem to model a garch whose mean equation includes an exogenous model; otherwise my mean equation is linear regression, that is, $y_i=a+bx_i+e_i$ that $i=1, \ldots, 43$ and $e_i$ follow a $\text{garch}(1,1)$. But when I run it simultaneously R says <code>you need at least 100 points</code>. I don't know what to do.</p>
"
"0.0286064783845312","0.0287242494810713"," 59918","<p>I need to create a graph like the one below.The idea is that I have three data groups with 5 replications on the first two groups and 3 replications on the last one. I must obtain a regression curve for each group and link it to the mean and SD for each group, like the image below. I chose to do the analysis in R; I can get the models but have no idea about how to link it to the mean and SD per group. If any of you has some tutorial, example or code which can help it would be really appreciated.</p>

<p><img src=""http://i.stack.imgur.com/zFicp.jpg"" alt=""enter image description here""></p>

<p>Thanks in advance. </p>
"
"0.107274293941992","0.107715935554017"," 59952","<p>In the ""Dynamic Linear Models with R"" book, the Regression models section reads: ""The static regression linear model corresponds to the case where $W_t = 0$ for any $t$, so that $\theta_t = \theta$ is constant over time.""  </p>

<p>I am not understanding what this means, because when I fit a <code>dlmModReg</code> with <code>dW = 0</code> (default values) and then plot the predicted values of the state vectors, my regression coefficients vary over time, and eventually stabilize to a value similar to that of standard least-squares regression.</p>

<p>Plot of regression slope coefficient with <code>dW = 0</code>: </p>

<p><img src=""http://i.stack.imgur.com/I5MLB.jpg"" alt=""dW = 0""></p>

<p>However, if I use <code>dlmMLE</code> to find the MLE of <code>dW</code> before fitting the model, plotting the predicted values of the state vectors results in non-sensible values with multiple large discontinuities.</p>

<p>Plot of regression slope coefficient with <code>dW</code> MLE and no intercept: </p>

<p><img src=""http://i.stack.imgur.com/VzdIB.jpg"" alt=""dW != 0 no intercept""></p>

<p>Plot of regression slope with <code>dW</code> MLE with intercept:
<img src=""http://i.stack.imgur.com/2Yu7P.jpg"" alt=""dw != 0 with intercept""></p>

<p>I've yet to find any literature on how the intercept coefficient gets set, but I'm observing a near perfect linear relationship between the intercept and slope coefficient.  The inclusion of an intercept term in the parameter MLE also causes my <code>dV</code> to drop from 16 to 0.003.  Can anyone point me to any references which discuss how the intercept term is set on each update?</p>

<p>Plot of intercept and slope with <code>dW</code> MLE:
<img src=""http://i.stack.imgur.com/TjvTW.jpg"" alt=""intercept and slope""></p>

<p><strong>My questions are:</strong></p>

<ol>
<li>Why are the regression coefficients dynamic/time-varying even with <code>dW = 0</code> in the first example?  If the regression coefficient changes at every time step regardless of whether <code>dW</code> is 0 or not due to the measurement update stage $\beta_t = \hat{\beta_t} + K_t(y_t - \alpha_t - \hat{\beta_t}x_t)$ ($K$ is the Kalman gain), what's the difference between simple linear regression and dynamic linear regression except for some random variation added in the time update equation $\hat{P_t} = P_{t-1} + dW$ ($P$ is estimate error covariance)?</li>
<li>Why does the plot of regression coefficients have so many discontinuities when my <code>dW != 0</code>?</li>
<li>What is the relationship between the intercept and the slope coefficients?  Plotting them reveals a near-perfect linear relationship, but I can't find any literature explaining this.  I haven't found the intercept term included in any formulation of the Kalman update equations.</li>
</ol>

<p>Would anyone be willing to take a look at my data/code?</p>
"
"0.0404556697031367","0.0406222231851194"," 60109","<p>I would like to understand what the following code is doing. The person who wrote the code no longer works here and it is almost completely undocumented. I was asked to investigate it by someone who thinks ""<em>it's a bayesian logistic regression model</em>""</p>

<pre><code>bglm &lt;- function(Y,X) {
    # Y is a vector of binary responses
    # X is a design matrix

    fit &lt;- glm.fit(X,Y, family = binomial(link = logit))
    beta &lt;- coef(fit)
    fs &lt;- summary.glm(fit)
    M &lt;- t(chol(fs$cov.unscaled))
    betastar &lt;- beta + M %*% rnorm(ncol(M))
    p &lt;- 1/(1 + exp(-(X %*% betastar)))
    return(runif(length(p)) &lt;= p)
}
</code></pre>

<p>I can see that it fits a logistic model, takes the transpose of the Cholseky factorisation of the estimated covariance matrix, post-multiplies this by a vector of draws from $N(0,1)$ and is then added to the model estimates. This is then premultiplied by the design matrix, the inverse logit of this is taken, compared with a vector of draws from $U(0,1)$ and the resulting binary vector returned. But what does all this <strong><em>mean</em></strong> statistically ?</p>
"
"0.0904616275314925","0.090834052439095"," 60274","<p>I have a dataset that I'm trying to classify into 2 groups, A and B, using a random forest model. I know the true grouping and I'm trying to see how well I can model it using the other available variables. I've tried 2 different approaches that I thought would be equivalent, but which are actually giving me quite different results:</p>

<ol>
<li>Reading in the grouping as a (non-numeric) factor in R, growing a classification forest, and taking the proportion of trees that vote for group A as my prediction.</li>
<li>Constructing an indicator variable for membership of group A, growing a regression forest, and taking the ensemble prediction as usual.</li>
</ol>

<p>The split between the 2 groups is roughly 90-10 A vs. B. I'm growing 240 trees from ~200k observations of the same variables. I've left most of the settings at the defaults for the R randomForest package, but to keep the processing time down to a manageable level I've increased the node size to 200. The results are as follows:</p>

<ol>
<li>In the vast majority of cases, all 240 trees vote for A. The average predicted chance of any one observation being in A is about 99.9%. Worse still, not a single member of group B gets a majority of votes for group B!</li>
<li>I get a wide range of predictions, with the mean prediction lying close to the observed mean of ~90%.</li>
</ol>

<p>How can two apparently similar methods give such different results?</p>

<p>As for how I ended up trying this - I was initially trying to classify my dataset into a larger number of groups, of which B was one, but I noticed that B was being classified almost 100% incorrectly. The other groups are all much better behaved, even though most of them make up a far smaller proportion of my data.</p>
"
"0.0990957479752576","0.0995037190209989"," 60476","<p>I've run a regression on U.S. counties, and am checking for collinearity in my 'independent' variables.  Belsley, Kuh, and Welsch's <em>Regression Diagnostics</em> suggests looking at the Condition Index and Variance Decomposition Proportions:</p>

<pre><code>library(perturb)
## colldiag(, scale=TRUE) for model with interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct inc09_10k:unins09
1    1.000 0.000       0.000     0.000   0.000    0.001             0.002        0.003        0.002       0.002      0.001         0.000            
2    3.130 0.000       0.000     0.000   0.000    0.002             0.053        0.011        0.148       0.231      0.000         0.000            
3    3.305 0.000       0.000     0.000   0.000    0.000             0.095        0.072        0.351       0.003      0.000         0.000            
4    3.839 0.000       0.000     0.000   0.001    0.000             0.143        0.002        0.105       0.280      0.009         0.000            
5    5.547 0.000       0.002     0.000   0.000    0.050             0.093        0.592        0.084       0.005      0.002         0.000            
6    7.981 0.000       0.005     0.006   0.001    0.150             0.560        0.256        0.002       0.040      0.026         0.001            
7   11.170 0.000       0.009     0.003   0.000    0.046             0.000        0.018        0.003       0.250      0.272         0.035            
8   12.766 0.000       0.050     0.029   0.015    0.309             0.023        0.043        0.220       0.094      0.005         0.002            
9   18.800 0.009       0.017     0.003   0.209    0.001             0.002        0.001        0.047       0.006      0.430         0.041            
10  40.827 0.134       0.159     0.163   0.555    0.283             0.015        0.001        0.035       0.008      0.186         0.238            
11  76.709 0.855       0.759     0.796   0.219    0.157             0.013        0.002        0.004       0.080      0.069         0.683            

## colldiag(, scale=TRUE) for model without interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct
1    1.000 0.000       0.001     0.001   0.000    0.001             0.003        0.004        0.003       0.003      0.001        
2    2.988 0.000       0.000     0.001   0.000    0.002             0.030        0.003        0.216       0.253      0.000        
3    3.128 0.000       0.000     0.002   0.000    0.000             0.112        0.076        0.294       0.027      0.000        
4    3.630 0.000       0.002     0.001   0.001    0.000             0.160        0.003        0.105       0.248      0.009        
5    5.234 0.000       0.008     0.002   0.000    0.053             0.087        0.594        0.086       0.004      0.001        
6    7.556 0.000       0.024     0.039   0.001    0.143             0.557        0.275        0.002       0.025      0.035        
7   11.898 0.000       0.278     0.080   0.017    0.371             0.026        0.023        0.147       0.005      0.038        
8   13.242 0.000       0.001     0.343   0.006    0.000             0.000        0.017        0.129       0.328      0.553        
9   21.558 0.010       0.540     0.332   0.355    0.037             0.000        0.003        0.003       0.020      0.083        
10  50.506 0.989       0.148     0.199   0.620    0.393             0.026        0.004        0.016       0.087      0.279        
</code></pre>

<p><code>?HH::vif</code> suggests that VIFs >5 are problematic:</p>

<pre><code>library(HH)
## vif() for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         8.378646         16.329881          1.653584          2.744314          1.885095          1.471123          1.436229          1.789454 
    elderly09_pct inc09_10k:unins09 
         1.547234         11.590162 

## vif() for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.859426          2.378138          1.628817          2.716702          1.882828          1.471102          1.404482          1.772352 
    elderly09_pct 
         1.545867 
</code></pre>

<p>Whereas John Fox's <em>Regression Diagnostics</em> suggests looking at the square root of the VIF:</p>

<pre><code>library(car)
## sqrt(vif) for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         2.894589          4.041025          1.285917          1.656597          1.372987          1.212898          1.198428          1.337705 
    elderly09_pct inc09_10k:unins09 
         1.243879          3.404433 
## sqrt(vif) for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.363608          1.542121          1.276251          1.648242          1.372162          1.212890          1.185108          1.331297 
    elderly09_pct 
         1.243329 
</code></pre>

<p>In the first two cases (where a clear cutoff is suggested), the model is problematic only when the interaction term is included.</p>

<p>The model with the interaction term has until this point been my preferred specification.</p>

<p>I have two questions given this quirk of the data:</p>

<ol>
<li>Does an interaction term always worsen the collinearity of the data?</li>
<li>Since the two variables without the interaction term are not above the threshold, am I ok using the model with the interaction term.  Specifically, the reason I think this might be ok is that I'm using the King, Tomz, and Wittenberg (2000) method to interpret the coefficients (negative binomial model), where I generally hold the other coefficients at the mean, and then interpret what happens to predictions of my dependent variable when I move <code>inc09_10k</code> and <code>unins09</code> around independently and jointly.</li>
</ol>
"
"0.0495478739876288","0.0497518595104995"," 60817","<p>I am having trouble interpreting the z values for categorical variables in logistic regression. In the example below I have a categorical variable with 3 classes and according to the z value, CLASS2 might be relevant while the others are not. </p>

<p>But now what does this mean?</p>

<p>That I could merge the other classes to one? <br>
That the whole variable might not be a good predictor?</p>

<p>This is just an example and the actual z values here are not from a real problem, I just have difficulties about their interpretation.  </p>

<pre><code>           Estimate    Std. Error  z value Pr(&gt;|z|)    
CLASS0     6.069e-02  1.564e-01   0.388   0.6979    
CLASS1     1.734e-01  2.630e-01   0.659   0.5098    
CLASS2     1.597e+00  6.354e-01   2.514   0.0119 *  
</code></pre>
"
"0.100122674345859","0.107715935554017"," 60952","<p>I would like to compare models selected with ridge, lasso and elastic net. Fig. below shows coefficients paths using all 3 methods: ridge (Fig A, alpha=0), lasso (Fig B; alpha=1) and elastic net (Fig C; alpha=0.5). The optimal solution depends on the selected value of lambda, which is chosen based on cross validation.</p>

<p><img src=""http://i.stack.imgur.com/e8dUs.jpg"" alt=""Profiles of coefficients for ridge (A, alpha=0), lasso (B, alpha=1) and elastic net (C, alpha=0.5) regression. Numbers at the top of the plot represent the size of the models.The optimal solution depends on the selected value of lambda. Selection of lambda is based on cross validation. ""></p>

<p>When looking at these plots, I would expect the elastic net (Fig C) to exhibit a grouping effect. However it is not clear in the presented case. The coefficients path for lasso and elastic net are very similar. What could be the reason for this ? Is it just a coding mistake ? I used the following code in R: </p>

<pre><code>library(glmnet)
X&lt;- as.matrix(mydata[,2:22])
Y&lt;- mydata[,23]
par(mfrow=c(1,3))
ans1&lt;-cv.glmnet(X, Y, alpha=0) # ridge
plot(ans1$glmnet.fit, ""lambda"", label=FALSE)
    text (6, 0.4, ""A"", cex=1.8, font=1)
    ans2&lt;-cv.glmnet(X, Y, alpha=1) # lasso
    plot(ans2$glmnet.fit, ""lambda"", label=FALSE)
text (-0.8, 0.48, ""B"", cex=1.8, font=1)
ans3&lt;-cv.glmnet(X, Y, alpha=0.5) # elastic net 
plot(ans3$glmnet.fit, ""lambda"", label=FALSE)
text (0, 0.62, ""C"", cex=1.8, font=1)
</code></pre>

<p>The code used to plot elastic net coefficients paths is exactly the same as for ridge and lasso. The only difference is in the value of alpha. 
Alpha parameter for elastic net regression was selected based on the lowest MSE (mean squared error) for corresponding lambda values. </p>

<p>Thank you for your help !</p>
"
"0.0960200924600074","0.103831970547663"," 61138","<p>I am trying to calculate the marginal effects of a multinomial logistic regression. To do this I use the <code>mlogit</code> package and the <code>effects()</code> function.</p>

<p>Here is how the procedure works (source : <code>effects()</code> function of <code>mlogit</code> package) :</p>

<pre><code>data(""Fishing"", package = ""mlogit"")
Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = ""wide"", choice = ""mode"")
m &lt;- mlogit(mode ~ price | income | catch, data = Fish)
# compute a data.frame containing the mean value of the covariates in the sample
z &lt;- with(Fish, data.frame(price = tapply(price, index(m)$alt, mean), 
	catch = tapply(catch, index(m)$alt, mean), 
income = mean(income)))
# compute the marginal effects (the second one is an elasticity
effects(m, covariate = ""income"", data = z)
effects(m, covariate = ""price"", type = ""rr"", data = z)
effects(m, covariate = ""catch"", type = ""ar"", data = z)
</code></pre>

<p>I have no problem with first step (<code>mlogit.data()</code> function). I think my problem is in the specification of the multinomial regression.</p>

<p>My regression (for example with three variables) is on the form: <code>Y ~ 0 | X1 + X2 + X3</code>. When I try to estimate the marginal effects for a model with 2 variables, there is no problem, however for 3 variables R console returns me the following error: ""Error in if (rhs% in% c (1, 3)) {: argument is of length zero "" (translation from error in R console in french).</p>

<p>To understand what is my problem I tried to perform a multinomial regression of similar shape on the dataset ""Fishing"", i.e.,: <code>mode ~ 0 | income + price + catch</code> (even if this form has no ""economic"" sense.) Again the R console returns me the same error for 3 variables but manages to estimate these effects for a model with two variables.</p>

<p>This leads me to think that my problem really comes from the specification of my multinomial regression.  Do you know how I could find a solution to my problem? Or could you suggest another logit multinomial regression form ?</p>

<p>Thank you for your help :)</p>
"
"0.0286064783845312","0.0287242494810713"," 61144","<p>I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (<a href=""http://www.miraibio.com/blog/2010/08/the-4-parameter-logistic-4pl-nonlinear-regression-model/"" rel=""nofollow"">reference</a>) is often used for regression these data following this function:
$$
F(x) = \left(\frac{A-D}{1+(x/C)^B}\right) + D 
$$
How can I do this in <code>R</code>? I want to get the $A$, $B$, $C$ and $D$ values and plot the curve.</p>

<p>PS. If I have some data, how can I use the calculated function $F(x)$ to get the value? I mean how do I go from ""data -> F(x) -> value""?</p>
"
"0.0583927294833815","0.0703597544730292"," 61480","<p>I am trying to estimate a GAM regression model using the implementation of <code>gam</code> from the <code>mgcv</code> package. I have a working Gaussian model for the dispersion and a log link for the linear predictors but I receive the error </p>

<pre><code>&gt;""Error in eval(expr, envir, enclos) : cannot find valid starting values: please specify some"". 
</code></pre>

<p><strong>Edit 1</strong> - The exact syntax is </p>

<pre><code>splineWAR &lt;- gam(WAR ~ s(zAge, bs=""cr"") + s(zAdjProd, bs=""cr"") + s(zSOPct, bs=""cr"") + s(zBBPct, bs=""cr""), family=gaussian(link=""log""), data = mydata,  start=c(0, 0, 0, 0, 0))
</code></pre>

<p>I have read the relevant threads <a href=""http://stackoverflow.com/questions/13567169/glm-function-in-r-with-log-link-not-working"">here</a> and <a href=""http://stackoverflow.com/questions/8212063/r-glm-starting-values-not-accepted-log-link"">here</a> but have unable to apply the steps suggested to a multiple regression. For instance, when I try and set start values for the 5 variables in my regression (1 dependent and 4 independent) by adding the <code>start=c(n1, n2, n3, n4, n5)</code> argument (where the <code>n</code>'s are the mean of the relevant variable), I receive the same error even though I am seemingly copying the syntax exactly from the first link. Can anyone make a suggestion as to what I should try next? Thanks. </p>

<p><strong>Edit 2</strong> The code in the <code>gam.fit</code> function that runs right before the error is - </p>

<pre><code>if (!(validmu(mu) &amp;&amp; valideta(eta))) 

stop(""Can't find valid starting values: please specify some"")
</code></pre>
"
"0.0286064783845312","0.0287242494810713"," 61747","<p>I have a set of values $x$ and $y$ which are theoretically related exponentially:</p>

<p>$y = ax^b$</p>

<p>One way to obtain the coefficients is by applying natural logarithms in both sides and fitting a linear model:</p>

<pre><code>&gt; fit &lt;- lm(log(y)~log(x))
&gt; a &lt;- exp(fit$coefficients[1])
&gt; b &lt;- fit$coefficients[2]
</code></pre>

<p>Another way to obtain this is using a nonlinear regression, given a theoretical set of start values:</p>

<pre><code>&gt; fit &lt;- nls(y~a*x^b, start=c(a=50, b=1.3))
</code></pre>

<p>My tests show better and more theory-related results if I apply the second algorithm. However, I would like to know the statistical meaning and implications of each method.</p>

<p>Which of them is better?</p>
"
"0","0.0287242494810713"," 61845","<p>I am trying to run a Cox regression on a sample 2,000,000 row dataset as follows using only R. This is a direct translation of a PHREG in SAS. The sample is representative of the structure of the original dataset.</p>

<pre><code>##
library(survival)

### Replace 100000 by 2,000,000

test &lt;- data.frame(start=runif(100000,1,100), stop=runif(100000,101,300), censor=round(runif(100000,0,1)), testfactor=round(runif(100000,1,11)))

test$testfactorf &lt;- as.factor(test$testfactor)
summ &lt;- coxph(Surv(start,stop,censor) ~ relevel(testfactorf, 2), test)

# summary(summ)
##

user  system elapsed 
9.400   0.090   9.481 
</code></pre>

<p>The main challenge is in the compute time for the original dataset (2m rows). As far as I understand, in SAS this could take up to 1 day, ... but at least it finishes.</p>

<ul>
<li><p>Running the example with only 100,000 observations take only 9 seconds. Thereafter the time increases almost quadratically for every 100,000 increment in the number of observations.</p></li>
<li><p>I have not found any means to parallelize the operation (e.g., we can leverage a 48-core machine if this was possible)</p></li>
<li><p>Neither <code>biglm</code> nor any package from Revolution Analytics is available for Cox regression, and so I cannot leverage those.</p></li>
</ul>

<p><strong>Is there a means to represent this in terms of a logistic regression (for which there are packages in Revolution) or if there are any other alternatives to this problem?</strong> I know that they are fundamentally different, but it's the closest I can assume as a possibility given the circumstances. </p>
"
"0.107274293941992","0.10053487318375"," 61869","<p>I am trying to replicate a path analysis SEM model using Lavaan in R, and was very confused about the results that it gave regarding the model fit statistics. </p>

<p><strong>The code is as follows:</strong> </p>

<pre><code>#Import Package
library(lavaan)

#Input Correlation Matrix
sigma &lt;- matrix(c(1.00, -0.03,  0.39, -0.05, -0.08,
                 -0.03,  1.00,  0.07, -0.23, -0.16,
                  0.39,  0.07,  1.00, -0.13, -0.29,
                 -0.05, -0.23, -0.13,  1.00,  0.34,
                 -0.08, -0.16 ,-0.29,  0.34,  1.00), nr=5, byrow=TRUE)
rownames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")
colnames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")

#Create Covariance Matrix
sdevs &lt;-c(66.5, 3.8, 18.4, 6.7, 624.8)
covmax &lt;- cor2cov(sigma, sdevs)
as.matrix(covmax)

#Specify Model 
mymodel&lt;-'Illness ~ Exercise + Fitness
Illness ~ Hardiness + Stress
Fitness ~ Exercise + Hardiness 
Stress ~ Exercise + Hardiness + Fitness 
Exercise ~~ Exercise 
Hardiness ~~ Hardiness 
Exercise ~~ Hardiness'

#Fit the model with the covariance matrix
N = 363
fit.path &lt;-sem(mymodel,sample.cov=covmax, sample.nobs=N, fixed.x=FALSE)

#Summary of the model fit
summary(fit.path, fit.measures = TRUE)
</code></pre>

<p><strong>And the output I get is as follows:</strong> </p>

<pre><code> lavaan (0.5-12) converged normally after  93 iterations

 Number of observations                         37300

 Estimator                                         ML
 Minimum Function Test Statistic                0.000
 Degrees of freedom                                 0
 P-value (Chi-square)                           1.000

 Model test baseline model:

 Minimum Function Test Statistic            16594.387
 Degrees of freedom                                10
 P-value                                        0.000

 Full model versus baseline model:

 Comparative Fit Index (CFI)                    1.000
 Tucker-Lewis Index (TLI)                       1.000

 Loglikelihood and Information Criteria:

 Loglikelihood user model (H0)             -882379.005
 Loglikelihood unrestricted model (H1)     -882379.005

 Number of free parameters                         15
 Akaike (AIC)                              1764788.009
 Bayesian (BIC)                            1764915.910
 Sample-size adjusted Bayesian (BIC)       1764868.240

 Root Mean Square Error of Approximation:

 RMSEA                                          0.000
 90 Percent Confidence Interval          0.000  0.000
 P-value RMSEA &lt;= 0.05                          1.000

 Standardized Root Mean Square Residual:

 SRMR                                           0.000

 Parameter estimates:

 Information                                 Expected
 Standard Errors                             Standard

                Estimate  Std.err  Z-value  P(&gt;|z|)
 Regressions:
 Illness ~
 Exercise          0.318    0.048    6.640    0.000
 Fitness          -8.835    0.174  -50.737    0.000
 Hardiness       -12.146    0.793  -15.321    0.000
 Stress           27.125    0.451   60.079    0.000
 Fitness ~
 Exercise          0.109    0.001   82.602    0.000
 Hardiness         0.396    0.023   17.211    0.000
 Stress ~
 Exercise         -0.001    0.001   -2.614    0.009
 Hardiness        -0.393    0.009  -44.332    0.000
 Fitness          -0.040    0.002  -19.953    0.000

 Covariances:
 Exercise ~~
 Hardiness        -7.581    1.309   -5.791    0.000

 Variances:
 Exercise       4422.131   32.381
 Hardiness        14.440    0.106
 Illness       318744.406 2334.012
 Fitness         284.796    2.085
 Stress           41.921    0.307
</code></pre>

<p><strong>These are my questions:</strong>  </p>

<ul>
<li>Why does the chi-squared say that there are no degrees of freedom? </li>
<li>Why are the p-values exactly 1? Why is the CFI and TLI exactly 1? </li>
<li><p>Why is the RMSEA 0?</p></li>
<li><p>What would I need to do to simulate a more realistic model that doesn't appear artificially ""perfect""? </p></li>
<li>Does it have to do with the model specification? </li>
</ul>
"
"0.118129972176712","0.112026511167878"," 62000","<p>I've tried to create three models (using R): an intercept only linear regression, a simple mixed effects regression and a by-subject effects mixed effects regression.</p>

<p>An intercept only regression models the grand mean of a response variable plus error. In <code>mtcars</code>, the variable <code>drat</code> may be considered a response variable. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus error?</p>

<pre><code>interceptOnly &lt;- lm(drat ~ 1, data=mtcars)
</code></pre>

<p>A simple mixed effects regression models the grand mean of a response variable, plus subject deviation, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable and <code>cyl</code> a subject deviation. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus error?</p>

<pre><code>library(lme4)
simpleMixedEffects &lt;- lmer(drat ~ (1|cyl), data=mtcars)
</code></pre>

<p>A by-subject effects mixed effects regression models the grand mean of a response variable, plus subject deviation, plus condition effect, plus error. In <code>mtcars</code>, <code>drat</code> may be considered a response variable, <code>cyl</code> a subject deviation and <code>wt</code> a condition effect. In the model below, have I correctly modelled the grand mean of <code>drat</code>, plus the deviation of <code>cyl</code> from <code>drat</code>, plus the effect of <code>wt</code>, plus error?</p>

<pre><code>bySubjectMixedEffects &lt;- lmer(drat ~ (1|cyl) + wt, data=mtcars)
</code></pre>

<p>I have one further question: </p>

<p>How can I model a by-subject varying condition effect model. This is a mixed effects model which models the grand mean of a response variable, plus group deviation from grand mean (random effect), plus condition effect (fixed effect), plus group deviation from condition effect (random effect), plus error. Could someone provide R code that outputs a ""by-subject varying condition effect model""?</p>
"
"0.0495478739876288","0.0497518595104995"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.110792414376932","0.111248539872496"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0762839423587497","0.0861727484432139"," 62829","<p>I'm trying to calculate partitioned sum of squares in a linear regression. In the first model, there are two predictors. In the second model, one of these predictors in removed. In the model with two predictors versus the model with one predictor, I have calculated the difference in regression sum of squares to be 2.72 - is this correct? If this is correct, why is the difference in sum of squares 2.72 (quite small) when the difference in r-squared is ~30% (quite large).</p>

<pre><code># model two predictors
mod &lt;- lm(drat ~ hp + wt, mtcars)

# regression sum of squares two predictors
regressionSumSquares &lt;- sum((predict(mod)-mean(mtcars$drat))*(predict(mod)-mean(mtcars$drat)))

# residuals two predictors
residualSumSquares &lt;- sum((predict(mod)-mtcars$drat)*(predict(mod)-mtcars$drat))

# r-squared two predictors
totalSumSquares &lt;- regressionSumSquares + residualSumSquares
rSquared &lt;- regressionSumSquares/totalSumSquares



# model one predictor 
modJustHp &lt;- lm(drat ~ hp, mtcars)

# regression sum of squares one predictor
regressionSumSquaresJustHp &lt;- sum((predict(modJustHp)-mean(mtcars$drat))*(predict(modJustHp)-mean(mtcars$drat)))

# residual sum of squares one predictor
residualSumSquaresJustHp &lt;- sum((predict(modJustHp)-mtcars$drat)*(predict(modJustHp)-mtcars$drat))

# r-squared one predictor
totalSumSquaresJustHp &lt;- regressionSumSquaresJustHp + residualSumSquaresJustHp
rSquaredJustHp &lt;- regressionSumSquaresJustHp/totalSumSquaresJustHp

# difference in sum of squares one vs two predictors
regressionSumSquares - regressionSumSquaresJustHp
</code></pre>
"
"0.114624397492221","0.108325928493652"," 62852","<p>I have implemented a Gibbs Sampler for the <strong>Bayesian Elastic Net</strong> (BEN) according to this paper on <a href=""http://www.stat.ufl.edu/~casella/Papers/BL-Final.pdf"" rel=""nofollow"">Penalized Regression by Kyung et al.</a><br>
In this paper, they execute a simulation study that has been used in other papers on Penalized Regression (LASSO, Bridge, Ridge) to compare the performance of the proposed models.</p>

<p>Here are details of the simulation taken from the above mentioned paper:  </p>

<blockquote>
  <p>We simulate data from the true model
  $$
y=X\beta+\sigma\epsilon \quad\epsilon_i\,{\raise.17ex\hbox{$\scriptstyle\sim$}}\,\text{iid}\,N(0,1)
$$
  We simulate data sets with $n=20$ to fit models and $n=200$ to compare prediction errors of proposed models with eight predictors. We let $\beta=(3,1.5,0,0,2,0,0,0)$ and $\sigma=3$. The pairwise correlation between $x_i$ and $x_j$ was set to be $corr(i,j)=0.5^{|i-j|}$.<br>
  Later on they say, that for the prediction error, they calculate the average mean squared error based on 50 replications. By average they mean the median in this case.</p>
</blockquote>

<p>To simulate this data and calculate the MSE I've used following code in R: </p>

<pre><code># Number of observations
n.train &lt;- 20
n.test  &lt;- 200
# Error variance
sigma &lt;- 3
# Pairwise correlation of X
cor &lt;- 0.5
# Number of predictors
p &lt;- 8
# Create training and test data set (package QRM and mvtnorm required)
Z &lt;- equicorr(p, rho=cor)
X.train &lt;- rmvnorm(n.train,sigma=Z)
X.test  &lt;- rmvnorm(n.test,sigma=Z)
# Create error 
error.train &lt;- rnorm(n.train,mean=0,sd=1)
error.test  &lt;- rnorm(n.test,mean=0,sd=1)
# Create beta
beta.true &lt;- c(3,1.5,0,0,2,0,0,0)
# Create both responses
Y.train &lt;- X.train %*% beta.true + sigma*error.train
Y.test  &lt;- X.test %*% beta.true + sigma*error.test

# Fit the training data set with the BEN Gibbs Sampler
beta.ben &lt;- BEN(X.train,Y.train, iter=11000, burn = 1000)
# Calculate the predicted response
Y.pred   &lt;- X.test %*% beta.ben
# Calculate the mean squared error (MSE)
MSE      &lt;- sum((Y.train - Y.pred)^2)/n.train
</code></pre>

<p>My problem is that my results are not even close to comparable to the ones in the paper which makes me doubt my simulation study ""setup"".<br>
As one of the authors of the paper has uploaded the Gibbs Sampler code and I could check if I did something wrong, I know that the problem doesn't lie there.</p>

<p>So my questions are:</p>

<ol>
<li>Does anybody have experience with this kind of simulation study and can check if I did something wrong?</li>
<li>Is the MSE I calculate the same as the one used in the paper? In researching on this topic I found many different ways to calculate the MSE and it was also sometimes used but actually the mean squared prediction error was meant. For example the Wikipedia article on MSE alone lists three variations.</li>
</ol>

<p>I don't need help with coding, rather more information on how this simulation is typically excecuted so I can figure out what I'm doing wrong.</p>
"
"0.0952081150392732","0.103566754353222"," 63039","<p>I've set up a Bayesian regression model in WinBUGS to determine values for the unknown parameters (b1, b2, b3, b4) and intercept value (b0) in a linear regression model. The code is as follows:</p>

<pre><code>model {
for (i in 1:(J-1)) {
  FC[i]       ~ dnorm(mu[i], tau)
  mu[i] &lt;- b0 + b1*(Factor_b1[i]-mean_Factor_b1) + b2*(Factor_b2[i]-mean_Factor_b2) + b3*(Factor_b3[i]-mean_Factor_b3) + b4*(Factor_b4[i]-mean_Factor_b4) 
}

b0        ~ dflat()
b1         ~ dflat()
b2         ~ dflat()
b3         ~ dflat()
b4         ~ dflat()
tau         &lt;- 1/sigma2
log(sigma2) &lt;- 2*log.sigma
log.sigma    ~ dflat()
}

Inits:
list(b0 =0,b1 = 0, b2 =0, b3 = 0, b4 =0, log.sigma=0)

Data1

list(J = 20, FC = c(1.87315166256848, 1.87315166256848, 
1.87315166256848, 1.8708501655802, 1.8708501655802, 1.8708501655802, 
1.93248104062608, 1.93248104062608, 1.93248104062608, 1.93248104062608, 
1.80846914258265, 1.80846914258265, 1.80846914258265, 2.10555453929548, 
2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 
2.12908503670568, 2.12908503670568), Factor_b1 = c(7.0057890192535, 
7.0057890192535, 7.0057890192535, 7.05012252026906, 7.05012252026906, 
7.05012252026906, 7.13329595489607, 7.13329595489607, 7.13329595489607, 
7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 
7.11720550316434, 7.11720550316434, 7.11720550316434, 7.11720550316434, 
7.11720550316434, 7.14124512235049, 7.14124512235049), mean_Factor_b1 = 7.09846620316814, 
Factor_b2 = c(7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.37650812632622, 7.37650812632622, 
7.37650812632622, 7.37650812632622, 7.37650812632622, 7.46565531013406, 
7.46565531013406), mean_Factor_b2 = 7.28441087641358, Factor_b3 = c(2.37954613413017, 
2.37954613413017, 2.37954613413017, 2.28238238567653, 2.28238238567653, 
2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 
2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 
2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 
2.33214389523559, 2.33214389523559, 2.33214389523559), mean_Factor_b3 = 2.31437347629025, 
Factor_b4 = c(2.06686275947298, 2.06686275947298, 2.06686275947298, 
2.09186406167839, 2.09186406167839, 2.09186406167839, 2.10413415427021, 
2.10413415427021, 2.10413415427021, 2.10413415427021, 2.06686275947298, 
2.06686275947298, 2.06686275947298, 2.32238772029023, 2.32238772029023, 
2.32238772029023, 2.32238772029023, 2.32238772029023, 2.2082744135228, 
2.2082744135228), mean_Factor_b4 = 2.15608963937253)
</code></pre>

<p>This WinBUGS code returns the following outputs for the unknown intercept (b0) and parameter (b1, b2, b3, b4) values:</p>

<pre><code>     node    mean    sd  MC error   2.5%    median  97.5%   start   sample
b0  1.957   0.009337    3.764E-5    1.939   1.957   1.976   1001    56000
b1  0.1068  0.3296  0.001438    -0.5529 0.1072  0.7615  1001    56000
b2  0.5977  0.2758  0.001068    0.05286 0.5967  1.147   1001    56000
b3  0.1892  0.4394  0.001825    -0.6871 0.1899  1.061   1001    56000
b4  0.5757  0.1886  7.423E-4    0.1986  0.5765  0.9472  1001    56000
</code></pre>

<p>MY PROBLEM: When I compare these Bayesian estimates with results from a linear MLE regression in R, I seem to be getting a different result for the intercept value (b0). The code for the R linear regression is as follows:</p>

<pre><code>FC = c(1.87315166256848, 1.87315166256848, 1.87315166256848, 1.8708501655802, 1.8708501655802, 1.8708501655802, 1.93248104062608, 1.93248104062608, 1.93248104062608, 1.93248104062608, 1.80846914258265, 1.80846914258265, 1.80846914258265, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.12908503670568, 2.12908503670568)

b1 = c(7.0057890192535, 7.0057890192535, 7.0057890192535, 7.05012252026906, 7.05012252026906, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.11720550316434, 7.11720550316434, 7.11720550316434, 7.11720550316434,  7.11720550316434, 7.14124512235049, 7.14124512235049) 

b2 = c(7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.46565531013406, 7.46565531013406)

b3 = c(2.37954613413017, 2.37954613413017, 2.37954613413017, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559) 

b4 = c(2.06686275947298, 2.06686275947298, 2.06686275947298, 2.09186406167839,       2.09186406167839, 2.09186406167839, 2.10413415427021, 2.10413415427021, 2.10413415427021, 2.10413415427021, 2.06686275947298, 2.06686275947298, 2.06686275947298, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.2082744135228, 2.2082744135228)

# ======================= Linear Model =======================


lmfit_Linear_Model_Test =lm(FC ~ (b1 + b2 + b3 + b4))

print (summary(lmfit_Linear_Model_Test))
</code></pre>

<p>And the results from this MLE regressions are as follows:</p>

<pre><code>Call:
lm(formula = FC ~ (b1 + b2 + b3 + b4))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.05837 -0.00823 -0.00044  0.01307  0.04593 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   -5.247      2.305   -2.28   0.0379 * 
b1             0.105      0.298    0.35   0.7280   
b2             0.674      0.195    3.45   0.0035 **
b3             0.177      0.394    0.45   0.6599   
b4             0.529      0.141    3.75   0.0019 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.0359 on 15 degrees of freedom
Multiple R-squared: 0.931,  Adjusted R-squared: 0.913 
F-statistic: 50.8 on 4 and 15 DF,  p-value: 0.0000000153 
</code></pre>

<p>SUMMARY: Why is the intercept value (b0) coming to -5.247 with the MLE model and 1.957 with the Bayesian model? Should they not be the same?</p>
"
"0.131226987728912","0.131767241095046"," 63233","<h2>Background</h2>

<p>In a paper from Epstein (1991): <a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%281991%29004%3C0365%3AOODCVF%3E2.0.CO%3B2"" rel=""nofollow"">On obtaining daily climatological values from monthly means</a>, the formulation and an algorithm for calculating Fourier interpolation for periodical and even-spaced values are given.</p>

<p>In the paper, the goal is to <strong>obtain daily values from monthly means</strong> by interpolation.</p>

<p>In short, it is assumed that unknown daily values can be represented by the sum of harmonic components:
$$
y(t) = a_{0} + \sum_{j}\left[a_{j}\,\cos(2\pi jt/12)+b_{j}\,\sin(2\pi jt/12)\right]
$$
In the paper $t$ (time) is expressed in months.</p>

<p>After some derviation, it is shown that the terms can be calculated by:
$$
\begin{align}
a_{0} &amp;= \sum_{T}Y_{T}/12 \\
a_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\cos(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
b_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\sin(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
a_{6} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right]\times \sum_{T}\left[Y_{T}\cos(\pi T)/12\right] \\
b_{6} &amp;= 0
\end{align}
$$
Where $Y_{T}$ denote the monthly means and $T$ the month.</p>

<p><a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281995%29123%3C2251%3ATIODSU%3E2.0.CO%3B2"" rel=""nofollow"">Harzallah (1995)</a> summarizes this aproach as follows: ""The interpolation is carried out by adding zeros to the spectral coefficients of data and by performing an inverse Fourier transform to the resulting extended coefficients. The method is equivalent to applying a rectangular filter to Fourier coefficients.""</p>

<hr>

<h2>Questions</h2>

<p>My goal is to use the above methodology for interpolation of <strong>weekly means to obtain daily data</strong> (see <a href=""http://stats.stackexchange.com/questions/59418/interpolation-of-influenza-data-that-conserves-weekly-mean/63135#63135"">my previous question</a>). In summary, I have 835 weekly means of count data (see the example dataset at the bottom of the question). There are quite a few things that I don't understand before I can apply the approach outlined above:</p>

<ol>
<li>How would the formulas have to be changed for my situation (weekly instead of monthly values)?</li>
<li>How could the time $t$ be expressed? I assumed $t/835$ (or $t/n$ with $n$ data points in general), is that correct?</li>
<li>Why does the author calculate 7 terms (i.e. $0\leq j \leq 6$)? How many terms would I have to consider?</li>
<li>I understand that the question can probably be solved by using a <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> and using the predictions for interpolation (thanks to Nick). Still, some things are unclear to me: How many terms of harmonics should be included in the regression? And what period should I take? How can the regression be done to ensure that the weekly means are preserved (as I don't want an exact harmonic fit to the data)?</li>
</ol>

<p>Using the <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> (which is also explained in <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0116"" rel=""nofollow"">this paper</a>), I managed to get an exact harmonic fit to the data (the $j$ in my example would run through $1, \ldots, 417$, so I fitted 417 terms). <strong>How can this approach be modified -$~$if possible$~$- to achieve the conservation of the weekly means?</strong> Maybe by applying correction factors to each regression term?</p>

<p>The plot of the exact harmonic fit is:</p>

<p><img src=""http://i.stack.imgur.com/7XuxU.png"" alt=""Exact harmonic fit""></p>

<p><strong>EDIT</strong></p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/signal/"" rel=""nofollow"">signal package</a> and the <code>interp1</code> function, here's what I've managed to do using the example data set from below (many thanks to @noumenal). I use <code>q=7</code> as we have weekly data:</p>

<pre><code># Set up the time scale

daily.ts &lt;- seq(from=as.Date(""1995-01-01""), to=as.Date(""2010-12-31""), by=""day"")

# Set up data frame 

ts.frame &lt;- data.frame(daily.ts=daily.ts, wdayno=as.POSIXlt(daily.ts)$wday,
                       yearday = 1:5844,
                       no.influ.cases=NA)

# Add the data from the example dataset called ""my.dat""

ts.frame$no.influ.cases[ts.frame$wdayno==3] &lt;- my.dat$case

# Interpolation

case.interp1 &lt;- interp1(x=ts.frame$yearday[!is.na(ts.frame$no.influ.case)],y=(ts.frame$no.influ.cases[!is.na(ts.frame$no.influ.case)]),xi=ts.frame$yearday, method = c(""cubic""))

# Plot subset for better interpretation
par(bg=""white"", cex=1.2, las=1)
plot((ts.frame$no.influ.cases)~ts.frame$yearday, pch=20,
     col=grey(0.4),
     cex=1, las=1,xlim=c(0,400), xlab=""Day"", ylab=""Influenza cases"")
lines(case.interp1, col=""steelblue"", lwd=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R1FE8.png"" alt=""Cubicinterpo""></p>

<p>There are two issues here:</p>

<ol>
<li>The curve seem to fit ""too good"": it goes through every point </li>
<li>The weekly means are not conserved</li>
</ol>

<p><strong>Example dataset</strong></p>

<pre><code>structure(list(date = structure(c(9134, 9141, 9148, 9155, 9162, 
9169, 9176, 9183, 9190, 9197, 9204, 9211, 9218, 9225, 9232, 9239, 
9246, 9253, 9260, 9267, 9274, 9281, 9288, 9295, 9302, 9309, 9316, 
9323, 9330, 9337, 9344, 9351, 9358, 9365, 9372, 9379, 9386, 9393, 
9400, 9407, 9414, 9421, 9428, 9435, 9442, 9449, 9456, 9463, 9470, 
9477, 9484, 9491, 9498, 9505, 9512, 9519, 9526, 9533, 9540, 9547, 
9554, 9561, 9568, 9575, 9582, 9589, 9596, 9603, 9610, 9617, 9624, 
9631, 9638, 9645, 9652, 9659, 9666, 9673, 9680, 9687, 9694, 9701, 
9708, 9715, 9722, 9729, 9736, 9743, 9750, 9757, 9764, 9771, 9778, 
9785, 9792, 9799, 9806, 9813, 9820, 9827, 9834, 9841, 9848, 9855, 
9862, 9869, 9876, 9883, 9890, 9897, 9904, 9911, 9918, 9925, 9932, 
9939, 9946, 9953, 9960, 9967, 9974, 9981, 9988, 9995, 10002, 
10009, 10016, 10023, 10030, 10037, 10044, 10051, 10058, 10065, 
10072, 10079, 10086, 10093, 10100, 10107, 10114, 10121, 10128, 
10135, 10142, 10149, 10156, 10163, 10170, 10177, 10184, 10191, 
10198, 10205, 10212, 10219, 10226, 10233, 10240, 10247, 10254, 
10261, 10268, 10275, 10282, 10289, 10296, 10303, 10310, 10317, 
10324, 10331, 10338, 10345, 10352, 10359, 10366, 10373, 10380, 
10387, 10394, 10401, 10408, 10415, 10422, 10429, 10436, 10443, 
10450, 10457, 10464, 10471, 10478, 10485, 10492, 10499, 10506, 
10513, 10520, 10527, 10534, 10541, 10548, 10555, 10562, 10569, 
10576, 10583, 10590, 10597, 10604, 10611, 10618, 10625, 10632, 
10639, 10646, 10653, 10660, 10667, 10674, 10681, 10688, 10695, 
10702, 10709, 10716, 10723, 10730, 10737, 10744, 10751, 10758, 
10765, 10772, 10779, 10786, 10793, 10800, 10807, 10814, 10821, 
10828, 10835, 10842, 10849, 10856, 10863, 10870, 10877, 10884, 
10891, 10898, 10905, 10912, 10919, 10926, 10933, 10940, 10947, 
10954, 10961, 10968, 10975, 10982, 10989, 10996, 11003, 11010, 
11017, 11024, 11031, 11038, 11045, 11052, 11059, 11066, 11073, 
11080, 11087, 11094, 11101, 11108, 11115, 11122, 11129, 11136, 
11143, 11150, 11157, 11164, 11171, 11178, 11185, 11192, 11199, 
11206, 11213, 11220, 11227, 11234, 11241, 11248, 11255, 11262, 
11269, 11276, 11283, 11290, 11297, 11304, 11311, 11318, 11325, 
11332, 11339, 11346, 11353, 11360, 11367, 11374, 11381, 11388, 
11395, 11402, 11409, 11416, 11423, 11430, 11437, 11444, 11451, 
11458, 11465, 11472, 11479, 11486, 11493, 11500, 11507, 11514, 
11521, 11528, 11535, 11542, 11549, 11556, 11563, 11570, 11577, 
11584, 11591, 11598, 11605, 11612, 11619, 11626, 11633, 11640, 
11647, 11654, 11661, 11668, 11675, 11682, 11689, 11696, 11703, 
11710, 11717, 11724, 11731, 11738, 11745, 11752, 11759, 11766, 
11773, 11780, 11787, 11794, 11801, 11808, 11815, 11822, 11829, 
11836, 11843, 11850, 11857, 11864, 11871, 11878, 11885, 11892, 
11899, 11906, 11913, 11920, 11927, 11934, 11941, 11948, 11955, 
11962, 11969, 11976, 11983, 11990, 11997, 12004, 12011, 12018, 
12025, 12032, 12039, 12046, 12053, 12060, 12067, 12074, 12081, 
12088, 12095, 12102, 12109, 12116, 12123, 12130, 12137, 12144, 
12151, 12158, 12165, 12172, 12179, 12186, 12193, 12200, 12207, 
12214, 12221, 12228, 12235, 12242, 12249, 12256, 12263, 12270, 
12277, 12284, 12291, 12298, 12305, 12312, 12319, 12326, 12333, 
12340, 12347, 12354, 12361, 12368, 12375, 12382, 12389, 12396, 
12403, 12410, 12417, 12424, 12431, 12438, 12445, 12452, 12459, 
12466, 12473, 12480, 12487, 12494, 12501, 12508, 12515, 12522, 
12529, 12536, 12543, 12550, 12557, 12564, 12571, 12578, 12585, 
12592, 12599, 12606, 12613, 12620, 12627, 12634, 12641, 12648, 
12655, 12662, 12669, 12676, 12683, 12690, 12697, 12704, 12711, 
12718, 12725, 12732, 12739, 12746, 12753, 12760, 12767, 12774, 
12781, 12788, 12795, 12802, 12809, 12816, 12823, 12830, 12837, 
12844, 12851, 12858, 12865, 12872, 12879, 12886, 12893, 12900, 
12907, 12914, 12921, 12928, 12935, 12942, 12949, 12956, 12963, 
12970, 12977, 12984, 12991, 12998, 13005, 13012, 13019, 13026, 
13033, 13040, 13047, 13054, 13061, 13068, 13075, 13082, 13089, 
13096, 13103, 13110, 13117, 13124, 13131, 13138, 13145, 13152, 
13159, 13166, 13173, 13180, 13187, 13194, 13201, 13208, 13215, 
13222, 13229, 13236, 13243, 13250, 13257, 13264, 13271, 13278, 
13285, 13292, 13299, 13306, 13313, 13320, 13327, 13334, 13341, 
13348, 13355, 13362, 13369, 13376, 13383, 13390, 13397, 13404, 
13411, 13418, 13425, 13432, 13439, 13446, 13453, 13460, 13467, 
13474, 13481, 13488, 13495, 13502, 13509, 13516, 13523, 13530, 
13537, 13544, 13551, 13558, 13565, 13572, 13579, 13586, 13593, 
13600, 13607, 13614, 13621, 13628, 13635, 13642, 13649, 13656, 
13663, 13670, 13677, 13684, 13691, 13698, 13705, 13712, 13719, 
13726, 13733, 13740, 13747, 13754, 13761, 13768, 13775, 13782, 
13789, 13796, 13803, 13810, 13817, 13824, 13831, 13838, 13845, 
13852, 13859, 13866, 13873, 13880, 13887, 13894, 13901, 13908, 
13915, 13922, 13929, 13936, 13943, 13950, 13957, 13964, 13971, 
13978, 13985, 13992, 13999, 14006, 14013, 14020, 14027, 14034, 
14041, 14048, 14055, 14062, 14069, 14076, 14083, 14090, 14097, 
14104, 14111, 14118, 14125, 14132, 14139, 14146, 14153, 14160, 
14167, 14174, 14181, 14188, 14195, 14202, 14209, 14216, 14223, 
14230, 14237, 14244, 14251, 14258, 14265, 14272, 14279, 14286, 
14293, 14300, 14307, 14314, 14321, 14328, 14335, 14342, 14349, 
14356, 14363, 14370, 14377, 14384, 14391, 14398, 14405, 14412, 
14419, 14426, 14433, 14440, 14447, 14454, 14461, 14468, 14475, 
14482, 14489, 14496, 14503, 14510, 14517, 14524, 14531, 14538, 
14545, 14552, 14559, 14566, 14573, 14580, 14587, 14594, 14601, 
14608, 14615, 14622, 14629, 14636, 14643, 14650, 14657, 14664, 
14671, 14678, 14685, 14692, 14699, 14706, 14713, 14720, 14727, 
14734, 14741, 14748, 14755, 14762, 14769, 14776, 14783, 14790, 
14797, 14804, 14811, 14818, 14825, 14832, 14839, 14846, 14853, 
14860, 14867, 14874, 14881, 14888, 14895, 14902, 14909, 14916, 
14923, 14930, 14937, 14944, 14951, 14958, 14965, 14972), class = ""Date""), 
    cases = c(168L, 199L, 214L, 230L, 267L, 373L, 387L, 443L, 
    579L, 821L, 1229L, 1014L, 831L, 648L, 257L, 203L, 137L, 78L, 
    82L, 69L, 45L, 51L, 45L, 63L, 55L, 54L, 52L, 27L, 24L, 12L, 
    10L, 22L, 42L, 32L, 52L, 82L, 95L, 91L, 104L, 143L, 114L, 
    100L, 83L, 113L, 145L, 175L, 222L, 258L, 384L, 755L, 976L, 
    879L, 846L, 1004L, 801L, 799L, 680L, 530L, 410L, 302L, 288L, 
    234L, 269L, 245L, 240L, 176L, 188L, 128L, 96L, 59L, 63L, 
    44L, 52L, 39L, 50L, 36L, 40L, 48L, 32L, 39L, 28L, 29L, 16L, 
    20L, 25L, 25L, 48L, 57L, 76L, 117L, 107L, 91L, 90L, 83L, 
    76L, 86L, 104L, 101L, 116L, 120L, 185L, 290L, 537L, 485L, 
    561L, 1142L, 1213L, 1235L, 1085L, 1052L, 987L, 918L, 746L, 
    620L, 396L, 280L, 214L, 148L, 148L, 94L, 107L, 69L, 55L, 
    69L, 47L, 43L, 49L, 30L, 42L, 51L, 41L, 39L, 40L, 38L, 22L, 
    37L, 26L, 40L, 56L, 54L, 74L, 99L, 114L, 114L, 120L, 114L, 
    123L, 131L, 170L, 147L, 163L, 163L, 160L, 158L, 163L, 124L, 
    115L, 176L, 171L, 214L, 320L, 507L, 902L, 1190L, 1272L, 1282L, 
    1146L, 896L, 597L, 434L, 216L, 141L, 101L, 86L, 65L, 55L, 
    35L, 49L, 29L, 55L, 53L, 57L, 34L, 43L, 42L, 13L, 17L, 20L, 
    27L, 36L, 47L, 64L, 77L, 82L, 82L, 95L, 107L, 96L, 106L, 
    93L, 114L, 102L, 116L, 128L, 123L, 212L, 203L, 165L, 267L, 
    550L, 761L, 998L, 1308L, 1613L, 1704L, 1669L, 1296L, 975L, 
    600L, 337L, 259L, 145L, 91L, 70L, 79L, 63L, 58L, 51L, 53L, 
    39L, 49L, 33L, 47L, 56L, 32L, 43L, 47L, 19L, 32L, 18L, 34L, 
    39L, 63L, 57L, 55L, 69L, 76L, 103L, 99L, 108L, 131L, 113L, 
    106L, 122L, 138L, 136L, 175L, 207L, 324L, 499L, 985L, 1674L, 
    1753L, 1419L, 1105L, 821L, 466L, 274L, 180L, 143L, 82L, 101L, 
    72L, 55L, 71L, 50L, 33L, 26L, 25L, 27L, 21L, 24L, 24L, 20L, 
    18L, 18L, 25L, 23L, 13L, 10L, 16L, 9L, 12L, 16L, 25L, 31L, 
    36L, 40L, 36L, 47L, 32L, 46L, 75L, 63L, 49L, 90L, 83L, 101L, 
    78L, 79L, 98L, 131L, 83L, 122L, 179L, 334L, 544L, 656L, 718L, 
    570L, 323L, 220L, 194L, 125L, 95L, 77L, 46L, 42L, 29L, 35L, 
    21L, 29L, 16L, 14L, 19L, 15L, 19L, 18L, 21L, 10L, 14L, 7L, 
    7L, 5L, 9L, 14L, 11L, 18L, 22L, 39L, 36L, 46L, 44L, 37L, 
    30L, 39L, 37L, 45L, 71L, 59L, 57L, 80L, 68L, 88L, 72L, 74L, 
    208L, 357L, 621L, 839L, 964L, 835L, 735L, 651L, 400L, 292L, 
    198L, 85L, 64L, 41L, 40L, 23L, 18L, 14L, 22L, 9L, 19L, 8L, 
    14L, 12L, 15L, 14L, 4L, 6L, 7L, 7L, 8L, 13L, 10L, 19L, 17L, 
    20L, 22L, 40L, 37L, 45L, 34L, 26L, 35L, 67L, 49L, 77L, 82L, 
    80L, 104L, 88L, 49L, 73L, 113L, 142L, 152L, 206L, 293L, 513L, 
    657L, 919L, 930L, 793L, 603L, 323L, 202L, 112L, 55L, 31L, 
    27L, 15L, 15L, 6L, 13L, 21L, 10L, 11L, 9L, 8L, 11L, 7L, 5L, 
    1L, 4L, 7L, 2L, 6L, 12L, 14L, 21L, 29L, 32L, 26L, 22L, 44L, 
    39L, 47L, 44L, 93L, 145L, 289L, 456L, 685L, 548L, 687L, 773L, 
    575L, 355L, 248L, 179L, 129L, 122L, 103L, 72L, 72L, 36L, 
    26L, 31L, 12L, 14L, 14L, 14L, 7L, 8L, 2L, 7L, 8L, 9L, 26L, 
    10L, 13L, 13L, 5L, 5L, 3L, 6L, 1L, 10L, 6L, 7L, 17L, 12L, 
    21L, 32L, 29L, 18L, 22L, 24L, 38L, 52L, 53L, 73L, 49L, 52L, 
    70L, 77L, 95L, 135L, 163L, 303L, 473L, 823L, 1126L, 1052L, 
    794L, 459L, 314L, 252L, 111L, 55L, 35L, 14L, 30L, 21L, 16L, 
    9L, 11L, 6L, 6L, 8L, 9L, 9L, 10L, 15L, 15L, 11L, 6L, 3L, 
    8L, 4L, 7L, 7L, 13L, 10L, 23L, 24L, 36L, 25L, 34L, 37L, 46L, 
    39L, 37L, 55L, 65L, 54L, 60L, 82L, 55L, 53L, 61L, 52L, 75L, 
    92L, 121L, 170L, 199L, 231L, 259L, 331L, 357L, 262L, 154L, 
    77L, 34L, 41L, 21L, 17L, 16L, 7L, 15L, 11L, 7L, 5L, 6L, 13L, 
    7L, 6L, 8L, 7L, 1L, 11L, 9L, 3L, 9L, 9L, 8L, 15L, 19L, 16L, 
    10L, 12L, 26L, 35L, 35L, 41L, 34L, 30L, 36L, 43L, 23L, 55L, 
    107L, 141L, 217L, 381L, 736L, 782L, 663L, 398L, 182L, 137L, 
    79L, 28L, 26L, 16L, 14L, 8L, 4L, 4L, 6L, 6L, 11L, 4L, 5L, 
    7L, 7L, 6L, 8L, 2L, 3L, 3L, 1L, 1L, 3L, 3L, 2L, 8L, 8L, 11L, 
    10L, 11L, 8L, 24L, 25L, 25L, 33L, 36L, 51L, 61L, 74L, 92L, 
    89L, 123L, 402L, 602L, 524L, 494L, 406L, 344L, 329L, 225L, 
    136L, 136L, 84L, 55L, 55L, 42L, 19L, 28L, 8L, 7L, 2L, 7L, 
    6L, 4L, 3L, 5L, 3L, 3L, 0L, 1L, 2L, 3L, 2L, 1L, 2L, 2L, 9L, 
    4L, 9L, 10L, 18L, 15L, 13L, 12L, 10L, 19L, 15L, 22L, 23L, 
    34L, 43L, 53L, 47L, 57L, 328L, 552L, 787L, 736L, 578L, 374L, 
    228L, 161L, 121L, 96L, 58L, 50L, 37L, 14L, 9L, 6L, 15L, 12L, 
    9L, 1L, 6L, 4L, 7L, 7L, 3L, 6L, 9L, 15L, 22L, 28L, 34L, 62L, 
    54L, 75L, 65L, 58L, 57L, 60L, 37L, 47L, 60L, 89L, 90L, 193L, 
    364L, 553L, 543L, 676L, 550L, 403L, 252L, 140L, 125L, 99L, 
    63L, 63L, 76L, 85L, 68L, 67L, 38L, 25L, 24L, 11L, 9L, 9L, 
    4L, 8L, 4L, 6L, 5L, 2L, 6L, 4L, 4L, 1L, 5L, 4L, 1L, 2L, 2L, 
    2L, 2L, 3L, 4L, 4L, 7L, 5L, 2L, 10L, 11L, 17L, 11L, 16L, 
    15L, 11L, 12L, 21L, 20L, 25L, 46L, 51L, 90L, 123L)), .Names = c(""date"", 
""cases""), row.names = c(NA, -835L), class = ""data.frame"")
</code></pre>
"
"0.0952081150392732","0.103566754353222"," 63796","<p>This is related to a <a href=""http://stats.stackexchange.com/questions/62646/pooled-time-series-regression-in-r"">question</a> I asked a couple weeks ago, but I've got a new question related to the same data. You can find the data and its accompanying explanation in the link provided.</p>

<p>I felt that a regression including year as a covariate along with year dummies would lead to a linear dependence problem, but I was told to try it anyway as </p>

<blockquote>
  <p>""the year dummies as independent variables [may] pick up year-specific
  random effects not accounted for by a time trend, e.g. for example the
  trend over all years could be down by say 2 percent per year which
  could apply to most years, but a negative macro shock in one
  particular year could make that year lie way off the regression
  line--a simple example of why the year dummies are not co-linear with
  a time trend.""</p>
</blockquote>

<p>This makes sense, I suppose, so I ran a regression that simply included year and year dummies for each year as the independent variables (including AR(1) corrections). This looked like the following:</p>

<pre><code>&gt; ## Generate YearFactor and AgeGroupFactor using factor()
&gt; 
&gt; YearFactor &lt;- factor(YearVar)
&gt; AgeGroupFactor &lt;- factor(AgeGroup)
&gt; 
&gt; ## Check to see that YearFactor and AgeGroupFactor are indeed factor variables
&gt; 
&gt; is.factor(YearFactor)
[1] TRUE
&gt; is.factor(AgeGroupFactor)
[1] TRUE
&gt;
&gt; ## Run regressions with both time trend and year dummies to determine if a linear dependence problem exists.
&gt; 
&gt; TrendDummies &lt;- gls(PPHPY ~ YearVar + YearFactor, correlation=corARMA(p=1))
Error in glsEstimate(object, control = control) : 
 computed ""gls"" fit is singular, rank 13
&gt; summary(TrendDummies)
Error in summary(TrendDummies) : object 'TrendDummies' not found
&gt;
</code></pre>

<p>I interpret the error message ""Error in glsEstimate(object, control = control) : 
     computed ""gls"" fit is singular, rank 13"" to mean that there indeed is a linear dependence problem in this case. Am I properly interpreting this? </p>

<p>Also, given the advice in quotes above, would my regression as constructed (if there were no linear dependence problems) capture the effects mentioned therein?</p>

<p>And finally, if I run the same regression as OLS with no AR(1) correlation structure, I do indeed get some results (instead of an error message). Any thoughts on that?</p>
"
"0.0904616275314925","0.090834052439095"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"NaN","NaN"," 65191","<p>I have a test dataset on which I run the following regression analysis in r:</p>

<p><code>fit &lt;- lm(Cost ~ Slope + YardDist + Removals + TreeVol, data = test)</code></p>

<p>I get an r-squared of 0.83</p>

<p>Next I change the regression equation to:</p>

<p><code>fit &lt;- lm(Cost ~ Slope + YardDist + Removals+ I(TreeVol^(-0.82)), data = test)</code></p>

<p>I get an r-squared of 0.9872.</p>

<p>I am wondering what the expression of <code>I(TreeVol^(-0.82))</code> exactly means?</p>

<p>When I hold all variables constant besides TreeVol, the TreeVol/Cost graph looks like the red one. I guess that is related but I don't fully understand it.
<img src=""http://i.stack.imgur.com/g1Ide.png"" alt=""enter image description here""></p>
"
"0.06396603026469","0.0642293744423385"," 65244","<p>I'm curious about how to understand the accuracy of my model which I computed with <code>glm( family = binomial(logit) )</code>.</p>

<p>In some articles it is mentioned that we should perform chisq test with residual deviance with it's DoF. 
When I call summary() of my glm module.
<strong>""Residual deviance: 9109.9 on 99993 degrees of freedom""</strong> 
Therefore when I perform pchisq test with these inputs: <strong>1-pchisq(9110, 99993)</strong> it returns 1.</p>

<p>Hence it is much more greater than our significance level. So we are curious about why does it return 1, is it a perfect model ?</p>

<p>In addition to these, here's the output of my Logistic Regression Model</p>

<pre><code>Logistic Regression Model

lrm(formula = bool.revenue.all.time ~ level + building.count + 
    gold.spent + npc + friends + post.count, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
level           0.0756 0.0193   3.92 &lt;0.0001 
building.count  0.0698 0.0091   7.64 &lt;0.0001 
gold.spent      0.0020 0.0002  11.05 &lt;0.0001 
npc             0.0172 0.0057   3.03 0.0024  
friends         0.0304 0.0045   6.82 &lt;0.0001 
post.count     -0.0132 0.0042  -3.17 0.0015 
</code></pre>

<p>This is validation with bootstrap's output</p>

<pre><code>  index.orig training   test optimism index.corrected    n
Dxy           0.5511   0.5500 0.5506  -0.0006          0.5518 1000
R2            0.1469   0.1469 0.1465   0.0005          0.1465 1000
Intercept     0.0000   0.0000 0.0002  -0.0002          0.0002 1000
Slope         1.0000   1.0000 0.9997   0.0003          0.9997 1000
Emax          0.0000   0.0000 0.0001   0.0001          0.0001 1000
D             0.0149   0.0149 0.0148   0.0000          0.0148 1000
U             0.0000   0.0000 0.0000   0.0000          0.0000 1000
Q             0.0149   0.0149 0.0148   0.0001          0.0148 1000
B             0.0086   0.0086 0.0086   0.0000          0.0086 1000
g             1.1410   1.1381 1.1365   0.0016          1.1394 1000
gp            0.0111   0.0111 0.0111   0.0000          0.0111 1000
</code></pre>

<p>And this is the output of my calibration curve:</p>

<pre><code>n=100000   Mean absolute error=0.002   Mean squared error=5e-05
0.9 Quantile of absolute error=0.002
</code></pre>

<p><img src=""http://i.stack.imgur.com/2AUEX.png"" alt=""Calibration Curve""></p>

<p>Thanks.</p>
"
"0.0756856276908142","0.0759972207238908"," 65463","<p>This is a question regarding using logistic regression, and relating it to gaussian distribution or a binomial distribution.  </p>

<pre><code>model&lt;-glm(target~ x1, data=data, type='response', family='binomial')
model&lt;-glm(target~ x1, data=data, type='response')  #defaults to gaussian
</code></pre>

<p>My understanding of binomial is that it is </p>

<pre><code>theta=chance of success
z=trails ending in success
k=trials ending in failure
(theta^z)*(1-theta)^k
</code></pre>

<p>And something Gaussian is </p>

<pre><code>theta = standard deviation
x = success
u = mean
Y = [ 1/Ïƒ * sqrt(2Ï€) ] * e -(x - Î¼)2/2Ïƒ2 
</code></pre>

<p>So I understand how to do GLM with R, I kind of understand what binomial and gaussian means, but I have no understanding of how you relate binomial or gaussian to logistic regression, and how binomial and gaussian are different in this context.</p>

<p>Question 1- Can someone explain the intuition behind how ""family='binomial'"" is used when building a model with GLM?</p>

<p>Question 2- Given that the shapes of a binomial distribution and a gaussian distribution look very much the same (they both peak in the middle and gradually go down towards the ends), how does choosing either binomial or guassian lead to different models built from GLM?</p>

<p>thanks!!</p>
"
"0.0858194351535935","0.0861727484432139"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"NaN","NaN"," 66946","<p>When you predict a fitted value from a logistic regression model, how are standard errors computed?  I mean for the <em>fitted values</em>, not for the coefficients (which involves Fishers information matrix).</p>

<p>I only found out how to get the numbers with <code>R</code> (e.g., <a href=""https://stat.ethz.ch/pipermail/r-help/2010-August/248241.html"">here</a> on r-help, or <a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">here</a> on Stack Overflow), but I cannot find the formula.</p>

<pre><code>pred &lt;- predict(y.glm, newdata= something, se.fit=TRUE)
</code></pre>

<p>If you could provide online source (preferably on a university website), that would be fantastic.</p>
"
"0.06396603026469","0.0642293744423385"," 67209","<p>I'm a novice attempting to predict automobile sales using a combination of previous sales (seasonal AR model), macroeconomic indicators such as CPI, consumer sentiment index etc. and more significantly, I'm using weekly data from google search trends for the automobile and other variables from the category (automobile) in google trends. (code developed using Choi and Varian's 2009 paper). The model essentially looks at search trend indices for three weeks before the month in question</p>

<p>In addition to running spike and slab regression to determine attribute importance, I also ran a gradient boosted machine (R package GBM). as of now, the model seems to be performing fairly satisfactorily, with the Mean Average Error = 5.4 (monthly sales are in the range of 10000), however there are clear seasonal trends in the difference between predicted variables and actual sales that seem to coincide with sales promotions and other promotional events. My question, therefore is:</p>

<p>How do I account for these seasonal trends while using GBM? or for a linear model?</p>
"
"0.0700712753800578","0.0703597544730292"," 67275","<p>I'm reading a paper that does not report the coefficients from two OLS regressions. In both cases there is 1 response variable and 1 predictor variable.  The predictor variable is the same in both cases. I know the subject matter of the paper well, which leads me to believe that the means of the slopes are almost certainly between 0 and 1. Although the slopes for these two regressions are not reported, the author does report that neither slope is significantly different from 0 (p â‰¥ 0.05).</p>

<p>If neither slope is different from 0, but both slopes are between 0 and 1, could the slopes be different from each other?</p>

<p>To try to figure this out, I did a quick test in R.  I used two slopes that were very different (0.99 and 0.01), but chose s.e.'s for each that would make them barely ""insignificant"". To compare the slopes, I used the formula from the answer to <a href=""http://stats.stackexchange.com/questions/55501/test-a-significant-difference-between-two-slope-values"">THIS</a> question.</p>

<pre><code>pnorm( 
  (0.99 - 0.01)/ #difference between means
  sqrt(0.61^2 + 0.0062^2), #sqrt of sum squares of s.e.'s
  lower.tail=FALSE
)
</code></pre>

<p>OK, so this quick-and-dirty test suggests that the two slopes in the author's analysis can't be different.  </p>

<p>Is it necessarily true that if two slopes are between 0 and 1, and neither different from 0, that they cannot be significantly different from each other?</p>
"
"NaN","NaN"," 67363","<p>I am running a Bayesian regression model by WinGUBS via R2WinBUGS package in R. Everything looks fine except for one parameter:</p>

<pre><code>nlssim$summary[37,][c(1,2,8,9)]
    mean           sd         Rhat        n.eff 
3.054326e-05 9.523965e-06 1.000000e+00 1.000000e+00 
</code></pre>

<p>The number of effective size is 1! It seems to indicate that the autocorrelation between samples are extremely large, but the trace plot looks all fine. And I then try the effectiveSize function in code package and the result is 4590.</p>

<p>I am curious how WinBUGS calculate the n.eff statistic, and if n.eff=1 is symptom of some of my mistakes?</p>

<p>I run 3 chains in parallel, and each chain has 60000 iterations. n.burnin=200, n.thin=30</p>

<p>Thank you very much in advance!</p>
"
"0.0756856276908142","0.0759972207238908"," 67385","<p>I am trying to model <strong>count data in R that is apparently underdispersed</strong> (Dispersion Parameter ~ .40). This is probably why a <code>glm</code> with <code>family = poisson</code> or a negative binomial (<code>glm.nb</code>) model are not significant. When I look at the descriptives of my data, I don't have the typical skew of count data and the residuals in my two experimental conditions are homogeneous, too. </p>

<p>So my questions are:</p>

<ol>
<li><p><strong>Do I even have to use special regression analyses for my count data, if my count data doesn't really behave like count data?</strong> I face non-normality sometimes (usually due to the kurtosis), but I used the percentile bootstrap method for comparing trimmed means (Wilcox, 2012) to account for non-normality. Can methods for count data be substituted by any robust method suggested by Wilcox and realized in the WRS package?</p></li>
<li><p><strong>If I have to use regression analyses for count data, how do I account for the under-dispersion?</strong> The Poisson and the negative binomial distribution assume a higher dispersion, so that shouldn't be appropriate, right? I was thinking about applying the <strong>quasi-Poisson</strong> distribution, but that's usually recommended for over-dispersion. I read about <strong>beta-binomial</strong> models which seem to be able to account for over- as well as underdispersion are availabe in the <code>VGAM</code> package of R. The authors however seem to recommend a <strong>tilded Poisson distribution</strong>, but I can't find it in the package. </p></li>
</ol>

<p><strong>Can anyone recommend a procedure for underdispersed data and maybe provide some example R code for it?</strong></p>
"
"0.06396603026469","0.0642293744423385"," 67460","<p>I fitted the following multinomial regression:</p>

<pre><code>library(car)
p1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,2,2,3,4,3,3,4,3,4)

d1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,1,2,3,4,3,2,2,2,1)

d1&lt;-as.ordered(d1)

library(nnet)
test&lt;-multinom(p1~d1)
predi&lt;-expand.grid(d1=c(""1"",""2"",""3"",""4""))

pre&lt;-predict(test,predi,type=""probs"")
</code></pre>

<p>The output is a table of the predicted probabilities for every coefficient. I can also order the results for the confidence interval of the coefficents with:</p>

<pre><code>confint(test)
</code></pre>

<p>My question is: is it possible to get the results for the confidence interval for the predicted probabilities? It means for every amount in the ""pre"" output! 
PS: I found a similar question here in 
[""plotting confidence intervals""][1]<a href=""http://stats.stackexchange.com/questions/29044/plotting-confidence-intervals-for-the-predicted-probabilities-from-a-logistic-re"">Plotting confidence intervals for the predicted probabilities from a logistic regression</a></p>

<p>The main answer is perfect for my question, but I do not know how to combine with multinomial regression. 
I hope you understand my bad english :) Thank you for your help</p>
"
"0.0904616275314925","0.090834052439095"," 67790","<p>I am dealing with a unreplicated factorial design. I have some illustrative examples but I need to simulate some unreplicated factorial designs. I do not how and what to use. Can $R$ handle this?</p>

<p>For example, I would like to analyse a $2^{4}$ factorial design (factors are A, B, C and D) with only one run and 15 contrasts. I have a single column for response. I would like to compare some methods in the literature to see which method detects active effects better. Thus, I set the active effects to have the same magnitude of $1.5\sigma$ and I would like to generate $100$ response vectors using errors that are i.i.d. with $\mathcal N(0 ,1)$. My true model has four active effects and I would like to simulate $100$ response vectors using this true model $y=3+1.5A+1.5B+1.5C+1.5BC$. But I do not know how to generate data like this using R. </p>

<p><img src=""http://i.stack.imgur.com/e7F8i.png"" alt=""An example from Montgomery for $2^4$ unreplicated factorial design""></p>

<hr>

<p>Thanks gung for your reply. I just wrote a simple code before I saw your answer here. I think, I need to build up a bit more R knowledge. Anyway, here it is:</p>

<p>For the analysis of unreplicated factorial designs with $k$ factors and $p=2^{k}-1$ factorial effects (the main effects and interactions), the following model is generally used</p>

<p>\begin{equation}
y=\sum\limits_{i=0}^{p}x_{i}\beta_{i}+\varepsilon_{i}
\end{equation}</p>

<p>So, Firstly I introduced my sign table for $2^{4}$ and $\beta$ coefficients of so-called active effects. </p>

<p>Sign table consists of rows (runs) and columns (contrasts with general mean).
<img src=""http://i.stack.imgur.com/e7F8i.png"" alt=""enter image description here""></p>

<p>And then, I created my regression equation with magnitudes of active effects and zeros of remaining inactive effects. My simulated model, for example, was $y=3+1.5A+1.5B+1.5C+1.5BC$.
<img src=""http://i.stack.imgur.com/9N4GA.png"" alt=""enter image description here""></p>

<p>And then, I run the code below</p>

<pre><code>x=read.csv(""sign2.txt"", header=TRUE)
sign= as.matrix(x)
is.matrix(sign)

y=read.csv(""beta2.txt"", header=TRUE)
beta= as.matrix(y)
is.matrix(beta)

signt=t(sign)

bs=t(beta %*% signt)

epsilon=matrix( rnorm(16*1,mean=0,sd=1), 16, 1) 

response=bs+epsilon
</code></pre>

<p>However, unfortunately, it's for one simulation. I will put a loop command to run the simulation n-times.</p>
"
"0.185441321869126","0.190535115826549"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.0404556697031367","0.0406222231851194"," 68183","<p>I was asked to do some regression analysis on anonymised data, which I don't have yet. It will have a few categorical inputs and ~100 boolean outputs, something like this:</p>

<pre><code>Category   Type         Size       Blood pressure     Y1   Y2    Y3  ....   Y99  
-------------------------------------------------------------------------------
cat        persian      small      high               1    0     0   ....   1
cat        persian      big        low                0    1     1   ....   0
dog        wolfhound    big        normal             1    1     0   ....   0
...        ...          ...        ...                      ......   
duck       scoter       small      above normal       0    1     1   ....   1  
</code></pre>

<p>(I just made up the meanings here). The number of samples in the real data set will be about a million, without missing values. The task is to predict the output, eg <code>P(Y86 = 1 | {cat, siamese, big, low})</code> and generally understand the relationship between inputs and outputs. </p>

<p>My questions: </p>

<ol>
<li>which approaches are worth trying, what are the pros/cons?</li>
<li>which <code>R</code> packages can be of help?</li>
</ol>
"
"0.0858194351535935","0.0861727484432139"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.145965139117999","0.151994441447782"," 68812","<p>I'm really new to ARIMA methods and am trying to forecast electricity load. I've integrated: electricity load, temperature, weekday (dummy), public holidays, and school holidays. My model tries to perform a non seasonal ARIMA with linear regression for each hour of the day.</p>

<p>Here is my code for an example of one of the 24 hours (6 AM):</p>

<pre><code># ElecLoad contains hourly loads and other data for 2005 and 2006 (=2*365*24 entries):
# 1. Electricity load in MW
# 2. day of weak: sunday=0, monday=1, etc 
# 3. Hour of the day 0 -&gt; 23
# 4. Public Holiday: 1 if Public Holiday, 0 otherwise
# 5. Scool vacation: 1 if no scool
# 6. Temperature in Â°F

# Create the weak matrix = dumy variables for the weakdays
weakmatrix&lt;-model.matrix(~as.factor(ElecLoad[,2]))
#Remove intercept
weakmatrix&lt;-weakmatrix[,-1]

#Generate FullTable
FullTable&lt;-cbind(load=ElecLoad[,1], weakmatrix, ElecLoad[,4],
                 ElecLoad[,3],ElecLoad[,5],ElecLoad[,5]^2, ElecLoad[,6])
colnames(FullTable)&lt;-c(""Load"",""mon"",""tue"",""wed"",""thu"",""fri"",""sat"",
                       ""ScoolHol"",""PubHol"",""Temp"",""Temp2"",""Hour"")

#Create the xreg = substed for a specific hour of the day (column 12 = Hour)
xreg&lt;-subset(FullTable[,2:11], FullTable[,12] == 7)

#Create the Load time serie, also a subset of the full table
LoadTs&lt;-ts(subset(FullTable[,1], FullTable[,12] == 7),start=1,frequency=1)

#Launch of auto.arima
ArimaLoad&lt;-auto.arima(LoadTs, xreg=xreg, lambda=0)
</code></pre>

<p>When I try to forecast with the same 2 years data as <code>xreg</code>, here is my output</p>

<pre><code>plot(forecast(ArimaLoad,xreg=xreg), include=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>While when I try to plot the fitted it looks identical to my original Load</p>

<pre><code>plot(fitted(ArimaLoad))
</code></pre>

<p><img src=""http://i.stack.imgur.com/zsw81.png"" alt=""enter image description here""></p>

<p>I don't understand why the <code>prediction()</code> is so much different than the <code>fitted()</code> with the same <code>xreg</code> matrix. Is this a normal behaviour, how can I improve my model to better fit with the real situation?</p>

<hr>

<p>Thank you so much for your support.</p>

<p>I'm not sure I understood everything from what you propose.</p>

<p>You mean that I should build a first model to forecast the daily average load (I prefer the average than the sum because due to DST, some days don't have 24 hours...). This model would be deterministic, but I don't see what kind of model you're thinking off? Is a multilinear regression ok? I prefer to consider the log(load) to make the different parameters multiplicative which I think is better fit to the reality.
Then I should have 24 hourly models, taking the daily average then split with a sort seasonal effect?
Should I use somewhere an ARIMA model?
I'm not convince of considering the month as having an effect, in my opinion there is no reason that consumption is more important in January than August except if we consider the Temperature and Holidays effects. The hour of the day is related to the activity that's the reason why I'm considering the specific model for each hour. The same way each day of the weak is different.</p>

<p>I've tried a multilinear regression for the same hour (7:00 AM) and the result looks not so bad.</p>

<pre><code>#Create the frame.data
Load&lt;-subset(FullTable[,1], FullTable[,12] == 7)
FullData&lt;-cbind(LogLoad=log(Load), xreg)
FrameData&lt;-data.frame(FullData)

# multilinear regression
mlin&lt;-lm(LogLoad ~ mon+tue+wed+thu+fri+sat+ScoolHol+PubHol+Temp+`Temp2`, FrameData)
plot(exp(mlin$model$LogLoad), type=""l"",col=""blue"")
lines(exp(fitted(mlin)), col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>fitted() in red which is now exactly the same as predict() if I re-use the same data entry (2005-2006) and looks not so far from the original load in blue (no so bad for a simple model). I still don't fully understand why it did not work with ARIMA as it also takes into consideration multilinear regression.</p>

<p>Now my ""simple"" model already takes into account several parameters, like the temperature, the holiday, the school vacations the day of the weak and the hour of the day (local time, not UCT).
How can I improve my model further more? How can I make sure that the parameters are invariant? Is there a specific method?</p>
"
"0.0330319159917525","0.033167906340333"," 69528","<p>This is my R code and running result:(See below)</p>

<p>How to judge is the linear regression model appropriate for this data set? Except R^2 value, Can the</p>

<p>p-value in last row of the running result mean something? What does this      p-value mean and can it mean the linear regression model appropriate for this data set? Why?</p>

<p>Thanks in advance.</p>

<pre><code>x=c(7,12,10,10,14,25,30,25,18,10,4,6)    
y=c(128,213,191,178,205,446,540,547,324,117,75,107)    
list(x,y)    
reg1 &lt;- lm(y~x)    
summary(reg1)    
plot(x, y)
abline(reg1)    
reg2 &lt;- lm(y~x-1)    
reg2    
summary(reg2)
#------------------    
Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-55.805 -21.085   3.139  14.946  80.859 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -22.753     21.846  -1.041    0.322    
x             19.556      1.335  14.652 4.38e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 37.23 on 10 degrees of freedom
Multiple R-squared:  0.9555,    Adjusted R-squared:  0.951 
F-statistic: 214.7 on 1 and 10 DF,  p-value: 4.38e-08
</code></pre>
"
"0.0404556697031367","0.0406222231851194"," 69860","<p>For the purpose of model selection, I am using the Bayes' factor to compare different combinations of predictors in a linear regression model.</p>

<p>I have used the function <code>regressionBF()</code> from the <code>library(BayesFactor)</code>, and I got the following results:</p>

<pre><code># &gt; regressionBF(return ~ FSCR + VAL, data = dataf)

# Bayes factor analysis
# --------------
#[1] FSCR       : 65.17482  Â±0%
#[2] VAL        : 0.1979875 Â±0.02%
#[3] FSCR + VAL : 23.58704  Â±0%

#Against denominator:
#  Intercept only 
</code></pre>

<p>I am not sure how to interpret these results. What do the percentage numbers next to the Bayes' factors mean?
Also, 65 and 23 seem pretty high for a Bayes' factor. How can I interpret that?</p>

<p>Any help would be appreciated. Thanks!</p>
"
"0.131589800568843","0.126386697716714"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.0286064783845312","0.0287242494810713"," 70209","<p>I have the following data:</p>

<pre><code>t       mean
147     1.4
143     3
137.5       1.8
133     1.9
129.5       1.8
124.5       2.5
115.5       1.9
107     2.5
102.5       6.3
98.5        6.5
94.5        5
89      5.5
81      4.8
73      9.3
</code></pre>

<p>To me, the slope looks more exponential than linear when plotted as a scatterplot. I've been using the following code in R:</p>

<pre><code>data&lt;-read.csv(""regression.csv"")
attach(data)
plot(t,mean)
data.lm&lt;-lm(mean~t,data=data)
summary(data.lm)
data.exp&lt;-lm(log(mean) ~ log(t) ,data=data)
summary (data.exp)
AIC(data.lm, k=2)
AIC(data.exp, k=2)
</code></pre>

<p>data.exp, the exponential regression, has a much lower p-value and a much lower AIC score than data.lm, the linear model: 6.869e-05 vs. 0.000194, and 11.4641 vs. 52.22926.</p>

<p>But (how) can I demonstrate that the data fits an exponential line better than a straight line? Is the use of AIC legitimate here? Sorry to ask such as simple question but I've looked online and haven't found an answer.</p>

<p>Thank you!</p>
"
"0.0814154647783432","0.090834052439095"," 70227","<p>I want to model the infection rates in bees based on weather conditions. The weather variables are rolling means for different time periods and durations. Dependent data is infection levels gathered in March and the independent variables are the weather aggregates (e.g. from 30 day period from Jan1-Jan30, 90 day period from Dec1-Feb28), a few thousands of them and highly correlated.</p>

<p>PCA techniques did not work since the infections are not so strongly related to weather. I have also tried Bayesian Model Averaging and Boosted Regression Trees, since variables could be selected based on variable importance they calculate.</p>

<p>But since, my data is longitudinal and my apiaries have a fixed location, I think mixed-models are a good choice. Is there a way to do variable selection based on mixed-models?</p>

<p>What I have done now is to<br>
1. run <code>glmer</code> for each of the independent variables separately,<br>
2. remove those variable whose p-values for fixed-effect estimates are below 0.05 (not sure if this is a right thing - if the estimate for a variable is not significant, that variable being the only one in the model, it is right to drop that variable, is it?)<br>
3. from the variables that are left over, test for correlation between the variables<br>
4. remove the variables that are highly correlated, giving preference to the variable that has the lowest AIC.  </p>

<p>Or should I at this stage, not worry about p-values of Intercepts and only focus on AIC (or BIC)? since some of the variables have high p-values but AICs lower than than those with low p-values. </p>

<p>I have tried reading up a lot, and there is no one fool-proof solution for variable selection, but would like to know if there is anything inherently wrong with my method. As I am not a statistician, equations often look like beautiful Arabic calligraphy and there lies my dead-end.</p>
"
"0.0495478739876288","0.0497518595104995"," 70261","<p>I am working with a set of historical data that I did not collect myself. I cannot add to this data set in any way. For a 350-year period I have many thousands of data points, each associated with one of ~50 time points. Some of the time points have hundreds of data points associated with them, so I'm sure that those time points can be used in the linear regression. However, others have very few, even just one.</p>

<p>So my question is, how can I determine a cut-off for the time points that have enough data (and should be included in my regression) and the time points that do not have enough data (and should be excluded)? I'd like to do this in R.</p>

<p>Also, perhaps this is relevant - I'm interested in the minimum for each time point, not the mean. Thank you!</p>
"
"0.0429097175767967","0.043086374221607"," 70675","<p>How should I read the output of the function <code>ar</code> in R. For example, take this VAR model:</p>

<pre><code>library(tseries)
data(USeconomic)
US.ar &lt;- ar(cbind(GNP, M1), method=""ols"",
            dmean=T, intercept=F)
</code></pre>

<p>(from the book <code>Introductory Time Series with R</code> by Cowpertwait)</p>

<p>So <code>Us.ar</code> produces the following output:</p>

<pre><code>&gt; US.ar

Call:
ar(x = cbind(GNP, M1), method = ""ols"", dmean = T, intercept = F)

$ar
, , 1

         GNP    M1
GNP  1.27181 1.167
M1  -0.03383 1.588

, , 2

         GNP      M1
GNP -0.00423 -0.6942
M1   0.06354 -0.4839

, , 3

         GNP      M1
GNP -0.26715 -0.5103
M1  -0.02859 -0.1295


$var.pred
       GNP    M1
GNP 618.69 16.38
M1   16.38 23.90
</code></pre>

<p>and <code>US.ar$ar</code> gives this other representation:</p>

<pre><code>&gt; US.ar$ar

, , GNP

           GNP          M1
1  1.271812104 -0.03383385
2 -0.004229937  0.06353801
3 -0.267154022 -0.02858942

, , M1

         GNP         M1
1  1.1674655  1.5876695
2 -0.6941813 -0.4838919
3 -0.5103451 -0.1294549
</code></pre>

<p>As I understand it, <code>,,1</code> refers to the order of the coefficient in the autoregression model. In the second representation, <code>, , M1</code> refers to the columns, whereas <code>GNP         M1</code> describe rows and the numbers describe the order. I think this output is describing the following model:</p>

<p>$$GNP_{t} = 1.27 GNP_{t-1} + 1.167 M1_{t-1} - 0.004 GNP_{t-2} - 0.6942 M1_{t-2} - 0.2671 GNP_{t-3} - 0.5104 M1_{t-3}$$</p>

<p>$$M1_{t} = - 0.03 GNP_{t-1} + 1.58 M1_{t-1} + 0.063 GNP_{t-2} - 0.48M1_{t-2} - 0.02 GNP_{t-3} - 0.12 M1_{t-3}$$</p>

<p>Therefore, in matrix notation, this should be equal to:
$$
\begin{pmatrix}
GNP_{t} \\
M1_{t} \\
\end{pmatrix} = \left[ \begin{pmatrix}
1.27 &amp; 1.167 \\
-0.03 &amp; 1.58 \\
\end{pmatrix}x + 
\begin{pmatrix}
-0.004 &amp; -0.6942 \\
0.063 &amp; -0.48 \\
\end{pmatrix}x^{2} +
\begin{pmatrix}
-0.267 &amp; -0.5104 \\
-0.02 &amp; -0.12 \\
\end{pmatrix}x^{3} \right] 
\begin{pmatrix}
GNP_{t} \\
M1_{t} \\
\end{pmatrix}$$</p>

<p>However, the book says that the model is this:</p>

<p><img src=""http://i.stack.imgur.com/lXHJA.png"" alt=""enter image description here""></p>

<p>As you can see, cross-terms are flipped. Is this a mistake?</p>
"
"0.0572129567690623","0.0574484989621426"," 71224","<p>I have a dataset that I have to perform a regression task. I split the dataset into 80% for training and 20% for validation.</p>

<pre><code>train.index &lt;- sample(N.rows, N.rows * 4/5, F)
train.data &lt;- data[train.index,]
val.data &lt;- data[-train.index,]
</code></pre>

<p>I used the GBM in R which has the parameters specified (and other take defaults). However, this applies to any kind of algorithms with a grain of randomness embedded.</p>

<pre><code>train.model &lt;- function(train.data, val.data)
{
    # 1) model &lt;- gbm(formula = /* ... */, data = train.data, n.trees = 1000, interaction.depth = 3, n.minobsinnode = 5, shrinkage =  0.01, distribution = ""gaussian"")
    # 2) predict via model on val.data
    # 3) calculate error
}
</code></pre>

<p>Suppose I executed train.model with the same train.data and val.data every time, and I do it about 50 times. The error (Mean absolute error) percentile is about 5-10%, which is rather high. I could select the best model out of the 50 runs, but this assumes I have a validation set which I do not use as training set.</p>

<p>My question is this:
<strong>Suppose I am using all the data for training (normal cross validation practice), how could I tell if the model is performing poorly and avoid these major fluctuations?</strong></p>

<p>Thank you.</p>
"
"0.143138302630928","0.149255578531498"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.0572129567690623","0.0574484989621426"," 71832","<p>I have a Poisson model with varying densities:</p>

<pre><code>set.seed(1)
df = data.frame(density = 1:5, events = rpois(2000, 1:5))
</code></pre>

<p>If I regress on this, I get that the intercept is approximately <code>log(3)</code>, which makes sense because 3 is the mean of 1:5.</p>

<pre><code>glm(events ~ 1, df, family = poisson)  # returns 1.089
</code></pre>

<p>But now suppose I want to read back the coefficients of density:</p>

<pre><code>glm(events ~ as.factor(density), df, family = poisson)
</code></pre>

<p>(For simplicity I've used <code>density</code> as both the ID of the field and its density.) I would expect the coefficient of <code>density[i]</code> to be <code>log(3-i)</code> because the intercept would still be 3. However, it doesn't seem like the intercept remains 3 - in this case, the intercept is set to <code>log(1)</code>. In playing around with this, it seems like glm sets the intercept to be the coefficient of the first factor.</p>

<p>Now I'm starting to wonder what the p values in a glm regression indicate. Is the null hypothesis that <code>density[i]</code> is the same as the intercept (aka <code>density[1]</code>)? Or is it that <code>density[i] = mean(density)</code>?</p>
"
"0.10703564115707","0.107476300250388"," 72421","<p>I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). </p>

<p>I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. </p>

<h2>Data set</h2>

<p>To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.</p>

<p><img src=""http://i.stack.imgur.com/aZm4N.jpg"" alt=""enter image description here""></p>

<p>Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.</p>

<h2>Preliminary Analysis</h2>

<p>I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. </p>

<p><img src=""http://i.stack.imgur.com/X4YZI.jpg"" alt=""correlation between daily data during each calendar month""></p>

<p>I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.</p>

<h3>Problems</h3>

<p>Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. </p>

<p>I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
<img src=""http://i.stack.imgur.com/NWzm2.jpg"" alt=""enter image description here""></p>

<p>The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.</p>

<p>I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.</p>

<h3>Questions</h3>

<p>I'm learning my way into this field and R at the same time, and would appreciate suggestions on:</p>

<ol>
<li>What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.</li>
<li>Are there more appropriate methods to show the correlation between multiple data sets separated in space?</li>
<li>... in particular, methods that are easy to show results from visually?</li>
<li>Are any of these implemented in R?</li>
<li>Do any of these approaches lend themselves to automation?</li>
</ol>
"
"0.0809113394062735","0.0812444463702388"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.0572129567690623","0.0574484989621426"," 73004","<p>I am fitting a gam model in R (using the <code>gam</code> function in <code>mgcv</code>) to account for some non-linear effects in my data. A stripped down example of what I am doing in R is:</p>

<pre><code>mod=gam(y~s(x)+s(z),data=df)
</code></pre>

<p>However, I want to add a slightly more complicated variance model to my regression of the form </p>

<p>$$\epsilon \sim N(0,\sigma^2),\ \sigma = f(\hat{\mu})$$</p>

<p>where $\hat{\mu}$ is the fitted value of the model. (Actually, it would be nice if $\epsilon \sim t_\nu$ for some $\nu$ but sticking to this for now. I have managed to do this in the <code>gls</code> function from <code>nlme</code> using the <code>varFunc(form=fitted(.))</code> type approach, but can't figure out if there is an option to do the same kind of thing using <code>gam</code>.</p>

<p>I recognise this is not really the intention of a GLM/GAM model, but I don't want to reinvent the wheel if I am just missing something obvious</p>

<p>Edit: In response to the question in the comment below, I am hoping to fit a linear or quadratic function for $f$. I do not know the exact form of $f$ but plan to iteratively estimate it from the residuals if this can't be done automatically.</p>

<p>Edit2: Typo in R code - first spline is not meant to be a function of y!</p>
"
"0.0993902382172794","0.107476300250388"," 73165","<p>I have a logistic regression model (fit via glmnet in R with elastic net regularization), and I would like to maximize the difference between true positives and false positives.  In order to do this, the following procedure came to mind:</p>

<ol>
<li>Fit standard logistic regression model</li>
<li>Using prediction threshold as 0.5, identify all positive predictions</li>
<li>Assign weight 1 for positively predicted observations, 0 for all others</li>
<li>Fit weighted logistic regression model</li>
</ol>

<p>What would be the flaws with this approach?  What would be the correct way to proceed with this problem?</p>

<p>The reason for wanting to maximize the difference between the number of true positives and false negatives is due to the design of my application.  As part of a class project, I am building a autonomous participant in an online marketplace - if my model predicts it can buy something and sell it later at a higher price, it places a bid.  I would like to stick to logistic regression and output binary outcomes (win, lose) based on fixed costs and unit price increments (I gain or lose the same amount on every transaction).  A false positive hurts me because it means that I buy something and am unable to sell it for a higher price.  However, a false negative doesn't hurt me (only in terms of opportunity cost) because it just means if I didn't buy, but if I had, I would have made money.  Similarly, a true positive benefits me because I buy and then sell for a higher price, but a true negative doesn't benefit me because I didn't take any action.</p>

<p>I agree that the 0.5 cut-off is completely arbitrary, and when I optimized the model from step 1 on the prediction threshold which yields the highest difference between true/false positives, it turns out to be closer to 0.4.  I think this is due to the skewed nature of my data - the ratio between negatives and positives is about 1:3.</p>

<p>Right now, I am following the following steps:</p>

<ol>
<li>Split data intto training/test</li>
<li>Fit model on training, make predictions in test set and compute difference between true/false positives</li>
<li>Fit model on full, make predictions in test set and compute difference between true/false positives</li>
</ol>

<p>The difference between true/false positives is smaller in step #3 than in step #2, despite the training set being a subset of the full set.  Since I don't care whether the model in #3 has more true negatives and less false negatives, is there anything I can do without altering the likelihood function itself?</p>
"
"0.0990957479752576","0.0829197658508324"," 73351","<p>Basically I'm trying to fit garch(1,1) model with arima order from auto.arima</p>

<pre><code>&gt; assign(paste(""spec.ret.fin."",colnames(base.name[1]),sep=""""),    
+ ugarchspec(variance.model = list(model = ""fGARCH"", garchOrder = c(1, 1), 
+ submodel = ""GARCH"", external.regressors = NULL, variance.targeting = FALSE), 
+ mean.model = list(armaOrder = c(2,3,4), include.mean = TRUE, archm = FALSE, 
+ archpow = 1, arfima = FALSE, external.regressors = NULL, archex = FALSE), 
+ distribution.model = ""norm"", start.pars = list(), fixed.pars = list()))
</code></pre>

<p>This gives the following result:</p>

<blockquote>
  <p>spec.ret.fin.chn</p>
</blockquote>

<pre><code>*---------------------------------*
*       GARCH Model Spec          *
*---------------------------------*

Conditional Variance Dynamics   
------------------------------------
GARCH Model     : fGARCH(1,1)
fGARCH Sub-Model    : GARCH
Variance Targeting  : FALSE 

Conditional Mean Dynamics
------------------------------------
Mean Model      : ARFIMA(2,0,3)
Include Mean        : TRUE 
GARCH-in-Mean       : FALSE 

Conditional Distribution
------------------------------------
Distribution    :  norm 
Includes Skew   :  FALSE 
Includes Shape  :  FALSE 
Includes Lambda :  FALSE 
</code></pre>

<p>But the same code with <code>arfima=TRUE</code> gives</p>

<blockquote>
  <p>spec.ret.fin.chn</p>
</blockquote>

<pre><code>*---------------------------------*
*       GARCH Model Spec          *
*---------------------------------*

Conditional Variance Dynamics   
------------------------------------
GARCH Model     : fGARCH(1,1)
fGARCH Sub-Model    : GARCH
Variance Targeting  : FALSE 

Conditional Mean Dynamics
------------------------------------
Mean Model      : ARFIMA(2,d,3)
Include Mean        : TRUE 
GARCH-in-Mean       : FALSE 

Conditional Distribution
------------------------------------
Distribution    :  norm 
Includes Skew   :  FALSE 
Includes Shape  :  FALSE 
Includes Lambda :  FALSE 
</code></pre>

<p>How does one replace that <code>d</code> with the integration order (d) of the arima?</p>
"
"0.0495478739876288","0.0497518595104995"," 73409","<p>I have time series of several variables of 60 or so rows of count data. I want to do a regression model <code>y ~ x</code>. I've chosen to use a Quasipoisson &amp; Negative Binomial GLMs as there's overdispersion etc. </p>

<pre><code>x
Min.   : 24000  
1st Qu.: 72000  
Median :117095  
Mean   :197607  
3rd Qu.:291388  
Max.   :607492  

y
Min.   : 136345
1st Qu.: 405239
Median : 468296
Mean   : 515937
3rd Qu.: 633089
Max.   :1218937
</code></pre>

<p>The data itself are very high and so it may be best to model these as count data (this is what I'm trying to investigate - at which point I can model count data as continuous). It seems to be very common practice, what I want to know is the motivation for this? </p>

<p>Are there any texts that actually show the problem of modelling high count data with Poisson distribution? Perhaps something that shows the factorial in the distribution makes things difficult. </p>
"
"0.0756856276908142","0.0759972207238908"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0429097175767967","0.043086374221607"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.0572129567690623","0.0574484989621426"," 74628","<p>I have completed analysis on the effects of two drug treatments over a period of time on the CD4 cell count of a number of patients. I have taken the square root of the initial CD4 count as a covariate and I have taken a summary measure of the 'slopes' for each patient. </p>

<p>My model is the following:</p>

<pre><code>&gt; slopes.aov &lt;- aov(individual.slope.trans[-29] ~ sqrt(initialCD4)[-29] + treatment.fac[-29])

&gt; summary(slopes.aov)
                   Df Sum Sq  Mean Sq F value Pr(&gt;F)
sqrt(initialCD4)[-29]   1 0.0060 0.006027   0.990  0.322
treatment.fac[-29]      1 0.0082 0.008184   1.344  0.249
Residuals             109 0.6638 0.006090         
</code></pre>

<p>I am quite new to data analysis I am not quite sure how to interpret this model?</p>

<p>Can I still use the regression coefficients to describe my summary measure even though we have no significance. I am really struggling with this. Also how can we describe to effect of the covariate.?</p>

<p>I understand that I have no evidence to suggest that the treatments have an effect on the 'slopes' (my summary measure) and so I have no evidence to say that one treatment is performing better than another.      </p>
"
"0.11100944184129","0.111466460825459"," 76918","<p>I have a question about how to do analysis of an experiment that has already been done, I hope you can help me with some advice!</p>

<p>I will try to keep it as simple as possible, but will give some detail so you know what I'm talking about!</p>

<p>What has been done is a ""screening trial"" to look at the activity of about 50 subjects (fungi) as antagonists (against a pest), the 50 individuals are members of groups (species), but some groups have many more members than others</p>

<p>I have results of several types of screening tests for each of the 50 individuals, with reps of each.  The screening tests look at different aspects, like growth rate, direct effects, and indirect effects.  </p>

<p>I can rank the isolates by their results in each screening test, and there looks like a lot of variability.</p>

<p>I want to be able to report the findings of screens, for each screening test and also to see if some individuals are in top ranks in different screening tests (and also the opposite, if some are great at some tests but not at others). I think what I want is to know if the results of the tests correlate for each individual....? </p>

<p>I am not sure how to say - this individual is the best - how can I tell if it is different than the next in the rank?
If I list the top ten from each screening test, I would like to know that they are statistically different from those I excluded from the list.  I would also like to compare them as groups, to be able to say, this species was the best, but with different numbers of representatives within the species, I dont think I could do this (please advise)</p>

<p>This seems like it would be a common research experiment, for example, for testing drugs in medical experiments, so I am looking for examples of what others have done to present this type of result.</p>

<p>I have seen a similar experiment to what I have to analyse but that had been done on a small scale, and the researchers used ANOVA to test differences among individuals and among groups, and some posthoc test to give each group little letters designating their means different than other groups.  </p>

<p>This seems to be unwieldy for 50 subjects, and I'm not sure about this.... I think I need some kind of mixed model regression to put all the test results in a model to test for correlation/covariance, but my understanding is weak!</p>

<p>I have been learning R and would like to do this analysis using R.</p>

<p>Can you give me advice/suggestions?  I would appreciate any help in understanding and clarifying this problem and solutions!  </p>
"
"0.0495478739876288","0.0497518595104995"," 78001","<p>I would like to perform something like a linear regression on my distribution of data, but I'm interested in a trendline that estimates the <strong>minimum</strong>, <em>not mean</em>, value for each time bin. I'd like to do this in R.</p>

<p>The image below shows a scatterplot of the minimum value for each time bin. The black line is a typical linear regression, which estimates the mean. What I'd like is something like what I painted in red - an estimation of the minimum.</p>

<p><img src=""http://i.stack.imgur.com/D0O7Q.gif"" alt=""enter image description here""></p>

<p>My data look like this:</p>

<p><img src=""http://i.stack.imgur.com/qGAiS.png"" alt=""enter image description here""></p>

<p>Those are just the first few lines but you get the idea.</p>

<p>Thank you!</p>
"
"0.0495478739876288","0.0497518595104995"," 78284","<p>I loaded the data to R and fitted it using GLM function.</p>

<blockquote>
  <p>fit.glm = glm(y~ aX+bZ+cW)</p>
</blockquote>

<p>Then, I found the cuttinf-point using ""segmented"" tool of R.</p>

<blockquote>
  <p>o&lt;-segmented(fit.glm,seg.Z=~X,psi=10)</p>
</blockquote>

<p>Now I have the cut-point and two different slope of X. </p>

<blockquote>
  <p>Call: segmented.glm(obj = fit.glm1, seg.Z = ~PTH, psi = 10)</p>
  
  <p>Meaningful coefficients of the linear terms: (Intercept)          X   Z       W</p>
  
  <p>19.43840           2.29574           0.08701           8.75784      </p>
  
  <p>Estimated Break-Point(s) psi1.PTH : 8.5 </p>
  
  <p>Degrees of Freedom: 324 Total (i.e. Null);  314 Residual Null
  Deviance:     17320  Residual Deviance: 7645      AIC: 1971</p>
</blockquote>

<p>However, I'm trying to get estimated y value of two linear regression models using the result of 'segmented'.</p>

<p>Is it possible using R, or I have to substitute Z and W with mean values of Z &amp; W, and calculate the y value myself?</p>
"
"0.114624397492221","0.115096299024505"," 78663","<p>Is any one here familiar with an R package called Zelig?</p>

<p>I have a data frame like this:</p>

<pre><code>IQ   AGE
80   50
100  18
90   25
</code></pre>

<p>etc.</p>

<p>What I need to do is build a model of IQ given AGE, I am running these commands:
<code>z.out &lt;- zelig(IQ~AGE,data=df,model=""ls"")</code>
this runs the what-if given age 110, what would be the IQ
<code>x.out &lt;- setx(z.out, AGE=110)</code>
This is a simulation model where given the age 110, after running 1 million runs of simulation, what would be the IQ with 95% confidence interval.
<code>s.out &lt;- sim(z.out,x.out, num=1000000, level=95)</code></p>

<p>I have a hard time understanding from what pool of data the <code>sim()</code> function draws the numbers. I read though the docs, but they are written for Ph.D. students, if not more advanced readers. I have asked the Zelig creators this question multiple times but they are directing me to the docs which I read multiple times, with no luck. However, one of the  person that works with Zelig sent me this email:</p>

<blockquote>
  <p>Suppose that you fit 
  $$\text{IQ} = a + \text{Age} * b + e$$
  Then you get a table of regression coefficients where a=50, b=2, and their standard errors are something like $\text{s.e.}(a)=\sqrt{10}$ and $\text{s.e.}(b)=1$. These are all hypothetical examples. 
  In maximum likelihood estimation, this regression output is another way of saying that $a$ and $b$ are distributed bivariate normal with means $[50,2]$ and there's a variance-covariance matrix that looks something like this (all numbers are made up):
  $$\begin{array}{cc}
10 &amp; cov(a,b) \\
cov(a,b) &amp; 1 \\
\end{array}$$
  So, the variance of $a$ is 10, the variance of $b$ is 1, and their covariance is $cov(a,b)$. It won't be shown in your regression table, but Zelig remembers it for you. Let's pretend it's 3.
  This variance-covariance matrix is the inverse of the Hessian I mentioned earlier. Don't worry about it. For this example, you need only remember that $\text{mean}(a,b) = [50,2]$ and $cov(a,b)=\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. For purposes of plain text email, I'm representing matrices with columns separated by commas and rows by semicolons.
  In addition, suppose that the error term $e$ is distributed with mean 0 and s.d.=1.
  Now, one way to predict what IQ you might get for somebody aged 88, based on this regression table, is exactly what you would expect: you simply calculate 50 + 88 * 2 = 226. This is your point estimate. The 95% confidence interval around this point estimate is a function of the standard errors of the coefficient estimates of $a=50$ and $b=2$, and the exact formula for that is in any econometrics textbook.
  Simulation makes it unnecessary to dig up that textbook. Instead, for 1000 rounds, <code>sim()</code> will come up with 1000 different pairs of $(a,b)$ estimates drawn from the bivariate normal with mean=[50,2] and cov=$\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. One such pair might be $(47,1.5)$; another might be $(52,3)$; yet another might be $(10,5)$. 
  Whatever they are, <code>sim()</code> plugs them into the formula and gives you 1000 different estimates for the IQ. Their average is your point estimate. If you stack them from lowest to highest, the ends of the 95% confidence interval are the top 25th value and the bottom 25th. That's it. That's all that <code>sim()</code> does.</p>
</blockquote>

<p>Given the above explanation, can anybody tell me in lay terms, what numbers <code>sim()</code> is picking? How are those numbers in pool generated? I would greatly appreciate if anyone brings some light into this.</p>
"
"0.124848907203078","0.125362902407434"," 79216","<p><strong>Problem</strong>: When trying to calculate the variance of timeseries sums I get a negative variance, mostly due to autocovariances at large lag steps. Does not seem realistic.</p>

<p>I have a timeseries which is calculated from another timeseries using a regression equation.
I would like to propagate the uncertainty in the regression to the final timeseries. Then I want to sum (or take mean values) different segments of the timeseries over different timeperiods, and get the uncertainty of the sums. The timeseries is originally in 1 hour frequency and I want to sum over periods of 1 day (resampling to daily frequency) up to several years. The timeseries is strongly autocorrelated at short lag times.</p>

<p>For getting the variance of the sum (in the case of 3 elements being summed):
$$Var(a+b+c)= \\ Var(a)+Var(b)+Var(c) + 2 \times (Cov(a,b) + Cov(a,c)+Cov(b,c))$$</p>

<p>I use <code>r</code> for the calculations. I get the variances for each timeseries element as $SE^2$, where $SE$ is the standard error (<code>se.fit</code>) returned from r's <code>predict()</code> function using the regression model. The covariances I get from the autocovariance function <code>acf()</code>.</p>

<p>Here is some code and a selection of the data (excuse clumsy R code, I'm very new to R):</p>

<pre><code>#tsY is the predicted timeseries from the regression
tsY=c(81.4,  79.0,  83.4,   81.7,   75.7,   68.3,   62.3,   57.2,   52.6,   48.8,   45.4,   42.6,   39.9,   37.6,   35.6,   33.8,   32.2,   30.8,   29.6,   28.4,   27.3,   26.2,   25.0,   23.9)
#tsSE is the standard error from the prediction (se.fit)
tsSE=c(1.55,  1.49, 1.60,   1.56,   1.41,   1.23,   1.09,   0.97,   0.87,   0.78,   0.71,   0.65,   0.60,   0.55,   0.51,   0.48,   0.45,   0.42,   0.40,   0.38,   0.36,   0.34,   0.32,   0.30)

tsVar=tsSE^2

#create a matrix of the autocovariances at different lag times, diagonal is lag=0
#rows and columns are indicies in timeseries
covmat&lt;-matrix(numeric(0), length(tsY),length(tsY)) 
for ( i in (1:(length(tsY)) ) ) {
  if (i == 1) {
    autocov&lt;-acf(tsY, type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }  else {
    autocov&lt;-acf(tsY[-(1:i-1)], type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }

}

# sum the matrix columns, but not the diagonal
sumofColumns &lt;- rep(NA, ncol(covmat))
for (i in (1:ncol(covmat))) {
  if (i == 1) {
    sumofColumns[i]=sum(covmat[-(1),i])  
  } else{ 
    sumofColumns[i]=sum(covmat[-(1:i),i])  
  }
}

sumofCov=sum(sumofColumns) # sum of the covariance (Cov(a,b) + Cov(a,c)+...)
sumofVar=sum(tsVar) # sum of the variances of each timeseries element
varofSum=sumofVar+2*sumofCov # variance of the sum of the timeseries

# from the covmat the negative variance occurs at larger lag times.
acf(tsY, type='covariance', lag.max= length(tsY))

&gt; sumofCov
[1] -1151.529
&gt; varofSum
[1] -2283.246
</code></pre>

<p><strong>So I have the following questions:</strong></p>

<blockquote>
  <ol>
  <li><p>Did I completely misunderstand how to calculate variance of sums?</p></li>
  <li><p>Is it better to use a cutoff from the max lags to be considered in the autocovariance? If so how would one determine this? This would especially be important with the complete data where the length is several thousand. </p></li>
  </ol>
  
  <p><strike>3. Why is the covariance negative in this sample data at large? When plotting tsY  <code>plot(tsY)</code> it looks like the covariance/correlation should remain positive.</strike> Because it is the variation in direction from their means.</p>
</blockquote>

<p><strong>EDIT:</strong></p>

<blockquote>
  <p>Comment on <strong>question 2</strong> above:
  I have realized that using n-1 lags, as above in the code, does not make a lot of sense. There appear to be few different ways to determine the maximum lags to consider.  Box &amp; Jenkins (1970) suggest n/4 and R by default 10*log10(n). This does not answer the question however, of how to determine an appropriate cutoff for summing the covariances.</p>
  
  <p>Does it make sense to look at the partial autocorrelation (function pacf()), in order not to overestimate the effect of the auto covariance in the summation term? The partial autocorrelation for my data is significantly different from zero only at 1 or 2 lags. Similarly, fitting an AR model using ar() function, I also get an order of 1 or 2.</p>
</blockquote>

<p>Cheers</p>

<p>Related post <a href=""http://stats.stackexchange.com/questions/10943/variance-on-the-sum-of-predicted-values-from-a-mixed-effect-model-on-a-timeserie"">Variance on the sum of predicted values from a mixed effect model on a timeseries</a></p>
"
"0.0286064783845312","0.0287242494810713"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.0404556697031367","0.0406222231851194"," 80312","<p>I have carried out this linear regression that includes month coded as a dummy variable:</p>

<pre><code>library(plyr)
set.seed(1)
y &lt;- rnorm(120)
x1 &lt;- c(rep(""adult"", 60), rep(""juvenile"", 60))
x2 &lt;- c(rep(""male"", 60), rep(""female"", 60))
x3 &lt;- unlist(llply(month.abb, function(x) rep(x, 10)))

summary(lm(y ~ x1 + x2 + x3))

Call:
lm(formula = y ~ x1 + x2 + x3)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.46354 -0.51524 -0.03981  0.57625  1.95041 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.12073    0.28564   0.423    0.673
x1juvenile   0.00663    0.40396   0.016    0.987
x2male            NA         NA      NA       NA
x3Aug       -0.37510    0.40396  -0.929    0.355
x3Dec       -0.24718    0.40396  -0.612    0.542
x3Feb        0.12812    0.40396   0.317    0.752
x3Jan        0.01147    0.40396   0.028    0.977
x3Jul        0.32385    0.40396   0.802    0.424
x3Jun        0.02273    0.40396   0.056    0.955
x3Mar       -0.25440    0.40396  -0.630    0.530
x3May        0.01341    0.40396   0.033    0.974
x3Nov        0.22012    0.40396   0.545    0.587
x3Oct       -0.01502    0.40396  -0.037    0.970
x3Sep             NA         NA      NA       NA

Residual standard error: 0.9033 on 108 degrees of freedom
Multiple R-squared:  0.04703,   Adjusted R-squared:  -0.05003 
F-statistic: 0.4845 on 11 and 108 DF,  p-value: 0.9093
</code></pre>

<p>I now want to present the results of this linear regression within a table. Instead of presenting the beta for every month, is there a way to summarise the overall effect of month on <code>y</code> within the same table? For example, would if be acceptable to summarise the beta, se, t value and p value of <code>x3</code> by using their mean values across months?</p>
"
"0.0286064783845312","0.0287242494810713"," 80463","<p>I have the following set of model-averaged fixed effects from a set of binomial GLMMs: </p>

<p><img src=""http://i.stack.imgur.com/yN4wR.png"" alt=""model parameters image""></p>

<p>I would like to plot the predicted effect of ""NBT"", along with confidence bands, while holding all the other variables at their baseline levels. My attempt to do this in ggplot:</p>

<pre><code>Xvars &lt;- seq(from=0, to=100, by=0.1)  #NBT range is 0-100
  binomIntercept &lt;- 1.317
  binomSlope &lt;- -0.0076     
  binomSE &lt;- 0.009    
Means &lt;- logistic(binomIntercept + binomSlope*Xvars)              
loCI &lt;- logistic(binomIntercept + (binomSlope - 1.96*binomSE)*Xvars)
upCI &lt;- logistic(binomIntercept + (binomSlope + 1.96*binomSE)*Xvars)
df &lt;- data.frame(Xvars,Means,loCI,upCI)
p &lt;- ggplot(data=df, aes(x = Xvars, y = Means)) + 
geom_line() +          
geom_line(data=df, aes(x = Xvars, y = upCI),col='grey') +
geom_line(data=df, aes(x = Xvars, y = loCI), col='grey')
p                                            
</code></pre>

<p><img src=""http://i.imgur.com/eMJBxQQ.png"" alt=""graph image""></p>

<p>I'm assuming that the confidence bands are cone shaped because I'm not accounting for uncertainty in the estimate for the intercept. Maybe this is okay (?), but it does look different from every regression line I've ever seen with confidence intervals plotted.</p>

<p>Can someone please tell me how I should be writing my equations to get the correct confidence intervals, given the intercept, slope, and standard errors from my model output?</p>

<p>(I know I can use the predict function to do this in R, but would like to know how to do it by hand.)  </p>
"
"0.06396603026469","0.0642293744423385"," 80880","<p>I have seen several articles and CrossValidated questions on bootstrapping ( <a href=""http://stats.stackexchange.com/questions/41625/can-i-use-boostrapping-why-or-why-not"">this</a>, <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">this</a> or <a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients"">this</a> for example); there are a lot of theoretical and statistical explanations, however since they are so theory based, I am afraid I might be understanding the use wrongly. Hence my questions:</p>

<p>1) When I make a non-parametric bootstrapping (changing the sample for every run) with logistic regression on my data, I basically will end up with several different coefficients for each predictor for each run. Eventually I'll have the confidence interval for each predictor as well. I understand until that point. My question is; assuming that the distribution is normal, when I want to come up with a final model on practice, can I just take the mean of the confidence intervals for each predictor and consider this as my final model coefficient?</p>

<p>2) If the answer to question #1 is yes, is this the only way of choosing coefficients while bootstrapping? If not, what else? I encountered in a few more articles a method called ""bagging"". This seems to be my main purpose. </p>

<p>3) This one is more of a curiosity question: Can above methodology be applied to the categorical predictors when they are assigned with Weight Of Evidences? I know we can split the  categorical predictors into dummy variables; but how would I treat each coefficient if I want to use WOE methodology?</p>
"
"0.0429097175767967","0.0574484989621426"," 80955","<p>Is there anyway that I can perform LASSO with Negative Binomial Regression on R?
I am performing a negative binomial regression on my dataset because the data are too dispersed to impose poisson regression. Meanwhile, I am also facing some multicollinearity problem. I already tried using <code>glmnet</code> with <code>family = poisson</code>, but the data is not fitting very well (for both alpha = 0 and alpha = 1)...I honestly don't know what to do to analyze this big mess of data :/</p>

<p>thank you</p>

<p>EDIT: here is variance-covariance table of the negative binomial fit</p>

<pre><code>       8.392729e+18  1.239178e+06  -3.624090e+05  1.896258e+17  -3.702521e+17
       1.239178e+06  1.119052e-04   5.201989e-06 -1.877590e+05  -2.558095e+05
      -3.624090e+05  5.201989e-06   5.179343e-06 -8.021543e+04  -1.436381e+05
       1.896258e+17 -1.877590e+05  -8.021543e+04  2.193290e+17   6.413947e+16
      -3.702521e+17 -2.558095e+05  -1.436381e+05  6.413947e+16   2.142183e+17
</code></pre>
"
"0.0990957479752576","0.0995037190209989"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0404556697031367","0.0406222231851194"," 81774","<p>I want to perform linear regression using the following command:</p>

<p><code>lm(clinicVisits ~ clinicRank)</code> where <code>clinicVisits</code> is the number of visits to a clinic, and <code>clinicRank</code> is the rank of the clinic. </p>

<p><code>clinicVisits</code> is an integer in the range of 1â€“200K, and clinicRank is a continuous variable and has a range of  1.0â€“150.0 where 1.0 is the best ranking. </p>

<p>I am trying to predict what an increase in rank will mean in terms of number of visits, as I believe there is a relationship between the two, so I am seeking this linear regression equation:</p>

<pre><code>clinicVisits = y intercept + clinicRank*(b1)
</code></pre>

<p>I also assume that <code>b1</code> will be an increasingly negative number as the <code>clinicRank</code> gets smaller (i.e. closer to 1, the best rank). </p>

<p>Do I need to apply a scale to the variables since the <code>clinicRank</code> is relatively small compared to visits, or a log to the <code>clinicVisits</code> since it has a relative large range?</p>
"
"0.0762839423587497","0.0765979986161901"," 81938","<p>This is an R question.  I have $n$ observations of the variable $y$, with each observation, $i$, weighted by $w(i)$.  Each $w(i)$ weight falls between zero and 1, with the sum of the weights, $m&lt;n$. I am testing whether the weighted mean of $y$ equals zero, with $std(mean)$ denoting the weighted sample standard deviation of the mean, computed using <code>cov.wt</code> and dividing by <code>sqrt(m)</code>.</p>

<p>At a later time, I would like to run a weighted regression of $y$ against some explanatory variables.  However, as an intermediate step, I have run the regression <code>lm(y~1,weights=w)</code>, which returns an intercept equal to the weighted mean of $y$.  However, the standard error of the intercept is based on $n-1$ degress of freedom and, therefore, is smaller than the standard deviation of the weighted sample standard deviation of the mean, computed using <code>cov.wt</code> and dividing by <code>sqrt(m)</code>.</p>

<p>Is there a way to run a regression in R in which the degrees of freedom in the simple regression above would be $m-1$ rather than $n-1$?</p>

<p>Note, I have $m$ ""cases"" of the variable $y$.  In most cases, there is only one observation.  But if there were two observations in a given case, I would give each a weight of $1/2$, if three observations in a case, each would get a weight of $1/3$, etc.</p>
"
"0.0404556697031367","0.0406222231851194"," 82974","<p>Suppose I have a bivariate responses with significant correlation. I am trying to compare the two ways to model these outcomes. One way is to model the difference between the two outcomes: 
$$(y_{i2}-y_{i1}=\beta_0+X'\beta)$$
Another way is to use <code>gls</code> or <code>gee</code> to model them:
$$(y_{ij}=\beta_0+\text{time}+X'\beta)$$</p>

<p>Here is a foo example:</p>

<pre><code>#create foo data frame

require(mvtnorm)
require(reshape)
set.seed(123456)
sigma &lt;- matrix(c(4,2,2,3), ncol=2)
y &lt;- rmvnorm(n=500, mean=c(1,2), sigma=sigma)
cor(y)
x1&lt;-rnorm(500)
x2&lt;-rbinom(500,1,0.4)
df.wide&lt;-data.frame(id=seq(1,500,1),y1=y[,1],y2=y[,2],x1,x2)
df.long&lt;-reshape(df.wide,idvar=""id"",varying=list(2:3),v.names=""y"",direction=""long"")
df.long&lt;-df.long[order(df.long$id),]
    df.wide$diff_y&lt;-df.wide$y2-df.wide$y1


#regressions
fit1&lt;-lm(diff_y~x1+x2,data=df.wide)
fit2&lt;-lm(y~time+x1+x2,data=df.long)
fit3&lt;-gls(y~time+x1+x2,data=df.long, correlation = corAR1(form = ~ 1 | time))
</code></pre>

<p>What's the fundamental difference between <code>fit1</code> and <code>fit2</code>? And between <code>fit2</code> and <code>fit3</code>, given they are so close on the $p$ values and estimates? </p>
"
"0.064873395163555","0.0759972207238908"," 83400","<p>I have response variable count data that should be treated as quasipoisson or something similar.  This data also contains outliers which are important to the dataset.  I cannot find an r package that will let me do robust regression with quasipoisson-type distribution.  I could, however, collect weights for each model using robust regression and then try to use those weights in glm.  Note those weights are always > 0.  My problem is that I cannot find a clear definition of how weights are used in glm with family=quasipoisson.  It sounds similar but I can't figure out if actually does what I want it to do (e.g. downweight outlier response variables to decrease their impact).</p>

<p>The response data are numbers of moths caught daily at pheromone traps, and the predictor variables are weather data.  All variables are diffed to handle temporal autocorrelation.  The outliers are high numbers of migratory moths in response to cold front passages.</p>

<p>I just found this reference which explains that low weights are interpreted as representing observations with a high variance, which does make sense in my case.  But does that mean the effect is to reduce the impact of that observation on the regression? <a href=""http://r.789695.n4.nabble.com/weights-in-glm-PR-8720-td910336.html"" rel=""nofollow"">http://r.789695.n4.nabble.com/weights-in-glm-PR-8720-td910336.html</a></p>
"
"0.0286064783845312","0.0287242494810713"," 83554","<p>Sorry for this beginner's question... I have googled this for a while with no success.</p>

<p>I do a linear regression using R lm function:</p>

<pre><code>x = log(errors)
plot(x,y)
lm.result = lm(formula = y ~ x)
abline(lm.result, col=""blue"") # showing the ""fit"" in blue
</code></pre>

<p><img src=""http://i.stack.imgur.com/2p7hZ.png"" alt=""enter image description here""></p>

<p>but it does not fit well. Unfortunately I can't make sense of the manual.</p>

<p>Can someone point me in the right direction to fit this better?</p>

<p>By fitting I mean I want to minimize the Root Mean Squared Error (RMSE).</p>

<hr>

<p><strong>Edit</strong>:
I have posted a related question (it's the same problem) here:
<a href=""http://stats.stackexchange.com/questions/83576/can-i-decrease-further-the-rmse-based-on-this-feature"">Can I decrease further the RMSE based on this feature?</a></p>

<p>and the raw data here:</p>

<p><a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>except that on that <a href=""http://tny.cz/c320180d"" rel=""nofollow"">link</a> x is what is called errors on the present page here, and there are less samples (1000 vs 3000 in the present page plot). I wanted to make things simpler in the other question.</p>
"
"0.0572129567690623","0.043086374221607"," 83576","<p>I have a feature x, that I use to predict a probability y.</p>

<hr>

<p><strong>Some background on (x,y)</strong></p>

<p>I can't go into too much details, but hopefully the following should be enough to explain what x and y are, at least conceptually <em>[square and circles are NOT the actual label I am working with]</em>:</p>

<p><strong>y</strong></p>

<p>y is the probability of an image being of Class 0 or 1, with: </p>

<ul>
<li>Class 0 means that the image contains a <em>square</em>.</li>
<li>Class 1 means that the image contains a <em>circle</em>.</li>
</ul>

<p>100 people watched the training images, and classified them.
y is the result probability, so y=0 means there is definitely a square, y=1 means there is definitely a round.</p>

<p><strong>x</strong></p>

<p>x is a feature derived from the images, by <em>trying to fit them to a model of a circle</em>, and calculating the error.
So for example when x is very low, the probability of the image having a circle is high (relatively).</p>

<hr>

<p>plot(x,y)</p>

<p><img src=""http://i.stack.imgur.com/05230.png"" alt=""enter image description here""></p>

<p>x,y (1000 values for each) pasted here:
<a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>Using mean(y) as a predictor, I get <strong>RMSE = 0.285204</strong>:</p>

<pre><code>N = length(x)
average = mean(y)
RMSE = sqrt( 1/N * sum( (average-y)^2 ) )
RMSE
[1] 0.285204
</code></pre>

<p>Then using a linear regression on log(x), I could improve a little bit the <strong>RMSE = 0.2694513</strong>:</p>

<pre><code>log_x = log(x)
plot(log_x,y)
lm.result = lm(formula = y ~ log_x)
abline(lm.result, col=""blue"") # not working very well
linear_prediction = predict( lm.result, new, se.fit = TRUE)
prediction_linear_regression = matrix(0,N,1)
prediction_linear_regression = linear_prediction$fit
RMSE_linear_regression = sqrt( 1/N * sum( (prediction_linear_regression-y)^2 ) )
RMSE_linear_regression
[1] 0.2694513
</code></pre>

<p><img src=""http://i.stack.imgur.com/59Etc.png"" alt=""enter image description here""></p>

<p>Can the RMSE be further improved? What should I try?</p>
"
"0.0990957479752576","0.0995037190209989"," 83826","<p>I estimated a robust linear model in <code>R</code> with MM weights using the <code>rlm()</code> in the MASS package. `R`` does not provide an $R^2$ value for the model, but I would like to have one if it is a meaningful quantity. I am also interested to know if there is any meaning in having an $R^2$ value that weighs the total and residual variance in the same way that observations were weighted in the robust regression. My general thinking is that, if, for the purposes of the regression, we are essentially with the weights giving some of the estimates less influence because they are outliers in some way, then maybe for the purpose of calculating $r^2$ we should also give those same estimates less influence? </p>

<p>I wrote two simple functions for the $R^2$ and the weighted $R^2$, they are below. I also included the results of running these functions for my model which is called HI9.  EDIT: I found web page of Adelle Coster of UNSW that gives a formula for <code>R2</code> that includes the weights vector in calculating the calculation of both <code>SSe</code> and <code>SSt</code> just as I did, and asked her for a more formal reference: <a href=""http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html"">http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html</a> (still looking for help from Cross Validated on how to interpret this weighted $r^2$.)</p>

<pre><code>#I used this function to calculate a basic r-squared from the robust linear model
r2 &lt;- function(x){  
+ SSe &lt;- sum((x$resid)^2);  
+ observed &lt;- x$resid+x$fitted;  
+ SSt &lt;- sum((observed-mean(observed))^2);  
+ value &lt;- 1-SSe/SSt;  
+ return(value);  
+ }  
r2(HI9)  
[1] 0.2061147

#I used this function to calculate a weighted r-squared from the robust linear model
&gt; r2ww &lt;- function(x){
+ SSe &lt;- sum((x$w*x$resid)^2); #the residual sum of squares is weighted
+ observed &lt;- x$resid+x$fitted;
+ SSt &lt;- sum((x$w*(observed-mean(observed)))^2); #the total sum of squares is weighted      
+ value &lt;- 1-SSe/SSt;
+ return(value);
+ }
 &gt; r2ww(HI9)
[1] 0.7716264
</code></pre>

<p>Thanks to anyone who spends time answering this. Please accept my apologies if there is already some very good reference on this which I missed, or if my code above is hard to read (I am not a code guy).</p>
"
"0.0707974219804893","0.0812444463702388"," 85913","<p>I want to fit a DLM with time-varying coefficients, i.e. an extension to the usual linear regression,</p>

<p>$y_t = \theta_1 + \theta_2x_2$.</p>

<p>I have a predictor ($x_2$) and a response variable ($y_t$), marine &amp; inland annual fish catches respectively from 1950 - 2011. I want the DLM regression model to follow,</p>

<p>$y_t = \theta_{t,1} + \theta_{t,2}x_t$</p>

<p>where the system evolution equation is</p>

<p>$\theta_t = G_t \theta_{t-1}$</p>

<p>from page 43 of Dynamic Linear Models With R by Petris et al.</p>

<p>Some coding here,</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- fishdata$marinefao
    y &lt;- fishdata$inlandfao

lmodel &lt;- lm(y ~ x)
summary(lmodel)
plot(x, y)
abline(lmodel)
</code></pre>

<p>Clearly time-varying coefficients of the regression model are more appropriate here. I follow his example from pages 121 - 125 and want to apply this to my own data. This is the coding from the example</p>

<pre><code>############ PAGE 123
require(dlm)

capm &lt;- read.table(""http://shazam.econ.ubc.ca/intro/P.txt"", header=T)
capm.ts &lt;- ts(capm, start = c(1978, 1), frequency = 12)
colnames(capm)
plot(capm.ts)
IBM &lt;- capm.ts[, ""IBM""]  - capm.ts[, ""RKFREE""]
x &lt;- capm.ts[, ""MARKET""] - capm.ts[, ""RKFREE""]
x
plot(x)
outLM &lt;- lm(IBM ~ x)
outLM$coef
    acf(outLM$res)
qqnorm(outLM$res)
    sig &lt;- var(outLM$res)
sig

mod &lt;- dlmModReg(x,dV = sig, m0 = c(0, 1.5), C0 = diag(c(1e+07, 1)))
outF &lt;- dlmFilter(IBM, mod)
outF$m
    plot(outF$m)
outF$m[ 1 + length(IBM), ]

########## PAGES 124-125
buildCapm &lt;- function(u){
  dlmModReg(x, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(IBM, parm = rep(0,3), buildCapm)
exp(outMLE$par)
    outMLE
    outMLE$value
mod &lt;- buildCapm(outMLE$par)
    outS &lt;- dlmSmooth(IBM, mod)
    plot(dropFirst(outS$s))
outS$s
</code></pre>

<p>I want to be able to plot the smoothing estimates <code>plot(dropFirst(outS$s))</code> for my own data, which I'm having trouble executing. </p>

<p><strong>UPDATE</strong></p>

<p>I can now produce these plots but I don't think they are correct.</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- as.numeric(fishdata$marinefao)
    y &lt;- as.numeric(fishdata$inlandfao)
xts &lt;- ts(x, start=c(1950,1), frequency=1)
xts
yts &lt;- ts(y, start=c(1950,1), frequency=1)
yts

lmodel &lt;- lm(yts ~ xts)
#################################################
require(dlm)
    buildCapm &lt;- function(u){
  dlmModReg(xts, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(yts, parm = rep(0,3), buildCapm)
exp(outMLE$par)
        outMLE$value
mod &lt;- buildCapm(outMLE$par)
        outS &lt;- dlmSmooth(yts, mod)
        plot(dropFirst(outS$s))

&gt; summary(outS$s); lmodel$coef
       V1              V2       
 Min.   :87.67   Min.   :1.445  
 1st Qu.:87.67   1st Qu.:1.924  
 Median :87.67   Median :3.803  
 Mean   :87.67   Mean   :4.084  
 3rd Qu.:87.67   3rd Qu.:6.244  
 Max.   :87.67   Max.   :7.853  
 (Intercept)          xts 
273858.30308      1.22505 
</code></pre>

<p>The intercept smoothing estimate (V1) is far from the lm regression coefficient. I assume they should be nearer to each other. </p>
"
"0.0495478739876288","0.0497518595104995"," 86057","<p>Here's <a href=""http://stats.stackexchange.com/questions/125/what-is-the-best-introductory-bayesian-statistics-textbook"">a link</a> to a good question regarding Textbooks on Bayesian statistics from some time ago.</p>

<p>People suggested John Kruschke's ""Doing Bayesian Data Analysis: A Tutorial Introduction with R and BUGS"" as one of the best options to get an introduction to Bayesian statistics.
Meanwhile, a potentially interesting book called ""Bayesian and Frequentist Regression Methods"" by Jon Wakefield was released, which also provides code for R and BUGS. 
Thus, they esentially both seem to cover the same topics.</p>

<p>Question 1: If you have read the book, would you recommend it to a frequentist economics masters graduate as both an introduction to Bayesian statstics and reference book for both frequentist and bayesian approaches?</p>

<p>Question 2: If you have read both Wakefield's and Kruschke's book, which one would you recommend better?</p>
"
"0.0572129567690623","0.0574484989621426"," 87071","<p>I am looking to use a mathematical model developed by Firbank &amp; Watkinson (1985) J. App. Ecol. 22:503-517 for the analysis of competition between plants grown in mixture.</p>

<p>The model is as follows:</p>

<p>$$W_{A}=W_{mA}\left(1 + a_{A}\left(N_{A}+\alpha N_{B}\right)\right)^{-b_{A}}$$</p>

<p>where $W_{A}$ is the mean yield per plant of species $A$ grown in the experiment, $W_{mA}$ is the mean yield of isolated plants of species $A$, $a_{A}$ is the surface area required to reach size $W_{mA}$, $N_{A}$ is the planting density of species $A$, $\alpha$ is the competition coefficient, and $-b_{A}$ is the 'resource use efficiency' parameter. </p>

<p>The model is a regression model as I understand it. I have data for density of species $A$ and $B$ and ($N_{A}$ and $N_{B}$) as well as the response variable $W_{A}$. I am unsure how I can use R to estimate the remaining values, most important of which is the competition coefficient, $\alpha$. If there is any more information that I need to provide please let me know.</p>
"
"0.11794753195637","0.11843311462705"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.051172824211752","0.0642293744423385"," 87578","<p>I am estimating cross-sectional regressions - fragment:</p>

<blockquote>
  <p>lm(rate~liqamih.log+cap.log+F1+F2, data=x)</p>
</blockquote>

<p>of the R code listed below.</p>

<p>F1 and F2 are the coefficients estimates of time series model.</p>

<p>In such case we need to deal with so called ""error in variables problem"". In literature (links: gendocs.ru/docs/23/22031/conv_1/file1.pdfâ€Ž (page 1091) and papers.ssrn.com/sol3/papers.cfm?abstract_id=6992 (whole article)) the MLE method is one of the effective solution of this problem.</p>

<p>I would like to implement method introduced by KIM (reference to literature-links above) in my R code below. HOW TO DO THAT ?</p>

<pre><code>data&lt;-read.table(""reg5-dane.csv"", head=T, sep="";"", dec="","")
  data$indx &lt;- as.numeric(gl(123*334,334,123*334))
lst1 &lt;- split(data[,-53],data[,53]) #max 53 variables in ""lst2"" regression
any(sapply(lst1,nrow)!=123)
#[1] FALSE
lst2 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) summary(lm(rate~liqamih.log+cap.log+F1+F2, data=x)) )
capture.output(lst2,file=""nooldor_regr_summ.txt"")
# - f2 -f6 /+f2 -f5 +F6 - f6 - f2 - liq + f6 - f6 +f2 - f4 - f4 +f3

capture.output(lst2,file=""nooldor_regr_summ.csv"")
# HERE - above- IS THE CORE / most important part of question ... for now you can skip what is below 
f4 &lt;- function(meanmod, dta, varmod) {
  assign("".dta"", dta, envir=.GlobalEnv)
  assign("".meanmod"", meanmod, envir=.GlobalEnv)
  m1 &lt;- lm(.meanmod, .dta)
  ans &lt;- ncvTest(m1, varmod)
  remove("".dta"", envir=.GlobalEnv)
  remove("".meanmod"", envir=.GlobalEnv)
  ans
}
library(car)
lst3 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) f4(rate~cap.log, x))
lst4 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) durbinWatsonTest(lm(rate~cap.log, x)))
lst5 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) vif(lm(rate~cap.log, x)))

res5 &lt;- do.call(rbind,lst5)
res4 &lt;- do.call(rbind,lapply(lst4,function(x) unlist(x[-4])))
library(tseries)

res6 &lt;- do.call(rbind,lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],function(x) {resid &lt;- residuals(lm(rate~.,data=x)); unlist(jarque.bera.test(resid)[1:3])}) )

capture.output(lst2,file=""nooldor_regr_summ.txt"")
capture.output(lst3,file=""nooldor_arch_test.txt"")
capture.output(lst4,file=""nooldor_durbin.txt"")
capture.output(lst5,file=""nooldor_vif.txt"")
</code></pre>

<p>There is 123 time observations on 334 subjects observations. For each of 123 time points I am running one regressions with 334 subjects (so I am repeating it 123 times for each point of time).
I post this topic also in stackoverflow.com - because I need help from one who have strong background in statistics/econometrics and also in R programming.
I would appreciate your valuable help.
Thank you.</p>
"
"0.0700712753800578","0.0703597544730292"," 87589","<p>I'm a manufacturing engineer trying to resolve an issue regarding non fitment of parts. There are a couple of components which have dimensions x1 to x6 (data below). Upon assembly, they form sub-assemblies with resulting dimensions y1 and y2. y1 and y2, in turn, determine whether the parts fit or don't fit, which is indicated in the ""fit"" column by ""OK"" or ""NOK"".</p>

<p>I am looking for some help in trying to predict which parts would fit (OK) or not fit (NOK), first by considering at dimensions y1 and y2 as the predictors, and then by using x1 to x6 as predictors.</p>

<p>I have started learning R for basic statistical analyses but am not familiar with how to use predictive modeling tools (like boosted regression).</p>

<p>I request the forum to kindly help in: 1. suggesting the best predictive model for the issue 2. writing some R code that will help me predict part fitment.</p>

<p>Thank you so much. Please note that the data set is very small (10 data points), we are in the process of collecting more and plan to have atleast 30. But meanwhile, I want to understand the approach in resolving the problem.</p>

<pre><code>#independent variables
x1&lt;-c(380.57,379.49,381.69,381.14,380.51,380.69,381.6,380.03,381.39,381.5)
x2&lt;-c(414.08,413.43,412.64,412.29,413.44,413.49,412.89,412.95,412.2,411.91)
x3&lt;-c(15.31,15.52,14.84,14.69,15.35,14.92,15.17,15.73,14.6,16.07)
x4&lt;-c(14.64,14.78,15.13,13.84,14.76,13.66,14.55,15.96,14.83,15.01)
x5&lt;-c(15.23,16.48,15.15,15.24,14.64,16.03,16.18,16.01,14.78,15.37)
x6&lt;-c(14.64,16.78,15.16,15.88,15.63,16.2,15.79,15.89,15.91,16.8)

#y1 and y2 depend on x1..x6
y1&lt;-c(378.06,377.14,381.21,379.11,379.53,378.74,379.63,377.92,381.11,379.15)
y2&lt;-c(378.77,378.01,378.76,375.96,375.55,378.94,378.67,377.04,376.96,379.07)

#fit depends on y1 and y2
fit&lt;-c(""OK"",""NOK"",""NOK"",""NOK"",""NOK"",""OK"",""OK"",""NOK"",""NOK"",""OK"")

f1&lt;-data.frame(x1,x2,x3,x4,x5,x6,y1,y2,fit)
rm(x1,x2,x3,x4,x5,x6,y1,y2,fit)
</code></pre>
"
"0.0908377689773195","0.0995037190209989"," 87608","<p>I have run a few tests/methods on my data and am getting contradictory results.</p>

<p>I have a linear model saying:
reg1 = lm(weight = height + age + gender (categorical) + several other variables). </p>

<p>If I model each term linearly i.e. no squared or interaction term, and run vif(reg1), 4 variables are >15. If I delete the variable with the highest vif number and re-run it the gifs change and now only 2 variables are >15. I repeat this until I'm left with 20 variables (out of 30) below 10. If I use stepwise directly on reg1 then it does not delete the 'highest vic' factor. <strong>I don't understand how it tells me 'what' is linearly dependant on 'what variable' and how (and I cannot seem to find this information despite googling for ages).</strong> </p>

<p>Furthermore, when I look at the residual plots, most appear horizontal except a few which are upside down u curved (none of these have high vifs). Does this means a transformation is needed? (I removed outliers, leverage points etc - but now there seem to be more!)</p>

<p>reg2 = lm(weight = (height + age + gender (categorical) + several other variables)^2). </p>

<p>If I run vif on this all of the terms are >500! </p>

<p>What else I have tried (without cutting any variables): 
(1) The errors seem correlated when i run diagnostics and check with Durbin Waston statistics indicating the model is not linear... however...
(2) Box Cox gives lambda = 1 so no transformation is needed.
(3) LASSO gives the lowest mallows cp on the full 30 variable model (i.e. least squares)
(4) Ridge regression gives lambda = 0 which did surprise me. </p>

<p>I'm getting really confused about this data. <strong>To determine a suitable model for weight should I be looking just at linear terms or linear and interaction terms (remember there are 25 variables so there are 30^2 interaction terms)?</strong> </p>

<p>When I check which ones are significant in reg2 only 12 predictors and 6 interaction terms seem significant (AIC is lowest with this combination after I run step). <strong>Should I just use this 'new model with deleted variables/interaction terms' and do all my tests e.g. stepwise method, LASSO etc or do I do it on the entire model?</strong> </p>

<p>I'm getting quite lost in terms of making sense of steps to find a suitable model for weight using the variables. </p>

<p><strong>My final question is once I have the model - how do i test/prove its the best/a decent model?</strong> </p>

<p>Any help would really be appreciated. </p>
"
"0.0756856276908142","0.0759972207238908"," 87872","<p>When I do a (logistic) regression in R, I run something like this:</p>

<pre><code>mydata &lt;- read.csv(""data.csv"")
mylogit &lt;- glm(a ~ c+d, data = mydata, family=""binomial"")
summary(mylogit)
</code></pre>

<p>As of a few months ago, the output for the coefficients might look like this:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6476     0.1898  -8.680  &lt; 2e-16 ***
c             2.4558     0.3414   7.194 6.29e-13 ***
d             2.3783     0.4466   5.326 1.01e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Trying it today (with a newer version of R), the output looks like the following:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6709     0.1924  -8.683  &lt; 2e-16 ***
c1            2.4961     0.3476   7.181 6.94e-13 ***
cc           18.2370   979.6100   0.019    0.985    
d1            2.4524     0.4630   5.296 1.18e-07 ***
dd                NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the ""c1"", ""cc"", etc fields mean?  I can't seem to find this any documentation, but perhaps I am looking in the wrong places?</p>
"
"0.0572129567690623","0.0574484989621426"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.0495478739876288","0.0497518595104995"," 88036","<p>I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).</p>

<ol>
<li><p>I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  </p>

<p><code>output &lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)</code><br>
RE is a factor with several levels.</p>

<p>This works. But I'm wondering why it's necessary to use the <code>(1 | RE)</code> syntax instead of just <code>DV~IV1+IV2 | RE</code>.</p></li>
<li><p>I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using <code>ranef()</code>. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?</p></li>
</ol>
"
"0.0756856276908142","0.0759972207238908"," 88880","<p>I performed principal component analysis (PCA) with R using two different functions (<code>prcomp</code> and <code>princomp</code>) and observed that the PCA scores differed in sign. How can it be?</p>

<p>Consider this:</p>

<pre><code>set.seed(999)
prcomp(data.frame(1:10,rnorm(10)))$x

            PC1        PC2
 [1,] -4.508620 -0.2567655
 [2,] -3.373772 -1.1369417
 [3,] -2.679669  1.0903445
 [4,] -1.615837  0.7108631
 [5,] -0.548879  0.3093389
 [6,]  0.481756  0.1639112
 [7,]  1.656178 -0.9952875
 [8,]  2.560345 -0.2490548
 [9,]  3.508442  0.1874520
[10,]  4.520055  0.1761397

set.seed(999)
princomp(data.frame(1:10,rnorm(10)))$scores
         Comp.1     Comp.2
 [1,]  4.508620  0.2567655
 [2,]  3.373772  1.1369417
 [3,]  2.679669 -1.0903445
 [4,]  1.615837 -0.7108631
 [5,]  0.548879 -0.3093389
 [6,] -0.481756 -0.1639112
 [7,] -1.656178  0.9952875
 [8,] -2.560345  0.2490548
 [9,] -3.508442 -0.1874520
[10,] -4.520055 -0.1761397
</code></pre>

<p>Why do the signs (<code>+/-</code>) differ for the two analyses? If I was then using principal components <code>PC1</code> and <code>PC2</code> as predictors in a regression, i.e. <code>lm(y ~ PC1 + PC2)</code>, this would completely change my understanding of the effect of the two variables on <code>y</code> depending on which method I used! How could I then say that <code>PC1</code> has e.g. a positive effect on <code>y</code> and <code>PC2</code> has e.g. a negative effect on <code>y</code>?</p>

<hr>

<p><strong>In addition:</strong> If the sign of PCA components is meaningless, is this true for factor analysis (FA) as well? Is it acceptable to flip (reverse) the sign of individual PCA/FA component scores (or of loadings, as a column of loading matrix)?</p>
"
"NaN","NaN"," 89172","<p>I understand the concept of scaling the data matrix to use in a linear regression model. For example, in R you could use: </p>

<pre><code>scaled.data &lt;- scale(data, scale=TRUE)
</code></pre>

<p>My only question is, for new observations for which I want to predict the output values, how are they correctly scaled? Would it be, <code>scaled.new &lt;- (new - mean(data)) / std(data)</code>?</p>
"
"0.12136700910941","0.121866669555358"," 89510","<p>I have a dataset that features a binary outcome, a binary predictor, and an unordered factor with 7 levels, and 120 subjects. Each of the 120 subjects were asked a binary question on seven issues, hence the 7-level unordered factor. Therefore, I have a total of 840 observations. Each of the 840 observations has a binary predictor, and a binary outcome. I want to regress the binary outcome, on the binary predictor of course.</p>

<p>The outcome is slightly complicated. I will use mac vs. PCs as an example. People are asked, ""Do you think macs are better or do you think PCs are better?"" That is recorded as a 1, for preferring macs, and 0 for preferring PCs. Then they are asked ""Is your opinion based on objective truth, or subjective opinion?"" In other words, do you think it is objective (1) or subjective (0). Then the last question is ""In 20 years, will people think macs are better or will people think PCs are better?"" Again, 1 will be coded for macs. A new variable is then created. The variable is 1 if their current opinion matches what they think the future opinion will be. Previous pilot studies found that people tend to believe their stance on issues is what the future will be (...I know, a pretty intuitive finding...people tend to think they are right). The outcome is that variable, whether their current opinion agrees with what they think the future will be. So the point is to see if how people view things as subjective vs. objective, predicts whether they think their vision of the future is in line with their beliefs. The mac vs PC is only an example of an issue. In reality, there are seven issues, ranging from gun control, to abortion, etc., and each of those seven issue ranges in ""objectivity"" given by how the participants answered the objectivity question.</p>

<ul>
<li><code>Participant ID</code>: 120 Factors </li>
<li><code>Objectivity</code>: Binary predictor of 0s and 1s </li>
<li><code>Outcome</code>: Binary outcome of 0s and 1s </li>
<li><code>Issue</code>: 7-factor level variable </li>
</ul>

<p>So I tried something like this:</p>

<pre><code>  # Dummy coding for all participants
contrasts(data$Participant) &lt;- contr.treatment(120)  #$
  # Deviance coding to compare to the overall mean
contrasts(data$Issue)       &lt;- contr.sum(7) 
model &lt;- glmer(Outcome ~ Objectivity*Issue + (Objectivity*Issue|Participant), 
               family=binomial, data=data)
</code></pre>

<p>This gives a bunch of warnings. I essentially want to see if my predictor predicts the outcome, based on the seven issue, while adding a random effect for the participant ID. </p>
"
"0.10703564115707","0.107476300250388"," 89760","<p>I am trying to use R to find the optimal solution for my problem with positive coefficients. Here are my data:  </p>

<pre><code>      th inp      tcyc        tinst     tmem      tcom
  1   2   2  26219765385  1975872868  52449810   782964
  2   2   4  38080459431  3155342008  76744867  1878903
  3   2   8  64572439641  6230494010 137754355  4351706
  4   2  16 140168021516 13757989992 285524252 10605705
  5   2  32 308925389816 31497131498 628391048 26040711
  6   4   2  13206650786   988226883  25631315   844126
  7   4   4  19078145632  1577873809  37085281  2125333
  8   4   8  33742095874  3114415906  65962626  5222236
  9   4  16  70956149286  6881357755 134957687 12180392
  10  4  32 153411672670 15754506070 296548768 31057252
  11  8   2   6572843040   494094967  12380740   808816
  12  8   4   9452222628   788984621  17538152  2034061
  13  8   8  16765943294  1557329849  30549900  5016827
  14  8  16  34677550217  3440679505  61614420 12493699
  15  8  32  74852648112  7876116794 133525620 29824686
  16 16   2   3252373719   247026385   5958559   672396
  17 16   4   4669800482   394452497   8097991  1676579
  18 16   8   8269859136   778889584  13651458  4196829
  19 16  16  16353025378  1720301596  26775255 10393194
  20 16  32  37113657641  3938965759  55505822 25011009
  21 32   2   1630888153   123512114   2683400   461526
  22 32   4   2293598746   197173135   3682504  1213596
  23 32   8   4045995970   389408822   5858031  3055324
  24 32  16   8217603991   860041282  10973460  7502244
  25 32  32  17978101850  1969647650  22909347 17953100
  26 48   2   1064344042    82295143   1822133   381178
  27 48   4   1523091067   131488491   2331228   949354
  28 48   8   2677097592   259536252   3552229  2381626
  29 48  16   5400541381   573140686   6489032  5875310
  30 48  32  11837404077  1313066425  13318331 13968230
</code></pre>

<p>I use linear regression in R, <code>s &lt;- lm(tcyc ~ 0+tinst+tmem+tcom, data=fit)</code>, to get the optimal value with intercept 0. But I get negative coefficients which does not make any sense.</p>

<pre><code>coef(s)

 tinst      tmem      tcom 
20.8745 -281.2288 -320.7204 
</code></pre>

<p>I am not sure whether is it the best way to model and find the optimal parameter for <code>tinst</code>, <code>tmem</code> and <code>tcom</code>. How do you find positive coefficients for the model?</p>

<p>Further explaining this problem in Detail:::</p>

<p>Background:
Trying to predict the execution time of an application in the future many-core systems empirically by learning the application behavior. As it is a multithreaded program, it will have communication contnention bottleneck if the application demands high inter-core communication. The general system equation looks like</p>

<p>Total executiong time cycles (T_cyc) = Total cycles spent in Instruction (T_inst) + Total cycle spent in Memory instructions (T_mem) + Total cycle spent in Communication (T_com)</p>

<p>i,e T_cyc=T_inst+T_mem+T_com.</p>

<p>If I use a simulator I can get the T_inst,T_mem and T_com directly and find out the independent contribution of each component to the T_cyc. But using a hardware, I can only get the counts or number of events. Ie, N_inst, N_mem and N_com. 
So what I have is </p>

<p>T_cyc= a* N_inst + b* N_mem + c* N_com</p>

<p>Where a,b,c has to be determined.</p>

<p>I tried solving the problem using lsqnonneg (non-negative least square method)  in MATLAB to find the a,b,c. At times from the data I get b and c value ZERO which is totally meaningless.</p>

<p>Things to notice:
N_inst is a very high value. N_mem and N_com are bit lower in magnitude and hence I face this problem of b and c results as ZERO. </p>

<p>Questions:
1.  Is this a proper tool to solve such a linear equation system? If not, what else should I try?
2.  Is it a problem due to the sample size fed to the solver?
3.  I see that for most applications trend of N_cyc, N_inst,N_mem are monotonic but N_com is non-monotonic and can it affect the solved values? If so, how to isolate this component and find its contribution individually?</p>
"
"0.0404556697031367","0.0406222231851194"," 89874","<p>I have a hypothetical data given below that consists of 11 pairs of points (xi, yi ), to which the simple linear regression mean function $\mathbb E(y|x) = Î²_0 + Î²_1x$ is fit.:</p>

<pre><code> X     Y
 10    8.04
  8    6.95
 13    7.58
  9    8.81
 11    8.33
 14    9.96
  6    7.24
  4    4.26
 12    10.84
  7    4.82
  5    5.68
</code></pre>

<p>I have got intercept parameter,$\beta_0=3.001$</p>

<p>But the plot of the data is not showing the y-intercept is $3.001$. Rather the y-intercept is more than $3.001$. <strong>WHY?</strong></p>

<p><img src=""http://i.stack.imgur.com/HtTTU.png"" alt=""enter image description here""></p>

<p>I have used <code>R software</code> to calculate the parameters, $\beta_0$,$\beta_1$ and also to produce the plot.</p>

<pre><code> x1 &lt;- c(10,8,13,9,11,14,6,4,12,7,5)
 y1 &lt;- c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)

 lm(y1~x1)

 plot(y1~x1)
 abline(lm(y1~x1))
</code></pre>

<p><strong>EDIT</strong></p>

<pre><code>  ht &lt;- c(169.6,166.8,157.1,181.1,158.4,165.6,166.7,156.5,168.1,165.3)
  wt &lt;- c(71.2,58.2,56.0,64.5,53.0,52.4,56.8,49.2,55.6,77.8)

  lm(wt~ht)

  windows(9,6)
  par(mfrow=c(1,2))

  plot(wt~ht)
  abline(lm(wt~ht))

  plot(wt~ht,xlim=c(0,180),ylim=c(0,75))
  abline(lm(wt~ht))
</code></pre>

<p><img src=""http://i.stack.imgur.com/i1bqw.png"" alt=""enter image description here""></p>

<p>How can i get the y-intercept? By expanding the straight line(population regression line) to negative axis of Y ?</p>
"
"0.0862517776135472","0.0866068708302494"," 89930","<p>I am attempting to construct a contrast matrix that I can run in R, using the limma bioconductor package, but I am not sure that I have coded the contrast matrix correctly. A previous <a href=""https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r?newreg=add2674ca9d04b7eb85fad255b45b7f5"">post</a> and the limma guide were helpful, but my two factorial design is more complicated than what is illustrated there.</p>

<p>The first factor is the treatment, with two levels (control=c and stress=s), and the second factor is the genotype, with five levels (g1, g2, g3, g4, g5). Each genotype/treatment consists of 3-biological replicates (30xsamples total). My dataset has already been normalized and log2 transformed. It consists of 1208 proteins (based upon spectral counting for those that care) that measures protein abundance differences in the five genotypes and two treatments. The dataset is complete, meaning each sample/condition has a datapoint.</p>

<h2>Subset of the data:</h2>

<pre><code>proteinID   g1.s1   g1.s2   g1.s3   g1.c1   g1.c2   g1.c3   g2.s1   g2.s2   g2.s3   g2.c1   g2.c2   g2.c3   g3.s1   g3.s2   g3.s3   g3.c1   g3.c2   g3.c3   g4.s1   g4.s2   g4.s3   g4.c1   g4.c2   g4.c3   g5.s1   g5.s2   g5.s3   g5.c1   g5.c2   g5.c3
prot1   -9.70583694 -9.940059478    -9.764489183    -9.691937821    -9.547306096    -9.668928704    -9.821333234    -10.00376839    -9.843380585    -10.0789111 -9.958506961    -9.791583706    -10.04996359    -10.10279896    -10.0689715 -9.989303332    -10.05414639    -10.00619809    -9.907032795    -10.09700113    -10.00902876    -10.05603575    -10.26218387    -10.15527373    -9.88009858 -9.748974338    -9.730010667    -9.899956956    -9.773955101    -9.957684691
prot2   -9.810354967    -9.844319231    -9.896748977    -9.777040294    -9.821308434    -9.906798728    -9.832236541    -9.876359355    -9.935535795    -10.05991278    -9.831098077    -9.789738587    -10.08470861    -10.18515166    -10.10371621    -10.01971224    -9.977142493    -10.09055782    -9.739831978    -9.586647999    -9.949407778    -9.800183583    -9.83900565 -9.943521592    -9.99229056 -9.744850134    -9.794814509    -9.98542989 -9.766324886    -9.95430439
prot3   -11.70842601    -11.72521838    -11.90389475    -11.98273998    -11.915401  -11.88620205    -11.91603643    -11.96029519    -12.14926486    -12.23846499    -12.26650985    -11.84300821    -12.64562082    -12.41471031    -12.66462278    -12.577619  -12.90001898    -12.31577711    -11.66323243    -11.50283992    -11.4844068 -11.60402491    -11.95270942    -11.68245512    -12.32380181    -12.24294758    -12.23990879    -12.21563403    -12.33730369    -12.437377
prot4   -10.88942769    -11.16906693    -11.13942576    -11.31332257    -11.04718433    -11.11811122    -11.17687812    -11.12503828    -10.9724186 -11.16837945    -11.19642214    -10.96468249    -11.3975887 -11.28808753    -11.32778647    -11.34124725    -11.30972182    -11.29564372    -10.74370929    -10.92223539    -10.97733154    -11.40528844    -11.1238659 -11.15938598    -11.24937805    -10.8691392 -11.12478375    -10.75566728    -10.99485703    -11.09493115
prot5   -10.0102959 -9.936796529    -9.964629149    -9.842835973    -9.791578592    -9.773380518    -9.72290866 -9.715837804    -9.79028651 -9.951486129    -9.636225505    -9.820715987    -10.41899204    -10.25269382    -10.26949484    -10.02644184    -10.13120897    -10.20756299    -9.752087376    -9.687001368    -10.07111473    -9.815279198    -9.995624174    -9.993526894    -9.722360141    -9.551502595    -9.551929198    -9.724500546    -9.502769792    -9.65324573
prot6   -10.34051005    -10.27571947    -10.14968761    -10.17419023    -10.47812301    -10.11019796    -10.40447672    -10.15885481    -10.22900798    -10.26612428    -10.21920493    -10.17186677    -10.66125689    -10.95438025    -10.63751536    -10.65825783    -10.60857688    -10.78516027    -10.33890785    -10.49726978    -10.47100414    -10.64742463    -10.78932619    -10.5318634 -10.26494688    -9.975182247    -10.24870036    -10.2356165 -10.26689552    -10.13061368
prot7   -10.24930429    -10.37307132    -10.03573128    -10.29985129    -9.991216794    -10.05854902    -10.1958704 -10.30549818    -10.2078462 -10.28795766    -10.23314344    -10.23897922    -9.997472306    -10.27461285    -10.20805608    -10.06261332    -10.24876706    -10.12643737    -9.906088449    -10.07316322    -10.23545822    -10.30970717    -10.40745591    -10.36432166    -10.22423532    -10.25703553    -10.44925268    -9.902554721    -9.891163766    -10.0695915
prot8   -10.98782595    -10.84184533    -10.76496107    -10.68290092    -10.55763113    -10.91736394    -10.87505278    -10.76474268    -10.58319007    -10.87547281    -10.71948079    -10.95011831    -10.99753277    -11.061728  -10.8852958 -10.86371208    -10.96638746    -11.24112703    -10.46809937    -10.78446288    -10.71240489    -10.80931259    -10.6598091 -10.54801115    -10.70612733    -10.7339808 -10.8184854 -10.53370359    -10.47323989    -10.62675183
prot9   -8.83857166 -8.736344638    -8.743339515    -8.8152675  -8.743086044    -8.719612156    -8.898093257    -8.902781886    -9.071574958    -8.945970659    -8.862394746    -8.825061244    -8.82313363 -9.161452294    -8.905846232    -8.940119002    -9.024995852    -8.943721201    -8.768488159    -8.802155458    -8.721187011    -8.84850416 -8.931513624    -8.86743278 -8.856904592    -8.675257846    -8.900833162    -8.676117406    -8.758661701    -8.925717389
prot10  -10.65297508    -10.74532307    -10.65940071    -10.36671791    -10.50431649    -10.54915637    -11.07154003    -10.79884265    -10.97164196    -11.1201714 -11.14821342    -10.9254445 -10.92875918    -10.90806369    -10.77581175    -11.2324716 -11.31360896    -11.01070959    -11.04450945    -10.89694291    -10.76865867    -10.92983387    -11.07365287    -11.43888216    -11.14948441    -10.69611194    -10.85827316    -10.64470128    -10.79046792    -10.86048168
</code></pre>

<h2>Code that I am attempting to utilize:</h2>

<pre><code>proteins.mat &lt;- as.matrix(proteins.df)
treat = c(""g1.s"",""g1.c"",""g2.s"",""g2.c"",""g3.s"",""g3.c"",""g4.s"",""g4.c"",""g5.s"",""g5.c"")
factors = gl(10,3,labels=treat)
design &lt;- model.matrix(~0+factors)
colnames(design) &lt;- treat
</code></pre>

<h2>Here is the design for my model:</h2>

<pre><code>&gt; design
   g1.s g1.c g2.s g2.c g3.s g3.c g4.s g4.c g5.s g5.c
1     1    0    0    0    0    0    0    0    0    0
2     1    0    0    0    0    0    0    0    0    0
3     1    0    0    0    0    0    0    0    0    0
4     0    1    0    0    0    0    0    0    0    0
5     0    1    0    0    0    0    0    0    0    0
6     0    1    0    0    0    0    0    0    0    0
7     0    0    1    0    0    0    0    0    0    0
8     0    0    1    0    0    0    0    0    0    0
9     0    0    1    0    0    0    0    0    0    0
10    0    0    0    1    0    0    0    0    0    0
11    0    0    0    1    0    0    0    0    0    0
12    0    0    0    1    0    0    0    0    0    0
13    0    0    0    0    1    0    0    0    0    0
14    0    0    0    0    1    0    0    0    0    0
15    0    0    0    0    1    0    0    0    0    0
16    0    0    0    0    0    1    0    0    0    0
17    0    0    0    0    0    1    0    0    0    0
18    0    0    0    0    0    1    0    0    0    0
19    0    0    0    0    0    0    1    0    0    0
20    0    0    0    0    0    0    1    0    0    0
21    0    0    0    0    0    0    1    0    0    0
22    0    0    0    0    0    0    0    1    0    0
23    0    0    0    0    0    0    0    1    0    0
24    0    0    0    0    0    0    0    1    0    0
25    0    0    0    0    0    0    0    0    1    0
26    0    0    0    0    0    0    0    0    1    0
27    0    0    0    0    0    0    0    0    1    0
28    0    0    0    0    0    0    0    0    0    1
29    0    0    0    0    0    0    0    0    0    1
30    0    0    0    0    0    0    0    0    0    1
attr(,""assign"")
[1] 1 1 1 1 1 1 1 1 1 1
attr(,""contrasts"")
attr(,""contrasts"")$factors
[1] ""contr.treatment""
</code></pre>

<h2>My contrast model. I want to test for interaction, differences between genotypes, and to see if specific genotypes respond differently to the treatment from one another:</h2>

<pre><code>cmtx &lt;- makeContrasts(
  GenotypevsTreatment=(g1.s-g1.c)-(g2.s-g2.c)-(g3.s-g3.c)-(g4.s-g4.c)-(g5.s-g5.c),
  genotype=(g1.s+g1.c)-(g2.s+g2.c)-(g3.s+g3.c)-(g4.s+g4.c)-(g5.s+g5.c),
  Treatment=(g1.s+g2.s+g3.s+g4.s+g5.s)-(g1.c+g2.c+g3.c+g4.c+g5.c),
  levels=design)
</code></pre>

<h2>What my contrast model looks like, but I don't think this is correct:</h2>

<pre><code>&gt; cmtx
      Contrasts
Levels GenotypevsTreatment Genotype Treatment
  g1.s                   1        1         1
  g1.c                  -1        1        -1
  g2.s                  -1       -1         1
  g2.c                   1       -1        -1
  g3.s                  -1       -1         1
  g3.c                   1       -1        -1
  g4.s                  -1       -1         1
  g4.c                   1       -1        -1
  g5.s                  -1       -1         1
  g5.c                   1       -1        -1
</code></pre>

<h2>Fitting the linear model by empirical bayes statistics for differential expression:</h2>

<pre><code>fit &lt;- eBayes(contrasts.fit(lmFit(proteins.mat, design), cmtx))
topTable(fit, adjust.method=""BH"")
</code></pre>

<h2>The below topTable proteins are the same as the subset of data from above:</h2>

<pre><code>&gt; topTable(fit, adjust.method=""BH"")
       GenotypevsTreatment Genotype    Treatment    AveExpr        F      P.Value    adj.P.Val
prot1        -0.40786338 60.30918  0.073054723  -9.918822 17308.55 1.124646e-39 1.232079e-36
prot2        -0.09255219 59.60864  0.061701713  -9.897968 15801.43 3.304533e-39 1.232079e-36
prot3        -0.23880357 73.48557  0.536672827 -12.090016 15650.65 3.701463e-39 1.232079e-36
prot4        -0.11834000 66.76931  0.305471823 -11.122034 15522.46 4.079731e-39 1.232079e-36
prot5        -0.15210172 59.21509 -0.183849274  -9.876144 14734.51 7.556112e-39 1.423908e-36
prot6        -0.15761118 62.87467  0.155340561 -10.389362 14565.87 8.658504e-39 1.423908e-36
prot7        -0.03886438 61.15652 -0.166795475 -10.182834 14551.88 8.757515e-39 1.423908e-36
prot8        -0.10425341 64.63523 -0.186904167 -10.780359 14461.18 9.429854e-39 1.423908e-36
prot9        -0.03426380 53.48057  0.007403722  -8.854471 13713.49 1.767090e-38 2.021378e-36
prot10       -0.75250251 66.62646  0.327497120 -10.894506 13480.51 2.164184e-38 2.021378e-36
</code></pre>

<p>Aside from thinking that I didnâ€™t do this correctly, the result for Genotype looks incorrect to me. Any input would be much appreciated.</p>
"
"0.0404556697031367","0.0406222231851194"," 90000","<p>I want to find the sample size for logistic regression where I have a covariate with 15 levels and the covariates interacts with time, which means that the effect of the covariates is different for different periods of times. Can anyone help me find the sample size and effect size? I wish I could do this using simulations. Is there a code in SAS or R?</p>
"
"NaN","NaN"," 90104","<p>Given a linear regression model with all the assumptions checked and validated, I would like to obtain the probability that $Y&gt;y|X=x$. For example for the iris dataset, I would do the following to obtain the probability of $Y&gt;5|X=1,2,3...7$:</p>

<pre><code>plot(Sepal.Length~Petal.Length, data=iris)
lm1&lt;-lm(Sepal.Length~Petal.Length, data=iris)
summary(lm1)
abline(lm1)
predict(lm1, newdata=data.frame(Petal.Length=1:7))
(summary(lm1))$sigma
    pnorm(5, mean = predict(lm1, newdata=data.frame(Petal.Length=1:7)),
        sd = (summary(lm1))$sigma, lower.tail = F)
</code></pre>

<p>Is such an approach correct assuming constant variance?</p>
"
"0.131226987728912","0.131767241095046"," 90503","<p>I have zero inflated response variable I am trying to predict. I am facing few issues applying different regression models that should correct for this.</p>

<p>This is my 10,000 obs dataframe</p>

<pre><code>    e_weight       left_size         right_size        time_diff        
 Min.   :0.000   Min.   :  1.000   Min.   :  1.000   Min.   :      737  
 1st Qu.:0.000   1st Qu.:  1.000   1st Qu.:  1.000   1st Qu.:  4669275  
 Median :0.000   Median :  3.000   Median :  3.000   Median : 12263474  
 Mean   :0.022   Mean   :  6.194   Mean   :  5.469   Mean   : 21000288  
 3rd Qu.:0.000   3rd Qu.:  5.000   3rd Qu.:  5.000   3rd Qu.: 25420278  
 Max.   :3.000   Max.   :792.000   Max.   :792.000   Max.   :155291532
</code></pre>

<p>Here the frequency count for my 3 variables
<img src=""http://i.stack.imgur.com/1yIvx.jpg"" alt=""enter image description here"">
Indeed I have a problem with zeros...</p>

<p>I tried respectively a Zero-Inflated Negative Binomial Regression and a Zero-inflated Poisson Regression</p>

<pre><code>library(pscl)
m1 &lt;- zeroinfl(e_weight ~ left_size*right_size | time_diff, data = s)
summary(m1)

# Call:
# zeroinfl(formula = e_weight ~ left_size * right_size | time_diff, data = s)
#
# Pearson residuals:
#     Min      1Q  Median      3Q     Max 
# -1.4286 -0.1460 -0.1449 -0.1444 19.6054 
#
# Count model coefficients (poisson with log link):
#                        Estimate Std. Error z value Pr(&gt;|z|)    
#  (Intercept)          -3.8826386  0.0696970 -55.707  &lt; 2e-16 ***
#  left_size             0.0022261  0.0006195   3.594 0.000326 ***
#  right_size            0.0033622         NA      NA       NA    
#  left_size:right_size  0.0001715         NA      NA       NA    
# 
# Zero-inflation model coefficients (binomial with logit link):
#               Estimate Std. Error  z value Pr(&gt;|z|)    
# (Intercept)  1.753e+01  6.011e+00    2.916  0.00354 ** 
#  time_diff   -3.342e-04  1.059e-06 -315.773  &lt; 2e-16 ***
#  ---
#  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
#
# Number of iterations in BFGS optimization: 28 
#  Log-likelihood: -1053 on 6 Df
#  Warning message:
#  In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>

<p>and </p>

<pre><code>library(MASS)
m2 &lt;- glm.nb(e_weight ~ left_size*right_size + time_diff, data = s) 
</code></pre>

<p>which gives</p>

<pre><code>There were 22 warnings (use warnings() to see them)
warnings()
Warning messages:
1: glm.fit: algorithm did not converge
...
21: glm.fit: algorithm did not converge
22: In glm.nb(e_weight ~ left_size * right_size + time_diff,  ... :
  alternation limit reached
</code></pre>

<p>If  I ask a summary for the second model </p>

<pre><code>summary(m2)

# Call:
# glm.nb(formula = e_weight ~ left_size * right_size + time_diff, 
#     data = s, init.theta = 0.1372733321, link = log)
#
# Deviance Residuals: 
#     Min       1Q   Median       3Q      Max  
# -3.4645  -0.2331  -0.1885  -0.1266   2.7669  
# 
# Coefficients:
#                        Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)          -3.239e+00  1.090e-01 -29.699  &lt; 2e-16 ***
# left_size            -4.462e-03  1.835e-03  -2.431 0.015047 *  
# right_size           -7.144e-03  2.118e-03  -3.374 0.000742 ***
# time_diff            -6.013e-08  8.584e-09  -7.005 2.48e-12 ***
# left_size:right_size  4.691e-03  2.749e-04  17.068  &lt; 2e-16 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#
# (Dispersion parameter for Negative Binomial(0.1374) family taken to be 1)
# 
#     Null deviance: 1106.5  on 9999  degrees of freedom
# Residual deviance:  958.5  on 9995  degrees of freedom
# AIC: 1967.2
# 
# Number of Fisher Scoring iterations: 12
# 
# 
#              Theta:  0.1373 
#          Std. Err.:  0.0223 
# Warning while fitting theta: alternation limit reached 
#
#
# 2 x log-likelihood:  -1955.2260
</code></pre>

<p>Also both models have very low p-values for heteroskedasticity </p>

<pre><code>bptest(m1)
# 
#   studentized Breusch-Pagan test
#
# data:  m1
# BP = 244.832, df = 3, p-value &lt; 2.2e-16
#
bptest(m2)
# 
#   studentized Breusch-Pagan test
#
# data:  m2
# BP = 277.2589, df = 4, p-value &lt; 2.2e-16
</code></pre>

<p>How should I approach this regression. Would make sense to simply add 1 to all my dataframe before running any regression?</p>
"
"0.0707974219804893","0.0812444463702388"," 90904","<p>I'm new to using CART trees, but have been asked to do so for a project I'm working on. I've had success running the scripts (from both RPART and PARTY packages) but I can't seem to get exactly what I'm looking for. I'm working with spectral data (Red, NIR, NDVI...) for 80 trees in four categories (Mesic-control, Mesic-fertilized, Xeric-control and Xeric-fertilized). There are significant differences in the mean values for spectral bands among the four categories and I'd like to use those differences to develop an algorithm for assigning category to unknown trees. </p>

<p>Here's a dummy tree I made using the RPART package:
<img src=""http://i.stack.imgur.com/7XUQ3.jpg"" alt=""RPART tree""></p>

<pre><code>fit &lt;- rpart(Category ~ red.top + NIR.top + R.NIR.top, method=""anova"", data=CCA)
plot(fit, uniform=T,main=""Classification Tree for Kyphosis"")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
</code></pre>

<p>And here's another tree I made with PARTY:
<img src=""http://i.stack.imgur.com/ourPe.jpg"" alt=""PARTY tree""></p>

<pre><code>library(party)
fit &lt;- ctree(Category ~ red.top + NIR.top + R.NIR.top, data=CCA)
plot(fit, main=""Conditional Inference Tree for Kyphosis"")
gtree &lt;- ctree(Category ~ ., data = CART)
plot(gtree)
</code></pre>

<p>Both look fine, except they don't really do what I want. The RPART one looks good, but I can't figure out how to determine the category identity of the trees in each 'leaf' and the PARTY one is what I want, except the tree is way simplified compared to the regression tree in the first example. My ultimate goal is to essentially combine the two and create a larger regression tree that uses more of the 'rules' from the data and gets me to output 'leaves' with categorical information and some predictive power. I'm not really too hung up on whether I use regression or categorization--as long as it has utilitarian value.</p>

<p>So, I guess what I'm really looking for is better scripts for either package that give me a more detailed tree with visual output (bar graphs on the leaves) or a way to determine the identity of the groups created by the RPART tree.</p>
"
"NaN","NaN"," 90947","<p><code>FullModel&lt;- (lm(Fubar~.-Foo-Bar,data=BarFoo))
NullModel&lt;-(lm(Fubar~1))
step(NullModel,scope=formula(FullModel),direction=""forward"",k=log(nrow(BarFoo)))</code></p>

<p>When doing the above forward stepwise regression, the forward steps halt before certain variables are added in. does this mean that they do not improve the AIC score or does it mean that the inputs are in error?</p>
"
"0.0858194351535935","0.0765979986161901"," 91745","<p>Please, i need to do a network meta-analysis and metaregression or a multivariate meta analysis and metaregression. I have multi arm randomized controlled studies.
I tried the metafor package</p>

<h1>Dummy database with controlled multiarm treatment studies</h1>

<pre><code>Study = c(1,2,3,1:2,4:10,1,2,7)
Group1 = as.factor(c(rep(1,3),rep(2,9),rep(3,3)))
numberInt= c(rep(30,5),rep(28,7),rep(40,3))
InterventionMean1  = c(rnorm(3, mean = 350, sd = 2), rnorm(9, mean = 540, sd = 2),rnorm(3, mean = 860, sd = 2))
InterventionSD  = rnorm(15, mean = 80, sd = 15)
numberCont= c(rep(29,4), 28,rep(30,7),rep(42,3))
ControlMean  = rnorm(15, mean = 230, sd = 2)
ControlSD  = rnorm(15, mean = 55, sd = 9)
Dose = c(40,50,40,100,100,100,100,100,100,100,100,100,200,200,200)
x=as.factor(c(1,2,1,rep(1,5),rep(2,3),rep(1,4)))
data = data.frame(cbind(Study,Group, numberInt,InterventionMean1,InterventionSD,
             numberCont, ControlMean, ControlSD,Dose,x))
</code></pre>

<h1>x and Dose are covariates</h1>

<h1>Loading libraries</h1>

<pre><code>library(metafor)
library(Matrix)
</code></pre>

<h1>Effect size : mean difference</h1>

<p>dataEscalc = escalc(measure = ""MD"", m1i = InterventionMean1, sd1i = InterventionSD, n1i = numberInt,
                    m2i = ControlMean, sd2i = ControlSD, n2i = numberCont,
                    data=data)</p>

<h1>Number of patients per studies</h1>

<pre><code>dataEscalc$Ni &lt;- unlist(lapply(split(data, data$Study), function(x) rep(sum(x$numberInt) + x$numberCont[1], each=nrow(x))))
</code></pre>

<h1>Variance-covariance matrix</h1>

<pre><code>    calc.v &lt;- function(x) {
    v &lt;- matrix(vi/x$numberCont[1] + outer(x$yi, x$yi, ""*"")/(2*x$Ni[1]), nrow=nrow(x), ncol=nrow(x))
      diag(v) &lt;- x$vi
          v
          }
    V &lt;- lapply(split(dataEscalc, dataEscalc$Study), calc.v)
V &lt;- as.matrix(bdiag(V))
V
</code></pre>

<h1>Conducting multivariate meta anlaysis</h1>

<p>names(dataEscalc)</p>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1)  - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) - 1, random =
  ~factor(Group1) |  : 
                    Error during optimization.</p>
</blockquote>

<h1>Conducting multivariate metaregression</h1>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1) + Group1:I(Dose) + Group1:I(x) - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) + Group1:I(Dose) +
  Group1:I(x) -  : 
                    Model matrix not of full rank. Cannot fit model.</p>
</blockquote>

<p>I followed the instructions from 
<a href=""http://www.metafor-project.org/doku.php/analyses%3agleser2009"" rel=""nofollow"">http://www.metafor-project.org/doku.php/analyses:gleser2009</a>
http://www.metafor-project.org/doku.php/analyses:vanhouwelingen2002</p>

<p>Thanks in advance</p>
"
"0.100122674345859","0.107715935554017"," 92150","<p>Actually, I thought I had understood what one can show a with partial dependence plot, but using a very simple hypothetical example, I got rather puzzled. In the following chunk of code I generate three independent variables (<em>a</em>, <em>b</em>, <em>c</em>) and one dependent variable (<em>y</em>) with <em>c</em> showing a close linear relationship with <em>y</em>, while <em>a</em> and <em>b</em> are uncorrelated with <em>y</em>. I make a regression analysis with a boosted regression tree using the R package <code>gbm</code>:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")
library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")
par(mfrow = c(2,2))
plot(gbm.gaus, i.var = 1)
plot(gbm.gaus, i.var = 2)
plot(gbm.gaus, i.var = 3)
</code></pre>

<p>Not surprisingly, for variables <em>a</em> and <em>b</em> the partial dependence plots yield horizontal lines around the mean of <em>a</em>. What me puzzles is the plot for variable <em>c</em>. I get horizontal lines for the ranges <em>c</em> &lt; 40 and <em>c</em> > 60 and the y-axis is restricted to values close to the mean of <em>y</em>. Since <em>a</em> and <em>b</em> are completely unrelated to <em>y</em> (and thus there variable importance in the model is 0), I expected that <em>c</em> would show partial dependence along its entire range instead of that sigmoid shape for a very restricted range of its values. I tried to find information in Friedman (2001) ""Greedy function approximation: a gradient boosting machine"" and in Hastie et al. (2011) ""Elements of Statistical Learning"", but my mathematical skills are too low to understand all the equations and formulae therein. Thus my question: What determines the shape of the partial dependence plot for variable <em>c</em>? (Please explain in words comprehensible to a non-mathematician!)     </p>

<p>ADDED on 17th April 2014:</p>

<p>While waiting for a response, I used the same example data for an analysis with R-package <code>randomForest</code>. The partial dependence plots of randomForest resemble much more to what I expected from the gbm plots: the partial dependence of explanatory variables <em>a</em> and <em>b</em> vary randomly and closely around 50, while explanatory variable <em>c</em> shows partial dependence over its entire range (and over almost the entire range of <em>y</em>). What could be the reasons for these different shapes of the partial dependence plots in <code>gbm</code> and <code>randomForest</code>?</p>

<p><img src=""http://i.stack.imgur.com/PrlC1.jpg"" alt=""partial plots of gbm and randomForest""></p>

<p>Here the modified code that compares the plots:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")

library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")

library(randomForest)
rf.model &lt;- randomForest(y ~ a + b + c, data = Data)

x11(height = 8, width = 5)
par(mfrow = c(3,2))
par(oma = c(1,1,4,1))
plot(gbm.gaus, i.var = 1)
partialPlot(rf.model, Data[,2:4], x.var = ""a"")
plot(gbm.gaus, i.var = 2)
partialPlot(rf.model, Data[,2:4], x.var = ""b"")
plot(gbm.gaus, i.var = 3)
partialPlot(rf.model, Data[,2:4], x.var = ""c"")
title(main = ""Boosted regression tree"", outer = TRUE, adj = 0.15)
title(main = ""Random forest"", outer = TRUE, adj = 0.85)
</code></pre>
"
"0.0495478739876288","0.033167906340333"," 92856","<p>I am using R and <code>MCMCpack</code> to do a Bayesian analysis of some data that I have.  I have generated posterior distributions (<code>postDist</code>) on the means of some parameters (<code>y1,y2,y3</code>) using <code>MCMCregress</code> (<code>postDist &lt;- MCMCRegress( x ~ y + z ,...)</code>).  </p>

<p>Now, I would like to take those posterior distributions on the means and generate a posterior distribution on the difference between the means.  Is that a reasonable thing to do in a Bayesian analysis, and if so, how do you do it (either in theory or in R)?</p>
"
"0.140142550760116","0.140719508946058"," 93372","<p>In cognitive psychology, it has been argued that the learning curve of a skill follows the power function, in which the practice of the skill yields progressively smaller decrements in error. I want to test this hypothesis with the data I have, and to do this I need to fit a power law function to the data.</p>

<p>I heard that the power function can be fit in R by <code>lm(log(y) ~ log(x))</code>. <a href=""http://stats.stackexchange.com/questions/61600/strange-outcome-when-performing-nonlinear-least-squares-fit-to-a-power-law"">An answer to this post</a>, however, suggests using <code>glm(y ~ log(x), family = gaussian(link = log))</code><del>, and indeed the resulting fit prefers the <code>glm</code> approach</del> (<strong>EDIT:</strong> See comments. This is not true.).</p>

<pre><code># Define a function to compute R-squared in linear space 
&gt; rsq &lt;- function(data, fit) {
+ ssres &lt;- sum((data - fit)^2)
+ sstot &lt;- sum((data - mean(data))^2)
+ rsq &lt;- 1 - (ssres / sstot)
+ return(rsq)
}

# generate data and fit a power function with lm() and glm()
&gt; set.seed(10)
&gt; lc &lt;- (1:10)^(-1/2) * exp(rnorm(10, sd=.25)) # Edited
&gt; model.lm &lt;- lm(log(lc) ~ log(1:10))
&gt; model.glm &lt;- glm(lc ~ log(1:10), family = gaussian(link = log))
&gt; fit.lm &lt;- exp(fitted(model.lm))
&gt; fit.glm &lt;- fitted(model.glm)

# draw a graph with fitted lines
&gt; plot(1:10, lc, , xlab = ""Practice"", ylab = ""Error rate"", las = 1)
&gt; lines(1:10, fit.lm, col = ""red"")
&gt; lines(1:10, fit.glm, col = ""blue"")
&gt; legend(""topright"", c(""original data"", ""lm(log(y) ~ log(x))"", 
+ ""glm(y ~ log(x), family = gaussian(link = log))""), 
+ pch = c(1, NA, NA), lty = c(NA, 1, 1), col = c(""black"", ""red"", ""blue""))
</code></pre>

<p><img src=""http://i.stack.imgur.com/F28SM.jpg"" alt=""enter image description here""></p>

<pre><code># compute R-squared values for both models
# (EDIT: See comments. These are not good measures to use.)
&gt; rsq(lc, fit.lm)
[1] 0.9194631
&gt; rsq(lc, fit.glm)
[1] 0.9205448
</code></pre>

<p><del>From the $R^2$ values, it seems that the <code>glm</code> model has a better fit. When the procedure was repeated 100 times, it was always the case that the <code>glm</code> model has a higher $R^2$ value than the <code>lm</code> model</del> (<strong>EDIT:</strong> See comments. $R^2$ is not a good measure for this purpose).</p>

<p>At this point I have two questions.</p>

<ul>
<li>What is the difference between the <code>lm</code> model and the <code>glm</code> model above? I thought a generalized linear model with the log link function is practically the same as regressing log(y) in general linear models.</li>
<li>I'm trying to compare the fit of the power function against other models (e.g., linear models as in <code>lm(y ~ x)</code>). To do this, I calculate the fitted values in linear space and compare the $R^2$ values computed in linear space, just like I did to compare <code>lm</code> and <code>glm</code> models above. Is this a proper way to compare this type of models?</li>
</ul>

<p>My final question concerns the estimation of the four-parameter power law function given in <a href=""http://link.springer.com/article/10.3758/BF03212979"" rel=""nofollow"">this paper</a> (p.186; I modified the notation a little).</p>

<p>$$
E(Y_N) = A + B(N + E)^{-Î²}
$$
where</p>

<ul>
<li>$E(Y_N)$ = expected value of $Y$ on practice trial $N$</li>
<li>$A$ = expected value of $Y$ after learning is completed (i.e., asymptote)</li>
<li>$E$ = the amount of prior learning before $N = 0$</li>
</ul>

<p>I have $N$ and $Y$ (<code>1:10</code> and <code>lc</code> respectively in the code above), and want to estimate $A$, $B$, $E$, and $-Î²$ from the data. In the example above, I assumed $A$ = $E$ = 0 mostly because I wasn't sure how to estimate four parameters concurrently. While this isn't a huge problem, I would like to estimate $A$ and $E$ from the data as well if possible. So my final question:</p>

<ul>
<li>Is there a way to estimate the four parameters in R? If not, is there a way to estimate three parameters, excluding $A$ from the above (i.e., assuming $A$ = 0)?</li>
</ul>

<p>The paper I linked above says the authors used the simplex algorithm to estimate the parameters. But I am familiar with neither the algorithm nor the <code>simplex</code> function in R.</p>

<p>I'm aware of Clauset et al's work and the <code>poweRlaw</code> package in R. I believe both of them are dedicated to modeling the distribution of one categorical variable, but neither models the curve of the power law.</p>

<p>Any help is appreciated.</p>
"
"0","0"," 93641","<p>I tried to do a CFA with the <code>lavaan</code> package in R.
Here is my model:</p>

<p><img src=""http://i.stack.imgur.com/f1w70.jpg"" alt=""enter image description here""></p>

<p>I know how to define my latent variables like this:</p>

<pre><code>MASTER.model &lt;- ' Usability  =~ PUS1 + PUS2 + PUS3 + PUS4 + PUS5 
                   PerceivedUsefulness =~ PU1 + PU2 + PU3 + PU4 + PU5
                   PerceivedEaseOfUse   =~ PEOU1 + PEOU2 + PEOU3 + PEOU4 + PEOU5
                   BehavioralIntent =~ BI1 + BI2
</code></pre>

<p>But I don't know how to define the links between <code>Usability</code>, <code>EOU</code>, etc.
Is this a covariance, written with e.g. <code>Usability~~PerceivedEaseOfUse</code> or do I write it like this: <code>PerceivedEaseOfUse~Usability</code> which means <code>PerceivedEaseOfUse</code> is regressed from <code>Usability</code>?</p>

<p>My try was:</p>

<pre><code>MASTER.model &lt;- ' Usability  =~ PUS1 + PUS2 + PUS3 + PUS4 + PUS5 
                PerceivedUsefulness =~ PU1 + PU2 + PU3 + PU4 + PU5
                   PerceivedEaseOfUse   =~ PEOU1 + PEOU2 + PEOU3 + PEOU4 + PEOU5
                   BehavioralIntent =~ BI1 + BI2
                   PerceivedUsefulness ~ PerceivedEaseOfUse
                   BehavioralIntent ~ PerceivedEaseOfUse
                   BehavioralIntent ~Â PerceivedUsefulness
                    Usability ~~ Usability
                    PerceivedEaseOfUse ~~ PerceivedEaseOfUse
                    PerceivedUsefulness ~~Â PerceivedUsefulness                 
                    PerceivedEaseOfUse ~ Usability'
</code></pre>

<p>This gave me proper results, but my RMSEA was really poor, which would mean my model doesn't fit my data. I just want to be sure it is not wrong because of me using R in a wrong way.
Thank you.</p>
"
"0.0404556697031367","0.0406222231851194"," 93815","<p>I have some experiences with time series modelling, in the form of simple ARIMA models and so on. Now I have some data that exhibits volatility clustering, and I would like to try to start with fitting a GARCH (1,1) model on the data. </p>

<p>I have a data series and a number of variables I think influence it. So in basic regression terms, it looks like: </p>

<p>$$
y_t = \alpha + \beta_1 x_{t1} + \beta_2 x_{t2} + \epsilon_t .
$$</p>

<p>But I am at a complete loss at how to implement this into a GARCH (1,1) - model? I've looked at the <code>rugarch</code>-package and the <code>fGarch</code>-package in <code>R</code>, but I haven't been able to do anything meaningful besides the examples one can find on the internet. </p>
"
"0.0495478739876288","0.0497518595104995"," 94009","<p>I've built a random forest model (regression model) using randomForest package in R, and I calculate the correlation between the predicted values and the actual ones in order to know how the trained model is going to perform, which is very high in my case, so I was wondering in such case how this correlation is build, I mean is it leave one out correlation or any type of cross validation correlation or just random and can't represent the real performance of the model when tested on unseen new cases???? the following is a snapshot of my script for calculating the correlation where x is the data (observations) and y is the numeric values I want the model to learn/predict (in the testing cases):</p>

<pre><code>mytr_all = randomForest(x, y, ntree = 500,corr.bias=TRUE)
cor(mytr_all$y,mytr_all$predicted)
</code></pre>
"
"0.11794753195637","0.11843311462705"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"0.103142124625879","0.103566754353222"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.0809113394062735","0.0812444463702388"," 95190","<p>I am conducting a <strong>meta-analysis</strong> from a large number of studies. In each study are <strong>compared weights of two groups</strong> (fishes with and without internal parasite). I am interested if the <strong>weight can explain the presence/absence of a parasite</strong>. From forest plot it seems to be clear that in each continent (and for world as a whole)  there is a <strong>clear preference towards bigger fishes</strong>.</p>

<p>I have studies from all over the world, that means <strong>the term ""bigger"" fish is a relative</strong>. The bigger fish in Africa could be very small in comparison with smallest fish in Australia. <strong>Because of this fact I have used random-effect model</strong>. It should be a good choice according to some textbooks.</p>

<p><strong>Is random-effect model appropriate for my situation?</strong></p>

<p>Could you please help me with <strong>interpretation of model output</strong>? There are some things I do not understand:</p>

<p>P-value from model results is highly significant. So it is highly probable that the weight has something to do with parasite infestation. However, the Test for Heterogeneity is also significant. Does it means that <strong>my model do not meet the assumption for normal distribution of residuals?</strong> Similarly like a in the case of some simple regression?</p>

<p><strong>And what about tau^2, tau, I^2 and H^2?
Is any of them similar to R-squared?</strong></p>

<p>Please give me some guidance (in layman terms if possible).</p>

<pre><code>library(metafor)
mod_weight &lt;- rma(yi, vi, data = dat_weight); summary(mod_weight)

Random-Effects Model (k = 62; tau^2 estimator: REML)

  logLik  deviance       AIC       BIC      AICc  
-65.0051  130.0103  134.0103  138.2320  134.2172  

tau^2 (estimated amount of total heterogeneity): 0.3618 (SE = 0.0808)
tau (square root of estimated tau^2 value):      0.6015
I^2 (total heterogeneity / total variability):   86.67%
H^2 (total variability / sampling variability):  7.50

Test for Heterogeneity: 
Q(df = 61) = 395.7163, p-val &lt; .0001

Model Results:

estimate       se     zval     pval    ci.lb    ci.ub          
  0.8007   0.0853   9.3830   &lt;.0001   0.6334   0.9679      *** 
</code></pre>

<p><strong><em>P.S. the fish-parasite research is a made-up story :)</em></strong></p>
"
"0.124692748408752","0.118616305942459"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.0495478739876288","0.0497518595104995"," 95494","<p>I am doing a regression analysis with multiple variables and comparing it to a one-variable null hypothesis. The goal is to see which model provides a better explanation. The topic is information diffusion, hence the names DIFfusion, INDustry, etc. Also I must say that DIF is many times zero (has a long tail) and all variables go between 0 and 1. Anyways, the hypothesis can be translated unto:</p>

<p>H_0: DIF ~ REL</p>

<p>H_1: DIF ~ REL+COM*REL+IND*REL</p>

<p>H_2: DIF ~ REL+COM*REL+IND*REL+SIZE1+SIZE2</p>

<p>REL multiplies the other two variables as theoretically they are related and together they 'should' give a better prediction of DIF.</p>

<p>Now, the tricky things for me are two.</p>

<p>First, (and most importantly), although R values are over 0.6 -good enough for this topic- when I see the residuals they tend to follow a pattern that is similar for all three hypotheses. I really don't know why or how to address it.</p>

<p><img src=""http://i.stack.imgur.com/XRh2q.png"" alt=""Residuals""></p>

<p><img src=""http://i.stack.imgur.com/JXeCI.png"" alt=""QQplot""></p>

<p><img src=""http://i.stack.imgur.com/ZCvJe.png"" alt=""histogram""></p>

<p>Second, I have the intuition that the dependent variable (DIF) behaves like an iceberg and sea-level. Meaning that the lower the sea level (represented by SIZE1-2), I could see more of the shape of the iceberg, where the shape of the iceberg is given by the other variables. Have you encountered situations like this? How would you model/test it?</p>

<p>Any advice?</p>

<p>Using R by the way.</p>
"
"0.131226987728912","0.131767241095046"," 95994","<p>I`d like to extract the parameters of a two-component mixture distribution of noncentral student t distributions which first has to be fitted to a one-dimensional sample.</p>

<p>My question is closely related to this thread, but as pointed out I want to use Student t components for the mixture:
<a href=""http://stats.stackexchange.com/questions/10062/which-r-package-to-use-to-calculate-component-parameters-for-a-mixture-model?newreg=fe1454a4702e4532a03bd2c705fe3b02"">Which R package to use to calculate component parameters for a mixture model</a></p>

<p>There are many packages for R that are capable of handling mixture distributions in one way or another. Some in the context of a Bayesian framework requiring kernels. Some in a regression framework. Some in a nonparametric framework. ...</p>

<p>In general the ""mixdist""-package seems to come closest to my wish. This package fits parametric mixture distributions to a sample of data. Unfortunately it doesn`t support the student t distribution.</p>

<p>I have also tried to manually set up a likelihood function as described here:
<a href=""http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions"">http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions</a>
But my result is far from perfect.</p>

<p>The ""gamlss.mx""-package might be helping, but originally it seems to be set up for another context, i.e. regression. I tried to regress my data on a constant and then extract the parameters for the estimated mixture error distribution. Is this a valid approach? </p>

<p>But with this approach the estimated parameters seem to be not directly accessable individually by some command (such as fit1$sigma). And more importantly there seem to be serious estimation problems even in pretty simple and nonambiguous cases.
E.g. in example 2 (see syntax below) I simulated a mixture which looks like this:</p>

<p><img src=""http://i.stack.imgur.com/MG7AA.jpg"" alt=""kernel density estimate of the mixture""></p>

<p>When trying to fit a two-component student t mixture to these data either I get this error message (the deeper meaning of which I don't understand):</p>

<p><img src=""http://i.stack.imgur.com/UPvg4.jpg"" alt=""enter image description here""></p>

<p>or I get wrong results (convergenve is reached only after approximately two hours as can be seen from the output):</p>

<p><img src=""http://i.stack.imgur.com/HjlfW.jpg"" alt=""enter image description here""></p>

<p>The means could be estimated well, but both the variance and the degrees of freedom are estimated badly. In the TF2 implementation of the student t, the sigma parameter denotes the standard deviation. Its estimate is NEGATIVE for the first component! And for the second component the degrees of freedom estimate is also NEGATIVE. Probably one should not use these results in practice :(</p>

<p>By the way: Is there a way to restrict these degree-of-freedom coefficient estimates to be natural numbers? </p>

<p>The following syntax is my gamlss.mx-setup so far:</p>

<pre><code>library(gamlss.dist)
library(gamlss.mx)
library(MASS)

# example 1 (real data):
data(geyser)
plot(density(geyser$waiting) )
fit1 &lt;- gamlssMX( waiting~1,data=geyser,family=""TF2"",K=2 )
fit1
# works fine

# example 2 (simulated data):
N &lt;- 100000
components &lt;- sample(1:2,prob=c(0.6,0.4),size=N,replace=TRUE)
mus &lt;- c(3,-6)    # denotes the mean of component 1 and 2, respectively
sds &lt;- c(1,9)     # ... the standard deviations
nus &lt;- c(25,3)    # ... the degrees of freedom
mixsim &lt;-data.frame(rTF2( N,mu=mus[components],sigma=sds[components],nu=nus[components] ))
colnames(mixsim) &lt;- ""MCsim""
plot(density(mixsim$MCsim) , xlim=c(-50,50))
fit2 &lt;- gamlssMX(MCsim~1,data=mixsim,family=""TF2"",K=2)
fit2
# error message or strange results (this also happens when using a sample of S&amp;P500 returns)
</code></pre>

<p>I would be very grateful for any advice!
I've read through many related manuals and vignettes so far but I`m still lost.</p>

<p>Thanks a lot in advance!!
Jo</p>
"
"0.171638870307187","0.167558121972916"," 96010","<p>I am struggling to fit alternative count models into my data. I guess my problem is just too many zeros.</p>

<p>This is my data</p>

<pre><code>&gt; summary(smpl)
    response        predict1          predict2        
 Min.   :0.000   Min.   :   1.00   Min.   :    22005  
 1st Qu.:0.000   1st Qu.:   3.00   1st Qu.:  4669705  
 Median :0.000   Median :   8.00   Median : 12540318  
 Mean   :0.017   Mean   :  23.27   Mean   : 20382574  
 3rd Qu.:0.000   3rd Qu.:  20.00   3rd Qu.: 25468156  
 Max.   :3.000   Max.   :1584.00   Max.   :145348049

&gt; table(smpl$response)
  0   1   2   3 
987  10   2   1 
</code></pre>

<p>I tried three regressions: basic Poisson, negative binomial and zero-inflated but the only formula returning coefficients without warnings is the Poisson:</p>

<pre><code>&gt; summary(glm(response ~ ., data = smpl, family = poisson))

Call:
glm(formula = response ~ ., family = poisson, data = smpl)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3871  -0.2214  -0.1722  -0.1148   4.7861  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.472e+00  3.521e-01  -9.862  &lt; 2e-16 ***
predict1     3.229e-03  7.271e-04   4.442 8.93e-06 ***
predict2    -6.258e-08  3.060e-08  -2.045   0.0409 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 150.67  on 999  degrees of freedom
Residual deviance: 135.84  on 997  degrees of freedom
AIC: 170.06

Number of Fisher Scoring iterations: 8
</code></pre>

<p>The negative binomial returns a warnings on both the convergence and the alternation limit</p>

<pre><code>summary(glm.nb(response ~ ., data = smpl))

Call:
glm.nb(formula = response ~ ., data = smpl, init.theta = 0.04901296596, 
    link = log)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.28844  -0.17677  -0.14542  -0.09808   2.38314  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.899e+00  4.587e-01  -8.499  &lt; 2e-16 ***
predict1     1.226e-02  2.144e-03   5.720 1.06e-08 ***
predict2    -5.982e-08  3.407e-08  -1.756   0.0791 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.049) family taken to be 1)

    Null deviance: 69.927  on 999  degrees of freedom
Residual deviance: 55.940  on 997  degrees of freedom
AIC: 152.37

Number of Fisher Scoring iterations: 1


              Theta:  0.0490 
          Std. Err.:  0.0251 
Warning while fitting theta: alternation limit reached 

 2 x log-likelihood:  -144.3700 
Warning messages:
1: glm.fit: algorithm did not converge 
2: In glm.nb(response ~ ., data = smpl) : alternation limit reached
</code></pre>

<p>and the zero-inflated (from the <code>pscl</code> package) doesn't return anything at all</p>

<pre><code>&gt; summary(zeroinfl(response ~ ., data = smpl, dist = ""negbin""))

Call:
zeroinfl(formula = response ~ ., data = smpl, dist = ""negbin"")

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-0.45252 -0.08817 -0.05515 -0.04210 19.56118 

Count model coefficients (negbin with log link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.477e+00         NA      NA       NA
predict1     2.678e-03         NA      NA       NA
predict2    -1.160e-07         NA      NA       NA
Log(theta)  -1.241e+00         NA      NA       NA

Zero-inflation model coefficients (binomial with logit link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.869e+00         NA      NA       NA
predict1    -1.329e-01         NA      NA       NA
predict2    -1.346e-07         NA      NA       NA
Error in if (getOption(""show.signif.stars"") &amp; any(rbind(x$coefficients$count,  : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Then my questions are: </p>

<ol>
<li>Is there anything I can do in terms of ""formula tweaking"" with the negative binomial (to avoid the warnings) and with the zero-inflated (to get the coefficients)?</li>
<li>Looking only at the results above (thus including problems with convergence and alternation limit) should I select the negative binomial model since it seems, looking at the AIC, to fit better than the Poisson in my data?</li>
</ol>
"
"0.0707974219804893","0.0812444463702388"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"NaN","NaN"," 96364","<p>Whenever I run a robust regression model in R with <code>rlm</code> and with either M or MM methods, I get the following error message:</p>

<blockquote>
  <p>Error: 'lqs' failed: all the samples were singular</p>
</blockquote>

<p>I am not sure what it means and what I can do to bypass this error.</p>
"
"NaN","NaN"," 96727","<p>My Error's plot from a linear regression</p>

<p><img src=""http://i.stack.imgur.com/bprYB.png"" alt=""enter image description here"">.</p>

<p>Is it Normal distributted? If not, why? What that negative big bar means?</p>

<p>The code used is:</p>

<pre><code>  library(MASS)
sresid &lt;- studres(reg3) 
hist(sresid, freq=FALSE, 
     main=""Distribution of Studentized Residuals"")
xfit&lt;-seq(min(sresid),max(sresid),length=40) 
yfit&lt;-dnorm(xfit) 
lines(xfit, yfit)
</code></pre>

<p>Edited:</p>

<p><img src=""http://i.stack.imgur.com/mWffB.png"" alt=""enter image description here""></p>
"
"0.0858194351535935","0.0861727484432139"," 96991","<p>I have done survival analysis. I used Kaplan-Meir to do the survival analysis. </p>

<p>Description of data: 
My data set is large and data table has close 120,000 records of survival information belong to 6 groups.</p>

<p>Sample: </p>

<pre><code>   user_id   time_in_days   event total_likes total_para_length group
1:       2          4657     1       38867        431117212   AA
2:       2          3056     1       31392        948984460   BB
3:       2            49     1          15            67770   CC
4:       3          4181     1       15778        379211806   BB
5:       3            17     1           3            19032   CC
6:       3          2885     1       12001        106259666   EE
</code></pre>

<p>After fitting the survival curves and plotting it, I see they are similar but yet at any given point in time their survival proportions don't seem to look like identical.</p>

<p>Here is the plot:
<img src=""http://i.stack.imgur.com/9wLYH.png"" alt=""Survival Curves""></p>

<p>I ran a hypothesis test where my H0: There is not difference between the survival curves and here is the results that I got. </p>

<pre><code>&gt; survdiff(formula= Surv(time, event) ~ group, rh=0)
Call:
survdiff(formula = Surv(time, event) ~ group, rho = 0)

             N Observed Expected (O-E)^2/E (O-E)^2/V
group=FF 28310    27993    28632      14.3      19.0
group=AA 64732    63984    67853     220.6     460.1
group=BB 19017    18690    16839     203.4     245.6
group=CC  9687     9536     8699      80.6      91.0
group=DD 13438    13187    11891     141.3     164.2
group=EE  3910     3847     3324      82.4      89.7

 Chisq= 788  on 5 degrees of freedom, p= 0  
</code></pre>

<p>I am little confuse by trying to figure out what it means, specially since I got <code>p-value=0</code>. </p>

<p>I am fairly new to survival analysis so after reading and digging through I realized that this is a non-parametric as I understand which means that it doesn't make any assumptions of the underline distributions of the time.</p>

<p>After reading about cox-proportional hazard function and going over <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">c-cran pdf</a> I performed a cox regression test and here is what I got from that: </p>

<pre><code>&gt; cox_model &lt;- coxph(Surv(time, event) ~ X)
&gt; summary(cox_model)
Call:
coxph(formula = Surv(time, event) ~ X)

  n= 139094, number of events= 137237 

         coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
X1 -7.655e-05  9.999e-01  1.504e-06 -50.897   &lt;2e-16 ***
X2 -1.649e-10  1.000e+00  5.715e-11  -2.886   0.0039 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   exp(coef) exp(-coef) lower .95 upper .95
X1    0.9999          1    0.9999    0.9999
X2    1.0000          1    1.0000    1.0000

Concordance= 0.847  (se = 0.001 )
Rsquare= 0.111   (max possible= 1 )
Likelihood ratio test= 16307  on 2 df,   p=0
Wald test            = 7379  on 2 df,   p=0
Score (logrank) test = 4628  on 2 df,   p=0
</code></pre>

<p>My big X is generated by doing rbind on total_like and total_para_length. Looking at Rsquare and P-Values I am not sure what really is going on here. If I can't throw away the Null-Hypothesis I should give a larger p-value. </p>
"
"0.0572129567690623","0.0574484989621426"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"0.0495478739876288","0.0497518595104995"," 97398","<p>I'm dealing with a GARCH-M model that I've estimated using R and EViews. Here are its mean and variance equations.</p>

<p>Mean equation:</p>

<p>$$ y_t=\mu + \rho \sigma^2_t + \varepsilon_t $$</p>

<p>Variance equation:</p>

<p>$$ \sigma^2_t = \omega + \alpha \varepsilon_{t-1}^2 + \beta \sigma^2_{t-1} + T$$</p>

<p>where T is a dummy variable containing 0 and 1 to indicate structural change.</p>

<p>Here is my EViews result:</p>

<blockquote>
  <p><img src=""http://s25.postimg.org/m8gc8y6ql/eviews_result.jpg"" alt=""eviews result""></p>
</blockquote>

<p>And my R code is as follows:</p>

<pre><code>#get data
re=read.table(""return.csv"",sep="","",header=TRUE)
......
xts&lt;-as.xts(re[,-1],order.by=re[,1])
......
T&lt;- as.matrix(xts[,2])

#GARCH specification
garchspec&lt;- ugarchspec(variance.model = list(model = ""sGARCH"", garchOrder = c(1, 1), 
                       submodel = NULL, external.regressors = T, 
                       variance.targeting = FALSE), 
                       mean.model = list(armaOrder = c(0, 0), include.mean = TRUE,
                       archm = TRUE, archpow = 1, arfima = FALSE, 
                       external.regressors = NULL, archex = FALSE), 
                       distribution.model = ""ged"",
                       start.pars = list(), fixed.pars = list())

#fitting
fit&lt;-ugarchfit(spec=garchspec, data=xts[,1], out.sample = 0,solver=""solnp"",
               solver.control = list(trace=0), fit.control = 
               list(stationarity = 1, fixed.se = 0, scale = 0, rec.init = 0.7))
show(fit)
</code></pre>

<p>It gives these results:</p>

<blockquote>
  <p><img src=""http://s25.postimg.org/ai2erkdy5/R_result.jpg"" alt=""R result""></p>
</blockquote>

<p>As you can see, the dummy variable (denoted by <code>vxreg1</code>) is totally insignificant using <code>rugarch</code> in R contrary to a 2.58% p-value in the EViews result. Other estimates have some differences with their counterparts, but they are all minor.</p>

<p>I checked the vignette of <code>rugarch</code> package for many times and cannot find any mistakes in the syntax, and R didn't show any error as well. I wonder what the problem is. 
I really appreciate it if you can solve my problem.</p>
"
"0.06396603026469","0.0642293744423385"," 99334","<p>I need a package for missing data imputation in R. But since I am dealing with big data, the number of missing data entries can also be high. The packages which impute using mean or median are of course working fast, but more complicated packages which impute using regression or PCA take too long for a high number of missing values. I tried <code>missMDA</code> and <code>missForest</code>, but as I said, they look like taking forever. There is a package named <code>FastImputation</code>, but I could not figure out how to use it when I have no patterns from some training data. Any suggestions of packages which would impute fast?</p>
"
"0.0572129567690623","0.043086374221607"," 99626","<p>I'm trying to show a correlation between growth in a petri dish of some fungi and its effect on a plant. I have ten strains of fungi which I tested in the plant and in petri dishes. I can put data from both experiments into linear models (lm) to get estimated means and variances. If I run a regression on the estimated means for each strain I find a significant correlation, but this doesn't take uncertainty in the strain means into account.
So far I've tried a parametric bootstrap, but I'm having a hard time figuring out the ultimate test statistic. I've included the parameter estimates and my R code. At the moment it generates simulated strain means from the estimated parameters. I need a p value for the covariance between growth and virulence. Any help would be greatly appreciated! My specific questions are:
1) Is a parametric boostrap the right way to go about analyzing this?
2) How would one go about doing this in R?</p>

<pre><code>Strain  MeanVirulence MeanGrowth    VirulenceVarGrowthVar  
1  -5.26064 0.066716    0.67834 0.053247  
2   -4.05482    -0.055524   0.68385 0.047111  
3   -5.47282    0.029047    0.68385 0.046739  
4   -3.50632    -0.161811   0.68385 0.047083  
5   -4.94051    -0.224949   0.68385 0.04727  
6   -4.04982    -0.062938   0.68385 0.047647  
7   -4.53178    -0.142985   0.68385 0.04788  
8   -3.01697    -0.199349   0.68385 0.047255  
9   -3.81093    -0.254793   0.68385 0.047255  
10  -1.61882    -0.325289   0.68385 0.0469
</code></pre>

<p>And here is my R code:</p>

<pre><code>gendata&lt;-function(par,npar=TRUE,print=TRUE){
    n     = 10
    k     = 2
    x=matrix(data=NA, nrow=n, ncol=k)
    for(i in 1:n){
      x[i,1] = rnorm(1,mean=par[i,2],sd=par[i,4])
      x[i,2] = rnorm(1,mean=par[i,3],sd=par[i,5])
    }
    return(x)
}

lmp &lt;- function (modelobject) {
    f &lt;- summary(modelobject)$fstatistic
    p &lt;- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) &lt;- NULL
    return(p)
}


samp=20000
rescor=matrix(data=NA, samp)
resvar=matrix(data=NA, samp)
pvals=matrix(data=NA, samp)
numsig = 0
numnotsig = 0
for (i in 1:samp){
    x&lt;-gendata(parameters)
    rescor[i]&lt;-cor(x[,1],x[,2], method = ""pearson"")
    resvar[i]&lt;-var(x[,1])
    a&lt;-lm(x[,1]~x[,2])
    pvals[i]&lt;-lmp(a)
    if (pvals[i] &lt; 0.05){
        numsig = numsig + 1
    }
    if (pvals[i] &gt; 0.05){
        numnotsig = numnotsig + 1
    }    
}

Strain Plate Growth  
1      1     200   
1      2     210  
1      3     190  
2      1     150   
2      2     130  
2      3     140  
...  

Strain Plant Growth  
1      1     70  
1      2     40  
1      3     50  
1      4     45  
2      1     80  
2      2     90  
2      3     85  
2      4     75  
...
</code></pre>
"
"0.0809113394062735","0.0812444463702388"," 99862","<p>Here's my situation.  </p>

<p>I have a multiple linear regression which I've used to come up with a prediction interval to predict a value y for a given (x1,x2,x3,x4,x5,x6).   It reads something like lower: 30, upper:48.  </p>

<p>I also have the same exact thing to predict a value y* at another given (x1*,x2*,x3*,x4*,x5*,x6*).  It reads something like lower:35, upper:51. </p>

<p>I want to answer this question:<br>
What is the probability that the value y* is greater than the value y?</p>

<p>I think it's a basic question, but I'm not sure. 
I could likely come up with this probability if I knew the formula for how the prediction interval is calculated in a multi-variable situation.<br>
Here's what I think should be done, but I wanted to run it by you guys first. </p>

<p>Prediction Intervals are based on a t-distribution with (n-6) degrees of freedom (I have a forced 0 y-int).  So I believe the margin of error calculated is then some constant multiplied by the corresponding value from the t-distribution (t_.05/2 with n-6 degrees of freedom).  The ""some constant"" would be the standard error of this particular estimate. </p>

<p>I then just do a basic 2 sample t-test using the point estimate prediction as the means and these constants as the standard errors with my n-6 degrees of freedom.   Is this accurate? </p>

<p>Is there a better way?</p>

<p>Thanks</p>
"
"0.121535457502911","0.115612873996209","100365","<p>We have route-level data (that I cannot share) on monthly bus ridership in New York City, creating a panel $N= 185$, $T=36$. We estimate a fixed effects model and random effects model with R's <code>plm</code> package. (For this MWE, I use the <code>Grunfeld</code> investment data, which illustrates the problems I am seeing fairly well).</p>

<pre><code>library(plm)
data(""Grunfeld"")
model_1A &lt;- lm(inv ~ value + capital, data= Grunfeld)
fe.plm &lt;- plm(formula(model_1A), model=""within"", index=c(""firm"", ""year""),
              data=Grunfeld)
re.plm &lt;- plm(formula(model_1A), model=""random"", index=c(""firm"", ""year""),
              data=Grunfeld)
</code></pre>

<p>A Hausman test indicates that the FE model is preferred, because the estimates differ (note that in the MWE, we fail to reject).</p>

<pre><code>phtest(fe.plm, re.plm)

##  Hausman Test

##data:  formula(model_1A)
##chisq = 2.3304, df = 2, p-value = 0.3119
##alternative hypothesis: one model is inconsistent

pbgtest(fe.plm)

##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models

##data:  formula(model_1A)
##chisq = 65.0632, df = 20, p-value = 1.14e-06
##alternative hypothesis: serial correlation in idiosyncratic errors

pbgtest(re.plm)

##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models

##data:  formula(model_1A)
##chisq = 69.9495, df = 20, p-value = 1.856e-07
##alternative hypothesis: serial correlation in idiosyncratic errors
</code></pre>

<p>This is where what I think should happen diverges from what many people try to do. My understanding of serial correlation is that it affects the standard errors but not the coefficients. This would suggest to me a serial correlation-robust standard error. For instance (<a href=""http://stats.stackexchange.com/a/60262/10026"">as in this answer</a>),</p>

<pre><code>fe.rse &lt;- sqrt(diag(vcovHC(fe.plm, type=""HC1"", cluster=""group"")))
re.rse &lt;- sqrt(diag(vcovHC(re.plm, type=""HC1"", cluster=""group"")))
</code></pre>

<p>** Why not just use sc-robust standard errors?**</p>

<p>But what many authors do instead is include specific AR(1) or ARMA disturbances because Stata makes this easy. For the FE models, we can use <code>gls</code> from the <code>nlme</code> package on demeaned data (note: <code>fe.plm</code> and <code>fe.gls</code> are virtually identical),</p>

<pre><code># within estimator is demeaned
demean &lt;- numcolwise(function(x) x - mean(x))
Grunfeld.dm &lt;- ddply(Grunfeld, .(firm), demean)
Grunfeld.dm$year &lt;- Grunfeld$year

fe.gls &lt;- gls(update(formula(model_1A), .~.-1), method=""ML"",  data=Grunfeld.dm)
fear.gls &lt;- update(fe.gls,  correlation = corAR1(form = ~ year | firm))
fearma.gls &lt;- update(fe.gls, correlation = corARMA(form = ~ year | firm, 
                                                   p=1,q=1))
</code></pre>

<p>The RE models can be estimated in with <code>lme</code> in the <code>nlme</code> package (again, <code>re.plm</code> and <code>re.lme</code> are identical).</p>

<pre><code>re.lme &lt;- lme(fixed = formula(model_1A), random = ~ 1|firm, data = mta)
rear.lme &lt;- update(re.lme,  correlation = corAR1(form = ~ year | firm))
rearma.lme &lt;- update(re.lme,  correlation = corARMA(form = ~ year | firm, 
                                                    p=1,q=1))
</code></pre>

<p>There are a few things I don't understand about this:</p>

<ul>
<li><p><strong>Why do the coefficients change when serial correlation doesn't (shouldn't?) affect estimates?</strong></p></li>
<li><p><strong>Can we still use the Hausman test to select between FE and RE models with autoregressive errors?</strong></p></li>
<li><p><strong>How can we test for residual autocorrelation? And if it exists, wouldn't we <em>still</em> need a robust standard error?</strong></p></li>
</ul>
"
"0.0809113394062735","0.0812444463702388","100682","<p>In ordinary least squares regression (OLS), if the plot of the residuals against the fitted values form a horizontal line around 0, then we can say that the dependent variable is linearly related to the independent variable.</p>

<p>I had thought that this is true because $E(y_i - \hat{y}_I)=0$ when the dependent variable is linearly related to the independent variable, see <a href=""http://stats.stackexchange.com/questions/100653/how-to-do-ordinary-least-squares-ols-when-the-observations-are-not-linear"">here</a>.</p>

<p>However, suppose:</p>

<p>$y_i = \alpha + \sin(x_i) + \epsilon_i$.</p>

<p>Then $E(y_i - \hat{y}_i)$ is still 0, see <a href=""http://stats.stackexchange.com/questions/100653/how-to-do-ordinary-least-squares-ols-when-the-observations-are-not-linear"">here</a> but then the plot of its residuals against its fitted value is no longer a horizontal line around 0, as this R code shows:</p>

<pre><code>n &lt;- 10^3
df &lt;- data.frame(x=runif(n, 1, 10))
df$mean.y.given.x &lt;- sin(df$x)
df$y &lt;- df$mean.y.given.x + rnorm(n)
model &lt;- lm(y ~ x, data=df)
plot(predict(model, newdata=df), residuals(model))
abline(a=0,b=0,col='blue')
</code></pre>

<p><img src=""http://i.stack.imgur.com/kyBwt.png"" alt=""enter image description here""></p>

<p>So my question is, which assumption(s) of OLS that causes the plot of the residuals and the fitted value to be a horizontal line around 0 and why/how is it true? </p>
"
"0.064873395163555","0.0759972207238908","100841","<p>Say I have below example data, where rows are observations and columns are variables, and NAs stand for missing values.</p>

<pre><code> 1  2 NA  4  5 6 14 5  2
 6 13  7  1 11 4 NA 9  6
15 12  3 12 NA 8  3 7 12
 8  1 NA  7  8 9  4 6  1
</code></pre>

<p>I want to impute the missing values by regression (I know I can impute by means, but I need to see how regression performs). There is a CRAN package named 'Amelia' for imputation by regression, but it gives an error for above data saying that #observations is smaller than #variables. 'mi' package also gives an error. I can code myself, but I do not want to reinvent the wheel since I am sure there is already a package for that which would work faster than the one I write (Speed is important since I will run this imputation for thousands of variables and hundreds of observations with lots of missing values). So, does anybody know about a package which would impute the values above by regression? Thanks.</p>
"
"0.06396603026469","0.0642293744423385","101020","<p>I am well aware how to read the model summary in R for a regression model when a factor is included. The ""first"" level, in terms of ABC, is regarded as the base level to which all further levels of that factor are compared to. In an ANOVA-style model the baseline value is found in the intercept (equals the mean response value for base-level class).</p>

<p>However, if one or several factors are mixed with continuous predictors, then how can I see what the base-level values are at all? to what would I compare? </p>

<p>In the model output below, there are two factors:</p>

<ol>
<li>LandUse (4 Levels)</li>
<li>Type_LU (4 Levels)</li>
</ol>

<p>I see now for example that <code>LandUseLow</code> is 0.35 units higher than the base-line <code>LandUseHigh</code>. Would one now simply look at the mean response for the class <code>LandUseHigh</code> and compare? Is it that simple? </p>

<pre><code>My_model: 
                  Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)
(Intercept)      4.6086772  1.7754606   1.7773711   2.593  0.00951 **
DiTempRange     -0.1409464  0.0764872   0.0765969   1.840  0.06575 .
LandUseLow       0.3520743  0.5989777   0.5997903   0.587  0.55721
LandUseMedium    0.2741413  0.3668149   0.3675811   0.746  0.45579
LandUseNone     -1.0128945  0.5735342   0.5744652   1.763  0.07787 .
MAP              0.0048810  0.0009128   0.0009142   5.339    1e-07 ***
Rivier           0.3502782  0.2252743   0.2257546   1.552  0.12076
TempRange        0.0823410  0.0606546   0.0607942   1.354  0.17560
Tmean           -0.1762862  0.0994486   0.0996353   1.769  0.07684 .
TYPE_LUconserva -0.9312487  0.4770681   0.4781244   1.948  0.05145 .
TYPE_LUprivate  -0.4839229  0.3289011   0.3296201   1.468  0.14207
TYPE_LUstate     0.0004062  0.4079678   0.4089744   0.001  0.99921
logVRM           0.1370973  0.1140166   0.1142342   1.200  0.23008
logTWI          -0.0735540  0.4195267   0.4202589   0.175  0.86106
logDAH           1.7132823  3.5937000   3.6028996   0.476  0.63441
</code></pre>
"
"0.0862517776135472","0.0866068708302494","103077","<p>I am having a lot of fun with regression analysis at the moment, and by fun I mean bashing myself repeatedly over the head. I have a set of 200 data points, by filtering on a property of interest, I end up with 153 points of use. </p>

<p>I initially used these 153 points to generate a linear regression, with an excellent R${^2}$ and a plot of fitted vs actual variables of almost a perfect diagonal. Great! However, it was suggested that this might only be an internally predictive model (which as I understand it means the model fits the data, rather than the opposite). So, I then tried this: I randomly selected a sample of 100 of the 153 results, and built the same model, it still gave a relatively good fit. I then used the predict function in R to try to predict the outcome of the other 53 records. It did not go well. What I got was one of 2 things.</p>

<ol>
<li>the predictions made no sense at all, not even on the same scale as the actual values.</li>
<li>most of the predictions made sense (although weren't very accurate) and one or two, were on an entirely different scale (orders of magnitude larger, or smaller).</li>
</ol>

<p>Since the model I am fitting has time as the response variable, it was suggested I use a Gamma fit regression instead of a plain old linear regression. I tried this and ended up essentially with the result.</p>

<p>So, am I using R correctly, was Gamma a good choice for this? I'm pretty sure my data is good (non biased) so if I am unable to predict, despite the good model - does this mean my model is useless? I've been working on this for some weeks now, and it would be great if I could salvage something.</p>

<p>The R commands I have used:</p>

<pre><code>modelSet&lt;-sample(1:nrow(myData),100)
modelData&lt;-myData[modelSet,]
predictData&lt;-myData[-modelSet,]

fit&lt;-lm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData)
pred&lt;-predict(fit, predictData)
plot(predictData$time, pred) &lt;- gives a really not useful plot


fit2&lt;-glm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData, family=Gamma) # tried with link=log too
pred2&lt;-predict(fit2, predictData)
plot(predictData$time, pred2) &lt;- gives an even less useful plot
</code></pre>
"
"0.072369302025194","0.090834052439095","103129","<p>I am trying to understand what the reported intercept is showing when I use <code>arima()</code> with <code>xreg=</code>. The documentation says</p>

<p>""If am xreg term is included, a linear regression (with a constant term if include.mean is true and there is no differencing) is fitted with an ARMA model for the error term.""</p>

<p>Thus I expect the intercept shown to come from the regression using <code>xreg=</code> as the X variables, before any arima model is done on those residuals. </p>

<p>However I tried to double check this by actually doing the regression with <code>lm()</code> and the intercept from that does not match what is reported from <code>arima()</code> (although the slope coefficient is pretty close). </p>

<p>Here is my example:</p>

<pre><code>set.seed(456)
v = rnorm(100,1,1)
x = cumsum(v)  ; x = as.xts(ts(x)) 

# Fit AR(1) after taking out a time trend (aka, drift)
model5 = arima(x, order=c(1,0,0), xreg=1:length(x), include.mean=TRUE)
# Coefficients:
#         ar1     intercept  1:length(x)
#       0.8995     0.8815       1.1113
# s.e.  0.0422     1.6193       0.0265


# Double check
MyTime = 1:length(x)
model5_Part1 = lm(x ~ MyTime )
# Coefficients:
#      (Intercept)       MyTime  
#         1.856           1.096
</code></pre>

<p>The intercepts do not match, thus I do not know what the intercept is showing from the arima with xreg.</p>

<p>Note the example shown is based on ""Issue 2"" shown here <a href=""http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm"" rel=""nofollow"">http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm</a></p>

<p>Also note that this isn't a problem particular to modeling drift. Here is another example, where in addition to the intercept not matching, even the slope coefficient on the <code>xreg=</code> variable doesn't match what is shown from using <code>lm()</code>. This example has nothing to do with drift and uses the cars dataset as if it were time series data.</p>

<pre><code>data(cars)
cars = as.xts(ts(cars, start=c(1980,1), freq=12))
model6 = arima(cars$speed, xreg=cars$dist, order=c(1,0,0), include.mean=TRUE)
# Coefficients:
#         ar1    intercept   dist
#       0.9979    15.2890  -0.0172
# s.e.  0.0030    10.5452   0.0055

model6_Part1 = lm(cars$speed ~ cars$dist)
# Coefficients:
#      (Intercept)    cars$dist  
#        8.2839        0.1656 
</code></pre>

<p>Intercepts do not match, slope coefficient does not match.</p>
"
"0.0404556697031367","0.0406222231851194","103234","<p>I'm working with an unbalanced panel dataset. (Country-Time) of approximate dimensions H=100 individuals i and average time length over individuals $mean(T_i)\approx7.5$. And about n= 8 regressors (appart from the fixed effects dummies).</p>

<p>I want to use the Durbin-Watson panel generalization from <em>Bhargava, Alok, Luisa Franzini, and Wiji Narendranathan. ""Serial correlation and the fixed effects model."" The Review of Economic Studies 49.4 (1982) 533-549</em> (<a href=""http://restud.oxfordjournals.org/content/49/4/533.short"" rel=""nofollow"">http://restud.oxfordjournals.org/content/49/4/533.short</a>)</p>

<p>$d_P = \frac{ \sum_{i=1}^H \sum_{t=2}^T \left( \tilde{u}_{it} - \tilde{u}_{it-1} \right)^2 }{ \sum_{i=1}^H \sum_{t=1}^T \tilde{u}_{it}^2 }$</p>

<p>Critical values are given in the paper and are dependent on (T, H, n) with n the number of regressors. Which i linearly interpolate. $cv(7.5, 100, 8) \to  d_{PL}=1.8561,\; d_{PU}=1.9039$</p>

<p>Now if i simulate data <em>without</em> autocorrelation I tend to find very low values for $d_P$ that reject $H0: \rho=0$</p>

<pre><code>#[R]
require(data.table)
set.seed(1)
DT &lt;- data.table(i=c(rep(1:50, each=7), rep(51:100, each=8)), 
                 t=c(rep(1:7, 50), rep(1:8, 50)), u=rnorm(100*7.5))
DT[, ':='(du2=c(NA, diff(u))^2, u2=u^2), by=i] # difference by individual

#       i t          u          du2         u2
#  1:   1 1 -0.6264538           NA 0.39244438
#  2:   1 2  0.1836433 0.6562573681 0.03372487
#  3:   1 3 -0.8356286 1.0389152808 0.69827518
#  4:   1 4  1.5952808 5.9093205817 2.54492084
# ---                                         
#746: 100 4 -1.3457937 0.1434371834 1.81116064
#747: 100 5  1.0336654 5.6618254864 1.06846414
#748: 100 6 -0.8117765 3.4056556180 0.65898102
#749: 100 7  1.8017255 6.8303923814 3.24621470
#750: 100 8  1.7715420 0.0009110448 3.13836092

DT.sumT &lt;- DT[, list(sumdu2=sum(du2, na.rm=T), sumu2=sum(u2)), by=i] #sum over T_i
#       i    sumdu2     sumu2
#  1:   1 12.239715  4.688696
#  2:   2  8.925133  8.698205
#  3:   3  2.463443  4.030220
# ---

d_P &lt;- DT.sumT[, sum(sumdu2)/sum(sumu2)] #sum over i
d_P # Durbin-Watson Statistic
# 1.708489
# next in seed gives --&gt; 1.617848, 1.735762, 1.614137
</code></pre>

<p>Which are all &lt; 2 so positive autocorrelation and all simulated $d_P&lt; d_{PL}$ so 5% significant. But the data is IID...</p>

<p>If a negative correlation of 0.2 is forced the test does flip side but still looks downward biased:</p>

<pre><code>DT[, u:=u-0.2*c(0, u[1:(length(u)-1)])]
# d_P --&gt; 2.038816
</code></pre>

<p>Am I missing something here? The tabulated 5%c.v. are for T=6 and T=10 so the short+wide property should not be a problem. Also balancing the simulation to e.g. $T_i=T=7$ gives low $d_P$</p>
"
"0.06396603026469","0.0642293744423385","103340","<p>I have a question about SAS and R. For a research, I used a longitudinal data and I initially used SAS (<code>GLIMMIX</code>) and then I analyzed the data with R (<code>glmer</code>) programming. There are differences between p-values of SAS and R. I expected that regression coefficient and standard error could be different for R and SAS. But there are differences for p value for some variables, which are significant in R, are not significant in SAS. </p>

<p>My R model and SAS model are respectively :</p>

<pre><code>#R
m3.glmm &lt;- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 +      
                     (1+timebefore+timeafter|id), 
                 data=data, family=binomial(link=""logit""), nAGQ=3)

#SAS
proc glimmix data=data METHOD=QUAD(QPOINTS=3) NOCLPRINT ;
  class id x2 x3 x4 x5;
  model y(event='1')=timebefore timeafter x1 x2 x3 x4 x5 
        x6 x7  x8 x9 x10 x11 /solution CL link = logit dist = binary;
  random intercept timebefore timeafter/subject = id GCORR SOLUTION;
run;
</code></pre>

<p>Eg: variable ""x1""(defined as age) was significant (p val= 0.04) in SAS but not in R (p val=0.1). But others were similar. It means that significant variables in SAS are found significant in R, or insignificant variables in SAS are insignificant in R. </p>

<p>Does anybody know about the differences?</p>
"
"0.10703564115707","0.107476300250388","103666","<p>I have trained my random forest model on a 74,000 training examples where each example consists of two proteins Amino Acids sequence (20 characters) and some numeric values representing the similarity between each individual pair of sequences, and finally a numeric value representing the overall similarity between the two proteins, this is a regression model, so I wish for testing I can use just protein sequence and use my trained model to predict the distance between my test case and each of my training protein sequences. A sample of my test case is:</p>

<pre><code>   test= G,Y,L,P,P,S, A,N,L,F,S,N, 1,-2,16,-4,-1,11, 21
</code></pre>

<p>where ""G,Y,L,P,P,S,"" represent a 6 character fragment of the first protein (my testing) and ""A,N,L,F,S,N,"" represent a 6 character fragment of the second protein (my training database) and the numbers ""1,-2,16,-4,-1,11,"" each number represent the similarity between individual pairs of the 6 Amino Acids, e.g., the similarity between ""G and A"" is 1 and the similarity between ""Y and N"" is -2 and the similarity between ""L and L"" is 16, and so on offcourse the higher the number means the higher the similarity between the pairs of characters, finally the last number ""21"" represent the sum of the previous 6 numbers which represents the overall similarity between the two sequences.</p>

<p>when I trained my model on 74,000 of such training datasets the correlation between the predicted distance and actual distance was as high as 0.86, however, when I used the trained model in for testing the correlation was very low 0.17, I strongly believe that this over-fitting problem, however, I'm not sure is it due to the may be not good training datasets or that my features aren't strong enough to give a good prediction especially since the ranking of the features according to their importance was very high for all the features? the following is my features importance according to each node purity. any help on how to recognize the source of the overfitting is highly appreciated:</p>

<pre><code>       IncNodePurity
 V3      24564.326
 V4      22503.744
 V5      25030.450
 V6      24583.235
 V7      24661.309
 V8      20757.662
 V9      22985.824
 V10     22189.759
 V11     23875.170
 V12     23674.853
 V13     23339.595
 V14     19576.762
 V15     10169.309
 V16     19527.972
 V17      5430.600
 V18      4415.307
 V19     12897.114
 V20      3963.717
 V21     62614.692
</code></pre>
"
"0.0756856276908142","0.0759972207238908","103786","<p>I have a model where time is the response variable. I'd like to generate confidence intervals for the estimates. I have established that the error in the estimation is roughly normally distributed (it may be more cauchy). The Mean and Median are very different, with the median more accurately representing the middle of the data. Am I allowed to use the median for my confidence interface and if so is there a different method for doing so?</p>

<p>I have reviewed this question: <a href=""http://stats.stackexchange.com/questions/21103/confidence-interval-for-median"">Confidence interval for median</a> but it is not clear if they are trying to accomplish the same thing I am.</p>

<p>EDIT
The model is for an estimation of the amount of time a process takes to complete. I performed a linear regression and established that the model has a relatively good fit. I then took repeatedly (2000 times) took a random sample of 75% of the original sample and rebuilt the model. I then predicted the time for the remaining 25%, and stored the error in each case. This led to ~90000 results, which roughly follow a normal distribution (or possibly cauchy) I would like to find an estimate for the confidence interval of an individual result, e.g. for one specific process the actual time taken was 46 seconds, and the predicted time taken was 1 minute. I'd like to be able to say with 95% certainty that my estimate is accurate within +- 15 seconds (for example).</p>
"
"0.0572129567690623","0.0574484989621426","103842","<p>Is there a way to statistically compare r-squared across 2 groups using nested models in multigroup analysis? I know how to use lavaan to test various other parameters across groups (e.g. regression coefficients, intercepts, variances...etc), but I can't find documentation for how to compare R-squared. </p>

<p>Here is an example of what I mean:</p>

<pre><code>HS.model &lt;-  'visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 
              visual + textual + speed ~ grade
              textual + speed ~ visual''
fit &lt;- cfa(HS.model, data=HolzingerSwineford1939, group=""sex"")
summary(fit, fit.measures=TRUE, rsquare=TRUE)
</code></pre>

<p>This produces (among other things) the following R-squared statistics:</p>

<pre><code>R-Square Group 1:

x1                0.519
x2                0.110
x3                0.375
x4                0.705
x5                0.729
x6                0.662
x7                0.458
x8                0.668
x9                0.266
visual            0.057
textual           0.217
speed             0.194

R-Square Group 2:

x1                0.669
x2                0.266
x3                0.304
x4                0.763
x5                0.721
x6                0.731
x7                0.320
x8                0.495
x9                0.515
visual            0.057
textual           0.258
speed             0.393
</code></pre>

<p>If I wanted to determine if ""grade"" and ""vision"" (predictors in the example) accounted for significantly more variability in speed for boys (.194) than girls (.393), how would I do that? </p>
"
"0.0572129567690623","0.0574484989621426","104548","<p>I followed <a href=""http://rtutorialseries.blogspot.hk/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">this tutorial</a> to learn Hierarchical Linear Regression (HLR) in R, but couldn't understand how to interpret its sample output of <code>&gt;anova(model1,model2,model3)</code></p>

<p><img src=""http://i.stack.imgur.com/MxXIM.png"" alt=""enter image description here""></p>

<p>The tutorial simply says </p>

<blockquote>
  <p>each predictor added along the way is making an important contribution to the overall model.</p>
</blockquote>

<p>But I would like some more details to <strong>quantify</strong> the contribution of each explanatory variable, like:</p>

<ol>
<li><p>""UNEM"" explains <code>X</code> (or <code>X%</code>) variance</p></li>
<li><p>Adding the ""HGRAD"" variable explains <code>Y</code> (or <code>Y%</code>) more variance</p></li>
<li><p>Adding the ""INC"" variable further explains <code>Z</code> (or <code>Z%</code>) more variance</p></li>
</ol>

<p>So, can I get the value of <code>X</code>, <code>Y</code>, and <code>Z</code> using the above ANOVA table? How? Specifically, what do <code>Res.Df</code>, <code>RSS</code>, <code>Sum of Sq</code> mean in this ANOVA table?</p>
"
"NaN","NaN","104571","<p>I have measurements for daily space heating vs daily mean outdoor temperature for two different control strategies. The data is shown here:</p>

<p><img src=""http://i.stack.imgur.com/c4L6Y.png"" alt=""enter image description here""></p>

<p>I have also performed linear regression, of the form:</p>

<pre><code>lm(Energy ~ Control * MeanOutdoorTemp)
</code></pre>

<p>This yields four coefficients:</p>

<pre><code>Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                170.8293     4.2083  40.594  &lt; 2e-16 ***
ControlMPC                 -30.7044     4.9025  -6.263 1.38e-07 ***
MeanOutdoorTemp             -6.2924     1.6466  -3.821 0.000413 ***
ControlMPC:MeanOutdoorTemp   0.8211     1.7162   0.478 0.634709    
</code></pre>

<p>I'd like to test whether these two regression lines cross at zero energy. What would be the statistically correct way to do that?</p>
"
"0.0990957479752576","0.0912117424359157","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.0572129567690623","0.0574484989621426","104733","<p>I have a vector of data with their standard errors:</p>

<pre><code>Estimate &lt;- c(0.254719513441046, 0.130492717014416, 0.0386710035855823, 0.14118562325405, 0.160649388742147, 0.60363287936294, 0.173485345603584, 0.425817607348994, 0.128802795868366, 0.104136474748465)
SE &lt;- c(0.126815201703205, 0.240179692822184, 0.248612907189712, 0.379800224602374, 0.0799874163236805, 0.170568135051654, 0.108163615496468, 0.0585237357996271, 0.16702614577514, 0.124308993809982)
</code></pre>

<p>I need to form a prediction interval for new elements that will be drawn from the same parent population. At first I thought I would do a linear regression to estimate the mean and the error on the mean:</p>

<pre><code>summary(lm(Estimate ~ 1, weights = 1/SE^2))
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.28014    0.04996   5.607 0.000331 ***
</code></pre>

<p>But when I try naively to call <code>predict()</code> here is what I get:</p>

<pre><code>predict(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0), interval = ""prediction"")
        fit       lwr      upr
1 0.2801405 -2.860802 3.421083
Warning message:
In predict.lm(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0),  :
  Assuming constant prediction variance even though model fit is weighted
</code></pre>

<p>Did I do this right? Is the 95% confidence interval really [-2.86 3.42]? Do I need to worry about the warning?</p>
"
"0.0904616275314925","0.090834052439095","104889","<p>I am working on cross-validation of prediction of my data with 200 subjects and 1000 variables. I am interested ridge regression as number of variables (I want to use) is greater than number of sample. So I want to use shrinkage estimators.  The following is made up example data:</p>

<pre><code> #random population of 200 subjects with 1000 variables 
    M &lt;- matrix(rep(0,200*100),200,1000)
    for (i in 1:200) {
    set.seed(i)
      M[i,] &lt;- ifelse(runif(1000)&lt;0.5,-1,1)
    }
    rownames(M) &lt;- 1:200

    #random yvars 
    set.seed(1234)
    u &lt;- rnorm(1000)
    g &lt;- as.vector(crossprod(t(M),u))
    h2 &lt;- 0.5 
    set.seed(234)
    y &lt;- g + rnorm(200,mean=0,sd=sqrt((1-h2)/h2*var(g)))

    myd &lt;- data.frame(y=y, M)
myd[1:10,1:10]

y X1 X2 X3 X4 X5 X6 X7 X8 X9
1   -7.443403 -1 -1  1  1 -1  1  1  1  1
2  -63.731438 -1  1  1 -1  1  1 -1  1 -1
3  -48.705165 -1  1 -1 -1  1  1 -1 -1  1
4   15.883502  1 -1 -1 -1  1 -1  1  1  1
5   19.087484 -1  1  1 -1 -1  1  1  1  1
6   44.066119  1  1 -1 -1  1  1  1  1  1
7  -26.871182  1 -1 -1 -1 -1  1 -1  1 -1
8  -63.120595 -1 -1  1  1 -1  1 -1  1  1
9   48.330940 -1 -1 -1 -1 -1 -1 -1 -1  1
10 -18.433047  1 -1 -1  1 -1 -1 -1 -1  1
</code></pre>

<p>I would like to do following for cross validation - </p>

<p>(1) split data into two halts - use first half as training and second half as test </p>

<p>(2) K-fold cross validation (say 10 fold or suggestion on any other appropriate fold for my case are welcome)  </p>

<p>I can simply sample the data into two (gaining and test) and use them: </p>

<pre><code># using holdout (50% of the data) cross validation 
training.id &lt;- sample(1:nrow(myd), round(nrow(myd)/2,0), replace = FALSE)
test.id &lt;- setdiff(1:nrow(myd), training.id)

 myd_train &lt;- myd[training.id,]
 myd_test  &lt;- myd[test.id,]   
</code></pre>

<p>I am using <code>lm.ridge</code> from <code>MASS</code> R package. </p>

<pre><code>library(MASS)
out.ridge=lm.ridge(y~., data=myd_train, lambda=seq(0, 100,0.001))
plot(out.ridge)
select(out.ridge)

lam=0.001
abline(v=lam)

out.ridge1 =lm.ridge(y~., data=myd_train, lambda=lam)
hist(out.ridge1$coef)
    out.ridge1$ym
hist(out.ridge1$xm)
</code></pre>

<p>I have two questions - </p>

<p>(1) How can I predict the test set and calculate accuracy (as correlation of predicted vs actual)?</p>

<p>(2) How can I perform K-fold validation? say 10-fold?</p>
"
"0.0707974219804893","0.0812444463702388","105346","<p>I am interested in estimating an adjusted risk ratio, analogous to how one estimates an adjusted odds ratio using logistic regression. Some literature (e.g., <a href=""http://aje.oxfordjournals.org/content/159/7/702.abstract"">this</a>) indicates that using Poisson regression with Huber-White standard errors is a model-based way to do this</p>

<p>I have not found literature on how adjusting for continuous covariates affects this. The following simple simulation demonstrates that this issue is not so straightforward: </p>

<pre><code>arr &lt;- function(BLR,RR,p,n,nr,ce)
{
   B = rep(0,nr)
   for(i in 1:nr){
   b &lt;- runif(n)&lt;p 
   x &lt;- rnorm(n)
   pr &lt;- exp( log(BLR) + log(RR)*b + ce*x)
   y &lt;- runif(n)&lt;pr
   model &lt;- glm(y ~ b + x, family=poisson)
   B[i] &lt;- coef(model)[2]
   }
   return( mean( exp(B), na.rm=TRUE )  )
}

set.seed(1234)
arr(.3, 2, .5, 200, 100, 0)
[1] 1.992103
arr(.3, 2, .5, 200, 100, .1)
[1] 1.980366
arr(.3, 2, .5, 200, 100, 1)
[1] 1.566326 
</code></pre>

<p>In this case, the true risk ratio is 2, which is recovered reliably when the covariate effect is small. But, when the covariate effect is large, this gets distorted. I assume this arises because the covariate effect can push up against the upper bound (1) and this contaminates the estimation.</p>

<p>I have looked but have not found any literature on adjusting for continuous covariates in adjusted risk ratio estimation. I am aware of the following posts on this site: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes"">Poisson regression to estimate relative risk for binary outcomes</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38004/poisson-regression-for-binary-data"">Poisson regression for binary data</a></li>
</ul>

<p>but they do not answer my question. Are there any papers on this? Are there any known cautions that should be exercised? </p>
"
"0.0404556697031367","0.0406222231851194","105457","<p>So, I had a weighted dynamic graph having info about 10 consecutive timesteps ( basically 10 files ). Now, I had to mine out patterns in the weight and structure of the complete graph. I did that. The output file (having around 10,000 rows) was something like,</p>

<pre><code>Node 1 Node 2 Pattern
 191    570    ""00""  
 21     570    ""00"" 
 378    570    ""00"" 
 459    570    ""00"" 
 552    570    ""00"" 
 223    570    ""00""
 197    570    ""00"" 
 570    689    ""00"" 
 ...................
</code></pre>

<p>Basically gives 2 nodes, and the pattern associated. What I wish to ask is, what kind of model like (linear regression, or bar plots, .... ) can I use here, in order to gather some meaningful info, i.e. let's say, I am able to come up with a prediction model or I can say, that these are 2 graphs which are similar in nature.</p>
"
"0.0404556697031367","0.0406222231851194","105796","<p>Im calculating a Structural Equation model with Partial Least Squares (with R).</p>

<p>Lets say a simple example:</p>

<ul>
<li>two Response values (R1, R2) are combined to a latent variable RespLV = weight1*R1 + weight2*R2</li>
<li>And a few covariates are also combined into latent variables (CoefLV1, CoefLV2, ...)</li>
<li>All latent Variables are standardized to with mean=0 and variance=1</li>
<li>Now a regression is performed with the result RespLV = beta1 * CoefLV1 + beta2 * CoefLV2 + ...</li>
</ul>

<p>It is now possible to do a prediction on the standardized RespLV. Is there a possibility to to a prediction on the unstandardized RespLV?</p>
"
"0.0756856276908142","0.0759972207238908","106183","<p>I want to run a regression in R with different datasets.
The question is whether stock performance (daily log return) is influenced by factors like interest rates (the one set by fed or ECB), size of the board of directors (e.g. has been long 7 and then switches to 8) and whether there is an audit committee or not (binary, <code>1</code> or <code>0</code>).</p>

<p>I guess in theory I could just add the interest rate, size of board, etc. to the daily log returns â€“ meaning the interest rate stays fixed for 80 days and then suddenly switches to a new value. However, because this is a large dataset of 10y daily log returns, and I just cannot figure out how to do it automatically in Excel, I am wondering whether it is possible to do that in R or to run a regression on the different data sets. Note: every dataset has a corresponding date.</p>

<p>Since 2 days ago, I've been searching the web but cannot find anything which solves my problem, so your help is really appreciated. If you have any links to papers or the like, it would be totally sufficient :) Although I did find a few papers on that topic, they were cutting-edge theory, which is way too complicated for me.
Thank you!</p>
"
"0.0904616275314925","0.090834052439095","106360","<p>I am running a binomial mixed effects logistic regression in R using <code>glmer</code> for a sociolinguistics project. I was asked to used deviation (effect) coding. From what I gather, in deviation coding the last level in a factor is assigned -1, because this is the level that is never compared to the other levels within that variable. Is it possible to obtain the <code>Estimate</code> (<code>Exp(B)</code> value) for the last level as well by using function <code>relevel</code>? I need to report the estimates for all the levels.</p>

<p>For example, my model has the independent variable called <strong>Orthography</strong> with four levels (<code>s</code>, <code>sh</code>, <code>s1</code>, <code>sh1</code>). The dependent variable is <strong>produced sibilant</strong>. In deviation coding the fourth level (<code>sh1</code>) will not be compared to the other three levels, and estimates will be available for the first three (<code>s</code>, <code>sh</code>, <code>s1</code>). The intercept is the mean of the means of all four levels (<code>s + sh + s1 + sh1 / 4</code>). I am interested in obtaining the estimate for the last level (<code>sh1</code>) as well. Does anyone know how to get that? Do I have to rerun the model by changing levels? If so, does anyone know how to do that? I have been unsuccessful with using function <code>relevel</code> to do this.</p>

<p>I have other terms in my model as well:  </p>

<ul>
<li>following segment, which has two levels (<code>vowel</code>, <code>consonant</code>), </li>
<li>position of sibilant in word (<code>initial</code>, <code>medial</code>, <code>final</code>), </li>
<li>grammatical function (<code>noun</code>, <code>verb</code>, <code>adjectives</code>), and </li>
<li>language of instruction (<code>English</code>, <code>Gujarati</code>).</li>
</ul>

<p>This is the code for my model:</p>

<pre><code>model.final_si = glmer(prod_sib ~ orthography + foll_segment + word_position + 
                                  grammatical_func + language_instruction + 
                                  (1|participant) + (1|item), 
                       family=""binomial"",data=data)
</code></pre>
"
"0.0572129567690623","0.0574484989621426","107643","<p>Let a linear regression model obtained by the R function lm would like to know if it is possible to obtain by the Mean Squared Error command.</p>

<p>I had the FOLLOWING output of an example</p>

<pre><code>&gt; lm &lt;- lm(MuscleMAss~Age,data)
&gt; sm&lt;-summary(lm)
&gt; sm

Call:
lm(formula = MuscleMAss ~ Age, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.1368  -6.1968  -0.5969   6.7607  23.4731 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 156.3466     5.5123   28.36   &lt;2e-16 ***
Age          -1.1900     0.0902  -13.19   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 8.173 on 58 degrees of freedom
Multiple R-squared:  0.7501,    Adjusted R-squared:  0.7458 
F-statistic: 174.1 on 1 and 58 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Multiple R-squared is the sum square error? if the answer is no could explain the meaning of Multiple R-squared and Multiple R-squared</p>
"
"0.156765720099655","0.157411114842338","107951","<p>The question I'm asking is related to this question <a href=""http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/106301#106301"">here</a> and <a href=""http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/106301#106301"">here</a>. And I apologize for asking so many questions here, as I am thoroughly a stats novice and probably my MD thinking is clouding my ability to interpret my findings. </p>

<p>So, because of the input I've received and following some contemplation, I've decided to run a Cox proportional hazards model on this dataset regarding the treatment of patients and how this affects their time of wound healing. As the duration ""wound healing"" is affected by censoring, following @Glen_b and @Frank Harrell's input, I believe running a normal linear and/or Poisson regression model (as the days are counted and the Poisson's residual qqplots actually have a more linear/normal distribution) isn't quite feasible, because neither of these models actually account for the fact, that when patients are healed, they drop out of the ""study"" and thus are ""censored"". Therefore I've decided to explore Cox proportional hazards modelling a bit further. </p>

<p>Although not originally planned from a study endpoint perspective (long story), I've decided to chose the need for operative intervention (because the wound fails to heal) as the time to event here (also because I need an event variable in the <code>coxph</code> and <code>Surv</code> function in R). 
The study sample size is 4918 patients, of which 575 required operative treatment (11.7%). Now I'm not even sure if this is a high enough event rate as the independent variables are n = 14 and if I go by the ""rule of thumb"" of 10 events per predictor variable, I'd actually require 1400 events (although I read <a href=""http://aje.oxfordjournals.org/content/165/6/710.full"" rel=""nofollow"">here</a>, that this rule can sometimes be relaxed), but that's also one of the reasons why I'm posting my question here. </p>

<p>I've run a multivariable Cox proportional hazards model, adjusting for those variables that may influence the decision to ""treat"" a patient and or/wound healing dynamics as well as the treatment the patients received. </p>

<p>The output looks as follows: </p>

<pre><code>library(survival)
multicoxph&lt;-coxph(Surv(Timetoheal,Operation==""Yes"")~FA
+Age+Woundsurface+Mechanism+Sup+Mid+Deep)
summary(multicoxph)
Call:
coxph(formula = Surv(Timetoheal, Operation == ""Yes"") ~ FA + Age + 
    Woundsurface + Mechanism + Sup + Mid + Deep)

  n= 4877, number of events= 575 
   (41 observations deleted due to missingness)

                         coef exp(coef)  se(coef)      z Pr(&gt;|z|)    
FAInadequate        -0.225084  0.798449  0.089056 -2.527   0.0115 *  
Age                 -0.012107  0.987966  0.002401 -5.042 4.60e-07 ***
Woundsurface         0.042953  1.043889  0.017338  2.477   0.0132 *  
Mechanism1          -0.341706  0.710557  0.168422 -2.029   0.0425 *  
Mechanism2          -0.169782  0.843848  0.312529 -0.543   0.5870    
Mechanism3          -0.421703  0.655929  0.523798 -0.805   0.4208    
Mechanism4          -0.131178  0.877062  0.175603 -0.747   0.4551    
Mechanism5           0.071292  1.073895  0.313708  0.227   0.8202    
Mechanism6          -1.132178  0.322331  0.473741 -2.390   0.0169 *  
Mechanism7          -1.216408  0.296292  0.308236 -3.946 7.94e-05 ***
Mechanism8          -0.011187  0.988876  0.162881 -0.069   0.9452    
SupYes              -1.476173  0.228511  1.121847 -1.316   0.1882    
MidYes              -0.056176  0.945373  1.010415 -0.056   0.9557    
DeepYes              1.568644  4.800135  1.002629  1.565   0.1177    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

                    exp(coef) exp(-coef) lower .95 upper .95
FAInadequate           0.7984     1.2524   0.67057    0.9507
Age                    0.9880     1.0122   0.98333    0.9926
Woundsurface           1.0439     0.9580   1.00901    1.0800
Mechanism1             0.7106     1.4073   0.51079    0.9885
Mechanism2             0.8438     1.1850   0.45734    1.5570
Mechanism3             0.6559     1.5246   0.23496    1.8311
Mechanism4             0.8771     1.1402   0.62167    1.2374
Mechanism5             1.0739     0.9312   0.58067    1.9861
Mechanism6             0.3223     3.1024   0.12737    0.8157
Mechanism7             0.2963     3.3750   0.16194    0.5421
Mechanism8             0.9889     1.0112   0.71862    1.3608
SupYes                 0.2285     4.3762   0.02535    2.0598
MidYes                 0.9454     1.0578   0.13048    6.8497
DeepYes                4.8001     0.2083   0.67269   34.2526

Concordance= 0.746  (se = 0.017 )
Rsquare= 0.066   (max possible= 0.769 )
Likelihood ratio test= 331.9  on 14 df,   p=0
Wald test            = 240.3  on 14 df,   p=0
Score (logrank) test = 288.5  on 14 df,   p=0
</code></pre>

<p>Now if I understand my results correctly, this is telling me that the variable <code>FAInadequate</code> reduces the hazards for the event which in this case is ""requirement for an operation"", right? Now this is a bit bizarre, because if I perform simple comparative statistics, I find that FAInadequate patients have longer healing times and more operations as shown by these results: </p>

<pre><code>t.test(log(Timetoheal+0.5)~FA, var.equal=T)

    Two Sample t-test

data:  log(Timetoheal + 0.5) by FA
t = -7.9285, df = 4916, p-value = 2.724e-15
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2773227 -0.1673666
sample estimates:
  mean in group Adequate mean in group Inadequate 
                2.338534                 2.560879 
</code></pre>

<p>and this</p>

<pre><code>require(gmodels)
CrossTable(Operation, FA, format=""SPSS"", chisq=T)

   Cell Contents
|-------------------------|
|                   Count |
| Chi-square contribution |
|             Row Percent |
|          Column Percent |
|           Total Percent |
|-------------------------|

Total Observations in Table:  4877 

             | FA 
    Operation|   Adequate | Inadequate |  Row Total | 
-------------|------------|------------|------------|
          No |      2583  |      1719  |      4302  | 
             |     2.764  |     3.835  |            | 
             |    60.042% |    39.958% |    88.210% | 
             |    91.143% |    84.141% |            | 
             |    52.963% |    35.247% |            | 
-------------|------------|------------|------------|
         Yes |       251  |       324  |       575  | 
             |    20.682  |    28.690  |            | 
             |    43.652% |    56.348% |    11.790% | 
             |     8.857% |    15.859% |            | 
             |     5.147% |     6.643% |            | 
-------------|------------|------------|------------|
Column Total |      2834  |      2043  |      4877  | 
             |    58.109% |    41.891% |            | 
-------------|------------|------------|------------|


Statistics for All Table Factors


Pearson's Chi-squared test 
------------------------------------------------------------
Chi^2 =  55.971     d.f. =  1     p =  7.354796e-14 

Pearson's Chi-squared test with Yates' continuity correction 
------------------------------------------------------------
Chi^2 =  55.29973     d.f. =  1     p =  1.03483e-13 


       Minimum expected frequency: 240.8704 
</code></pre>

<p>The fact of the matter is, that in plain ""medical"" terms, the longer it takes a wound to heal, the worse the outcome for the patient, so I would've originally expected the inadequate treatment to increase the hazards, especially in light of the simple t-test cross-tabulation results. Or could it be, that in light of this, running a proportional hazards function is incorrect and I should be looking at cumulative hazards? If that is the case how can I run a cumulative hazards function in R?  Further, I'm a bit concerned, that the likelihood ratio, logrank and Wald test all resulted in p-values = 0. Finally, how would you ideally chose to cross-validate this model and adjust for the slightly ""too low"" event per variable rate? </p>

<p>I'm sorry for all the questions, but this has been thoroughly been driving me nuts over the last couple of days, as I can't really wrap my head around the output for some reason. </p>

<p>I appreciate any help greatly. </p>

<p>Thanks. </p>
"
"0.0572129567690623","0.0574484989621426","108143","<p>I'm currently struggling with the latent change model that I try to realize in R with the lavaan package. R can't estimate all standard errors, which makes me believe that the model is not fully identified.
I did not have identification problems with the latent state model, so I guess the critical part may be due to the change parameter. </p>

<pre><code>      Latent_change_prepost &lt;- '
                 # first order latent variable definition
                      T1_43_1 =~ 1*A43_1_1 
                               + 1*A43_1_2 
                               + 1*A43_1_3 
                               + 1*A43_1_4 
                               + 1*A43_1_5 

                      T1_43_2 =~ 1*A43_2_1 
                               + 1*A43_2_2 
                               + 1*A43_2_3 
                               + 1*A43_2_4 
                               + 1*A43_2_5 

                      T1_43_7 =~ 1*A43_7_1 
                               + 1*A43_7_2 
                               + 1*A43_7_3 
                               + 1*A43_7_4 
                               + 1*A43_7_5 

                      T1_43_3 =~ 1*A43_3_1 
                               + 1*A43_3_2 
                               + 1*A43_3_3 
                               + 1*A43_3_4 

                      T1_43_4 =~ 1*A43_4_1 
                               + 1*A43_4_2 
                               + 1*A43_4_3 
                               + 1*A43_4_4 

                      T1_43_5 =~ 1*A43_5_1 
                               + 1*A43_5_2 
                               + 1*A43_5_3 


                  # second order latent variable definition
                      T1 =~ 1*T1_43_1
                          + 1*T1_43_2
                          + 1*T1_43_7

                      T2 =~ 1*T1_43_3
                          + 1*T1_43_4
                          + 1*T1_43_5

                  # regressions
                      change =~ 1*T2 #(no regression, need to introduce ""change"")
                      T2 ~ 1*T1

                  # (co)variances
                      change ~~ T1
                      T2 ~~ 0*T2

                  # intercepts
                      T1 ~ NA*1
                      T2 ~ NA*1
                      change ~ NA*1
                      T1_43_1 ~ NA*1
                      T1_43_2 ~ NA*1
                      T1_43_7 ~ NA*1
                      T1_43_3 ~ NA*1
                      T1_43_5 ~ NA*1
                    '

  fit2_latent_change &lt;- sem(
                            model = Latent_change_prepost
                          , data = diss_prepost_w_interv_kovar
                          , missing = ""fiml""
                          , meanstructure = TRUE
                          )
</code></pre>

<p>I get estimates of most parameters, but no std. errors. Also if I alternate the model slightly I might get std. errors but they are astronomically high.</p>

<p>Any ideas and help would be much appreciated!</p>
"
"0.0495478739876288","0.0497518595104995","108302","<p>I am running multiple linear regression with R.</p>

<pre><code>mod=lm(varP ~ var1 +var2+var3+var4)
</code></pre>

<p>The table is:</p>

<pre><code>all:
lm(formula = varP ~ var1 + var2 + var3 + var4)

Residuals:
    Min      1Q  Median      3Q     Max     
-4.9262 -0.6985  0.0472  0.7319  4.3305 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.700823   0.084737   8.271 1.45e-15 ***
var1      1.080172   0.175348   6.160 1.59e-09 ***
var2     -0.057803   0.007777  -7.432 5.25e-13 ***
var3     -9.924772   4.268235  -2.325   0.0205 *  
var4     -0.015104   0.001290 -11.710  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.139 on 460 degrees of freedom
Multiple R-squared:  0.657, Adjusted R-squared:  0.654 
F-statistic: 220.3 on 4 and 460 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>it means that my model explains 65.4% of the variance.
But now, I would like to determine the importance of each predictor.</p>

<p>I was using: </p>

<pre><code>lm.sumSquares(mod) 
</code></pre>

<p>Is dR-sqr relevant to interpret this importance ?</p>

<pre><code>              SS       dR-sqr pEta-sqr  df        F p-value
(Intercept)   88.73054 0.0510   0.1294   1  68.4015  0.0000
var4         177.88026 0.1022   0.2296   1 137.1262  0.0000
var2          71.65234 0.0412   0.1072   1  55.2361  0.0000
var1          49.22579 0.0283   0.0762   1  37.9477  0.0000
var3           7.01377 0.0040   0.0116   1   5.4069  0.0205

Error (SSE)  596.71237     NA       NA 460       NA      NA    
Total (SST) 1739.76088     NA       NA  NA       NA      NA
</code></pre>
"
"0.103142124625879","0.103566754353222","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.051172824211752","0.0642293744423385","108517","<p>I need to preform a multivariate normal regression in R. The question is: </p>

<blockquote>
  <p>Let $Y_1$, $Y_2$, and $Y_3$ follows multivariate normal distribution. What is </p>
  
  <ol>
  <li>the conditional of $Y_3$ given $Y_1$ and $Y_2$</li>
  <li><p>the conditional of $Y_2$ given $Y_1$</p>
  
  <p>From these two, derive:</p></li>
  <li><p>the joint distribution of $Y_3$ and $Y_2$ given $Y_1$. </p></li>
  </ol>
</blockquote>

<p>Now suppose you have a sample of size $n$ from the multivariate normal distribution. Do the two regressions in (1) and (2). <strong>How can I combine them to get (3), the regression of $Y_3$ and $Y_2$ on $Y_1$?</strong></p>

<pre><code>library(mvtnorm)
mu  &lt;- c(1,2,3)
Sig &lt;- matrix(c(4,2,1,2,4,-1,1,-1,4), nrow=3, ncol=3) 
Y   &lt;- rmvnorm(20, mean=mu, sigma=Sig) #generate multivariate normal distribution
y3  &lt;- lm(Y[,3]~Y[,1] + Y[,2]) 
y2  &lt;- lm(Y[,2]~Y[,1])
</code></pre>
"
"0.0202278348515684","0.0406222231851194","108899","<p>Can anyone explain the theory (or the formula) about computing Sum Sq (bold highligh below) related to regression items?  The Wikipedia <a href=""http://en.wikipedia.org/wiki/Partition_of_sums_of_squares"" rel=""nofollow"">link</a> gives an introduction on how to calculate the total, model, and regression sum of squares. Is it similar to the Sum Sq computation? Is the regression sum of squares equal to (0.000437+ 0.002545+ 0.060984+ 0.062330+ 0.060480)?</p>

<pre><code>TraingData &lt;- data.frame(x1 = c(3.532,2.868,2.868,3.532,2.868,2.536,3.864),
                         x2 = c(1.992,1.992,1.328,1.328,1.328,1.66,1.66),
                         y  = c(9.040330254,8.900894412,8.701929163,9.057944749,
                                8.701929163,8.74317832,9.10859913)
                         )
lm.sol &lt;- lm(y~1+x1+x2+I(x1^2)+I(x2^2)+I(x1*x2), data=TraingData)
anova(lm.sol)

Analysis of Variance Table

Response: y
            Df   **Sum Sq**     Mean       Sq F    value Pr(&gt;F)
x1          1   0.000437  0.000437    0.1055    0.8001
x2          1   0.002545  0.002545    0.6141    0.5768
I(x1^2)     1   0.060984  0.060984   14.7162    0.1623
I(x2^2)     1   0.062330  0.062330   15.0409    0.1607
I(x1 * x2)  1   0.060480  0.060480   14.5945    0.1630
Residuals   1   0.004144  0.004144  
</code></pre>
"
"0","0.0287242494810713","108904","<p>I did stepwise regression with my multiple regression model and using AIC as a measure of fit with the <code>step</code> function in R. Afterwards some variables that the stepwise regression did not eliminate was not significant (> 0.05 p-value). Does this mean i have to take out those variables with large p-values or what is a normal procedure?  </p>
"
"0.0904616275314925","0.090834052439095","109232","<p>I am using randomForest in R for regression, I have many categorical predictors (all of them have the same 3 categories (0,1,2)) and I want to see which of them can predict the response (continuous). I am trying this with many different response variables (one at the time) and all the models have a very low explained variance (basically 0, almost always negative).</p>

<p>I checked chi-square between pairs of variables and removed the ones that could be associated (p-value &lt; 0.05), but the result is the same.</p>

<p>My questions are:</p>

<p>1 - Is this possible? Am I doing something very wrong without noticing? If no:</p>

<p>2 - In random forest, do I have to throw everything away or can I still use the variable importance for classifying the predictors? (I don't think so, but since I couldn't find anything about this, I still hope I can get something out of it - BTW why does the plot predicted vs observed look good??). If no:</p>

<p>3 - Any suggestion? Also for alternative methods?</p>

<p>In the example below I don't divide the data in training and test for simplicity, but I did it in my code - same problem. Also, my original data set is much bigger (>500 observations and almost 100 predictors)</p>

<pre><code>## predictors
&gt; pred
   X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 X15 X16 X17 X18 X19 X20
1   0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0
2   0  1  2  2  2  0  1  2  0   0   1   0   0   1   1   2   2   1   1   2
3   0  1  0  2  2  1  1  2  1   1   2   1   0   0   1   2   2   2   0   0
4   0  0  1  1  1  1  1  1  1   1   1   1   0   0   2   0   2   2   0   1
5   0  1  1  2  2  0  1  2  2   1   2   0   0   0   1   1   0   2   0   1
6   1  1  0  2  2  1  1  1  2   1   0   1   0   1   1   2   2   2   1   2
7   0  1  1  1  1  1  2  1  2   1   2   1   0   1   1   2   1   2   1   1
8   0  1  2  1  0  1  0  2  1   1   1   2   0   0   1   2   1   2   1   2
........

## response
&gt; resp

[1]  19.416  46.058  39.496  79.752 301.012 746.377 277.721  13.922  15.598  82.195  86.263
[12]  82.522  30.829 101.369  31.496  39.366 133.510

## find optimal value of mtry for randomForest
&gt; bestmtry &lt;- tuneRF(pred, resp, ntreeTry=100,
+                    stepFactor=1.5,improve=0.01, trace=F, plot=F, dobest=FALSE)

## extract optimal value of mtry for randomForest
&gt; ind &lt;- as.numeric(names(which.min(bestmtry[,2])))

## Random Forest
&gt; RF &lt;-randomForest(pred, , y = resp, mtry=ind, ntree=500,
+ keep.forest=TRUE, importance=TRUE)

&gt; RF

Call:
     randomForest(x = pred, y = resp, ntree = 500, mtry = ind, importance = TRUE,          keep.forest = TRUE) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 9

          Mean of squared residuals: 32713.86
                    % Var explained: -6.5

## Low explained variance (pseudo - r sqaured)

&gt; RF.pr = predict(RF,pred)

## the plot isn't that bad though... it is if I use the test data set though
&gt; plot(RF.pr, resp)
&gt; abline(c(0,1),col=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/78z9Z.png"" alt=""enter image description here""></p>

<pre><code>&gt; varImpPlot(RF)
</code></pre>

<p><img src=""http://i.stack.imgur.com/1MHBJ.png"" alt=""enter image description here""></p>

<p>I have been stuck with this for a while now... any help is extremely appreciated</p>
"
"0.0286064783845312","0.0287242494810713","109234","<p>I applied the DW test to my regression model in R and I got a DW test statistic of 1.78 and a p-value of 2.2e-16 = 0.  </p>

<p>Does this mean there is no autocorrelation between the residuals because the stat is close to 2 with a small p-value or does it mean although the stat is close to 2 the p-value is small and thus we reject the null hypothesis of there existing no autocorrelation?</p>
"
"0.164402831052504","0.165079667307113","109464","<p>I am new to regression and having problem in solving Heteroscedasticity in OLS. Have done lots of homework and test before seeking your advice. Sharing the background and what I have done to solve the problem. Hope you can share your thoughts if my approach was correct.</p>

<p><strong>Objectives:</strong></p>

<ol>
<li>To find the relationship (model) between an explanatory variable (x) and an explained variable (y) using OLS regression.</li>
<li>if a model (relationship) is found, its usefulness and accuracy of prediction will be studied.</li>
</ol>

<p><strong>Dataset (Cross-sectional):</strong></p>

<ol>
<li>Have 4 datasets, with each 350 sample size.</li>
<li>Each dataset obtained using different intensity of experiment and this is already captured by the explanatory variable in x.</li>
<li>Due to the heterogenity of data, not possible to lump all into a single dataset.</li>
</ol>

<p><strong>Requirement:</strong></p>

<p>One common and statistically acceptable model for all the 4 datasets using OLS</p>

<p><strong>Steps Followed:</strong></p>

<ol>
<li><p>Explanatory Analysis: Found Non-linear relationship </p></li>
<li><p>As intending to use OLS, did 3 transformations of variables in attempt to have linearity:
a) ln(x) ~ ln(y);
b) ln(x) ~ y;
c) x ~ ln(y).
<strong>Note:</strong> Kept d) x ~ y as benchmark</p></li>
<li><p>Did heteroscedasticity test using Breusch-Pagan (BP) test in R for 2(a)-(d) for all the datasets in attempt to find valid model(s).
On the best case i.e 2b), only 2 out of 4 datasets passed the BP test (p-value>0.05)</p></li>
<li><p>As the aim is to have one common model for all the 4 datasets, another variable transformation is done using Tukey's Ladder of Transformation in attempt to have homoscedasticity:
a) ? ? {-2,-1,-0.5, 0.5, 1, 2} is used for x/y/x and y for each of the models in 2(a)-(d). Have total of 64 models (16 x 4) to consider. X and Y refer to the transformed x and y;
b) Now have 2 models passed BP test for 3 out of 4 datasets in the best case;
c) The one that failed has p-value &lt;2.20E-16.</p></li>
<li><p>[deadlock unable to find one valid model that passes all the 4 datasets]</p></li>
<li><p>Proceeded to take the two valid models in Step 4 and done inference Test:
a) the p-values for t-test and F-test are below 0.05 for all the 4 datasets;
b) R-square are above 0.9402 for all the 4 datasets.</p></li>
<li><p>Did cross validation and selected the best model using the smallest mean square error against the two ""valid"" models. Did back transformation on the original scale first before the selection is done so that its apple to apple data comparison. The mean average percentage error for the best model is below 10%</p></li>
<li><p>Now tried to use the best model for prediction:
a) Selected 20 random x values which were not part of the dataset;
b) Predicted y and compared it against Measured y;
c) the  mean average percentage error is below 8% and within the model's mean average percentage error i.e below 10%.</p></li>
</ol>

<p><strong>The problem:</strong></p>

<p>With the steps above I am unable to get a model that passes the heteroscedasticity test all the 4 datasets. Have I done anything incorrectly or is there anything more can be done in Step 4? </p>

<p>Believe mis-specification issue has duly been attended. Not intending to use GLS as I need to use .OLS</p>

<p>I have used heteroscedasticity robust standard errors as a remedy of heteroscedasticity on the one dataset that failed BP test per the Youtube below.
Refer - <a href=""https://www.youtube.com/watch?v=hFoDDwTF4KY"" rel=""nofollow"">https://www.youtube.com/watch?v=hFoDDwTF4KY</a></p>

<p>The standard error increased and t-value decreased for Y for the HC3 corrected dataset. 
But the Y= a  + b X model remain the same.</p>

<p>Is it sufficient to show the p-value for t-test and F-test for the corrected dataset are still below 0.05 hence its ok to use the same Y= a+bX though it failed the BP test earlier?</p>

<p>Hope you can share your thoughts as I am new to regression. </p>

<p>Using many reference books to learn such as </p>

<ol>
<li>Introduction to Econometrics by Wooldridge</li>
<li>Basic Econometrics by Gujerati</li>
<li>Regression Analysis by Example by Chatterjee</li>
</ol>

<p><strong>Original:</strong></p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.612116   0.009006  -68.76   &lt;2e-16 ***
Y                     5.955984   0.039653  145.65   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic: 2.092e+04 on 1 and 348 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Heteroskedasticity Robust Standard Errors corrected using HC3:</strong></p>

<pre><code>Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.61212    0.01767  -33.77   &lt;2e-16 ***
Y                     5.95598    0.08432   69.12   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic:  4640 on 1 and 348 DF,  p-value: &lt; 2.2e-16

Note: Heteroscedasticity-consistent standard errors using adjustment hc3 
</code></pre>

<p>Thanks</p>
"
"0.0756856276908142","0.0759972207238908","109796","<p>I am currently working on a project using a sales system and trying to come up with a way to use the current pipeline of potential sales to predict the amount of product that will be sold in the future. Iâ€™m looking for advice on how to approach this problem and hopefully some resources to teach me what approach to use and why.</p>

<p>The sales system Iâ€™m using has historical data for opportunities (potential sales). Around 50,000 of the opportunities are â€œclosedâ€ meaning that they are either won or lost. I have around 1,000 â€œopenâ€ opportunities that have not yet been won or lost. Some variables that I have on each sale include the product (which is generally homogenous except for the amount), the amount, the salesman, the date, the time it was input into the system, the customer, and other data about the customer.</p>

<p>I understand that if I want to predict a dichotomous variable like win / lose then I should look at a logistic regression. However, Iâ€™m looking for general advice on how to </p>

<ol>
<li>Predict the probability of each individual opportunity closing as won using the data I have (and how to tell if I've done it correctly).</li>
<li>Estimate the total amount of won opportunities for a period.</li>
</ol>

<p>I found a similar question here <a href=""http://stats.stackexchange.com/questions/66276/using-a-logistic-model-on-the-estimates-of-several-other-classification-models"">Using a logistic model on the estimates of several other classification models</a> but Iâ€™m hoping for a response that gives me a better idea of where to start. Iâ€™m comfortable using R or any other statistical software, but ideally I'd like some kind of book or other reference material that is as low-level as possible.</p>
"
"0.12793206052938","0.122035811440443","110033","<p>I am running a post-hoc analysis on the data collected during an experiment in which 15 unique stimuli were presented to participants. Having run a least squares regression using the lm() function in R I have found significant results for a subset of the data including 90 observations from 6 participants with two continuous variables and their interaction.</p>

<p>Taking advice from an article by Judd, Westfall &amp; Kenny (2012) I attempted to use a combination of the lmer() function found in the lme4 package in combination with a Kenward-Roger approximation through the KRmodcomp() function in the pbkrtest package (see the appendix in the article) in order to control for random effects:</p>

<pre><code>lmer(Prediction_Difference_Scale~Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale + (1|Unique_ID) + (Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale|Block),data=Data)
</code></pre>

<p>The first variable after the DV is the fixed effect, the second variable in parentheses indicates that the intercept is random with respect the unique stimuli (Unique_ID) and the third variable in parentheses indicates that both the intercept and the Condition slopes are random with respect to participant (Block) and that a covariance between the effects should be estimated. </p>

<p>When running the lmer() function I get the following error message:</p>

<pre><code>Error in checkNlevels(reTrms$flist, n = n, control) : 
  number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>This is obviously because the number of observations equal the number of unique stimuli.</p>

<p>The function works when excluding the (1|Unique_ID) random  effect, which if I understand correctly is the same as carrying out a 'by stimulus' analysis. However, the authors warn against this by stating: ""Conceptually, a significant by-participant result suggests that experimental results would be likely to replicate for a new set of participants, but only using the same sample of stimuli. A significant by-stimulus result, on the other hand, suggests that experimental results would be likely to replicate for a new set of stimuli, but only using the same sample of participants. However, it is a fallacy to assume that the conjunction of these two results implies that a result would be likely to replicate with simultaneously new samples of both participants and stimuli.""</p>

<p>I would like to control for the random effects of both stimuli and participants, but I am unsure how to proceed?</p>

<p>The article can be accessed here: <a href=""http://jakewestfall.org/publications/JWK.pdf"" rel=""nofollow"">http://jakewestfall.org/publications/JWK.pdf</a></p>

<hr>

<p>To clarify the question regarding the 15 unique stimuli, this is 15 unique stimuli per participant, meaning the sample of 90 observations consists of 6 participants. The stimuli for all of the 90 observations are unique however.</p>

<p>I suppose what my question boils down to is whether there is even a need to include the (1|Unique_ID) 'variable' in the function formula as there is no error dependence between any of the stimuli?</p>
"
"0.0404556697031367","0.0406222231851194","110578","<p>I am trying to reproduce table 2.1 (p. 41) of LeSage and Pace (2009) by means of the <strong>open R software</strong>.</p>

<p>This is a table containing the direct, indirect and total impacts of a SAR model (since the regression coefficients cannot be interpreted directly because of spillovers), <strong>partitioned by W-order</strong>.</p>

<p>I am able to produce the first part of the table ( impacts() in R ), but for the moment I don't have a clue on <strong>how to split these impacts per power of W in the R software</strong>.</p>

<p>Please, could somebody give me a hint on how to do this? I would be very grateful!</p>

<p>Thank you very much in advance,</p>

<p>Janka</p>
"
"NaN","NaN","110597","<p>My question is simple: is there a function in <code>R</code> which estimates the linear regresion model in a similar fashion as <code>lm</code>, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.</p>
"
"0.064873395163555","0.0651404749061921","110618","<p>I have a regression with ARMA errors, which I am fitting with <code>arima()</code>. I know that the ARMA model is being fit on these residuals from the regression. My problem is that when I use <code>include.mean=TRUE</code> the output does not return any estimate of the mean (of the residuals, which yes, I know is always zero). The documentation says that ARMA model will include a mean if this option is set to <code>TRUE</code>. </p>

<p>I do see an intercept reported, which is the intercept from the regression of y on x, not the intercept from the ARMA model on the residuals.</p>

<p>How do I get the <code>include.mean</code> argument to work? Is there any other way to get the intercept from the ARMA model on residuals? Is R just censoring this because residuals are always mean zero (if intercept is in regression)?</p>

<p>MWE is below, thanks in advance.</p>

<p><code>set.seed(123)
y = as.xts(ts(rnorm(20)+3, start=c(1980,1), freq=12))
x = as.xts(ts(rnorm(20)+5, start=c(1980,1), freq=12))</code></p>

<p><code>fit1 = arima(y, xreg=as.data.frame(x), order=c(1,0,0), include.mean=TRUE)
fit1 # intercept shown is from the regression, 3-(-.0935)*(5), not for the residual ARMA model</code></p>
"
"0.0904616275314925","0.072667241951276","111287","<p>For a simulation study I have to generate data according to a trivariate mediation model (Baron &amp; Kennys causal step approach). Assuming $X$, $M$, and $Y$ are continuous variables of sample size $n$, the following regression equations constitute the data generating process: </p>

<p>$M = \alpha + aX + \epsilon$</p>

<p>$Y = \alpha + cX + bM + \epsilon$</p>

<p>Additionally, $X$ is not normally distributed, but is sampled from a distribution with skewness. I chose log-normal distribution in my current R implementation (see below). Sample size is also highly variable, starting at $n = 15$ and going up to $n = 15.000$.    </p>

<p>Here is my admittedly really primitive and inefficient code: </p>

<p>Sample from <code>rlnorm()</code> until a random sample with desired skewness is found (for clarification: <code>tol</code> refers to a tolerance parameter set to $.01$). </p>

<pre><code>library(moments)
s &lt;- 1 # desired skewness
tol &lt;- .01
n &lt;- 20
a &lt;- b &lt;- c &lt;- .59
skewness &lt;- sqrt(log(-1+2^(1/3)/(2+s^2+sqrt(4*s^2+s^4))^(1/3)+(2+s^2+sqrt(4*s^2+s^4))^(1/3)/2^(1/3)))

repeat{X &lt;- rlnorm(n=n, meanlog=0, sdlog=skewness)
    if (skewness(X) &gt; (skewness - tol) &amp;&amp; skewness(X) &lt; (skewness + tol)) {break}
} 
</code></pre>

<p>Then calculate $M$ and $Y$ from $X$ in a similar fashion and according to the regression equations above (where $a$, $b$ and $c$ are the correlations between the variables):</p>

<pre><code>repeat{
    M &lt;- a * X + rnorm(n)
    if (cor(X, M) &gt; (a - tol) &amp;&amp; cor(X, M) &lt; (a + tol)) {break}
}

repeat{
    Y &lt;- c * X + b * M + rnorm(n)
    if (cor(Y, X) &gt; (c - tol) &amp;&amp; cor(Y, X) &lt; (c + tol) &amp;&amp;
        cor(Y, M) &gt; (b - tol) &amp;&amp; cor(Y, M) &lt; (b + tol)) {break}
}
</code></pre>

<p>Again, this code is woefully ineffective and is only applicable for very low $n$, as the computation time is pretty low for small samples. Once higher sample sizes are reached (about $n=200$), it stops working altogether.  </p>

<p>So here are my actual questions:</p>

<ul>
<li>Is there a way to generate intercorrelated data ($X$, $M$ and $Y$) when $X$ isn't normally distributed and therefore <code>mvrnorm</code> from the <code>MASS</code> package can't be used? Also the correlation between the variables has to be either an exact value, or within a certain range ($\pm$ tolerance). </li>
<li>Assuming it can be done theoretically (which I am quite sure of), how can it be implemented in <code>R</code>?           </li>
</ul>
"
"0.06396603026469","0.0513834995538708","111902","<p>I am conducting a two-sample test (1-way ANOVA with 2 treatments), and the goal is to estimate the ratio of cell means assuming that the data are lognormal. A simple approach is to log the response and fit a model </p>

<p>$\log Y = b_0 + b_1 * X$</p>

<p>and then estimate the ratio as</p>

<p>$R = e^{b_1}$</p>

<p>However, that gives the ratio of geometric cell means rather than arithmetic cell means. </p>

<p>I assumed that if I fit a ""proper"" lognormal model using either <code>gamlss</code> in R or <code>PROC GLIMMIX</code> in SAS, I will get the ratio of arithmetic means, but for some reason both procedures generate the same slope as the $\log Y$ regression.</p>

<p>This is odd because when I use this approach with Poisson or Negative Binomial regression, I do get the ratio of arithmetic means. What am I missing?</p>

<hr>

<p>P.S.</p>

<p>I think I identified the source of confusion, but I don't have an explanation for it. A lognormal setup with the identity link function is:</p>

<p>$\log Y_1 \sim N(b_0, \sigma^2)$</p>

<p>$\log Y_2 \sim N(b_0 + b_1, \sigma^2)$</p>

<p>which implies </p>

<p>$\frac{E[Y_2]}{E[Y_1]} = \frac{e^{b_0 + b_1 +\sigma^2/2}}{e^{b_0 + \sigma^2/2}} = e^{b_1}$</p>

<p>To me, it means that $e^{b_1}$ should have a point estimate equal to the ratio of arithmetic means for the original response.</p>

<p>On the other hand,</p>

<p>$E[\log Y_1] = b_0$</p>

<p>$E[\log Y_2] =  b_0 + b_1$</p>

<p>$b_0$ is estimated as arithmetic mean of $\log Y_1$, $b_0 + b_1$ is estimated as arithmetic mean of $\log Y_2$. Hence, $e^{b_1}$ should have
a point estimate equal to the ratio of geometric means for the original response, and it does, given the output from those two packages. Where did I make a mistake?</p>
"
"NaN","NaN","112087","<p>I have a dataframe with the following column: Year, Temperature, Num.Species which are all numeric variables. Each row represent the number of species observed a certain year and the mean temperature that year.</p>

<p>I want to test if the regression slope of Year~Temperature, differs from the regression slope of Year~Temperature when using Num.Species as a weighting factor.</p>

<p>From what I understood so far, it is possible to do this with an ANCOVA. But there is no categorical variable in my data.  </p>

<p>Any help would be appreciated!</p>
"
"0.0756856276908142","0.0759972207238908","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"NaN","NaN","112442","<p>While building a regression model in R (<code>lm</code>), I am frequently getting this message</p>

<pre><code>""there are aliased coefficients in the model""
</code></pre>

<p>What exactly does it mean?</p>

<p>Also, due to this <code>predict()</code> is also giving a warning.</p>

<p>Though it's just a warning, I want to know how can we detect/remove aliased coefficients before building a model.</p>

<p>Also, what are the probable consequences of neglecting this warning?</p>
"
"0.0809113394062735","0.0710888905739589","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.0495478739876288","0.0497518595104995","113476","<p>I am wondering are there any assumptions that must be met in the proportional hazard regression model with frailty? I remember that in regular proportional hazard model without frailty all variables need to significant in log-rank test which means that the survival curves for each variable differ significantly and are proportional.</p>

<p>Moreover I was thinking about model diagnostics. Is Schoenfeld test appropriate for model with frailties? Also do scored Schoenfeld's residuals (or martingale residuals) are a good measure of model fit like in model without frailty?</p>

<p>Maybe there are no assumptions and diagnostics for this kind of model?</p>

<p>And quick question about model conclusions. Below is syntax of a model fit and I am curious about p-value next to frailty. It is higher than 0.05. Does it mean there is no significant frailty in the data? Also <code>Variance of random effect= 0.493</code>. Does it stand for frailty or opposite that the frailty model is inappropriate?</p>

<p>Thanks for any help!</p>

<pre><code>Call:
coxph(formula = Surv(time, inf) ~ age + sex + frailty(patient, 
    dist = ""gaus"") + as.factor(type), data = kidneyExt)

                          coef     se(coef) se2    Chisq DF   p      
age                        0.00489 0.015    0.0106  0.11  1.0 0.74000
sex                       -1.69728 0.461    0.3617 13.55  1.0 0.00023
frailty(patient, dist = ""                          17.89 12.1 0.12000
as.factor(type)1           0.21308 0.516    0.3828  0.17  1.0 0.68000
as.factor(type)2          -1.31617 0.781    0.5817  2.84  1.0 0.09200
as.factor(type)3          -0.17986 0.545    0.3927  0.11  1.0 0.74000

Iterations: 7 outer, 42 Newton-Raphson
     Variance of random effect= 0.493 
Degrees of freedom for terms=  0.5  0.6 12.1  1.7 
Likelihood ratio test=47.5  on 14.9 df, p=2.82e-05  n= 76  
</code></pre>
"
"0.0495478739876288","0.0497518595104995","113737","<p>I am performing linear regression in R and I have a variable called diversityscore which is a value ranging from 1 to 10 indicating #activities a user performs with 1 meaning one activity to 10 meaning all ten activities. I am not sure if this is to be counted as a factor variable or a non-factor variable. How do I make this decision?</p>

<p>(Expanding the question for the comprehensive answer below:...)</p>

<p>If it is (not) a factor, is this true for all ratio variables (with meaningful 0 and equal intervals)? For ex, temperature in Kelvin (assuming discrete values in output), age, etc? How about dates? I'm guessing they must be counted as factors. Can we take nominal and ordinal variables in general to be factors? or any more exceptions that I should be watchful of?</p>
"
"0.0858194351535935","0.0861727484432139","113756","<p>I'd like to test the <em>anova rbf kernel</em> included in the <strong>kernlab</strong> package in <strong>caret</strong>. Following excelent tutorial (<a href=""https://topepo.github.io/caret/custom_models.html"" rel=""nofollow"">https://topepo.github.io/caret/custom_models.html</a>) I've come up with the following code:</p>

<pre><code>SVManova &lt;- list(type = ""Regression"", library = ""kernlab"", loop = NULL)
prmanova &lt;- data.frame(parameter = c(""C"", ""sigma"", ""degree"", ""eps""),
                     class = rep(""numeric"", 4),
                     label = c(""Cost"", ""Sigma"", ""Degree"", ""Eps""))
SVManova$parameters &lt;- prmanova
    svmGridanova &lt;- function(x, y, len = NULL) {
    library(kernlab)
    sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)
    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,
                C = 2 ^(-5:len), degree = 1:2) # len = tuneLength in train
    }
    SVManova$grid &lt;- svmGridanova
svmFitanova &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  ksvm(x = as.matrix(x), y = y,
       kernel = ""anovadot"",
       kpar = list(sigma = param$sigma, degree = param$degree),
       C = param$C, epsilon = param$epsilon,
       prob.model = classProbs,
       ...) #default type = ""eps-svr""
}
SVManova$fit &lt;- svmFitanova
    svmPredanova &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata)
    SVManova$predict &lt;- svmPredanova
svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type=""probabilities"")
SVManova$prob &lt;- svmProb
    svmSortanova &lt;- function(x) x[order(x$C), ]
SVManova$sort &lt;- svmSortanova
</code></pre>

<p>I then asked for the model to train some dataset:</p>

<pre><code>set.seed(100) #use the same seed to train different models
svrFitanova &lt;- train(R ~ .,
                data = trainSet,
                method = SVManova,
                preProc = c(""center"", ""scale""),
                trControl = ctrl, tuneLength = 20,
                allowParallel = TRUE) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = ""ROC""
#Print the results
svrFitanova
</code></pre>

<p>But I get the following error:</p>

<pre><code>Error in train.default(x, y, weights = w, ...) : 
  The tuning parameter grid should have columns C, sigma, degree, eps
</code></pre>

<p>I don't see why this error occurs.... tune grid has four columns as requested... Any ideas? Thanks</p>
"
"0.0917448352774886","0.107476300250388","114468","<p>I am using the metafor package in R. I have fit a random effects model with a continuous predictor as follows </p>

<pre><code>SIZE=rma(yi=Ds,sei=SE,data=VPPOOLed,mods=~SIZE)
</code></pre>

<p>Which yields the output:</p>

<pre><code>R^2 (amount of heterogeneity accounted for):            63.62%
Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 9.3255, p-val = 0.0023

Model Results:

                 se    zval    pval   ci.lb   ci.ub    
intrcpt  0.3266  0.1030  3.1721  0.0015  0.1248  0.5285  **
SIZE     0.0481  0.0157  3.0538  0.0023  0.0172  0.0790  **
</code></pre>

<p>Below I have plotted the regression.The effect sizes are plotted proportionally to the inverse of the standard error. I realize that this is a subjective statement, but the R2 (63% variance explained) value seems a lot larger than is reflected by the modest relationship shown in the plot (even taking weights into account).</p>

<p><img src=""http://i.stack.imgur.com/3JNmM.jpg"" alt=""enter image description here""></p>

<p>To show you what I mean, If I then do the same regression with the lm function (specifying study weights in the same way):</p>

<pre><code>lmod=lm(Ds~SIZE,weights=1/SE,data=VPPOOLed)
</code></pre>

<p>Then the R2 drops to 28% variance explained. This seems closer to the way things are (or at least, my impression of what kind of R2 should correspond to the plot). </p>

<p>I realize, after having read this article (including the meta-regression section): (<a href=""http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme"">http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme</a>), that differences in the way the lm and rma functions apply weights can influence the model coefficients. However, it is still unclear to me why the R2 values are so much larger in the case of meta-regression. Why does a model that looks to have a modest fit account for over half the heterogeneity in effects?</p>

<p>Is the larger R2 value because the variance is partitioned differently in the meta analytic case? (sampling variability v other sources) Specifically, does the R2 reflect the percent of heterogeneity accounted for <em>within the portion that cant be attributed to sampling variability</em>?. Perhaps there is a difference between ""variance"" in a non-meta-analytic regression and ""heterogeneity"" in a meta-analytic regression that I am not appreciating.</p>

<p>I'm afraid subjective statements like ""It doesn't seem right"" are all I have to go on here. Any help with interpreting R2 in the meta-regression case would be much appreciated. </p>
"
"0.174066697682044","0.174783319475946","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.0858194351535935","0.0861727484432139","115126","<p>I need to do a Multiple Imputation on a dataset with several missing values, and I need to do it with mice, because later I'll have to compare the results with those of imputations ran with other programs.</p>

<p>My colleague obtained a completa dataset by running a MI on the incomplete dataset, with 5 iterations, and then taking the 5 imputed datasets and manually calculating mean values. Se essentially he pooled manually. I'm far from an expert so I don't know if this operation is valid. </p>

<p>Anyways, in MICE so far I could run the imputation (again, with maxit=5), using the function ""imp&lt;-mice(eco)"", where ""eco"" is the incomplete dataset. So I obtained the 5 imputed datasets, stored in the object ""imp"", of class ""MIDS"". Now I just need to pool the 5 completed datasets to obtain a unique complete one, i don't wanna run analyses on the 5 datasets and then pool the results. Can that be done? If I got it right from the manual, it seems that MICE allows you to pool only after you ran some analysis on the imputed datasets. The analysis is repeated on each dataset and the results are stored in an object of class ""MIRA"". I tried to run the function ""pool()"" on ""imp"" but it can't be done because imp is of class ""mids"" and you can only pool mira objects.</p>

<p>The manual also says that to pool you need a variance/covariance relation, and in the example given there they run a linear regression and then they pool the results of the regression. But I doubt if it's what I need. I'm confused</p>
"
"0.0908377689773195","0.0995037190209989","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.0908377689773195","0.0912117424359157","115219","<p><strong><em>Imagine the situation:</em></strong> Mythical Seafolk use holes in the seabed as their burrows. Each hole has two parameters - diameter and depth. <strong>Majority of holes are unoccupied</strong> due to their surplus (n = 235). Occupied holes (n = 15) are (generally) expected to be much deeper and with larger diameter than random.</p>

<pre><code># generate data
set.seed(1234)
x &lt;- runif(250, min=0, max=10)
y &lt;- runif(250, min=0, max=10)
rbPal   &lt;- colorRampPalette(c(""deepskyblue"",""darkblue""))
my.data &lt;- data.frame(x_coor = x, y_coor = y,
                      diameter = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      depth = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      usage = rep(c(1,0), times = c(15, 235)))
my.data$col &lt;- rbPal(10)[as.numeric(cut(my.data$depth,breaks = 10))]

# look at the situation
# occupied holes are marked by red circles
plot(my.data$x, my.data$y, cex = my.data$diameter, col = my.data$col, pch = 20)
grid(5, 5, lwd = 0.75, lty = 2, col = ""grey"") 
points(my.data$x[1:25], my.data$y[1:25], pch = 1, cex = 2,
       col = ""red"", lwd = 1.75)
</code></pre>

<p><img src=""http://i.stack.imgur.com/8O1Xg.png"" alt=""enter image description here""></p>

<p>Although this analysis seem to be rather straightforward, <strong>I have some doubts about comparing unequal samples (15 vs 235)</strong>. Such problem do not occur in other <em>Seafolk burrow selection studies</em> because <strong>mapping of seabed is very expensive and time demanding</strong> and when researchers find 10 used holes, they continue to map the seabed only till they get find additonal 10 nonused holes (which are most probably located in close vicinity of used holes). Due to this
data collection approach the sample sizes of compared groups are usually almost equal.
<strong>However, we were able to find ALL holes in seabed - which can be now considered as a ""handicap"".</strong></p>

<p><strong><em>We do not want to do random sampling of 15 holes from 235 to obtain equal sizes (15 vs 15). Why to throw away such large amount of data!? Is there any solution/approach to this situation which is able to fully embrace such unique dataset?</em></strong></p>

<p><em>I have in mind two possible ways (please comment or add others):</em></p>

<p><strong>1.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, each of these reduced groups will be compared with sample of occupied holes by binomial logistic regression. By this I will obtain 50 logistic curves which will
be ""averaged"" into just one curve - final model.</p>

<p><strong>2.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, from each of these reduced groups (and from sample of used holes also) we gain (mean, sd, sample size). Thus, we will have six numbers for each comparison and we calculate effect size - standardized mean difference (SMD). Effect sizes will be inserted into meta-analytic model and tested if there is a heterogeneity present among effect sizes (<em>to see if the ""randomnes"" of creating unused holes sample has the significant impact on test result</em>).</p>
"
"0.10703564115707","0.107476300250388","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.0700712753800578","0.0703597544730292","115424","<p>I am new to R and I am trying to do some predictive modelling on data set which has 16 feature variables and the target value is numeric in R. I am not sure if the steps I am following will help me to fit the model in the best possible way. </p>

<ol>
<li>Handling the missing values: The data had a lot of missing values, so I replaced it with the mean of the column. Is this a right way to handle missing values?</li>
<li>Used Stepwise Regression to select the right set of most predictive variables in a model. Is there any better way to decide the variables than Stepwise regression.</li>
<li>After deciding the variables, I used glm() function to fit the model. </li>
</ol>

<p>Can someone please help me to understand the process of predictive modeling in R.</p>

<p>I was actually following the below document to get a sense of predictive modelling.
<a href=""http://blog.fractalanalytics.com/wp-content/uploads/2013/04/Predictive_Analytics_Methdology_Using_R_v1.0.pdf"" rel=""nofollow"">http://blog.fractalanalytics.com/wp-content/uploads/2013/04/Predictive_Analytics_Methdology_Using_R_v1.0.pdf</a></p>
"
"0.0286064783845312","0.0287242494810713","115441","<p>I am trying to construct a regression model in R.I am getting an error while predicting the model. I am not sure if the newdata(which is my validation set) should be a data frame?</p>

<pre><code>data&lt;-read.csv(""training.csv"")
data&lt;- na.omit(data)
model&lt;-lm(Y ~ X1+X2+...+X10, data=data)
newdata&lt;-read.csv(""validation.csv"")
predict(model,newdata)
</code></pre>

<p>I am new to statistic and was trying to perform bartlett test and leneve test to test for constant variance. I am not sure if this is the right way to do it.</p>

<pre><code>bartlett.test(list(X1,X2,X3..X10),data= data)
levene.test(X1,X2..., X10, location=""mean"")
</code></pre>

<p>It would be great if anyone could help.</p>
"
"0.06396603026469","0.0513834995538708","116196","<p>My goal is to investigate a dependent variable which is metric (time in hours). The independent variables include 3 metric, 2 binary (factors), and one factor variable, which consists of 11 districts of a city.</p>

<p>I tried to conduct a GLM.</p>

<p>Can I put all this together in one model? It seems to be difficult to interpret the output!
Should I rather use different/various models, with only one factor per regression?
If I use the GLM which kind of family and link function should I use?</p>

<p>The idle time seems to be a right skewed distribution and thefore I chose a gamma family with a inverse link function. </p>

<p>The output of a model wich contains all the independent variables as decribed above: </p>

<p><img src=""http://i.stack.imgur.com/N1hEi.jpg"" alt=""enter image description here""></p>

<p>How can I eleminate the NAs? Or what do they actually mean? </p>

<p>Moreover the ANOVA test was conducted to get a closer look on the district variable called Bezirk, which shows massive differences in the mean value! Is this consistent with small coefficients in the GLM regression? (The means vary between from 3,7 in T.Mitte and 15 in T.Treptow)</p>

<p>Best regards</p>
"
"0.064873395163555","0.0651404749061921","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.0330319159917525","0.0497518595104995","116487","<p>I need to predict payment day of the month (1-31) for each client (I have at most 9 month of payments and on average is 5). I have both categorical variables and numerical. I tried to use rpart to do a regression tree (method='anova') but I'm not sure if it's using the nominal variables. </p>

<p>I also tried a regression (linear to start) and doesn't work good either, but it's better then the regression tree.</p>

<p>If I use a Weibull for this, will it mean that each client is going to have a parameter of shape and scale? what about the other variables? How can I insert them into the distribution?</p>

<p>So, what model would you recommend?</p>

<p>Thanks</p>
"
"0.0809113394062735","0.0812444463702388","116659","<p>I have a collection of continuous data from the literature, including the mean, the standard deviation and the number of observations for both experimental and control groups, as well some environmental variables. A meta-analysis coupled with a meta-regression could be done. However, some studies had several experimental sites, i.e. each line is not a single study, but a single site taken from a study: a random site effect could thus be nested in studies. The following meta mixed-model can be described.</p>

<ol>
<li>effect-size as response</li>
<li>environmental variables as fixed effects,</li>
<li>sites nested in studies as random effects and</li>
<li>each observation is weighted on the sampling variance.</li>
</ol>

<p>I tried two approaches with R, that returned somewhat different results.</p>

<p>As first step, let's create a dummy data frame.</p>

<pre><code>set.seed(40)
Study = factor(c(1,1,1,1,1,2,2,3,3,3,4,5,5,5,5))
Site = factor(c(1,2,3,4,5,1,2,1,2,3,1,1,2,3,4))
n_case = length(Study)
experimental_mean = rnorm(n_case,5,2)
experimental_sd = abs(rnorm(n_case,1,0.1))
experimental_n = round(runif(n_case, 2, 10))
control_mean = experimental_mean+runif(n_case,0,5)
control_sd = abs(rnorm(n_case,1,0.1))
control_n = round(runif(n_case, 2, 10))
A = experimental_mean/control_mean * runif(n_case, 0.5, 0.8)
B = rnorm(n_case,-1,1)
C = factor(letters[round(runif(n_case, 1, 3))])

data_table = data.frame(Study, Site, 
                        experimental_mean, experimental_sd, experimental_n,
                        control_mean, control_sd, control_n,
                        A, B, C)
</code></pre>

<p>Then, compute the effect size and the sampling variance.</p>

<pre><code>library(metafor)
meta_table = escalc(measure='ROM', data=data_table,
                  m1i=experimental_mean, m2i=control_mean,
                  sd1i=experimental_sd, sd2i=control_sd,
                  n1i=experimental_n, n2i=control_n)
</code></pre>

<p>The rma.mv function from the metafor package could be used to run the meta mixed-model.</p>

<pre><code>res = rma.mv(yi, vi, data=meta_table, method=""REML"", level=95,
              mods = ~ A+B+C, random=~1|Study/Site)
summary(res)
</code></pre>

<p>Another option is to run a mixed-model on the effect-size.</p>

<pre><code>library(nlme)
mixed_meta_model = lme(data = meta_table,
                       fixed = yi ~ A+B+C, # yi is the effect size
                       random = ~1|Study/Site,
                       weights = varIdent(~vi)) # vi is the sampling variance
summary(mixed_meta_model)
</code></pre>

<p>Three questions:</p>

<ol>
<li>Is the global approach valid?</li>
<li>Which approach would you suggest?</li>
<li>Is my R code correct? - with special attention to the weights argument <strong>varIdent(~vi)</strong> in the lme function.</li>
</ol>

<p>Many thanks,</p>

<p>S.-Ã‰. Parent
Laval University, Canada</p>
"
"0.0404556697031367","0.0406222231851194","116681","<p>I am working on a large linear regression with a volume metric as my dependent. Right now I am multiplying the model.matrix by the respective coefficients to get to the relative volume contribution by variable.</p>

<pre><code>decomp =  t(apply(model.matrix(fit$terms, data = Data[Data$RetailRead == ""Retail"",]), 1, function(x) {x*fit$coef})) 
</code></pre>

<p>However, this leads to some of the volume being meaningless due to a factor variables within the model. The factor variables need to be added to the base category in order to get to the total overall volume for that variable. </p>

<pre><code>asgn = attr(model.matrix(fit$terms, data = Data), ""assign"") #find indexes which need to be summed

merged =data.frame(t(apply(decomp, 1, function(x) {tapply(x, asgn, sum)})))#sum each appropriate collumn
colnames(merged) = c(""Intercept"", attr(terms(fit), ""term.labels"")) #label collumns
</code></pre>

<p>The code I have written to summarize the volume driven per variable is clunky and inefficient - there are several further steps then above to allow each column of the variable decomposition to be able to stand alone. I am surprised that there is little literature on doing this within R and I wonder if any one know of a package to better perform this task.</p>
"
"0.132802241941751","0.133348980531739","117664","<p>In order to run Lasso and elastic net multiple regressions on my company's SAS server (which doesn't support R), I've been working on a coordinate descent macro for performing least squares regressions (as described in the 2010 paper <a href=""http://www.jstatsoft.org/v33/i01/paper"" rel=""nofollow"">""Regularization Paths for Generalized Linear Models via Coordinate Descent""</a> by Jerome Friedman, Trevor Hastie, and Rob Tibshirani). </p>

<p>Ideally, I would like the coefficient estimates from my SAS algorithm to match the outcomes from the  <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet package</a> in R written by Friedman, et al. which also implements coordinate descent for least squares regression. </p>

<p>I've decided to test the algorithm on the Fitness data from SAS documentation, with Oxygen as the response variable: </p>

<pre><code>data fitness;
  input Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse @@;
  datalines;
   44 89.47 44.609 11.37 62 178 182   40 75.07 45.313 10.07 62 185 185
   44 85.84 54.297  8.65 45 156 168   42 68.15 59.571  8.17 40 166 172
   38 89.02 49.874  9.22 55 178 180   47 77.45 44.811 11.63 58 176 176
   40 75.98 45.681 11.95 70 176 180   43 81.19 49.091 10.85 64 162 170
   44 81.42 39.442 13.08 63 174 176   38 81.87 60.055  8.63 48 170 186
   44 73.03 50.541 10.13 45 168 168   45 87.66 37.388 14.03 56 186 192
   45 66.45 44.754 11.12 51 176 176   47 79.15 47.273 10.60 47 162 164
   54 83.12 51.855 10.33 50 166 170   49 81.42 49.156  8.95 44 180 185
   51 69.63 40.836 10.95 57 168 172   51 77.91 46.672 10.00 48 162 168
   48 91.63 46.774 10.25 48 162 164   49 73.37 50.388 10.08 67 168 168
   57 73.37 39.407 12.63 58 174 176   54 79.38 46.080 11.17 62 156 165
   52 76.32 45.441  9.63 48 164 166   50 70.87 54.625  8.92 48 146 155
   51 67.25 45.118 11.08 48 172 172   54 91.63 39.203 12.88 44 168 172
   51 73.71 45.790 10.47 59 186 188   57 59.08 50.545  9.93 49 148 155
   49 76.32 48.673  9.40 56 186 188   48 61.24 47.920 11.50 52 170 176
   52 82.78 47.467 10.50 53 170 172
   ;
run;
</code></pre>

<p>Here's my first attempt at writing the code for a simple OLS model. (I realize running a data set inside a macro loop is bad form &amp; slows down execution times - this is just a first pass at the problem.)</p>

<p>For the example here I'm fitting a model for a single value of lambda and alpha in an elastic net model. I'm achieving the closest match to glmnet output when I standardize the six predictor variables using proc standard. Initial values for the coefficients are fit via proc reg. Output coefficient values are then converted back to the original unstandardized scale (scroll to bottom of the code below). </p>

<pre><code>            /* Calculate mean and stnd dev values for standardizing fitness variables. */
            proc means data=fitness mean std;
               var Oxygen Age Weight RunTime RestPulse RunPulse MaxPulse;
               output out=fitness_mean_std;
            run;

            data fitness_mean_std (drop=_TYPE_ _FREQ_);
            set fitness_mean_std;
               if _STAT_ in ('MEAN','STD');
            run;

            %let t=7;
            data _null_;
            set fitness_mean_std;
               if _STAT_='MEAN' then do;
                 array mean[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do m = 1 to &amp;t;
                    call symputx(cats('mean',m),mean[m],'g');
                 end;
               end;
               else if _STAT_='STD' then do;
                 array std[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do s = 1 to &amp;t;
                     call symputx(cats('std',s),mean[s],'g');
                 end;
               end;
            run;

            /* Create input dataset for coordinate descent macro. */
            proc standard data=fitness mean=0 std=1 out=fitness_stnd;
               var Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;

            proc reg data=fitness_stnd outest=params_stnd;
               model Oxygen = Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;
            quit;

            %let t=6;
            data _null_;
            set params_stnd;
               array x[0:&amp;t] Intercept Age Weight RunTime RunPulse RestPulse MaxPulse;
               do _n_ = 0 to &amp;t;
                  call symputx(cats('p',_n_),x[_n_],'g');
               end;
            run;
            %put &amp;p0 &amp;p1 &amp;p2 &amp;p3 &amp;p4 &amp;p5 &amp;p6;

            %macro assignvar(k);
            data fitness_array (drop=Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse);
            set fitness_stnd;
               y=Oxygen;
               array a[6] Age Weight RunTime RunPulse RestPulse MaxPulse;
               array x[6];
               %do i=1 %to 6;
                 x[&amp;i]=a[&amp;i];
               %end;
            run;
            %mend;
            %assignvar(6)    

            /* Coordinate descent macro. */
            %macro test(dataset=, numvars=, numiter=, lambda=, alpha=);
               %do i=1 %to &amp;numiter;
                 %do j=1 %to &amp;numvars;
                    data &amp;dataset (keep=y x1-x&amp;numvars);
                    set &amp;dataset end=end_data;
                       array x[&amp;numvars] x1-x&amp;numvars;
                       %let gamma = %sysevalf(&amp;lambda*&amp;alpha);

                       /* Calculate partial residuals for fitting coefficients.*/
                       yhat_&amp;j = &amp;p0 - &amp;&amp;p&amp;j*x[&amp;j];
                       %do k=1 %to &amp;numvars;
                            yhat_&amp;j = yhat_&amp;j + &amp;&amp;p&amp;k*x[&amp;k];
                       %end;
                       if _n_=1 then z_&amp;j = x&amp;j*(y - yhat_&amp;j);                                           else z_&amp;j = x&amp;j*(y - yhat_&amp;j) + z_&amp;j; end;
                       if end_data then do;
                          z_avg_&amp;j = z_&amp;j/_n_;
                          if (z_avg_&amp;j &gt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j - &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;  
                          else if (z_avg_&amp;j &lt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j + &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                          else if &amp;gamma &gt;= abs(z_avg_&amp;j) then do;
                             p&amp;j = 0;
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                       end;
                       retain z_&amp;j;
                    run;
                 %end;
               %end;
               %put _user_;
            %mend;
            %test(dataset=fitness_array, numvars=6, numiter=50, lambda=.1, alpha=.5)            

            /* Return regression coefficients in original scale. */
                %let p0_unstand = %sysevalf(&amp;p0-(&amp;p1*&amp;mean2/&amp;std2)-(&amp;p2*&amp;mean3/&amp;std3)-(&amp;p3*&amp;mean4/&amp;std4)-(&amp;p4*&amp;mean5/&amp;std5)-(&amp;p5*&amp;mean6/&amp;std6)-(&amp;p6*&amp;mean7/&amp;std7));
                %let p1_unstand = %sysevalf(&amp;p1/&amp;std2);
                %let p2_unstand = %sysevalf(&amp;p2/&amp;std3);
                %let p3_unstand = %sysevalf(&amp;p3/&amp;std4);
                %let p4_unstand = %sysevalf(&amp;p4/&amp;std5);
                %let p5_unstand = %sysevalf(&amp;p5/&amp;std6);
                %let p6_unstand = %sysevalf(&amp;p6/&amp;std7);

                %put 
                p0_unstand = &amp;p0_unstand 
                p1_unstand = &amp;p1_unstand 
                p2_unstand = &amp;p2_unstand 
                p3_unstand = &amp;p3_unstand 
                p4_unstand = &amp;p4_unstand 
                p5_unstand = &amp;p5_unstand 
                p6_unstand = &amp;p6_unstand;
                /*
                p0_unstand = 107.068671308076 
                p1_unstand = -0.24178321947146 
                p2_unstand = -0.05008520720235
                p3_unstand = -2.47736090772018 
                p4_unstand = -0.16124847253703 
                p5_unstand = -0.03822018686055
                p6_unstand = 0.06524084784678
                */
</code></pre>

<p>The corresponding commands in R:</p>

<pre><code>&gt; elasticnet_fit = glmnet(x, y, family=""gaussian"", lambda=.1, alpha=.5); 
&gt; coef(elasticnet_fit);   
7 x 1 sparse Matrix of class ""dgCMatrix""
                      s0
(Intercept) 105.10824556
x1           -0.22996264
x2           -0.05775625
x3           -2.64766834
x4           -0.01998125
x5           -0.26222028
x6            0.18003526
</code></pre>

<p>My question: The coefficients output from the SAS macro doesn't match the output from glmnet, although the values are close. Is there a flaw in my code or should I not be too concerned? Thanks!</p>
"
"0.0286064783845312","0.0287242494810713","117980","<p>I have some data that I'm fitting a multiple regression to, with the twist that the error distribution is t (with user-defined degrees of freedom) instead of Gaussian. I've been coding up my own function to do this, but recently I saw some posts that the <code>rlm</code> function in the MASS package has this ability already. Is this true? I have the accompanying book* and it mentions a number of M-estimators with Huber's as the default. I don't see how any of them correspond to the t distribution.</p>

<p>(Background: the t is because I'm interested in simulating the distribution of the response, not just estimating its conditional mean. From looking at the residuals, it appears that a suitably scaled t distribution with 4-5 df is a reasonable fit, and certainly much better than the Gaussian. It's only tangentially related to robustness considerations.)</p>

<p><br></p>

<p>* <em>Modern Applied Statistics with S</em> 4th Ed, Venables &amp; Ripley (2002)</p>
"
"0.0495478739876288","0.0497518595104995","118396","<p>I'm trying to fit a quantile regression model for rigth censoring data and I'm using R with the package quantreg and its function  crq. I'm trying the Portnoy method that it's suposed to estimate the full range of tau (quantiles) , but the results only contains 85 taus, ending in tau=0.4 and giving me estimated values of that lasts taus that are very far away from the real 0.4 quantile and more around 0.95 quantile.</p>

<p>I read in the quantreg doc that Portnoy and Peng-Huang may be unable to estimate upper conditional quantiles if censoring is heavy in the upper in the upper tail, but this doesn't seem to be my case. In fact the general distribution summary of time is:</p>

<pre><code>Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.0   223.0   447.0   482.5   714.0  1251.0 
</code></pre>

<p>the quantiles vary obviously with levels of VAR1 and VAR2 but the shape of the distribution is almost the same.I can produce Kaplan-Mier estimation without problem. </p>

<p>Here is my syntax</p>

<pre><code>qreg1&lt;-crq(Surv(TIME,EVENT,type=""right"")~VAR1+VAR2,
       data=DATA_TRAIN,method = ""Portnoy"")
</code></pre>

<p>What am I doing wrong?</p>

<p>Thank you in advance,</p>
"
"0.0583927294833815","0.0703597544730292","120201","<p>Being aware of <a href=""http://www.ssicentral.com/lisrel/techdocs/HowLargeCanaStandardizedCoefficientbe.pdf"" rel=""nofollow"">that article</a>, I am curious about the question how big standardized coefficients can get. I had a discussion with my professor about that issue and she was arguing standardized coefficients (beta) in multiple linear regressions can not become greater than |1|. I have also heard that predictors with standardized coefficients greater than 1 should not be be included/appear in multiple linear regression. When I recently estimated a multiple linear regression in R using lm(), I estimated the standardized coefficients with lm.beta() function from the package 'lm.beta'. In the results I could observe a standardized coefficient greater than one. Right now I am just not sure about what is the truth.</p>

<p>Can standardized coefficients become greater than |1|?
If yes, what does that mean and should they be excluded from the model?
If yes, why?</p>

<p>I would be very thankful, if somebody could make this issue clear for me. </p>

<p>Thanks in advance!!</p>
"
"NaN","NaN","120472","<p>I'm running a beta regression in <code>R</code> where the outcome variable is continuous but bounded between 0-1. I need the results to be understandable to an audience with a very basic stats background. I need some help interpreting the coefficients. My model is very basic and look like:</p>

<p><code>betareg(Y ~ ANE + factor(Year) +factor(Month) +factor(Sector))</code></p>

<p>I'm not sure how to interpret the coefficient on <code>ANE</code>. Below are the results I got for the <code>ANE</code> variable:</p>

<blockquote>
<pre><code>Coefficients (mean model with logit link):
              Estimate  Std. Error  z value  Pr(&gt;|z|) 
(Intercept) 1.036e+00 : 7.735e-02 : 13.395 : &lt; 2e-16 ***
ANE        -1.693e-05 : 6.948e-06 : -2.437 : 0.014803 *
</code></pre>
</blockquote>
"
"0.124692748408752","0.118616305942459","120892","<p>I have on question regarding standardized coefficients (beta) in linear models. I have already asked one question <a href=""http://stats.stackexchange.com/questions/120201/magnitude-of-standardized-coefficients-beta-in-multiple-linear-regression"">here</a>. From the answers I assume that I should use R's <code>scale()</code> function on the dependent variable as well as on all independent variables (IV), to estimate the standardized coefficients for the model. But when I used the <code>scale()</code> function on an IV, which belongs to the factor class I get following error message:</p>

<p><code>Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric</code></p>

<p>To illustrate my problem here is a MWE:</p>

<p>First the linear model with unstandardized coefficients:</p>

<pre><code>&gt; data(ChickWeight)
&gt; aa &lt;- lm(weight ~ Time + Diet, data=ChickWeight)
&gt; summary(aa)

Call: 
lm(formula = weight ~ Time + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-136.851  -17.151   -2.595   15.033  141.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  10.9244     3.3607   3.251  0.00122 ** 
Time          8.7505     0.2218  39.451  &lt; 2e-16 ***
Diet2        16.1661     4.0858   3.957 8.56e-05 ***
Diet3        36.4994     4.0858   8.933  &lt; 2e-16 ***
Diet4        30.2335     4.1075   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 35.99 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Now I want to estimate the standardized coefficients using the <code>scale</code> function, which results in following error message:</p>

<pre><code>&gt; bb &lt;- lm(scale(weight) ~ scale(Time) + scale(Diet), data=ChickWeight)
Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
</code></pre>

<p>As I figured out by myself the error message appears, because <code>Diet</code> belongs to the factor class and is not a numeric variable as required from the <code>scale()</code> function. I tried the following alternatively by including the <code>Diet</code> variable without <code>scale()</code>:</p>

<pre><code>&gt; cc &lt;- lm(scale(weight) ~ scale(Time) + Diet, data=ChickWeight)
&gt; summary(cc)

Call:
lm(formula = scale(weight) ~ scale(Time) + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.92552 -0.24132 -0.03652  0.21151  1.99538 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.24069    0.03415  -7.048 5.25e-12 ***
scale(Time)  0.83210    0.02109  39.451  &lt; 2e-16 ***
Diet2        0.22746    0.05749   3.957 8.56e-05 ***
Diet3        0.51356    0.05749   8.933  &lt; 2e-16 ***
Diet4        0.42539    0.05779   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5064 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>My question now is, if this is the right way to estimate the standardized coefficients for a model with both numeric and factor variables?</p>

<p>Thank you very much in advance for an answer.</p>

<p>Regards,</p>

<p>Magnus</p>
"
"0.0700712753800578","0.0703597544730292","121037","<p>I have a dataset with approximately <strong>4000 rows and 150 columns</strong>. I want to predict the values of a single column (= target).</p>

<p>The data is on cities (demography, social, economic, ... indicators). A lot of these are highly correlated, so I want to do a PCA - Principal Component Analysis. </p>

<p>The problem is, that <strong>~40% of the values are missing</strong>.</p>

<p>My current approach is:
Remove target indicator and do <strong>PCA with mean/median imputation of missing values</strong>.
Select x principal components (PC).
Append target indicator to these PC.
Use PC as predictors for the target variable and try common regression techniques, e.g. knn, linear regression, random forest etc.</p>

<p>With this approach, I'm getting quite good results. My metric is RMSE% - root mean squared relative prediction error. I tried this for all columns in the dataset, the RMSE% is between 0.5% and 8% (depending on the column). These errors are for values I actually know, NOT imputed values.</p>

<p>So, here's my problem: <strong>I'm not sure how much my data is distorted by replacing the missing values with the column mean/median</strong>. Is there any other way of imputing the missing values with minimal effect on the PCA results?</p>
"
"0.103142124625879","0.103566754353222","121192","<p>I have some data I need to fit a model to that can be used for prediction (interpolation). The data is summarized by the plot below. The black line is x=y.</p>

<p><img src=""http://i.stack.imgur.com/x0FqM.png"" alt=""enter image description here""></p>

<p>I want to be able to fit a model so as I can use it to predict any value of the y axis as a function of x axis, as well as get the uncertainty in that estimate.</p>

<p>However in my data, the variance of the y axis variable increases as the x axis variable increases.
In addition, there is another continuous explanatory variable called SequenceSize (plotted as factor to clearly see the colours) which I think I have to take into account, as it is also correlated (negatively) with the variance of the y axis variable, whilst not really affecting the mean so much. As can be seen in the two plots below.</p>

<p><img src=""http://i.stack.imgur.com/7KdHZ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/d43W1.png"" alt=""enter image description here""></p>

<p>So from a model fit to the data I would like to be able to use it to do.</p>

<ol>
<li>Plugin the value of SequenceSize and the x axis variable.</li>
<li>Get out an estimate of the y axis variable along with some measure of uncertainty in the y estimate, given how the variance and uncertainty is affected by the x axis variable and by SequenceSize.</li>
</ol>

<p>However I'm reading the massive R book by Crawley, and I'm having trouble deciding which model would be best to do this. I'm thinking maybe a multiple regression if I linearized the data by taking log of x and y, but I'm unsure if that's right because of how the variance of the data acts.  </p>

<p>Thanks,
Ben W.</p>
"
"0.140142550760116","0.140719508946058","121255","<p>I am working on a dataset with a continuous response (which could be dichotomized), one continuous covariate, and multiple categorical variables. The continuous covariate (weight) is directly correlated to the response, and must be accounted for so that we can determine which of the categorical variables are most influential to the response. Here is <a href=""http://pastebin.com/891mheRf"" rel=""nofollow"">example data</a>.</p>

<p>Each row is an individual subject, with the continuous response, the covariate of underlying primary importance (weight), then 10 categorical variables that are to be tested (individuals can score yes = 1 to multiple categories). </p>

<p>My first thought in working with this data was a linear model, with stepwise elimination of categorical variables.</p>

<pre><code> lm(Response~Weight+var1+var2...+var11)
</code></pre>

<p>However, I believe there is extensive collinearity, since some variables may be eliminated early, but then are significant if you add them back into the model at the end. I'm curious if there is a better way to approach this data in R, that may help sort through which of the variables are of most importance to influencing the response. My two thoughts are</p>

<p>1) Building a single model with the continuous covariate and 5 categorical variables that were selected to be of most interest before the study, and refrain from any stepwise reduction of this model</p>

<p>2) Some sort of princicpal component regression, which I know little about at this point and thus wanted to ask advice before proceeding down that path</p>

<p>To help visualize the data, and the effect of Weight on the Response, I've constructed the follow plots. In the second plot, I attempt to control for the natural Response~Weight relationship.</p>

<pre><code> #GRAPH
 library(ggplot2)
 library(reshape2)

 Data &lt;- read.table(""Fake Data.txt"",header=TRUE)
 #Creating long format for ggplot2
 Data2&lt;-melt(Data, id.vars = c(""Subject"",""Response"",""Weight""), measure.vars = c(""var1"",""var2"",""var3"",""var4"",""var5"",""var6"",""var7"",""var8"",""var10"",""var11""))

 #Adding in weight to the varibles to be plotted
 Data2&lt;-rbind(Data2,Data2[1:31,])
 levels(Data2$variable)&lt;-c(levels(Data2$variable),""Weight"")
 Data2[311:341,4]&lt;-""Weight""
 Data2[311:341,5]&lt;-1

 #Removing rows where the categorical variable is 0=No
 for(i in 1:length(Data2[,1])){
 if(Data2[i,5]==0)Data2[i,]&lt;-NA
 }
 Data3&lt;-na.omit(Data2)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables
  scatter &lt;- ggplot(Data3, aes(Weight, Response, colour = variable))
 scatter + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/AgQog.jpg"" alt=""enter image description here""></p>

<pre><code> #Zeroing the Response~Weight relationship to remove its influence. Correction coefficients from linear model fit to Response~Weight
 Data4&lt;-Data3
 Data4$Response&lt;-Data4$Response-(0.01494*(Data4$Weight)+ 84.67715)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables for zeroed Response~Weight relationship (as seen in bottom right facet)
 scatter2 &lt;- ggplot(Data4, aes(Weight, Response, colour = variable))
 scatter2 + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/JfCrn.jpg"" alt=""enter image description here""></p>

<p>This second plot helps to show how, when the Response~Weight relationship is controlled for, variables like 'var10' have no influence on the response, while variables like 'var11' have all individuals below that zero-centered mean. Thus, from a visual test, I could identify var11 as a categorical variable of interest that negatively influences our response.</p>

<p>Additionally, this plot shows some of the confounding in this dataset, as you can see certain categorical variables 'clump'/are only documented in certain weight ranges. This is due to the underlying biology.</p>

<p>As a final note, I wonder if it is appropriate to use the corrected response in the second plot as the 'Response' for a linear model, thus eliminating the need for a 'Weight' covariate, or if it is incorrect to use such a transformation</p>

<p>Any thoughts are much appreciated</p>
"
"0.0707974219804893","0.0609333347776791","121566","<p>Here is a problem that was puzzling me. Suppose I simulate the AR(2) process with constant and trend using the code below (I apologize for inefficiency and inelegance - the aim was to get job done at this point; also - it may seem strangely constructed, but it has some other purpose too for which is irrelevant here).</p>

<p>My question is - why the constant estimates are so poor? The true value is <code>70</code> but if we average 1000 regressions each over 1000 observations I get an average of <code>381.9234</code>. </p>

<p>Is it because I interpret something wrong or the did I make a mistake somwhere?</p>

<pre><code>set.key(123)

#parameter values
V=7
P=10
S=4
r1 = 50/(50+P)
r2 = V/(30+V)
mu = 10*P
l2 = 10*(S+V)
a0 = 10*V
d0 = 10*P
a1 = 0
d1 = P+V
s2 = 2*(P+V+S)

#simulate and estimate the parameters
data&lt;-NULL
data50 &lt;- NULL

for (firm in 1:1000){

  y_zero &lt;- rnorm(1, mean = mu, sd = l2)
  gamma_0 &lt;- rnorm(1, mean = a0, sd = d0)
  gamma_1 &lt;- rnorm(1, mean = a1, sd = d1)

  y_first &lt;- r1*y_zero + gamma_0 + gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_second &lt;- r1*y_first - r2*(y_first - y_zero) + gamma_0 + 2*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_third &lt;- r1*y_second - r2*(y_second - y_first) + gamma_0 + 3*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_fourth &lt;- r1*y_third - r2*(y_third - y_second) + gamma_0 + 4*gamma_1 + rnorm(1, mean = 0, sd = s2)

  column &lt;- cbind(""firm"" = firm, ""t"" = 1:4, ""y"" = c(y_first, y_second, y_third, y_fourth))

  data &lt;- rbind(data, column)
  ###################################################################################
  firm50 &lt;- NULL

  y_fifth &lt;- r1*y_fourth - r2*(y_fourth - y_third) + gamma_0 + 5*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_sixth &lt;- r1*y_fifth - r2*(y_fifth - y_fourth) + gamma_0 + 6*gamma_1 + rnorm(1, mean = 0, sd = s2)

  y_previous1 &lt;- y_sixth
  y_previous2 &lt;- y_fifth

  firm50 &lt;- cbind(""firm"" = firm, ""t"" = c(5,6), ""y"" = c(y_fifth, y_sixth), ""ro1-ro2"" = c(y_fourth, y_fifth), ""ro2"" = c(y_third, y_fourth))

  for (run in 1:5000){
    time &lt;- run + 6

    the_y &lt;- r1 * y_previous1 - r2 * (y_previous1 - y_previous2) + gamma_0 + time*gamma_1 + rnorm(1, mean = 0, sd = s2)

    firm50 &lt;- rbind(firm50, cbind(""firm"" = firm, ""t"" = time, ""y"" = the_y, ""ro1-ro2"" = y_previous1, ""ro2"" = y_previous2))

    y_previous2 &lt;- y_previous1
    y_previous1 &lt;- the_y

  }
  firm50 &lt;- cbind(firm50, ""gamma0"" = gamma_0, ""gamma1"" = gamma_1)
  data50 &lt;- rbind(data50, firm50)
}

#estimate the coefficients
data &lt;- data.table(as.data.frame(data50))[t %in% c(4000:5000)]
coefs &lt;- NULL
for(i in 1:1000){
  coefs &lt;- rbind(coefs, t(coef(arima(data[firm==i, y], c(2,0,0), xreg = data[firm==i, t])))
}
</code></pre>
"
"NaN","NaN","121749","<p>I am wondering about the exact definition of ARIMA model in function <code>arima</code> in <code>R</code> when exogenous regressors are included.</p>

<p>I understand that <code>arima(y, order=c(p,0,q), xreg=x)</code> is equivalent to estimating the following equation (where $\mu_y$ and $\mu_x$ stand for the means of $y$ and $x$, respectively):</p>

<p><strong>(1)</strong> $(y_t-\mu_y)=\varphi_0+\phi_1(y_{t-1}-\mu_y)+...+\varphi_p(y_{t-p}-\mu_y)+\varepsilon_t+\theta_1\varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}+\beta_1x_t$</p>

<p>Or is it</p>

<p><strong>(2)</strong> $(y_t-\mu_y)=\varphi_0+\phi_1(y_{t-1}-\mu_y)+...+\varphi_p(y_{t-p}-\mu_y)+\varepsilon_t+\theta_1\varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}+\beta_1(x_t-\mu_x)$</p>

<p>(only the last term differs between <strong>(1)</strong> and <strong>(2)</strong>)?</p>

<p>Or perhaps I got both of them wrong?</p>

<p><strong>Edit:</strong> I now realize that including both {$\mu_x$ and $\mu_y$} and $\varphi_0$ in <strong>(2)</strong> was superfluous.</p>
"
"0.0990957479752576","0.0995037190209989","122066","<p>I am interested in better understanding the delta method for approximating the standard errors of the average marginal effects of a regression model that includes an interaction term. I've looked at related questions under <a href=""/questions/tagged/delta-method"" class=""post-tag"" title=""show questions tagged &#39;delta-method&#39;"" rel=""tag"">delta-method</a> but none have provided quite what I'm looking for.</p>

<p>Consider the following example data as a motivating example:</p>

<pre><code>set.seed(1)
x1 &lt;- rnorm(100)
x2 &lt;- rbinom(100,1,.5)
y &lt;- x1 + x2 + x1*x2 + rnorm(100)
m &lt;- lm(y ~ x1*x2)
</code></pre>

<p>I am interested in the average marginal effects (AMEs) of <code>x1</code> and <code>x2</code>. To calculate these, I simply do the following:</p>

<pre><code>cf &lt;- summary(m)$coef
me_x1 &lt;- cf['x1',1] + cf['x1:x2',1]*x2 # MEs of x1 given x2
me_x2 &lt;- cf['x2',1] + cf['x1:x2',1]*x1 # MEs of x2 given x1
mean(me_x1) # AME of x1
mean(me_x2) # AME of x2
</code></pre>

<p>But how do I use the delta method to calculate the standard errors of these AMEs?</p>

<p>I can calculate the SE for this particular interaction by hand:</p>

<pre><code>v &lt;- vcov(m)
sqrt(v['x1','x1'] + (mean(x2)^2)*v['x1:x2','x1:x2'] + 2*mean(x2)*v['x1','x1:x2'])
</code></pre>

<p>But I don't understand how to use the delta method.</p>

<p>Ideally, I'm looking for some guidance on how to think about (and code) the delta method for AMEs of any arbitrary regression model. For example, <a href=""http://stats.stackexchange.com/questions/55795/how-to-calculate-the-standard-error-of-the-marginal-effects-in-interactions-rob"">this question</a> provides a formula for the SE for a particular interaction effect and <a href=""https://files.nyu.edu/mrg217/public/interaction.html#errors"" rel=""nofollow"">this document from Matt Golder</a> provide formulae for a variety of interactive models, but I want to better understand the general procedure for calculating SEs of AMEs rather than the formula for the SE of any particular AME.</p>
"
"0.06396603026469","0.0513834995538708","122593","<p>I wanted to check whether the level of satisfaction relates to the level of support to the value of democracy.
Dependent variable (support) is binary variable (Good/Bad) and independent variable is ordinary variable (level of satisfaction).
After doing binary logistic regression, I got this â€œstrangeâ€ figure. Is this result correct? Or how can I fix it?</p>

<pre><code>satisfaction &lt;- data_american$V23c
    support &lt;- data_american$V130b
dat=as.data.frame(cbind(satisfaction,suport)) 
library(ggplot2)
ggplot(dat, aes(x=satisfaction, y=support)) + geom_point() + 
  stat_smooth(method=""glm"", family=""binomial"", se=FALSE)
</code></pre>

<p>Here is a part of data:</p>

<pre><code>   satisfaction support
1             7       1
2             8       1
3             8       1
4             8       1
5            10       1
6             6       1
7             7       1
8             7       1
9             7       1
10            8       0
11            8       1
12            7       1
13            7       0
14            1       1
15            7       1
16            8       1
17            6       1
18            7       1
19            8       1
20            8       1
</code></pre>

<p><img src=""http://i.stack.imgur.com/CgjzY.png"" alt=""results I got""></p>

<p>[Added 1]
Based on the first answer, I got this results:</p>

<pre><code>&gt; satisfaction_j &lt;- jitter(satisfaction)
&gt; chisq.test(table(satisfaction_j,support))

    Pearson's Chi-squared test

data:  table(satisfaction_j, support)
X-squared = 2158, df = 2157, p-value = 0.4899

&gt; t.test(satisfaction_j~support)

    Welch Two Sample t-test

data:  satisfaction_j by support
t = -2.7775, df = 459.931, p-value = 0.005703
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.57390716 -0.09829989
sample estimates:
mean in group 0 mean in group 1 
       7.164214        7.500317 
</code></pre>
"
"NaN","NaN","122825","<p><img src=""http://i.stack.imgur.com/moYDi.jpg"" alt=""enter image description here"">I am trying to experiment with glmnet for building the regression model. The cross validation result is shown in the following figure. Looks like to me that mean-square error is totally out of control. Is there any possible reason for this kind of scenario. </p>

<pre><code>cv.out&lt;-cv.glmnet(x,y,alpha=0)
plot(cv.out)
</code></pre>
"
"0.0495478739876288","0.0497518595104995","122875","<p>I am doing multiple linear regression analysis in R and I got the following summary:</p>

<pre><code>Call:
lm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
    X10 + X11 + X12 + X13)

Residuals:
ALL 20 residuals are 0: no residual degrees of freedom!

Coefficients: (151 not defined because of singularities)
                   Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)       -15462.94         NA      NA       NA
X1                    63.31         NA      NA       NA
X2                  1363.12         NA      NA       NA
X31,266,019,376     5518.54         NA      NA       NA
X31,483,786,035    29894.78         NA      NA       NA
X31,619,000,000    39338.01         NA      NA       NA
X31,687,000,000    65308.07         NA      NA       NA
X31,720,264,324    35548.79         NA      NA       NA
X31,749,000,000    31693.75         NA      NA       NA

.......................................................

X13692,062,808           NA         NA      NA       NA
X13693,179,733           NA         NA      NA       NA
X13724,817,439           NA         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:    NaN 
F-statistic:   NaN on 19 and 0 DF,  p-value: NA
</code></pre>

<p>Could anybody explain what does that result mean? And what should I do?</p>

<p>Thank you!</p>

<p>Another question.  What should I do if the following error appears: </p>

<pre><code>Error in step(model) : 
number of rows in use has changed: remove missing values?
</code></pre>
"
"0.0202278348515684","0.0406222231851194","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.0762839423587497","0.0765979986161901","123210","<p>I am doing a logistic regression in R, where I am modeling how potholes and weather correlate to accidents. When I run a logistic regression, I get the message ""Algorithm does not converge""</p>

<p>The problem I think I have is that I have 24,000 accidents with only 350 potholes related to these accidents. Is this to small of a sample size?</p>

<p>The other possible issue I thought of, is that when I look at sample logistic regressions, the outcome is either zero or one, but the only outcome I have is the outcome of one, or accident in my case. I do not have any non accident data in my set, could this be what is causing the problem? </p>

<p>I will attach my current code and its output.</p>

<pre><code>require(ggplot2)
require(sandwich)
require(msm)
mydata &lt;- read.csv(""C:\\Users\\myname\\downloads\\logreg1.csv"")
## view the first few rows of the data
head(mydata)
summary(mydata)
mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")

&gt; head(mydata)
      Date Pothole Rain Snow Accident
1 1/1/2012       0    0    0        1
2 1/1/2012       0    0    0        1
3 1/1/2012       0    0    0        1
4 1/1/2012       0    0    0        1
5 1/1/2012       0    0    0        1
6 1/1/2012       0    0    0        1
&gt; summary(mydata)
         Date          Pothole            Rain              Snow            Accident
 1/8/2014  :   87   Min.   :0.0000   Min.   :0.00000   Min.   : 0.0000   Min.   :1  
 1/30/2013 :   82   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:1  
 3/21/2013 :   77   Median :0.0000   Median :0.00000   Median : 0.0000   Median :1  
 12/21/2012:   76   Mean   :0.0173   Mean   :0.08077   Mean   : 0.1129   Mean   :1  
 3/10/2013 :   66   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:1  
 12/13/2013:   59   Max.   :8.0000   Max.   :3.32000   Max.   :11.1000   Max.   :1  
 (Other)   :23606                                                                   
&gt; mylogit &lt;- glm(Accident ~  Rain + Snow, data = mydata, family = ""binomial"")
Warning message:
glm.fit: algorithm did not converge
</code></pre>
"
"0.103142124625879","0.103566754353222","123498","<p>I would like to generate a confidence interval for predicted vs actual rates.</p>

<p>I am auditing my group of anaesthetists (aka anesthesiologists) to see how we compare on a number of potentially preventable complications (eg post-operative nausea, severe pain, hypothermia).</p>

<p>I have 20000 surgical operation records and I can make a GLM to make a ""case-mix adjusted risk"" (using age, gender, type of surgery, duration of surgery as risk factors) and thus <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">generate a risk</a> for each patient.</p>

<p>I can then aggregate the risk and actual per clinician I can generate an actual and predicted rate for each complication. I can make a confidence interval for my actual - but it seems a bit simplistic to just test to see if the confidence interval on the actual rate includes the rate generated from summing the glm-predicted risks.</p>

<p><a href=""http://stats.stackexchange.com/questions/7344/how-to-graphically-compare-predicted-and-actual-values-from-multivariate-regress"">This question</a> has some pointers to a package but I am hoping for some more specific suggestions.</p>

<p>To clarify what I have already (using ""requirement for pain protocol"" as an example):</p>

<pre><code># make model (dependent variable has values 1/0)
model.pp = glm(
pain_protocol1 ~
age + log_age + age2 + inv_age
+ op_time + log_op_time + op_time2
+ gender
+ category
+ thimble,
family = ""binomial"",
data=d4)
# calculate predicted PACU time and then difference between predicted and actual:
d4$pred_pp = predict(model.pp, newdata=d4, type=""response"", na.action=""na.pass"")

d4$extra_pp = d4$pain_protocol1 - d4$pred_pp
# aggregate deviation from predicted rate
ppr_pa &lt;- aggregate(extra_pp ~ adult_anaesthetist, data=d4, FUN=mean)
barplot(ppr_pa$extra_pp, name=ppr_pa$adult_anaesthetist,las=2) 
</code></pre>

<p>So I can make this plot for my colleagues, showing the variation we have in how much pain our patients experience in the ""post anaesthesia care unit"". These variations are great enough so that they definitely represent a material difference in patient experience, and most of the difference will also be unlikely to be variation due to chance (ie ""statistically significant""). However, as I examine smaller subgroups and other complications that are less frequent it would be good to be able to calculate confidence intervals.</p>

<p><img src=""http://i.stack.imgur.com/FFe8W.png"" alt=""bar plot illustrating difference between predicted and actual rates of &quot;needing pain protocol&quot;""></p>

<p>Note that each clinician has a different number of cases, and each clinician is given a bird code-name for anonymity.</p>
"
"0.140559022854369","0.141137695630083","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.0495478739876288","0.0497518595104995","124129","<p>I would like to fit the following model <code>Y (t) = m (t) + b * t + g * C (t) + N (t)</code> with m (t) to be the long term mean monthly values (remove seasonal component), b the trend coefficient, C to be the matrix of explanatory variables, and N (t) the error term being AR (1).</p>

<p>I would like to ask you if this model is the same as the following:</p>

<p><code>Y (t) - m (t) = b * t + g * C (t) + N (t)</code>. Forced to have 0 intercept term, or I should also substract the mean from my regressors also.</p>

<p>Moreover, I would like to know if you can propose how this could be implemented, preferably in Matlab, or secondly in R.</p>

<p>I am not familiar with this kind of models yet, so thanks in advance, all help is very much appreciated.</p>
"
"0.0908377689773195","0.0912117424359157","124233","<p>I am running following data and code for analyzing non-linear regression and to get simplest equation of curve that fits the data:</p>

<pre><code>&gt; dput(ddf)
structure(list(xx = 1:23, yy = c(10L, 9L, 11L, 9L, 7L, 6L, 9L, 
8L, 5L, 4L, 6L, 6L, 5L, 4L, 6L, 8L, 4L, 6L, 8L, 11L, 8L, 10L, 
9L)), .Names = c(""xx"", ""yy""), row.names = c(NA, -23L), class = ""data.frame"")
&gt; 
&gt; head(ddf)
  xx yy
1  1 10
2  2  9
3  3 11
4  4  9
5  5  7
6  6  6
</code></pre>

<p><img src=""http://i.stack.imgur.com/vkx9G.png"" alt=""enter image description here""></p>

<pre><code>&gt; fit = lm(yy ~ poly(xx, 9), data=ddf)
&gt; summary(fit)

Call:
lm(formula = yy ~ poly(xx, 9), data = ddf)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9890 -1.2031  0.1086  0.7493  2.4248 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   7.347826   0.356758  20.596 2.62e-11
poly(xx, 9)1 -0.880172   1.710953  -0.514 0.615582
poly(xx, 9)2  7.821383   1.710953   4.571 0.000524  # NOTE THIS
poly(xx, 9)3  0.424579   1.710953   0.248 0.807892
poly(xx, 9)4 -2.151779   1.710953  -1.258 0.230641
poly(xx, 9)5 -0.876964   1.710953  -0.513 0.616857
poly(xx, 9)6 -0.961726   1.710953  -0.562 0.583610
poly(xx, 9)7 -0.002171   1.710953  -0.001 0.999007
poly(xx, 9)8 -0.051884   1.710953  -0.030 0.976269
poly(xx, 9)9  0.840177   1.710953   0.491 0.631571

Residual standard error: 1.711 on 13 degrees of freedom
Multiple R-squared:  0.6451,    Adjusted R-squared:  0.3993 
F-statistic: 2.625 on 9 and 13 DF,  p-value: 0.05575
</code></pre>

<p>If I use 'raw=TRUE' : </p>

<pre><code>&gt; fit = lm(yy ~ poly(xx, 9, raw=TRUE), data=ddf)
&gt; summary(fit)

Call:
lm(formula = yy ~ poly(xx, 9, raw = TRUE), data = ddf)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9890 -1.2031  0.1086  0.7493  2.4248 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)               2.844e+00  1.529e+01   0.186    0.855
poly(xx, 9, raw = TRUE)1  1.310e+01  2.711e+01   0.483    0.637
poly(xx, 9, raw = TRUE)2 -8.439e+00  1.723e+01  -0.490    0.632  # NOTE THIS
poly(xx, 9, raw = TRUE)3  2.637e+00  5.432e+00   0.485    0.635
poly(xx, 9, raw = TRUE)4 -4.719e-01  9.715e-01  -0.486    0.635
poly(xx, 9, raw = TRUE)5  5.112e-02  1.048e-01   0.488    0.634
poly(xx, 9, raw = TRUE)6 -3.400e-03  6.937e-03  -0.490    0.632
poly(xx, 9, raw = TRUE)7  1.355e-04  2.757e-04   0.492    0.631
poly(xx, 9, raw = TRUE)8 -2.967e-06  6.032e-06  -0.492    0.631
poly(xx, 9, raw = TRUE)9  2.739e-08  5.578e-08   0.491    0.632

Residual standard error: 1.711 on 13 degrees of freedom
Multiple R-squared:  0.6451,    Adjusted R-squared:  0.3993 
F-statistic: 2.625 on 9 and 13 DF,  p-value: 0.05575
</code></pre>

<p>I find that if I do not use 'raw=TRUE', one P value (2nd) is significant, but it is not significant if I use 'raw=TRUE'. Why does this occur and what does it mean?</p>

<p>I asked above question at stackoverflow but was advised to post here. Thanks for your help.</p>
"
"0.0872741054526671","0.0876334075296498","124532","<p>I am estimating a (semi)parametric and a parametric model for a panel data set, and I want to test the functional form by applying the method proposed by <a href=""http://www.sciencedirect.com/science/article/pii/S030440760800016X"" rel=""nofollow"">Henderson et al. (2008, p.267)</a>. In particular, given the two models</p>

<pre><code>y = beta1 X + Z'gamma + u  (parametric)
y = beta2 X + g(z) + e    (semiparametric)
</code></pre>

<p>they use <code>H0</code> to denote the null hypothesis of the linear regression model, against <code>H1</code>: the corresponding alternative is the semiparametric model. The test statistic for testing <code>H0</code> is</p>

<pre><code>I= [beta1* X + Z'gamma* - beta2* X + g(z)]^2
</code></pre>

<p>Under <code>H0</code>. <code>I</code> converges to 0 in probability, whil it converges to a positive constant under <code>H1</code> . Therefore, the statistics <code>I</code> can be used to detect whether <code>H0</code> is true or not. However, given some problems with the asymptotical distribution of <code>I</code>, the authors propose to compute its empirical distribution by resampling <code>n</code> times the residuals <code>u</code>, using them to generate <code>n</code> new <code>y</code> and then re-estimating <code>n</code> times both models. By this way it is possible to obtain <code>n</code> times <code>I*</code> and its empiric distribution which should be approximating the null distribution of <code>I</code>. Therefore it can be used to detect whether <code>H0</code> holds.</p>

<p>My problem is in the interpretation of the results of the test. I reasoned as follows: I plotted the empirical distribution of <code>I*</code> which, in my case, has <code>mean &gt; 0</code>. Therefore what I did was to verify $where$  the <code>I</code> statistics lies. This is what I obtain:</p>

<p><img src=""http://i.stack.imgur.com/TihCM.png"" alt=""Empirical distribution and test""></p>

<p>where I.BB.IVO is my <code>I</code>. Can I say that <code>I</code> belongs to the empirical distribution (since it is not in the rejection zone) with mean <code>&gt; 0</code>, therefore the null hypothesis is rejected?</p>

<p>Is there any other alternative of doing the job?</p>
"
"0.0756856276908142","0.0759972207238908","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.0404556697031367","0.0203111115925597","124690","<p>Right now I am working with vector autoregressive models in order to make 3 months forecasts for a commodity good (sawlogs) y. I have several time-series of ""follow-up-products"" of sawlogs that should work as ""predictors"" for saw-log prices from a logical point of view. 
I encountered within the VAR-function from package ""vars"" (<a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a>), that one attribute called ""type"" has the following expressions: ""const"", ""both"", ""trend"", ""none"". I really don't know what this means from a statistical point of view.</p>

<p>Since neither the package-description nor other literature I've screened so far can give me an answer I actually understand I'd like to ask you guys the following:</p>

<p>How should I interpret/understand and use the argument ""type"" in R's VAR() Function?</p>

<p>What do those 4 different arguments really mean? ""both"", ""none"", ""trend"", ""constant""?
Could anyone explain this in a simple way and probably provide an example as well?</p>

<p>Does this mean that I can directly use non-stationary time series for my VAR-model since I can consider trend/season afterwards by setting the ""type-argument"" to both, or am I wrong here?</p>
"
"0.140142550760116","0.140719508946058","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"0.0429097175767967","0.0574484989621426","125152","<p>i would like your help to implement this model in R</p>

<p><img src=""http://i.stack.imgur.com/dGwpe.gif"" alt=""enter image description here""></p>

<p>or more explicity</p>

<p><img src=""http://i.stack.imgur.com/ff5iD.gif"" alt=""enter image description here""></p>

<p>where</p>

<ul>
<li>yt = monthly mean values</li>
<li>Î¼i = mean value in month i, i = 1 . . . 12 .</li>
<li>I1;t = Indicator series for month i of the year, i.e., 1 if the month
corresponds to month i of the year, and 0 otherwise.</li>
<li>bi = Trend in month i of the year.</li>
<li>Rt = t/12</li>
<li>Z1;t= Regressor 1, with c1 the associated coefficient.</li>
<li>Z2;t= Regressor 2, with c2 the associated coefficient.</li>
<li>Nt = Residual noise series, modeled as an autoregressive AR(1) series</li>
</ul>

<p>so i would like to get 12 coefficients for monthly and trend component and 1 for regressors 1 and 2.</p>

<p>Thanks in advance, all help is very much appreciated.</p>
"
"0.121978433679867","0.128604641888975","125414","<p>I have two variables:</p>

<ul>
<li>urban areas</li>
<li>protected areas.</li>
</ul>

<p>My observations are urban areas and protected areas in each year. But these observations are the cumulative ones, so observations in each variable have auto-correlation.</p>

<p>Can I use the general correlation such as yielded by <code>cor()</code> in R to measure the correlation between these two variables? If not, which indicator or method can I use?</p>

<p>I have the scatter plot: the horizontal variable is urban area in a specific year, and the vertical variable is another one in that specific year. And these two variables are increasing as years pass. I can see these two variables present a linear relationship. And my purpose is to find a indicator which can measure this linear relationship. I actually have tested the linear regression: the urban area as independent variable, the protected area as dependent variable, and I put 14 pairs of each year into the regression model, and the coefficients can pass the t-test, and model can pass the t-test, the $R^2$ can reach more than 0.9. </p>

<p>I want to research the relationship between urban development and protected area development. And the scatter plot below is urban and protected area pairs on global scale for 1950-2014 with 5 year intervals (except for 2010 and 2014).</p>

<p>I want to test two questions: First, are these two areas (urban and protected areas) both increasing over the research period? Second, does urbanization (here I mean the development of urban area) cause the development of protected areas?</p>

<p>I want to use some correlation analysis to solve the first question, such as correlation, linear regression or MIC value. However, because my data are time series, I'm not sure it can be used in the calculation of correlation? So I raise this question. In addition, I don't know other methods that could be used to measure strength of linear relationship between two time series. </p>

<p>And for the second question, I want to use Granger causality test to test the causality relationship between these two areas statistically. I know the result of Granger causality can't be sure to determine the causality relationship. And in my opinion, the reasons to improve the development of urban areas or protected areas are both complex, and some of them may be shared. At this level, I simply want to test the causality relationship between these two variables.</p>

<p><img src=""http://i.stack.imgur.com/tHlOm.jpg"" alt=""scatter plot between urban and farm land, the point is a variable pair in a specific year""></p>
"
"0.145965139117999","0.151994441447782","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0404556697031367","0.0406222231851194","125523","<p>What I'm trying to do is to construct a linear model in a form like</p>

<p>$$
Y = \beta_0X_0-\beta_1X_1+\beta_2X_2 + \beta_3
$$</p>

<p>where $\beta_0$, $\beta_1$ and $\beta_2$ are coefficient of predictors $X_0$, $X_1$ and \beta_2 respectively. And, They all are <strong>positive</strong>, which means my assumption of the model is that $X_1$ has negative effect towards the response $Y$. Perhaps, I misunderstand the concept of regression, but if anyone has an idea how to achieve this in R, please enlighten me. Or any other approach apart from regression model. Thanks in advance.</p>
"
"0.0495478739876288","0.0497518595104995","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"0.0993902382172794","0.0921225430717611","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.0707974219804893","0.0812444463702388","126356","<p>For linear and parametric regression there are multiple tests where variables and residuals are used by means of performing a linear regression function to test serial correlation of regression errors and homocedasticity of regression errors. </p>

<p>My question is about non linear and non parametric regression for prediction or classification such us SVM, NeuralNets, knn, Recursive Partitioning, Adaptive Regression Spline, etc. </p>

<p>In this regard my questions are:</p>

<ol>
<li><p>As is not linear regression what is the equivalent of OLS assumptions for non linear non parametric regression. Are the consequences of OLS violation in the context of nonlinear and non-parametric regression still valid? </p></li>
<li><p>How could I test or what tests exist for serial correlation of errors for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes in mind like testing for significant acf or pacf on the residual errors - Unsure if this is OK).</p></li>
<li><p>How could I test or what tests exist for homocedasticity for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes too mind like homogenity of distances between the residual errors across time).</p></li>
<li><p>Would it be better to transfor the data into linear by seeking some adequate transformation as to avoid all the non linearity issues mentioned above? </p></li>
</ol>

<p>Thank you</p>
"
"0.0404556697031367","0.0203111115925597","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.0858194351535935","0.0765979986161901","126715","<p>I am doing a logistic regression. The predictor variables are a mixture of categorical and continuous. I ran glm and out of the 80 predictors about 36 came out to be significant based on the p value. The accuracy of the model was also very good.</p>

<ol>
<li><p>I am still stuck with 36 variables but I want to narrow it down further to identify which of the predictors have the greatest impact. I understand that all the 36 predictors are statistically significant but all of these variables do not impact the DV equally. Is their a way to rank these variables based on their influence on the DV? Please feel free to suggest any methods/algorithms you know that does this efficiently.</p></li>
<li><p>Once I narrow down the predictors of my interest, I want to come up with rules based on the variables, much like a decision tree gives. I have tried running <code>rpart</code> and <code>ctree</code> on the 80 variable dataset but the output tree is very small meaning only a few variables appear in the tree, and thus there are very few rules which I can make based on that. I wonder if their is a way to increase the size of my tree to include more variables. Suppose I narrow down to 10-12 predictors, what all modeling techniques can I use to makes rules.</p>

<p>For example, I want something like: when x1 in range (a, b), x2 in range (c, d), ... and so on then the probability of $y(dv) &gt; 0.5$ or the event occurs i.e., $y = 1$ so that the range of values of the predictors can act as rules for determining when the event occurs.</p></li>
</ol>
"
"0.0948769553749019","0.0952675579132743","126990","<p>I would like to conduct a meta-analysis in the context where I have studies available that measure a continuos variable at multiple time points (0, 1, 2, 3, 4, 5). Time 0 represents the baseline where values are at 100%. Right afterwards there is an intervention and the effect of the intervention is measured over time (114% represents a 14% change relative to baseline). Also I have given two different groups that received different interventions.</p>

<p>Please consider the following dummy data set:</p>

<pre><code>library(ggplot2)
library(metafor)
library(dplyr)
n &lt;- 10
a &lt;- c(rnorm(n,100,0), rnorm(n, 110,2), rnorm(n,130,2), rnorm(n,135,2), rnorm(n,130,2), rnorm(n,125,2))
b &lt;- c(rnorm(n,100,0), rnorm(n,107,2), rnorm(n,122,2), rnorm(n,128,2), rnorm(n,122,2), rnorm(n,125,2))
sd &lt;- rnorm(n,10,1)
my_dat &lt;- data.frame(mean=c(a, b), sd=rep(sd,12), time=rep(c(rep(0,n), rep(1,n), rep(2,n), rep(3,n), rep(4,n), rep(5,n)),2), group=c(rep(""A"", 60), rep(""B"",60)), n=rep(n,120))
my_dat$study &lt;- 1:10
p &lt;- ggplot(aes(y=mean, x=time, colour=group), data=my_dat)
p + geom_jitter() + geom_smooth() + ylab(""% relative to baseline"") + xlab(""time"") 
</code></pre>

<p><img src=""http://i.stack.imgur.com/Lx6TY.png"" alt=""raw data example""></p>

<p>I would like to :</p>

<p>1) investigate the main effect of time (as well as post-hoc tests) for each group individually using the metafor package.</p>

<p>2) investigate the main effect of group (as well as post-hoc tests) for each point in time using the metafor package.</p>

<p>3) investigate group-time interactions.</p>

<p>Thus I rearrange the data and calculate hegdes g relative to baseline t0:</p>

<pre><code>t0_dat &lt;- summarise(group_by(my_dat[my_dat$time==0,], study, group), t0_mean=mean(mean), t0_sd=mean(sd))
my_dat &lt;- merge(my_dat, t0_dat, by=c(""study"", ""group""), all.x=T)
my_dat &lt;- escalc(m1i=mean, m2i=t0_mean, sd1i=sd, sd2i=t0_sd, n1i=n, n2i=n, measure=""SMD"", data=my_dat, append=T)
p &lt;- ggplot(aes(y=yi, x=time, xmin=yi-vi, xmax=yi+vi, colour=group), data=my_dat)
p + geom_point() + geom_smooth() + ylab(""hedges g"") + xlab(""time"") + xlim(c(0,5)) + ylim(c(0,5))
</code></pre>

<p><img src=""http://i.stack.imgur.com/Gf11j.png"" alt=""enter image description here""></p>

<p>Finally I can run the meta-analysis:</p>

<pre><code>m1 &lt;- rma(yi,vi, data=my_dat, mods=~time*group)
summary(m1)
</code></pre>

<p>This indicates a sig. effect of time, a sig. effect of group but no interaction:
Model Results:</p>

<pre><code>         estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt        1.4500  0.2159   6.7158  &lt;.0001   1.0269   1.8732  ***
time           0.3294  0.0667   4.9363  &lt;.0001   0.1986   0.4602  ***
groupB        -0.6808  0.2994  -2.2738  0.0230  -1.2676  -0.0940    *
time:groupB    0.0802  0.0932   0.8609  0.3893  -0.1024   0.2628     

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Is this an valid approach?
Would it be appropriate to instead of converting to effect size (hedges g) to use the percentage values (as extracted from the papers) and log-transform them as suggested <a href=""http://stats.stackexchange.com/questions/34057/estimating-percentages-as-the-dependent-variable-in-regression"">in this question</a>, <a href=""http://stackoverflow.com/questions/9958722/r-variable-selection-for-multiple-regression-w-percentage-dependent-variable"">in this question</a> or in the comments below?
Hints to papers that conducted comparable analysis are more then welcome!</p>
"
"NaN","NaN","127226","<p>I'm trying to simulate a logistic regression. My goal is showing that if <code>Y=1</code> is rare, than the intercept is biased. In my R script I define the logistic regression model through the latent variable's approach (see for example pp. 140 <a href=""http://gking.harvard.edu/files/abs/0s-abs.shtml"" rel=""nofollow"">http://gking.harvard.edu/files/abs/0s-abs.shtml</a>):</p>

<pre><code>x   &lt;- rnorm(10000)

b0h &lt;- numeric(1000)
b1h &lt;- numeric(1000)

for(i in 1:1000){
  eps &lt;- rlogis(10000)
  eta &lt;- 1+2*x+eps
  y   &lt;-numeric(10000)
  y   &lt;- ifelse (eta&gt;0,1,0)

  m      &lt;- glm(y~x,family=binomial)
  b0h[i] &lt;- coef(m)[1]
  b1h[i] &lt;- coef(m)[2]
}

mean(b0h)
mean(b1h)
hist(b0h)
hist(b1h)
</code></pre>

<p>The problem here is that I don't know how to force the observations y to be balanced before (50:50), then unbalanced (90:10). As we can see with the function table(), in my script the proportion of ones is random.</p>

<pre><code>table(y)
</code></pre>

<p>How to solve this problem?</p>
"
"0.0572129567690623","0.0574484989621426","128704","<p>We have 2 correlated variables and a lot of binomial factors (around 200),
here illustrated with just $f1$ and $f2$:</p>

<pre><code>x &lt;- rnorm(100)
y &lt;- rnorm(100)
f1 &lt;- rbinom(100, 1, 0.5)
f2 &lt;- rbinom(100, 1, 0.5)
</code></pre>

<p>Which gives four possible groups: A $(f1=1,f2=1)$, B $(f1=0,f2=1)$, C $(f1=1,f2=0)$, and D $(f1=0,f2=0)$.</p>

<p>We then run the model</p>

<pre><code>&gt; glm(y ~ x * f1 + x * f2)

Call:
glm(formula = y ~ x * f1 + x * f2)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.72028  -0.58501   0.03167   0.60097   1.86332  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.03188    0.17388  -0.183   0.8549  
x            0.08105    0.20540   0.395   0.6940  
f1           0.26823    0.19309   1.389   0.1681  
f2          -0.34568    0.19488  -1.774   0.0793 .
x:f1         0.10301    0.20183   0.510   0.6110  
x:f2        -0.25875    0.20828  -1.242   0.2172  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.8906953)

    Null deviance: 88.754  on 99  degrees of freedom
Residual deviance: 83.725  on 94  degrees of freedom
AIC: 280.02

Number of Fisher Scoring iterations: 2
</code></pre>

<p>We can simplify this output and make a regression ($y = a + b \times x$) for each group by doing:</p>

<p>$a_A = (-0.03188) + (0.26823) + (-0.34568)=-0.10932806$
$b_A = (0.08105) + (0.10301) + (-0.25875)=-0.07468630$</p>

<p>$a_B = (-0.03188) + (-0.34568)=-0.37755949$
$b_B = (0.08105) + (-0.25875)=-0.17769345$</p>

<p>And the same for the C and D groups. My question is: How do I calculate a standard deviation or confidence intervals for the individual group slopes. Is it additive like the estimates or is it the means or something else?
Thank you for any help.</p>
"
"0.0286064783845312","0.0287242494810713","128951","<p>I have been asked to compared between Robustness of absolute lost regression and its variants compared to least squares. I have done the least squares should I use Lasso now?</p>

<pre><code>con = url (""http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data"")
prost=read.csv(con,row.names=1,sep=""\t"")

summary(prost)

plot (prost$age, prost$lcavol) # standard plot

plot (prost) # all vs all in the R window
# into file:
#       postscript(""prost.ps"")
#       plot (prost) #into postscript file in current directory (also see commands pdf, jpeg, etc..)
#       dev.off()


prost.tr = prost[prost$train,] # train observations
    prost.te = prost[!prost$train,] # test observations

attach(prost.tr) # now we can treat columns as variables
summary (age)
detach()


####linear regression#####

prost.linreg =  lm (lpsa~.-train, data=prost.tr)
summary(prost.linreg)


lin.pred.te = predict (prost.linreg, newdata=prost.te)
lin.pred.te[14] = 500000000


summary((prost.te$lpsa-lin.pred.te)^2)
    rmse.lin = sqrt(mean((prost.te$lpsa-lin.pred.te)^2))


y.lin=0
x.lin= c(1:30)
for (i in 1:30){
  y.lin[i] = lin.pred.te[i]

}


lin.reg=summary(lm(y.lin~x.lin))
</code></pre>
"
"0.0700712753800578","0.0703597544730292","129657","<p>What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? </p>

<p>Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or ""with shrinkage"".</p>

<p>I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge.</p>

<p>There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential.</p>

<p>This could be done via <code>lme4</code> in <code>R</code>. However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model?</p>

<p>EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc.</p>

<p>EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts.</p>

<p>EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).</p>
"
"0.0700712753800578","0.0703597544730292","130643","<p>I tried a regression in the form ${\rm logit}(Y) = {\rm coefficient}\times X + 0 + e$, where $Y$ is a binomial variable and $X$ is a factor variable with $n$ levels. I noticed that removing the intercept yields higher $p$ values. I'm wondering how to interpret it though.</p>

<p>Since removing the intercept makes it equal to $0$, I believe that the coefficients returned are relative to a $0$ probability of the event $Y$ and that all $X$ factors are in the $0$ state. But I think this is impossible isn't it?</p>

<p>$X$ are mutually exclusive factors, therefore it's impossible to have a case where no factor is $1$, at least in the presented observations. And it cannot be interpreted like the coefficient is relative to hypothetical cases in which really no one of the factors is present, because we have no data like that.</p>

<p>Regarding $Y$ having a $0$ intercept, wouldn't it mean forcing the probability of the event to $0$ when none of the factors is present? Again, this is an impossible case.</p>

<p>Nonetheless this kind of regression would allow me to retrieve pure probability range of the event Y given a factor by transforming the coefficients in the confidence intervals given as $\exp({\rm coefficient})/(1 + \exp({\rm coefficient}))$, and the $p$ values would test whether this probability is not $50\%$. This could also be a valuable result, since it would give independent probabilities for each factor.</p>

<p>Am I wrong?</p>
"
"0.0990957479752576","0.0995037190209989","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.0862517776135472","0.0866068708302494","131261","<p>I have three macro economic variables (ICS - consumer sentiment, ER - employment rate, DGO - durable goods order) and have run Granger causality tests in R on them. I don't really know how to interpret the results of a Granger test. Could anyone give me a hand with making some sense of the results? </p>

<p>I know that we are checking to see if one variable can be used to predict another and I understand that if that is true then there must be some lag in one of the variables and that the order of the Granger test has to do with the order. I don't know how to interpret the fact that 2 models are reported here. I can see that one model is with the regressor variable and the other model is without the regressor. I assume the Lags vector 1:3 means that we are testing 1, 2,and 3 month lags. </p>

<pre><code>grangertest(ICS~ER, order = 3, data=modeling.mts)

Granger causality test

Model 1: ICS ~ Lags(ICS, 1:3) + Lags(ER, 1:3)
Model 2: ICS ~ Lags(ICS, 1:3)
  Res.Df Df      F Pr(&gt;F)
1    258                 
2    261 -3 2.0352 0.1094

grangertest(ICS~DGO, order = 3, data=modeling.mts)

Granger causality test

Model 1: ICS ~ Lags(ICS, 1:3) + Lags(DGO, 1:3)
Model 2: ICS ~ Lags(ICS, 1:3)
   Res.Df Df     F   Pr(&gt;F)   
1    258                      
2    261 -3 4.8621 0.002625 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

grangertest(DGO~ER, order = 3, data=modeling.mts)

Granger causality test

Model 1: DGO ~ Lags(DGO, 1:3) + Lags(ER, 1:3)
Model 2: DGO ~ Lags(DGO, 1:3)
  Res.Df Df      F  Pr(&gt;F)  
1    258                    
2    261 -3 3.2704 0.02181 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.171638870307187","0.172345496886428","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.06396603026469","0.0642293744423385","132761","<p>I am wanting to calculate hazard ratio in a matched cohort design.
For such I am using the <code>coxph()</code> function in R.
But I have been recommended to stratify by matched pair.
My matching has been simulated using the <code>match()</code> and <code>MatchBalance()</code> commands.</p>

<p>But I am confused as to what 'stratify by matched pair' means.
Can someone explain in simple terms please?
ie. in the simulation does this mean that each matched pair is given a unique identity</p>

<p>OR </p>

<p>are the all the case with exposure given an identity, (say 1), and all case without exposure given a different identity (say 2).....</p>

<p>I have been unable to find any literature in relation to R, or even a tutorial to follow.
The <code>â€˜RcmdrPlugin.EZR</code> can be used for the simulation, but it does explain the <em>Stratified Cox proportional hazard regression for matched-pair analysis</em></p>
"
"0.210213826140173","0.203261512922084","132925","<p>I have data from the following experimental design: my observations are counts of the numbers of successes (<code>K</code>) out of corresponding number of trials (<code>N</code>), measured for two groups each comprised of <code>I</code> individuals, from <code>T</code> treatments, where in each such factor combination there are <code>R</code> replicates. Hence, altogether I have 2 * I * T * R <em>K</em>'s and corresponding <em>N</em>'s. </p>

<p>The data are from biology. Each individual is a gene for which I measure the expression level of two alternative forms (due to a phenomenon called alternative splicing). Hence, <em>K</em> is the expression level of one of the forms and <em>N</em> is the sum of expression levels of the two forms. The choice between the two forms in a single expressed copy is assumed to be a Bernoulli experiment, hence <em>K</em> out of <em>N</em> copies follows a binomial. Each group is comprised of ~20 different genes and the genes in each group have some common function, which is different between the two groups. For each gene in each group I have ~30 such measurements from each of three different tissues (treatments). I want to estimate the effect that group and treatment have on the variance of K/N.</p>

<p>Gene expression is known to be overdispersed hence the use of negative binomial in the code below.</p>

<p>E.g., <code>R</code> code of simulated data:</p>

<pre><code>library(MASS)
set.seed(1)
I = 20 # individuals in each group
G = 2  # groups
T = 3  # treatments
R = 30 # replicates of each individual, in each group, in each treatment

groups     = letters[1:G]
ids        = c(sapply(groups, function(g){ paste(rep(g, I), 1:I, sep=""."") }))
treatments = paste(rep(""t"", T), 1:T, sep=""."")
 # create random mean number of trials for each individual and 
 #  dispersion values to simulate trials from a negative binomial:
mean.trials = rlnorm(length(ids), meanlog=10, sdlog=1)
thetas      = 10^6/mean.trials
 # create the underlying success probability for each individual:
p.vec = runif(length(ids), min=0, max=1)
 # create a dispersion factor for each success probability, where the 
 #  individuals of group 2 have higher dispersion thus creating a group effect:
dispersion.vec = c(runif(length(ids)/2, min=0, max=0.1),
                   runif(length(ids)/2, min=0, max=0.2))
 # create empty an data.frame:
data.df = data.frame(id=rep(sapply(ids, function(i){ rep(i, R) }), T),
                     group=rep(sapply(groups, function(g){ rep(g, I*R) }), T),
                     treatment=c(sapply(treatments, 
                                        function(t){ rep(t, length(ids)*R) })),
                     N=rep(NA, length(ids)*T*R), 
                     K=rep(NA, length(ids)*T*R) )
 # fill N's and K's - trials and successes
for(i in 1:length(ids)){
  N     = rnegbin(T*R, mu=mean.trials[i], theta=thetas[i])
  probs = runif(T*R, min=max((1-dispersion.vec[i])*p.vec[i],0),
                max=min((1+dispersion.vec)*p.vec[i],1))
  K     = rbinom(T*R, N, probs)
  data.df$N[which(as.character(data.df$id) == ids[i])] = N
  data.df$K[which(as.character(data.df$id) == ids[i])] = K
}
</code></pre>

<p>I'm interested in estimating the effects that group and treatment have on the dispersion (or variance) of the success probabilities (i.e., <code>K/N</code>). Therefore I'm looking for an appropriate glm in which the response is K/N but in addition to modelling the expected value of the response the variance of the response is also modeled.</p>

<p>Clearly, the variance of a binomial success probability is affected by the number of trials and the underlying success probability (the higher the number of trials is and the more extreme the underlying success probability is (i.e., near 0 or 1), the lower the variance of the success probability), so I'm mainly interested in the contribution of  group and treatment beyond that of the number of trials and the underlying success probability. I guess applying the arcsin square root transformation to the response will eliminate the latter but not that of the number of trials.</p>

<p>Although in the simulated example data above the design is balanced (equal number of individuals in each of the two groups and identical number of replicates in each individual from each group in each treatment), in my real data it is not - the two groups do not have an equal number of individuals and the number of replicates varies. Also, I'd imagine the individual should be set as a random effect.</p>

<p>Plotting the sample variance vs. the sample mean of the estimated success probability (denoted as p hat = K/N) of each individual illustrates that extreme success probabilities have lower variance:
<img src=""http://i.stack.imgur.com/2AWFd.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/ArGnD.png"" alt=""enter image description here""></p>

<p>This is eliminated when the estimated success probabilities are transformed using the arcsin square root variance stabilizing transformation (denoted as arcsin(sqrt(p hat)):
<img src=""http://i.stack.imgur.com/Ktj4r.png"" alt=""enter image description here""></p>

<p>Plotting the sample variance of the transformed estimated success probabilities vs. the mean N shows the expected negative relationship:
<img src=""http://i.stack.imgur.com/i1CwL.png"" alt=""enter image description here""></p>

<p>Plotting the sample variance of the transformed estimated success probabilities for the two groups shows that group b has slightly higher variances, which is how I simulated the data:
<img src=""http://i.stack.imgur.com/lr5uo.png"" alt=""enter image description here""></p>

<p>Finally, plotting the sample variance of the transformed estimated success probabilities for the three treatments shows no difference between treatments, which is how I simulated the data:
<img src=""http://i.stack.imgur.com/xQlHD.png"" alt=""enter image description here""></p>

<p>Is there any form of a generalized linear model with which I can quantify the group and treatment effects on the variance of the success probabilities? </p>

<p>Perhaps a heteroscedastic generalized linear model or some form of a loglinear variance model?</p>

<p>Something in the lines of a model which models the Variance(y) = ZÎ» in addition to E(y) = XÎ², where Z and X are the regressors of the mean and variance, respectively, which in my case will be identical and include treatment (levels t.1, t.2, and t.3) and group (levels a and b), and probably N and R, and hence Î» and Î² will estimate their respective effects.</p>

<p>Alternatively, I could fit a model to the sample variances across replicates of each gene from each group in each treatment, using a glm which only models the expected value of the response. The only question here is how to account for the fact that different genes have different numbers of replicates. I think the weights in a glm could account for that (sample variances that are based on more replicates should have a higher weight) but exactly which weights should be set?</p>

<p>Note:
I have tried using the <code>dglm</code> R package:</p>

<pre><code>library(dglm)
dglm.fit = dglm(formula = K/N ~ 1, dformula = ~ group + treatment, family = quasibinomial, weights = N, data = data.df)
summary(dglm.fit)
Call: dglm(formula = K/N ~ 1, dformula = ~group + treatment, family = quasibinomial, 
    data = data.df, weights = N)

Mean Coefficients:
               Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) -0.09735366 0.01648905 -5.904138 3.873478e-09
(Dispersion Parameters for quasibinomial family estimated as below )

    Scaled Null Deviance: 3600 on 3599 degrees of freedom
Scaled Residual Deviance: 3600 on 3599 degrees of freedom

Dispersion Coefficients:
                Estimate Std. Error      z value  Pr(&gt;|z|)
(Intercept)  9.140517930 0.04409586 207.28746254 0.0000000
group       -0.071009599 0.04714045  -1.50634107 0.1319796
treatment   -0.001469108 0.02886751  -0.05089138 0.9594121
(Dispersion parameter for Gamma family taken to be 2 )

    Scaled Null Deviance: 3561.3 on 3599 degrees of freedom
Scaled Residual Deviance: 3559.028 on 3597 degrees of freedom

Minus Twice the Log-Likelihood: 29.44568 
Number of Alternating Iterations: 5 
</code></pre>

<p>The group effect according to dglm.fit is pretty weak. I wonder if the model is set right or is the power this model has.</p>
"
"0.06396603026469","0.0642293744423385","133387","<p>I'm trying to create a prediction model for estimation of continuous variable based on about 35 Independent variables.My data set has circa 27k observartions.
Here is the summary of the the targeted continuous variable:</p>

<pre><code>              Frequency Percent
(0,5]              2706  10.053
(5,10]             5226  19.415
(10,25]            4397  16.335
(25,100]           7142  26.533
(100,1e+03]        6465  24.018
(1e+03,1e+05]       981   3.645
Total             26917 100.000
</code></pre>

<p>I tried (by using R) Random Forest (RandomForest package),Linear regression, Conditional Inference Trees (ctree function in party package) but all of them have results that have a significant overestimation.
Here are the results of the prediction where I counted number of observations by thier distance from the actual values:
Any idea how can i balance the results?</p>

<p><img src=""http://i.stack.imgur.com/y70OM.png"" alt=""enter image description here""></p>

<p>Here are some views on the data:
The target variable is LTV for a user, I would like to predict LTV value after 180 days  based on users behavior of the first 7 days.
Here Is a summary fot the target variavle:</p>

<pre><code>  vars     n   mean     sd median trimmed   mad  min      max    range skew kurtosis   se
1    1 26917 178.35 622.29  33.49   66.63 39.28 0.03 22103.73 22103.71 14.1   325.08 3.79
</code></pre>

<p>UPDATE:
Here are the distributions of the targeted variable (first)and the prediction (secound)results:
<img src=""http://i.stack.imgur.com/b3MBs.png"" alt=""targeted variable"">
<img src=""http://i.stack.imgur.com/3N7d1.png"" alt=""prediction results based on the linear regression model that was the best""></p>
"
"0.0572129567690623","0.0574484989621426","133488","<p>I am trying to run regression on financial data in R. I am new to regression analysis so I am finding it to difficult to interpret certain scenarios. I have the code as follows:</p>

<pre><code>#regression analysis
fit &lt;- lm(fiveMinReturns~RegressionData, data=maindata)
summary(fit) # show results
#correlation
cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
</code></pre>

<p>My output is: </p>

<pre><code>Call:
lm(formula = fiveMinReturns ~ RegressionData, data = maindata)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.205790 -0.001144 -0.000062  0.001117  0.156418 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    6.346e-05  8.785e-06   7.223 5.09e-13 ***
RegressionData 1.597e-07  1.432e-08  11.155  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.004035 on 210912 degrees of freedom
Multiple R-squared:  0.0005896, Adjusted R-squared:  0.0005849 
F-statistic: 124.4 on 1 and 210912 DF,  p-value: &lt; 2.2e-16

cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
[1] 0.02428219
</code></pre>

<p>p-value is very small that means two variables are tightly coupled, but correlation is small too.
My question is how do I evaluate this situation?
Can we say that this equation will give correct results almost every time?
Which scenario suggests both p-value and correlation both to be really small?
What measures should i take to improve the result? </p>
"
"0.118129972176712","0.12520610071704","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.0948769553749019","0.0866068708302494","134141","<p>Recently I am reading a paper where the authors use the GAM to make predictions. In brief, the data looks like following:</p>

<pre><code>  y    i    j     x    weekend
5.6    1    1   4.6    Mon.
6.5    1    2   5.6    Mon.
...
4.6    2    1   6.7    Sta.
2.4    2    2   1.2    Sta.
...
</code></pre>

<p>where <code>y</code>, <code>x1</code>, <code>x2</code> are continuous numbers, <code>weekend</code> is the day of the week. In the paper, the authors use the following formula:  </p>

<p>$$y_{ij} = \beta_0 + b_{0i} + \beta_1{\rm weekend}_i + f_1(x_{ij}, {\rm weekend}_i) + \varepsilon_{ij}$$</p>

<p>In the formula, $\beta_0$ is the overall mean, $b_{0i}$ is the random intercept, ${\rm weekend}_i$ determines whether it is weekday or weekend. Ans so I transform ${\rm weekend}$ from {Mon., Thu., .., Sun.} into {0, 1}. And $f_1$ is cubic regression function with 17 spline knots, and in fact will generate two smooth functions one for weekday, another for weekend.</p>

<p>I want to use following code:  </p>

<pre><code>gam(y~ s(i,bs=""re"") + weekend + s(x, by=weekend, bs=""cr"", k=17))
</code></pre>

<p>But I'm not sure whether it fits the formula or not. My questions are:</p>

<ol>
<li><code>gam</code> will automatically generate the mean of the model, so there is no need to specify a $\beta_0$ in the code?  </li>
<li>Is it right that by using <code>s(i,bs=""re"")</code>, the <code>gam</code> will calculate different random effect with distribution $N(0, \delta_i)$ for every $i$ specifically?</li>
<li>Is it good to transform weekend into 0-1 value? and in the code <code>s(x, by=weekend, bs=""cr"", k=17)</code>, does the <code>by</code> keyword mean that it will generate different smooth functions of <code>x</code> for different <code>weekend</code> value?</li>
<li>The last question is that without specifying <code>knots=list()</code>, as in the above code, the default behaviour of the model is to put knot points evenly of the range of value?</li>
</ol>
"
"0.151708761386763","0.147255559046058","135043","<p>I have three questions concerning accelerated failure time models (AFT), one statistical, one regarding how to implement these models in R, and one related to finding out information about what R is doing. In short my questions are;</p>

<p>1) What is the relationship between the Gumbel and Weibull distributions?</p>

<p>2) How can I use (1) to simulate a AFT model using Gumbel errors and fit this model in R?</p>

<p>3) Where can I find formulae regarding exactly what distribution specification R is using when fitting a Weibull distribution, and exactly what model is being fitted?</p>

<p>I am having difficulties implementing 2), which may be due to my mis-understanding of 1), but which I can't seem to resolve due to 3). Question (3) is self-explanatory but (2) and (3) require more detail;</p>

<p>1) It seems a standard result that if $U\sim Gumbel(\alpha,\beta)$ then $V:=\exp(U)\sim Weibull(\lambda,\sigma)$ where $\alpha=\log(\sigma)$ and $\beta=1/\lambda$. However using the definition of the Gumbel and Weibull distributions commonly used (for example Wikipedia), when I do the derivation I can only get the transformation $V':=1/\exp(U)$ to give this result but where $\alpha=-\log(\sigma)=\log(1/\sigma)$. Thus can anyone confirm or not any knowledge of this relationship, or perhaps suggest where I have gone wrong (for brevity in the first instance I do not supply the detail)?</p>

<p>2) My approach is to use</p>

<p>$Y_{i}:=\log\left(\frac{1}{T_{i}}\right)=\beta_{0} + \beta_{1}x_{i} + e_{i},\hspace{20pt}i=1,...,N$,</p>

<p>as a data-generating mechanism for the logarithm of the time to event where $e_{i}\sim Gumbel(\alpha,\beta)$, where $i$ indexes subjects, $x_{i}$ is a scalar covariate, and the $e_{i}$ are all independent. I choose $\alpha=-\beta*c$ where $c$ is Euler's constant in order to ensure $E[e_{i}]=\alpha+c\beta=0$. This gives</p>

<p>$Y_{i}\sim Gumbel(\beta_{0} + \beta_{1}x_{i}+\alpha,\beta)$,</p>

<p>and using (1)</p>

<p>$T_{i}\sim Weibull(1/\beta,\exp[-(\beta_{0} + \beta_{1}x_{i}+\alpha)])$</p>

<p>The code at the end of this post is a minimal working example of this approach, where I censor subjects if $T_{i}$ is greater than the median of the $N$ theoretical medians of $\{T_{1},...,T_{N}\}$, and create an event if not. This gives $50-60\%$ of subjects being censored, the balance having events, and I interpret this to be right-censoring (say the end of a study).</p>

<p>I then use the survreg package in R to try to fit an AFT to $Y_{i}$ using the ""dist=weibull"" option. Using $\beta_{0}=-10$ and $\beta_{1}=0$ gives the following output</p>

<p><img src=""http://i.stack.imgur.com/4sQlb.png"" alt=""enter image description here""></p>

<p>which gives the intercept being positive when it should be negative. Things get worse when using $\beta_{0}=-10$ and $\beta_{1}=2$ which gives the following output</p>

<p><img src=""http://i.stack.imgur.com/i7o3f.png"" alt=""enter image description here""></p>

<p>which is obviously wrong. Thus I would like to know what model I am actually fitting when using the survreg package.</p>

<p>The code below is a minimal working example (apart from some code to produce plots which can be helpful).</p>

<pre><code># minimal working example
set.seed(123)
require(survival)
#params of the gumbel(alpha_gum,beta_gum) distribution so that E[X]=0
beta_gum = 1/5 #
alpha_gum = -(beta_gum*(-digamma(1)))

#calc the mean of the errors using Eulers constant as the negative of the diagamma function
mu_e = alpha_gum + (beta_gum*(-digamma(1)))#should be 0   

# regression parameters
intercept = -10;
beta1 =0;
#beta1 =2;

#number of subjects
N=1000;

# vector of uniform random numbers
U = runif(N)

#vector for gumbel distributed errors
e = matrix(,nrow=N,ncol=1)


# log of time to event, time to event, mean LTTE
logTTE = matrix(,nrow=N,ncol=1)
Xbeta_LTTE= matrix(,nrow=N,ncol=1)
TTE = matrix(,nrow=N,ncol=1)
TTE2 = matrix(,nrow=N,ncol=1)

#censoring variable
censor = matrix(,nrow=N,ncol=1)

#simulate covariate from a normal distribution
covariate1 = rnorm(N,6,4)

for (i in 1:N)
{
  # calculate the Gumbel RV from the inverse CDF of the Gumbel
  e[i,1] = alpha_gum + (-beta_gum*log(-log(U[i])))

  #generate the mean log TTE  
  Xbeta_LTTE[i,1] = intercept + (beta1*covariate1[i])

  #add the errors
  logTTE[i,1] = Xbeta_LTTE[i,1] + e[i,1]  

  #transform to raw time variable - this is a Weibull dist
  #TTE_i ~ Weibull[1/beta_gum , exp(-[logTTE_i+alpha_gum])
  TTE[i,1] = 1/exp(logTTE[i,1])      
}

#calc the median the TTE given TTE ~ Weibull[1/beta_gum , exp(-[X_i^t*beta+alpha_gum])
lambda_array = exp(-(Xbeta_LTTE + alpha_gum + (beta_gum*(-digamma(1)))))
kappa = 1/beta_gum
median_TTE_array = (lambda_array)*(log(2)^(1/kappa))
median_TTE = median(median_TTE_array)

# calculate the censoring variable
for (i in 1:N)
{
  #censoring: subjects with a TTE &gt;median_TTE will be right-censored
  #i.e. study ends at T=median_TTE say
  if (TTE[i,1]&gt;median_TTE)
  {
    censor[i,1]=1 
    TTE2[i,1]=median_TTE
  }
  else
  {
    censor[i,1]=0    
    TTE2[i,1]=TTE[i,1]
  }  
}

#calculate the percentage of censored subjects and do a plot
pc_censored = sum(censor)/N

#fit AFT model
datframe_surv = data.frame(covariate1)
attach(datframe_surv)

m.surv = Surv(TTE2,censor,type=""right"")
m.surv.fit = survreg(m.surv~covariate1,dist=""weibull"",scale=1)
sum = summary(m.surv.fit)
print(sum)



###################  plots ########################


#histogram of the errors - gumbel dist
h1 = hist(e, breaks=50, plot=FALSE) 

#histogram of the mean log TTE - gumbel dist
h2 = hist(logTTE, breaks=50, plot=FALSE) 

#histogram of the fixed means
h3 = hist(Xbeta_LTTE, breaks=50, plot=FALSE) 

#histogram of the TTE - weibul dist
h4 = hist(TTE, breaks=50, plot=FALSE) 

#calc the mean of the log TTE given logTTE ~ Gumbel(X_i^t*beta+alpha_gum,beta_gum)
median_logTTE_array = Xbeta_LTTE + alpha_gum - (beta_gum*(log(log(2))))
median_logTTE = median(median_logTTE_array)



#calc the means
ylim_h1 = c(min(h1$density),max(h1$density) )
xlim_h1 = c(mu_e,mu_e )

ylim_h2 = c(min(h3$density),max(h3$density) )
xlim_h2 = c(median_logTTE,median_logTTE )

ylim_h3 = c(min(h3$density),max(h3$density) )
xlim_h3 = c(mean(Xbeta_LTTE),mean(Xbeta_LTTE) )


ylim_h4 = c(min(h4$density),max(h4$density) )
xlim_h4 = c(median_TTE,median_TTE )


#dev.off()
par(mfrow=c(2,2))

plot(h1$mids,h1$density,col='red',main=""errors - gumbel dist"",xlab=""errors (log time)"")
lines(xlim_h1,ylim_h1)

plot(h3$mids,h3$density,col='red',main=""mean log TTE (X*beta) - fixed"",xlab=""mean log TTE (log time)"")
lines(xlim_h3,ylim_h3)

plot(h2$mids,h2$density,col='red',main=""log TTE - gumbel dist"",xlab=""log TTE (log time)"")
lines(xlim_h2,ylim_h2)


plot(h4$mids,h4$density,col='red',main=""TTE - Weibull dist"",xlab=""TTE (time)"")
lines(xlim_h4,ylim_h4)
</code></pre>
"
"0.0858194351535935","0.0765979986161901","135258","<p>I'm doing a linear regression using the h2o deep learning interface with R.  I'm comparing the predictions to the ones I'm getting from the randomForest R module.  The predictions from randomForest seem to roughly match up with what I'd expect given the distribution of the variable that I'm trying to predict.  However, the predictions from the h2o deep learning module don't seem to match up with what I'd expect.  Here's the summary from the variable I'm trying to predict:</p>

<pre><code>Profit            
Min.   :-1438.56  
1st Qu.: -133.80  
Median :   -0.59  
Mean   :   19.54  
3rd Qu.:  127.39  
Max.   :  508.41 
</code></pre>

<p>and here are the predictions from h2o deep learning:</p>

<pre><code>preddp             
Min.   :-0.079954  
1st Qu.:-0.017919  
Median :-0.010088  
Mean   :-0.011903  
3rd Qu.:-0.003921  
Max.   : 0.060259
</code></pre>

<p>and here are the predictions from randomForest:</p>

<pre><code>Min.       1st Qu.  Median   Mean     3rd Qu. Max. 
-212.500   -6.346   20.530   24.690   50.600  244.700
</code></pre>

<p>which is roughly what I'd expect.  Why are the h2o deep learning predictions so strange?  Also the correction between the predictions and the target variable are strongly positive with randomForest and even more strongly negative with h2o deep learning so I figure I have to be doing something wrong.</p>

<p>Here's the command I'm using to train the deep learning model:</p>

<pre><code>sdmodel.deep &lt;- h2o.deeplearning(columnIndices, 7, df, classification=FALSE)
</code></pre>
"
"0.0495478739876288","0.0497518595104995","135332","<p>I did a regression analysis with the following variables: Predictor = dummy variable, dependent Variable = metric, moderator variable = metric. I now want to show my results in a figure. The interaction should be shown by three regression lines. One for moderator = Mean (0), one for moderator = -1 SD, and one for moderator = + 1 SD.</p>

<p>How can I do this in R?</p>
"
"0.0707974219804893","0.0812444463702388","135613","<p>I'm using R to create a linear regression model from survey data about public sentiment for a new technology. I am encountering a problem where the addition of a new explanatory variable raises the model's $R^2$ value from 0.52 to precisely 1. This is absurd, but I'm new to this stuff and can't figure out what's going on.</p>

<p>The survey asks several questions about demographic and values and technical knowledge. These items become the explanatory variables in the model. Most are either dummy variables or likert scales that extend from 1 to 7 (meaning that for every such question, each respondent chooses a number between 1 and 7). The survey also asks respondents to what extent they'd support government investment in the new technology. That question becomes the dependent variable in the model. It is also a likert scale that extends from 1 to 7.</p>

<p>I'm using R's <code>lm()</code> function to regress the knowledge, demographics, and values variables against the support for new technology variable. The functional form is:</p>

<pre><code>lm(support~demographics+values+knowledge,data=survey). 
</code></pre>

<p>Out of about 2000 survey responses, 900 remain after NA's are discarded. I created a model comprising approximately 20 explanatory variables, with an $R^2$ value of 0.52. Then, I added in a 21st explanatory variable, and the $R^2$ jumped to 1. When I do a simple regression of only this new variable and the dependent variable, the $R^2$ is 0.67. What could be going on?</p>
"
"0.0700712753800578","0.0703597544730292","135621","<p>I want to do linear regression between vector inputs and vector output. That is each y is a vector with M components, and each x is a vector with N components and the answer should look like y ~ Ax + b where A is an M x N matrix and b is a vector with M components.</p>

<p>I have a very clear understanding of the concept and what I want R to do, but it is the proper syntax I am lacking.</p>

<p>Trying to google around to find this has been quite difficult because terms like multivariable seem to always point me to answers of the form </p>

<p>y ~ x1 + x2 + x3 + .. + xn</p>

<p>where there are multiple input sources (or rather, a multidimensional input), but never with multidimensional outputs.</p>

<p>If I just feed in matrices for y and x that MIGHT give what I want, but it might also just treat each y component as directly related to each x component and give an answer based on that (M = N for the important instance I have). So I have to be sure that I am doing it correctly.</p>

<p>What is the correct means for using R to do linear regression of the sort</p>

<p>y ~ A x + b </p>

<p>where the solution A is an M x N matrix, and b is a vector of length M, and each datum x is a vector of length N and each corresponding datum y is a vector of length M?</p>
"
"0.0862517776135472","0.0952675579132743","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.0960200924600074","0.111248539872496","136012","<p>I want to do the following:</p>

<p>1) OLS regression (no penalization term) to get beta coefficients $b_{j}^{*}$; $j$ stands for the variables used to regress. I do this by </p>

<pre><code>lm.model = lm(y~ 0 + x)
betas    = coefficients(lm.model)
</code></pre>

<p>2) Lasso regression with a penalization term, the selection criteria shall be the Bayesian Information Criteria (BIC), given by</p>

<p>$$\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$$</p>

<p>where $j$ stands for the variable/regressor number, $T$ for the number of observations, and $b_{j}^{*}$ for the initial betas obtained in step 1). I want to have regression results for this specific $\lambda_j$ value, which is different for each regressor used. Hence if there are three variables, there will be three different values $\lambda_j$. </p>

<p>The OLS-Lasso optimization problem is then given by</p>

<p>$$\underset{b\epsilon \mathbb{R}^{n} }{min} = \left \{ \sum_{t=1}^{T}(y_{t}-b^{\top} X_{t}  )^{2} + T\sum_{j=1}^{m} ( \lambda_{t}|b_{j}| )\right \}$$</p>

<p>How can I do this in R with either the lars or glmnet package? I cannot find a way to specify lambda and I am not 100% sure if I get the correct results if I run </p>

<pre><code>lars.model &lt;- lars(x,y,type = ""lasso"", intercept = FALSE)
predict.lars(lars.model, type=""coefficients"", mode=""lambda"")
</code></pre>

<p>I appreciate any help here.</p>

<hr>

<p><strong>Update:</strong></p>

<p>I have used the following code now:</p>

<pre><code>fits.cv = cv.glmnet(x,y,type=""mse"",penalty.factor = pnlty)
lmin    = as.numeric(fits.cv[9]) #lambda.min
fits    = glmnet(x,y, alpha=1, intercept=FALSE, penalty.factor = pnlty)
coef    = coef(fits, s = lmin)
</code></pre>

<p>In line 1 I use cross validation with my specified penalty factor ($\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$), which is different for each regressor. 
Line 2 selects the ""lambda.min"" of fits.cv, which is the lambda that gives minimum mean cross-validation error.
Line 3 performs a lasso fit (<code>alpha=1</code>) on the data. Again I used the penalty factor $\lambda$.
Line 4 extracts the coefficients from fits which belong to the ""optimal"" $\lambda$ chosen in line 2.</p>

<p>Now I have the beta coefficients for the regressors which depict the optimal solution of the minimization problem </p>

<p>$$\underset{b\epsilon \mathbb{R}^{n} }{min} = \left \{ \sum_{t=1}^{T}(y_{t}-b^{\top} X_{t}  )^{2} + T\sum_{j=1}^{m} ( \lambda_{t}|b_{j}| )\right \}$$</p>

<p>with a penalty factor $\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$. The optimal set of coefficients is most likely a subset of the regressors which I initially used, this is a consequence of the Lasso method which shrinks down the number of used regressors.</p>

<p>Is my understanding and the code correct? </p>
"
"0.0993902382172794","0.0997994216610746","136146","<p>I'm trying to manually demean panel data by both time demeaning and cross-sectional demeaning, yet haven't been able to do it well. The best thing I can think of is looping through the means for each year and then for each entity, plus for each variable. Which seems like it would get slow with a lot of a data (especially if I plan on doing simulations). Is there a better way to do this?</p>

<p>I couldn't find a function in R to do it either.</p>

<p>Here is what I have so far: (DATA is a data frame that of the format (n,T, Variables) where n is the entity id and T is the time id); </p>

<pre><code>demean=function(DATA){

names=colnames(DATA)
T=(max(DATA[,2])-min(DATA[,2])+1)
N=max(DATA[,1])

##Cross-Sectional Demeaning
widedata=reshape(DATA, direction=""wide"", v.names=names[-c(1:2)],     idvar=names[2], timevar=names[1])
crossmean=matrix(NA, ncol=length(colnames(widedata))-1)
crossmean[,1:length(t(crossmean[1,]))]=colMeans(widedata[,-c(1)], na.rm=TRUE)
crosswidedata=widedata
for(i in 1:T){
crosswidedata[i,-c(1)]=widedata[i,-c(1)]-crossmean
}

crossdemeaned=reshape(crosswidedata, direction=""long"", times=names[2] )

##Time Demeaning
widedata=reshape(crossdemeaned, direction=""wide"", v.names=names[-c(1:2)], idvar=names[1], timevar=names[2])
timemean=matrix(NA, ncol=length(colnames(widedata))-1)
timemean[,1:length((t(timemean[1,])))]=colMeans(widedata[,-c(1)], na.rm=TRUE)
timewidedata=widedata
for(i in 1:N){
timewidedata[i,-c(1)]=widedata[i,-c(1)]-timemean
}

demeaned=reshape(timewidedata, direction=""long"", times=names[2] )


demeaned=demeaned[order(demeaned[,names[2]],demeaned[,names[1]]),]

return(demeaned)
}
</code></pre>

<p>I simulate a random spatial autoregressive panel data set with fixed and time effects, demean the data, and then run a test within regression of the demeaned data as following:</p>

<pre><code>    ##Creating a Random Spatial Autoregressive Panel Dataset with Fixed and Time Effects

library(spdep)
library(splm)

set.seed(44222)

################################################################
# RANDOMLY INSERT A CERTAIN PROPORTION OF NAs INTO A DATAFRAME #
################################################################
NAins &lt;-  NAinsert &lt;- function(df, prop = .1){
n &lt;- nrow(df)
m &lt;- ncol(df)
num.to.na &lt;- ceiling(prop*n*m)
id &lt;- sample(0:(m*n-1), num.to.na, replace = FALSE)
rows &lt;- id %/% m + 1
cols &lt;- id %% m + 1
sapply(seq(num.to.na), function(x){
        df[rows[x], cols[x]] &lt;&lt;- NA
    }
)
return(df)
}
############## df means data frame, and prop is the proportion of missing ##data desired

############################
########CREATES A RANDOM WEIGHT MATRIX
############################
DAG.random &lt;- function(v, nedges=1) {
edges.max &lt;- v*(v-1)/2
# Assert length(v)==1 &amp;&amp; 1 &lt;= v
# Assert 0 &lt;= nedges &lt;= edges.max
index.edges &lt;- lapply(list(1:(v-1)), function(k) rep(k*(k+1)/2, v-k)) 
index.edges &lt;- index.edges[[1]] + 1:edges.max
graph.adjacency &lt;- matrix(0, ncol=v, nrow=v)
graph.adjacency[sample(index.edges, nedges)] &lt;- 1
graph.adjacency
}

###################################
###### Create Data ################
###################################

n=50                                    ##Number of Observations
T=20                                    ##Number of years
connect=6                               ##average number of connections
W=DAG.random(n,n*.5*connect)            ##Creates a random weight matrix      
W=W+t(W)
W=sweep(W, 1, rowSums(W), FUN=""/"")      ##Row Normalizes the Random Weight

v=3                                     ##Number of Independent Variables

X=matrix(data=NA, nrow=n*T, ncol=(v+2))
for(jjj in 1:T){
X[((jjj-1)*n+1):(jjj*n),1]=c(1:n)
X[((jjj-1)*n+1):(jjj*n),2]=jjj
for(jj in 3:(v+2)){
X[((jjj-1)*n+1):(jjj*n),jj]=rnorm(n, 1, 2)
}
}


sigma=1                             ##standard deviation of model
p=.9                                ##coefficient on spatial lag of Y
B0=matrix(1, nrow=n, ncol=1)
B1=matrix(2, v , 1)
B2=matrix(.5, v, 1)
yrcoef=matrix(runif(1:(T), -20,20), (T), 1)     #Year Fixed Effects
yrcoef[1]=0
alpha=runif(n, -20,20)                      # Nation Fixed Effects

e=matrix(rnorm(n*T,0,sigma), n*T, 1)

inv=solve((diag(n)-p*W))
Y=matrix(data=NA, nrow=n*T, ncol=1)

for(jjj in 1:T){
invY= as.matrix(alpha) + X[((jjj-1)*n+1):(jjj*n),(3:(2+v))] %*% B1 +    yrcoef[jjj,1] + e[((jjj-1)*n+1):(jjj*n),1]
Y[((jjj-1)*n+1):(jjj*n),1]=inv %*% invY
}

Y=data.frame(Y)
X=data.frame(X)
DATA=cbind(X,Y)
names=c(""N"", ""T"", ""X1"", ""X2"", ""X3"", ""Y"")
colnames(DATA)=names

DATA=demean(DATA)

model1=as.formula(paste(names[length(names)], paste(names[-c(1:2,length(names))], collapse= "" + ""), sep="" ~ ""))

lw=mat2listw(W)

mod=spml(model1, data=DATA, listw=lw, model=""within"", effect=c(""twoways""),    lag=TRUE, spatial.error=""none"")

effects(mod)
</code></pre>

<p>However, the time period effects are non-zero (the spatial fixed effects are numerically zero). I can't seem to figure out why the spatial fixed effects is working while the time period effects portion is not.</p>

<p>Any thoughts?</p>
"
"0.11794753195637","0.11843311462705","136843","<p>I have a Cox proportional hazards model in R (see made-up example below) that models the effect of some variable, say weight. From this model, I'd like to extrapolate what a change in weight from say 90 to 60 would mean to survival, taking into account the fact that for such a change occurring at say age 40, certain amount of risk has already accumulated (and assuming weight change is instantaneous).</p>

<p>I've attached some code which involves</p>

<ol>
<li>fitting the Cox model (using age as the time scale);</li>
<li>extracting the predicted cumulative survival $S(t)$ using survfit for weight=90 and 60;</li>
<li>getting the cumulative hazard $H(t) = -\log(S(t))$;</li>
<li>getting the ""instantaneous"" hazard $h(t)$ via differencing $H(t)$ (plus small fudge factor to avoid zero hazard), which seems to do the job but probably a bit hacky;</li>
<li>adding a constant to the $\log(h(t))$ for all timepoints after the change, equivalent to the $\beta$ coefficient from the Cox regression times the <em>difference</em> in weights (90-60=30);</li>
<li>get the new survival functions $S^\prime(t)$ as $\exp(-{\rm cumsum}(\exp(\log(h^\prime(t)))))$.</li>
</ol>

<p>This procedure produces reasonable results (plotted as $1 - S(t)$), but is it correct or am I just lucky?</p>

<p><img src=""http://i.stack.imgur.com/ax5Bu.png"" alt=""enter image description here""></p>

<pre><code>library(survival)
set.seed(1)
rm(list=ls())

# Simulate some semi-realistic data
n      &lt;- 1e3
age    &lt;- round(runif(n, 1, 60))
weight &lt;- round(rnorm(n, 70, 10))
height &lt;- round(runif(n, 1.3, 1.9), 2)
sex    &lt;- sample(c(""M"", ""F""), length(age), replace=TRUE, prob=c(0.7, 0.3))
d.time &lt;- ceiling(rexp(n, weight / 1e4))
cens   &lt;- round(runif(n, 1, 60))
death  &lt;- d.time &lt;= cens
d.time &lt;- pmin(d.time, cens)
d      &lt;- data.frame(age=age, weight=weight, height=height, difftime=d.time, 
                     time=d.time + age, sex=sex, death=death)

s     &lt;- coxph(Surv(age, time, death) ~ height + weight, data=d)
d.new &lt;- data.frame(weight=c(60, 90), height=1.7)
sf    &lt;- survfit(s, d.new)

# The cumulative hazard function H is -log(S(t)) where S(t) is the survivor function
# (aka cumulative survival)
S &lt;- sf$surv[,2]

# Assume we start off with high weight
H &lt;- -log(S)

# The hazard is the derivative (here, finite difference) of the cumulative hazard H
# But the hazard can't be zero exactly as when we take log hazard, won't make sense
h &lt;- diff(c(0, H)) + 1e-6

# We introduce a changepoint in the hazard, but must make sure that the
# hazard does not become negative - this is naturally achieved because the
# Cox model is linear in the log-hazard. This means that the final survivor
# function will always be monotonically decreasing for any value of delta in 
# (-Inf, +Inf); delta &gt; 0 increases hazard, delta &lt; 0 decreases hazard
delta &lt;- coef(s)[""weight""] * (d.new$weight[1] - d.new$weight[2])
logh  &lt;- log(h)
age   &lt;- 40
logh[sf$time &gt; age] &lt;- logh[sf$time &gt; age] + delta
h     &lt;- exp(logh)

# Get the new cumulative hazard and new survivor functions
H &lt;- cumsum(h)
S &lt;- exp(-H)

# Compare original survivor function with modified one
plot(sf, lwd=5, col=1:2, conf.int=FALSE, mark=NA, fun=""event"",
     xlab=""Age"", ylab=""Cumulative risk"")
lines(c(0, sf$time), 1 - c(1, S), type=""s"", col=3, lwd=5)
abline(v=age, lty=2)
legend(x=""topleft"", legend=c(""Weight=60"", ""Weight=90"", ""Weight decreased 90 to 60""),
       col=1:3, lwd=5)
</code></pre>
"
"0.0858194351535935","0.0765979986161901","136925","<p>I have some short grouped time series data. I would like to fit a dynamic multilevel regression model in R, with random coefficients for the mean and first order auto-correlation in each group, and with no cross correlation between the two variance parameters; i.e. this model:</p>

<p>$y_{i,t} - \mu_{i} = \rho_{i} (y_{i,t-1} - \mu_{i}) + \epsilon_{i,t}$</p>

<p>$\mu_{i} = \mu + v_{\mu}$ </p>

<p>$\rho_{i} = \rho + v_{\rho}$ </p>

<p>$\epsilon_{i,t} \sim N(0,\sigma_\epsilon^2), v_{\mu} \sim N(0,\sigma_\mu^2), v_{\rho} \sim N(0,\sigma_\rho^2)$ </p>

<p>The best I can do so far in R is:</p>

<pre><code>library(nlme)
library(dplyr)

#create toy data set
df0 &lt;- Orthodont %&gt;% 
  group_by(Subject) %&gt;% 
  mutate(lag1=lag(distance)) %&gt;% 
  filter(!is.na(lag1))

#multilevel model, mean (not Subject specific mean) centered
m1 &lt;- lme(fixed = distance ~ I(lag1 - mean(distance)), data=df0, 
          random= list(Subject = pdDiag(~ + I(lag1 - mean(distance)))) )
m1
# Linear mixed-effects model fit by REML
#   Data: df0 
#   Log-restricted-likelihood: -164.8976
#   Fixed: distance ~ I(lag1 - mean(distance)) 
#              (Intercept) I(lag1 - mean(distance)) 
#               25.7907622                0.8289975 
# 
# Random effects:
#  Formula: ~+I(lag1 - mean(distance)) | Subject
#  Structure: Diagonal
#          (Intercept) I(lag1 - mean(distance)) Residual
# StdDev: 7.892818e-05                0.3031237 1.675277
# 
# Number of Observations: 81
# Number of Groups: 27 
</code></pre>

<p>This 1) does not estimate ($\mu$) and 2) centres the distance lag ($y_{i,t-1}$) on the population mean ($\mu$) rather than the subject ($i$) specific mean ($\mu_i$) that I desire. </p>
"
"0.0700712753800578","0.0703597544730292","136998","<p>I'm trying to classify a variable into either 0 or 1, using 50 factors, with a sample size of 2000. 25% of the dependent variables are 0 and the rest are 1.</p>

<p>Of these factors, 30 are categorical. I've been having a lot of trouble doing PCA in R with the pcr function from the pls library:</p>

<p>--It seems that pcr will only allow a qualitative variable to have two categories. For example, my dependent variable can come from 5 countries. The pcr function will only allow my ""country"" factor to have 2 values--if there are 3, it throws an error. (Also annoying, the factor can only have one character--it will allow ""A"" but throw an error for ""AFRICA"")</p>

<p>--If I use model.matrix to get dummy variables for my qualitative factors, pcr starts going insane when the number of variables goes over 50 or so. It'll go from CV=.2717 to CV=3e8 from 49 comps to 50 comps, for example.</p>

<p>I'm wondering if PCA in general is not meant for high-dimensional data, particularly with a lot of categorical factors? Or am I just formatting my data incorrectly?</p>

<p>Note: I have used glmnet with alpha = 0, 1, and .5 (ridge, lasso, and elastic net regression) and it worked perfectly using the model.matrix. I wanted to compare my results with PCA, but I do know that there are alternatives for feature selection.</p>
"
"0.0809113394062735","0.0812444463702388","137209","<p>When I run regression analysis I find it important to run some model diagnostics, such as detection of outliers, influential observations, multi-collinearity (much like these examples <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">http://www.statmethods.net/stats/rdiagnostics.html</a>).</p>

<p>Example of Diagnostics I use:</p>

<pre><code>#Assessing the Assumption of Independence, using Durbin Watson Test
dwt(lmModel)

#Controlling for Multicollinearity
vif(lmModel)
1/vif(lmModel)
mean(vif(lmModel))
</code></pre>

<p>I have a sample with a lot of missing data across most variables. Thus, I need to use multiple imputations. </p>

<p>However, model diagnostics seems to be impossible to explore when using multiple imputations. So far, I have used the mice package and since I am still a novice at R my multiple imputation script basically looks like this:</p>

<pre><code>#Imputes 5 datasets    
imp &lt;- mice(myData, m=5)    

#Runs regression analysis on each imputed dataset    
fit &lt;- with(imp, lm(A ~ B + C))   

#Pools the results
pooled &lt;- pool(fit)
summary(pooled)
</code></pre>

<p>Is there some way to use the diagnostic test on the pooled data? or do I have to use diagnostic tests on each imputed dataset (before being pooled)? or is there some other smart way of solving this issue?</p>

<p>Thanks for your time</p>
"
"0.0583927294833815","0.0703597544730292","137498","<p>I implemented both those tests with R, using the lmtest package.  Both tests directionally say the same thing (I think) with a very similar p-value of very close to 0.  But, are those tests saying that the underlying regression model's residuals are adequately linear.  Or are they saying just the opposite.  I know that the tests have slightly different nuances.  The Harvey-Collier test indicates whether the residuals are linear.  Meanwhile, the Rainbow test indicates whether the linear fit of the model is adequate even if some underlying relationships are not linear.  Any insight, on the interpretation of those results is greatly appreciated.   </p>

<p>I am posting the results of the tests below:</p>

<p>In R with lmtest package.</p>

<blockquote>
  <p>harvtest(Regression, order.by = NULL)</p>
</blockquote>

<pre><code>    Harvey-Collier test
</code></pre>

<p>data:  Regression
HC = 4.3826, df = 119, p-value = 2.543e-05</p>

<blockquote>
  <p>raintest(Regression, fraction = 0.5, order.by = NULL, center = NULL)</p>
</blockquote>

<pre><code>    Rainbow test
</code></pre>

<p>data:  Regression
Rain = 1.7475, df1 = 62, df2 = 58, p-value = 0.01664</p>
"
"0.0404556697031367","0.0406222231851194","137901","<p>I am using the package MatchIt in R to perform propensity score matching. I have chosen to use nearest neighbor matching with a caliper of 0.2 and since in my case i have more cases than controls i have to use the replacement=TRUE option, so that a control can be used more than once.</p>

<p>The graphical histogram check is satisfying and the stand.mean differences are all small with a max of 0.03 (btw any other suggestions for testing the matching?)
I want to use the matched dataset to check the treatment effect after all the matching(perform logistic regression with mortality as outcome and treatment as explanatory variable now) and i am wondering if i should take into consideration the weights that were resulted from the matching. Since i used the replacement option not all observations have a weight of 1 anymore. Shall i use this somehow or can i just perform an unweighted final logistic regression on the matched data to estimate the effect of treatment.</p>
"
"0.0404556697031367","0.0406222231851194","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"0.125262124650325","0.125777821045271","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.0809113394062735","0.0812444463702388","138847","<p>Working with a panel data set with a daily time series structure I was told to include a lagged dependent variable.
The dependent variable is daily electricity consumption of a medium size sample (>200) over a metering period of about 1 year.</p>

<p>In my existing pooled OLS model, outdoor temperature and some consumer specific variables like floor space or number of residents are the most important variables. $R^2$ is around 0.7.</p>

<p>Including yesterday's consumption yields an $R^2$ of about 0.9. However, I am not sure if including the lagged value is meaningful.
Today's consumption $y_{t}$ is highly correlated with yesterday's consumption $y_{t-1}$ but this is probably because $y_{t}$ and $y_{t-1}$ are influenced by the same variables (temperature, dwelling size,...). Thus, $y_{t-1}$ and the other independent variables are also correlated which violates some OLS assumptions (if I remember correctly...).
<strong>Is it useful and in accordance with OLS assumptions to include a lagged value in pooled OLS?</strong></p>

<p>Further, as I have never worked with lagged values before I really have some problems understanding its practical use. There is a <a href=""http://stats.stackexchange.com/questions/52458/inclusion-of-lagged-dependent-variable-in-regression"">similar question</a> in SE but still I don't understand...</p>

<p>When I want to model/predict consumption of day $t$, I will probably not have access to consumption of day $t-1$. <strong>So how do I use a model with lagged DVs in practice? Do I just use modelled values so that every $\hat{y}_{t}$ relies on a previously estimated value $\hat{y}_{t-1}$? If so, do I need some kind of starting value for the first day of the time series?</strong></p>

<p>Many thanks in advance for any help in understanding this!</p>
"
"0.107274293941992","0.10053487318375","139861","<p>I'm examining the effect of income (categorized into quintiles) on a response variable during different years (from 2003 to 2014). I adjust for some other covariates and have repeated measurements on the same individual.</p>

<p><strong>The problem:</strong> To obtain the effect of income on the response variable each year, I include an interaction term between year and income quintile. This gives me implausible results, since those who are rich tend to have higher adjusted values, which is unlikely. So I redid the analysis, fitting one separate regression model each year, and that gave me plausible results; i.e the rich had lower adjusted values.</p>

<p><em>Shouldn't the two methods yield somewhat similar results?</em></p>

<p>My calculations:</p>

<pre><code>    &gt; # START
    &gt; 
    &gt; # Converting the year variable into a factor
    &gt; data$year &lt;- as.factor(data$year)
    &gt; 
    &gt; # Fitting a model with random effects for person
&gt; require(lme4)
&gt; fit  &lt;- lmer(response ~ income*year + sex + age + education + biomarker + (1 | id), data=data)
&gt; # Using lsmeans to predict means (95% confidence intervals)
&gt; require(lsmeans)
&gt; results1 &lt;- lsmeans(fit, ~ income*year)
&gt; summary(results1)
 income     year   lsmean        SE       df lower.CL upper.CL
 Quintile 1 2003 64.95472 0.2826723 190216.2 64.40069 65.50875
 Quintile 2 2003 65.49504 0.2716893 189962.8 64.96254 66.02755
 Quintile 3 2003 65.39961 0.2713204 189648.3 64.86782 65.93139
 Quintile 4 2003 65.51872 0.2715941 189734.3 64.98640 66.05103
 Quintile 5 2003 65.47502 0.2744592 190425.5 64.93709 66.01296
 Quintile 1 2004 64.03440 0.2541402 191982.7 63.53630 64.53251
 Quintile 2 2004 65.04364 0.2472738 191720.0 64.55899 65.52829
 Quintile 3 2004 64.98069 0.2425866 191644.4 64.50523 65.45616
 Quintile 4 2004 65.14219 0.2459067 191477.1 64.66022 65.62416
 Quintile 5 2004 65.25515 0.2496510 191947.6 64.76584 65.74446
 Quintile 1 2005 63.62453 0.2345767 192988.5 63.16476 64.08429
 Quintile 2 2005 64.18179 0.2294655 192853.2 63.73205 64.63154
 Quintile 3 2005 64.00250 0.2308264 192458.4 63.55008 64.45491
 Quintile 4 2005 63.79620 0.2276547 192775.7 63.35000 64.24240
 Quintile 5 2005 64.70116 0.2344171 193212.2 64.24171 65.16062
 Quintile 1 2006 64.17463 0.2228096 193482.8 63.73793 64.61134
 Quintile 2 2006 64.37025 0.2158588 193425.3 63.94717 64.79333
 Quintile 3 2006 64.30762 0.2141169 193366.8 63.88795 64.72728
 Quintile 4 2006 64.38315 0.2168376 193329.6 63.95815 64.80814
 Quintile 5 2006 64.22019 0.2198772 193526.5 63.78923 64.65114
 Quintile 1 2007 63.84188 0.2185725 193602.5 63.41349 64.27028
 Quintile 2 2007 63.92346 0.2104634 193527.6 63.51096 64.33597
 Quintile 3 2007 63.67954 0.2094212 193511.7 63.26908 64.09000
 Quintile 4 2007 64.08718 0.2103912 193478.5 63.67482 64.49955
 Quintile 5 2007 64.03627 0.2113704 193649.9 63.62199 64.45055
 Quintile 1 2008 64.65587 0.2043770 193372.1 64.25529 65.05644
 Quintile 2 2008 64.30141 0.1957655 193499.2 63.91772 64.68511
 Quintile 3 2008 65.04454 0.1955247 193589.4 64.66131 65.42776
 Quintile 4 2008 64.94073 0.1947715 193488.8 64.55898 65.32247
 Quintile 5 2008 65.00096 0.1979129 193119.2 64.61305 65.38886
 Quintile 1 2009 64.74611 0.1941592 191979.5 64.36556 65.12666
 Quintile 2 2009 64.68663 0.1872505 192801.9 64.31962 65.05363
 Quintile 3 2009 64.89048 0.1858611 192919.0 64.52620 65.25476
 Quintile 4 2009 65.19469 0.1848586 192734.9 64.83238 65.55701
 Quintile 5 2009 65.18344 0.1896058 192025.8 64.81182 65.55506
 Quintile 1 2010 64.99407 0.1863001 188874.1 64.62893 65.35922
 Quintile 2 2010 65.14159 0.1758624 190272.3 64.79691 65.48628
 Quintile 3 2010 65.21003 0.1740553 190655.7 64.86889 65.55118
 Quintile 4 2010 65.65492 0.1731845 190157.8 65.31548 65.99435
 Quintile 5 2010 65.42802 0.1774762 189156.3 65.08017 65.77587
 Quintile 1 2011 65.77711 0.1807193 179035.8 65.42290 66.13131
 Quintile 2 2011 65.81373 0.1719076 186787.1 65.47679 66.15066
 Quintile 3 2011 66.25649 0.1692650 187174.9 65.92473 66.58824
 Quintile 4 2011 66.24046 0.1672252 185968.9 65.91271 66.56822
 Quintile 5 2011 66.31895 0.1717513 184631.5 65.98232 66.65558
 Quintile 1 2012 66.45412 0.1815487 178604.9 66.09829 66.80995
 Quintile 2 2012 66.39743 0.1691640 185466.1 66.06587 66.72899
 Quintile 3 2012 66.61760 0.1674829 185934.4 66.28934 66.94586
 Quintile 4 2012 66.69909 0.1672601 185774.9 66.37126 67.02692
 Quintile 5 2012 66.74211 0.1697808 183081.3 66.40934 67.07487
 Quintile 1 2013 66.20088 0.1804738 177511.5 65.84716 66.55461
 Quintile 2 2013 66.05285 0.1710873 185354.9 65.71752 66.38817
 Quintile 3 2013 65.57061 0.1667456 185103.9 65.24379 65.89742
 Quintile 4 2013 65.96563 0.1669031 184335.3 65.63851 66.29276
 Quintile 5 2013 66.19121 0.1723801 183746.1 65.85335 66.52907
 Quintile 1 2014 65.37137 0.3060358 191891.8 64.77155 65.97120
 Quintile 2 2014 66.37503 0.2882805 188873.8 65.81001 66.94006
 Quintile 3 2014 65.49851 0.2876551 188265.5 64.93471 66.06231
 Quintile 4 2014 66.08503 0.2867954 188729.4 65.52291 66.64714
 Quintile 5 2014 65.98435 0.2901786 189169.5 65.41561 66.55309

Results are averaged over the levels of: sex, education 
Confidence level used: 0.95 
&gt; # As you can see above, the richest (quintile 5) has higher values of a harmful biomarker, this is not likelybiomarker, this is not likely
    &gt; 
    &gt; # Redo the analysis by stratification, i.e one regression each year (I'll give 4 examples, years 2005, 2008, 2010 and 2014)
    &gt; fit2005  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2005"",])
        &gt; fit2008  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2008"",])
    &gt; fit2010  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2010"",])
        &gt; fit2014  &lt;- lmer(response ~ income + sex + age + education + biomarker + (1 | id), data=data[data$year==""2014"",])
    &gt; 
    &gt; lsmeans(fit2005, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 64.57677 0.3644627 7104.87 63.86232 65.29123
     Quintile 2 63.65426 0.3715929 7114.04 62.92582 64.38269
     Quintile 3 63.97948 0.3773103 7119.28 63.23984 64.71912
     Quintile 4 63.46368 0.3727073 7117.62 62.73306 64.19429
     Quintile 5 63.57556 0.3853513 7117.67 62.82016 64.33096

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2008, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 64.27986 0.3268165 9880.81 63.63924 64.92049
     Quintile 2 65.07827 0.3288624 9866.53 64.43363 65.72291
     Quintile 3 64.98781 0.3265577 9859.02 64.34769 65.62793
     Quintile 4 64.89630 0.3305190 9868.49 64.24842 65.54419
     Quintile 5 63.66509 0.3428045 9898.35 62.99313 64.33706

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2010, ""income"")
     income       lsmean        SE       df lower.CL upper.CL
     Quintile 1 65.39530 0.2961521 11897.22 64.81480 65.97581
     Quintile 2 65.61791 0.2911321 11892.26 65.04724 66.18858
     Quintile 3 65.48423 0.2947050 11892.60 64.90656 66.06190
     Quintile 4 65.14303 0.2914349 11925.62 64.57177 65.71429
     Quintile 5 63.89145 0.3030998 11935.15 63.29733 64.48558

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
    &gt; lsmeans(fit2014, ""income"")
     income       lsmean        SE      df lower.CL upper.CL
     Quintile 1 67.05015 0.4888236 4523.42 66.09182 68.00849
     Quintile 2 65.41100 0.4801968 4523.32 64.46958 66.35242
     Quintile 3 65.35658 0.4740396 4525.63 64.42723 66.28592
     Quintile 4 65.03556 0.4793119 4526.56 64.09587 65.97524
     Quintile 5 64.94690 0.5001898 4529.88 63.96628 65.92751

    Results are averaged over the levels of: sex, education 
    Confidence level used: 0.95 
</code></pre>

<p>I must have misunderstood something (important) here...</p>

<p>Any advice?</p>
"
"0.0330319159917525","0.0497518595104995","139928","<p>how to simulate data (in R) to generate , sample values
1) variables with specific correlation values for a particular model AND 
2) with predefined regression coefficients? 
3) Can we also set the mean and SD in the same process? 
4) Also how does one simulate the p value/significance of the variable. </p>

<p>This is for imitating existing models for analysis and teaching purposes</p>

<p>Sorry for not being specific : this is for multiple regression, sample values. I would like to specify the mean and SD if possible (apparently not, I can specify only one in order to specify the regression coefficients?)</p>

<p>Thanks for the help.</p>
"
"0.121978433679867","0.122480611322833","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.0948769553749019","0.0952675579132743","140622","<p>In the past I've run separate multiple regression models for <strong>many correlated independent variables</strong> and <strong>one dependent variable</strong>. For this I've been using the R package multtest (<a href=""http://www.bioconductor.org/packages/release/bioc/html/multtest.html"" rel=""nofollow"">http://www.bioconductor.org/packages/release/bioc/html/multtest.html</a>). This allowed me to compute adjusted p-values that took the correlation structure of my matrix of independent variables into account.</p>

<p>Now, I want to do the same thing but for <strong>several dependent variables</strong> as well. Put differently, I have a matrix Y with my dependent variables and a matrix X with my independent variables. From this I want to estimate <strong>X*Y regression models</strong>. Importantly, I want the <strong>adjusted p-values</strong> to account for the correlation structure of both the independent and the dependent variables. I'm looking forward to your suggestions. I've suggested to the authors of multtest before to extend their library to accomodate this case but this hasn't happened yet.</p>

<p><strong>Example</strong> (added):
Let's say I have gene expression data from 10 different tissues. Now I want to know if gene expression is correlated with a 100 different SNPs. This means I'm effectively testing 100*10 = 1000 hypothesis. However, all these hypothesis are not independent of each other. The SNPs might be correlated to each other due to linkage disequilibrium and gene expression might also be correlated accross different tissues, depending on their similarity. Therefore a Bonferroni correction of my p-values for this 1000 statistical tests would be too conservative. I'm looking for a way to <em>derive adjusted p-values that accounts for the above described dependencies within both the independent and the dependent variables</em>. </p>
"
"0.0960200924600074","0.103831970547663","140761","<p>I have a question regarding the use of propensity score in a survival analysis with use of mutliple imputation to handle missing data. The question is of theoretical nature and may well apply to other situations.</p>

<p>I have a data set of <em>n</em> individuals. The aim is to estimate the effect of a treatment on a binary outcome (death). The analysis is based on propensity score; the propensity score is derived by means of logistic regression, which includes 30 predictors variables. Effect estimation is carried out by means of Cox regression (which uses the propensity score in various ways [stratification, covariate adjustments etc]). There are a large number of patients, and on average 2â€“7% missing for each variable (of which there are 30 included in the prop. score).</p>

<p>Thus, I have a large data set with a substantial amount of missing data (at least in terms of complete cases) which is why I use multiple imputation - 5 complete data sets are imputed. Now the question is what to do with the muliply imputed data sets; which one of the strategies below should I prefer?</p>

<p><strong>1.</strong> Calculate one average propensity score for each individual using the 5 separate data sets. That way, each individual will have one propensity score, which is the average from the n complete data sets. Then do the Cox regression..</p>

<p><strong>2.</strong> Analyze each separate multiply imputed data set (with Cox regression), and then pool the 5 hazard ratio estimates to one hazard ratio.</p>

<p>The second method appears to be used more often, but is it better/worse?</p>

<p>Any thoughts about this?</p>
"
"0.06396603026469","0.0642293744423385","141255","<p>I am trying to determine the best coding system for my categorical variables to use in a regression with categorical and continuous variables. I have been using this page as a resource but none of the coding methods seem to match what I want to do: <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm</a></p>

<p>I have reason to expect that there could be differences among all four levels of a categorical variable and therefore would like to code it to perform all pairwise comparisons (1vs2, 1vs3, 1vs4, 2vs3, 2vs4, 3vs4,). I thought that a Sum (deviation) method would accomplish this as it says it compares each level to the grand mean. But in my model I only see terms for levels 1, 2, 3, and do not have a term for level 4 of the variable. Is this level incorporated into the intercept? If so is there another coding system I should use to get at the comparisons I am looking for?</p>
"
"0.0540611626362958","0.0759972207238908","141422","<p>I'm trying to estimate the value of a real estate upon its characteristics. To do so, I'm using the <code>Hedonic Model</code> and I'm doing the regression using <code>lmRob (R package: robust)</code>.</p>

<p>There is a problem with the model and I don't know how to solve it.</p>

<p>If the real estate for which I want to evaluate the price, has more characteristics than the real estates used for the model creation, the evaluation wouldn't be complete.</p>

<p><strong>Example:</strong></p>

<p>Let's say that I want to evaluate a house which has 4 rooms, 190 mÂ², an inside pool, 3 balconies and a sauna.</p>

<p>None of the other house from the neighborhood (houses which will be used for the regression) have an inside pool, the rest of the characteristics are similar.</p>

<p>This means that the house with the inside pool must have a bigger price than the other, but the regression will not take into account that the house has a inside pool.</p>

<p>My question is: how can I find the coefficient of the inside pool, in order to use it for the evaluation.</p>

<p>Thank you! </p>
"
"0.0572129567690623","0.0574484989621426","141583","<p>I am working on a few (both simple and multivariable) regression analyses, and I have cases where the residuals are non-normal, to varying degrees. As I've understood, the Gauss-Markov theorem states that normality of residuals is not necessary for the coefficient point estimates to be correct, i.e., I can trust that the regression summary tells me the BLUE coefficient estimates. However, the standard errors may be biased, and thus, the corresponding t- and p-values may not be correct.</p>

<p>A previous question (<a href=""http://stats.stackexchange.com/questions/83012/how-to-obtain-p-values-of-coefficients-from-bootstrap-regression"">How to obtain p-values of coefficients from bootstrap regression?</a>) asked if it was possible to calculate p-values from bootstrapped coefficients and their CIs. However, if I bootstrap the coefficients and use the bootstrapped mean and the bootstrapped SE to re-calculate t- and p-values, would that be a sound approach?</p>

<p>Here is the code I use, in R:</p>

<pre><code># create linear model
mod &lt;- lm(Y~X,data=dataset); summary(mod)

# create function to return coefficient
mod.bootstrap &lt;- function(data, indices) {    
d &lt;- data[indices, ]
mod &lt;- lm(Y~X, data=d)  
return(coef(mod))
}

# set seed
set.seed(1234)

# begin bootstrap using the boot package
mod.boot &lt;- boot(data=dataset, statistic=mod.bootstrap, R=2000)

# now here is how I re-calculate t- and p-values
bootmean &lt;- mean(mod.boot$t[,2])
booter &lt;- sd(boot.t[,2])
tval &lt;- bootmean/booter
p &lt;- 2*pt(-abs(tval),df=mod$df.residual)
</code></pre>

<p>The new t- and p-values come out fairly close to the LM summary, albeit a bit more conservative, as expected. Is this something I could report? I am very new at this, so I can't really tell if my logic is valid or not.</p>

<p>Edit: Clarified a bit.</p>
"
"0.0404556697031367","0.0406222231851194","141719","<p>I am using the package <code>caret</code> and GBM method for my predictions.</p>

<pre><code>fitControl &lt;- trainControl(## 10-fold CV
        method = ""repeatedcv"",
        number = 10,
        ## repeated ten times
        repeats = 10)

gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1)

gbmFit &lt;- train(target ~ ., data = traindf,
                method = ""gbm"",
                trControl = fitControl,
                verbose = FALSE,
                ## Now specify the exact models 
                ## to evaludate:
                tuneGrid = gbmGrid,
                metric = ""ROC"")
</code></pre>

<p>There is one concept that I misunderstand. User guides of <code>caret</code> say that ""<strong>By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models).</strong>"" So, when I run <code>ggplot(gbmFit)</code> I get this graphic:</p>

<p><img src=""http://i.stack.imgur.com/mETe8.png"" alt=""enter image description here""></p>

<p>When I type <code>gbmFit</code> in the console, I see that ""<strong>The final values used for the model were n.trees = 300, interaction.depth = 9 and shrinkage = 0.1</strong>""  How can I manually change these settings in order to make my predictions with different number of boosting iterations and trees?:</p>

<pre><code>predictions_gbm &lt;- predict(gbmFit, newdata = testdf, type = ""raw"")
</code></pre>
"
"0.118129972176712","0.118616305942459","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.11794753195637","0.111466460825459","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"NaN","NaN","142248","<p>When Performing a linear regression in <code>r</code> I came across the following terms. </p>

<pre><code> NBA_test =read.csv(""NBA_test.csv"")
 PointsPredictions  = predict(PointsReg4, newdata =  NBA_test)
 SSE = sum((PointsPredictions - NBA_test$PTS)^2)
     SST = sum((mean(NBA$PTS) - NBA_test$PTS) ^ 2)
 R2 = 1- SSE/SST
</code></pre>

<p>In this case I am predicting the number of points. I understood what is meant by SSE(sum of squared errors), but what actually is SST and R square? Also what is the difference between R2 and RMSE?</p>
"
"0.0286064783845312","0.0287242494810713","142312","<p>I'm having an interesting dilemma with the <code>neuralnet</code> and <code>nnet</code> packages in <code>R</code>.  I recently tried a series of feed-forward neural networks giving each the same data sets and every single time, no matter how I tweak the algorithms, hidden layers, neuron sizes, maximum iterations or error thresholds, both functions keep converging their predictions to approximately the mean of whatever they are training on.</p>

<p>A linear regression does way better for each series in terms of fit, and both of these packages seem to do a better job fitting random data from the <code>rnorm</code> function than real data.  In regards to the mathematics of the problem, what could be causing this and how should I resolve?  I have sample code below and can paste a sample dataset below if requested.  Thanks!</p>

<pre><code>model6 &lt;- neuralnet(
    target ~ 1 + majorholiday + mon + sat + sun + thu + tue + wed + tickets + l1_target + l7_target, data = data_nn
    ,algorithm = ""rprop+"", hidden = c(8), stepmax = 500000
    ,err.fct = ""sse"", threshold = 0.01, lifesign = ""full"", lifesign.step = 100
    , linear.output= T)
</code></pre>

<p><strong>EDIT</strong></p>

<p>A user requested I paste some data.  Here is one set below and I just tried the same code again prior to uploading and the same thing happens, converges to the mean of <code>target</code> at about 17.45</p>

<pre><code>    row.names   target  majorholiday    mon sat sun thu tue wed backtickets l1_target   l7_target
1   8   18.976573088    0   0   0   0   0   0   0   13806   18.114001584    36.521334684
2   9   20.701716096    0   1   0   0   0   0   0   15308   18.976573088    35.477867979
3   10  25.014573616    0   0   1   0   0   0   0   13439   20.701716096    28.173601042
4   11  15.706877377    1   0   0   0   0   0   0   11283   25.014573616    27.602288128
5   12  19.633596721    0   0   0   0   1   0   0   12272   15.706877377    13.801144064
6   13  20.049395337    0   0   0   0   0   1   0   9528    19.633596721    32.777717152
7   14  21.720178282    0   0   0   1   0   0   0   13747   20.049395337    18.114001584
8   15  23.390961226    0   0   0   0   0   0   0   15277   21.720178282    18.976573088
9   16  16.707829447    0   1   0   0   0   0   0   16058   23.390961226    20.701716096
10  17  15.872437975    0   0   1   0   0   0   0   14218   16.707829447    25.014573616
11  18  23.295531996    1   0   0   0   0   0   0   11249   15.872437975    15.706877377
12  19  22.363710716    0   0   0   0   1   0   0   13993   23.295531996    19.633596721
13  20  24.227353276    0   0   0   0   0   1   0   13402   22.363710716    20.049395337
14  21  20.500068156    0   0   0   1   0   0   0   14244   24.227353276    21.720178282
15  22  26.090995836    0   0   0   0   0   0   0   14502   20.500068156    23.390961226
16  23  18.636425597    0   1   0   0   0   0   0   16296   26.090995836    16.707829447
17  24  15.840961757    0   0   1   0   0   0   0   13694   18.636425597    15.872437975
18  25  20.650050308    1   0   0   0   0   0   0   10774   15.840961757    23.295531996
19  26  13.467424114    0   0   0   0   1   0   0   12348   20.650050308    22.363710716
20  27  19.752222033    0   0   0   0   0   1   0   12936   13.467424114    24.227353276
21  28  27.832676502    0   0   0   1   0   0   0   14342   19.752222033    20.500068156
22  29  18.854393759    0   0   0   0   0   0   0   14390   27.832676502    26.090995836
23  30  10.773939291    0   1   0   0   0   0   0   16724   18.854393759    18.636425597
24  31  12.569595839    0   0   1   0   0   0   0   14091   10.773939291    15.840961757
25  32  28.153882107    1   0   0   0   0   0   0   11250   12.569595839    20.650050308
26  33  24.400031160    0   0   0   0   1   0   0   12803   28.153882107    13.467424114
27  34  21.584642949    0   0   0   0   0   1   0   13318   24.400031160    19.752222033
28  35  27.215419370    0   0   0   1   0   0   0   14193   21.584642949    27.832676502
29  36  21.584642949    0   0   0   0   0   0   0   14312   27.215419370    18.854393759
30  37  15.015403791    0   1   0   0   0   0   0   16445   21.584642949    10.773939291
31  38  26.276956633    0   0   1   0   0   0   0   13753   15.015403791    12.569595839
32  39  15.139500902    1   0   0   0   0   0   0   11619   26.276956633    28.153882107
33  40  12.467824272    0   0   0   0   1   0   0   14006   15.139500902    24.400031160
34  41  21.373413039    0   0   0   0   0   1   0   14098   12.467824272    21.584642949
35  42  8.015029889 0   0   0   1   0   0   0   14462   21.373413039    27.215419370
36  43  16.030059779    0   0   0   0   0   0   0   15367   8.015029889 21.584642949
37  44  19.592295285    0   1   0   0   0   0   0   17868   16.030059779    15.015403791
38  45  18.701736409    0   0   1   0   0   0   0   15052   19.592295285    26.276956633
39  46  16.002499062    1   0   0   0   0   0   0   10035   18.701736409    15.139500902
40  47  16.943822536    0   0   0   0   1   0   0   13708   16.002499062    12.467824272
41  48  11.295881691    0   0   0   0   0   1   0   13463   16.943822536    21.373413039
42  49  19.767792959    0   0   0   1   0   0   0   13998   11.295881691    8.015029889
43  50  19.767792959    0   0   0   0   0   0   0   14745   19.767792959    16.030059779
44  51  16.943822536    0   1   0   0   0   0   0   16156   19.767792959    19.592295285
45  52  14.119852113    0   0   1   0   0   0   0   13552   16.943822536    18.701736409
46  53  22.869570079    1   0   0   0   0   0   0   11554   14.119852113    16.002499062
47  54  10.481886286    0   0   0   0   1   0   0   13437   22.869570079    16.943822536
48  55  19.057975066    0   0   0   0   0   1   0   14076   10.481886286    11.295881691
49  56  20.010873819    0   0   0   1   0   0   0   14567   19.057975066    19.767792959
50  57  9.528987533 0   0   0   0   0   0   0   14277   20.010873819    19.767792959
51  58  21.916671326    0   1   0   0   0   0   0   16545   9.528987533 16.943822536
52  59  11.000000000    1   0   0   0   0   0   1   15599   21.916671326    14.119852113
53  60  17.000000000    0   0   0   0   1   0   1   17463   11.000000000    22.869570079
54  61  10.000000000    0   0   0   0   0   1   1   17935   17.000000000    10.481886286
55  62  20.000000000    0   0   0   1   0   0   1   18357   10.000000000    19.057975066
56  63  19.000000000    0   0   0   0   0   0   1   19246   20.000000000    20.010873819
57  64  17.000000000    0   1   0   0   0   0   1   21234   19.000000000    9.528987533
58  65  11.000000000    0   0   1   0   0   0   1   18493   17.000000000    21.916671326
59  66  9.000000000 1   0   0   0   0   0   1   15315   11.000000000    11.000000000
60  67  22.000000000    0   0   0   0   1   0   1   17841   9.000000000 17.000000000
61  68  9.000000000 0   0   0   0   0   1   1   18312   22.000000000    10.000000000
62  69  11.000000000    0   0   0   1   0   0   1   17880   9.000000000 20.000000000
63  70  5.000000000 0   0   0   0   0   0   1   19371   11.000000000    19.000000000
64  71  15.000000000    0   1   0   0   0   0   1   21696   5.000000000 17.000000000
65  72  12.000000000    0   0   1   0   0   0   1   18829   15.000000000    11.000000000
66  73  10.000000000    1   0   0   0   0   0   1   14749   12.000000000    9.000000000
67  74  15.000000000    0   0   0   0   1   0   1   17928   10.000000000    22.000000000
68  75  7.000000000 0   0   0   0   0   1   1   18254   15.000000000    9.000000000
</code></pre>
"
"0.06396603026469","0.0642293744423385","142337","<p>Basically I need to replicate Hartley's '<a href=""http://www.econ.ucdavis.edu/faculty/kdsalyer/LECTURES/Ecn235a/Linearization/ugfinal.pdf"" rel=""nofollow"">A User's Guide to Solving Real Business Cycle Models</a>' . Specifically (to make question relevant to stats.stackexchange), I want to simulate the dynamical system implied by the model which is specified as follows:</p>

<p><img src=""http://i.stack.imgur.com/2LuOx.png"" alt=""enter image description here""></p>

<p>where $c$ is consumption, $h$ is labour supply, $k$ is capital, $z$ is the autoregressive technological process, $y$ is the output and $i$ is investment. The important point is that these represent percentage deviations from steady state (and so are growth rates) and shocks are initiated through $z_t$ - autoregressive process.</p>

<p>I simulate it using the following logic: say at time $t$, everything is at steady state and all the values are 0 (there are no deviations from steady state), from which we have $k_{t+1}$. Then, at $t+1$ by giving a shock to the system through $\varepsilon$ (which is assumed to be normally distributed with mean 0 and sd 0.007), i solve for $c_{t+1}$ and $h_{t+1}$ (as I have the 'shocked' $z_{t+1}$ and previously obtained $k_{t+1}$. Then, I plug those two to retrieve the rest, namely - $y_{t+1}, i_{t+1}, k_{t+2}$ and repeat the process.</p>

<p>Unfortunately, I get an explosive process which doesn't make sense (the series should be stationary as implied by the economic theory):</p>

<p><img src=""http://i.stack.imgur.com/weeDr.png"" alt=""enter image description here""></p>

<p>I also include R code that is used to simulate this:</p>

<pre><code>n&lt;-300

data.simulated &lt;- data.table(t = 0, zval = 0, cval = 0, hval = 0, kval = 0, yval = 0, ival = 0)
data.simulated &lt;- rbind(data.simulated, data.table(t = 1, kval = 0), fill = TRUE)

for (ii in 1:n){

  ##initial shocks
  eps &lt;- rnorm(1, mean = 0, sd = 0.007)
  zt1 &lt;- data.simulated[t == ii - 1, zval]*0.95 + eps
  kt1 &lt;- data.simulated[t == ii, kval]

  ##solve for ct, ht
  lmat &lt;- matrix(c(1, -0.54, 2.78, 1), byrow = T, ncol = 2)
  rmat &lt;- matrix(c(0.02 * kt1 + 0.44 * zt1, kt1 + 2.78 * zt1), ncol = 1)

  solution &lt;- solve(lmat, rmat)
  ct1 &lt;- solution[1, ]
  ht1 &lt;- solution[2, ]

  ##now solve for yt1 and kt2 and it1
  yt1 &lt;- zt1 + 0.36 * kt1 + 0.64 * ht1
  kt2 &lt;- -0.07 * ct1 + 1.01 * kt1 + 0.06 * ht1 + 0.1 * zt1
  it1 &lt;- 3.92 * yt1 - 2.92 * ct1

  ##add to the data.table the results
  data.simulated[t == ii, c(""zval"", ""cval"", ""hval"", ""yval"", ""ival"") := list(zt1, ct1, ht1, yt1, it1)]
  data.simulated &lt;- rbind(data.simulated, data.table(t = ii + 1, kval = kt2), fill = TRUE)
}


a &lt;- data.simulated[, list(t, cval, ival, yval)]
a &lt;- data.table:::melt.data.table(a, id.vars = ""t"")
ggplot(data = a, aes(x = t, y = value, col = variable)) + geom_line()
</code></pre>

<p>Sy my question is simple - is the system that is specified by the paper is inherently unstable? I'm not sure how could check it analytically, so hopefully someone could help.</p>
"
"0.164402831052504","0.160224382974551","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.066748449563906","0.0670232487891664","142693","<p><strong><em>Is the following a reasonable illustration of the OVB problem?</em></strong></p>

<p>We build up fictional data around the regression line:</p>

<p>$$y = 7.2 + 2.3 \, x_1 + 0.1 \, x_2 + 1.5 \, x_3 + 0.013 \, x_4 + eps$$</p>

<p>by using this function:</p>

<pre><code>correlatedValue = function(x, r){
  r2 = r**2
  ve = 1 - r2
  SD = sqrt(ve)
  e  = rnorm(length(x), mean = 0, sd = SD)
  y  = r * x + e
}
</code></pre>

<p>-thank you, @gung for this post:
<a href=""http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of"">How to generate correlated random numbers (given means, variances and degree of correlation)?</a></p>

<p>And the following function, which generates four variables (<strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong> and <strong><em>x4</em></strong>) as well as noise (<strong><em>eps</em></strong>). <strong><em>x1</em></strong> and <strong><em>x3</em></strong> are sample from normal distributions; <strong><em>x2</em></strong> is extracted from a uniform; and <strong><em>x4</em></strong> from a Poisson.</p>

<pre><code>variables &lt;- function(){
x &lt;- rnorm(1000)
x1 &lt;- 50 + 15 * x
x3 &lt;- 28 + 11 * correlatedValue(x = x, r = 0.6)
x2 &lt;- runif(1000, 0, 100)
x4 &lt;- rpois(1000,50)
eps &lt;- rnorm(1000,5, 7)
y = 7.2 + 2.3 * x1 + 0.001 * x2 + 1.5 * x3 + 0.013 * x4 + eps
dat &lt;- as.data.frame(cbind(y, x1, x2, x3, x4))
c &lt;- as.numeric(coef(lm(y ~ x2 + x3 + x4, dat))[3])
d &lt;- as.numeric(coef(lm(y ~ x1 + x2 + x3 + x4, dat))[4])
c(c,d)
}
</code></pre>

<p><strong><em>x1</em></strong> and <strong><em>x3</em></strong> are highly influential on <strong><em>y</em></strong> and are correlated with each other, setting the values up to observe <strong><em>OVB</em></strong>. <strong><em>x2</em></strong> and <strong><em>x4</em></strong> are less influential.</p>

<p>Here is the plotting of <strong><em>y</em></strong> against <strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong>  and <strong><em>x4</em></strong>, and <strong><em>x1</em></strong> over <strong><em>x3</em></strong> with added regression lines:</p>

<p><img src=""http://i.stack.imgur.com/I4u0S.png"" alt=""enter image description here""></p>

<p>And following is the variance-covariance matrix:</p>

<pre><code>             y           x1           x2         x3          x4
y   1.00000000  0.944410945  0.014421682 0.77571067 -0.01463981
x1  0.94441094  1.000000000 -0.001726526 0.56504020 -0.03562991
x2  0.01442168 -0.001726526  1.000000000 0.03537959  0.02253922
x3  0.77571067  0.565040198  0.035379590 1.00000000  0.02573827
x4 -0.01463981 -0.035629906  0.022539218 0.02573827  1.00000000
</code></pre>

<p>Predictably, the regression including all variables shows similar coefficients to the initial equation:</p>

<pre><code>coef(lm(y~.,dat))[2:5]
         x1          x2          x3          x4 
2.253353226 0.004899445 1.547915198 0.017710038 
</code></pre>

<p>Wrapping up, a quick simulation is carried out to obtain the mean of the <strong><em>x3</em></strong> coefficient in 1,000 simulations <em>WITHOUT</em> including <strong><em>x1</em></strong> (""coef_x3"") and then <em>WITH</em> <strong><em>x1</em></strong> (""coef_x3_full""):</p>

<pre><code>coef_x3 &lt;- NULL
coef_x3_full &lt;- NULL
for (i in 1:1000){
  coef_x3[i] = variables()[1]
  coef_x3_full[i] = variables()[2]
}
mean(coef_x3)
mean(coef_x3_full)
</code></pre>

<p>obtaining a coefficient for <strong><em>x3</em></strong> of <strong>3.383</strong> when <strong><em>x1</em></strong> is excluded versus a coefficient for <strong><em>x3</em></strong> of <strong>1.502</strong> when included. So when <strong><em>x1</em></strong> is included we have an unbiased estimation of the true <strong><em>x3</em></strong> coefficient (<strong><em>1.5</em></strong>), whereas the estimation is biased when we exclude <strong><em>x1</em></strong>.</p>
"
"0.0872741054526671","0.0876334075296498","144096","<p>I have data from gene expression arrays and I have clinical data associated with the samples used. I am using gene expression (discrete), age at diagnosis (discrete) and ethnicity (categorical) to build a regression model. I'd like to predict gene expression by age at diagnosis and ethnicity so the model would be something like:</p>

<p>$$y = b_0 + b_1x_1 + b_2x_2 + \epsilon$$</p>

<p>in R it looks something like:</p>

<pre><code>y &lt;- as.numeric(gene_expression_myFavoriteGenes)
age &lt;- age_diagnosis 
ethnicity &lt;- ethnicity 
l &lt;- lm(y ~ age + ethnicity)
</code></pre>

<p>now...when I look at the coefficients I get something like:</p>

<pre><code>Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -0.442785   0.151175  -2.929 0.008299 ** 
age_diagnosis    -0.005616   0.002422  -2.319 0.031090 *  
Caucasian        -0.910633   0.115870  -7.859 1.53e-07 ***
Hispanic         -0.801088   0.125429  -6.387 3.13e-06 ***
Honduran         -0.682405   0.210694  -3.239 0.004114 ** 
Peurto Rican     -0.679251   0.209620  -3.240 0.004100 ** 
South Asian      -1.237134   0.213569  -5.793 1.14e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1815 on 20 degrees of freedom
Multiple R-squared:  0.7795,    Adjusted R-squared:  0.7023 
F-statistic:  10.1 on 7 and 20 DF,  p-value: 2.21e-05
</code></pre>

<p>I'm a lil confused on how to interpret the data.
I first built the model without correcting for ethnicity, nothing was significant. 
then I added ethnicity, checked with <code>anova(fist_lm, second_lm)</code> and the difference is significant.</p>

<p>so now I have all ethnicity associated with a significant p-value and and adjusted R^2 of 0.7 ...so, my questions, sorry if they sound silly, are:</p>

<ul>
<li>in this specific case, can age + ANY ethnicity significantly predict expression of myFavoriteGene?</li>
<li>models with other genes only have one or two significant ethnicity, does that mean that those are the only ones that can reliably predict expression of myFavoriteGene?</li>
</ul>

<p>------- edit ---------</p>

<p>here an example of how my data look:</p>

<pre><code>&gt; data[1:10,1:6]
           skin.AA_2  skin.AA_3  skin.AA_4  skin.AA_5  skin.AA_6   skin.AA_7
100_g_at   5.5526731  4.7001569  5.3724104  5.3700587  5.7571421  5.76974711
1000_at    7.7757596  5.4761710  7.3896019  5.5514442  8.2559761  7.14107706
1001_at    1.4554496  0.3315354  1.3311387  1.4979105  2.0579317  1.50734217
1002_f_at -0.4427732 -0.7381431 -0.3714425 -0.3159300 -0.2270056  0.31245288
1003_s_at  1.7908548  1.2590320  1.4839795  1.6727171  1.8550568  1.99870500
1004_at    1.8082815  0.9940647  1.4085169  1.8658939  1.9275267  2.25192977
1005_at    3.1792907 10.2456153  6.1170771  9.9058017  8.3695269  5.02225258
1006_at   -0.3059731 -0.8761517 -0.7151807 -0.4620902 -0.5923052  0.02495093
1007_s_at  9.9911387 10.2839949 10.1105075  9.9944011 10.3866696 10.31211765
1008_f_at  7.9190579  4.5957139  4.0043624  4.6297893  4.2067368  7.62499810

&gt; clinicalData[1:10,1:5]  
patient_ID        ethnicity age_diagnosis age colname_data_matrix_SB
1      AA1005 African American            39  47              skin.AA_1
2      AA1007        Caucasian            32  50              skin.AA_3
3      AA1008        Caucasian            50  50              skin.AA_4
4      AA1009        Caucasian            18  68              skin.AA_5
5      AA1010        Caucasian            31  40              skin.AA_6
6      AA1015        Caucasian            41  43                       
7      AA3001         Hispanic            49  49              skin.AA_7
8      AA3006         Hispanic            48  51              skin.AA_8
9      AA3007         Hispanic            51  51              skin.AA_9
10     AA3008         Hispanic            49  50             skin.AA_10
</code></pre>
"
"0.0756856276908142","0.0759972207238908","144223","<p>I was looking at the mtcars dataset and exploring the relationship between MPG and the transmission modes (auto/manual). I decided to use the following linear models with the regressors specified in the below R code:</p>

<pre><code>&gt; data(mtcars)
&gt; fit &lt;- lm(mpg ~ I(wt - mean(wt)) + I(qsec - mean(qsec)) + factor(am), data = mtcars)
&gt; round(summary(fit)$coeff, 4)
                     Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)           18.8979     0.7194 26.2707   0.0000
I(wt - mean(wt))      -3.9165     0.7112 -5.5069   0.0000
I(qsec - mean(qsec))   1.2259     0.2887  4.2467   0.0002
factor(am)1            2.9358     1.4109  2.0808   0.0467
</code></pre>

<p>From the above, the P-value of the slope coefficient ""factor(am)1"" is lower than 0.05 and hence we'd reject the null hypothesis and infer that cars of manual transmission has higher MPG value than those of manual transmission.</p>

<p>However, I also tried to explore the equivalent linear model without intercept terms, as per R code below:</p>

<pre><code>&gt; fit2 &lt;- lm(mpg ~ I(wt - mean(wt)) + I(qsec - mean(qsec)) + factor(am)-1, data=mtcars)
&gt; round(summary(fit2)$coeff, 4)
                     Estimate Std. Error t value Pr(&gt;|t|)
I(wt - mean(wt))      -3.9165     0.7112 -5.5069    0e+00
I(qsec - mean(qsec))   1.2259     0.2887  4.2467    2e-04
factor(am)0           18.8979     0.7194 26.2707    0e+00
factor(am)1           21.8338     0.9438 23.1344    0e+00
&gt; confint(fit2)
                          2.5 %    97.5 %
I(wt - mean(wt))     -5.3733342 -2.459673
I(qsec - mean(qsec))  0.6345732  1.817199
factor(am)0          17.4244109 20.371471
factor(am)1          19.9005360 23.767021
</code></pre>

<p>From the 95% Confidence Interval constructed, a car (of auto transmission) with average wt and qsec has a MPG interval [17.4244, 20.3714], while a car (of manual transmission) with average wt and qsec has a MPG interval [19.9005. 23.7670].</p>

<p>The two Confidence Intervals overlap, and we failed to reject the null hypothesis where statistically there's no difference between the MPG performance of cars (with auto transmission) and MPG performance of cars (with manual transmission).</p>

<p>I used two equivalent linear models and they gave me different conclusions. Could you enlighten me on what I may have missed here?</p>
"
"0.10340625341847","0.111248539872496","144348","<p><strong>Note</strong><br>
I've edited the example to be more intuitive and closer to my real data</p>

<p><strong>Intro</strong><br>
I've got data on customers purchases and with it am trying to predict which customers are more likely to make next purchase at some time in the future. Data consist of customers' features like sex, age etc., and their prior purchase behavior like total spendings and number of orders, one row for every customer. The last two columns are the indicator of wether he have made next purchase or not, and number of days till purchase or till today, in case of no purchase.  </p>

<p><strong>Problem</strong><br>
I am building a Cox regression and then want to predict probability of next purchase for individual observations in, say, 30 days from last purchase.</p>

<p>Reproducible example:</p>

<pre><code>library(survival)
library(rms)
library(pec)
library(ggplot2)

data(cost)

# split into train and test sets
set.seed(1)
ind &lt;- sample(1:nrow(cost), 100)
test.set &lt;- cost[ind, ]
train.set &lt;- cost[-ind, ]
</code></pre>

<p>For Cox regression I use <code>cph</code> from <code>rms</code> package, for prediction - <code>predictSurvProb</code> from <code>pec</code> package as suggested in <a href=""http://stats.stackexchange.com/a/36016/72401"">this</a> discussion.</p>

<pre><code># fit Cox model
fit &lt;- cph(Surv(time, status) ~ ., data = train.set, surv = TRUE)

# predict pobability of event in 30 days
test.set$predicted.probs &lt;- 1 - predictSurvProb(fit, newdata = test.set, times = 1000)[, 1]
</code></pre>

<p>Thus, for every customer we have his probability of making a purchase in 1000 units of time. I want to validate prediction against real data.  </p>

<p><strong>Now to the question:</strong> what is the best/valid way to do it?  </p>

<p>Here's what I've tried:<br>
I expect that valid model would predict higher probabilities for customers who made their purchase earlier so correlation between probabilities and number of days to event' would be negative and strong (e.g. for customer who actualy made next purchase in 2 days, probability of buying in 30 days would be very high).</p>

<pre><code>with(test.set, cor(predicted.probs, time))
# [1] -0.5221604
</code></pre>

<p>Also, probability for those who made purchase (status = 1) would be higher than for those who didn't.</p>

<pre><code>with(test.set, by(predicted.probs, status, mean))
# status: 0
# [1] 0.2371247
# --------------
#   status: 1
# [1] 0.4083586
</code></pre>

<p>And a graph to eyeball my assumptions:</p>

<pre><code>qplot(data = test.set, x = time, y = predicted.probs, color = time)
</code></pre>

<p>Am I correct in my reasoning?</p>
"
"0.038379618158814","0.0642293744423385","144609","<p>I am examining BMI effect on wage. I have included a lot of explanatory variables in the regression and all (most) coefficients were significant, but once I add ability which correlates with education (already in regression) slightly (34%) BMI measures turn insignificant. </p>

<p>Here is the question, what does it mean if BMI turns insignificant once we add another variable in the regression? </p>

<p>Estimated by OLS, cross sectional. Adj R  0.60, F=160. </p>

<p>PS: I added a variable and variable of interest turned insignificant, not vice versa. 
Thank you for all the examples below, they address the same topic, but different issue. </p>
"
"0.0404556697031367","0.0406222231851194","144650","<p>Numerous books and lecture slides start to discuss regression analysis as follows:<br>
$$Y=X\beta+\epsilon;\text{ where, } Y\sim N(X\beta, \sigma^2 ) \text{ and } \epsilon\sim N(0, \sigma^2)$$
George Seber wrote in his book (Linear Regression Analysis, Ed 2) at page 42:
$$Var[Y] = Var[Y-X\beta] = Var[\epsilon]$$
I was intended to verify it as follows:</p>

<pre><code>set.seed(123)
n=10000
int=rep(1,n)
x1=rnorm(n,5,3)
x2=rnorm(n, 20, 10)
x3=rnorm(n, -10, 2)
x=cbind(int,x1,x2, x3)
beta=c(10, 2, 0.5, 1.5)
err=rnorm(n, 0, 7)
y=x%*%beta+err
mean(err) # very close to zero
var(err) # very close to 49
mean(y) # very close to 15
var(y) # very close to 118
</code></pre>

<p>Somehow I triggered to check whether <code>R square</code> have a role in equating $Var[Y]$ and $Var[\epsilon]$. Then I checked:</p>

<pre><code>var(y)*(1-(summary(lm(y~x1+x2+x3))$r.squared)) # very close to 49
</code></pre>

<p>Yes it is. So $Var[\epsilon] = Var[Y]\times (1-R^2)$. I do not know what I am missing here! Why George Seber and many others never mention it? Certainly, I am wrong, not George Seber. But what is my mistake?<br>
Any lights on my confusion will be appreciated.<br>
Thanks.</p>
"
"0.0286064783845312","0.0287242494810713","144925","<p>There are responses of my school's alumni on the skills they perceive they acquired during the studies and are required from their job now. 18 skills evaluated from 1 to 5 (ordinal values, right?). Also they are 18 classes of graduation (around 30 responses from each year). There are also such attributes of the respondents as a master's degree, current status, so on. There is no regression model I could construct up to this moment, but I'm at least interested if the difference in averages of the skills between different groups are significant. I try R over and over. I managed only to tabulate the means for the groups. But then I'm lost.</p>

<ul>
<li>Are there tests I should run the data through first?</li>
<li>How to know what distribution my data follows?</li>
<li>And what are the ways to see if the means differ significantly across the groups?</li>
<li>Is it possible without a regression model at all?</li>
</ul>
"
"0.051172824211752","0.0642293744423385","145455","<p>So I want to compute a regression using R. The problem is, that I want to compute the regression with log transformed variables. Here is what I am trying to do:</p>

<pre><code>reg1 &lt;- lm(y~x+z+u+log(p))
</code></pre>

<p>Now since you cant take the log of 0, the following message pops up: </p>

<pre><code>Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  NA/NaN/Inf in 'x' 
</code></pre>

<p>How can I compute a regression even though there are NaNs (in my case they are produced only for the first few observations of the variable p)? Many thanks</p>

<pre><code>Date        y        x      z       u      p
25.06.2009  0.582   1.145   0.603   26.36   0
26.06.2009  0.604   1.12    0.61    25.93   0
29.06.2009  0.647   1.108   0.647   25.35   0
30.06.2009  0.597   1.099   0.669   26.35   0
01.07.2009  0.604   1.085   0.633   26.22   0
02.07.2009  0.54    1.072   0.63    27.95   0
06.07.2009  0.543   1.048   0.57    29      0
07.07.2009  0.512   1.044   0.567   30.85   0
08.07.2009  0.496   1.029   0.533   31.3    0
09.07.2009  0.487   1.018   0.515   29.78   23
10.07.2009  0.482   1.007   0.504   29.02   66
13.07.2009  0.473   0.996   0.503   26.31   162
14.07.2009  0.471   0.985   0.503   25.02   235
15.07.2009  0.472   0.979   0.492   25.89   585
16.07.2009  0.441   0.969   0.486   25.42   668
17.07.2009  0.431   0.954   0.461   24.34   1080
20.07.2009  0.438   0.944   0.451   24.4    1883
21.07.2009  0.435   0.937   0.451   23.87   2398
</code></pre>

<p><strong>EDIT</strong>:I think I formulated my question unclear. So if the vector p has a zero, it means that on this date nothing happend. So I think it is no problem to exclude the obeservations from p which contain a zero from the regression. But still, how can I tell R it should not include those observations? (I think I could also replace the 0 with NA since nothing happend in the variable p till 09.07.2009) I tried <code>nan.action=nan.exclude</code> but this doesnt work...</p>
"
"0.0904616275314925","0.090834052439095","145657","<p>Basically I'm attempting to recreate the results of an example from class in R. What I'm trying to do is decide whether it's best to use a single regression line for an entire data set or two lines based on a categorical variable. The teacher indicates there are three steps to this:</p>

<ol>
<li>Determine if two different lines are required</li>
<li>If yes, determine if they differ in slope</li>
<li>If yes, determine if they differ in intercept</li>
</ol>

<p>Here is my data:</p>

<pre><code>&gt; example
   Predictor Response Group
1         21       11     A
2         24       21     A
3         26       23     A
4         29       29     A
5         35       34     A
6         45       51     A
7         51       59     A
8         68       73     A
9         72       83     A
10        76       95     A
11        17       11     B
12        21       55     B
13        26       34     B
14        28       44     B
15        32       26     B
16        36       34     B
17        40       15     B
18        45       21     B
19        51       16     B
20        68       21     B
</code></pre>

<p>I've realized that if I add the interaction and group terms to the model:</p>

<pre><code>ex_mod &lt;- lm(Response ~ Predictor,data = example)
ex_mod2 &lt;- lm(Response ~ Predictor + Group + Predictor:Group,data = example)
</code></pre>

<p>And then perform ANOVA on this. I get the right answer for step 1:</p>

<pre><code>&gt; anova(ex_mod,ex_mod2)
Analysis of Variance Table

Model 1: Response ~ Predictor
Model 2: Response ~ Predictor + Group + Predictor:Group
  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    
1     18 6616.4                                 
2     16 1583.8  2    5032.6 25.42 1.078e-05 ***   
</code></pre>

<p>Which means I need different lines, but now I need to know if they differ in slope or y-intercept or both. And here is where I'm stuck. I cant seem to get the right answer (F = 293.17 for slope, and F = 170.77 for intercept). </p>

<p>The teacher indicates that the next steps are: 1) to generate RSS in which the slope is fixed,but the y-intercepts are allowed to vary; and 2) generate RSS in which the y-intercept is fixed, but the slopes are allowed to vary.</p>

<p>I apologize if the question is confusing or simplistic, but I dont know how to proceed from here.</p>

<p>Thanks</p>
"
"0.10703564115707","0.107476300250388","145684","<p>My problem (question at the end) is to calculate confidence interval (CI) (NOT prediction interval) of the response of a nonlinear model.</p>

<p>I am working with R but this question is not R-specific.</p>

<p>I want to model some data after the following equation (model):</p>

<p>Y ~ a * X^b/b</p>

<p>First, I estimate the parameters a and b through nonlinear regression (using R's ""nls()""), which yields estimates and error on the corresponding estimate.</p>

<pre><code> Nonlinear regression model
 model: Y ~ (A * X^B/B)
  data: data.frame(X = X, Y = Y)
    A      B 
  7.4154 0.6041 
   residual sum-of-squares: 88983
</code></pre>

<p>Then I calculate 95% CI for a and b (using confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1)))</p>

<pre><code> &gt; confint(nlm)
 Waiting for profiling to be done...
         2.5%     97.5%
 A 1.21719414 11.549562
 B 0.08583486  1.482389
</code></pre>

<p>In order to calculate 95%CI for Y, given some fixed, certain value of X, my first idea was to propagate uncertainties on a and b to Y through the model equation. This yields some value for 95% CI of Y, given X.</p>

<p>I then came accross the ""propagate"" package that proposes to calculate 95%CI of Y, given X, ""based on asymptotic normality"" (citation from ""<a href=""http://127.0.0.1:22638/library/propagate/html/predictNLS.html"" rel=""nofollow"">http://127.0.0.1:22638/library/propagate/html/predictNLS.html</a>""). However this method yields a VERY different 95%CI. </p>

<p><strong>My question is: Why aren't these two CI equal ?</strong></p>

<p>A worked example (with some random equation that just crossed my mind):</p>

<p>Values needed for error propagation : A, CI(A), B, CI(B), X, CI(X) :</p>

<p>Parameters (A &amp; B)' estimates and 95%CI were calculated from 
     confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1))</p>

<p>X was then fixed at 30 for the sake of the argument, and considered error-free.</p>

<pre><code>                A         B  X
 value   7.415380 0.6041404 30
 95% CI  5.166184 0.6982769  0
</code></pre>

<p>The general formula for uncertainties propagation (works for sd, se, ci95%) is :</p>

<p>Y=f(Ai | i = 1 to n)
=> delta(Y) = sqrt( sum( ( dY/dAi * delta(Ai) )^2 ) )</p>

<p>The equation being        Y =  A * X^B/B </p>

<p>Partial derivatives are then: </p>

<pre><code> dF/dA  =  X^B/B
 dF/dB  =  A * (X^B * log(X))/B - A * X^B/B^2
 dF/dX  =  A * (X^(B - 1) * B)/B
</code></pre>

<p>Then</p>

<pre><code> dF/dA = 12.9196498927581
 dF/dB = 167.269472901412
 dF/dX = 1.92930443474376
</code></pre>

<p>This yields</p>

<pre><code> Y = 95.8041099173585 +- 134.526084150286
</code></pre>

<p>However, when using the predictNLS() function from ""propagate"" R package:</p>

<pre><code> predictNLS(nlm, newdata=data.frame(X=30), interval = ""confidence"")$summary

 Propagating predictor value #1 ...
   Prop.Mean.1 Prop.Mean.2 Prop.sd.1 Prop.sd.2 Prop.2.5%
      95.80411    102.8339  20.89399  24.86949  51.89104
   Prop.97.5% Sim.Mean   Sim.sd Sim.Median  Sim.MAD  Sim.2.5%
     153.7767  93.5643 1712.894   97.85209 21.98703 -117.3541
   Sim.97.5%
    210.3916
</code></pre>

<p>Which yields</p>

<pre><code>    Y = 95.80411 +- (153.7767-51.89104)/2
 =&gt; Y = 95.80411 +- 50.94283
</code></pre>

<p>Obviously I must have missed / misunderstood some essential information about CI of response variable, because I believe the person who coded the predictNLS() function must be way more knowledgeable than me about it.</p>

<p>Thanks in advance for your explanations.</p>
"
"0.06396603026469","0.0513834995538708","145790","<p>I'm trying to figure out how to produce an ANOVA Table in R for a multiple regression model. So far I can only produce it for each regressor, and the Mean Square is calculating as the same as Sum Of Squares.</p>

<pre><code>&gt; anova(nflwin.lm)
Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
pass_yard     1  76.193  76.193  26.172 3.100e-05 ***
percent_rush  1 139.501 139.501  47.918 3.698e-07 ***
oppo_rush     1  41.400  41.400  14.221 0.0009378 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I'm trying to produce something like</p>

<pre><code>Analysis of Variance Table

Response: wins
             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
Model         3  76.193  76.193  26.172 3.100e-05 ***
Residuals    24  69.870   2.911                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"NaN","NaN","145836","<p><img src=""http://i.stack.imgur.com/QPcxY.png"" alt=""enter image description here""></p>

<p>I am doing a project on cloud cover and cosmic rays and have undertaken a regression model in R. Above is the regression diagnostic plot and from the QQ plot I can see that the tails are skewed, meaning it isn't a normal distribution. How significantly will this affect my results? Below is the results, showing no relationship. Would I even expect data from the natural world to follow a normal distribution?</p>

<p>Here is the output for this particular model:</p>

<pre><code>Call:
lm(formula = magadanlc ~ magadancr, data = lc)      Output: 1
Residuals:
    Min      1Q  Median      3Q     Max 
-26.038 -11.044  -1.454  10.202  39.652 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 15.049904  12.494996   1.204    0.229
magadancr    0.002291   0.001492   1.536    0.126

Residual standard error: 13.19 on 314 degrees of freedom
  (2 observations deleted due to missingness)
Multiple R-squared:  0.007456,  Adjusted R-squared:  0.004295 
F-statistic: 2.359 on 1 and 314 DF,  p-value: 0.1256
</code></pre>
"
"0.0404556697031367","0.0406222231851194","145977","<p>I am working on Boston data set from MASS library. I separated the training and test data (70 / 30)</p>

<p>In order to train my data, should I run linear regression multiple times on training data? Is this what training a dataset means? I'm using <code>lm</code> function in R to do this. Here is my code:</p>

<pre><code>smp_size &lt;- floor(.7 * nrow(Boston))

set.seed(133)
train_boston &lt;- sample(seq_len(nrow(Boston)), size=smp_size)
train_ind    &lt;- sample(seq_len(nrow(Boston)), size=smp_size)
train_boston &lt;- Boston[train_ind, ]
test_boston  &lt;- Boston[-train_ind,]
nrow(train_boston)   # [1] 354
nrow(test_boston)    # [1] 152
train_boston.lm &lt;- lm(lstat~medv, train_boston)
summary(train_boston.lm)
</code></pre>
"
"0.0904616275314925","0.090834052439095","146046","<p>I'm investigating whether there is a relationship between the day of the week and an outcome value using linear regression in <code>R</code>, and would like to understand how to interpret the residual plots.</p>

<p><strong>Data</strong></p>

<p>Example dummy data (the mean and SD are based on actual data I have): </p>

<pre><code>set.seed(14)
mon &lt;- data.frame(id=seq(6, 60*7, by=7), value = rnorm(60, 4372, 145))
tue &lt;- data.frame(id=seq(7, 60*7, by=7), value = rnorm(60, 4433, 206))
wed &lt;- data.frame(id=seq(1, 60*7, by=7), value = rnorm(60, 4671, 143))
thu &lt;- data.frame(id=seq(2, 60*7, by=7), value = rnorm(60, 4555, 154))
fri &lt;- data.frame(id=seq(3, 60*7, by=7), value = rnorm(60, 4268, 149))
sat &lt;- data.frame(id=seq(4, 60*7, by=7), value = rnorm(60, 1579, 110))
sun &lt;- data.frame(id=seq(5, 60*7, by=7), value = rnorm(60, 1136, 68))
startdate &lt;- seq.Date(as.Date(""2014-01-01""), by=""day"", length.out=(60*7) )
id &lt;- seq(1, 60*7)
wd &lt;- weekdays(startdate)
df &lt;- data.frame(id, startdate, wd)
days &lt;- rbind(mon, tue, wed, thu, fri, sat, sun)
df &lt;- merge(df, days)

head(df)
  id  startdate        wd    value
1  1 2014-01-01 Wednesday 4593.117
2  2 2014-01-02  Thursday 4686.159
3  3 2014-01-03    Friday 4352.982
4  4 2014-01-04  Saturday 1825.172
5  5 2014-01-05    Sunday 1206.759
6  6 2014-01-06    Monday 4276.032
</code></pre>

<p>which looks like</p>

<pre><code>library(ggplot2)
ggplot(data=df, aes(x=startdate, y=value, colour=wd)) +
  geom_point() +
  geom_smooth( alpha=.3, size=1, aes(fill=wd)) +
  facet_wrap(~wd) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/lX6CG.png"" alt=""data plot""></p>

<p><strong>Model</strong></p>

<p>Modelling the data using <code>fit &lt;- lm(data=df, value ~ wd)</code> produces the coefficients:</p>

<pre><code>summary(fit)
....
Coefficients:
             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  4242.60      18.83  225.319  &lt; 2e-16 ***
wdMonday      148.93      26.63    5.593 4.07e-08 ***
wdSaturday  -2661.93      26.63  -99.965  &lt; 2e-16 ***
wdSunday    -3113.78      26.63 -116.933  &lt; 2e-16 ***
wdThursday    299.65      26.63   11.253  &lt; 2e-16 ***
wdTuesday     189.04      26.63    7.099 5.51e-12 ***
wdWednesday   412.52      26.63   15.492  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 145.9 on 413 degrees of freedom
Multiple R-squared:  0.9896,    Adjusted R-squared:  0.9894 
F-statistic:  6539 on 6 and 413 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The plot of the data, and the coefficients seem to suggests there is a relationship between day of the week and the outcome value. </p>

<p>However, I know I also need to consider the residual plots when interpreting the validity of a model. For this example the residual plots are:</p>

<pre><code>par(mfrow=c(2,2))
plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gzpGZ.png"" alt=""residual plots""></p>

<p><strong>Question</strong></p>

<p>Through various stats courses/uni/research (e.g. <a href=""http://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions"">this question</a>) I know that for a good linear model you are looking for unbiased homoscedastic residuals. But my knowledge on this subject is a bit rusty. Therefore, do my residuals suggest a linear model is not an appropriate fit for the data? And/or is there another aspect to this that I should be considering, or have I completely missed the point all together?</p>
"
"0.0700712753800578","0.0703597544730292","147033","<p>I am trying to calculate standard errors of group means for a two-way-anova. I found two ways to do this (<code>predict.lm(, se = T)</code> and <code>summary.lm()</code>:</p>

<pre><code>set.seed(42234)
exmpl &lt;- data.frame(DV = rnorm(40) + rep(3:6 * 10, each = 10), #  Dependent Variable
                    IV1 = factor(rep(LETTERS[1:2], each = 20)), # Independent Variable (Treatment) 1 
                    IV2 = factor(rep(rep(LETTERS[3:4], each = 10), 2))) #  Independent Variable (Treatment) 2

exmpl.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl) #  Example data was generated without interactions

summary(exmpl.lm)
as.data.frame(predict(exmpl.lm, data.frame(IV1 = c('A', 'B', 'A', 'B'),
                                           IV2 = c('C', 'C', 'D', 'D')), se = T))
</code></pre>

<p>The standard errors of some group means differ. I managed to recalculate the standard errors given by <code>predict()</code> with the explanations from <a href=""http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf"" rel=""nofollow"">Practical Regression and Anova using R</a> from Faraway (section 3.5). I couldn't find any information about the algorithms used by the <code>summary()</code> function. Any ideas?
 What I find really confusing is that you can change the output by relabelling one factor:</p>

<pre><code>exmpl2 &lt;- exmpl
exmpl2$IV1 &lt;- factor(exmpl2$IV1, levels = LETTERS[2:1])

exmpl2.lm &lt;- lm(DV ~ IV1 + IV2, data = exmpl2)
summary(exmpl2.lm)
summary(exmpl.lm)
</code></pre>

<p>In the first example group A-C has a standard error of 0.2013. In the second example the standard error is given as 0.2324. The data of both examples is the same, only the order of the labels of a categorial (not ordinal) variable were changed. How does this influence the statistical model?</p>
"
"0.0707974219804893","0.0710888905739589","147619","<p>I'm trying to build a model that can predict streamflow for an alpine (snowmelt-fed) watershed using snow albedo (roughly, the energy reflectance of the snow) data. Albedo controls the melt of the snowpack, and higher albedo means slower melt, and vice versa. I have daily time-series data for both the snow albedo and streamflow, for 12 years from 2002-2013. The albedo time-series was obtained by spatially-averaging albedo data (raster files) from NASA's MODIS satellite.</p>

<p>I have tried various methods (simple regression, GLMs, GAMs, decision trees and random forests) to build the flow prediction model, but all of them fail because of the autocorrelated relationship between albedo and flow. Since the albedo is a snowpack property, there is a lag between it and the flow (related to snowmelt).</p>

<p>The Cross correlation function (CCF) between albedo and flow is shown below:</p>

<p><a href=""http://imgur.com/PpW1Kpy"" rel=""nofollow""><img src=""http://i.imgur.com/PpW1Kpy.png"" title=""source: imgur.com"" /></a></p>

<p>I have tried to include albedo lags of various days into the models, but I'm not able to mimic the distributed lag relationship between albedo and flow. I have tried to add precipitation, temperature and other climatic data to the predictors, but they don't seem to help. There are similar lagged and cross-correlation problems between these other predictors and flow.</p>

<p>The albedo, flow, precipitation and air temperature time-series are shown below:</p>

<p><a href=""http://imgur.com/Kb8Ta6q"" rel=""nofollow""><img src=""http://i.imgur.com/Kb8Ta6q.png"" title=""source: imgur.com"" /></a></p>

<p>Is there a statistical or machine learning technique in R that I can explore to build the albedo-streamflow model?</p>

<p>Thank you.</p>
"
"0.0495478739876288","0.0497518595104995","149004","<p>I'm trying to fit a Bayesian hierarchical poisson regression. To do so, I'm using MCMChpoisson function from MCMCpack in R. Based on this package, the model is:</p>

<p>$$Y_i \sim Poisson(\lambda_i)$$
$$\phi(\lambda_i) = X_i\beta + W_i \beta_i + \epsilon_i$$
$$\epsilon_i \sim N(0, \sigma^2 I_{k_i})$$
$$ \dots $$</p>

<p>In the model above, $\phi$ is the link function.</p>

<p>I skipped the rest of the model as only the parts of above are related to my question. My question is why they consider an measurement error ($\epsilon_i$) in the systematic component whereas in GLM we have a function of the mean; in other words, sampling from poisson will itself generate a measurement error. </p>

<p>Also, I think the extra $\epsilon_i$ term above causes me to get strange results. Does anyone know any other function/package in R to fit a model very similar to the model above with no measurement error in the systematic component.</p>

<p>Thanks very much for your help,</p>
"
"0.0904616275314925","0.090834052439095","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.0908377689773195","0.0995037190209989","149064","<p>I have a nominal categorical predictor and a continuous dependent variable..I want to perform linear regression using lm in R. If the contrasts are such that the resulting dummy variables are uncorrelated then the regression is merely the direct linear combination of dummy variables weighted by their respective coefficients obtained from regression  of continuous variable with individual dummy variable..To have this advantage what way should the categorical predictor be contrast coded?I found this method <a href=""http://www.psychstat.missouristate.edu/multibook/mlt08m.html"" rel=""nofollow"">here</a> ..
It is helpful but the only problem is the order seems to be important here..The relation between only adjacent categories can  be interpreted from the result of linear regression..</p>

<p>So my question is - for nominal categorical predictor is there anyway to get good insights about dependent variable at category level of the predictor  from regression analysis.</p>

<p><strong>edit</strong> :</p>

<p>I'd like to  provide some clarifications here</p>

<p>Why do i need uncorrelated dummies? </p>

<p>bcoz in case of uncorrelated dummies i need not worry about which dummy enters the regression model first. The p value for the dummy1 is different when it enters the model second when compared to that when it enters first..By 'enters the model' i mean stepwise linear regression..So to avoid that problems i want them to be uncorrelated.</p>

<p>But if you see the pain vs treatment regression from the link provided by me the order certainly matters while doing contrast coding..I have no prior knowledge of the categories of my nominal category variable..so i cant order them like in pain vs treatment case. For more details - my dependent variable is Sales and category variable is product category which has 15 categories.</p>
"
"0.0495478739876288","0.0497518595104995","152203","<p>I've found <a href=""http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/linearregression/linearregression.R"" rel=""nofollow"">this line</a> of code to calculate predicted values from a ridge.lm model:</p>

<pre><code># Predict is not implemented so we need to do it ourselves
y.pred.ridge = scale(data.test[,1:8],center = F, scale = m.ridge$scales)%*% m.ridge$coef[,which.min(m.ridge$GCV)] + m.ridge$ym
</code></pre>

<p>Why center is set to FALSE? And why do I need to add the mean of $Y$ to the predicted values?</p>

<p>I thought $X$ values should be scaled before running a ridge regression, which implies the out of samples predictors should be centered and scaled?
And scaling the predictors doesn't imply centering the outcomes of the regression, so why to add the mean of $Y$ to the predictions?</p>
"
"0.06396603026469","0.0513834995538708","152394","<p>I am using R's flexsurvreg function (in the flexsurv package) to fit a AFT model to my data. </p>

<p>This is the line of code that fits the model to the data:</p>

<pre><code>TestModel &lt;- flexsurvreg(Surv(time,death) ~ param1 + param2 + param3 + param4 + param5 + param6 + param7 + param8 + param9 + param10 + param11 + param12 + param13, data = DataTest, dist = ""weibull"")  
</code></pre>

<p>Once the model fits, this is a summary of the results:</p>

<pre><code>Estimates: 
        data mean     est        L95%       U95%       se         exp(est)   L95%       U95%     
shape      NA         9.99e-01         NA         NA         NA         NA         NA         NA
scale      NA         2.20e+02         NA         NA         NA         NA         NA         NA
param1     1.32e-01   2.51e-01         NA         NA         NA   1.29e+00         NA         NA
param2     1.61e-01  -1.54e-02         NA         NA         NA   9.85e-01         NA         NA
param3     1.89e-01  -4.68e-02         NA         NA         NA   9.54e-01         NA         NA
param4     1.76e-01  -2.25e-02         NA         NA         NA   9.78e-01         NA         NA
param5     1.87e-01  -5.35e-02         NA         NA         NA   9.48e-01         NA         NA
param6     7.56e-01  -2.74e-01         NA         NA         NA   7.60e-01         NA         NA
param7     2.28e-01   3.23e-02         NA         NA         NA   1.03e+00         NA         NA
param8     1.58e-01  -1.69e-02         NA         NA         NA   9.83e-01         NA         NA
param9     4.32e-01  -1.89e-02         NA         NA         NA   9.81e-01         NA         NA
param10    1.30e+02  -1.01e-03         NA         NA         NA   9.99e-01         NA         NA
param11    2.26e+01  -4.08e-03         NA         NA         NA   9.96e-01         NA         NA
param12    5.54e+02  -2.84e-04         NA         NA         NA   1.00e+00         NA         NA
param13    9.57e+01  -4.69e-03         NA         NA         NA   9.95e-01         NA         NA

N = 40320,  Events: 32154,  Censored: 8166
Total time at risk: 2584693
Log-likelihood = -171611.5, df = 15
AIC = 343253.1
</code></pre>

<p>I want to measure how the covariates affect the survival time. The estimates provide an understanding of this. Also, as I read <a href=""http://stats.stackexchange.com/questions/6026/how-do-i-interpret-expb-in-cox-regression"">here</a>, $exp(est)$ provides an estimate of how the hazard changes with change in 1 unit of a covariate by keeping the other covariates fixed. Is there a way I can calculate p-values for these covariates?</p>

<p>I have fitted a Weibull distribution to my dataset.</p>
"
"0.0572129567690623","0.0574484989621426","152474","<p>I have a continuous outcome variable and several different <code>lasso</code> models to predict the outcome. Something like</p>

<pre><code>outcome ~ explain1 + explain2 + confounders
outcome ~ explain3 + explain4 + confounders
outcome ~ explain1 + explain2 + explain3 + explain4 + confounders
</code></pre>

<p>I know that I can calculate several goodness of fit measures like <a href=""http://en.wikipedia.org/wiki/PRESS_statistic"" rel=""nofollow"" title=""PRESS"">PRESS</a> or just the <a href=""http://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow"" title=""mean square error"">mean square error</a>. And models with lower PRESS/MSE predict the outcome more accurately. But how can I quantify this difference, ideally with a p-value, which indicates if one model is significantly better than the other?</p>

<p>For ""normal"" regression models I could use anova, but this doesn't seem to work for lasso models (at least in <code>R</code>).</p>
"
"0.0572129567690623","0.043086374221607","153113","<p>How can I calculate adjusted means for a regression model with fixed and random effects? I'd like to calculate the adjusted means for a lme regression with this formula</p>

<pre><code>mymodel &lt;- myDV ~ experiment_condition + (1|subject_aptitude) + (1|subjects_teacher/subjects_class) 
</code></pre>

<p>where myDV is the dependent variable, experiment_condition is an independent fixed effect and subject_aptitude (participants past class average) and subjects_teacher/subjects_class (classroom nested within teacher) are random effects</p>

<p>The ultimate goal here is to visualize this data with adjusted means because the raw means (before the random effects variance is removed) do not accurately depict the results of the LMER </p>
"
"0.0858194351535935","0.0861727484432139","153122","<p>As we know, if we are doing many tests or multiple comparison, we don't use the same $\alpha$ value and use some $\alpha$ correction methods like Bonferroni. This is done because when we do multiple tests, we have higher chance of getting something as significant compared to doing for fewer numbers of tests. </p>

<p>But my main question is this: </p>

<p>1) It is said if you are comparing multiple sample means using ANOVA and once you find there is some significant difference then you can do a <em>post hoc</em> analysis by doing pairwise comparison. But now you don't have to actually do a Bonferroni correction. Why is that? Isn't this <em>post hoc</em> analysis same as other pairwise t test where we use Bonferroni correction?</p>

<p>2) If Bonferroni correction is required because more tests leads to more chances of getting something significant then why we don't use the same thing, where we are doing something like regression where we are testing significance of $\beta$ estimates, or whether a variable is significant or not for feature selection using p value/F score? In that case also we are doing multiple comparison in checking whether each variable is significant or not. Then why don't we use Bonferroni correction on critical $\alpha$ there?</p>

<p>Please advise.</p>
"
"0.0700712753800578","0.0703597544730292","153510","<p>I am trying to fit a regularized logistic regression to my data using glmnet. Using $\alpha=1$ I get a LASSO-regression, which is what I want. My problem is though that I don't know how the intercept is fitted. In glmnet one has the option to put <code>Intercept=TRUE</code> or <code>Intercept=FALSE</code>. As far as I understand <code>FALSE</code> sets my intercept to 0. When <code>TRUE</code>, I understood that the intercept was fitted as the mean of the $y$-values. Since my data is balanced binary data with values 0 and 1, $\bar{y}=0.5$, but my analysis gives me the value -2.6. </p>

<p>I read <a href=""http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet"">How is the intercept computed in GLMnet? </a> but I don't understand it, so I hope someone will give some details. Also, in the link's article there is a likelihood function (13) and (14) on page 8 and I don't understand why it has $1/N$ in front.  </p>
"
"0.0858194351535935","0.0861727484432139","153761","<p>I don't have a lot of experience working with time series data. Now I have a 3 year, monthly data for several entities (you can think about them as different stores), that I would like to do some analysis, e.g. regression. I am not sure if there are trend and seasonality effects on these series.
Using the package <code>Forecast</code> in <code>R</code>, and applying the function <code>stl</code>, I decomposed the series and plotted them. I have attached some of the resulting plots (for different stores):</p>

<p><img src=""http://i.stack.imgur.com/np21E.jpg"" alt=""trend 2""></p>

<p><img src=""http://i.stack.imgur.com/ISKHJ.jpg"" alt=""trend 3""></p>

<p><img src=""http://i.stack.imgur.com/Rsxhw.jpg"" alt=""trend 4""></p>

<p>When I print the result of the fitted <code>stl</code> function, I get something like this: </p>

<pre><code> Call:
 stl(x = dlr1, s.window = ""period"")

 Components
            seasonal      trend   remainder
 Jan 2010 -0.05643233 -0.2151193 -0.02526416
 Feb 2010 -0.14799311 -0.2193160  0.13137861
 Mar 2010  0.10125889 -0.2235127  0.13747509
 Apr 2010 -0.29720645 -0.2266819 -0.47611165
 May 2010 -0.22746429 -0.2298511  0.28988090
 Jun 2010  0.12403035 -0.2320100  0.05470502
 Jul 2010 -0.10418880 -0.2341688  0.15684340
 Aug 2010  0.14560622 -0.2358294 -0.25225647
 Sep 2010  0.16699531 -0.2374901 -0.35570221
 Oct 2010 -0.21709617 -0.2402783  0.13772671
 Nov 2010  0.20363225 -0.2430665  0.19027750
 Dec 2010  0.30885826 -0.2444804 -0.11424289
 Jan 2011 -0.05643233 -0.2458944  0.16533482
 Feb 2011 -0.14799311 -0.2329029  0.09169095
 Mar 2011  0.10125889 -0.2199115 -0.33798701
 Apr 2011 -0.29720645 -0.2111018  0.58659147
 May 2011 -0.22746429 -0.2022921 -0.56724011
 Jun 2011  0.12403035 -0.2105492 -0.38534202
 Jul 2011 -0.10418880 -0.2188064  0.45324407
 Aug 2011  0.14560622 -0.2282670  0.15884344
 Sep 2011  0.16699531 -0.2377275  0.07834284
 Oct 2011 -0.21709617 -0.2372438  0.38658989
 Nov 2011  0.20363225 -0.2367601 -0.38545840
 Dec 2011  0.30885826 -0.2406277 -0.02293191
 Jan 2012 -0.05643233 -0.2444953 -0.14810135
 Feb 2012 -0.14799311 -0.2603740 -0.23092833
 Mar 2012  0.10125889 -0.2762527  0.19282561
 Apr 2012 -0.29720645 -0.2778357 -0.11752792
 May 2012 -0.22746429 -0.2794186  0.27095247
 Jun 2012  0.12403035 -0.2747109  0.32488093
 Jul 2012 -0.10418880 -0.2700031 -0.61519386
 Aug 2012  0.14560622 -0.2649596  0.08891071
 Sep 2012  0.16699531 -0.2599160  0.27346079
 Oct 2012 -0.21709617 -0.2550556 -0.52784826
 Nov 2012  0.20363225 -0.2501951  0.19201780
 Dec 2012  0.30885826 -0.2451385  0.13415736
</code></pre>

<p>Now based on these results, I am not sure how to decide whether there is a strong/weak trend and seasonality, and whether I should remove the trend and seasonality effects in order to build my regression model?
In another words I would like to know how to interpret the plots and the resulting trend and seasonality numbers, e.g. what does it mean in Dec 2012 when it says ""seasonal"" = 0.30885826, etc.?</p>

<p>Thanks</p>
"
"0.10703564115707","0.107476300250388","153846","<p>I am familiar with linear regression models, but I am in the process of learning about linear mixed effects models.</p>

<p>My data consists of measurements for each month for a set of subjects over a long period of time (~15 years). The subjects and time frames are partially crossed - subjects do not appear for each time point. I also have a number of covariates measured at the per date per subject level, and a single boolean variable indicating a whether a <code>count</code> is before or after a particular time point. The point of this particular model is to measure whether or not a particular event (occurring at the ""mid date"") had an effect on the <code>count</code> variable. Due to the partially crossed, longitudinal nature of my data and the general discontinuity of my data over time, I don't believe that simple paired t-tests can properly answer this question. My data frame is as follows:</p>

<p><code>head</code></p>

<pre><code>   subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
1:          0   2013-05-01     3        2011-07-01                           1     afteronly                TRUE                    22.33333                    195.7986
2:          0   2013-04-01     1        2011-07-01                           1     afteronly                TRUE                    21.33333                    194.7986
3:          0   2013-02-01    19        2011-07-01                           1     afteronly                TRUE                    19.36806                    192.8333
4:          0   2013-12-01     3        2011-07-01                           1     afteronly                TRUE                    29.46806                    202.9333
5:          0   2013-10-01     4        2011-07-01                           1     afteronly                TRUE                    27.43333                    200.8986
</code></pre>

<hr>

<p><code>tail</code></p>

<pre><code>       subject_id date_monthly count subject_join_date covar1_per_subject_per_date subject_group after_mid_date_bool covar2_per_subject_per_date covar3_per_subject_per_date
22407:       6911   2013-08-01     3        2011-08-01                           1     afteronly                TRUE                    24.36667                    198.8653
22408:       6911   2013-07-01     1        2011-08-01                           1     afteronly                TRUE                    23.33333                    197.8319
22409:       6911   2013-06-01     1        2011-08-01                           1     afteronly                TRUE                    22.33333                    196.8319
22410:       6931   2009-05-01     7        2009-05-01                           1    beforeonly               FALSE                     0.00000                    147.0986
22411:        238   2013-09-01     1        2012-10-01                           1     afteronly                TRUE                    11.16667                    199.8986
</code></pre>

<p><code>count</code> is the response I am looking to model.</p>

<p>I've read through all of Bates' lme4 paper, but I am still confused as to how to specify the random effects part of my model.</p>

<p>My attempt at a model specification is:</p>

<pre><code>lmer(log(count) ~ covar1_per_subject_per_date + covar2_per_subject_per_date + 
covar3_per_subject_per_date + after_mid_date_bool + 
subject_group + subject_join_date + (1|subject_id) + (1|date_monthly),
data=df, REML=F)
</code></pre>

<p>Which ""works"" (no errors from <code>lmer</code>). However, my primary question is:</p>

<p>Is this the correct specification for a mixed effects model with random, uncorrelated intercepts for <code>subject_id</code> and <code>date_monthly</code>? Correct here means that we model independent fixed effects for each of the fixed effects specified in the model, accounting for multiple trials of the same subject over time with subjects not appearing at every time point.</p>

<p>A secondary but related question is:</p>

<p>Have I organized my data frame in the proper way? My worry is that the <code>after_mid_date</code> column may be specified improperly.</p>

<p>I apologize if this is long-winded or too-specific of a question. My intention of providing my exact data is to be as clear as possible with my question.</p>
"
"0.0572129567690623","0.0574484989621426","154043","<p>In polynomial regression, it is recommended to center predictor input variables to break multi colinear relationships of x to x^2.</p>

<p>From Wikipedia: The underlying monomials can be highly correlated ""For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). ""</p>

<p>When a variable x is between -1 and 1, x^2 makes the magnitude smaller while when x is outside of that range, x^2 makes x's magnitude larger.</p>

<p>Making the variable into an integer variable could change the behavior.</p>

<p>E.g.</p>

<pre><code>df$x=round((df$x - mean(df$))*100)
</code></pre>

<p>Any opinions on the scale especially in regards to interval [-1,1] vs [-100,100]</p>

<p>It is common to normalize predictors subtracting the mean and dividing by the standard deviation when doing inference analysis but this question pertains to regression prediction.</p>

<p>Asking a similar question in regards to natural log, a variable that has a range (0,1] has a dramatically different transformed value than [1,100].</p>

<pre><code>log(seq(0.1,1,.1)) #mostly negative
log(seq(0.1,1,.1)*100) #rather positive
</code></pre>

<p>If the predictor variable in the case of log happened to be sometimes less than 1 and others greater than 1, that could make the transformation act a little ""wild"". Would it be best to transform the variable to be within (0,1] or [1,] but not both?</p>
"
"0.107274293941992","0.114896997924285","154112","<p>I have a dataset with more than 20 predictors and a single binary response variable. With only $n=181$ observations (64 deaths, 117 survivors), I decided to apply penalized logistic regression to modeling, with all predictors involved (so that I avoid problems associated with model selection). Nevertheless, I have to produce a ''simpler'' model too (i.e. one that is simple enough to be suitable for a nomogram-style hand calculation in clinical setting). For that end, I intend to use <code>rms</code>'s <code>fastbw</code>.</p>

<p>To exemplify my questions, I'll use the <code>support</code> dataset from <code>Hmisc</code>:</p>

<pre><code>library( rms )
getHdata( support )
fit &lt;- lrm( hospdead ~ rcs( age ) + sex + rcs( meanbp ) + rcs( crea ) + rcs( ph ) + rcs( sod ), data = support, x = TRUE, y = TRUE )
fit
</code></pre>

<p>First, I apply penalization:</p>

<pre><code>p &lt;- pentrace( fit, seq( 0, 10, by = 0.01 ) )
plot( p )
fitPen &lt;- update( fit, penalty = p$penalty )
fitPen
</code></pre>

<p>I hope I'm correct up to this point.</p>

<p>Next, I validate the model and calculate its calibration curve. If I understand it correctly, I shouldn't validate/calibrate the simpler model, rather, I have to run the necessary functions on the <em>original</em> model, but with <code>bw=T</code>. That is:</p>

<pre><code>validate( fitPen, B = 1000, bw = TRUE )
plot( calibrate( fitPen, B = 1000, bw = TRUE ) )
</code></pre>

<p><strong>Question #1</strong>: Am I correct in this? I.e. is it true that to get the simpler model's validation/calibration I have to run these not on the simpler model, but on the original one (with <code>bw=T</code>)? And the results will be those pertaining to the simpler model, despite the fact that I haven't run validation/calibration on the simpler model itself?</p>

<p>Next, I try to come up with the simpler model <em>explicitly</em>. Interestingly, <a href=""http://www.aliquote.org/cours/2011_health_measures/harrell98.pdf"" rel=""nofollow"">(Harrell, 1998)</a> uses a method which is based on calculating the logits for the observations, then modeling them with OLS, then narrowing this model with <code>fastbw</code>. Although it is surely my statistical shortcoming, I simply can't understand why this is necessary.</p>

<p><strong>Question #2</strong>: Why can't we <em>directly</em> use <code>fastbw</code> on the logistic regression model? Such as:</p>

<pre><code> fastbw( fitPen )
 fitApprox &lt;- lrm( as.formula( paste( ""hospdead ~"", paste( fastbw( fitPen )$names.kept, collapse = ""+"" ) ) ), data = support, x = TRUE, y = TRUE )
</code></pre>

<p>And finally, I am not completely sure on where should I apply penalizing in the whole process.</p>

<p><strong>Question #3</strong>: Should I penalize the original model, then run <code>fastbw</code> (see above), and then re-penalize the obtained model? I.e.</p>

<pre><code>p &lt;- pentrace( fitApprox, seq( 0, 10, by = 0.01 ) )
plot( p )
fitApproxPen &lt;- update( fitApprox, penalty = p$penalty )
fitApproxPen
</code></pre>

<p>Or I don't have to re-penalize the narrowed model? Or I don't have to penalize the original model and it is sufficient to penalize the simpler one? (I suspect that the very first option is the correct, but I'm not entirely sure.)</p>
"
"0.107274293941992","0.114896997924285","154588","<p>I'm trying to build a covariance-based structural equation model (SEM) using both reflective and formative specifications of latent variables. I use the <code>sem</code> function in the <code>lavaan</code> package for estimation (R version 3.1.3, lavaan version 0.5-18). But estimates turn always out to be zero which is unreasonable.</p>

<p>The lavaan model syntax uses <code>=~</code> for reflective specification of latent variables, <code>&lt;~</code> for formative specification of latent variables, and <code>~</code> for regressions (<a href=""http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax"" rel=""nofollow"">http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax</a>). Here is a simple working example with only reflective specifications (it is a simplified version of the example provided at <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">http://lavaan.ugent.be/tutorial/sem.html</a> and by <code>example(sem)</code>)</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 =~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>Now assume that based on prior theory I would know that dem60 is a formative construct composed of y1 and y2. Thus I change the specification from <code>=~</code> to <code>&lt;~</code> and obtain the following code</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 &lt;~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>The estimates for both y1 and y2 turn out to be zero. Analogously, the regression effect of ind60 on dem60 turns out to be zero. What do I need to change to get a meaningful result?</p>

<p>Several websites and blogs suggested the following modifications:
(1) Fix one parameter in the formative construct, i.e. <code>dem60 &lt;~ 1*y1 + y2</code>.
(2) Allow for covariance of the manifest indicators, i.e. <code>y1 ~~ y2</code>. 
(3) Fix the variance of the formative construct, i.e. <code>dem60 ~~ 1</code>.
(4) Free the variance of the formative construct, i.e. <code>dem60 ~~ NA*dem60</code>. 
None of these are working. Again: What do I need to change to get a meaningful result?</p>
"
"0.114624397492221","0.121866669555358","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.0700712753800578","0.0703597544730292","154917","<p>I have written <code>R</code> codes for simulating data from Multilevel logistic regression model . </p>

<p>I focus on the following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , $u_{0j}\sim N(0,\sigma_0^2)$ , $u_{1j}\sim N(0,\sigma_1^2)$ , $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$</p>

<p>In this <a href=""http://www.biomedcentral.com/1471-2288/7/34#sec2"" rel=""nofollow"">paper</a> in equation (2) , they assumed $\text{cov}(u_{0j},u_{1j})=\sigma_{01}$ , that is not independent . But also they mentioned in the methodology section that :</p>

<blockquote>
  <p>The group random components $u_{0j}$ and $u_{1j}$
  are ""independent"" normal variables with mean zero and
  standard deviations $Ïƒ_0$ and $Ïƒ_1$. </p>
</blockquote>

<p>So I assumed $\text{cov}(u_{0j},u_{1j})=0$ . </p>

<p>R code :</p>

<pre><code>## Simulating data from multilevel logistic regression 

set.seed(1234)
x &lt;- rnorm(1000) ### individual level variable
z &lt;- rnorm(1000) ### group level variable

##fixed effect parameter
g_00 &lt;- -1
g_01 &lt;- 0.3
g_10 &lt;- 0.3
g_11 &lt;- 0.3

g &lt;- matrix(c(g_00,g_01,g_10,g_11),ncol=1)

require(mvtnorm)

##need variance values as input 
s2_0 &lt;- 0.36
s2_1 &lt;- 1
s01 &lt;- 0

##generate bi-variate normal rv for u0, u1

avg &lt;- c(0,0) ##mean
sigma &lt;- matrix(c(s2_0,s01,s01,s2_1),ncol=2)

u &lt;- rmvnorm(1000,mean=avg,sigma=sigma,method=""chol"")

pi_0j &lt;- g_00 +g_01*z + as.vector(u[,1])
pi_1j &lt;- g_10 +g_11*z + as.vector(u[,2])
p &lt;- exp(pi_0j+pi_1j*x)/(1+exp(pi_0j+pi_1j*x))

y &lt;- rbinom(1000,1,p)
</code></pre>

<p>But i am not understanding where is to consider the group ? If i select number of groups to be $100$ $(j=1,2,\ldots, 100)$, then will I assign the groups randomly against each $y_{i,j}$ ?</p>

<ul>
<li>Have i correctly simulated data from <code>Multilevel Logistic Distribution</code> ?</li>
</ul>
"
"0.06396603026469","0.0642293744423385","154986","<p>We want to do the logistic regression analysis to consider the effect of Age, CD4 on drug resistance mutations. The code that we wrote is:</p>

<pre><code>logist.summary(glm(DRM ~ Age, data = Database, family = binomial),""wald"")
</code></pre>

<p>The results are: </p>

<pre><code>            log.OR OR lower.CI upper.CI p.value
(Intercept)  -0.31 0.74     0.05     9.95  0.8169
Age          -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>However, we want to do the test like, we will consider whether, 20 years old differences between the subjects, what the results is? Is it relative to DRMs? We wrote:</p>

<pre><code>logist.summary(glm(DRM ~ I(Age+20), data = Database, family = binomial),""wald"")
</code></pre>

<p>Results:</p>

<pre><code>            log.OR   OR lower.CI upper.CI p.value
(Intercept)   1.17 3.22     0.05   190.62  0.5742
I(Age + 20)  -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>I want to ask:</p>

<ul>
<li>Is the code we wrote correct?</li>
<li>Can you help me explain what is meaning of these table?</li>
<li>Why it is the same results for the Age and Age+20? But differences in the Intercept? What does intercept meaning in this case?</li>
</ul>
"
"0.0990957479752576","0.0912117424359157","155040","<p>I am struggling with interpreting coefficients from a multiple regression analysis with multiple categorical (dummy) variables. I am running a linear mixed model with biodiversity (<code>LnS_Add1</code>) as independent variable, and several continuous and categorical dependent variables.</p>

<p>With a single categorical/dummy variable (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3</code>; where <code>AREA_AM_2.5</code> is continuous and <code>System_Type3</code> is categorical with 3 levels, i.e. <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em>) this is pretty straightforward. In this case the intercept represents the mean of the reference dummy variable (e.g. <em>Arable</em>) and the mean of the 2nd and 3rd levels <em>Grassland</em> and <em>Orchard</em> can be calculated manually by adding intercept to the slope coefficient.</p>

<pre><code>globmod1 &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + 
     (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1)
</code></pre>

<p>Which returns</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
(Intercept)            0.3585534  0.1238470   2.895
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Grassland -0.5227684  0.0915722  -5.709
System_Type3Orchard   -0.4057969  0.5477567  -0.741
</code></pre>

<p>To get a summary output that shows the means of both <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em> in R I suppress the intercept by adding a -1 (or +0) to the model.</p>

<pre><code>globmod1.coef &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 -1 +
                   (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1.coef)
</code></pre>

<p>Which returns:</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Arable     0.3585534  0.1238470   2.895
System_Type3Grassland -0.1642149  0.1341851  -1.224
System_Type3Orchard   -0.0472434  0.5457304  -0.087
</code></pre>

<p>But what do I do if I have multiple categorical variables (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + Habitat2</code>; where Habitat2 is a categorical variable with 3 levels, i.e. <em>Farm aggregated</em>, <em>Outside field</em>, and <em>Within field</em>)?. Now the intercept represents the mean of the reference level of a combination of <code>System_Type3</code> and <code>Habitat2</code> (e.g. all data in arable systems and measured at farm aggregate level). But what I am interested in are the means for the different levels of each of my 2 categorical variables, holding everything else constant.</p>

<p>How do I create a summary table that contains means of all levels of all categorical variables in my model? The -1 command doesnt help me anymore, as it removes the intercept but the intercept now represents a mean of 2 reference dummy variables. I am only interested here in the fixed effect estimates, not in any hypothesis testing.</p>
"
"0.0572129567690623","0.0574484989621426","155509","<p>Working on a linear regression problem in R, I created a first model </p>

<pre><code>flights_lm = lm(freq~dist+capa+nbrt+depf+lcco+prbi)
</code></pre>

<p>where freq is frequency, dist is distance, capa for capacity, nrt stands for number of roads, fuel is depf. I then started to remove variables which doesn't contribute to the model explination, and ended up with
 Coefficients:</p>

<pre><code>              Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept) 2.822e+02  6.239e+03   0.045  0.96400    
 nbrt        8.072e+01  6.515e+00  12.390  &lt; 2e-16 ***
 depf        3.052e-05  1.128e-05   2.704  0.00784 ** 
</code></pre>

<p>meaning that the frequency was well explained by nbrt and depf. Then I proceeded to remove points that affect the model (I guess they're called outliers). for that I used the R function <code>Cooks.distance</code> and couldn't get rid of all the points. each time I apply the function and plot it according to the model some other points pop out of the 0.65 limit that set for the model.
I have one doubt though, once I remove the points, Do I need to restart the process from the beginning (meaning creating the model with all the variables then delete each that p-value is the biggest until I get all the p values &lt;0.05)
or just delete the points and plot the model ?
Another question is :  could the function log help me with this case ?  </p>
"
"0.0756856276908142","0.0651404749061921","156037","<p>The R Vars package has a Vector Auto Regression function called var. The arguments include (among other things) ""p"" defined as the ""Integer for the lag order"" and ""lag.max,"" which is defined as ""Integer, determines the highest lag order for lag length selection according to the choosen (sic) ic."" See cran.r-project.org/web/packages/vars/vars.pdf.</p>

<p>My questions are:</p>

<p>What is the definition of the ""lag order?""</p>

<p>I thought that ""lag.max"" meant the highest number of lags to consider, but the package documentation defines ""lag.max"" as the highest lag order. So, if the ""lag.max"" is the highest ""lag order"" then, clearly the ""lag order"" argument is not asking for the maximum ""lag order"" because that would be redundant. So, what is the argument for ""lag order"" asking for? ... the minimum lag order? ... the actual order of the lags, i.e., an order such as t-1, t-3, t-2 instead of t-1, t-2, t-3?</p>

<p>The definition of ""lag.max"" uses the term ""lag length."" What is that? I would have thought that it would also be the maximum number of lags to consider, but clearly from the context, that is not the case. So, what are the definitions of ""lag.max"" and ""lag length?""</p>

<p>Statistics would not be hard if statisticians would learn to define their terms!</p>
"
"0.06396603026469","0.0642293744423385","156275","<p>I have two paired samples following normal distributions N(0, $\sigma_1^2$) and N(0, $\sigma_2^2$). Samples represent estimation errors (residuals) of two linear regression models used to predict the same response variable using two different methods/independent variables. I have 30 pairs of residuals, so I would like to apply Wilcoxon signed-rank test to check whether means of absolute values or relative errors are different. Since absolute values do not follow normal distribution, I cannot use t-test or something similar.</p>

<p>I would like to find type II error and statistic power of Wilcoxon signed-rank test.
Is there some R function (or any other tool) that can be used? I have found a number of functions for testing the power of tests here <a href=""http://www.statmethods.net/stats/power.html"" rel=""nofollow"">http://www.statmethods.net/stats/power.html</a>  but Iâ€™m not sure could they be applied on Wilcoxon signed-rank test. If there is no built-in function is there some other tool or algorithm to manually calculate error?   </p>
"
"0.0862517776135472","0.0952675579132743","156619","<p>I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.</p>

<p><strong>What I have and what I do</strong></p>

<ul>
<li>my data consists out of real estates with following charcteristics: <code>price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.</code></li>
<li>I run a robust regression without intercept <code>lmRob(price ~ . -1)</code></li>
</ul>

<p><strong>What I want</strong></p>

<ul>
<li>a model with which I can predict the price of real estates, but which are not in the used data set</li>
<li>also it would be nice to have some constraints on the coefficients</li>
</ul>

<p><strong>Problems</strong></p>

<ul>
<li>very often I get bad values for the coefficients <code>ex: bathroomCount = -80000</code>. it's not possible that with a additive bathroom , the price of the house will sink with <code>80.000â‚¬</code></li>
<li><p>also I tried to use the function <code>pcls</code> in order to put some constraints on the coefficients, but this method gave very bad results. In the plot <code>Y = price</code> and <code>X = livingArea</code>. as you can see, the regression line isn't correct.
<img src=""http://i.stack.imgur.com/7PHp1.png"" alt=""enter image description here""></p>

<ul>
<li>another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do it</li>
<li>also I tried to use different regression methods <code>lm, lmrob, ltsReg, MARS</code>, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)</li>
<li>I think that the big number of dummy variables damages a little bit the regression</li>
</ul></li>
</ul>

<p>Is my approach false?</p>

<p>Does someone have some hints, tricks for me? (<em>I'm not a statistician</em>)</p>

<p><strong>[UPDATE]</strong></p>

<p><img src=""http://i.stack.imgur.com/a2kLe.png"" alt=""price ~ livingArea""></p>

<p>This is how the plotted data looks like. LivingArea is the only non-dummy variable.</p>

<p><strong>[UPDATE 2]</strong></p>

<pre><code>y = bX 

     means

y = b_0*X_0 + b_1*X_1 + ... + b_k*X_k

     which is an equation system like this:

y[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]
.
.
.
y[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]
</code></pre>

<p>Did I got it right? </p>

<p>If so, isn't possible to add some inequality constraints equation to it. example:</p>

<pre><code>b_0 &gt;= 2000
b_2 &lt;= b_0/2
</code></pre>

<p><strong>[UPDATE 3]</strong></p>

<p>I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0mÂ².
<img src=""http://i.stack.imgur.com/LYPB0.png"" alt=""enter image description here"">
but it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?</p>
"
"0.0404556697031367","0.0203111115925597","156804","<p>I have estimated a mixed-effects logistic regression with glmer
and want to draw a bootstrapped confidence-region for the mean predicted probability for two subgroups of the sample.</p>

<p>I have a $1000 \times 2$ Matrix $X$ containing the bootstrapped mean predicted probabilities for the two groups.
One could now compute the empirical covariance matrix $S$ and draw a
circle around the means using the metric induced by $S^{-1}$,
i.e. drawing a confidence ellipsoid based on normality-assumption.</p>

<p>Are there any widely used alternatives to this approach that do not imply distributional assumptions? </p>

<p>skeletor</p>
"
"0.114425913538125","0.107715935554017","157851","<p>In the <code>lmer</code> function within <code>lme4</code> in <code>R</code> there is a call for constructing a model matrix of random effects, $Z$, as explained <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">here</a>, pages 7 - 9.</p>

<p>Calculating $Z$ entails KhatriRao and/or Kronecker products of two matrices, $J_i$ and $X_i$.  </p>

<p>The matrix $J_i$ is a mouthful: ""Indicator matrix of grouping factor indices"", but it seems to be a sparse matrix with dummy coding to select which unit (for example, subjects in repetitive measurements) corresponding to higher hierarchical levels are ""on"" for any observation. The $X_i$ matrix seems to act as a selector of measurements in the lower hierarchical level, so that the combination of both ""selectors"" would yield a matrix, $Z_i$ of the form illustrated in the paper via the following example:</p>

<pre><code>(f&lt;-gl(3,2))

[1] 1 1 2 2 3 3
Levels: 1 2 3

(Ji&lt;-t(as(f,Class=""sparseMatrix"")))

6 x 3 sparse Matrix of class ""dgCMatrix""
     1 2 3
[1,] 1 . .
[2,] 1 . .
[3,] . 1 .
[4,] . 1 .
[5,] . . 1
[6,] . . 1

(Xi&lt;-cbind(1,rep.int(c(-1,1),3L)))
     [,1] [,2]
[1,]    1   -1
[2,]    1    1
[3,]    1   -1
[4,]    1    1
[5,]    1   -1
[6,]    1    1
</code></pre>

<p>Transposing each of these matrices, and performing a Khatri-Rao multiplication:</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp;. &amp;. &amp;. &amp;.\\.&amp;.&amp;1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;1&amp;1 \end{smallmatrix}\right]\ast \left[\begin{smallmatrix}\,\,\,\,1 &amp; 1 &amp;\,\,\,\,1 &amp;1 &amp;\,\,\,\,1 &amp;1\\-1&amp;1&amp;-1&amp;1&amp;-1&amp;1 \end{smallmatrix}\right]=
\left[\begin{smallmatrix}\,\,1 &amp; 1 &amp;.&amp;.&amp;.&amp;.\\\,\,\,\,-1 &amp;1&amp;.&amp;.&amp;.&amp;.\\ .&amp;.&amp;\,\,\,\,\,1 &amp;1&amp;.&amp;.\\.&amp;.&amp;\,\,-1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;\,\,\,1&amp;1\\.&amp;.&amp;.&amp;.&amp;-1&amp;1 \end{smallmatrix}\right]$</p>

<p>But $Z_i$ is the transpose of it:</p>

<pre><code>(Zi&lt;-t(KhatriRao(t(Ji),t(Xi))))

6 x 6 sparse Matrix of class ""dgCMatrix""

[1,] 1 -1 .  . .  .
[2,] 1  1 .  . .  .
[3,] .  . 1 -1 .  .
[4,] .  . 1  1 .  .
[5,] .  . .  . 1 -1
[6,] .  . .  . 1  1
</code></pre>

<p>It turns out that the authors make use of the database <code>sleepstudy</code> in <code>lme4</code>, but don't really elaborate on the design matrices as they apply to this particular study. So I'm trying to understand how the made up code in the paper reproduced above would translate into the more meaningful <code>sleepstudy</code> example.</p>

<p>For visual simplicity I have reduced the data set to just three subjects - ""309"", ""330"" and ""371"":</p>

<pre><code>require(lme4)
sleepstudy &lt;- sleepstudy[sleepstudy$Subject %in% c(309, 330, 371), ]
rownames(sleepstudy) &lt;- NULL
</code></pre>

<p>Each individual would exhibit a very different intercept and slope should a simple OLS regression be considered individually, suggesting the need for a mixed-effect model with the higher hierarchy or unit level corresponding to the subjects:</p>

<pre><code>    par(bg = 'peachpuff')
    plot(1,type=""n"", xlim=c(0, 12), ylim=c(200, 360),
             xlab='Days', ylab='Reaction')
    for (i in sleepstudy$Subject){
                fit&lt;-lm(Reaction ~ Days, sleepstudy[sleepstudy$Subject==i,])
            lines(predict(fit), col=i, lwd=3)
            text(x=11, y=predict(fit, data.frame(Days=9)), cex=0.6,labels=i)
        }
</code></pre>

<p><img src=""http://i.stack.imgur.com/opwVvm.png"" alt=""enter image description here""></p>

<p>The mixed-effect regression call is:</p>

<pre><code>fm1&lt;-lmer(Reaction~Days+(Days|Subject), sleepstudy)
</code></pre>

<p>And the matrix extracted from the function yields the following:</p>

<pre><code>parsedFormula&lt;-lFormula(formula= Reaction~Days+(Days|Subject),data= sleepstudy)
parsedFormula$reTrms

$Ztlist
    $Ztlist$`Days | Subject`
6 x 12 sparse Matrix of class ""dgCMatrix""

309 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . . . . . . . . . . . .
309 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . . . . . . . . . . . .
330 . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . .
330 . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . .
371 . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1
371 . . . . . . . . . . . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9
</code></pre>

<p>This seems right, but if it is, what is linear algebra behind it? I understand the rows of <code>1</code>'s being the selection of individuals like. For instance, subject <code>309</code> is on for the baseline + nine observations, so it gets four <code>1</code>'s and so forth. The second part is clearly the actual measurement: <code>0</code> for baseline, <code>1</code> for the first day of sleep deprivation, etc.</p>

<p><strong>But what are the actual</strong> $J_i$ <strong>and</strong> $X_i$ <strong>matrices and the corresponding</strong> $Z_i= (J_i^{T}âˆ—X_i^{T})^âŠ¤$ <strong>or</strong> $Z_i= (J_i^{T}\otimes X_i^{T})^âŠ¤$, <strong>whichever is pertinent?</strong></p>

<p>Here is a possibility,</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.  &amp;. &amp;. &amp;. &amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.\\
.&amp;.&amp;.&amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.\\&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1\end{smallmatrix}\right]\ast \left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1&amp;1&amp;1&amp;1 &amp; 1 &amp; 1 &amp; 1\\0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right]=$</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\0 &amp; 1 &amp; 2 &amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\ &amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0 &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\\&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right] $</p>

<p>The problem is that it is not the transposed as the <code>lmer</code> function seems to call for, and still is unclear what the rules are to create $X_i$.</p>
"
"0.0572129567690623","0.0287242494810713","158361","<p>I'm working with some panel style data and I was wonder if it makes sense to subtract the mean from my predictors as a whole or to do it by year. So lets say I was doing a pooled regression of some panel data:</p>

<pre><code>brand  year  y  x1  x2
----------------------
brand_1 2013 12 123 63
brand_1 2014 12 45  345
brand_1 2015 53 23  345
brand_2 2013 35 13  65
brand_2 2014 74 45  35
brand_2 2015 47 23  45
.
.
.
brand_n 2013 54 155 600
brand_n 2014 54 15  38
brand_n 2015 23 333  32
</code></pre>

<p>If I were to add a quadratic term into the model generally one is suppose to subtract the mean from the the original variable for interpretation sake. Should the mean be subtracted by year or just overall?  </p>

<pre><code>lm(y ~ x1 + x2 + I(x1^2) + I(x2^2), data = df)
</code></pre>
"
"0.0707974219804893","0.0812444463702388","158433","<p>I keep trying to perform parametric bootstrap on simple regression analysis to grasp the concept. The internet is full of tutorials on non-parametric one, but I found no explanation or steps concerning parametric bootstrap, so I did it on my own. Since I'm not sure if what was done is o.k., I kindly ask you to correct if I'm wrong (... or praise me if I'm right:) ).</p>

<pre><code>library(car)
library(boot)
attach(Anscombe) # I'm going to use Anscombe data

lm.out&lt;-lm(education~income, data=Anscombe) #simple regression to obtain coef.
regre.mle&lt;-coef(lm.out)
</code></pre>

<p>The model was built, so I was able to get sample $\sigma$ for errors</p>

<pre><code>mean(lm.out$resid) # 3.957485e-15
sd(lm.out$resid)   # 34.58725

regre.sim &lt;- function(data, mle){
  n &lt;- dim(data)[1]
  data$education &lt;- mle[2]*data$income+mle[1]+rnorm(n, mean=0, sd=34.58725)
  return(data)
}

regre.stat&lt;- function(data) {
  lm.out&lt;-lm(education~income, data=data)
  return(lm.out$coefficients)
}

boot.out&lt;-boot(Anscombe, statistic=regre.stat, R=100,
               ran.gen=regre.sim,
               sim=""parametric"", mle=regre.mle)

boot.ci(boot.out, type = ""basic"", index = 1) #for intercept
boot.ci(boot.out, type = ""basic"", index = 2) #for beta coef
</code></pre>

<p>Finally I get this:</p>

<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 99 bootstrap replicates

CALL : 
boot.ci(boot.out = boot.out, type = ""basic"", index = 2)

Intervals : 
Level      Basic         
95%   ( 0.0359,  0.0727 )  
Calculations and Intervals on Original Scale
Some basic intervals may be unstable
</code></pre>

<p>But the question is - this is it (parametric bootstrap on regression)? </p>
"
"0.0495478739876288","0.0497518595104995","158701","<p>I use the svm function (for regression) to make forecast like I would with for exemple the arima function:<br>
<code>fit&lt;-auto.arima(ts)</code><br>
<code>prediction&lt;-forecast(fit,h=20)</code><br>
which returns different attributes : </p>

<blockquote>
  <ol>
  <li><code>prediction$mean</code> which is the actual prediction  </li>
  <li><code>prediction$lower</code> and <code>prediction$upper</code> which are the   <strong>boundaries of the confidence intervals</strong> on each points of the   <code>prediction$mean</code>.  </li>
  </ol>
</blockquote>

<p>I would like the <code>svm</code> function (from <em>e1071</em> package) to return a more detailed answer than just the value (like the <code>forecast()</code> would).<br>
 But I guess it is not implemented in the function yet.
Is there another function to do it ? Or should I use <strong>bootstrap</strong> methods to try to estimate those boundaries? And if I should use this are they pre-implemented version of them instead of using sample over a for loop which is very time-consuming ?</p>
"
"0.0330319159917525","0.0497518595104995","158821","<p>In R, lm(y~x) will get the linear regression in terms of y=a+bx, giving the estimates of both slope and intercept. Now I want to set the slope to 1, and do the regression, which means the form of the regression should be y=a+x (since b is forced to be one), and the estimate of intercept will be generated. How can I achieve that in R? And how can I compare the regression of the slope 1 with regression of the estimated slope b?</p>
"
"0.0404556697031367","0.0406222231851194","159301","<p>I have a classification problem I am attempting to model using logistic regression (via the <code>glm</code> package in R): </p>

<pre><code>cols &lt;- c(""x"", ""z"", ""a"", ""b"", ""c"") 
formula = paste0(""x ~ "", paste(cols, collapse = ""+""))
formula = as.formula(formula)
</code></pre>

<p>I have a bunch of explanatory variables at the moment. How advisable is it to model this relationship using <code>gbm</code>, see the relative inference strength of each variable, and then remove seemingly meaningless variables before <code>glm</code> regression?</p>

<p>I ask just because I have done this in the past, with <code>glm</code> and <code>gbm</code> giving seemingly contradictory signals on different explanatory variables. </p>
"
"0.0948769553749019","0.0952675579132743","159428","<p>I have a set of data, let's say average weight of employees, captured every month over a period of 5 years (2010 - 2014). I cannot find a seasonality trend in the data over these years. Also, I have found that it is not dependent on any other factors.</p>

<p>I am trying to forecast values for 2015 to get a general sense of this data as it is an important metric in the operations of my business. </p>

<p>I have tried ARIMA, R-regression, Exponential smoothing, Excel forecast to find any seasonality whatsoever. However, my efforts are yet to materialize. </p>

<p>My question is: How do I forecast a variable that has no seasonality?</p>

<p>I have attached my data herewith. </p>

<p><strong>Graphs</strong></p>

<p>Yearly Values for years 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/rmoeD.jpg"" alt=""enter image description here""></p>

<p>Value Cumulative over 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/iwyh8.jpg"" alt=""enter image description here""></p>

<p>All Values from 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/dfcGd.jpg"" alt=""enter image description here""></p>

<p><strong>Auto ARIMA in R</strong></p>

<pre><code># Map 1-based optional input ports to variables
dataset1 &lt;- maml.mapInputPort(1) # class: data.frame
library(forecast)


dates &lt;-  dataset1$Date
values &lt;- dataset1$Weight

dates &lt;-  as.Date(dates, format = '%m/%d/%Y')
values &lt;- as.numeric(values)

train_ts &lt;- ts(values, frequency=12)
fit1 &lt;- auto.arima(train_ts)
train_model &lt;- forecast(fit1, h = 12)
plot(train_model)

# produce forecasting
train_pred &lt;- round(train_model$mean,2)
data.forecast &lt;- as.data.frame(t(train_pred))
#colnames(dataset1.forecast) &lt;- paste(""Forecast"", 1:data$horizon, sep="""")

# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""data.forecast"");
</code></pre>

<p><strong>Forecasted Value with Auto ARIMA</strong></p>

<pre><code>Date        Weight
01-01-15    11.77
01-02-15    11.76
01-03-15    11.77
01-04-15    11.76
01-05-15    11.77
01-06-15    11.77
01-07-15    11.76
01-08-15    11.77
01-09-15    11.76
01-10-15    11.77
01-11-15    11.77
01-12-15    11.76
</code></pre>

<p><strong>Data</strong></p>

<pre><code>Date        Weight      Cumulative Weight
01-01-10    11.8800     11.8800
01-02-10    10.4000     22.2800
01-03-10    6.9500      29.2300
01-04-10    15.5000     44.7300
01-05-10    17.0400     61.7700
01-06-10    10.4700     72.2400
01-07-10    12.1400     84.3800
01-08-10    2.5800      86.9600
01-09-10    12.6300     99.5900
01-10-10    11.6800     111.2700
01-11-10    9.0700      120.3400
01-12-10    10.8900     131.2300
01-01-11    1.7500      132.9800
01-02-11    -1.7700     131.2100
01-03-11    5.9300      137.1400
01-04-11    -4.9200     132.2200
01-05-11    4.3900      136.6100
01-06-11    1.5100      138.1200
01-07-11    1.2200      139.3400
01-08-11    10.2900     149.6300
01-09-11    13.0600     162.6900
01-10-11    10.1400     172.8300
01-11-11    8.5250      181.3550
01-12-11    6.4350      187.7900
01-01-12    -5.5100     182.2800
01-02-12    -4.3000     177.9800
01-03-12    2.3200      180.3000
01-04-12    4.0700      184.3700
01-05-12    12.2700     196.6400
01-06-12    14.7400     211.3800
01-07-12    8.4600      219.8400
01-08-12    11.6300     231.4700
01-09-12    -0.1500     231.3200
01-10-12    2.5200      233.8400
01-11-12    6.7400      240.5800
01-12-12    35.6300     276.2100
01-01-13    26.4000     302.6100
01-02-13    26.1300     328.7400
01-03-13    16.2100     344.9500
01-04-13    56.0800     401.0300
01-05-13    32.2300     433.2600
01-06-13    17.5100     450.7700
01-07-13    3.6700      454.4400
01-08-13    7.7700      462.2100
01-09-13    -14.2800    447.9300
01-10-13    1.0800      449.0100
01-11-13    9.4000      458.4100
01-12-13    7.3400      465.7500
01-01-14    6.1400      471.8900
01-02-14    3.8200      475.7100
01-03-14    16.7600     492.4700
01-04-14    0.4900      492.9600
01-05-14    17.9800     510.9400
01-06-14    14.8000     525.7400
01-07-14    12.6400     538.3800
01-08-14    5.7300      544.1100
01-09-14    -2.0900     542.0200
01-10-14    9.1300      551.1500
01-11-14    12.5100     563.6600
01-12-14    -1.3900     562.2700
</code></pre>

<p><strong>Actual Values for 2015</strong></p>

<pre><code>Date        Weight
01-01-15    -18.43
01-02-15    13.94
01-03-15    26.14
01-04-15    24.36
01-05-15    18.37
</code></pre>
"
"0.0762839423587497","0.0861727484432139","159457","<p>I have bee wondering why in a multivariate OLS-Regression it is not possible for RÂ² to decrease when increasing the number of explanatory variables. 
The Point is that for example in the model Y=ÃŸ0+ÃŸ1x1+ÃŸ2x2+u, x1 and x2 are also correlated. Thus when using partialling out I only take the part of x1 that is not correlated or let's say orthogonal to x2. I also only take the part of x2 that is orthogonal to x1 before regressing. But what if the explanatory value is very small of x1 keeping x2 constant and of x2 keeping x1 constant as compared to only including one of the two explanatory variables into the model.
To better illustrate my thought let's assume a circle C that represents the variation in Y and a circle A that represents the variation in x1 and a circle B that represents the Variation in x2. All of the three circles overlap.
In a univariate Regression with only x1 as explanatory variable RÂ² represents the fraction of the area Where A and C overlap to the area of C. In the Regression with x1 and x2 A and B usually overlap as well. Partialling out would mean to eliminate the area where A, B overlap and thus where A, B and C overlap. Thus, if x1 and x2 are strongly correlated the remaining area where A overlaps with C only and B overlaps with C only could be smaller that the area where A and C overlap in the univariate case. Thus it should be possible that RÂ² decreases when increasing the number of explanatory variables..
But it never does..</p>

<p>My question is why and if there is some technique to divide the area where A,B and C overlap to the explanatory variables, how do they do it (e.g. 50:50?)?</p>
"
"0.0700712753800578","0.0703597544730292","159470","<p>I have a collection of data, obtained from different studies. To plot the ratio of means against different CO2 concentrations, I used a random effects model with a continues predictor (the CO2 concentrations given in ppm). </p>

<p>I also did a meta-regression, by assigning co2 values in to three groups (High co2, medium co2 and low co2). The ratio of means was the highest for the ""low co2"" group and the lowest for the ""medium co2"" group. The response is clearly not linear. </p>

<p>How could one fit various non linear models do the date, and test how well they fit the data? A <a href=""http://stats.stackexchange.com/questions/122196/nonlinear-meta-regression"">very nice tutorial</a> on fitting a quadratic polynomial model to the data exits. Is the best way to just try different polynoms and see which model result gets the highest p value?</p>

<pre><code>dat&lt;- read.csv(file=""C:/data.csv"",head=TRUE,sep="","")
library(metafor)

dat&lt;- escalc(measure=""ROM"", m1i = mean_t, m2i = mean_c, sd1i = sd_t, sd2i = sd_c, n1i = n, n2i = n, data = dat)
metaa&lt;- rma(yi, vi, method=""DL"", data=dat, mods=cbind(ppm))

wi &lt;- 1/sqrt(dat$vi)
size  &lt;- 0.5 + 3.0 * (wi - min(wi))/(max(wi) - min(wi))
plot(dat$ppm, exp(dat$yi), pch=19, cex=size,  xlab=""PPM"", ylab=""Reaction rate"",las=1, bty=""l"", log=""y"")

preds &lt;- predict(metaa, newmods=c(540:740), transf=exp)
lines(540:740, preds$pred)
lines(540:740, preds$ci.lb, lty=""dashed"")
lines(540:740, preds$ci.ub, lty=""dashed"")
</code></pre>

<p>The data: </p>

<pre><code>   number mean_c mean_t   sd_c   sd_t  n ppm
1       1  36.85  48.61  28.40  24.54 20 700
2       2  31.36  29.01  16.83  21.04 20 700
3       8  29.00  35.00   3.03   3.03  4 700
4       9  26.12  41.05   7.50   4.14 12 700
5      13  38.20  34.90   9.68  11.23 15 550
6      14  38.20  36.30   9.68  12.01 15 550
7      15  21.00  55.20  12.07   8.05 20 550
8      16  62.00  62.00   9.80   9.80  6 700
9      17  53.00  53.00   7.35   9.80  6 700
10     18  76.00  63.00   7.35  17.15  6 700
11     23 258.00 249.00 101.19 199.22 10 700
12     24  12.00  23.75   6.48   6.48  8 560
13     25  11.25  20.63   6.48   6.48  8 560
14     26  17.63  25.75   6.48   6.48  8 560
15     27  16.38  19.00   6.48   6.48  8 560
16     46 360.00 360.00 200.92 259.81 12 600
17     47 170.00 234.00  90.07  62.35 12 600
18     48 228.00 284.00  38.11 131.64 12 600
19     49 260.00 340.00 263.27 443.41 12 600
20     50  75.00 147.00  65.82 110.85 12 600
21     51 138.00 240.00 110.85 242.49 12 600
22     52  94.00 157.00 110.85 138.56 12 600
23     82 154.00 154.00  90.07  31.18 12 540
24     83 156.00 329.00 110.85  76.21 12 540
25     84 163.00 293.00 100.46  45.03 12 540
26     94 376.00 418.00 148.63 132.82 10 740
27     95  29.00  36.00  41.11  82.22 10 740
28     96 188.00 403.00 117.00  94.87 10 740
29     97 121.30 207.80  34.47  43.64 10 700
30     98 278.30 146.20  82.54  25.93 10 700
31    120 212.00 226.00 153.36 169.79 30 700
32    121 568.00 663.00  83.14 121.24 12 550
33    122 677.00 648.00 131.64 173.21 12 550
34    123 279.00 449.00 117.00 154.95 10 730
35    124 266.00 352.00 139.14 211.87 10 730
36    125  51.66  53.94  52.81  40.49  8 700
37    126  44.81  44.19  66.90  61.03  8 700
38    127  14.56  21.10  26.76  17.60  8 700
</code></pre>

<p><img src=""http://i.stack.imgur.com/Vqknd.jpg"" alt=""enter image description here""></p>
"
"0.114425913538125","0.114896997924285","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.145965139117999","0.146566068538932","159745","<p>I want to test a regression model with neuroticism as focal predictor, agreeableness as moderator and RT variability as dependent measure (covariates: attentional control and mean RT). Previously, I have used the modprobe macro in SPSS by Andrew Hayes for this (see: <a href=""http://link.springer.com/article/10.3758%2FBRM.41.3.924"" rel=""nofollow"">http://link.springer.com/article/10.3758%2FBRM.41.3.924</a>). I am in the process of transitioning to R, however, and would like to learn how to run a similar routine there. I have set up my regression model as follows:</p>

<pre><code>m3&lt;-lm(data=stp2_sub2, all_SD~Neuroticism*Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # full interaction model
m33&lt;-lm(data=stp2_sub2, all_SD~Neuroticism+Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # reduced model
</code></pre>

<p>I know that I can obtain F-change and p-change, using:</p>

<pre><code>anova(m3, m33) # provides F-change and p-change
</code></pre>

<p>What I still donâ€™t know yet is how to obtain the R squared change value, which gives me the effect size of the interaction effect. I have already posted a similar question at one of the sister websites (<a href=""http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#"">http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#</a>), although it seems that my question was slightly off-topic there. The users there have been very helpful (especially @gung), but I still have some remaining questions. Hence me posting here.</p>

<p>Basically, the recommendations have been so far to compute (a) semi-partial r-squared or (b) partial eta squared. For (a) semi-partial r-squared a custom-written function already exists (see: <a href=""http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615"">http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615</a>). Unfortunately I am a bit at a loss as to how to exactly adapt it to my own case, particularly the following bit: y = 4 + .5*x1 - .3*x2 + rnorm(10, mean=0, sd=1). Moreover, even if I did succeed at adapting y to my own needs, I would still need to know how to compute the r change value. The function for semi-partial r-squared yields a single r value (i.e. it doesnâ€™t provide a change value, yet). In my case, would I need to, in a first step, run this function for (1) the full model (here: m3) as well as for (2) the reduced model (here: m33), thereby providing me with two semi-partial r-squared values (full vs. reduced)? In a second step, would I then subtract semi-partial r-squared (reduced) from semi-partial r-squared (full), with the outcome being the r-change value that I need?</p>

<p>As for b, I have been told to use the following Â« formula Â» ( SSE(reduced)-SSE(full) ) / SSE(reduced) to compute partial eta squared for the interaction effect. I have found code for computing the sum of squared errors (see: <a href=""http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq"">http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq</a>) and have applied this to my case as follows:</p>

<pre><code>SSEfull&lt;-sum(m3$residuals^2) # sum of squared errors/residual sum of squares for the full regression model
SSEred&lt;-sum(m33$residuals^2) # sum of squared errors/residual sum of squares for the reduced regression model

pes&lt;-(SSEred-SSEfull)/SSEred # computing partial eta-squared
</code></pre>

<p>I would like to know whether I have correctly implemented the code to compute partial eta squared for the interaction effect. Moreover, given that I need the r squared change value, I was wondering how I might be able to convert the partial eta squared value to the r squared change value that I need. That said, as mentioned above, I just need to know how to compute the r squared change value for my interaction model. Ultimately, I donâ€™t mind whether I do this following suggestions (a) or (b) (but note that I still have some questions in this regard) or use a completely different way. Indeed, perhaps thereâ€™s already a function/package in R that calculates the r change value (I havenâ€™t found one yet)? I feel like being very close to finding what I need, but just need a final few pointers. Apologies for the long-winded way of writing this simple question, but I thought it could be of use to show what thought (even if not yet conclusive) has already gone into this. Any guidance on how to best go about this would be much appreciated.</p>
"
"0.0330319159917525","0.0497518595104995","160096","<p>I am going through the LAB section Â§6.6 on Ridge Regression/Lasso in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/index.html"" rel=""nofollow"">'An Introduction to Statistical Learning with Applications in R'</a> by James, Witten, Hastie, Tibshirani (2013).</p>

<p>More specifically, I am trying to do apply the scikit-learn <code>Ridge</code> model to the 'Hitters' dataset from the R package 'ISLR'. I have created the same set of features as shown in the R code. However, I cannot get close to the results from the <code>glmnet()</code> model. I have selected one L2 tuning parameter to compare. ('alpha' argument in scikit-learn).</p>

<p><strong>Python:</strong><BR></p>

<pre><code>regr = Ridge(alpha=11498)
regr.fit(X, y)
</code></pre>

<p><a href=""http://nbviewer.ipython.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%206.ipynb"" rel=""nofollow"">http://nbviewer.ipython.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%206.ipynb</a></p>

<p><strong>R:</strong></p>

<p>Note that the argument <code>alpha=0</code> in <code>glmnet()</code> means that a L2 penalty should be applied (Ridge regression). The documentation warns not to enter a single value for <code>lambda</code>, but the result is the same as in ISL, where a vector is used.</p>

<pre><code>ridge.mod &lt;- glmnet(x,y,alpha=0,lambda=11498)
</code></pre>

<p>What causes the differences?</p>

<p><B>Edit:</B><BR>
When using <code>penalized()</code> from the penalized package in R, the coefficients are the same as with scikit-learn.</p>

<pre><code>ridge.mod2 &lt;- penalized(y,x,lambda2=11498)
</code></pre>

<p>Maybe the question could then also be: 'What is the difference between <code>glmnet()</code> and <code>penalized()</code> when doing Ridge regression?</p>

<p><B>New python wrapper for actual Fortran code used in <em>R</em> package glmnet</B><BR>
<a href=""https://github.com/civisanalytics/python-glmnet"" rel=""nofollow"">https://github.com/civisanalytics/python-glmnet</a></p>
"
"0.0286064783845312","0.0287242494810713","160182","<p>let me ask the question in detail with an example --</p>

<p>I have a historical dataset with columns(a,b,c,d,e,f,g)</p>

<p>Now i have to predict (b,c,d,e,f,g) based on the value of 'a' and all the variables are inter-dependent on each other!</p>

<p>I did use K-NN, K-Means, regression model algorithms where i am able to predict one response variable. but in my case i need to predict multi-variables!</p>

<p>Can this done? Just need a right direction how to do that?</p>
"
"0.0862517776135472","0.0952675579132743","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.06396603026469","0.0642293744423385","160435","<p>I'm diving into arima models and was trying to repreduce the results of auto regression.</p>

<p>here is a reproducable example:</p>

<pre><code>set.seed(1)
z=arima.sim(n = 101, list(ar = c(0.8)))
</code></pre>

<p>when running ar(1) without an intercept </p>

<pre><code>&gt; ceof(arima(z, order = c(1,0,0),include.mean =FALSE))
ar1 
0.7622461
</code></pre>

<p>when comparing to a linear regression </p>

<pre><code>&gt; coef(lm(z[2:101] ~ z[1:100] + 0))
z[1:100] 
0.7586725 
</code></pre>

<p>which are very similar and can be explained by the different methods used.
However when I do this comparison with models that include an intercept, I get again similar results in the ar1 coefficient but very different measures for the intercept. while the intercept that I get in the arima model is the one that makes less sense to me.</p>

<pre><code>&gt; coef(arima(z, order = c(1,0,0)))
      ar1 intercept 
0.7274511 0.4241322 
&gt; coef(lm(z[2:101] ~ z[1:100]))
(Intercept)    z[1:100] 
  0.1578015   0.7130261 
</code></pre>

<p>Any ideas on these differencing and in what way the arima procedure is different?</p>
"
"0.107274293941992","0.107715935554017","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.0825797899793814","0.0829197658508324","161102","<p>I have a tobit regression model in hand and I want to calculate the marginal effects at means for the dummy variables in the reg equation. There are two dummy variables in the regression equation and three continuous variables. For SAS, this link has some information for the marginal effects for a tobit model : 
(<a href=""http://support.sas.com/rnd/app/examples/ets/margeff/"" rel=""nofollow"">http://support.sas.com/rnd/app/examples/ets/margeff/</a>). 
However this tells me how to compute the marginal effects for continuous variables and not for a dummy variable. Could someone please help on how to do this?</p>

<p>Edit: So this is what I have done following the SAS link. Note that f and g are dummy variables which takes the value of 1 or 0.</p>

<p>proc qlim data=tobit1; <br/>
model a = b c d f g; <br/>
endogenous a~censored(lb=0 ub=1); <br/>
run;<br/></p>

<p>Here I input the means of the variables. Note that for the marginal effect of dummy variable g, I cannot set the value of f to its mean value as it is a dummy variable and set it at 0.</p>

<p>data tobit2; <br/>
input b c d e f g a; <br/>
datalines; <br/>
0.48 0.34 5.59 5.57 0 1 . <br/>
0.48 0.34 5.59 5.57 0 0 . <br/>
run;<br/></p>

<p>data tobit; <br/>
set tobit1 tobit2; <br/>
run;<br/></p>

<p>proc qlim data=tobit noprint; <br/>
model a = b c d f g; <br/>
endogenous a~censored(lb=0 ub=1); <br/>
output out = tob2 marginal; <br/>
run; <br/>
proc print data = man2;run;<br/></p>

<p>After inputting the extra values and running the tobit regression, I get the marginal values for g 'Meff_g' for the last two added observations. In order to get the marginal effect of g at means I use: <br/>
Meff_g(at g =1) - Meff_g(at g=0) i.e. substracting the Meff_g for the last two observations. Is this correct?</p>

<p>I am open to solutions using R too.</p>
"
"0.0858194351535935","0.0861727484432139","161113","<p>I am working on example 7.3.1 from the Second Edition of the book <a href=""https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=an+introduction+to+generalized+linear+models+second+edition+pdf"" rel=""nofollow"">An Introduction to Generalized Linear Models</a> in section <em>7.3 Dose response models</em>. This example fits a simple logistic regression model on the following data: </p>

<p><img src=""http://i.stack.imgur.com/YkHCG.png"" alt=""enter image description here""></p>

<p>This seems easy enough. However, I am having an issue with the Deviance Statistic calculated for this example. The following is my R code that will reproduce a Deviance Statistic $D=11.23$ just like this example in the book has. </p>

<pre><code>#original data
#copied in by row
( df &lt;-  data.frame( 
  Trial = 1:8,
  Dose = c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839),
  Yes = c(6, 13, 18, 28, 52, 53, 61, 60),
  No = c(59, 60, 62, 56, 63, 59, 62, 60)- c(6, 13, 18, 28, 52, 53, 61, 60),
  Total = c(59, 60, 62, 56, 63, 59, 62, 60)
) )

#Logistic Regression Model
mle_beet &lt;- glm(cbind(Yes, No)~Dose, family=binomial(logit), data=df)
mle_beet$deviance
##
</code></pre>

<p>Section 5.6.1 of this same book derives the <em>Deviance Statistic</em> for the Binomial Model to be: </p>

<p>$D = 2\sum^{N}_{i=1}y_{i}[ log_{e}(\frac{y_i}{\hat{y_i}})+(n_i - y_i)log_{e}(\frac{n_i - y_i}{n_i - \hat{y_i}}) ]$</p>

<p>However, looking closely at the given data, it can be seen that for the last row, the number of beetles killed is the same as the total number of beetles ( $n_{8}=y_{8}$ ). This means that the very last part in the sum for <code>D</code> is: </p>

<p>$ y_{8}log_{e}(\frac{y_8}{\hat{y_8}})+(n_8 - y_8)log_{e}(\frac{n_8 - y_8}{n_8 - \hat{y_8}}) = 60log_{e}(\frac{60}{\hat{y_8}})+(0)log_{e}(\frac{0}{n_8 - \hat{y_8}})$</p>

<p>In particular, this value contains: </p>

<p>$0log_{e}(0)=0(-\infty)=$ <strong><em>undefined</em></strong></p>

<p>Here is the R code that agrees with this: </p>

<pre><code>sum( 2*(df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) + (df$Total-df$Yes)*
log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) )
</code></pre>

<p>My question is: What is the mathematical reasoning for computing the Deviance Statistic when $n_i=y_i$? What do the book and R do in the background to obtain $D=11.23$?</p>

<p>(Note that the book likely didn't use R to get this value, but the two agree)</p>

<p>Thank you!</p>

<p>EDIT: See the accepted answer and its comments for a great explanation.</p>

<p>If you happen to be computing the Deviance through the formula in R (you likely shouldn't since <code>mle_beet$deviance</code> shows this for you), you can replace <code>-Inf</code> or <code>Nan</code> in each vector that results from an individual operation. The following works for this example: </p>

<pre><code>x &lt;- df$Yes*(log(df$Yes/(mle_beet$fitted.values*df$Total))) 
x[is.na(x) | x==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 
y &lt;- (df$Total-df$Yes)*
    log((df$Total-df$Yes)/(df$Total-mle_beet$fitted.values*df$Total) ) ) 
    y[is.na(y) | y==-Inf ] &lt;- 0 #only in a case $n_i = y_i$ 

sum(x+y)*2 #the deviance
</code></pre>
"
"0.0572129567690623","0.0574484989621426","161121","<p>I have an R question. I'm wondering why there is a difference in p-values in the original regression analysis using lm versus in the k-fold cross-validation using the DAAG package.</p>

<p>So, first I run the regression.</p>

<pre><code>Model = lm(ExampleData$DependentVariable ~ ExampleData$IV1  + 
           ExampleData$IV2  + ExampleData$IV3  + ExampleData$IV4)
</code></pre>

<p>This gives me the p-values for the predictors.</p>

<pre><code>Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)     -55.6644    23.4690  -2.372  0.01958 * 
ExampleData$IV1   1.2118     0.6277   1.931  0.05631 .
ExampleData$IV2   6.2636     2.0563   3.046  0.00295 **
ExampleData$IV3   2.1531     0.7490   2.875  0.00492 **
ExampleData$IV4  -5.4468     1.8859  -2.888  0.00473 **
</code></pre>

<p>Then, I go to cross-validate the model using cv.lm in the DAAG package.</p>

<pre><code>cv.lm(df=ExampleData, Model_forCV, m=5)
</code></pre>

<p>This gives me the cross-validation results along with the p-values for the predictors.</p>

<pre><code>Response: DependentVariable
           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
IV1         1  26755   26755    3.23 0.07541 .  
IV2         1 104332  104332   12.58 0.00059 ***
IV3         1  36119   36119    4.36 0.03938 *  
IV4         1  69167   69167    8.34 0.00473 ** 
Residuals 102 845806    8292   
</code></pre>

<p>Why are the p-values different?</p>

<p>Thank you! </p>
"
"0.06396603026469","0.0642293744423385","161234","<p>Les say I have a data set with several measures and one factor (classification) like the one bellow (for the sake of simplicity, I'm simulating 10 rows and 5 variables only)</p>

<p>I'd like to know how much each variable contribute to the overall classification. I thought about running a linear regression, but I'm wondering if it makes sense to use it to ""explain"" a factor </p>

<p>When I run <code>lm(classification ~ ., data =data)</code> I get a warning   saying </p>

<pre><code>Warning messages:
1: In model.response(mf, ""numeric"") :
  using type = ""numeric"" with a factor response will be ignored
2: In Ops.factor(y, z$residuals) : â€˜-â€™ not meaningful for factors
</code></pre>

<p>but I do get a result (intercept and coefficients for each variable).</p>

<p>My questions are: do they make any sense? And: is there a better way to get to the answer I'm looking for?</p>

<pre><code>   classification  variable_1  variable_2 variable_3 variable_4 variable_5
1               5 -0.90174176 -0.64796703  1.2106427 -0.9229394 -0.6578518
2               5  1.75760214  0.18486432  0.2018499  0.1301168 -0.6510428
3               8 -0.29445029 -0.23108298 -2.6244614  0.3745607  0.3124868
4               4  0.78639724  1.04943276 -0.6047869 -0.4275781  0.6395614
5               3 -2.06554518  0.07336021  2.8142735  1.0558045 -0.1818247
6               4  0.04374419 -0.13775079  0.6132946 -0.5890983  1.9965892
7              10 -1.46731867  1.00367532 -0.8626940 -1.8378582  0.2702731
8               8  0.27206146 -0.13775707  2.6827356  1.5554446  0.1549394
9               5  0.58075881  2.03567118  0.2056770 -0.2935464 -1.3586576
10              9  0.57725709 -0.25396790  0.6640166 -1.9626897  0.3650243
</code></pre>

<p>Code to reproduce it:</p>

<pre><code>data &lt;- data.frame(classification=sample(3:10,replace=TRUE,size=10))

for(i in 1:5){
  data[,paste0(""variable_"",i)]&lt;-rnorm(10)
}
</code></pre>

<p>thanks</p>
"
"0.0809113394062735","0.0812444463702388","161319","<p>First let me start off by saying I know the consequences that come with removing/ignoring outliers.. but for this particular case I am just looking at weekly trends in the equipment I collect data from (a little over 100 sensors). I need to ignore ""leverage points"" that ruin R sqd values. I need to see if any of my sensors are trending out of their normal operating limits over time using simple regressions. </p>

<pre><code>rm(list=ls())
mtcars &lt;- mtcars
myList &lt;- lapply(mtcars, function(x) summary(lm(mtcars$wt ~ x))$r.squared)
myList

## For example, Rsqd value is .75 without outliers added to my data set 
##$mpg
##[1] 0.7528328
##Works Great! 

##Now we add some outliers 
mtcars$mpg[c(5,10,15,20)] &lt;- 100

## without lmrob the value for mpg is myList$mpg [1] 0.001461735
##using robust regression
require(robustbase)
summary(lmrob(mtcars$wt ~ mtcars$mpg))$r.squared

##summary(lmrob(mtcars$wt ~ mtcars$mpg))$r.squared
## [1] 0.7187418
##gives me a representative r squared value for the data set and ""Leverage""
##points/extreme outliers don't ruin relationships I am trying to see.

#but if I try this... 
myList &lt;- lapply(mtcars, function(x) summary(lmrob(mtcars$wt ~ x))$r.squared)

#it doesn't work and by that I mean I receive the error
#Warning messages:
#1: In lmrob.S(x, y, control = control, mf = mf) :
# S-estimated scale == 0:  Probably exact fit; check your data
#2: In seq_len(ar) : first element used of 'length.out' argument

#Error in summary(lmrob(mtcars$wt ~ x)) : 
# error in evaluating the argument 'object' in selecting a method for     function 'summary': Error in numeric(seq_len(ar)) : invalid 'length' argument 
</code></pre>

<p>I have also tried just removing any data points in each column that are outside of +-3 Sigma but I wasn't able to get that working... </p>
"
"0.160324589970304","0.160984636359424","161941","<p>Something I rather vaguely asked a few months back, saw the tumbleweed roll by (actually hacked some hardware in the time, to get a few answers) before the question was deleted, so I'll try again on a slightly more specific note, as there will be a cleaner / quicker way to get the answer.</p>

<p>If I know a regression equation derives a particular answer from a given set of variables and coefficients, but I don't know the values for at least one set, how can I model this in R / get R to have a guess at deriving the unknowns.    </p>

<p>I ask as I was curious as to the maths embedded in number of Bioelectrical Impedance Analysis (BIA) devices I had around (A mix of Salter and Withings).
A quick Google revealed <a href=""http://pubs.sciepub.com/ijcn/2/1/1/"" rel=""nofollow"">dozens of published regression formula</a>, <a href=""http://ajcn.nutrition.org/content/64/3/436S.full.pdf"" rel=""nofollow"">to estimate: Total body Water / Fat / Lean Mass</a>, so was interested in which had made their way into the devices eg.</p>

<blockquote>
  <p>Lukaski &amp; Bolonchuk's (1988):</p>
  
  <p><strong>TBW</strong> = (0.372 * HeightCM * HeightCM /
  Resistance)  + (0.142 * massKg) - (0.069 * ageYears) + (3.05 * isMale)</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>Matias et al. (2015): </p>
  
  <p><strong>TBW</strong> = 0.286 + (0.195 * HeightCM * HeightCM /
  Resistance) + (0.385 * massKg) + (5.086 * isMale)</p>
</blockquote>

<p>Anyway the devices show the values of all but any Resistance (R) and possibly imaginary Rectance (X) values used (Withings), so thought I'd pester / learn a bit of R to see if there was an alternative to an afternoon without socks, but with a multimeter, a few bits of wire and a tweaked iPad.</p>

<p><strong>FYI:</strong> My original five min play, in April, with a <strong>Salter 9141 WH3R</strong> suggested that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= x + (0.1 * AgeYears) + (0.4 * MassKg) - (8 * isMale) + y</p>
  
  <p><strong>TBW %</strong>  ~= x - (0.2 * AgeYears) - (0.27 * MassKg) + (7.58 * isMale) + y</p>
</blockquote>

<p>where x:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM</strong></p>

<p>and with a <strong>Withings WS-50</strong> that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= X + (0.12 * AgeYears) + (0.29 * BodyMassKg) - (16.64 * isMale) + (5 * isJapanese) + z</p>
</blockquote>

<p>where X:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM + d * Reactance</strong></p>

<p>So is there a way that I a can ger <strong>R</strong> to have a guess at solving these unknowns, or must I stick with a: screwdriver, multimeter, tweaked iPad and / or logging proxy (mitProxy / HoneyProxy).</p>

<pre><code># Quick play to try to identify a few of the constants used by the Salter 9141 WH3R body fat scale
# sex ( 1 = male, 0 = female)
sex &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
ageYrs &lt;- c(33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53, 33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53)
heightCM &lt;- c(191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171, 191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171)
heightCM2 &lt;- heightCM * heightCM
massKg &lt;- c(81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6, 81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6)
bodyWaterPct &lt;- c(62, 61.6, 59.9, 59.6, 57.9, 57.5, 58, 55.9, 55.5, 53.9, 53.2, 51.2, 49.2, 54.3, 54, 52.3, 52, 50.3, 50, 50.4, 48.4, 47.9, 46.4, 45.7, 43.6, 41.6)
bodyFatPct &lt;- c(17.1, 17.7, 18.1, 18.7, 19.2, 19.8, 22.6, 23.7, 24.2, 24.7, 29.1, 30.1, 31.1, 25.2, 25.7, 26.2, 26.7, 27.2, 27.7, 30.5, 31.6, 32.3, 32.9, 37, 38.1, 39.1)

bodyFat.Salter = data.frame(sex, ageYrs, heightCM, heightCM2, massKg, bodyWaterPct, bodyFatPct)
bodyFat.Salter
summary(bodyFat.Salter)
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater1 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyWater2 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater3 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)
summary(fitBodyWater1)
summary(fitBodyWater2)
summary(fitBodyWater3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt; summary(fitBodyFat1)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42607 -0.20940  0.08889  0.15588  0.27051 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 101.747418   6.105948  16.664 1.39e-13 ***
sex          -8.007692   0.092545 -86.528  &lt; 2e-16 ***
ageYrs        0.105000   0.005899  17.801 3.80e-14 ***
heightCM     -0.591111   0.006422 -92.051  &lt; 2e-16 ***
massKg        0.400794   0.079446   5.045 5.39e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2359 on 21 degrees of freedom
Multiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 
F-statistic:  4442 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat2)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.5345 -0.3141  0.1192  0.2024  0.3494 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.843e+01  8.409e+00   5.759 1.02e-05 ***
sex         -8.008e+00  1.238e-01 -64.696  &lt; 2e-16 ***
ageYrs       1.050e-01  7.889e-03  13.310 1.05e-11 ***
heightCM2   -1.626e-03  2.365e-05 -68.758  &lt; 2e-16 ***
massKg       3.972e-01  1.063e-01   3.738  0.00121 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3156 on 21 degrees of freedom
Multiple R-squared:  0.9979,    Adjusted R-squared:  0.9975 
F-statistic:  2481 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat3)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.12607 -0.02949 -0.01111  0.03162  0.17393 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 249.776918   9.660823   25.86  &lt; 2e-16 ***
sex          -8.007692   0.026174 -305.94  &lt; 2e-16 ***
ageYrs        0.105000   0.001668   62.94  &lt; 2e-16 ***
heightCM     -2.225111   0.104938  -21.20 3.53e-15 ***
heightCM2     0.004500   0.000289   15.57 1.20e-12 ***
massKg        0.400794   0.022469   17.84 9.48e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.06673 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 4.448e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.19309 -0.12797 -0.06759  0.18355  0.31346 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.639382   4.922117   0.130  0.89788    
sex          7.576923   0.074602 101.565  &lt; 2e-16 ***
ageYrs      -0.202500   0.004755 -42.587  &lt; 2e-16 ***
heightCM     0.432037   0.005177  83.460  &lt; 2e-16 ***
massKg      -0.269841   0.064043  -4.213  0.00039 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1902 on 21 degrees of freedom
Multiple R-squared:  0.999, Adjusted R-squared:  0.9988 
F-statistic:  5087 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater2)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25119 -0.16896 -0.09268  0.25881  0.39269 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.960e+01  6.631e+00   5.972  6.3e-06 ***
sex          7.577e+00  9.760e-02  77.635  &lt; 2e-16 ***
ageYrs      -2.025e-01  6.221e-03 -32.553  &lt; 2e-16 ***
heightCM2    1.188e-03  1.865e-05  63.728  &lt; 2e-16 ***
massKg      -2.670e-01  8.378e-02  -3.187  0.00443 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2488 on 21 degrees of freedom
Multiple R-squared:  0.9982,    Adjusted R-squared:  0.9979 
F-statistic:  2970 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater3)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.078205 -0.027991 -0.002778  0.038408  0.069017 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.200e+02  6.685e+00  -17.95 8.46e-14 ***
sex          7.577e+00  1.811e-02  418.32  &lt; 2e-16 ***
ageYrs      -2.025e-01  1.154e-03 -175.41  &lt; 2e-16 ***
heightCM     1.763e+00  7.262e-02   24.28 2.58e-16 ***
heightCM2   -3.667e-03  2.000e-04  -18.34 5.63e-14 ***
massKg      -2.698e-01  1.555e-02  -17.35 1.59e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.04618 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 6.911e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+heightCM2+massKg
Crit= -53.5831033999792
Mean crit= 117.663945821469
Completed.
</code></pre>

<p><strong>Withings</strong></p>

<pre><code>...
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Withings)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Withings)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Withings)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3348 -0.5042  0.0597  0.5361  6.5767 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 120.39668    9.97149  12.074  &lt; 2e-16 ***
sex         -16.64221    0.19636 -84.754  &lt; 2e-16 ***
ageYrs        0.12039    0.01490   8.082 2.78e-13 ***
heightCM     -0.61641    0.01784 -34.552  &lt; 2e-16 ***
massKg        0.29065    0.11652   2.494   0.0138 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.9957 on 139 degrees of freedom
Multiple R-squared:  0.9848,    Adjusted R-squared:  0.9844 
F-statistic:  2251 on 4 and 139 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+massKg
Crit= 414.926000298605
Mean crit= 752.216409496029
Completed.
</code></pre>
"
"0.0286064783845312","0.0287242494810713","162699","<p>At the zero-order level, X is not correlated with Y. When I add X and A into a regression analysis to predict Y, only A is a significant predictor. A itself is correlated highly with Y at zero-order. However, when I add an interaction term into the analysis, A*X and X significantly predicts Y, and A no longer predicts. What does this mean? What kind of variable is X, and what kind of variable is Y?</p>

<pre><code># additive model
Call:lm(formula = Y ~ X + A, data = dat)
Residuals:
Min      1Q  Median      3Q     Max 
-30.247  -8.150  -1.416   8.692  24.263 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  67.3713     9.8646   6.830 2.04e-09 ***
X             0.4805     2.1935   0.219    0.827
A           -10.4961     1.8172  -5.776 1.69e-07 ***

# interaction model
Call:lm(formula = Y ~ X * A, data = dat)

Residuals:
Min      1Q  Median      3Q     Max 
-20.778  -8.834  -2.135   8.925  24.410 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)     -17.620     40.762  -0.432   0.6668  
X                30.294     14.057   2.155   0.0345 *
A                10.275      9.841   1.044   0.2999  
X:A              -7.297      3.400  -2.146   0.0352 *
</code></pre>
"
"0.0404556697031367","0.0406222231851194","163000","<p>I have a problem to determine my R-Squared value. I do a polynomial regression:</p>

<blockquote>
  <p>fit3 &lt;- lm(value ~ date + I(date^2)+ I(date^3),data=training)</p>
</blockquote>

<p>I have a R-Squared value (0.9416) when I do</p>

<pre><code>summary(fit3)
</code></pre>

<p>But when I try to compute it in the testing dataset my R-Squared value is -84.20259. I don't understand why because when I plot it the results look goods.</p>

<p>I use 2 methods.</p>

<ul>
<li><p>First one</p>

<p>pred.lin &lt;- predict(fit3, newdata=testing)
actual &lt;- testing$value
SS.total      &lt;- sum((actual - mean(actual))^2)
SS.residual   &lt;- sum((actual - pred.lin)^2)
SS.regression &lt;- sum((actual - mean(actual))^2)</p>

<p>test.rsq &lt;- 1 - SS.residual/SS.total<br>
test.rsq</p></li>
<li><p>And the second method is:</p>

<p>1 - sum((actual-pred.lin)^2)/sum((actual-mean(actual))^2)</p></li>
</ul>

<p>Here the graph the points are the real data and the curve the model. The blue points are the testing dataset.</p>

<p><a href=""http://i.stack.imgur.com/JI6vb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JI6vb.png"" alt=""enter image description here""></a></p>

<p>Can you help please? I am new in this area :)</p>
"
"0.0700712753800578","0.0703597544730292","163074","<p>So I've been learning how to forecast over this summer and I've been using Rob Hyndman's book Forecasting: principles and practice.  I've been using R, but my questions aren't about code.  For the data I've been using, I've found that an average forecast of multiple models has produced higher accuracy levels that any sole model by itself.  </p>

<p>Recently I read an blog that talked about averaging forecasting methods and assigning weights to them.  So in my case, lets say I assign 11 different models to my set of data (Arima, ETS, Holt Winters, naive, snaive, and so forth) and I want to average a few of these to get a forecast.  Has anyone had any experience with this or can point me to an article that might give some insight on the best way of going about this?</p>

<p>As of right now, I'm using cross validation and Mean Absolute Error to figure out which models perform best and which perform worst. I can even use this to identify the top k # of models.</p>

<p>I guess my questions are</p>

<p>1) How many models would you suggest selecting? (2,3,4,5,6, etc)</p>

<p>2) Any ideas on weights?  (50% to the best, 25% to the second best, 15% third best, 10% to the 4th best, etc)</p>

<p>3) Are any of these forecasting models redundant and shouldn't be included? 
(Arima, snaive, naive, HW's ""additive"", ETS, HoltWinters exponential smoothing, HoltWinters smoothing w/ trend, HoltWinters w/ trend/seasonality, multiple regression)</p>
"
"0.0993902382172794","0.107476300250388","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.0858194351535935","0.0861727484432139","163604","<p>I'm using a plate reader to measure optical density of different bacterial
strains so I can compare their responses (growth rates and changes in them over
time) to stress conditions. The growth curves often don't follow any standard
shape so I'm fitting them empirically with the <code>loess</code> or <code>locfit</code> functions in
R, breaking the fits into intervals, and taking the derivatives to get growth
rates. My plots look like this:</p>

<p><a href=""http://i.stack.imgur.com/fiiLH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fiiLH.png"" alt=""locfit fitted data points""></a>
<a href=""http://i.stack.imgur.com/4dui4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4dui4.png"" alt=""simplified fitted curves""></a>
<a href=""http://i.stack.imgur.com/4t7K4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4t7K4.png"" alt=""derivatives of simplified curves""></a></p>

<p>As you can see the fitted curves have confidence intervals, but I'm not sure
how to transform them into a meaningful form (95% confidence or standard
deviation for example). And assuming that's doable, how do I go on to calculate
uncertainty in the rates?</p>

<p>I suppose I could just use the worst-case difference in slopes like this:</p>

<p><a href=""http://i.stack.imgur.com/fcEaY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fcEaY.png"" alt=""bad idea""></a></p>

<p>But that seems like a bad idea.</p>

<p>I could fit each well separately or split them into groups--there are a few
replicates for each strain and I could add more if needed--and just use the
standard deviation of the final calculated rates. Is that the best way? If so,
how do I decide the optimal group size to balance accurate fits with a good
number of replicates? I would also be open to using a different type of fit of course.</p>

<p>I've found a couple related questions, but neither one quite answers it:</p>

<ul>
<li><p><a href=""http://stats.stackexchange.com/questions/70629/calculate-uncertainty-of-linear-regression-slope-based-on-data-uncertainty"">This one</a> seems to rely on the true relationship being linear, which my curves violate</p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/18391/how-to-calculate-the-difference-of-two-slopes"">This one</a> may well be correct but my stats knowledge is too basic to understand the answer</p></li>
</ul>

<p>EDIT: I'm using <code>deg=1</code> for both types of fits because I expect growth during log-phase to be linear on a log-transformed scale, but maybe higher-degree polynomials would be more accurate?</p>

<p>EDIT: <a href=""http://stats.stackexchange.com/questions/147106/determining-if-two-growth-curves-are-significantly-different"">This answer</a> looks very promising and I'm off to read the suggested paper.
EDIT: Nope, also depends on having a known underlying physical model.</p>
"
"0.0404556697031367","0.0406222231851194","164314","<p>I have a data set with performance and training data that looks something like (this is not the exact data, but gives a general idea):</p>

<pre><code>&gt;dat
Performance Training
1           1
0           1
1           2
0           2
1           3
1           3
</code></pre>

<p>I want to find if there is are any significant differences between performance means for the respective levels of performance in R.  I have tried linear regression and anova, such as: <code>summary(lm(performance~training))</code> or <code>summary(aov(performance~training))</code> both of which yield non-significant results.  However, when I do a T-test to compare some of the means manually it is telling be significant differences exist.  Any thoughts on how to code what I am looking for or what might be going on here?</p>
"
"0.0404556697031367","0.0406222231851194","164333","<p>I have been looking at some tutorials and articles and couldn't get a scenario where two variables are in different scales and used in modeling.</p>

<p>So, firstly lets assume I have one metric of numeric type, other in percentages, and other in decimals. </p>

<ol>
<li>If I want to use those variables in a regression model for
prediction then do I need to do some standardization before fitting a<br>
model to the variables? If so how do we it in R or Python?</li>
<li>Moreover, if I want to use these features in k-means
clustering, do I need to follow the same steps as mentioned above?</li>
</ol>
"
"0.0404556697031367","0.0406222231851194","164438","<p>I just ran a linear regression in R, where the following is my result:</p>

<pre><code>Call:
lm(formula = Posttest ~ TotalHints + Pretest, 
    data = all)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51904 -0.09000  0.01243  0.11979  0.41820 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept)      0.3205183  0.0450642   7.112 1.88e-10 ***
    TotalHints      -0.0007066  0.0003323  -2.127    0.036 *  
    Pretest         0.4323069  0.0770656   5.610 1.88e-07 ***
</code></pre>

<p>All terms in the regression significantly contribute to the model, but the coefficient term of TotalHints is significant with an estimate of... 0?  I'm not sure how to conceptually understand this. Does that mean the model is saying that the TotalHints term significantly had no effect?</p>
"
"0.064873395163555","0.0759972207238908","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.051172824211752","0.0642293744423385","165018","<p>I think I understand why orthogonality matters when doing regression with polynomial fits (so that the linear and quadratic, cubic, etc... can be evaluated independently). However, I don't understand what orthogonality even means when it comes to doing simply a linear regression. Specifically, in a regression with both continuous (age) and categorical (sex) variables, how I set up my continuous variable in the model will affect all coefficients.</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
agevar=1:60
sexvar &lt;- rep(c(""male"",""female""),each=30)
set.seed(8093)
xvals &lt;- sample(-100:100,60)
m1 &lt;- summary(lm(xvals~sexvar*poly(agevar,1,raw=T))) # uses raw contrasts (i.e. 1:60)
# m1b &lt;- summary(lm(xvals~sexvar*agevar)) # alternative to m1, same result

coef(m1)

                                    Estimate Std. Error    t value  Pr(&gt;|t|)
(Intercept)                      25.13963663 28.0506961  0.8962215 0.3739710
sexvar1                          35.44078606 28.0506961  1.2634548 0.2116605
poly(agevar, 1, raw = T)          0.09955506  0.7997637  0.1244806 0.9013805
sexvar1:poly(agevar, 1, raw = T) -1.26395996  0.7997637 -1.5804168 0.1196438

m2 &lt;- summary(lm(xvals~sexvar*poly(agevar,1,raw=F))) # uses set of contrasts that sum to zero

coef(m2)

                                    Estimate Std. Error    t value   Pr(&gt;|t|)
(Intercept)                        28.176066   13.85039  2.0343159 0.04666608
sexvar1                            -3.109993   13.85039 -0.2245419 0.82315296
poly(agevar, 1, raw = F)           13.354858  107.28465  0.1244806 0.90138052
sexvar1:poly(agevar, 1, raw = F) -169.554469  107.28465 -1.5804168 0.11964377
</code></pre>

<p>Any idea on why the result is different and which way is correct? My actual data set uses the same variables, but is much larger with an unequal number of males/females and different sample size at each age. </p>
"
"0.0572129567690623","0.0574484989621426","166461","<p>I have created an example in R to illustrate the problem:</p>

<pre><code>&gt; set.seed(10)
&gt; Ydata&lt;-rnorm(200,15,5)*rep(1:200)^3
&gt; Xdata&lt;-rep(1:200)

&gt; lm.test&lt;-lm(log(Ydata)~Xdata)
&gt; summary(lm.test)$r.squared 

[1] 0.7665965

&gt; Yfit&lt;-fitted.values(lm.test)
&gt; lm.test2&lt;-lm(Yfit1~log(Ydata))
&gt; summary(lm.test2)$r.squared 

[1] 0.7665965

&gt; ExpYfit&lt;-exp(fitted.values(lm.test))
&gt; lm.test3&lt;-lm(ExpYfit~Ydata)
&gt; summary(lm.test3)$r.squared

[1] 0.6088178
</code></pre>

<p>When calculating the r-squared of some exponential model, fitted values for log(Y) run against observed log(Y) give the same r-squared as the original regression as expected:</p>

<p>log(Y) = fitted values = a + bX</p>

<p>but when we want to estimate the level of Y, exponentials of both sides are taken:</p>

<p>Y= exp(a + bX) = exp(fitted values)</p>

<p>but when running level Y against exponential fitted values, the R-squared is calculated incorrectly. </p>

<p>Why is this? and does this mean my predictions of Y are wrong?</p>
"
"0.0862517776135472","0.0952675579132743","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.0286064783845312","0.0287242494810713","167361","<p>I have run a probit regression and am now trying to run post-hoc tests.  I am trying to compare differences between a 3 level factor variable.</p>

<p>I am confused about the difference between running a 'simultaneous tests for general linear hypotheses' and running the same thing but with a 'Tukey' adjustment- they get very similar answers but is either 'better' or 'worse'? Or does it not matter?  For example:  </p>

<pre><code>library(lsmeans)
lsmeans(m1, pairwise~Name.Origin, adjust=""tukey"") 

library(multcomp)
summary(glht(m1, lsm(pairwise~Name.Origin)))
</code></pre>

<p>In addition, is there a more formal name for the first method which just fits the model?</p>
"
"0.0990957479752576","0.0995037190209989","167363","<p>I have no training in Bayesian data analysis, so I can't wrap my head around how to start solving the following problem and am hoping you can help:</p>

<p>I am using linear regression to forecast the net scores (home - visitor) of (American) pro-football games from differences in team-strength scores (home - visitor). Those strength scores fall on a 0-100 scale, and they represent the percent chance that the team in question would beat another team selected at random from the 31 others in the league. The differences between those strength scores and the net game scores are both normally distributed.</p>

<p>Right now, I am using team-strength scores that are fixed for the entire season in a mixed-effects model that also includes random intercepts for each team as the home team. The strength scores are fixed because they come from a preseason survey. I would like to see if I can make the predictions more accurate by using Bayesian updating to allow that team-strength score to vary over the course of the season, as we learn more about how teams are performing relative to preseason expectations.</p>

<p>The single piece of information that strikes me as most useful in that regard is the cumulative sum of each team's prediction errors --- in other words, the cumulative sum of the differences between the team's predicted game performance (based on the preseason strength scores and where each game is played) and its actual game performance. </p>

<p>How might I go about doing that? In R, I have gotten as far as computing those cumulative errors, which turn out to be normally distributed for the season with a mean of ~0 and sd of ~50. I have tinkered with algebraic ways to adjust the strength scores as a function of that cumulative error. The forecasts based on those algebraic adjustments are slightly more accurate, but the approach seems clunky, and I'd like to use this problem as an opportunity to learn about Bayesian updating if I can. Any suggestions on how to do that in the context of this problem --- and, ideally, in R --- would be much appreciated.</p>
"
"NaN","NaN","167523","<p>I have run a probit regression and the size of my coefficients seem to be quite big with respect to other similar studies.  For example, 0.254 vs 1.207 - does this mean anything in particular or is it all just relative to your model etc.?</p>
"
"0.0404556697031367","0.0406222231851194","167854","<p>I am running a probit glmer, with a binary response varaible and a categorical explanatory variable with three dummy levels and have tried to calculate the marginal effect using the following code:</p>

<pre><code>    ProbitScalar&lt;- mean(dnorm(predict(m1,type = ""link""))) 
</code></pre>

<p>The ProbitScalar value is then multiplied by the coefficient estimates from the regression output.</p>

<p>I get the following values: </p>

<p>-0.2946806 (referring to the intercept and reference level)
-0.1527443
-0.07252501</p>

<p>I am slightly confused how to interpret them as they seem quite low compared to what I would expect from the raw data.</p>

<p>Is it correct that the second variable has a 15% lower chance of achieving success (the binary response variable) than the reference group and the final variable has a 7% less chance of achieving success than the reference group?</p>
"
"0.100122674345859","0.107715935554017","168167","<p>My dependent variable is a probability. As such, values lie between 0 and 1. The most common values are 0, 0.5, and 1 each occurring in 20% to 30% of the observations but any value in between is possible and some do occur. </p>

<p><strong>Question 1: Which regression model is best to explain such data?</strong></p>

<ul>
<li><p>Ordinary least squares (OLS, function <code>lm</code> in Râ€™s <code>stats</code> package) is not suitable as it does neither account for the limited interval nor the accumulation at the margins.</p></li>
<li><p>Logit regression (function <code>glm</code> with parameter <code>family=""binomial""</code> in Râ€™s <code>stats</code> package) accounts for the accumulation at 0 and 1 but does not allow intermediate values.</p></li>
<li><p>Ordered logit regression (function <code>polr</code> in Râ€™s <code>MASS</code> package) could be applied when I divide the [0, 1] interval in subintervals. However, I lose the continuous nature of the dependent variable.</p></li>
<li><p>For probit and ordered probit regressions, the same applies as for logit and ordered logit.</p></li>
<li><p>Left- and right-censored tobit regression (function <code>tobit</code> with parameters <code>left=0</code> and <code>right=1</code> in Râ€™s <code>AER</code> package) might be appropriate. However, I found the following quote: â€œSome researchers have considered using censored normal regression techniques such as tobit ([R] tobit) on proportions data that contain zeros or ones. However, this is not an appropriate strategy, as the observed data in this case are not censored: values outside the [0, 1] interval are not feasible for proportions data.â€ (p. 302 in Baum (2008), <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0147"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0147</a>). </p></li>
</ul>

<p>Below you find a code example </p>

<pre><code># Load libraries
library(stats, MASS, AER)
# Generate data
set.seed(123)
data &lt;- data.frame(x1 &lt;- runif(60, min = 0, max = 1), x2 &lt;- runif(60, min = 0, max = 1))
data$y  &lt;- -0.7 + data$x1 + 2 * data$x2 + rnorm(60, mean = 0, sd = 0.5)
    data$y  &lt;- ifelse(data$y &lt; 0, 0, data$y)
data$y  &lt;- ifelse(data$y &gt; 0.4 &amp; data$y &lt; 0.6, 0.5, data$y)
data$y  &lt;- ifelse(data$y &gt; 1, 1, data$y)
    data$yCat &lt;- data$y
    data$yCat &lt;- ifelse(data$yCat &gt; 0 &amp; data$yCat &lt; 0.5, 0.25, data$yCat)
    data$yCat &lt;- ifelse(data$yCat &gt; 0.5 &amp; data$yCat &lt; 1, 0.75, data$yCat)
    data$yCat &lt;- as.factor(data$yCat)
    hist(data$y, breaks=101)
# Different regression models
summary(lm(y ~ x1 + x2, data=data)) # OLS
summary(glm(y ~ x1 + x2, data=data, family=""binomial"")) # Logit
summary(polr(yCat ~ x1 + x2, data=data)) # Ordered logit
summary(tobit(y ~ x1 + x2, data=data, left=0, right=1)) # Tobit
</code></pre>

<p>To make matters worse, my data is panel data. I know how to handle individual, time, and mixed effects and random and fixed effects models using plm from Râ€™s plm package and F-test, LM-test, and Hausman test do decide which of these is best. </p>

<p><strong>Question 2: For the dependent variable described above, which panel regression model is best?</strong> </p>

<p>Below your find a code example for the data structure. This extends the prior example.</p>

<pre><code># Load library
library(plm)
# Generate data (builds on prior example)
data$id &lt;- rep( paste( ""F"", 1:15, sep = ""_"" ), each = 4)
    data$time &lt;- rep( 1981:1984, 15 )
pData &lt;- pdata.frame(data, c( ""id"", ""time"" ))
# Panel regression example
summary(plm(y ~ x1 + x2, data=pData, model=""within"", effect=""twoways"")) # Based on OLS
</code></pre>
"
"0.0572129567690623","0.0574484989621426","168482","<p>I am running a probit regression with a random effect:</p>

<pre><code>m1&lt;-glmer(Binary~Explan+(1|Random),family=binomial(link=""probit""))
</code></pre>

<p>where Explan is a three-level categorical variable. </p>

<p>I want to calculate the mean predicted probabilities for each level of Explan. I tried doing so using this code:</p>

<pre><code>newdata=data.frame(Explan=""First"")
predict(m1,newdata,type=""response"")
</code></pre>

<p>where First is a level of the categorical Explan variable.</p>

<p>However I get the following error message:</p>

<pre><code>Error: (p &lt;- ncol(X)) == ncol(Y) is not TRUE
</code></pre>

<p>Were this a logit model, I would simply strip the model of the intercept and then back-transform the model summary coefficients to get the predicted values that I'm after, but I am unsure of how I would go about this with a mixed-effects probit model. </p>

<p>Any help in extracting the predicted probabilities would be greatly appreciated.</p>
"
"0.134176277047853","0.128604641888975","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.0286064783845312","0.0287242494810713","168717","<p>this is my first post.  I have an irregular time series that exhibits large shifts in both mean and in the direction of the trend.  It looks something like this (though this is far cleaner than reality):</p>

<pre><code>set.seed(1)
x = sort(rep(seq(as.POSIXct('2015/01/01'),as.POSIXct('2015/01/10'),by='day'),100) + runif(1000,0,80000))
y = c(seq(300),seq(90,70,-.2),seq(20,50,.1),seq(238.5,90,-.5)) + runif(1000,50,80)
plot(x,y)
</code></pre>

<p>The task is to:
 1. accurately partition/segment the data
 2. extract the change point indices
 3. fit regression separately for each segment</p>

<p>I have tried several routes, including:
a) hierarchical clustering based on dissimilarity matrix
b) function cpt.mean/var/meanvar from package changepoint (does not seem to work well)
c) function 'breakpoints' from package strucchange (slow and often inaccurate)
d) various types of kmeans (inappropriate, I know)</p>

<p>I have also explored some other packages, such as TSclust, urca, and DTW, but these seem better suited to clustering <em>sets</em> of time-series, not individual values within a time series.</p>

<p>Can someone point me in the correct direction? Perhaps I am not considering the appropriate data model?</p>

<p><strong>UPDATE</strong>
Thank you all for your very considered responses.  I went back to the strucchange package, and after some fiddling, have gotten it to work quite well.  I had not initially appreciated the h 'minimal segment size' argument.
Finished product:
<a href=""http://i.stack.imgur.com/qBgeY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBgeY.jpg"" alt=""enter image description here""></a></p>
"
"0.114624397492221","0.115096299024505","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.0495478739876288","0.0497518595104995","168893","<p>I am performing a penalized B-spline regression on a simple time series of count data in R using the mgcv package. When I calculate a pointwise confidence band from the standard error of the fit based on the estimated degrees of freedom, it turns out to be slightly <em>wider</em> than the simultaneous confidence band produced by posterior simulation of the fitted GAM (as per:  <a href=""http://stats.stackexchange.com/questions/33327/confidence-interval-for-gam-model"">Confidence interval for GAM model</a>). As per Wood (2006), I'm using the Bayesian posterior covariance matrix from mgcv.</p>

<p>Some difference may be attributable to using the t distribution for calculating the pointwise band and assuming a multivariate normal for the posterior simulation, but I had expected the latter to reflect more uncertainty about the mean response due to the multiple comparisons issue. Am I correct in assuming that approximate equivalence of the simultaneous and pointwise confidence bands is a special case, and if so, are there specific conditions required to obtain this result?</p>

<p>Wood, S.N. (2006) Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC.</p>
"
"0.0952081150392732","0.103566754353222","171193","<p>I recently employed multiple quantile regression in my area of research and found some interesting quantile differences across the distribution of Y, but I don't quite understand what they all really mean.  Unlike the traditional methods such as dividing the sample into multiple groups where I have access to the groups' data on various variables which then allows me to make sense of, for example, why the correlation between X and Y is 0 for group 1 and .7 for group 2, I feel like I have no idea where those quantile regression estimates come from, especially when there are more than 2 predictors in the QR model. Another way of putting this is I don't know which specific data points contribute heavily to a given quantile regression estimate and so this makes it very difficult for me to understand what the quantile differences really mean.  </p>

<p>Based on my understanding of QR, it uses all the data points in the full sample but weights the data points that are farther from a quantile of interest less heavily than the data points that are closer to that same quantile of interest, is this correct? If so, as a follow up, can I divide my full sample into 10 groups, e.g., 10th quantile, 20th quantile, 30th quantile group, and then examine how the 10 groups differ on various variables of interest in order to make sense of the 10 quantile regression estimates that I got? I know the subgroups approach is not ideal, which is why I used QR, but if you think this is a terrible idea, please let me know why. And if you know of any other methods  that allow me to have a more fine-grained understanding of my results, please help.  I conducted QR using the <code>quantreg</code> package in R.  </p>
"
"0.10703564115707","0.107476300250388","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.0707974219804893","0.0710888905739589","171745","<p>[Moved from stack overflow)
While I have done a good amount of reading regarding the usage of PSM, I am still struggling a bit to see if it can be used in my application. </p>

<p>I am trying to analyze the impact of an advertising campaign on in-store sales lift. While I know Propensity Scores are typically used in non-randomized studies, this study will be randomized (some stores will randomly see the advertising campaign, some will not). I have sales level data by store (such as pre-sales volume etc), and I want to conduct a sub-group analysis, to make sure I am comparing stores that behave similarly (start out at same level of sales volume, received roughly the same amount of spend, etc).<br>
Can anyone advise the best way to go about this?</p>

<p>I was initially thinking about using propensity scores to formulate the matching, but I know that seems counter-intuitive in this randomized setting.  However, in what way should I go about making a matched control group based on those variables? Would it still use the MatchIt package- but does that strictly use some variation of Propensity Scores? </p>

<p>In addition, after a matched set, would a simple multiple regression be the best means of finding the treatment effect? </p>

<p>Any thoughts/help greatly appreciated, thanks!</p>
"
"0.0495478739876288","0.0497518595104995","171763","<p>I am working on a paper about sexual coherence in women. Sexual coherence is defined as the relationship between subjective (SA) and genital sexual (GA) arousal. There is research that shows that this coherence can be higher or lower, depending on other factors, like age or arousability...</p>

<p>Both measures (SA and GA) have been measured continuously over a period of 5 minutes. I divided these 5-minutes into 15-second intervalls and calculated the mean for both arousal measures for each section.</p>

<p>Additionally, I have 2 questionnaire scores (P1, P2) that might influence SA, GA or (most importantly) the relationship between SA and GA</p>

<p>I use the package nlme in R and my data is transformed into long format.</p>

<p>My first question: Is it, in your opinion, possible to assess sexual coherence between GA and SA with a regression analysis, in which SA is the outcome and GA is the predictor?</p>

<p>My second question: If I want to investigate the impact of P1 and P2 on sexual concordance (the association between GA and SA), is it feasible to add the questionnaires to the above mentioned regression?
The model would look something like this: Coherence.model &lt;-nlme ( SA ~ GA + P1 + P2 + (GAP1) + (GAP2) + (P1*P2)) My idea is that you can assess the direct influence of GA, P1 and P2 on SA and (if the interaction terms (GA*P1) is significant) you can say that, e.g., P1 is a moderator of the relationship between GA and SA.</p>

<p>What do you think? Or do you have another idea, who to work with an ""coherence measure"" as outcome variable?</p>

<p>Best, Julia</p>

<p>Please excuse that I did not get into detail regarding syntax or programming. But I hope that this is not necessary at this moment.</p>
"
"0.0495478739876288","0.0497518595104995","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0700712753800578","0.0703597544730292","172189","<p>I have a dataframe <code>df</code> (see below):</p>

<pre><code>dput(df)
structure(list(x = c(49, 50, 51, 52, 53, 54, 55, 56, 1, 2, 3, 
4, 5, 14, 15, 16, 17, 2, 3, 4, 5, 6, 10, 11, 3, 30, 64, 66, 67, 
68, 69, 34, 35, 37, 39, 2, 17, 18, 99, 100, 102, 103, 67, 70, 
72), y = c(2268.14043972082, 2147.62290922552, 2269.1387550775, 
2247.31983098201, 1903.39138268307, 2174.78291538358, 2359.51909126411, 
2488.39004804939, 212.851575751527, 461.398994384333, 567.150629704352, 
781.775113821961, 918.303706148872, 1107.37695799186, 1160.80594193377, 
1412.61328924168, 1689.48879626486, 260.737164468854, 306.72700499362, 
283.410379620422, 366.813913489692, 387.570173754128, 388.602676983443, 
477.858510450125, 128.198042456082, 535.519377609133, 1028.8780498564, 
1098.54431357711, 1265.26965941035, 1129.58344809909, 820.922447928053, 
749.343583476846, 779.678206156474, 646.575242339517, 733.953282899613, 
461.156280127354, 906.813018662913, 798.186995701282, 831.365377249207, 
764.519073183124, 672.076289062505, 669.879217186302, 1341.47673353751, 
1401.44881976186, 1640.27575962036)), .Names = c(""x"", ""y""), row.names = c(NA, 
-45L), class = ""data.frame"")
</code></pre>

<p>I have created on a non-linear regression (nls) based on my dataset.</p>

<pre><code>nls1 &lt;- nls(y~A*(x^B)*(exp(k*x)), 
            data = df, 
            start = list(A = 1000, B = 0.170, k = -0.00295), algorithm = ""port"")
</code></pre>

<p>I then computed a bootstrap for this function to get multiple sets of parameters (A,B and k) and created a dataframe which contains the different set of parameters. </p>

<pre><code>Boo &lt;- nlsBoot(nls1, niter = 200)
Param_Boo &lt;- Boo$coefboot
</code></pre>

<p>I have then plotted all the 200 output functions from the bootstrapping (see below).</p>

<pre><code># Plot curves with bootstrapped params
x &lt;- seq(min(df$x),max(df$x),length=50)
curveDF &lt;- data.frame(matrix(0,ncol = 3,nrow = 200*length(x)))

for(i in 1:200)
{
  for(j in 1:length(x))
  {
    # Function value
    curveDF[j+(i-1)*200,1] &lt;- Param_Boo[i,1]*(x[j]^Param_Boo[i,2])*(exp(Param_Boo[i,3]*x[j]))
    # Bootstrap sample number
    curveDF[j+(i-1)*200,2] &lt;- i
    # x value
    curveDF[j+(i-1)*200,3] &lt;- x[j]
  }
}
colnames(curveDF) &lt;- c('ys','bsP','xs')

p1 &lt;- ggplot(curveDF, aes(x=xs, y=ys, group=bsP)) +
  geom_line() +
  ggtitle(""Curves for bootstrapped params"")
</code></pre>

<p>However, the visibility of this plot is not nice if someone wants to add the points of my dataframe on the plot. Therefore,I was wondering if it was possible to plot one curve (the mean of the 200 curves for instance) with the upper and lower confidence interval (or something else). Visually it will look a bit like the picture (top right) below. 
<a href=""http://i.stack.imgur.com/8Ues6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Ues6.png"" alt=""enter image description here""></a></p>

<p>Can someone help me out with that? Thanks in advance. </p>
"
"0.103142124625879","0.103566754353222","172782","<p>Newbie question using R's mtcars dataset with anova() function. My question is how to use anova() to select the best (nested) model. Here's some example data:</p>

<pre><code>&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+am,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + am
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1     30 317.16                                
2     29 246.68  1    70.476 8.0036 0.008535 **
3     28 246.56  1     0.126 0.0143 0.905548   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; anova(lm(mpg~disp,mtcars),lm(mpg~disp+wt,mtcars),lm(mpg~disp+wt+hp,mtcars))
Analysis of Variance Table

Model 1: mpg ~ disp
Model 2: mpg ~ disp + wt
Model 3: mpg ~ disp + wt + hp
  Res.Df    RSS Df Sum of Sq       F   Pr(&gt;F)   
1     30 317.16                                 
2     29 246.68  1    70.476 10.1201 0.003571 **
3     28 194.99  1    51.692  7.4228 0.010971 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My understanding is anova() compares the reduction in the residual sum of squares to report a corresponding p-value for each nested model, where lower p-values means that nested model is more significantly different from the first model. </p>

<p>Question 1: Why is it that changing the 3rd regressor variable effects results from the 2nd nest model? That is, the p-value for <code>disp+wt</code> model changes from 0.008535 to 0.003571 going from the first to the second example. (does anova's model 2 analysis use data from model 3???)</p>

<p>Question 2: Since the 3rd model's <code>Sum of Sq</code> value is much lower in the first example (e.g. 0.126 versus 51.692), I'd expect the p-value to be lower as well, but it in fact increases (e.g. 0.905548 versus 0.010971). Why?</p>

<p>Question 3: Ultimately I'm trying to understand, given a dataset with a lot of regressors, how to use anova() to find the best model. Any general rules of thumb are appreciated. </p>
"
"0.121535457502911","0.128458748884677","172904","<p>I am trying to build a regression model for the forecast of stock market returns. The regression takes about <strong>100 variables</strong> as input and my training data consists of <strong>n=234</strong> weekly data points. I am using a lasso regression for the regression + variable selection.</p>

<p>My problem is that before I can input the data into the regression I need to find the ""optimal lag"" time for it. The variables can be <strong>categorised into 4 groups</strong> and I have got data for the variables up to <strong>113 weeks before the first point in time of my training data</strong>. To calculate the ""optimal"" lag time I tried following approach:</p>

<p>Take every variable for one categorie and apply a rolling window to the training data, which moves forward in quarters e.g. datapoints 1:13,14:26,... This gives <strong>18 seperate</strong> quarters for the training data. For the first quarter calculate the correlation (spearman in this case) between the stock course and every variable in the categorie with a lag of 0. Then take the average of the absoulte correlation of all the variables for this quarter and enter it into a matrix at point [1,1]. Repeat this for every possible lack time up to 113 and then move on to the next quarter. </p>

<p>This results in a <strong>[114,18] matrix</strong> which includes the average absolute correlation for every lag in every quarter. Now I just take the average for every row in the matrix and the row with the highest average correlation should give me the most reliable lag for my data of that category.</p>

<p><strong>Q:</strong> Has anyone seen a similar approach like this in literature before? Is there something I am missing? E.g. would it be smarter to just calculate the correlation over the whole trainingset at once and look for the optimal lag that way? As far as I am concerned that is the most common approach in literature, but it seems to me that it gives you a pretty biased result.</p>

<p>For some more insight you can find my code below:</p>

<pre><code>Correlation.Maximiser = function(Stock, Category){

  Result = matrix(nrow = 114,ncol =  18)

  for(tmp in 1:18){
    Start.Test=1+(tmp-1)*13
    End.Test=13+(tmp-1)*13
    Sample = Stock[Start.Test:End.Test]

    for(i in 0:113){
      int.low=101-i+13*tmp
      int.high=113-i+13*tmp
      NA.omitter = cor(Sample,Category[int.low:int.high,-1], method = ""spearman"")
      NA.omitter[is.na(NA.omitter)] = 0 #some Variables have a lot of 0 values so that it can happen that they are only zero in the quarter we look at which results in an NA
      Result[i+1,tmp]=mean(abs(NA.omitter))
    }
  }  
  return(Result)
}
</code></pre>

<p><strong>EDIT:</strong> I just realised that the comparison in categories makes no real sense, since some variables might have a positive while others might have a negativ correlation to the dependent variable. I tried to compensate that by averaging over the absolute correlation, but I realised that this would not penalise variables that change sign from quarter to quarter and such behavior would be really bad for the regression. In general my question stays the same though. Only now the procedure is applied to every variable on its own.</p>
"
"0.104071351726209","0.104499807023868","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.0404556697031367","0.0406222231851194","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.0814154647783432","0.0817506471951855","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0707974219804893","0.0710888905739589","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.0700712753800578","0.0703597544730292","173207","<p>I have a dataset gpa2 ddata that can be found here <a href=""https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1"" rel=""nofollow"">https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1</a></p>

<p>I estimate the model colgpa = athelte.</p>

<pre><code>gpa2 &lt;- read.csv(~""/path/ddata.csv"")
model1 &lt;- lm(formula = colgpa ~ athlete, data = gpa2)
summary(model1)
</code></pre>

<p>And now I want to see if I can get <strong>Std.Error by this formula</strong>
$$se(\beta_j) = \frac{\hat \sigma_u}{SST_j(1âˆ’R_j^2)}$$
where $$SST_j = \sum_{i=1}^n (x_{ij} - \bar x_j)^2$$ is the total sample variation in $x_j$
and $R_j^2$ is the $R^2$ from regressing $x_j$ on all the other independent variables.</p>

<p>From this answer 
<a href=""http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression"">How are the standard errors of coefficients calculated in a regression?</a>
we know that the standard error of the estimated slope, $se(\beta_1)$ in our case, is
$$\sqrt{\widehat{\textrm{Var}}(\hat{b})} = \sqrt{[\hat{\sigma}^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}]_{22}} = \sqrt{\frac{n \hat{\sigma}^2}{n\sum x_i^2 - (\sum x_i)^2}}.$$</p>

<p>I do this with an anova-table, and try to </p>

<pre><code>anova(model1) # the anova table
# now I take out elements of the anova
model1_hatsigmau &lt;- anova(model1)[[3]][2] #takes row 3 column 2 in the anova-table.
model1_MSathlete &lt;- anova(model1)[[3]][1]
model1_SSathlete &lt;- anova(model1)[[2]][1]
numerator &lt;- n*model1_hatsigmau
meanathlete &lt;- mean(gpa2$athlete)
denominator &lt;- n*model1_SSathlete - (n*meanathlete)^2 
sqrt(numerator /  denominator) # should be se(beta_1) for model1. 
</code></pre>

<p>But I get </p>

<pre><code>&gt; sqrt(numerator /  denominator) # should be se(beta_1) for model1.
[1] 0.270643
</code></pre>

<p>And not $0.04824$ as in the summary-output (see below). So the problem is that $$0.270643 \neq 0.04824$$</p>

<pre><code>&gt; summary(model1)

Call:
lm(formula = colgpa ~ athlete, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.66603 -0.43603  0.00397  0.46397  1.61851 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.66603    0.01045 255.212  &lt; 2e-16 ***
athlete     -0.28453    0.04824  -5.898 3.97e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.656 on 4135 degrees of freedom
Multiple R-squared:  0.008343,  Adjusted R-squared:  0.008104 
F-statistic: 34.79 on 1 and 4135 DF,  p-value: 3.966e-09
</code></pre>
"
"0.06396603026469","0.0642293744423385","173410","<p>I am working with a lasso regression with the glmnet package. I read these threads: <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">When conducting multiple regression, when should you center your predictor variables &amp; when should you standardize them?</a>, <a href=""http://stats.stackexchange.com/questions/19523/need-for-centering-and-standardizing-data-in-regression"">Need for centering and standardizing data in regression</a> and <a href=""http://stats.stackexchange.com/questions/86434/is-standardisation-before-lasso-really-necessary"">Is standardisation before Lasso really necessary?</a>.</p>

<p>Based on the responses I decided that I need to standardize my data before using it. I do have some questions however:</p>

<ul>
<li>Do I need to standardize the predictors and the responses or only the predictors?</li>
<li>I am using the function scale(myData, center = TRUE, scale = TRUE) for building the model, but I am wondering what do I do when I want to do predictions with a test data set. I think I should also standardize and center the test data, but how to I do that? Substracting the mean from the initial (training) dataset and the dividing it by the standard deviation of the initial dataset? </li>
<li>When I get a result do I need to ""backscale"" it (using the original mean and standard deviation) or do I already get the ""final"" result? </li>
</ul>
"
"0.0583927294833815","0.0703597544730292","173629","<p>When applying the ""urca"" package function <code>ur.df</code>, like </p>

<pre><code>summary(ur.df(data$col1, type = c(""none""), lags = 12, selectlags = c(""AIC"")))
</code></pre>

<p>I get following result:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-12928366  -2888728   1284718   4218373   7179531 

Coefficients:
                 Estimate    Std. Error  t value  Pr(&gt;|t|)   
(Intercept)  5.391984e+07  1.638362e+07  3.29108 0.0043123 **
z.lag.1     -2.438154e+00  7.557134e-01 -3.22629 0.0049588 **
tt           6.579260e+05  2.730453e+05  2.40959 0.0275861 * 
z.diff.lag1  1.712004e+00  6.595980e-01  2.59553 0.0188537 * 
z.diff.lag2  1.402824e+00  6.379412e-01  2.19899 0.0420083 * 
z.diff.lag3  1.321555e+00  5.294537e-01  2.49607 0.0231329 * 
z.diff.lag4  1.099430e+00  4.720412e-01  2.32910 0.0324428 * 
z.diff.lag5  8.132753e-01  4.181477e-01  1.94495 0.0685140 . 
z.diff.lag6  1.797331e-01  3.654326e-01  0.49184 0.6291254   
z.diff.lag7  5.890640e-01  2.939590e-01  2.00390 0.0612825 . 
z.diff.lag8  3.919041e-01  2.794371e-01  1.40248 0.1787705   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6708593 on 17 degrees of freedom
Multiple R-squared:  0.7237276, Adjusted R-squared:  0.5613144 
F-statistic: 4.253547 on 10 and 17 DF,  p-value: 0.003348755


Value of test-statistic is: -3.2263 3.9622 5.2635 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.15 -3.50 -3.18
phi2  7.02  5.13  4.31
phi3  9.31  6.73  5.61
</code></pre>

<p>Now the question:</p>

<ol>
<li>I do understand that ""-3.2263"" is the critical value (t-value)</li>
<li><strong>There is a unit root</strong> with trend since -3.2263 > -3.18 (tau3@10pct)
This means the time-series is <strong>non-stationary</strong> at a 10% significance level.</li>
<li>But, what is the meaning of ""p-value: 0.003348755""? Should I list this value in a table summarizing my unit root test results or rather mark the 0.1 significance level (*10%)?</li>
</ol>

<p>The <a href=""http://www.inside-r.org/packages/cran/urca/docs/ur.df"" rel=""nofollow"">documentation</a> says that critical values are based on Hamilton (1994) and Dickey and Fuller (1981)"". </p>
"
"0.0858194351535935","0.0861727484432139","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.11794753195637","0.11843311462705","174136","<p>I have a dataset of a metric predictor variable $X$, and an ordered categorical predicted value $Y$ for several individuals. The dataset are from two groups $G_1$ and $G_2$. I want to estimate $Y$ from $X$, and I want to be able to compare the forecast accuracy of models, in group and individual level. For example, I want to know if these models helps to estimate $Y$ from a $X$ for a new user of a category, or a new experiment from the same user of a known category?</p>

<p>In <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">ordered probit</a>, we suppose that $Y^*$ is the exact but unobserved dependent variable, and $X$ is the vector of independent variables, and $\beta$ is the a regression coefficient which we wish to estimate.</p>

<p>$Y^* = \mathbf{x}' \beta + \epsilon$</p>

<p>We can not observer $y*$ directly, but we instead can only observe the categories of response:</p>

<p>$
Y= \begin{cases}
0~~ \text{if}~~y^* \le 0, \\
1~~ \text{if}~~0&lt;y^* \le \mu_1, \\
2~~ \text{if}~~\mu_1 &lt;y^* \le \mu_2 \\
\vdots \\
N~~ \text{if}~~ \mu_{N-1} &lt; y^*.
\end{cases}
$</p>

<p>I came across this article from Gelman et al. that describes Bayesian Hierarchical Model: <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">Multilevel (Hierarchical) Modeling: What It Can and Cannot Do</a>, which has been implemented in Python <a href=""http://nbviewer.ipython.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb"" rel=""nofollow"">here</a>.</p>

<p>I am processing data in R, and I have selected a <strong>thresholded Bayesian hierarchical model</strong> to use with the <strong>generalized linear model</strong>. I have calculated the parameters of it using MCMC. My question is that how should I compare accuracy of ordered probit, and the equivalent Bayesian hierarchical model in R?</p>

<p>Gelman has used <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow"">RMSE</a> for comparison using cross-validation. First he <em>removed single data points and checked the prediction from the model fit to the rest of the data, then removed single counties and performed the same procedure. For each cross-validation step, we compare complete-pooling, no-pooling, and multilevel estimates.</em></p>

<p>I have done MCMC simulation using RJags, which gave me the posterior distribution of the parameters, but how can I compare posterior distribution with a single point estimate of <strong>ordered probit</strong> to compare accuracy? Should I do as Gleman did and use RMSE? How? Or should I compare posterior distribution with results of several experiments with ordered probit? Is <a href=""http://www.stat.columbia.edu/~gelman/presentations/ggr.pdf"" rel=""nofollow"">posterior predictive check</a> usable here? I usually prefer cross-validation, but I don't know how to do this here.</p>

<p>PS: The notion of <strong>Goodness of fit</strong> in Bayesian analysis is ambigious to me. <a href=""http://people.stat.sfu.ca/~tim/papers/survey.pdf"" rel=""nofollow"">This paper</a> states:</p>

<blockquote>
  <p>GOODNESS-OF-FIT:</p>
  
  <p>In Bayesian statistics, there is no consensus on the
  correct"" approach to the assessment of goodness-of fit. When Bayesian
  model assessment is considered, it appears that the prominent modern
  approaches are based on the posterior predictive distribution (Gelman,
  Meng and Stern 1996).</p>
</blockquote>
"
"0.159567453668607","0.160224382974551","174257","<p>I want to do a path analysis with lavaan but encounter a few problems and would appreciate any help.</p>

<p>The structural model looks like this:</p>

<p><a href=""http://i.stack.imgur.com/y8ZZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y8ZZh.png"" alt=""structural model""></a></p>

<p>The relation between one observed independent (s) and one observed dependent variable (v) is mediated through a latent variable (m) that is defined by two observed indicator variables (x1, x2). This is basically a simplified version of the <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">SEM example</a> in the tutorial on the lavaan project website.</p>

<p>When I enter my code (given further below) into R, I encounter two problems:</p>

<p>(1) The results change when I change the order of the indicator variables.</p>

<p>This model:</p>

<pre><code># measurement model
    m =~ x1 + x2
</code></pre>

<p>returns a different result than this model:</p>

<pre><code># measurement model
    m =~ x2 + x1
</code></pre>

<p>How can that be? Isn't the order of the indicators arbitrary? And if not, how do I know which is the correct order, if my model does not presuppose a specific order?</p>

<p>(2) There are a few warnings that I don't understand: for the first model, no standard errors could be computed; and the second model did not ""converge"" (whatever that means). The warnings are given in context in the full code posted below.</p>

<p>What do I have to do to obtain reliable estimates?</p>

<hr>

<p>Here is the full R output to provide context to my questions.</p>

<pre><code># data

s &lt;- c(2, 5, 4, 4, 4, 8, 2, 9, 1, 1, 3, 3, 2, 3, 2, 5, 5, 7, 4, 7, 8, 4, 10, 10, 2, 4, 0, 2, 4, NA, 1, 5, 2, 6, 3, 5, 0, 5, 3, 6, 4, 9, 4, 9, 4, 5, 6, 1, 8, 0, 6, 9, 1, 5, 1, 6, 2, 5, 0, 5, 6, 2, 4, 10, 3, 4)
v &lt;- c(8, 10, 1, 4, 0, 2, 3, 2, 1, 1, 2, 5, 1, 5, 0, 5, 4, 5, 2, 10, 0, 6, 5, 5, 6, 1, 1, 0, 0, NA, 1, 0, 1, 8, 1, 3, 0, 5, 6, 3, 2, 10, 0, 5, 5, 10, 4, 1, 1, 0, 0, 0, 2, 10, 1, 8, 2, 3, 2, 2, 4, 4, 2, 5, 6, 2)
x1 &lt;- c(2.500000, 3.789474, 1.514563, 5.846868, 4.588235, 5.600000, 5.066667, 11.647059, 2.000000, NA, 4.461538, 18.000000, 1.058824, 9.217391, 27.840000, 15.375000, NA, 6.000000, 9.714286, 12.484848, 16.503497, 20.666667, 3.500000, 4.658824, 4.750000, 4.000000, 2.800000, 14.228571, 11.000000, NA, 2.666667, 3.764706, 4.705882, 13.272727, 2.000000, 18.444444, 17.555556, 14.222222, 2.000000, 4.000000, 8.461538, 19.200000, 13.902439, 13.000000, 3.000000, NA, 7.360000, 1.611374, 1.500000, 3.365854, 22.375000, 10.838710, 2.923077, 3.488372, 5.176471, 37.666667, 1.176471, 7.454545, 36.235294, 6.823529, 2.222222, 6.133333, 11.428571, 42.705882, 28.105263, 18.333333)
x2 &lt;- c(8.125000, 14.273684, 7.339806, 23.387471, 113.058824, 22.200000, 17.466667, 43.647059, 9.230769, NA, 13.538462, 83.555556, 5.058824, 37.391304, 100.000000, 59.250000, NA, 22.470588, 38.428571, 50.787879, 76.223776, 92.888889, 15.375000, 16.235294, 18.875000, 13.647059, 10.133333, 55.885714, 36.428571, NA, 6.933333, 13.294118, 14.117647, 81.818182, 6.117647, 67.777778, 76.333333, 51.888889, 6.428571, 14.200000, 34.000000, 59.680000, 68.634146, 40.500000, 12.250000, NA, 29.760000, 8.909953, 5.400000, NA, 71.125000, 39.741935, 9.846154, 13.116279, 18.823529, 204.000000, 4.588235, 49.090909, 188.470588, 19.647059, 10.222222, 22.933333, 38.285714, 140.235294, 137.526316, 79.000000)
dat &lt;- data.frame(cbind(s, v, x1, x2))

# first model

model &lt;- '
    # measurement model
        m =~ x2 + x1
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats,  :
#   lavaan WARNING: could not compute standard errors!
#   lavaan NOTE: this may be a symptom that the model is not identified.

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# lavaan (0.5-18) converged normally after 147 iterations
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                0.565
#   Degrees of freedom                                 0
#   Minimum Function Value               0.0043451960201
#
# Model test baseline model:
#
#   Minimum Function Test Statistic              126.904
#   Degrees of freedom                                 6
#   P-value                                        0.000
#
# User model versus baseline model:
#
#   Comparative Fit Index (CFI)                    0.995
#   Tucker-Lewis Index (TLI)                       1.000
#
# Loglikelihood and Information Criteria:
#
#   Loglikelihood user model (H0)               -797.558
#   Loglikelihood unrestricted model (H1)       -797.275
#
#   Number of free parameters                         12
#   Akaike (AIC)                                1619.115
#   Bayesian (BIC)                              1645.208
#   Sample-size adjusted Bayesian (BIC)         1607.435
#
# Root Mean Square Error of Approximation:
#
#   RMSEA                                          0.000
#   90 Percent Confidence Interval          0.000  0.000
#   P-value RMSEA &lt;= 0.05                          1.000
#
# Standardized Root Mean Square Residual:
#
#   SRMR                                           0.027
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x2                1.000                              14.272    0.330
#     x1                0.384                               5.482    0.588
#
# Regressions:
#   m ~
#     s                 1.732                               0.121    0.323
#   v ~
#     s                 0.335                               0.335    0.306
#     m                 0.012                               0.171    0.059
#
# Covariances:
#   x2 ~~
#     x1              292.112                             292.112    0.951
#
# Intercepts:
#     x2               35.558                              35.558    0.823
#     x1                7.220                               7.220    0.775
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x2             1663.119                            1663.119    0.891
#     x1               56.783                              56.783    0.654
#     v                 7.591                               7.591    0.892
#     m               182.367                               0.895    0.895
#
# R-Square:
#
#     x2                0.109
#     x1                0.346
#     v                 0.108
#     m                 0.105

model &lt;- '
    # measurement model
        m =~ x1 + x2
    # regressions
        m ~ s
        v ~ s + m
    # residual correlations
        x1 ~~ x2
'
fit &lt;- sem(model, data = dat, missing = ""fiml"")

# Warning messages:
# 1: In lav_data_full(data = data, group = group, group.label = group.label,  :
#   lavaan WARNING: some cases are empty and will be removed:
#   30
# 2: In lavaan::lavaan(model = model, data = dat, missing = ""fiml"", model.type = ""sem"",  :
#   lavaan WARNING: model has NOT converged!

summary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# ** WARNING ** lavaan (0.5-18) did NOT converge after 9438 iterations
# ** WARNING ** Estimates below are most likely unreliable
#
#                                                   Used       Total
#   Number of observations                            65          66
#
#   Number of missing patterns                         3
#
#   Estimator                                         ML
#   Minimum Function Test Statistic                   NA
#   Degrees of freedom                                NA
#   P-value                                           NA
#
# Parameter estimates:
#
#   Information                                 Observed
#   Standard Errors                             Standard
#
#                    Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all
# Latent variables:
#   m =~
#     x1                1.000                               0.526    0.056
#     x2             1606.326                             845.326   19.343
#
# Regressions:
#   m ~
#     s                -0.001                              -0.001   -0.004
#   v ~
#     s                 0.355                               0.355    0.325
#     m                 0.004                               0.002    0.001
#
# Covariances:
#   x1 ~~
#     x2              -69.375                             -69.375   -0.009
#
# Intercepts:
#     x1               10.099                              10.099    1.083
#     x2               48.281                              48.281    1.105
#     v                 1.761                               1.761    0.604
#     m                 0.000                               0.000    0.000
#
# Variances:
#     x1               86.614                              86.614    0.997
#     x2            -712666.446                            -712666.446 -373.157
#     v                 7.617                               7.617    0.895
#     m                 0.277                               1.000    1.000
#
# R-Square:
#
#     x1                0.003
#     x2                   NA
#     v                 0.105
#     m                 0.000
# Warning message:
# In .local(object, ...) :
#   lavaan WARNING: fit measures not available if model did not converge
</code></pre>

<hr>

<p><em>Note.</em> I have posted the same question to the <a href=""https://groups.google.com/forum/#!forum/lavaan"" rel=""nofollow"">lavaan Google Group</a>, but this is part of my bachelor's thesis, which I have to turn in on Monday, so I'm a bit pressed for time and hope you forgive me for crossposting.</p>
"
"0.0858194351535935","0.0861727484432139","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.0572129567690623","0.0574484989621426","174920","<p>Are there any easy to use alternatives to stepwise variable selection for GLMMs? I have seen implementations of e.g. LASSO for linear regression, but so far not seen anything for mixed models. Mixed models seem non-trivial in general, so I am wondering if any of the fancy new methods have been adapted from them (and possibly implemented in R). Using whatever selection procedure you like and then validating the results seems a sensible way to go in the meantime.</p>

<p>To give some context: in my current project, I am looking at approximately 700 variables and 5000 binary observations. Stepwise selection takes about 1 day; many variables have about 10% missingness.</p>

<p>Edit: Thank you for the very interesting answers so far! Two concerns that I have are: do these new methods have longer runtimes than stepwise selection and can they deal with missing data (if each variable has different missingness, than for hundreds of variables it is very easy to loose all observations in a complete case analysis - something that stepwise selection can deal with by only using small subsets of the available variables at the same time).</p>
"
"0.0495478739876288","0.0497518595104995","175051","<p>Howdy internet strangers!</p>

<p>I've put a sample of my data below, but basically I have the number of accidental falls in a particular set of patients vs. drugs that the patients were taking. The drugs they were taking are coded in a binary manner - 1 if they were on the drug, 0 if they were not. Any ideas on how to analyze this data? </p>

<p>The question I would like answered is ""Do patients on drug X have an increased risk of falling?"" And an risk ratio for each drug would also be great. </p>

<p>I have an engineering background and can run scripts in R but I am by no means a statistic expert. I was doing GLM with Poisson regression (in R) for each drug, but I have an inkling this is the wrong test to be doing. I would like to use some sort of multiple comparisons (that takes into account all of the drugs that each individual patient was on) but I don't know what test is most appropriate. Thank you in advance for your help!</p>

<p><a href=""http://i.stack.imgur.com/O0sD8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/O0sD8.jpg"" alt=""Sample Data""></a></p>
"
"0.0572129567690623","0.0574484989621426","175079","<p>Is it so that:</p>

<ul>
<li>$y_i$ is not a discrete value, but a range with probability density function</li>
<li>Which means for the same predictor(s) value $y_i$ could have different results</li>
<li>In linear regression this distribution can only be normal</li>
<li>In GLM, this distribution can be any distribution from the exponential family</li>
<li>distribution of a single $y_i$ has nothing to do with distribution of all $y(s)$</li>
<li>$\mu_i$ is expected value of $y_i$</li>
<li>In practical use, $\mu_i$ is the predicted value $y_i$, specially if dataset has only one y for given predictor(s)</li>
</ul>

<p>Are above correct? Where am I wrong?</p>

<p>Based on the above I've tried simulating <code>glm</code> with <code>lm</code> in R, and it kinda works:</p>

<pre><code>library(boot)
download.file(""https://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""./ravensData.rda"",method=""curl"")
load(""./ravensData.rda"")
# download manually and loadhere if above fails
# load(""/yourpath/ravensData.rda"")

# calling logit(ravensData$ravenWinNum) results in 
# [1]  Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf
# [17] -Inf -Inf  Inf -Inf
# that's way too much, as inv.logit goes to 1 at 20
# so we'll write our own dummy ""logit"" routine
# this will give us 5 when winNum=1 and -5 when it's zero
win &lt;- ravensData$ravenWinNum*10-5

# now we can do a simple lm
fit &lt;- lm(win~ravensData$ravenScore)

# and get probability of win using inv.logit
fitwin &lt;- inv.logit(fit$fitted.values)
plot(ravensData$ravenScore, fitwin)

# now glm
fitglm &lt;- glm(ravensData$ravenWinNum ~ ravensData$ravenScore, family=""binomial"")
plot(ravensData$ravenScore,fitglm$fitted)
</code></pre>
"
"0.110792414376932","0.111248539872496","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0908377689773195","0.0912117424359157","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.0809113394062735","0.0812444463702388","175996","<p>I have been studying a few simple statistical models for (univariate) time series. From my understanding,</p>

<ul>
<li><p>ARIMA and its siblings are used to model the <em>mean</em> of a time series. Rather than a static measure like <code>mean()</code>, the result is a series estimating the mean.</p></li>
<li><p>ARCH and its brothers are used to model the <em>volatility</em> of a time series. Rather than the usual <code>sd()</code>, the result is a series estimating the variance. </p></li>
</ul>

<h2>Question</h2>

<p>What would be a credible model for the correlation of two time series?</p>

<h2>Notes</h2>

<p>While mean models explore the idea of regressing lagged values of the time series, volatility models (eg. ARCH model) explore the idea of regressing lagged residuals where residuals are the difference of a mean model to its original time series.</p>

<p>In its general sense and for a variety of reasons, ARIMA and ARCH are <em>superior</em> models than rolling windows with <code>mean()</code> (popularly known as moving averages outside statistics world) and <code>sd()</code>.</p>

<p>However, there is no such a thing for the <em>correlation</em> of two time series X and Y to my knowledge.</p>

<p>The closest thing would be rolling a sad, straight window with <code>cor()</code>, Pearson's coefficient function in R, and work around the resulting series.</p>

<h2>A poor solution</h2>

<p>Trying to replicate Pearson's correlation model,</p>

<pre><code>p_(X,Y) = cov(X,Y) / (sd(X) sd(Y))
        = E((X-mean(X))(Y-mean(Y))) / (sd(X) sd(Y)),
</code></pre>

<p>to the time series world, I had the above without the intended success.</p>

<pre><code>library('forecast')
library('fGarch')

X &lt;- 1:200 + rnorm(200, sd=10)
Y &lt;- 50 + (1:200)/100 + rnorm(200, sd=5)

plot(1:200, X, t='l', main=""What would be a resulting ts correlation of X and Y?"")
lines(1:200, Y, t='l', col='blue')

# Mimic Pearson correlation, cov(X,Y)/(sd(X)*sd(Y)).
Xm &lt;- as.vector(X) - as.vector(fitted(Arima(X, order=c(2,0,1))))
Ym &lt;- as.vector(Y) - as.vector(fitted(Arima(Y, order=c(2,0,1))))

Xv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=X)@sigma.t
Yv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=Y)@sigma.t

correlation &lt;- Xm * Ym / (Xv * Yv)    # this can be forecast

plot(correlation, t='l', col='blue', ylim=c(-2, 2), main='Correlation models')
abline(h=c(-1, 1))
abline(h=cor(X, Y), col='red', lwd=5)

# Correlation rolling window of size 10.
df &lt;- data.frame(X, Y)
crw &lt;- rep(NA, 10)
for (i in 11:nrow(df))
  crw &lt;- c(crw, cor(df[(i-10):i, 1], df[(i-10):i, 2]))

lines(crw, col='darkgreen', lwd=5)

legend('topright',
  c('pearson mimic', 'static cor()', 'rolling cor() like moving averages'),
  col=c('blue', 'red', 'darkgreen'), lwd=c(1, 5, 5))
</code></pre>
"
"0.124692748408752","0.12520610071704","176111","<p>I need to conduct a meta-analysis for a publication, but this is my first meta-analysis and I still donâ€™t feel confident. I will describe the steps I have followed, and hopefully some of you might find errors on my methods, and suggest alternatives.</p>

<p>For this particular analysis, long-term studies should be more important than short-term because a few experiments showed transient effects, hence short-term studies might fail to capture that the effect is not really significant in the long-term. On my dataset, about half the studies have several non-independent measurements taken at different time-points (i.e. several annual measurements). Other experiments, despite having been carried out for several years, show the data already aggregated, with only one row per study with mean and standard deviation. I considered running a multivariate meta-analysis to solve this issue, but it would unbalance the analysis, giving more importance to the experiments with several rows of data (annual measurements) than to the experiments with aggregated data in only one row (pooled across several years). Am I right? This is an example of the dataset:</p>

<ul>
<li>Study 1, Year 1, Effect Size 1 </li>
<li>Study 1, Year 2, Effect Size 2 </li>
<li>Study 1, Year 3, Effect Size 3    </li>
<li>Study 2, Year 1, Effect Size 4    </li>
<li>Study 3, Years 1-4, Effect Size 5</li>
<li>Study 4, Years 1-3, Effect Size 6</li>
</ul>

<p>Alternatively, I decided to try and aggregate the data, so that finally there is only one row per study. I followed these steps:</p>

<p>Calculate effect sites for each row, including those studies with several rows (annual data). In this case, I calculated the log response ratio (ROM):</p>

<pre><code>dat &lt;- escalc (measure=""ROMâ€, n1i=elev.rep, n2i=control.rep, m1i=elev.ANPP.mean, m2i=control.ANPP.mean, sd1i=elev.SD, sd2i=control.SD, data=all)
</code></pre>

<p>Aggregate studies using the function agg {MAd}. I used the Borenstein et al. 2009 method, and correlation=1:</p>

<pre><code>datAgg &lt;- agg(id = id,es = yi,var = vi, cor =1,method = ""BHHR"", data = dat)
</code></pre>

<p>I have now only one row per study. However, since long-term experiments are more important, I have created user-defined weights that take into account the number of replicates and the number of years of each study:</p>

<pre><code>datAgg$weightsTime &lt;- with(datAgg, ((control.rep * elev.rep)/(control.rep + elev.rep)) + ((nyears^2)/(2*nyears)))
</code></pre>

<p>Run the mixed-effects meta-regression with two moderators, using Hedges Estimator (HE) and the Knapp and Hartung approach:</p>

<pre><code>m &lt;- rma.uni(yi, vi, mods= ~ factor(A) * factor(B), method=""HE"", data=datAgg, weights=weightsTime, knha=TRUE)
</code></pre>

<p>Am I doing something wrong? Can this method be improved? So far the results confirm my hypothesis, but of course I might be using a sub-optimal approach. Many thanks</p>
"
"0.131091352563232","0.131631047527805","176203","<p>I'm running a basic <a href=""https://stat.ethz.ch/R-manual/R-patched/library/stats/html/wilcox.test.html"" rel=""nofollow"">Mann-Whithney U test in R</a> between two groups; each group represents the abundance values of a particular bacteria within an animal.  Therefore a 0 means that bacteria is not present within the animal:</p>

<pre><code>a &lt;- c(0,0,12,0,0,76,0,0,81,0,0,0,0,0)
b &lt;- c(427,928,0,127,0,0,189,0,0,0,0,0,312,583,0)
wilcox.test(a,b,exact=FALSE)
</code></pre>

<p>The returned p-value is 0.1362, however I would have expected it to be &lt;0.05 since the two groups are quite different (at least IMHO).  I take it that the abundance of zeros is causing this.</p>

<p><a href=""http://i.stack.imgur.com/PnyTk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PnyTk.png"" alt=""enter image description here""></a></p>

<p>Is there another test to use in order to check whether these two group are different?  It looks like a zero-inflated negative binomial regression was suggested <a href=""http://stats.stackexchange.com/a/85625/82427"">here</a>, however I'm not familiar with that test or whether it applies to my data set.</p>

<p>Can I simply omit the zeros? <code>wilcox.test(a[a!=0],b[b!=0],exact=FALSE)</code> yields a p-value of 0.02, but I'm not sure if this is a good approach.</p>

<p><strong>UPDATE</strong></p>

<p>Given tristan's update, I've looked into zero-inflated count data regression.  I'm not sure if I'm running things properly (since I'm new to the models) but I'll post code and results:</p>

<pre><code>library(pscl)
library(lmtest)
df&lt;-data.frame('Abundance'=append(a,b))
df$Group &lt;- 'groupA'
df[(length(a) + 1):(length(a) + length(b)),2] &lt;- 'groupB'
summary(m1 &lt;- zeroinfl(Abundance ~ Group | Group, data = df))
summary(mnull &lt;- update(m1, . ~ 1))
lrtest(m1, mnull)
</code></pre>

<p>Returns:</p>

<pre><code>Call:
zeroinfl(formula = Abundance ~ Group | Group, data = df)

Pearson residuals:
      Min        1Q    Median        3Q       Max 
-0.864258 -0.864258 -0.728962 -0.002854  4.105212 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  3.53223    0.07647   46.19   &lt;2e-16 ***
GroupgroupB  2.52612    0.07898   31.98   &lt;2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   0.5878     0.5578   1.054    0.292
GroupgroupB  -0.3001     0.7764  -0.387    0.699
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 10 
Log-likelihood:  -655 on 4 Df
&gt; summary(mnull &lt;- update(m1, . ~ 1))

Call:
zeroinfl(formula = Abundance ~ 1, data = df)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.8018 -0.8018 -0.8018 -0.1681  6.8098 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  5.51672    0.01911   288.6   &lt;2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   0.4353     0.3870   1.125    0.261
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 8 
Log-likelihood: -1705 on 2 Df
&gt; lrtest(m1, mnull)
Likelihood ratio test

Model 1: Abundance ~ Group | Group
Model 2: Abundance ~ 1
  #Df   LogLik Df  Chisq Pr(&gt;Chisq)    
1   4  -654.97                         
2   2 -1705.50 -2 2101.1  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.11794753195637","0.111466460825459","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0948769553749019","0.0952675579132743","176622","<p>In many papers in the social sciences, missing data are handled by <em>direct</em> or <em>full information</em> maximum likelihood estimation (FIML). Unfortunately this is almost always done with closed source software.  </p>

<p>To get an idea how FIML works, I tried to implement it myself in R. </p>

<p>In case of a multivariate normal distribution the loglikelihood for a single observation is given by (according to Enders, 2001, p. 134):</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log|\Sigma_i|-\frac{1}{2}(x_{i}-\mu_{i})^{'}\Sigma^{-1}(x_{i}-\mu_{i})
$$</p>

<p>Where $K_{i}$ is the numer of observed variables for obesrvation $i$. </p>

<p>In case of a simple linear regression model with $N$ observations and assumed homoskedascity the formula should reduce to:</p>

<p>$$
log L_{i} = K_{i} - \frac{1}{2}log(\sigma^{2})-\frac{1}{2\sigma^2}(y_{i}-X_{i}\beta_{i})^{2}
$$</p>



<pre><code>set.seed(42)
x &lt;- matrix(rnorm(1000), nrow=500) ; x &lt;- cbind(1,x)
y &lt;- x %*% c(2,1,3) + rnorm(500, mean = 0, sd = 1)
z.full &lt;- cbind(y,x)

# MCAR predictors
x[sample.int(n = 500, size = 50),2] &lt;- NA
x[sample.int(n = 500, size = 50),3] &lt;- NA
z.miss &lt;- cbind(y,x)

llog.single &lt;- function(z, beta, sigma){
    y &lt;- z[1]
    x &lt;- z[-1]
    idx &lt;- !is.na(x)  # = K_{i}
    return(sum(idx) + dnorm(y, mean = x[idx]%*%beta[idx], sd = sqrt(sigma), log = TRUE))
}

loglikelihood &lt;- function(y, x, theta) {
    p &lt;- ncol(x)
    beta  &lt;- theta[1:p]
    sigma &lt;- theta[p+1]
    return(-sum(apply(cbind(y,x), 1, llog.single, beta = beta, sigma = sigma)))
}

# Full information maximum likelihood 
optim(par = c(0,0,0,1), fn = loglikelihood, method = ""BFGS"", x=z.mis[,-1], y=z.miss[,1])$par
# OLS on full dataset
lm(V1 ~ V3 + V4, data = as.data.frame(z.full))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What should I do, if there is missing data in the outcome $Y_{i}$?</em></p>

<p>From my current point of view I would just ignore cases with missing data in $Y$. Is this correct or is there maybe a better way?</p>

<hr>

<p>Enders, C. K. (2001). A primer on maximum likelihood algorithms available for use with missing data. Structural Equation Modeling, 8(1), 128-141.</p>
"
"NaN","NaN","176788","<p>I'm running a LASSO regression following this <a href=""https://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">guide</a>. I pre - processed my dependent variable using a simple power transformation to obtain a standard normal distribution. Unfortunately, this means I have NA's in my dependent variable, so I can't run LASSO using glmnet (returns: <code>Error in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian,  : 
  NA/NaN/Inf in foreign function call (arg 6)</code>. </p>

<p>Is there anyway to overcome this? </p>
"
"0.06396603026469","0.0642293744423385","176918","<p>In modeling claim count data in an insurance environment, I began with Poisson but then noticed overdispersion. A Quasi-Poisson better modeled the greater mean-variance relationship than the basic Poisson, but I noticed that the coefficients were identical in both Poisson and Quasi-Poisson models. </p>

<p>If this isn't an error, why is this happening? What is the benefit of using Quasi-Poisson over Poisson?</p>

<p><strong>Things to note:</strong></p>

<ul>
<li>The underlying losses are on an excess basis, which (I believe) prevented the Tweedie from working - but it was the first distribution I tried. I also examined NB, ZIP, ZINB, and Hurdle models, but still found the Quasi-Poisson provided the best fit. </li>
<li>I tested for overdispersion via dispersiontest in the AER
package. My dispersion parameter was approximately 8.4, with p-value
at the 10^-16 magnitude.  </li>
<li>I am using glm() with family = poisson or quasipoisson and a log link
for code. </li>
<li>When running the Poisson code, I come out
with warnings of ""In dpois(y, mu, log = TRUE) : non-integer x = ..."".</li>
</ul>

<p><strong>Helpful SE Threads per Ben's guidance:</strong></p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression"">Basic Math of Offsets in Poisson regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/167964/poisson-glm-with-non-count-data-rate-data?lq=1"">Impact of Offsets on Coefficients</a></li>
<li><a href=""http://stats.stackexchange.com/questions/175349/in-a-poisson-model-what-is-the-difference-between-using-time-as-a-covariate-or?"">Difference between using Exposure as Covariate vs Offset</a></li>
</ol>
"
"0.0583927294833815","0.0586331287275243","177388","<p>I am a beginner and trying to learn new concepts in statistical analysis.</p>

<p>I have some very basic question. With a given data set of individuals I am trying to ascertain as to whether or not they are eligible to be granted for a loan - credit scoring.</p>

<p>My data set has 300 obs of 10 variables which I figured out using:
str(data)
summary(data):</p>

<p><a href=""http://i.stack.imgur.com/2q8Dl.png"" rel=""nofollow"">Detailed Data Desc</a></p>

<p>After applying linear regression on all variables:  </p>

<pre><code>linreg=lm(Rating~.,data=data)  
cor(linreg$fitted.values,data$Rating)
</code></pre>

<p><a href=""http://i.stack.imgur.com/DtbQk.png"" rel=""nofollow"">Linear Regression detailed</a></p>

<p><strong>I understand:</strong>  </p>

<ol>
<li>Having 3 stars - p value means very significant and high and positive Estimates indicats that it has a positive significance and vice versa  </li>
<li>The correlation between fitted values and Rating comes to  0.9867324</li>
</ol>

<p><strong>Questions:</strong>  </p>

<ol>
<li>Does this mean regression predicts correctly 98.67% of the observations  </li>
<li>If everything else are equal then variables like Education and Gender have a positive impact on the rating? Because they have lowest p-values.  </li>
<li>What about Student and Income variables?  </li>
<li>Also does it mean individuals with high Income will have a greater rating, everything else being equal?  </li>
</ol>
"
"0.0404556697031367","0.0406222231851194","177522","<p><a href=""http://i.stack.imgur.com/LKZnw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LKZnw.png"" alt=""enter image description here""></a></p>

<p>Here is my multiple regression model diagnostic plot.  my r2 is 89 adj r squared is 88 but my residuals vs fitted and scale-Location doesn't look good. how do i improve this model or fixing the residuals issue</p>

<pre><code>Residuals:
   Min     1Q Median     3Q    Max 
-29454  -2250   1234   2622  24778 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34789.903   7678.279   4.531 2.58e-05 ***
age             48.076      2.213  21.725  &lt; 2e-16 ***
income        -140.777     23.370  -6.024 8.81e-08 ***
language       55.671     38.910   1.431    0.157    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 7759 on 65 degrees of freedom
Multiple R-squared:   0.89, Adjusted R-squared:  0.8849 
F-statistic: 175.3 on 3 and 65 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Also what is the meaning for -ve slope for income coefficient?</p>
"
"0.0700712753800578","0.0703597544730292","177556","<p>I recently read an article <a href=""http://Simultaneous%20Confidence%20Intervals%20Based%20on%20the%20Percentile%20Bootstrap%20Approach"" rel=""nofollow"">Simultaneous Confidence Intervals Based on the Percentile Bootstrap Approach</a> in which the authors present the following algorithm to compute a simultaneous confidence interval.  I was interested in implementing the first algorithm in R.  The algorithm is shown here:</p>

<p><a href=""http://i.stack.imgur.com/0l19J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0l19J.png"" alt=""enter image description here""></a></p>

<p>Would this awesome community mind reviewing my R code below to (1) verify that I've implemented the algorithm correctly and optionally (2) help me gain an intuitive understanding of why this algorithm works?  I read the article, but the notation, and explanation is a bit confusing.  Feel free to answer only one or both of my questions.  Here is my R Code, which uses a toy example with the built-in R datasets ""trees.""  I tried to comment my code and name variables in a way similar to the published algorithm. </p>

<pre><code>library(MASS)
alpha&lt;-.05
B=100000
#Use toy example from R's trees dataset
myfit&lt;-lm(Height~Girth+Volume, data=trees)
myfit
param&lt;-coef(myfit)
coefficients&lt;-length(param)
mycov&lt;-vcov(myfit)
#Simulate 10K bootstrap samples using the estimated mean vector and covariance matrix
#from the regression model
#Step 1.
theata.tilde.b&lt;-mvrnorm(B, mu=param, Sigma=mycov)
head(theata.tilde.b)

#Step 2
rbj&lt;-apply(theata.tilde.b, 2, rank)

#Step 3.
sample.b.rank.upper&lt;-apply(rbj, 1, max)
sample.b.rank.lower&lt;-apply(rbj, 1, min)

boot.df&lt;-data.frame(cbind(theata.tilde.b, rbj, sample.b.rank.upper, sample.b.rank.lower))
names(boot.df)&lt;-c(""B0"", ""B1"", ""B2"", ""Rank0"", ""Rank1"", ""Rank2"", ""SampleBRankUpper"", ""SampleBRankLower"")

upper.index&lt;-B*(1-alpha/2)
lower.index&lt;-B*(alpha/2)
#Sort datasets by max and then min rank of the sample-b rank
upper&lt;-(boot.df[order(boot.df$SampleBRankUpper),])
    lower&lt;-(boot.df[order(boot.df$SampleBRankLower),])

#Step 5
upper[upper.index,1:coefficients]
lower[lower.index,1:coefficients]
</code></pre>

<p>Thanks!</p>
"
"0.0495478739876288","0.0497518595104995","177671","<p>I am analysing the unconditional variance of a time series, with the <code>rugarch</code> package in R. However with an external regressor which is a dummy variable 0 before a certain  date and 1 after this date.</p>

<p>Here is the code:</p>

<pre><code>specgarch &lt;- ugarchspec(variance.model=list(model=""sGARCH"", external.regressors= dummy),
                        mean.model=list(armaOrder=c(0,0)), distribution=""norm"")

garchfit &lt;- ugarchfit(data=return, spec=specgarch)
uncvariance(garchfit)  
</code></pre>

<p>My question is: <strong>How can the unconditional variance of this model be calculated and how can the conditional variance be calculated (i.e., when dummy = 0 and when dummy = 1)?</strong> How does <code>rugarch</code> do it? </p>

<p>Here is the output:</p>

<pre><code>mu                  omega        alpha1         beta1        vxreg1 
-1.938049e-04  1.428670e-06  5.485199e-02  9.420126e-01  1.751348e-06
</code></pre>

<p>and unconditional variance:</p>

<pre><code>0.0007350932
</code></pre>
"
"0.0908377689773195","0.0912117424359157","177805","<p>R and statistics beginner here, trying to do a quantile regression on a non-linear dataset. </p>

<p>I want to identify datapoints that have a higher y axis value that expected given their value on the x axis. 
I should highlight that the y-data are means of discrete values (0.1-1, in steps of 0.1) taken in dependence on the x-data. x values are number of SNPs in a gene. Each SNP has a discrete value and the y value is a mean of these SNP values for each gene.</p>

<p>After initially investigating  funnel plots it seems that a quantile regression might be most appropriate for this dataset, though thoughts on this are welcome.  I'd appreciate any guidance in fitting a quantile regression to identify that don't fall within 95 percent of the data.</p>

<p>Sample of data (I actually have ~20,000 datapoints):</p>

<pre><code>GENE    mean  total
X1  0.1 3
X2  0.1466666667    30
X3  0.1375  8
X4  0.24    5
X5  0.2625  8
X6  0.2 1
X7  0.1466666667    15
X8  0.2 1
X9  0.1666666667    9
X10 0.1 1
X11 0.1928571429    14
X12 0.1 2
X13 0.1545454545    11
X14 0.1333333333    3
X15 0.1666666667    3
X16 0.2117647059    34
X17 0.1452380952    42
X18 0.16    5
X19 0.2 1
X20 0.25    2
X21 0.125   4
X22 0.2 13
X23 0.1714285714    7
X24 0.15    6
X25 0.2 3
X26 0.2894736842    19
X27 0.2352941176    17
X28 0.1333333333    6
X29 0.12    5
X30 0.2 3
X31 0.1 1
X32 0.1571428571    7
X33 0.2125  8
X34 0.18125 16
X35 0.26    10
X36 0.1368421053    19
X37 0.1333333333    6
X38 0.15    2
X39 0.14    5
X40 0.18    15
X41 0.14    5
X42 0.3 1
X43 0.1 2
X44 0.1 6
X45 0.1 4
X46 0.1 1
X47 0.1333333333    3
X48 0.1166666667    6
X49 0.225   4
X50 0.2 15
X51 0.125   12
X52 0.1 3
X53 0.1714285714    14
X54 0.175   4
X55 0.3404761905    42
X56 0.1 1
X57 0.25    2
X58 0.15    4
X59 0.1 1
X60 0.1666666667    3
X61 0.3 2
X62 0.225   4
X63 0.3076923077    13
X64 0.1 1
X65 0.1666666667    3
X66 0.1666666667    6
X67 0.1 3
X68 0.1 3
X69 0.1166666667    6
X70 0.125   8
X71 0.2 1
X72 0.2 2
X73 0.1333333333    42
X74 0.1 1
X75 0.2 8
X76 0.1444444444    9
X77 0.1666666667    15
X78 0.1 2
X79 0.176744186 43
X80 0.1275  40
X81 0.1666666667    3
X82 0.125   4
X83 0.2545454545    11
X84 0.1304347826    46
X85 0.21    10
X86 0.1571428571    7
X87 0.3 9
X88 0.275   16
X89 0.11    10
X90 0.1333333333    6
X91 0.2333333333    3
X92 0.2 2
X93 0.2866666667    15
X94 0.25    2
X95 0.1125  8
X96 0.4 11
X97 0.1 1
X98 0.2 2
X99 0.15    2
X100    0.1625  8
X101    0.24    5
X102    0.175   4
X103    0.15    4
X104    0.1333333333    3
X105    0.4 2
X106    0.2 3
X107    0.25    2
X108    0.32    5
X109    0.2333333333    3
X110    0.1714285714    7
X111    0.2 1
X112    0.225   4
X113    0.2 1
X114    0.1714285714    7
X115    0.15    2
X116    0.1166666667    6
X117    0.16875 16
X118    0.1555555556    9
X119    0.15    6
X120    0.12    5
X121    0.1 1
X122    0.1333333333    6
X123    0.2333333333    3
X124    0.1 1
X125    0.2333333333    3
X126    0.1333333333    3
X127    0.1 1
X128    0.1827586207    29
X129    0.25    8
X130    0.2 7
X131    0.25    6
X132    0.1 1
X133    0.125   4
X134    0.2 1
X135    0.1666666667    3
X136    0.1 3
X137    0.12    5
X138    0.1 1
X139    0.175   4
X140    0.1 1
X141    0.1666666667    3
X142    0.1666666667    3
X143    0.1 1
X144    0.1375  8
X145    0.1 9
X146    0.1 2
X147    0.125   4
X148    0.1333333333    3
X149    0.1769230769    13
X150    0.15    2
X151    0.1214285714    14
X152    0.1 1
X153    0.2555555556    18
X154    0.2 1
X155    0.1 1
X156    0.1 1
X157    0.1 1
X158    0.4 1
X159    0.14    5
X160    0.1 2
X161    0.1333333333    3
X162    0.375   8
X163    0.2263157895    19
X164    0.1636363636    11
X165    0.3 1
X166    0.1 3
X167    0.2 1
X168    0.3 1
X169    0.1428571429    7
X170    0.1 2
X171    0.1222222222    9
X172    0.1 8
X173    0.1 5
X174    0.1 8
X175    0.1666666667    3
X176    0.2 5
X177    0.1 4
X178    0.1166666667    6
X179    0.15    2
X180    0.3666666667    3
X181    0.25    4
X182    0.1 1
X183    0.1 2
X184    0.1 1
X185    0.1 1
X186    0.1 1
X187    0.184   25
X188    0.2333333333    3
X189    0.2333333333    3
X190    0.1 2
X191    0.32    5
X192    0.1 2
X193    0.12    5
X194    0.1 5
X195    0.2 1
X196    0.1 6
X197    0.1 2
X198    0.4 1
X199    0.2 2
X200    0.1 2
X201    0.2 1
X202    0.2333333333    6
X203    0.35    2
X204    0.1 1
X205    0.12    5
X206    0.14    5
X207    0.125   4
X208    0.3333333333    3
X209    0.1 2
X210    0.1 3
X211    0.1 1
X212    0.2 4
X213    0.15    8
X214    0.125   4
X215    0.1548387097    31
X216    0.2 7
X217    0.225   4
X218    0.125   4
X219    0.15    2
X220    0.4 1
X221    0.275   4
X222    0.325   4
X223    0.2 3
X224    0.175   4
X225    0.3 1
X226    0.1 1
X227    0.19    10
X228    0.25    4
X229    0.2666666667    9
X230    0.1 1
X231    0.2 1
X232    0.3 1
X233    0.2166666667    6
X234    0.26    5
X235    0.225   4
X236    0.1 1
X237    0.1857142857    7
X238    0.58    5
X239    0.25    10
X240    0.6066666667    15
X241    0.3 1
X242    0.5 2
X243    0.2333333333    3
X244    0.25    2
X245    0.1 4
X246    0.1 1
X247    0.1714285714    7
X248    0.16875 16
X249    0.2 1
X250    0.4 3
X251    0.1 1
X252    0.1666666667    6
X253    0.2 6
X254    0.3166666667    12
X255    0.1 1
X256    0.1 2
X257    0.4 1
X258    0.1333333333    3
X259    0.225   4
X260    0.2571428571    7
X261    0.4 5
X262    0.15    10
X263    0.1571428571    7
X264    0.2 11
X265    0.2285714286    7
X266    0.15    4
X267    0.3 1
X268    0.1384615385    13
X269    0.1 4
X270    0.1 1
X271    0.16    5
X272    0.1285714286    7
X273    0.1 1
X274    0.2222222222    9
X275    0.2083333333    12
X276    0.2153846154    13
X277    0.1888888889    9
X278    0.1 1
X279    0.1 2
X280    0.3 2
X281    0.17    10
X282    0.1 5
X283    0.2833333333    6
X284    0.1333333333    6
X285    0.1833333333    6
X286    0.1833333333    12
X287    0.1953488372    43
X288    0.2526315789    19
X289    0.1 1
X290    0.125   4
X291    0.26    5
X292    0.1 2
X293    0.2578947368    19
X294    0.2545454545    11
X295    0.1 1
X296    0.3666666667    3
X297    0.1714285714    7
X298    0.1833333333    6
X299    0.16    5
X300    0.2733333333    15
X301    0.275   4
X302    0.1 1
X303    0.2 7
X304    0.1583333333    12
X305    0.1666666667    3
X306    0.1 1
X307    0.1 6
X308    0.1642857143    14
X309    0.1 1
X310    0.1606060606    33
X311    0.1428571429    7
X312    0.1888888889    9
X313    0.2 2
X314    0.1388888889    18
X315    0.35    2
X316    0.3 2
X317    0.1 4
X318    0.15    16
X319    0.1166666667    12
X320    0.1888888889    9
X321    0.16    5
X322    0.2333333333    3
X323    0.1857142857    14
X324    0.31    20
X325    0.2 1
X326    0.1 1
X327    0.1952380952    21
X328    0.215625    32
X329    0.1 1
X330    0.1 1
X331    0.1307692308    13
X332    0.1 4
X333    0.1666666667    3
X334    0.2 14
X335    0.1583333333    12
X336    0.1961538462    26
X337    0.2222222222    9
X338    0.1 3
X339    0.1 2
X340    0.1285714286    14
X341    0.175   4
X342    0.125   4
X343    0.1 4
X344    0.1428571429    7
X345    0.1 4
X346    0.1 2
X347    0.15    2
X348    0.25    4
X349    0.22    5
X350    0.1 2
X351    0.1 3
X352    0.14    10
X353    0.1666666667    18
X354    0.1333333333    3
X355    0.2 3
X356    0.16    5
X357    0.3 1
X358    0.175   4
X359    0.5 1
X360    0.1111111111    9
X361    0.2333333333    6
X362    0.175   4
X363    0.227027027 37
X364    0.3857142857    7
X365    0.1 2
X366    0.2 3
X367    0.1916666667    12
X368    0.1428571429    14
X369    0.2666666667    3
X370    0.2 9
X371    0.25    2
X372    0.2 1
X373    0.1 2
X374    0.225   4
X375    0.1 1
X376    0.1 3
X377    0.3 2
X378    0.1 1
X379    0.1545454545    11
X380    0.1730769231    52
X381    0.1 3
X382    0.1333333333    3
X383    0.1814814815    27
X384    0.108   25
X385    0.2666666667    6
X386    0.1666666667    3
X387    0.25    8
X388    0.225   4
X389    0.24    25
X390    0.2666666667    6
X391    0.1 2
X392    0.15    4
X393    0.1666666667    6
X394    0.1 1
X395    0.2375  8
X396    0.125   4
X397    0.1 7
X398    0.1 7
X399    0.1 4
X400    0.1 2
X401    0.1625  8
X402    0.3 1
X403    0.3 2
X404    0.25    4
X405    0.2 1
X406    0.1285714286    7
X407    0.15    8
X408    0.5 1
X409    0.1 1
X410    0.1285714286    7
X411    0.1 1
X412    0.2166666667    30
X413    0.22    5
X414    0.2714285714    14
X415    0.1214285714    14
X416    0.2 8
X417    0.28    5
X418    0.24    35
X419    0.15    4
X420    0.1333333333    12
X421    0.125   4
X422    0.1 1
X423    0.1666666667    3
X424    0.2111111111    9
X425    0.3 4
X426    0.2 2
X427    0.2 3
X428    0.1 1
X429    0.1 1
X430    0.1617021277    47
X431    0.15    8
X432    0.1142857143    14
X433    0.15    4
X434    0.1384615385    13
X435    0.1 2
X436    0.1166666667    12
X437    0.1714285714    14
X438    0.2416666667    12
X439    0.1 1
X440    0.1428571429    7
X441    0.1 1
X442    0.1416666667    12
X443    0.3333333333    6
X444    0.2 1
X445    0.14    5
X446    0.2 3
X447    0.225   28
X448    0.1571428571    14
X449    0.1 1
X450    0.1583333333    12
X451    0.1518518519    27
X452    0.1363636364    11
X453    0.2 1
X454    0.1666666667    6
X455    0.1 1
X456    0.1333333333    3
X457    0.2368421053    19
X458    0.1222222222    9
X459    0.15    2
X460    0.2 1
X461    0.1625  24
X462    0.2 6
X463    0.1666666667    3
X464    0.1 3
X465    0.3 8
X466    0.1523809524    21
X467    0.1 3
X468    0.1 3
X469    0.15    4
X470    0.1 1
X471    0.1642857143    28
X472    0.1 5
X473    0.1 2
X474    0.12    15
X475    0.1 3
X476    0.1090909091    11
X477    0.1346153846    26
X478    0.125   4
X479    0.1444444444    9
X480    0.2 1
X481    0.1 1
X482    0.1 3
X483    0.2 3
X484    0.1375  8
X485    0.1 4
X486    0.12    5
X487    0.1739130435    23
X488    0.25    2
X489    0.1333333333    6
X490    0.3 1
X491    0.225   20
X492    0.175   4
X493    0.1 3
X494    0.1222222222    9
X495    0.1 1
X496    0.175   4
X497    0.2333333333    6
X498    0.1615384615    13
X499    0.15    8
X500    0.1666666667    6
X501    0.2 2
X502    0.1777777778    9
X503    0.15    4
X504    0.2666666667    3
X505    0.1 4
X506    0.1222222222    9
X507    0.15    2
X508    0.2 3
X509    0.1333333333    15
X510    0.14    5
X511    0.1 1
X512    0.4 1
X513    0.2125  8
X514    0.36    5
X515    0.34    5
X516    0.4 1
X517    0.1428571429    7
X518    0.3333333333    3
X519    0.1 3
X520    0.2277777778    18
X521    0.1916666667    12
X522    0.2 4
X523    0.1857142857    7
X524    0.1 2
X525    0.1 5
X526    0.2222222222    9
X527    0.1818181818    11
X528    0.2151515152    33
X529    0.1 3
X530    0.1214285714    14
X531    0.2 1
X532    0.1 2
X533    0.1 3
X534    0.1166666667    12
X535    0.1 2
X536    0.1 2
X537    0.1 1
X538    0.2379310345    29
X539    0.175   4
X540    0.1363636364    11
X541    0.1 1
X542    0.1479166667    48
X543    0.1928571429    28
X544    0.4 1
X545    0.1951219512    41
X546    0.1333333333    3
X547    0.15    4
X548    0.2833333333    6
X549    0.1547619048    42
X550    0.1555555556    9
X551    0.2363636364    11
X552    0.2142857143    7
X553    0.5 1
X554    0.15    4
X555    0.1709677419    31
X556    0.17    10
X557    0.1 2
X558    0.2866666667    15
X559    0.4 2
X560    0.15    2
X561    0.1424242424    66
X562    0.25    2
X563    0.1 3
X564    0.1285714286    7
X565    0.12    5
X566    0.25    4
X567    0.2263157895    19
X568    0.1 12
X569    0.1666666667    6
X570    0.5 1
X571    0.147826087 23
X572    0.1 1
X573    0.1818181818    11
X574    0.2 2
X575    0.15    2
X576    0.2 3
X577    0.16    15
X578    0.1621621622    37
X579    0.1333333333    3
X580    0.1333333333    12
X581    0.18    5
X582    0.1534482759    58
X583    0.1538461538    26
X584    0.1 9
X585    0.2142857143    7
X586    0.1 1
X587    0.1222222222    9
X588    0.1 1
X589    0.1 3
X590    0.1 6
X591    0.15    2
X592    0.1 2
X593    0.3 1
X594    0.1285714286    21
X595    0.2 2
X596    0.12    5
X597    0.1 1
X598    0.1 1
X599    0.1 2
X600    0.1153846154    13
X601    0.1 15
X602    0.1 1
X603    0.1 1
X604    0.1 4
X605    0.15    10
X606    0.15    4
X607    0.15    4
X608    0.2 1
X609    0.14    5
X610    0.2 1
X611    0.1 2
X612    0.1 3
X613    0.125   4
X614    0.172   25
X615    0.2 4
X616    0.1727272727    11
X617    0.2090909091    22
X618    0.1333333333    3
X619    0.1 7
X620    0.15    4
X621    0.1181818182    11
X622    0.1375  8
X623    0.1666666667    3
X624    0.1 3
X625    0.1090909091    11
X626    0.125   8
X627    0.1 2
X628    0.12    5
X629    0.1 8
X630    0.13    40
X631    0.1666666667    3
X632    0.34    5
X633    0.1714285714    7
X634    0.1636363636    11
X635    0.1 1
X636    0.1 1
X637    0.18125 16
X638    0.2 4
X639    0.2 8
X640    0.1 2
X641    0.1 1
X642    0.1166666667    6
X643    0.2 1
X644    0.6 1
X645    0.2666666667    9
X646    0.2666666667    3
X647    0.2 2
X648    0.1 2
X649    0.1 1
X650    0.1 2
X651    0.1 1
X652    0.125   4
X653    0.15    2
X654    0.1 1
X655    0.1 1
X656    0.35    4
X657    0.2666666667    3
X658    0.1 2
X659    0.1 1
X660    0.2 1
X661    0.1 2
X662    0.1 2
X663    0.1333333333    3
X664    0.1 2
X665    0.1 1
X666    0.225   4
X667    0.1666666667    6
X668    0.1 2
X669    0.1 3
X670    0.175   4
X671    0.1 3
X672    0.15    4
X673    0.1666666667    3
X674    0.1 3
X675    0.175   4
X676    0.25    8
X677    0.25    4
X678    0.2571428571    7
X679    0.1 1
X680    0.2571428571    7
X681    0.208   25
X682    0.325   12
X683    0.1 1
X684    0.25    2
X685    0.1 2
X686    0.3047619048    21
X687    0.24    5
X688    0.15    6
X689    0.1333333333    6
X690    0.3 1
X691    0.1 1
X692    0.15    2
X693    0.23    20
X694    0.2 2
X695    0.1666666667    6
X696    0.1342857143    35
X697    0.25    6
X698    0.2 8
X699    0.2 5
X700    0.5 1
X701    0.1333333333    6
X702    0.3 1
X703    0.15    2
X704    0.15    2
X705    0.1833333333    6
X706    0.15    6
X707    0.1493506494    77
X708    0.36    5
X709    0.3 2
X710    0.15    2
X711    0.38    5
X712    0.2666666667    3
X713    0.25    4
X714    0.225   4
X715    0.5 1
X716    0.1 2
X717    0.16    5
X718    0.3 2
X719    0.3538461538    13
X720    0.1 2
X721    0.175   4
X722    0.22    5
X723    0.175   4
X724    0.2333333333    6
X725    0.34    5
X726    0.2 7
X727    0.1 1
X728    0.3 3
X729    0.1 1
X730    0.1 3
X731    0.3 5
X732    0.35    6
X733    0.2875  8
X734    0.1 1
X735    0.1 2
X736    0.2 5
X737    0.1714285714    7
X738    0.375   4
X739    0.1 4
X740    0.3 1
X741    0.1 1
X742    0.1142857143    7
X743    0.1 1
X744    0.2285714286    7
X745    0.14    5
X746    0.15    6
X747    0.1 1
X748    0.125   4
X749    0.1666666667    6
X750    0.125   8
X751    0.1 1
X752    0.15    2
X753    0.2 1
X754    0.225   4
X755    0.3 1
X756    0.3 5
X757    0.175   4
X758    0.1 3
X759    0.1333333333    18
X760    0.1230769231    13
X761    0.2 1
X762    0.11    10
X763    0.1666666667    6
X764    0.1 1
X765    0.2090909091    11
X766    0.145   20
X767    0.14    5
X768    0.2375  8
X769    0.1571428571    7
X770    0.1 1
X771    0.1 2
X772    0.2 2
X773    0.16    5
X774    0.2 1
X775    0.1777777778    9
X776    0.1210526316    19
X777    0.2 1
X778    0.225   12
X779    0.1666666667    3
X780    0.1 6
X781    0.2333333333    6
X782    0.1692307692    13
X783    0.19    10
X784    0.2 3
X785    0.1489361702    47
X786    0.2 5
X787    0.45    2
X788    0.1666666667    6
X789    0.18    5
X790    0.3 1
X791    0.2 2
X792    0.11    10
X793    0.3333333333    3
X794    0.25    2
X795    0.2 1
X796    0.25    2
X797    0.2 2
X798    0.2 1
X799    0.1 3
X800    0.1333333333    18
X801    0.1473684211    19
X802    0.2 5
X803    0.14    5
X804    0.125   4
X805    0.1583333333    12
X806    0.1857142857    7
X807    0.1 1
X808    0.2 1
X809    0.1769230769    26
X810    0.1 1
X811    0.1 2
X812    0.1833333333    6
X813    0.1409090909    22
X814    0.1416666667    24
X815    0.1307692308    13
X816    0.1235294118    17
X817    0.1 1
X818    0.1 1
X819    0.18    30
X820    0.2514285714    35
X821    0.18    5
X822    0.2 4
X823    0.1 1
X824    0.2333333333    9
X825    0.1222222222    9
X826    0.15    2
X827    0.14    5
X828    0.1588235294    51
X829    0.15    2
X830    0.2 4
X831    0.1 2
X832    0.1391304348    23
X833    0.18    20
X834    0.15    2
X835    0.3 1
X836    0.1 8
X837    0.1666666667    9
X838    0.1954545455    22
X839    0.225   16
X840    0.1222222222    9
X841    0.1210526316    19
X842    0.1 2
X843    0.1 2
X844    0.125   4
X845    0.1 4
X846    0.1 1
X847    0.2 2
X848    0.275   4
X849    0.1 3
X850    0.2833333333    6
X851    0.175   4
X852    0.32    5
X853    0.1 1
X854    0.1428571429    7
X855    0.2277777778    18
X856    0.15    8
X857    0.12    5
X858    0.1 2
X859    0.175   4
X860    0.18    5
X861    0.16    5
X862    0.2333333333    6
X863    0.1 1
X864    0.3333333333    3
X865    0.1 2
X866    0.15    12
X867    0.1636363636    11
X868    0.4 1
X869    0.4 1
X870    0.1 3
X871    0.1555555556    9
X872    0.2 1
X873    0.3 1
X874    0.2 2
X875    0.15    12
X876    0.1 1
X877    0.1181818182    11
X878    0.1428571429    7
X879    0.1461538462    13
X880    0.3076923077    13
X881    0.2 2
X882    0.3 1
X883    0.205   20
X884    0.2 5
X885    0.1333333333    3
X886    0.15    2
X887    0.25    2
X888    0.15    4
X889    0.3 1
X890    0.125   4
X891    0.1875  8
X892    0.1428571429    7
X893    0.2333333333    3
X894    0.1 2
X895    0.1 1
X896    0.35    6
X897    0.1444444444    9
X898    0.2 2
X899    0.3 1
X900    0.1 2
X901    0.1 1
X902    0.25    2
X903    0.1 1
X904    0.1 1
X905    0.7 1
X906    0.2 1
X907    0.45    4
X908    0.25    2
X909    0.15    4
X910    0.1 2
X911    0.4 13
X912    0.1 2
X913    0.1842105263    19
X914    0.1 1
X915    0.1333333333    3
X916    0.2 2
X917    0.1 7
X918    0.1 1
X919    0.225   4
X920    0.2 1
X921    0.2 3
X922    0.18    5
X923    0.1 1
X924    0.1875  8
X925    0.2833333333    6
X926    0.5 3
X927    0.2 1
X928    0.1 1
X929    0.1 2
X930    0.2 3
X931    0.4 1
X932    0.2875  16
X933    0.1857142857    7
X934    0.1 1
X935    0.2 2
X936    0.1 1
X937    0.2 13
X938    0.2444444444    9
X939    0.1 1
X940    0.1714285714    7
X941    0.3 1
X942    0.1 1
X943    0.2857142857    7
X944    0.15    2
X945    0.1 1
X946    0.15625 16
X947    0.1666666667    3
X948    0.3 1
X949    0.2 2
X950    0.1 8
X951    0.1 1
X952    0.1 3
X953    0.3 1
X954    0.3 1
X955    0.1 3
X956    0.1125  8
X957    0.18    5
X958    0.2666666667    3
X959    0.2 1
X960    0.125   4
X961    0.1333333333    3
X962    0.2444444444    9
X963    0.25    10
X964    0.25    4
X965    0.2 1
X966    0.225   4
X967    0.1625  8
X968    0.1333333333    3
X969    0.1333333333    3
X970    0.1 1
X971    0.2 7
X972    0.3 10
X973    0.1 1
X974    0.3 2
X975    0.225   4
X976    0.1 1
X977    0.1 2
X978    0.4 1
X979    0.1333333333    3
X980    0.1333333333    9
X981    0.13125 16
X982    0.1 1
X983    0.2 1
X984    0.1782608696    23
X985    0.2225806452    31
X986    0.15    4
X987    0.1 3
X988    0.1 3
X989    0.15    4
X990    0.2285714286    14
X991    0.2384615385    26
X992    0.4 1
X993    0.4 2
X994    0.1 1
X995    0.1 1
X996    0.1666666667    3
X997    0.1 6
X998    0.13    20
X999    0.2666666667    3
</code></pre>

<p>Code I am using:</p>

<pre><code>Asianpig &lt;- NULL; Asianpig$x &lt;- (Asianpig_data$total)
Asianpig$y &lt;- (Asianpig_data$mean)
plot(Asianpig)

#increase maxiterations for nls
nlc &lt;- nls.control(maxiter = 21811)

# fit first a nonlinear least-square regression
Dat.nls &lt;- nls(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, control = nlc); Dat.nls
lines(1:8000, predict(Dat.nls, newdata=list(x=1:8000)), col=1)

# and finally ""external envelopes"" holding 95 percent of the data
Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.025, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)

Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Asianpig, tau=0.975, trace=TRUE)
lines(1:8000, predict(Dat.nlrq, newdata=list(x=1:8000)), col=4)
</code></pre>

<p>How this looks: </p>

<p><a href=""http://i.stack.imgur.com/tF8Vu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tF8Vu.png"" alt=""enter image description here""></a></p>

<p>I was expecting the quantile regression line to more dynamically follow the slope of the datapoints. 
I adapted the code from an example that was using <code>SSlogis()</code> for the input data:</p>

<pre><code># build artificial data with multiplicative error
Dat &lt;- NULL; Dat$x &lt;- rep(1:25, 20)
    set.seed(1)
    Dat$y &lt;- SSlogis(Dat$x, 10, 12, 2)*rnorm(500, 1, 0.1)
plot(Dat)
</code></pre>

<p>I have a feeling I should not be using <code>SSlogis()</code> in my code, but instead should be modelling an exponential distribution. SSlogis is a selfStart model evaluates the logistic function and its gradient. It has an initial attribute that creates initial estimates of the parameters Asym, xmid, and scale.</p>

<p>But I am still trying to understand how to fit a quantile regression for this non-linear data.</p>

<p>Here is a hexbin plot that gives a feeling for how the data is clustered:<a href=""http://i.stack.imgur.com/NCrLX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NCrLX.png"" alt=""enter image description here""></a></p>
"
"0.13731109624575","0.143621247405357","177823","<p><strong>I need to perform manually two-stage Least Squares(to illustrate its advantages)</strong>, where the first stage is <em>repeated median estimate</em> and the second stage should be weighted least squares, where weights are obtained(as far, as I understand) from polynomial regression of first-stage residuals on regressors.</p>

<p>Suppose I have generated the following heteroscedastic model:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=Y_i%20%3D%20b_0%2Bb_1%20X_i%20%2B%20%5Cepsilon_i"" alt=""Y_i = b_0+b_1 X_i + \epsilon_i""></p>

<p>where error depends on regressor:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cepsilon_i%20%5Csim%20N[0%2C[X_i-1]%5E2]"" alt=""\epsilon_i \sim N(0,(X_i-1)^2)""></p>

<pre><code>set.seed(100)
b&lt;-c(12,7.25) ## my coefficients
num&lt;-50 ## number of observations

raw_x&lt;-runif(num,min=0,max=2) ## regressors

my_y&lt;-as.vector(b%*%t(data.frame(rep(1,num),raw_x))+
     rnorm(num,mean=0,sd=(raw_x-1)^2)) ## observations

l&lt;-lm(my_y~raw_x) ## let's create linear model

plot(fitted(l),residuals(l)) ## we see heteroskedasticity
## we got to higher values, our residuals explode

abline(0,0)
title(""Residual vs Fit. value"");
</code></pre>

<p><a href=""http://i.stack.imgur.com/4S6pY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4S6pY.png"" alt=""Residual vs fit value""></a></p>

<p>So I perform repeated median regression (formula in the <a href=""http://www.cs.huji.ac.il/~werman/Papers/rep.pdf"" rel=""nofollow"">Introduction</a>):</p>

<pre><code>## Generating first model using repeated median

## slope 

fij = function(i,j)
{
  (my_y[i]-my_y[j])/(raw_x[i]-raw_x[j])
}

bij&lt;-outer(1:num,1:num,fij) ##NaN's were produced on the diagonal    

rowmeds &lt;- apply(bij, 1, median,na.rm=TRUE)
b_med&lt;-median(rowmeds)

# colmeds &lt;- apply(bij, 2, median,na.rm=TRUE) ## column medians are the same
# b_med3&lt;-median(colmeds)

## Intercept    

## med(y_i - b*x_i)

a_med&lt;-median(my_y-b_med*raw_x)
</code></pre>

<p><strong>The fit is extremely accurate!</strong> In this example <code>a_med</code> is 11.97634 and <code>b_med</code> equals 7.27022.</p>

<p>Now I perform 2nd order polynomial regression of residuals:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon_i%7D%3Da_0%20%2B%20a_1X_i%2Ba_2X_i%5E2%2B%5Cdelta_i"" alt=""\hat{\epsilon_i}=a_0 + a_1X_i+a_2X_i^2+\delta_i""></p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon%7D%20%3D%20%5Cbegin%7Bpmatrix%7D%201%20%26%20X_1%20%26%20X_1%5E2%20%5C%5C%20%5Cvdots%20%5C%5C%201%20%26%20X_m%20%26%20X_m%5E2%20%5Cend%7Bpmatrix%7D%5Cbegin%7Bpmatrix%7Da_0%20%5C%5C%20a_1%20%5C%5C%20a_2%20%5Cend%7Bpmatrix%7D%2B%5Cdelta"" alt=""\hat{\epsilon} = \begin{pmatrix} 1 &amp; X_1 &amp; X_1^2 \\ \vdots \\ 1 &amp; X_m &amp; X_m^2 \end{pmatrix}\begin{pmatrix}a_0 \\ a_1 \\ a_2 \end{pmatrix}+\delta""></p>

<p>so that (<strong>X</strong> here is m x 3 matrix):</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7Ba%7D%20%3D%20%5BX%5ETX%5D%5E%7B-1%7DX%5ET%5Chat%7B%5Cepsilon%7D"" alt=""\hat{a} = [X^TX]^{-1}X^T\hat{\epsilon}""></p>

<p>I was told that as long as residual variances can be roughly estimated from only one observation, actual fit from this model can be used; residual variances = coefficients for the weighted least squares:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Chat%7B%5Csigma%7D%7D%3DX%5Chat%7Ba%7D"" alt=""\hat{\hat{\sigma}}=X\hat{a}""></p>

<pre><code>## Obtaining 2nd order polynomial estimator for residuals

Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))

## coef_var = (X^T*X)^(-1)*X^T^e
coef_var&lt;-solve(t(Xmatr)%*%Xmatr)%*%t(Xmatr)%*%t(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))

## Obtaining sigma (residual deviation) esitmate

my_sigma&lt;-t(Xmatr%*%coef_var)

## performing regression with sigma-weights

b_wls&lt;-lm(as.numeric(my_y)~raw_x,weights=as.numeric(my_sigma^2))$coef

## final plot

library(scales)
plot(raw_x,my_y, pch=20,col=alpha(""salmon"",0.6))

abline(b[1],b[2], col=""black"") ## real line
abline(a_med,b_med,col=""blue"") ## repeated median fit
abline(b_wls, col=""magenta"")
legend('bottomright', c(""Real"",""Repeat median"",""Two-Level LS"") , 
       lty=1, col=c('black', 'blue','magenta'), bty='n', cex=.75)
</code></pre>

<p>The resulting fit is always worse (and sometimes turns into complete garbage). <strong>Please, can you explain me what I'm doing wrong? I need to obtain the result where two-level LS is better than repeated median fit(provided the error depends on regressors as shown before)</strong>.</p>

<p><a href=""http://i.stack.imgur.com/Wo0ZM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Wo0ZM.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/huTRw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/huTRw.png"" alt=""enter image description here""></a></p>

<p>EDIT: Using R function lm seems to produce the same picture:</p>

<pre><code>Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))
residual&lt;-as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))
polycoef&lt;-lm(residual ~ poly(raw_x, 2, raw=TRUE))$coefficients
my_sigma&lt;-t(Xmatr%*%polycoef)
</code></pre>

<p>EDIT2: Checking the quality of the residual fit (as far as I understand what's going on):</p>

<pre><code>plot(raw_x,abs(residual))
lines(sort(raw_x),(sort(raw_x)-1)^2) ## real residuals
lines(sort(raw_x), my_sigma[order(raw_x)],col = ""magenta"") ## fitted residuals
legend('topleft', c(""Real res"",""Fitted res"") , 
       lty=1, col=c('black','magenta'), bty='n', cex=.75)
</code></pre>

<p><a href=""http://i.stack.imgur.com/3J5D3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3J5D3.png"" alt=""enter image description here""></a></p>

<p>EDIT3: As long as my standard deviation is $(X_i-1)^2$, so the variance has power 4 and maybe I should do for the residual fit</p>

<pre><code>residual&lt;-abs(as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x)))
</code></pre>

<p>this makes residual fit by the quadratic function better, but the regression line moves even more far than before.
<a href=""http://i.stack.imgur.com/nt1Ij.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nt1Ij.png"" alt=""enter image description here""></a></p>
"
"0.0700712753800578","0.0703597544730292","177987","<p>Good afternoon,
I need help in intpretting the significance results when performing a linear regression with a categorical interaction.  I'm running this analysis in R.</p>

<p>The question is whether I can use qsec as an explantory variable in the below model given that one of the categorical variables is not significantly different to the other.</p>

<p>Model &amp; Outputs</p>

<pre><code>lm(mpg~qsec+am+qsec*am, data=mtcars)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  -9.0099     8.2179  -1.096  0.28226   
qsec          1.4385     0.4500   3.197  0.00343 **
am1         -14.5107    12.4812  -1.163  0.25481   
qsec:am1      1.3214     0.7017   1.883  0.07012 . 
</code></pre>

<p>The second coefficient is not statistical significant - though only just with a p value of 0.07.
Does this mean that I should look for another explanatory variable?</p>

<p>When I run the regression on one of the values - am1 - alone, then it is still significantly different to zero.  As is am0 from the above results.</p>

<p>When I look at the data, the slope appears significantly different so I think I can use it however the understanding of the theory says no - which is frustrating.  I would add my plot to highlight my point but I'm struggling to load it.</p>

<p>I hope this isn't a stupid question to ask...I've found some good explanations of categorical variables in general, however nothing that tells me how I should interpret this secondary variable.</p>

<p>James</p>

<p>** edit changed data to be qsec per question below</p>
"
"0.11100944184129","0.11843311462705","178861","<p>This is my first question in Cross Validated.  I was redirected here from StackOverflow, so I hope this is the right place and I get the question right...  I've searched for this topic in the forums and didn't find any similar question, so I hope somebody can help me.</p>

<p>I am trying to fit a loess model with two predictors, one of them is locally weighted while the other is parametric.  Once the model is fitted, I want to get the slope of the regression line on the parametric predictor, conditioning on a certain value of the non-parametric predictor.  When computing this slope with whichever two values from the regression line, I understand I should get a constant value corresponding to the slope.  However, if I compute the slope with every two successive values, I get a curve resembling much of a parabola.  Here you have some code for showcasing this issue:</p>

<pre><code>## DATA

criterion &lt;- c(
  -1.8914741214789, -0.864604956700496, 2.43013372852099, -1.32227168040904, 
  0.861585875211724, -1.25506145955845, 1.1201893940246, -1.18560159611826, 
  1.24871681364416, -1.24362634687504, -2.22058652097061, -0.67920682239687, 
  -1.14096010679208, 0.228758533000768, -1.3607742780652, 0.473165865126464, 
  0.0438948075908679, -1.14355404117161, 2.60406860120487, 0.539583593348819, 
  -0.599388766026817, -1.14918693916554, -0.17334788506616, -0.836478866743926, 
  2.88908329995278, -0.401016464464891, -0.292311619619775, 1.12804091879547, 
  3.6733647991105, -3.31190363769332, -0.672558641084861, 0.498844902537884, 
  -0.037062115762172, -1.02017987978252, -0.854805324374525, 1.34168735130207, 
  -0.242996720017973, 1.14871933640721, -0.736312690622741, -2.51965992948912, 
  -3.16327863554555, -0.269020543067839, 0.552150179356176, 0.320523449469915, 
  1.14736382025547, 0.891590469554733, -0.717678520852477, 3.15631635301073, 
  -0.225648864790169, -1.35189421566795, -0.558073572773821, -1.33356547103824, 
  -2.01215450549957, 1.63719762873429, -2.0218275161466, 0.513289109719022, 
  -1.30263029019454, 1.85949704911488, -1.22544429584118, 0.336732253617053, 
  -2.45126282746359, -4.94155955259729, 0.743231639698684, 1.04320562270113, 
  3.99232225357353, -1.07752259470057, -0.353671379249384, -0.748973055922694, 
  0.467443998802691, 0.966013090920868, 1.32739645813748, 1.07159468103619, 
  -1.8542024758483, 0.360922743179635, -4.99642432601298, 0.596072047320551, 
  -1.48256500350222, -0.251689130094422, -0.104867519535428, -3.23675187957067, 
  1.15657910171856, -0.640772355492231, 2.21198640279181, -0.229386564567888, 
  -2.8014148535931, 0.325261825780768, 1.65431768179619, -0.701353356393564, 
  1.56301740126489, -2.91989037858617, 0.560634128846807, -3.40972988669857, 
  0.519955616184439, -0.673752119923202, -0.126511467211613, -1.49156456253545, 
  2.68041989003066, -3.18246878051744, -1.05338046600476, -0.122679130411665, 
  0.619202563903638, -2.80132012240656, -1.50106228060585, -1.78428153598023, 
  -0.17959372353835, -3.7657930817963, 1.74830598714522, 0.199267717912346, 
  -0.187088254090319, -0.431926901631399, -2.50168001668916, -0.715294537723936, 
  4.8050892573889, 3.48017935641437, -2.29413209640673, 1.88045620792631, 
  -0.125724128270772, -0.514660621563394, 1.28920199656138, -0.888250921411933, 
  -1.53336797414911, 0.566890809767711, 2.18492239723917, 1.45986142278563, 
  2.29475550546227, -2.91360806155925, -1.28474245565384, -1.15236199251384, 
  -2.68344935749574, -1.0406761060411, 0.236606541573282, -1.0577344636865
)

lw.predictor &lt;- c(
  3.97543828892376, 3.74367045733871, 2.8031293667213, 3.12721154621725, 
  3.57809163186571, 3.85490258924953, 3.04509486547235, 3.06527167000886, 
  3.42172751371031, 3.48342454710053, 3.03382754767655, 3.29840143930276, 
  3.42532870135535, 3.3466401061363, 4.19719410513314, 4.27624851591474, 
  3.90521253346176, 3.66434131328865, 3.78008480009251, 3.26961939052607, 
  3.37557706887951, 3.07887188021758, 3.20615845753053, 3.25681582455448, 
  3.07575583761175, 3.4678563115072, 3.63412290863538, 2.7341072520575, 
  3.04486992771651, 4.0244118093156, 2.75978954925269, 2.94469571886678, 
  3.64916954335701, 3.23529338509991, 3.09993371559714, 3.92201374051927, 
  2.63693470862178, 3.19438720086359, 4.10395732841177, 4.14695795420843, 
  3.37963279191973, 2.75059146814963, 3.05430305033533, 4.07380539679535, 
  3.41070032578776, 3.52175236580135, 3.9922870844146, 2.83689005875791, 
  3.59280102122038, 3.744585131656, 4.07464596327428, 2.94632345777042, 
  3.79563556141373, 2.92157773101387, 3.60631105869347, 3.90380916892684, 
  3.4349134104059, 2.86428168814471, 2.80459505537921, 3.52738795051305, 
  3.90100092529829, 3.95557522270349, 2.88144750533953, 3.4177217437656, 
  2.72281079587565, 3.44307922077723, 3.58191805968554, 2.85374063580388, 
  3.61825659978916, 3.35154840518957, 2.78055872970075, 3.31559205647068, 
  3.72845408487401, 3.45439961676827, 3.47673283886995, 3.38348122582045, 
  2.88524826215121, 3.37314129436951, 4.17608980116535, 2.78621853030773, 
  4.23989532049638, 2.65428059546312, 2.75954135557898, 3.90836826127649, 
  3.70911502202012, 3.9502037401938, 2.72934334016319, 3.56908337873796, 
  3.53107535330031, 4.10445798400541, 3.37029733251965, 2.77784779211612, 
  4.00205426701893, 3.390559011573, 3.75061638769833, 3.67591196400612, 
  4.00034245109432, 4.19507212536165, 2.64290209936905, 2.84965751366817, 
  2.93584367468566, 3.5792399897338, 3.87103752330681, 4.11112756588916, 
  3.69820393282565, 3.47909608792875, 3.021611653787, 3.38833615773409, 
  3.53688974115618, 3.86802841711593, 3.04014239032067, 3.83441517392514, 
  2.78055872970075, 2.95676609717858, 3.46963343371941, 2.69652236718191, 
  4.21169279083643, 3.59508797308864, 2.90229007358549, 3.74055889165213, 
  4.22533130742836, 3.10942006661772, 2.81532012340698, 3.39540382330109, 
  3.11580153059886, 2.73435775434875, 3.80464748245529, 3.63431137604942, 
  4.09744323888573, 2.77908036429911, 3.30047734198123, 3.7238586557053
)

par.predictor &lt;- c(
  94.3333333333333, 105.333333333333, 135.666666666667, 113.666666666667, 
  116.333333333333, 112.666666666667, 117.333333333333, 100.333333333333, 
  118, 118.333333333333, 119.666666666667, 109.666666666667, 116.333333333333, 
  107.666666666667, 110.666666666667, 97, 99.6666666666667, 108.666666666667, 
  118.333333333333, 119, 106.333333333333, 121.666666666667, 103, 
  132, 99, 121.666666666667, 99.6666666666667, 104.666666666667, 
  124.333333333333, 106.666666666667, 118.666666666667, 111, 125, 
  101.666666666667, 101, 103, 102, 108.333333333333, 119.333333333333, 
  131.333333333333, 105.666666666667, 118.666666666667, 124.333333333333, 
  111.333333333333, 108.333333333333, 126.666666666667, 111, 107.333333333333, 
  118.666666666667, 120.333333333333, 114, 117.666666666667, 118, 
  104.666666666667, 115.333333333333, 117.666666666667, 101.666666666667, 
  118, 103.333333333333, 120, 105, 106.666666666667, 104.333333333333, 
  112.333333333333, 109.333333333333, 103.333333333333, 114, 112, 
  122.333333333333, 115, 135.666666666667, 102.666666666667, 116, 
  115.333333333333, 118, 107, 104, 113.666666666667, 130, 128.333333333333, 
  110.333333333333, 127.666666666667, 129.333333333333, 107, 110.666666666667, 
  97.3333333333333, 120.333333333333, 90.6666666666667, 122.333333333333, 
  104, 93.6666666666667, 102.333333333333, 111.333333333333, 121.666666666667, 
  127.666666666667, 115, 103, 91.6666666666667, 150.666666666667, 
  127, 126.333333333333, 111.666666666667, 117.666666666667, 109, 
  103, 104, 103.666666666667, 109, 122.666666666667, 122, 116.333333333333, 
  117.333333333333, 102, 156, 105.666666666667, 106.333333333333, 
  124.333333333333, 105.333333333333, 121.333333333333, 92.6666666666667, 
  117.666666666667, 122.666666666667, 88.3333333333333, 119, 121.333333333333, 
  97, 107, 109.333333333333, 113.666666666667, 103.666666666667, 
  117, 112.666666666667
)

# Predictor variables are standardized prior to fitting the loess model
data &lt;- data.frame(criterion = criterion, lw.predictor = scale(lw.predictor), par.predictor = scale(par.predictor))


## MODEL FIT

lwr.fit &lt;- loess(
  criterion ~ lw.predictor * par.predictor, data,
  parametric = ""par.predictor"",
  span = .7,
  family = ""gaussian"", degree = 2, drop.square = ""par.predictor"",
  normalize = FALSE
)


## PREDICTION ON GRID

# Statistics to standardize axes (based on the sample)
par.mean &lt;- mean(par.predictor)
par.sd &lt;- sd(par.predictor)
lw.mean &lt;- mean(lw.predictor)
lw.sd &lt;- sd(lw.predictor)

# Axes (standardized with the sample parameters)
pred.lw.axis &lt;- (sqrt(seq(7, 18.25, by = 1/12)) - lw.mean) / lw.sd
pred.par.axis &lt;- (89:156 - par.mean) / par.sd

# Grid generation
std.prediction.values &lt;- expand.grid(
  lw.predictor = pred.lw.axis,
  par.predictor = pred.par.axis
)

# Prediction
lwr.prediction &lt;- predict(lwr.fit, std.prediction.values, se = TRUE)


## GRAPHIC REPRESENTATION OF THE PARAMETRIC REGRESSION

# Slope computing and plotting:

# Values for the parametric predictor, conditioned on the first value of the locally weighted predictor
test.values &lt;- lwr.prediction$fit[1, ]

# Slope computed with every two successive values of the parametric predictor
slopes &lt;- (test.values[-1] - test.values[-length(test.values)]) / (pred.par.axis[-1] - pred.par.axis[-length(pred.par.axis)])

# Plotting the results
plot(slopes, type = ""l"", ylab = ""Slopes"")
</code></pre>

<p>Well, if you run this code, you will get the plot of a (near) parabolic curve as I said.  I may be getting it wrong, but I think the plot should show a constant line.  That is very weird in my opinion; one could think it is due to quantification error, but I doubt this as it would be somehow random and the values much smaller.  On the other hand, I have tried also changing the parameters of the loess model, but the problem seems to be persistent, and happens also with many different data sets.</p>

<p>Thank you so much in advance for your help.</p>
"
"0.124692748408752","0.12520610071704","179049","<p>I'm trying to learn some basic Machine Learning and some basic R. I have made a very naive implementation of $L_2$ regularization in R based on the formula:</p>

<p>$\hat w^{ridge} = (X^TX +\lambda I)^{-1} X^T y$ </p>

<p>My code looks like this:</p>

<pre><code>fitRidge &lt;- function(X, y, lambda) {
     # Add intercept column to X:
  X &lt;- cbind(1, X)
     # Calculate penalty matrix:
  lambda.diag &lt;- lambda * diag(dim(X)[2])
     # Apply formula for Ridge Regression:
  return(solve(t(X) %*% X + lambda.diag) %*% t(X) %*% y)
}
</code></pre>

<p>Note that I'm not yet trying to find an optimal $\lambda$, I'm simply estimating $\hat w^{ridge}$ for a given $\lambda$. However, something seems off. When I enter $\lambda = 0$ I get the expected OLS result. I checked this by applying lm.ridge(lambda = 0) on the same dataset and it gives me the same coefficients. However, when I input any other penalty, like $\lambda=2$ or $\lambda=5$ my coefficients and the coefficients given by lm.ridge disagree wildly. I tried looking at the implementation of lm.ridge but I couldn't work out what it does (and therefore what it does differently).</p>

<p>Could anyone explain why there is a difference between my results and the results from lm.ridge? Am I doing something wrong in my code? I've tried playing around with <code>scale()</code> but couldn't find an answer there.</p>

<p>EDIT:</p>

<p>To see what happens, run the following:</p>

<pre><code>library(car)
X.prestige &lt;- as.matrix.data.frame(Prestige[,c(1,2,3,5)])
y.prestige &lt;- Prestige[,4]

fitRidge(X.prestige, y.prestige, 0)
coef(lm.ridge(formula = prestige~education+income+women+census, data = Prestige, lambda = 0))
fitRidge(X.prestige, y.prestige, 2)
coef(lm.ridge(formula = prestige~education+income+women+census, data = Prestige, lambda = 2))
</code></pre>

<p>EDIT2:</p>

<p>Okay, so based on responses below, I've gotten a somewhat clearer understanding of the problem. I've also closely re-read the section about RR in TESL by Hastie, Tibshirani and Friedman, where I discovered that the intercept is often estimated simply as the mean of the response. It seems that many sources on RR online are overly vague. I actually suspect many writers have never implemented RR themselves and might not have realized some important things as many of them leave out 3 important facts:</p>

<ol>
<li>Intercept is not penalized in the normal case, the formula above only applies to the other coefficients.</li>
<li>RR is not equivariant under scaling, i.e. different scales gives different results even for the same data.</li>
<li>Following from 1, how one actually estimates intercept.</li>
</ol>

<p>I tried altering my function accordingly:</p>

<pre><code>fitRidge &lt;- function(X, Y, lambda) {
  # Standardize X and Y
  X &lt;- scale(X)
  Y &lt;- scale(Y)
  # Generate penalty matrix
  penalties &lt;- lambda * diag(ncol(X))
  # Estimate intercept
  inter &lt;- mean(Y)
  # Solve ridge system
  coeff &lt;- solve(t(X) %*% X + penalties, t(X) %*% Y)
  # Create standardized weight vector
  wz &lt;- c(inter, coeff )
  return(wz)
}
</code></pre>

<p>I still don't get results equivalent to lm.ridge though, but it might just be a question of translating the formula back into the original scales. However, I can't seem to work out how to do this. I thought it would just entail multiplying by the standard deviation of the response and adding the mean, as usual for standard scores, but either my function is still wrong or rescaling is more complex than I realize.</p>

<p>Any advice?</p>
"
"0.0872741054526671","0.103566754353222","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.0286064783845312","0.0287242494810713","179541","<p>As the complexity parameter is calculated? What is the meaning of it?</p>

<p>From what I read, the cp is a value at which the tree makes divisions in the nodes until the reduction in the relative error is less than a certain value.</p>

<p>There are places I read that say the CP affects only the growth of the tree and others say that interferes with pruning too. For min appears that it interferes only in growth but not sure.</p>

<p>I am using rpart () package to create trees, in the case of the classification tree exists missclassification rate to evaluate the ratings, but in the case of regression is not anything to evaluate the predictions beyond the MSE?</p>
"
"0.0404556697031367","0.0406222231851194","179803","<p>Can Theil-Sen be defined for multiple linear regression? If so, is there an implementation in R for it?</p>

<p>I simply want a formula <code>a~b+c</code>, but the package <code>mblm</code> fails with the (misleading) error message: <code>stop(""Only linear models are accepted"")</code>. Of course, this is a linear model: but they mean that Theil-Sen only works for two-dimensional linear models. </p>
"
"NaN","NaN","180337","<p>I always report odds ratios when using logistic regression for predictions. 
I wanted know is it meaningful to report odds ratios when modeling with gradient boosting approach? 
I am using gbm package in R to make the predictions.</p>

<p>Thanks!</p>
"
"0.107274293941992","0.114896997924285","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.06396603026469","0.0642293744423385","180488","<p>I am trying to compare the effect of breastfeeding on child outcomes using PSM in R. Breastfeeding is my treatment whereby mothers who breastfed are coded as 1(treatment) and those who don't as 0(control). My initial sample is comprised of 10537 obs. and I have 15 vars I want to match on. I have played around with different matching and what seems to work best for this cohort is nearest neighbour with caliper .25. </p>

<p>My syntax for calculating the propensity score was: </p>

<pre><code>m.out=matchit(treated~partner+matage+matedu+class3+mcard+matwork+ethnic+matdep+smoke+sib+prem+nicu+bweight+bsex+delivery,data=psa3.covars,method=""nearest"",caliper=0.25*ps.sd)
</code></pre>

<p>This resulted in a subset of data with only the matched participants (i.e., 3010 Treated and 3010 Control). I checked the results of the balance with:</p>

<pre><code>#post-matching balance
match.data&lt;-match.data(m.out)
treated1&lt;-match.data$treat==1
str(treated1)
treated1
summary(treated1)
cov1&lt;-match.data[ ,1:16]
std.diff1&lt;-apply(cov1,2,function(x)100*(mean(x[treated1])-mean(x[!treated1]))/(sqrt(.5*(var(x[treated1])+var(x[!treated1])))))
std.diff1
</code></pre>

<p>which revealed a good balance. My question is this: I now want to run my post regression analyses with the matched sampled only both pre and post matching.</p>

<p>I was given the following syntax for post matching:</p>

<pre><code>mod&lt;-glm(vocab3~treated+partner+matage+matedu+class3+mcard+matwork+ethnic+matdep+smoke+sib+prem+nicu+bweight+bsex+delivery,data=psa3.covars)
</code></pre>

<p>but it doesn't seem logical as the distance score created from the matching is not included yet all the matching variables are. Would this syntax be my pre-matched regression and then I would just add ""+distance"" to the same syntax at the end of my covariates list?</p>
"
"0.103142124625879","0.103566754353222","180521","<p>I have a time series that includes some rare extreme values. We are talking about daily data, in total 1461 observations and 11 extreme values. I adjusted those 11 values with a multiple regression. Now I am using the <code>tbats()</code> on the original time series and the adjusted one. </p>

<pre><code>accuracy(original)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 10.23539 4202.19 2921.593 NaN  Inf 0.6777689 -0.0003493096
accuracy(adjusted)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 43.35625 3803.618 2787.39 NaN  Inf 0.6827622 -0.004749092

#original AIC
&gt;35101.43
#adjusted AIC
&gt;34798.24
</code></pre>

<p>How can I see if the model improves due to the adjustment or not? Since I reduced those 11 extreme values, I can't just compare MAE, RMSE or AIC. MASE is the only measure that should work?</p>

<p>I could divide MAE, RMSE and AIC by the mean of the respective time series.</p>

<pre><code># original
0.4962245 # MAE/mean(original)
0.7137304 # RMSE/mean(original)
5.96188 # AIC/mean(original)

# adjusted
0.4862567 # MAE/mean(adjusted)
0.6635364 # RMSE/mean(adjusted)
6.07051 # AIC/mean(adjusted)
</code></pre>

<p>Is that a legitimate way to compare the results?</p>

<p>Here are the <code>pacf</code>-diagrams of both models:</p>

<p><strong>original</strong>:</p>

<p><a href=""http://i.stack.imgur.com/nFARp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nFARp.png"" alt=""original""></a></p>

<p><strong>adjusted</strong>:</p>

<p><a href=""http://i.stack.imgur.com/YXIGF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YXIGF.png"" alt=""adjusted""></a></p>

<p><strong>Update:</strong></p>

<p>I just realized that when i use the <code>accuracy()</code> function of the <code>forecast</code> package with a <code>tbats()</code> based on a <code>msts()</code> object the resulting MASE is using an in-sample naive forecast for scaling. I guess that is not optimal? It should be better to use an in-sample naive seasonal forecast with the longest season of the <code>msts()</code> object.</p>

<pre><code>MASE(original) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6339

MASE(adjusted) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6287
</code></pre>
"
"0.0700712753800578","0.0586331287275243","180546","<p>I am trying to use Random Forest regression. I have a response variable:</p>

<pre><code>y = rnorm(10000, mean=0, sd=3)
</code></pre>

<p>And a few predictor variables (which are just the response with added noise):</p>

<pre><code>x = data.frame(v1=y + rnorm(10000, mean=0, sd=3), v2=y + rnorm(10000, mean=0, sd=3), v3=y + rnorm(10000, mean=0, sd=3))
</code></pre>

<p>I build the random forest:</p>

<pre><code>r = randomForest(x, y)
</code></pre>

<p>The model is good, explaining ~73% of the variance. However, when I look at the residuals:</p>

<pre><code>plot(y, y - r$predicted)
</code></pre>

<p><a href=""http://i.stack.imgur.com/3OF4z.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3OF4z.png"" alt=""Instead of being centered around zero, the residuals are correlated with the response variable""></a></p>

<p>Instead of being centered around zero, they are correlated with the response variable. It seems that the model should correct this. Maybe, since each OOB prediction is an average, this behavior is some kind of ""regression to the mean""? Does anyone know why this happens? Is anything I can do about it?</p>

<p>I am trying to build a model and use the residuals to estimate something. Right now, they are useless because they only reflect the value I'm trying to predict. If anyone can help, I'd really appreciate it!</p>
"
"0.0495478739876288","0.0497518595104995","180813","<p>I know linear regression is the workhorse of machine learning. I understand the internals of it and I am playing with some real data samples.</p>

<p>Obviously using a simple line (polynomial degree = 1) is not very useful for most of the datasets, my understanding is that as I increase the polynomial degree I will</p>

<ul>
<li>Get a more accurate prediction</li>
<li>Eventually will face the danger of overfiting</li>
</ul>

<p>Now, I have been playing with R and some datasets and this is what I got...</p>

<pre><code>stock &lt;- EuStockMarkets[, 'DAX']
plot(stock)
model &lt;- lm(stock ~ lm(poly(time(stock), 1, raw=TRUE)))
points(time(stock), predict(model), type=""l"", col=""blue"", lwd=2)
model10 &lt;- lm(stock ~ poly(time(stock), 10, raw=TRUE))
points(time(stock), predict(model10), type=""l"", col=""red"", lwd=2)
text(1994, 5000, paste(""Degree, blue=1, red=10""), pos=2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/0BLlF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0BLlF.png"" alt=""linear regression output""></a></p>

<p>Now, the red line is definately a muuuuch better fit thant the blue one, that said, two questions come to my mind</p>

<ol>
<li>The red line is almost always (if not always) ascending, meaning that technically any time would be good to buy shares, that does not represent the truth (from 1994 to 1995 there are ups and downs, not to mention 1997 to 1998)</li>
<li>Does R automatically apply regularization to prevent overfiting.</li>
</ol>

<p>I am fully aware that this is a very simplistic example (normally I would use more features, not just the date, in order to predict the price.</p>

<p>Would a more sophisticated tech such as neuronal network provide a better output here?</p>
"
"0.0700712753800578","0.0703597544730292","181237","<p>I am using the coxph function to model a Cox regression.
By using stepwise BIC selection I obtained an model with 6 variables.
One of the variables I had to transform using the logarithm to make it fulfill the proportional hazard criterion. All Variables are marked as high significant with very low p-values- What I am now confused about is the fact, that the confidence intervals of two variables are very high. Therefore I wonder how ""useful"" those variables are! Since they are selected by the algorithm I have no doubt that they are useful in a mathematical way, but what is the meaning of a variable whose 95% confidence interval is spanning over a wide range.</p>

<p><a href=""http://i.stack.imgur.com/y7jVq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y7jVq.jpg"" alt=""Cox Regression Result""></a></p>

<p>As you can see the variables log(F) and especially E have a very high confidence interval. How can E be so important for the model (there have been more than 70 variables to choose from) and still be so ""uncertainly"" determined.
I hope I was able to formulate my problem in an understandable way.</p>

<p>Thanks in advance!
Mark</p>
"
"0.0952081150392732","0.103566754353222","181333","<p>I am analyzing count data with a lot of zeros and found that although the data do not fit a poisson glm, they fit the zero-inflated poisson (ZIP) regression significantly better than the standard poisson glm. </p>

<p>This analysis is for a BACI study, in which I have data before, during, and after the treatment, in three zones: control, on-trail (treatment1) and off-trail (treatment2). </p>

<p>I am interested in the difference in change of detection rate of a species between the on-trail (treatment) site vs. control site, before and after the treatment. I have performed contrast on this data to determine this difference with a simple linear regression model (using lm), but I'm unsure how to find this difference using the zero-inflated poisson model. </p>

<p>The results of my ZIP model are here (ZP = Zone+Phase combined into one variable; the ""After"" phase is called ""Open"" in the dataset)</p>

<pre><code>Call:
zeroinfl(formula = Deer ~ ZP | ZP, data = zinb)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.6756 -0.5180 -0.4137 -0.1243 14.8998 

Count model coefficients (poisson with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.076730   0.031034  34.695  &lt; 2e-16 ***
ZPCDuring    0.080611   0.055391   1.455    0.146    
ZPCOpen     -0.696793   0.092432  -7.538 4.76e-14 ***
ZPFBefore    0.062467   0.042638   1.465    0.143    
ZPFDuring    0.112727   0.067928   1.659    0.097 .  
ZPFOpen     -0.765391   0.080475  -9.511  &lt; 2e-16 ***
ZPTBefore   -0.008428   0.045729  -0.184    0.854    
ZPTDuring   -0.063361   0.063193  -1.003    0.316    
ZPTOpen     -0.717266   0.078422  -9.146  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.40428    0.06617   6.110 9.97e-10 ***
ZPCDuring    0.85610    0.11076   7.729 1.08e-14 ***
ZPCOpen      0.79893    0.13448   5.941 2.84e-09 ***
ZPFBefore   -0.04894    0.09287  -0.527    0.598    
ZPFDuring    1.51339    0.13035  11.610  &lt; 2e-16 ***
ZPFOpen      0.20254    0.12932   1.566    0.117    
ZPTBefore   -0.08598    0.09830  -0.875    0.382    
ZPTDuring    0.98729    0.11720   8.424  &lt; 2e-16 ***
ZPTOpen      0.17416    0.12823   1.358    0.174    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 25 
Log-likelihood: -8572 on 18 Df
</code></pre>

<p>and the code i have been using to get the contrasts using the lm model is here (based on <a href=""http://people.stat.sfu.ca/~cschwarz/Stat-650/Notes/PDFbigbook-R/R-part013.pdf"" rel=""nofollow"">this tutorial</a>): </p>

<pre><code>result.lm &lt;- lm(Deer ~ Zone + PhaseBDO + Zone:PhaseBDO, data=counts)
options(contrasts=c(unordered=""contr.sum"", ordered=""contr.poly"")) 
result.lsmo.SP &lt;- lsmeans::lsmeans(result.lm, ~Zone:PhaseBDO)

contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1)))
confint(contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1))))
</code></pre>

<p>Can anyone suggest how I can calculate the contrast from my ZIP regression model to test for significant differences between the difference in detection rates between the two time periods (before/after) in the two zones (control/treatment)? </p>

<p>For example, I want to be able to say if there was a significantly larger increase (or decrease) in the detection rate of deer after the treatment was implemented, in the treatment zone compared with the control zone. </p>

<p>Thanks in advance for your suggestions.</p>
"
"0.0495478739876288","0.0497518595104995","181463","<p>I made a logistic regression in R statistics, but I don't know how to interpret it with 2 categorical variables (the examples I found on the internet and / or stackoverflow were just with one and I have difficulties to imagine it with two). </p>

<p>So imagine I want to see which factors infuence the fact of having a special desease (1: yes, 0: no) and I have:</p>

<pre><code>City: Manhattan, New York
hospital: St. Mary, Avante, Copperfield
bloodshugar: 1, 28, 7 ... , 66 (numeric)
timetoreact: 113, 423, 334, ... (numeric),
</code></pre>

<p>I give it all in a glm-model glm (desease dependent on: <code>City</code>, <code>hospital</code>, <code>City:hospital</code>, ...)</p>

<p>In the output I have the problem that it's all comprised with the factor level of the first letter of the alphabet, so i.e. ""Manhattan"" and ""Avante"" doesn't appear anymore. </p>

<p>There is just written: </p>

<pre><code>NewYork:Bloodshugar: Coeff.: 0.034 
</code></pre>

<p>and I don't know now what it is... Manhattan:Bloodshugar doesn't appear. Is it the difference of the incline from the probability on bloodshugar in Newyork in comparison to Manhattan? Where can I see if the probability to get the desease sinks or inclines with more bloodshugar in New York? When there's written bloodshugar: Coef.: 0.021, is it the bloodshugar ""mean"" of Manhattan and New York or is it just from Manhattan?  </p>

<p>What is the intercept now? Is it the probability to show the desease when cured in the Avantehospital and raised in Manhattan (because it's always the first letter)?</p>

<p>I hope I explained it well, I still can add some more explanations if you'd like to. </p>
"
"0.0286064783845312","0.0287242494810713","181790","<p>I would like to know how I can go about examining the results of a partial least squares regression. Specifically, I am interested to know what the coefficient is for each component, and what the linear combination of variables within each component looks like.</p>

<p>Lets consider this as an example:</p>

<pre><code>library(AppliedPredictiveModeling)
library(plsr)
data(solubility)

trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY

plsFit &lt;- plsr(Solubility ~ ., data = trainingData, ncomp=8)
</code></pre>

<p>Furthermore, if somebody could also help me understand what the different attributes within the mvr object are I would be greatly appreciative.</p>

<pre><code>&gt; attributes(plsFit)
$names
 [1] ""coefficients""    ""scores""          ""loadings""        ""loading.weights""
 [5] ""Yscores""         ""Yloadings""       ""projection""      ""Xmeans""         
 [9] ""Ymeans""          ""fitted.values""   ""residuals""       ""Xvar""           
[13] ""Xtotvar""         ""fit.time""        ""ncomp""           ""method""         
[17] ""call""            ""terms""           ""model""          
</code></pre>

<p>How do coefficients, scores, loadings, and weights all relate to each other?</p>

<p>Thanks!</p>
"
"0.0904616275314925","0.090834052439095","181980","<p>I would like to use a kernel matrix generated with a custom kernel function to fit a PLS-DA model (I am thinking of caret's PLS-DA at the moment), with only one binary response variable in the Y block. Before beginning, I am centering the kernel matrix on feature space with </p>

<p><a href=""http://i.stack.imgur.com/E7Hhd.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/E7Hhd.gif"" alt=""enter image description here""></a></p>

<p>A few remarks:</p>

<ol>
<li>I see that caret's <code>plsda</code> function relies on the <code>pls</code> package functions <code>mvr</code> and <code>plsr</code>. When fitting a PLS-DA model, the method used to fit the model defaults to <code>kernelpls</code>, which is the version described on algorithm 1 on <em>Dayal, B. S. and MacGregor, J. F. (1997) Improved PLS algorithms. Journal of Chemometrics, 11, 73-85.</em> In this paper, they propose to compute a kernel matrix directly as <a href=""http://i.stack.imgur.com/Ajfv3.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ajfv3.gif"" alt=""enter image description here""></a> as part of the algorithm, and they rely directly on X as well during other steps. Therefore, it seems to me that using this method would mean to calculate a kernel matrix again over my kernel matrix.</li>
<li>I've seen three different methods in the literature that involve kernels and PLS. The first one is Dayal and MacGregor's kernel algorithm, the second one is <a href=""http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf"" rel=""nofollow"">K-PLS</a> (<em>Rosipal, Roman, and Leonard J. Trejo.""Kernel partial least squares regression in reproducing kernel hilbert space."" The Journal of Machine Learning Research 2 (2002): 97-123.</em>) and the third one is <a href=""http://homepages.rpi.edu/~bennek/papers/KB-ME-PLS.pdf"" rel=""nofollow"">DK-PLS</a> (direct kernel PLS). My understanding is that K-PLS is just a modification of the NIPALS algorithm (oscorespls fitting method in the <code>pls</code> package) to use a kernel matrix, and therefore I suspect that this might be the one I should be using. DK-PLS seems to use a kernel matrix as input as well.</li>
</ol>

<p>In short, I guess my question can be summarized as: Which method should I use to fit a PLS-DA model for a binary response, with a custom kernel matrix as input data? Any insights would be appreciated!</p>
"
"0.0572129567690623","0.043086374221607","182071","<p>I'm analyzing presence/absence data using Rmark occupancy analysis (i.e. Rmark is the package which runs Program MARK in R). This is for a BACI (before-after-control-impact) design and I want to answer the question of ""is the amount of change between before and after phases significantly different for the impact site as compared with the control site?"".  I'm able to do this using regression for the count data, but am not sure how to calculate lsmeans to use in the BACI contrast using the occupancy results. </p>

<p>Here is an example: </p>

<p>Analysis of the count data using zero-inflated poisson regression: 
(ZP = zone/phase combination: zones are control/impact, phases are before/after). Note: extra script from this answer was needed to run lsmeans on the zerofinl object (<a href=""http://stats.stackexchange.com/questions/181333/zero-inflated-poisson-regression-how-can-i-calculate-contrasts-for-baci-before/181502?noredirect=1#comment345516_181502"">see this answer for more details</a>).</p>

<pre><code>summary(m2 &lt;- zeroinfl(Opossum ~ ZP|ZP, data = bact))
result.lsmo.SP2 &lt;- lsmeans::lsmeans(m2, ~ZP)
contrast(result.lsmo.SP2, list(bact=c(1, -1, -1, 1)))
confint(contrast(result.lsmo.SP2, list(bact=c(1, -1, -1, 1))))
</code></pre>

<p>Analysis of the presence/absence data using occupancy analysis in Rmark: </p>

<pre><code>BEAR.models=mark(BEAR, model=""Occupancy"", group=c(""Zone"", ""Phase""), model.parameters=list(p=list(formula=~WeekDay+DetectionDist+Trail),Psi=list(formula=~Zone+Phase)), invisible=FALSE)
</code></pre>

<p>Any ideas of how I can calculate lsmeans from the occupancy results (which includes occupancy estimate (psi), standard error, and upper/lower 95% confidence interval limits)? Thanks!</p>
"
"0.0404556697031367","0.0406222231851194","182079","<p>I am running a meta-analysis using <strong>metafor</strong> R package. I am comparing studies on a continuous variable, that can be synthetized by the mean. Such studies can be grouped in three blocks. I used a meta regression, e.g. </p>

<pre><code>dat &lt;- escalc(measure=""MN"", mi=mean, sdi=sd, ni=num, data=dbtemp)
res &lt;- rma(yi, vi, mods =~ factor(group), data=dat)
</code></pre>

<p>I know that I can know whether group is significant or not. But how can I assess whether each level within group is significant from the other. I.e., how I can perform multiple comparisons (post hoc analysis) within a meta - analysis context?</p>
"
"0.0572129567690623","0.0574484989621426","182595","<p>I'm trying to use sparse linear model for my data,input x(29*50),output y(29*1). In R, the package of <strong><em>glmnet</em></strong> can be used. </p>

<p>Firstly, cv.glmnet() choose lambda and coefficients(at min error), here with leave-one-out cv method,and then plot it. </p>

<pre><code>cv.fit = cv.glmnet(x,y,family=""gaussian"",nfolds=29)

plot(cv.fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PEEeb.png"" rel=""nofollow"">the plot of mse aganist log(lambda) in cv model</a></p>

<p>Next, print the coefficients</p>

<pre><code>coef(cv.fit,s=""lambda.min"")
</code></pre>

<blockquote>
  <p>51 x 1 sparse Matrix of class ""dgCMatrix""               </p>

<pre><code>              1
</code></pre>
  
  <p>(Intercept)   267.7241</p>
  
  <p>cluster_0  .<br>
  cluster_1     .<br>
  cluster_2     .<br>
  cluster_3     .<br>
  cluster_4     .<br>
  ...</p>
  
  <p>cluster_47    .<br>
  cluster_48    .<br>
  cluster_49    .  </p>
</blockquote>

<p>Finally, to measure the model's ability for prediction, accuracy is calculated(defined as 1 minus average absolute error divided by numeric range of y)</p>

<pre><code>py &lt;- predict(cv.fit,newx=x,s=""lambda.min"")
py
</code></pre>

<blockquote>
  <p>V1     267.7241</p>
  
  <p>V2     267.7241</p>
  
  <p>...</p>
  
  <p>v29    267.7241  </p>
</blockquote>

<pre><code>ave_abs_error &lt;- mean(abs(py-y))
n_range &lt;- max(y)-min(y)
acc &lt;- 1-ave_abs_error/n_range
acc
</code></pre>

<blockquote>
  <blockquote>
    <p>0.918365</p>
  </blockquote>
</blockquote>

<p>Although the acc(0.918365) is very high, there is a serious problem. As seen from the plot above, the lambda.min is very large(73.03439),and all coefficients  are zero(only with intercept value 267.7241), all predicted py are the same as intercept.
That's really weird! </p>

<p>I searched lots of threads in forum, here<a href=""http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome</a> explains that there is no local min for too few observations and all coefficients were shrunk to zero with the shrinkage penalties.</p>

<p>Does anybody has other interpretations?</p>

<p>Thanks in advance!</p>
"
"0.0572129567690623","0.0574484989621426","182689","<p>My name is Ashley. I'm working on the analyses for my dissertation which involves a meta-analysis of 4 predictors, 1 mediator, and one outcome. So far, I've calculated the meta-analytic correlation matrix between predictors and outcome(s) and harmonic mean of N to run analyses on the model level. 
Out of two major studies published in my area of the social sciences (DeChurch &amp; Mesmer-Magnus, 2010; Joseph et al., 2015), this is all of the information provided for running meta-regression with more than one predictor. No programs are specified and no other matrices are indicated. </p>

<p>I'm having trouble identifying how to run the multivariate metaregression analysis. From what I've found so far, mvmeta package in R is the closest that I've come to identifying a program/package that will produce the estimates that I need. However, I would have to calculate corrected rho for each study/predictor-outcome relationship individually. Also, there is no place to indicate harmonic mean of N. And because of this, I'm skeptical in using this package. </p>

<p>Is anyone aware of another R package, SPSS macro or other statistical software program/package that can handle this type of analysis? Or is mvmeta the best bet?</p>
"
"0.114624397492221","0.121866669555358","182905","<p>I am trying to create a regression comparing resampled data from 2 sampling periods using $n=6$ samples. 
I have tried a few different models (e.g., linear, log-transform, and exponential), and have received mixed results. </p>

<p>For example, the linear models look best graphically, but because I have so few data points, each point has a strong effect on the trend and therefore points with greater distance from the mean are resulting in the model having a negative adjusted $R^2$ value. </p>

<p>An exponential model has a much improved, and positive, adjusted $R^2$ value, but graphically doesn't make as much sense as the linear model. 
Essentially, the exponential model is reacting to the dispersion of points around the linear regression line and trying to make up for the residual error of more outlier-ish points. 
However, these points would almost certainly be within reasonable distance from the mean if more points were added.</p>

<p>So I have 2 questions:</p>

<ol>
<li>Do I go with what looks more sensible graphically, or just blindly believe the adjusted $R^2$?</li>
<li>Obviously, more data points would be beneficial, but what are my options if I can't supplement with more data?</li>
</ol>

<p><strong>Examples:</strong></p>

<p>3 variables demonstrating this issue. Solid color lines are model fits and dashed lines are 95% CI. Black = linear, red = log-trasnformed, green = exponential. x-axis = 1977, y-axis = 2015. Created in R using <code>lm()</code>.</p>

<p><a href=""http://i.stack.imgur.com/Evpzp.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Evpzp.jpg"" alt=""Example1""></a> 
<a href=""http://i.stack.imgur.com/7yJrx.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7yJrx.jpg"" alt=""Example2""></a>
<a href=""http://i.stack.imgur.com/fssgU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fssgU.jpg"" alt=""Example3""></a></p>

<p>Update: </p>

<p>Background info:
I have 21 soil variables I resampled in 1977 &amp; 2015. Each of these variables had 6 sample plots shared b/w each sampling period. However, I have a number of 1977 samples &amp; 2015 samples that were only sampled once (in 1 year &amp; not the other). The soil processing was different b/w years, so the 2 years aren't directly comparable. I'm trying to 'correct' the non-resampled 1977 samples to match 2015 data (not sampled in 1977) by regressing my 6 resampled samples for each variable. I'm assuming soil has not changed in 40 years, so I'm not trying to compare soil change, but rather trying to lump all extant samples (resampled and not) for common analyses. </p>

<p><strong>My example data:</strong></p>

<pre><code>dput(soil.dat)
structure(list(plot.2015 = c(5L, 10L, 24L, 25L, 44L, 51L), Silt.2015 = c(28.88, 21.84, 22.89, 25.34, 16.96, 22.36), Sand.2015 = c(63.92, 73.67, 60.38, 69.03, 81.6, 70.18), BS.2015 = c(70, 60.4, 43.2, 41, 84.4, 54), Silt.1977 = c(44L, 38L, 40L, 42L, 50L, 36L), Sand.1977 = c(37L, 49L, 37L, 36L, 39L, 42L), BS.1977 = c(36.6, 26.48, 44.08, 36.6, 53.32, 44.08)), .Names = c(""plot.2015"", ""Silt.2015"", ""Sand.2015"", ""BS.2015"", ""Silt.1977"", ""Sand.1977"", ""BS.1977""), class = ""data.frame"", row.names = c(1L, 2L, 15L, 16L, 18L, 19L))
</code></pre>
"
"0.0993902382172794","0.0997994216610746","182947","<p>In the context of a (multilevel) meta-analysis, I want to caluclate grand effect size estimates for subsets of data corresponding to the levels of a categorical variable.
Using the example below, I'd like to calculate the grand effect size for Gifted students and NotGifted students. I thought of approaching this two ways: run two meta-analyses (one with Gifted students, the other with NotGifted students), or include Gifted status as a moderator in an analysis inclusive of all students (i.e. a meta-regression (though I knwo this term is frowned upon)).
To my confusion, the two approaches produce different values for effect sizes for Gifted and NotGifted students. I have two questions. </p>

<p>Why are the results of the two methods different? Which approach is better, i.e. which is a better estimator of the true effect size for Gifted or NotGifted students?</p>

<p>Any help would be greatly appreciated. Thank you</p>

<pre><code>library(metafor)

set.seed(123)
df &lt;- data.frame(
School = rep(1:5, each = 4),
Class = rep(1:2, times = 10),
Pupil = rep(letters[1:4], times = 5),
Gifted = sample(c('Yes', 'No'), size = 20, replace = TRUE),
yi = rnorm(20, 70, 15),
V = rnorm(20, mean = 5, 2))

Gifted &lt;- rma.mv(yi, V, random = list(~ 1 | Class, ~ 1 | School), 
             method = 'REML', data = subset(df, Gifted == 'Yes'))
NotGifted &lt;- rma.mv(yi, V, random = list(~ 1 | Class, ~ 1 | School), 
                method = 'REML', data = subset(df, Gifted == 'No'))
Regression &lt;- rma.mv(yi ~ Gifted, V, random = list(~ 1 | Class, ~ 1 | School), 
                 method = 'REML', data = df)

matrix(round(c(NotGifted$b, Gifted$b, Regression$b[1], Regression$b[1] + Regression$b[2]), 3), 
   2, 2, TRUE, list(' ' = c('Subset', 'Regression'), ' ' = c('NotGifted', 'Gifted')))
</code></pre>
"
"0.0286064783845312","0.0287242494810713","183052","<p>I am building a regression tree in R, using the 'tree' package. Up until now, I only have worked with classification trees, so, I have relied on the misclassification error rate to judge how good my classification tree is at fitting the data. However, for my regression tree, I am only given ""residual mean deviance"". So, for example, for one of my trees I get: </p>

<p>Residual mean deviance:  2.687 = 1247 / 464 </p>

<p>Can someone please explain what this means?</p>
"
"0.0429097175767967","0.043086374221607","183206","<p>Simply put, I'd like to know how the plm package in R calculates the residuals of a random-effect regression.</p>

<p>I ask this because i'm getting some ""weird"" outputs. Let-me reproduce them here using the Grunfeld data for four firms, like Gujarati in his Basic Econometrics do:</p>

<pre><code>require(plm)
require(foreign)

Grunfeld&lt;-read.dta(""Data.dta"")
Grunfeld&lt;-pdata.frame(Grunfeld,index = c(""id"",""t""))

grun.re &lt;- plm(Y~X2+X3,data=Grunfeld,model=""random"",index=""id"")

#Means by id
X2M&lt;-tapply(Grunfeld$X2,Grunfeld$id,FUN = mean)
X3M&lt;-tapply(Grunfeld$X3,Grunfeld$id,FUN = mean)
YM&lt;-tapply(Grunfeld$Y,Grunfeld$id,FUN = mean)

#Random Effect: Fit the model and the calculate residuals ""by hand""
fit.re&lt;-grun.re$coefficients[1]+grun.re$coefficients[2]*Grunfeld$X2+grun.re$coefficients[3]*Grunfeld$X3
    calcResid.re&lt;-(Grunfeld$Y-fit.re)

#Random Effect:
head(cbind(grun.re$residuals,Grunfeld[,11:13],calcResid.re))

  grun.re$residuals   alphaRE       eRE        uRE calcResid.re
1         99.395803 -169.9282 116.23154  -53.69666    -53.69666
2         18.023715 -169.9282  34.85946 -135.06874   -135.06874
3        -39.256625 -169.9282 -22.42089 -192.34909   -192.34908
4         -2.857048 -169.9282  13.97869 -155.94951   -155.94951
5        -28.334107 -169.9282 -11.49837 -181.42656   -181.42656
6          6.475226 -169.9282  23.31096 -146.61723   -146.61723
</code></pre>

<p>In this table, uRE is the overall residual of the regression provided by Stata (which is identical to Gretl's) and calcResid.re is the manually calculated residuals from the fitted model. So, Stata, Gretl and I did the same. But what plm package do?</p>

<p>We can se that calcResid.re and uRE are equals. But the residuals provided by the plm estimation (grun.re$residuals) completely differs.</p>

<p>Here is a link to the dataset and results: <a href=""https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0</a></p>
"
"0.0809113394062735","0.0812444463702388","183337","<p>I am trying to test the predictive accuracy of regression using training sets of varying sizes.</p>

<pre><code>Y &lt;- rnorm(100)
X &lt;- replicate(5, Y+rnorm(100) )   
data &lt;- as.data.frame(cbind(Y,X))
</code></pre>

<p>Let's say the training set is 2% of the data:</p>

<pre><code>train &lt;- nrow(data) * 0.02
test &lt;- nrow(data) - train 
</code></pre>

<p>I repeat the process for 1000 times:</p>

<pre><code>MSE &lt;- vector()
for( i in 1:1000){

train.elements &lt;- sample(1:nrow(data),train)
train.set &lt;- data[train.elements,]
test.set &lt;- data[setdiff(1:nrow(data), train.elements),]

# then I fit a regression model:

    model &lt;- lm(train.set[,1]~ train.set[,2]+train.set[,3]+train.set[,4]+train.set[,5])

#I now use this model to predict the values in the test set:
predictions &lt;- predict.lm(model,data=test.set)

MSE[i] &lt;- mean((test.set[,1] - predictions)^2)

}
</code></pre>

<p>My problem is that due to the small sample size the MSE sometimes is extremely huge.</p>

<p>Is this normal? I am unable to plot a curve of the MSE as a function of training set size because the MSE for small sample sizes are so large.</p>
"
"0.0495478739876288","0.033167906340333","183760","<p>I was reading this blog post about using R to perform Fama macbeth regresion: <a href=""https://landroni.wordpress.com/2012/06/02/fama-macbeth-and-cluster-robust-by-firm-and-time-standard-errors-in-r/"" rel=""nofollow"">https://landroni.wordpress.com/2012/06/02/fama-macbeth-and-cluster-robust-by-firm-and-time-standard-errors-in-r/</a></p>

<p>I am having difficulty deriving the equivalence of the two estimators. what's the definition of mean group regression? Greenes (15th ed.) in chapter 11 defined an estimator called group means estimator p358. That one is simply the between estimator in plm, I believe. I ran these two estimators, with plm() between model and pmg(), the estimates are different. So this must not be what the author means by ""group means estimator"", then what is it?</p>

<p>What is this group mean estimator and why is it equivalent to FM estimator?</p>
"
"0.0286064783845312","0.0287242494810713","183845","<p>I'm going crazy, because I can't find a simple description how the coefficients are calculated in R statistics in the multivariable logistic regression (and I'm not a mathematician). 
Are they standardised? So i.e. when I have x ~ y1 + y2 and the coefficient for y1 = 0.2, is this the coefficient in the model when the parameter y2 is 0, the mean of y2 or somehow all the parameters of y2? </p>

<p>Sorry, I'm stuck on this simple question... </p>

<p>p.s.: I also have an interaction y1:y2 if this changes anything...</p>
"
"0.0429097175767967","0.043086374221607","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"0.0948769553749019","0.0952675579132743","183908","<p>I have a binary logistic regression with just one binary fixed factor predictor. The reason I don't do it as a Chi square or Fisher's exact test is that I also have a number of random factors (there are multiple data points per individual and individuals are in groups, although I don't care about coefficients or significances for those random variables). I do this with R glmer.</p>

<p>I would like to be able to express the coefficient and associated confidence interval for the predictor as a risk ratio rather than an odds ratio. This is because (maybe not for you but for my audience) risk ratio is much easier to understand. Risk ratio here is the relative increase in chance of the outcome being 1 rather than 0 if the predictor is 1 rather than 0.</p>

<p>The odds ratio is trivial to get from the coefficient and associated CI using exp(). To convert an odds ratio to a risk ratio, you can use ""RR = OR / (1 â€“ p + (p x OR)), where p is the risk in the control group"" (source: <a href=""http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/"" rel=""nofollow"">http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/</a>). But, you need the risk in the control group, which in my case means the chance that the outcome is 1 if the predictor is 0. I believe the intercept coefficient from the model is in fact the odds for this chance, so I can use prob=odds/(odds+1) to get that. I'm pretty much so-far-so-good on this as far as the central estimate for the risk ratio goes. But what worries me is the associated confidence interval, because the intercept coefficient also has its own associated CI. Should I use the central estimate of the intercept, or to be conservative should I use whatever limits of the intercept CI make my relative risk CI widest? Or am I barking up the wrong tree entirely?</p>
"
"0.0948769553749019","0.0952675579132743","183973","<p>I'm reaching out to you because I am unsure whether my implementation of a group of random forests in R (using library randomForest) is valid or whether I have an error in reasoning.</p>

<p>I have a sales dataset with a binary outcome (1: Sale, 0: No Sale) and a set of possibly significant predictors x1-x14. My data is highly imbalanced, with ~124k '0' observations (No Sale) and ~18k '1' observations (Sale). I balance it by randomly cutting down the 124k observations to 18k, as suggested in <a href=""http://bit.ly/1I7F0AC"" rel=""nofollow"">http://bit.ly/1I7F0AC</a>.</p>

<p>Cross-validation is not necessary due to the nature of random forests, however: In order to find a random forest with a good F-score, I loop through a set of possible predictors and a set of tree-numbers for the forest:</p>

<pre><code>possiblyUsefulPredictors=
  c(""x1"",...""x14"") # Shortened to pseudo-code

treerange=c(1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50,60,70,80,90,100,
            200,300,400,500,750,1000)
# Create a multitude of models by looping 
# through different settings for parameters
for (i in 2:length(possiblyUsefulPredictors)){
for (j in treerange){

### Choose model here by setting data, outcome and predictors:
x=possiblyUsefulPredictors[1:i] # Set predictors
ntree=j # Set number of trees
# Tune mtry
bestMtry=tuneRF(x=x, y=y, ntreeTry=1, 
                stepFactor=1, improve=0.01, trace=FALSE, 
                plot=FALSE, doBest=FALSE)    
# Run random forest
rf=randomForest(y=y,x=x,data=df,mtry=bestMtry,ntree=ntree,
type=""classification"",importance=T)
}
}
</code></pre>

<p>I then store model diagnostics precision, recall, and F-score in a table and choose the model that created the highest F-score (13 predictors, 90 trees, mtry=1, which leads to an F-score of 78%).</p>

<p>Specific questions:</p>

<ol>
<li><p>Obviously, the way I subset and loop through the predictors is highly arbitrary. Could a more sophisticated approach (e.g. looping through all possible subsets) get me anywhere, or does a random forest inherently choose significant predictors, so that I wouldn't have to try to find a meaningful subset myself (like I do when using step-wise in linear regression)?</p></li>
<li><p>By building a set of 416 random forests, do I simply overfit the dataset? I am skeptical that the predictors are as good as my best model suggests.</p></li>
</ol>

<p>Thank you and kind regards,
Jan</p>
"
"NaN","NaN","184327","<p>The covTest package in R gives significance values for a LASSO regression. How should the results be interpreted? I get negative predictor numbers and NAs for the p-values. What do these mean? More specifically, it might be assumed that the predictor number is an index for the external regressors, but how can this be negative?</p>

<p>Here is an example:</p>

<pre><code> significance_pars &lt;- covTest(lars_output,x,y, sigma.est = ""full"")
</code></pre>

<p>When calling significance_pars$results, the output is:</p>

<pre><code> Predictor_Number Drop_in_covariance P-value
          118          8830.0759  0.0000
          105          1557.1987  0.0000
          104           755.5005  0.0000
          119           833.6093  0.0000
           46           183.8750  0.0000
           45             3.3674  0.0345
          103            56.8601  0.0000
           60           214.5187  0.0000
           44             0.4365  0.6463
          113           248.4377  0.0000
          120           220.6486  0.0000
          124             1.2863  0.2763
         -118                 NA      NA
</code></pre>
"
"0.051172824211752","0.0642293744423385","184341","<p>I am using the Deming function provided by Terry T. on <a href=""http://www.mail-archive.com/r-help@r-project.org/msg85070.html"">this archived r-help thread</a>.  I am comparing two methods, so I have data that look like this:</p>

<pre><code>y  x     stdy   stdx
1  1.2   0.23   0.67
2  1.8   0.05   0.89
4  7.5   1.13   0.44
... ...  ...   ...
</code></pre>

<p>I have done my Deming regression (also called ""total least squares regression"") and I get a slope and intercept. I would like to get a correlation coefficient so I've start calculating the $R^2$. I have manually entered the formula: </p>

<pre><code>R2 &lt;- function(coef,i,x,y,sdty){
    predy    &lt;- (coef*x)+i
    stdyl    &lt;- sum((y-predy)^2)   ### The calculated std like if it was a lm (SSres)
    Reelstdy &lt;- sum(stdy)          ### the real stdy from the data  (SSres real)
    disty    &lt;- sum((y-mean(y))^2) ### SS tot
    R2       &lt;- 1-(stdyl/disty)    ### R2 formula
    R2avecstdyconnu &lt;- 1-(Reelstdy/disty) ### R2 with the known stdy
    return(data.frame(R2, R2avecstdyconnu, stdy, Reelstdy))
}
</code></pre>

<p>This formula works and gives me output.</p>

<ul>
<li>Which of the two $R^2$s makes more sense? (I personally think of both of them as kind of biased.)  </li>
<li>Is there a way to get a correlation coefficient from a total least squared regression?</li>
</ul>

<p>OUTPUT FROM THE DEMING REGRESSION:</p>

<pre><code>Call:
deming(x = Data$DS, y = Data$DM, xstd = Data$SES, ystd = Data$SEM,     dfbeta = T)

               Coef  se(coef)         z            p
Intercept 0.3874572 0.2249302 3.1004680 2.806415e-10
Slope     1.2546922 0.1140142 0.8450883 4.549709e-02

   Scale= 0.7906686 
&gt; 
</code></pre>
"
"0.0286064783845312","0.0287242494810713","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"0.0707974219804893","0.0812444463702388","184595","<p>I want to compare estimate with standard error in function of a continuous variable and a categorial variable . Here an example of what my data look like.</p>

<pre><code>y   stdy   ConVar  CatVar
1.3    0.1    1    Bob
2.4   0.4     1    Bob
1.5    0.3    2    Bob
3.6    0.2    3    Henri
...
</code></pre>

<p>I would like to perform a regression of my y estimate in function of the ConVar in first place. Then I would like to compare the estimate in function of the categorial variable. </p>

<p>I want to rectify my slope and average comparaison with the known standard error (stdy).</p>

<p>Is it possible .</p>

<p>I know orthogonal regression to compare two variables with known error but I don't known of a regression in which I can input standard error only on the y value.</p>

<p>Is that would do it if I do a mean of the standard error. mean of the standard error is sqrt(sum(std^2)/numberofobs^2)</p>

<pre><code>library(MethComp)
Deming(ConVar,y,stdy, boot=FALSE, keep.boot=FALSE, alpha=0.05)
</code></pre>

<p>Thanks</p>
"
"0.06396603026469","0.0642293744423385","184795","<p>I have a number (48) bivariate relationships (N = 10 for each) where I want to fit a linear model and estimate the confidence interval (CI) using bootstrapping. </p>

<p>What I want to present, is the slope and CI for this regression. However, instead of picking one CI, I'd rather present the distribution of the slope estimates so the reader can judge for himself. What I thought about doing, is to present the histogram of the bootstrapped slope estimates along with the information about how many % of the estimates where > 0.</p>

<p>Is that a valid and/or good way to present the data? And is it valid to say that if 97.7 % of the slope estimates are > 0, the slope is significant with alpha = 0.023?</p>

<p>here is an example of what I mean</p>

<p><a href=""http://i.stack.imgur.com/33AxH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/33AxH.png"" alt=""scatterplots""></a></p>

<p><a href=""http://i.stack.imgur.com/FMaFT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FMaFT.png"" alt=""bootstrapped slope estimates, 10000 draws""></a></p>

<p>another way to formulate the question would be: prior to going this way, I calculated bootstrapped CI with the <code>boot.ci</code> function in in the <code>boot</code> package in <code>R</code>. However, the CI seem to be wider that what is suggested by the histograms. How exactly are bootstrapped CI calculated and is it wrong to assume that it should span 95% of the bootstrapped slope estimates? </p>
"
"0.121535457502911","0.122035811440443","185116","<p>Goal is to evaluate chess players using a novel analysis system I'm been working on -- not all wins are created equal, finding the only move in razor sharp positions is better than finding the best move when the ten-best-alternates are negligibly worse, etc.</p>

<p>Current dataset I'm working with towards proof of concept has 30 players. The design matrix has players as the columns, but each player gets two columns: one for when they're playing as white, one for when they're playing as black. Each row of the design matrix represents half of a match, and 1/0/-1 dummies are used for white/not present/black.</p>

<p>Example: if Player 4 and Player 9 played a match, the design matrix will have two rows for this match. One row will have p4w assigned a ""1"" and p9b assigned a ""-1"". The other row will have p4b assigned a ""-1"" and p9w assigned a ""1"". All other player columns are 0.</p>

<p>The result vector is the Engine's score for the player playing as white in that half of the match.</p>

<p>There's also two other columns, Sw and Sb, to attempt to quantify the value of being white first in any given match and if a penalty exists for the player who started as black once they switch to white -- since white always moves first, and white wins more games than black, black is more likely to be disadvantaged after the first game.</p>

<p>Using matrix math rather than an R function.</p>

<pre><code>csv &lt;- read.csv(""~/chess.csv"", header=TRUE)
engine &lt;- as.numeric(csv$Engine)

# ready design matrix/remove dropped variables
csv$Engine &lt;- NULL
csv$Sb &lt;- NULL
csv$P30w &lt;- NULL
csv$P30b &lt;- NULL

# readies X and Y
X &lt;- data.matrix(csv)
Y &lt;- engine

# remove copies
remove(csv)
remove(engine)

# Add one column of ""1"" to X
one.col &lt;- matrix(1, nrow(X), 1)
X &lt;- cbind(X, one.col)

# transposing X
X.t = t(X)

# X'X, X'Y
X.t.X &lt;- X.t %*% X
X.t.Y &lt;- X.t %*% Y

# MATHS
betahat = solve(X.t.X) %*% X.t.Y
</code></pre>

<p>Here's the CSV: <a href=""http://www.filedropper.com/chess_1"" rel=""nofollow"">http://www.filedropper.com/chess_1</a></p>

<p>Right from the top, I have to drop Sb -- it's redundant. I then am forced to drop a player to defeat the ""system is computationally singular"" error. In this case, I'm dropping the same player lm() would: the last one.</p>

<p>I have no philosophical objections to dropping variables but for the purposes of this, for evaluating players against each other, the incompleteness is troublesome.</p>

<p>Using Ridge Regression ""works"" to prevent any variable from being dropped, but this is unsatisfying -- are the results really then meaning what they should? X + 0 doesn't help matters for this problem either.</p>

<p>Are there any other tools I'm missing? Is ridge regression the right path to take for this problem but, rather than penalize towards zero, penalize towards priors?</p>
"
"0.0572129567690623","0.0574484989621426","185141","<p>So I am using panel data that includes both categorical and numerical variables. I am trying to get a regression equation in R, the issue is that I am not getting any information on my dummy variables (CP, IP, UP). I am going to paste my code and my output... 
**> palldata &lt;- plm.data(alldata, indexes=3)</p>

<blockquote>
  <p>fixed &lt;- plm(Y~NX+I+C+G+CP+IP+UP, palldata, model=""within"")</p>
  
  <p>summary(fixed)</p>
</blockquote>

<p>Oneway (individual) effect Within Model</p>

<p>Call:
plm(formula = Y ~ NX + I + C + G + CP + IP + UP, data = palldata, 
    model = ""within"")</p>

<p>Balanced Panel: n=3, T=16, N=48</p>

<p>Residuals :
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-1.22e+11 -1.36e+10 -2.52e+09  0.00e+00  1.51e+10  1.84e+11</p>

<p>Coefficients :</p>

<p>Estimate Std. Error t-value  Pr(>|t|)  </p>

<p>NX  1.201395   0.092049 13.0517 3.443e-16 ***</p>

<p>I     1.119823   0.020110 55.6842 &lt; 2.2e-16 ***</p>

<p>C    1.228684   0.033998 36.1403 &lt; 2.2e-16 ***</p>

<p>G   -0.055998   0.053056 -1.0554    0.2974    </p>

<hr>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Total Sum of Squares:    1.8266e+26
Residual Sum of Squares: 8.868e+22
R-Squared      :  0.99951 
      Adj. R-Squared :  0.85375 
F-statistic: 21102.8 on 4 and 41 DF, p-value: &lt; 2.22e-16**</p>

<p>I already coded my dummy variables as 0 or 1. There is variance within them so that isn't the issue. What can I do to get information on my dummy variables? 
Thank you so much for your help.</p>
"
"0.0404556697031367","0.0406222231851194","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.103142124625879","0.103566754353222","185247","<p>I am trying to use random forest to show the results from Cox regression analysis.</p>

<p>I have 757 samples with complete follow-up data (456 cases and 301 control). The case is defined as <code>scores</code> &lt;12, and control was <code>scores</code> >=12. The <code>status</code> of case is coded as 1, and control is 0.The follow-up time is showed in another variable called <code>time</code>. The other variables were <code>age</code>, glucose <code>glu</code>, interested molecular levels <code>marker</code> and the other confounders.</p>

<p>Firstly, I used the survival analysis to find out the most significant model as below:</p>

<pre><code> coxph(Surv(time, status) ~ age + glu + marker + 
       hdl + smoking + drinking, data=raw)
</code></pre>

<p>It turns out the <code>glu</code> and <code>marker</code> has significant interaction: </p>

<p><a href=""http://i.stack.imgur.com/b72U1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/b72U1.png"" alt=""enter image description here""></a></p>

<p>the <code>marker</code> is significantly higher in the patients than the control if the patients also has high <code>glu</code>.</p>

<p>The I use the random forest in two models. One is for <code>scores</code>, the other is for <code>status</code>:</p>

<pre><code> model1&lt;-randomForest(formula = as.factor(scores) ~ age + glu + 
             marker + hdl + smoking + drinking + time,
             data = raw, importance = TRUE, proximity = TRUE, 
             na.action = na.omit) 

 model2&lt;-model1&lt;-randomForest(formula = as.factor(status) ~ age + glu
             + marker + hdl + smoking + drinking + time, 
             data = raw, importance = TRUE, proximity = TRUE, 
             na.action = na.omit) 
</code></pre>

<p>The importance of predictors in the two models were like this:</p>

<p><a href=""http://i.stack.imgur.com/X4agu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X4agu.png"" alt=""enter image description here""></a></p>

<pre><code>model1: OOB estimate of  error rate: 82.3%
model2: OOB estimate of  error rate: 33.55%
</code></pre>

<p>I prefer to use model1 because the MeanDecreaseAccuracy is better to be showed in the paper, but its accuracy is only 17.7%. Do you think this accuracy is acceptable? Or the model2 is better?</p>

<p>What is the other suggestion you would give in this case?</p>

<p>I posted this question in the wrong place stackoverflow: <a href=""http://stackoverflow.com/questions/34111732/how-to-interperate-the-accuracy-and-importance-in-randomforest-test-from-cox-reg"">http://stackoverflow.com/questions/34111732/how-to-interperate-the-accuracy-and-importance-in-randomforest-test-from-cox-reg</a>
so I migrated it to cross-validated.</p>
"
"0.0904616275314925","0.0817506471951855","185449","<p>I've implemented a comparison between the performance of 80%-forecast intervals is in the forecast package - see 1st part of the code below providing a number of hits
This number states, how many times the forcast interval was right for the left-out data entries. Btw regarding variable names: the German ""preis"" means ""price"" and ""absatz"" means ""sales"", i.e.
""preise"" means ""prices"" and ""absaetze"" is the plural for ""sales"".</p>

<p>So, I compared the formula-based prediction interval to what I think bootstrapping is - see 2nd part of the code. But the number of hits in the 2nd case by no means resembles the 80% of the first case.
The following actions did not help to reproduce the 80% : using less data in the given data frame, using median formulas for bootstrapping instead of the upper/lower computation in the loop,
more samples resampling in the resampling.</p>

<p>I cannot imagine the bootstrapping approach performing so bad - what did I do wrong?  </p>

<pre><code>#given

# data frame

preis&lt;-c(1:100)
absatz&lt;-(-2*preis)+1000+rnorm(100)


jeansData&lt;-data.frame(absaetze=absatz,preise=preis)

#### implementation ###


#leave-one-out cross-validation for formula, i.e. with the borders         given     above  
###### (1ST PART) ########

numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame            (absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)
fit&lt;-lm((absaetze~preise), data=jeansData)

#check, if in interval and count as hit, if value is in interval

if(absatzCandidateToBeChecked &lt;= (forecast(fit,     newdata=preisCandidateToBeChecked)$upper[1]) &amp; (absatzCandidateToBeChecked &gt;= (forecast(fit, newdata=preisCandidateToBeChecked)$lower[1])) )
{numberOfHits&lt;-numberOfHits+1}

}

#execute code until here and inspect numberOfHits; the hit rate pretty much resembles the 80% assumed

#then execute the rest

#leave-one-ot cross-validation for bootstrapping (not using the bootstrap function)  ###### (2ND PART) ########


numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame(absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)

#ten or hundred or thousand regressions by bootstrapping

allPredictions&lt;-c()

for(j in (1:10)){

fit&lt;-lm((absaetze~preise), data=jeansData[sample(nrow(jeansData),10,replace=TRUE),])

allPredictions&lt;-c(allPredictions,forecast(fit,     newdata=preisCandidateToBeChecked)$mean)

}

#build and name bootstrapped forecast interval from regressions

upper&lt;-sort(allPredictions)[9]
lower&lt;-sort(allPredictions)[2]

if((absatzCandidateToBeChecked &lt;= upper) &amp; (absatzCandidateToBeChecked &gt;= lower) )
{numberOfHits&lt;-numberOfHits+1}

} #inspect numberOfHitsAgain - it's around 40%. What is foul here?!
</code></pre>
"
"0.0495478739876288","0.0497518595104995","186078","<p>I have some doubts in how to interpret the interaction term in an ANCOVA using R since in other statistical programs this term is not provided. 
If I am right, one assumption of ANCOVA is the Homogeneity of regression slopes (that is, they must be parallel). Not following this assumption means you cannot use ANCOVA. </p>

<p>My question is, can the interaction term be interpreted as a test for the homogeneity of regression slopes? I meant, if is significance is because the slopes are not parallel and the model cannot be ran. 
If not, how can I test for it? </p>

<p>Here is an example 
<a href=""http://i.stack.imgur.com/pfirq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pfirq.jpg"" alt=""enter image description here""></a></p>

<p>Thanks, </p>
"
"0.06396603026469","0.0642293744423385","186240","<p>Hi I am trying a mediation analysis (using library(""mediation"") in R)</p>

<p>My model has 3 predictors and one mediator (n=455), but I am only interested in predictor 1. There is some collinerarity between predictor 1 and 2 - 0.383444 (Pearson). No collinerarity between predictor 3 and the others. The Mediator is correlated with IV1 and slightly with IV2. Predictors, Mediator and dependent variable are all continuous.</p>

<pre><code>lm(DV ~ IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Only IV2 is significant, R2 = 0.050</p>

<pre><code>lm(DV ~ Mediator + IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Mediator and IV2 is significant, R2 = 0.056</p>

<p>I have a much bigger dataset with n = 1200, but unfortunately I don't have Mediator information available for them. If I do a linear regression to predict DV with this dataset, IV1 and IV2 are both highly significant, the standardized beta meaningful.</p>

<ol>
<li><p>With this information can I investigate the mediating effect of the mediator on IV1 with my small dataset with 455 subjects (using the mediate()-Function of the ""mediation""-package in R) , even though the dataset itself is too small to show a significant effect of IV1 on the DV?</p></li>
<li><p>Also, I was wondering whether my mediator might mediate IV2-effect. The correlation between IV1 and the mediator is higher than between IV2 and the mediator though. </p></li>
</ol>

<p>I am thankful for any ideas.</p>
"
"0.134303277811777","0.134856196073306","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.0990957479752576","0.0995037190209989","187487","<p>Let's say we have data that looks like this:</p>

<pre><code>set.seed(1)
b0 &lt;- 0 # intercept
b1 &lt;- 1 # slope
x &lt;- c(1:100) # predictor variable
y &lt;- b0 + b1*x + rnorm(n = 100, mean = 0, sd = 200) # predicted variable
</code></pre>

<p>We fit a simple linear model:</p>

<pre><code>mod.1 &lt;- lm(y~x) 
summary(mod.1) 
#             Estimate   Std. Error  t value  Pr(&gt;|t|)
# (Intercept) 26.3331    36.3795     0.724    0.471
# x           0.9098     0.6254      1.455    0.149 
b0.est &lt;- summary(mod.1)$coefficients[1,1]
b1.est &lt;- summary(mod.1)$coefficients[2,1]
</code></pre>

<p>And a model where we (1) subtract off the intercept term fit in the first model from the dataset and (2) prevent the intercept term from being fit (or in other words, force the model through zero):</p>

<pre><code>mod.2  &lt;- lm(y - b0.est  ~ 0 + x) 
summary(mod.2) 
#             Estimate   Std. Error t value   Pr(&gt;|t|)   
# x           0.9098     0.3088     2.946     0.00401 **
b1.est.2 &lt;- summary(mod.2)$coefficients[1,1]
</code></pre>

<p>As to be expected the slope parameter stays the same (0.9098).</p>

<p>However, while the slope parameter was not significant in the first model, it is in the second model (the standard error on the estimate in the second model is much lower than in the first model, 0.3088 vs. 0.6254).</p>

<p>The data is the same shape in both models with the same slope parameter being estimated by the two models. <strong>How is it the second model is so much more ""certain"" of the slope parameter estimate?</strong></p>

<p><strong>Or to put it another way, how are these standard errors calculated?</strong> </p>

<p>Using the equation for standard error I found <a href=""http://stattrek.com/regression/slope-test.aspx?Tutorial=AP"" rel=""nofollow"">here</a>, I calculated the standard errors for model 1 and 2 this way:</p>

<pre><code># Model 1
DF &lt;- length(x)-2 
y.est &lt;- b0.est + b1.est*x 
numerator &lt;- sqrt(sum((y - y.est)^2)/DF) 
denominator &lt;- sqrt(sum((x - mean(x))^2))
numerator/denominator 
# SE = 0.6254
</code></pre>

<p>This matches the R output.</p>

<pre><code># Model 2
DF &lt;- length(x)-1 
y.est &lt;- b1.est.2*x 
numerator &lt;- sqrt(sum((y - (y.est+b0.est))^2)/DF) 
denominator &lt;- sqrt(sum((x - mean(x))^2))
numerator/denominator 
# SE = 0.6223
</code></pre>

<p>This doesn't match the R output which has the SE = 0.3088. </p>

<p>What am I missing?</p>
"
"0.0858194351535935","0.0861727484432139","187839","<p>I am studying t-intervals calculation concept and trying to perform the calculations in R.</p>

<p>My issue is that I am performing the calculation through three different methods and I am getting three different results, so I guess I am doing something wrong.</p>

<p>Here are my calculations:
I am using the R <code>mtcars</code> dataset and estimating a Linear Model (using R function <code>lm</code>) of <code>mtcars$mpg</code> as the outcome and <code>mtcars$wt</code> (car's weight) as the regressor:</p>

<pre><code>data(mtcars)
y &lt;- mtcars$mpg; x &lt;- mtcars$wt; n &lt;- length(y)
se &lt;- sd(y)/sqrt(n) # Standard Error of the Estimation
#now, I calculate the conf interval (ci) using the concept's formula
ci &lt;- mean(x) + c(-1,1)*qt(.975,df=n-1)*se
ci
[1] 1.044304 5.390196
f &lt;- lm(y ~ x)
c &lt;- summary(f)$coefficients
c
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 37.285126   1.877627 19.857575 8.241799e-19
x           -5.344472   0.559101 -9.559044 1.293959e-10
b0 &lt;- c[1,1] ; b1 &lt;- c[2,1]
yh &lt;- b0 + b1*mean(x)
yh
20.09062
c(yh-ci[1],yh+ci[2])
19.04632 25.48082 #this is my t interval using the concept formula

#Now, the t-interval calculation using R t.test function
t.test(y)
One Sample t-test
data:  y
t = 18.8569, df = 31, p-value &lt; 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
  17.91768 22.26357
sample estimates:
mean of x 
  20.09062 
#So, you can see I get the same estimation of y (20.09...) but a different t-   interval as that calculated before with the formula

#Finally, the t-interval calculated with R predict formula
predict(f, data.frame(x=mean(x)), interval=""confidence"")
    fit      lwr      upr
1 20.09062 18.99098 21.19027
#and, again, I got the same fit value of y (20.09..) but another different value for the t interval
</code></pre>

<p>So, I wonder if I have an error in the concept or if I have an error using the R functions.</p>
"
"0.0990957479752576","0.0995037190209989","188098","<p>I am trying to estimate a model for an event modelled by probability of happening which is a linear function of x (distributed normally) plus an error term, u.</p>

<p>Then I simulate whether the event really happened for each X comparing the probability of it happening against a uniformly distributed random variable.</p>

<p>So, I wrote a little function that simulates this model for a given b0, b1, X (mean and sd.) and error term (mean = 0 and sd.):</p>

<pre><code>SAMPLE_SIZE = 10000

underlying &lt;- function(b0, b1, mean_x, sd_x, sd_u) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean_x, sd_x)
  us &lt;- rnorm(SAMPLE_SIZE, 0.0, sd_u)
  ys &lt;- b0 + (b1 * xs) + us
  ws &lt;- runif(SAMPLE_SIZE) &lt; ys  
  list(ws = ws, ys = ys, xs = xs, us = us)
}
</code></pre>

<p>It neatly returns both the probability of the event taking place in the ys component plus a simulation on the ws component.</p>

<p>I then tested whether I can correctly estimate b0 and b1 using linear regressions. And I got a very weird result.</p>

<p>This is how I simulated the samples and did the regressions:</p>

<pre><code>b1s &lt;- seq(from = 0.0, to = 1.0, length.out = 100)

datasets &lt;- lapply(b1s, FUN = function(x) underlying(0.5, x, 1.0, 0.2, 0.05)) 
regs     &lt;- lapply(datasets,  FUN = function(x) lm(data = x, ws ~ xs))
b0s_hat = sapply(regs, function(x) x$coefficients[[1]])
    b1s_hat = sapply(regs, function(x) x$coefficients[[2]])
</code></pre>

<p>So, for different b1s (and b0 = 0.5) I can plot the estimated b0 and b1 against the real b1:</p>

<pre><code>plot(b1s, b0s_hat)
plot(b1s, b1s_hat)
</code></pre>

<p>And what we get for b1s_hat looks sigmoid-ish like a cumulative distribution function, and b0s_hat looks like a bell curve (like the density function).</p>

<p>I thought I could recover the coefficients using the linear regression. What exactly is smelling weird here?</p>
"
"0.0762839423587497","0.0765979986161901","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"NaN","NaN","188289","<p>(This is a follow up to this great answer: <a href=""http://stats.stackexchange.com/a/136597/99091"">How to perform orthogonal regression (total least squares) via PCA?</a>.)</p>

<p>I have run this:</p>

<pre><code>v    &lt;- prcomp(cbind(X,y), center=TRUE, scale=TRUE)$rotation
beta &lt;- -v[-ncol(v),ncol(v)] / v[ncol(v),ncol(v)]
</code></pre>

<p>Now I would like to know how to transform my beta back to the scale and center of my original variables. I've tried multiplying by the sd and adding the mean but I've had no luck and I'm really confused. </p>
"
"NaN","NaN","188399","<p>I would like to train a model that has a probability (a success rate between 0 and 1) as outcome.</p>

<p>So the data looks like this:</p>

<pre><code>feature1  feature2   success_rate
0.1       0.3        0.55
0.3       0.6        0.45
</code></pre>

<p>I started using <em>xgboost</em> (gradient boosting machine) with:</p>

<pre><code>""objective"" = ""reg:logistic""
""eval_metric"" = ""auc""
</code></pre>

<p>which means I doing a logistic regression using the Area Under the Curve (AUC) as evaluation function to measure the improvement of the model.</p>

<p>But I understand a logistic regression is usually trained with a categorical target (success or failure), not a probability.
Does this matter? and is this the right approach?</p>
"
"0","0.0287242494810713","188753","<p>I'm attempting to predict vegetation productivity based on climatic and land use variables (the latter are categorical). I found that there is a multicollinearity problem between the predictors (especially land use) as seen from the Variance Inflation Factor (VIF of the Ordinary Least Squares Regression). </p>

<p>Although my knowledge of lasso regression is basic, I assume lasso regression might solve the multicollinearity problem and also select variables that are driving the system. I appreciate an R code for estimating the standardized beta coefficients for the predictors or approaches on how to proceed.</p>

<pre><code>Variable           Coeff.  Std Coeff.  VIF    Std Error    t      P  Value 
Constant          -0.228   0            0      0.086       -2.644  0.008  
Precipitation      &lt;.001   0.151       2.688   &lt;.001        8.541  0.0  
Solar Rad          0.002   0.343       2.836   &lt;.001        18.939 &lt;.001  
Temp              -0.116  -1.604       28.12   0.004       -28.11  0.0  
Water Stress       0.881   0.391       2.352   0.037        23.7   &lt;.001  
Vapor Pressure     0.135   1.382       30.49   0.006        23.259 0.0    
  1               -0.103   -0.109      52.086  0.074       -1.398  0.162    
  2               -0.14    -0.048      6.49    0.079       -1.761  0.078   
  3               -0.11    -0.048      10.007  0.077       -1.42   0.156    
  4               -0.104   -0.234      236.288 0.073       -1.416  0.157    
  5               -0.097   -0.242      285.244 0.073       -1.331  0.183    
  6               -0.104   -0.09       35.067  0.074       -1.406  0.16    
  8               -0.119   -0.261      221.361 0.073       -1.629  0.103 
ELEVATION          &lt;.001   -0.115      3.917   &lt;.001       -5.381  &lt;.001
Condition Number: 59.833 
Mean of Correlation Matrix: 0.221 1st    
Eigenvalue divided by m: 0.328
</code></pre>
"
"0.0572129567690623","0.0574484989621426","189712","<p>I am studying the 7th chapter of Introduction to Statistical Learning with Applications in R (ISLR) and I don't understand why the confidence interval associated with polynomial regressions (as seen in the first plot <a href=""https://lagunita.stanford.edu/c4x/HumanitiesandScience/StatLearning/asset/ch7.html"" rel=""nofollow"">here</a>) is so narrow. </p>

<p>According to the book (and to the R code provided), the dotted lines correspond to the predicted value, plus or minus twice the standard error - which, for normally distributed error terms, corresponds to ~95% confidence interval. However, according to the R code below (sorry it's so ugly), only 254 out of 3000 observations fall within the confidence interval:</p>

<pre><code>library(ISLR)

age_range = range(Wage$age)
age.grid = seq(age_range[1], age_range[2])

fit = lm(wage~poly(age, 4), data=Wage)

preds = predict(fit, newdata=list(age=age.grid), se=TRUE)

within_interval = 0

for (i in 1:nrow(Wage)) {
    wage = Wage[i, ""wage""]
    age = Wage[i, ""age""]
    index = which(age.grid==age)
    pred = preds$fit[index]
    se = preds$se.fit[index]

    if (wage &gt;= pred - 2*se &amp; wage &lt;= pred + 2*se) {
        within_interval = within_interval + 1
    }
}
</code></pre>

<p>So why does the model gives out such a narrow confidence interval? Are the errors not normally distributed, and is the confidence interval meaningless? (The residual plot does show a long right tail.) What am I missing here?</p>
"
"0.10703564115707","0.107476300250388","189822","<p>What difference does centering (or de-meaning) your data make for PCA? I've heard that it makes the maths easier or that it prevents the first PC from being dominated by the variables' means, but I feel like I haven't been able to firmly grasp the concept yet. </p>

<p>For example, the top answer here <a href=""http://stats.stackexchange.com/questions/22329"">How does centering the data get rid of the intercept in regression and PCA?</a> describes how not centering would pull the first PCA through the origin, rather than the main axis of the point cloud. Based on my understanding of how the PC's are obtained from the covariance matrix's eigenvectors, I can't understand why this would happen.</p>

<p>Moreover, my own calculations with and without centering seem to make little sense.</p>

<p>Consider the setosa flowers in the <code>iris</code> dataset in R. I calculated the eigenvectors and eigenvalues of the sample covariance matrix as follows.</p>

<pre><code>data(iris)
df &lt;- iris[iris$Species=='setosa',1:4]
    e &lt;- eigen(cov(df))
    &gt; e
    $values
[1] 0.236455690 0.036918732 0.026796399 0.009033261

$vectors
            [,1]       [,2]       [,3]        [,4]
[1,] -0.66907840  0.5978840  0.4399628 -0.03607712
[2,] -0.73414783 -0.6206734 -0.2746075 -0.01955027
[3,] -0.09654390  0.4900556 -0.8324495 -0.23990129
[4,] -0.06356359  0.1309379 -0.1950675  0.96992969
</code></pre>

<p>If I center the dataset first, I get exactly the same results. This seems quite obvious, since centering does not change the covariance matrix at all. </p>

<pre><code>df.centered &lt;- scale(df,scale=F,center=T)
e.centered&lt;- eigen(cov(df.centered))
e.centered
</code></pre>

<p>The <code>prcomp</code> function results in exactly this eigenvalue-eigenvector combination as well, for both the centered and uncentered dataset. </p>

<pre><code>p&lt;-prcomp(df)
p.centered &lt;- prcomp(df.centered)
Standard deviations:
[1] 0.48626710 0.19214248 0.16369606 0.09504347

Rotation:
                     PC1        PC2        PC3         PC4
Sepal.Length -0.66907840  0.5978840  0.4399628 -0.03607712
Sepal.Width  -0.73414783 -0.6206734 -0.2746075 -0.01955027
Petal.Length -0.09654390  0.4900556 -0.8324495 -0.23990129
Petal.Width  -0.06356359  0.1309379 -0.1950675  0.96992969
</code></pre>

<p>However, the <code>prcomp</code> function has the default option <code>center = TRUE</code>. Disabling this option results in the following PC's for the uncentered data (<code>p.centered</code> remains the same when <code>center</code> is set to false):</p>

<pre><code>p.uncentered &lt;- prcomp(df,center=F)
&gt; p.uncentered
Standard deviations:
[1] 6.32674700 0.22455945 0.16369617 0.09766703

Rotation:
                    PC1         PC2        PC3         PC4
Sepal.Length -0.8010073  0.40303704  0.4410167  0.03811461
Sepal.Width  -0.5498408 -0.78739486 -0.2753323 -0.04331888
Petal.Length -0.2334487  0.46456598 -0.8317440 -0.19463332
Petal.Width  -0.0395488  0.04182015 -0.1946750  0.97917752
</code></pre>

<p>Why is this different from my own eigenvector calculations on the covariance matrix of the uncentered data? Does it have to do with the calculation? I've seen mentioned that <code>prcomp</code> uses something called the SVD method rather than the eigenvalue decomposition to calculate the PC's. The function <code>princomp</code> uses the latter, but its results are identical to <code>prcomp</code>. Does my issue relate to the answer I described at the top of this post?</p>

<p><strong>EDIT:</strong> Issue was cleared up by the helpful @ttnphns. See his comment below, on this question: <a href=""http://stats.stackexchange.com/questions/125937"">What does it mean to compute eigenvectors of a covariance matrix if the data were not centered first?</a> and in this answer: <a href=""http://stats.stackexchange.com/a/22520/3277"">http://stats.stackexchange.com/a/22520/3277</a>. In short: a covariance matrix implicitly involves centering of the data already. PCA uses either SVD or eigendecomposition of the centered data $\bf X$, and the covariance matrix is then equal to ${\bf X'X}/(n-1)$.</p>
"
"0.0700712753800578","0.0703597544730292","191611","<p>I'm using the set.seed() function in R to achieve reproducability of my results. I compare different regression methods (e.g. RandomForest, SVM, GAM) by their MSE derived from a cross-validation procedure. To my surprise, I realized that results differ whether I place 'set.seed(123)' at the beginning of my code (and then running the whole script) or whether I place 'set.seed(123)' just before calling each method in the script.</p>

<p>To illustrate pls follow my example below (although the answer by 'Sean Easter' and the example given by 'Cliff AB' below should explain as well): </p>

<pre><code>data(iris)
iris
myf&lt;- Sepal.Length ~ 
 Sepal.Width+
 Petal.Length+
 Petal.Width+
 Species

# required packages
library(sperrorest)
library(randomForest)
library(rpart)

##### Regression Tree
set.seed(123)
ctrl &lt;- rpart.control(cp = 0.001)
fit_rpart &lt;- rpart(myf, data = iris, control = ctrl)

#5-repeated 10-fold CV
mypred.rpart &lt;- function(object, newdata) predict(object, newdata)
eval_ns_rpart &lt;- sperrorest(data = iris, formula = myf, model.fun= 
                            rpart, model.args = list(control = ctrl),
                            pred.fun = mypred.rpart, smp.fun = 
                            partition.cv, smp.args = 
                            list(repetition=1:5, nfold=10))
summary(eval_ns_rpart$error)

##### Random Forest
#set.seed(123) # REMOVE HASH IN 2ND RUN!!!! 
fit_rf &lt;- randomForest(myf, data = iris, ntree=1000)

#5-repeated 10-fold CV
mypred.rf &lt;- function(object, newdata) predict(object, newdata)
eval_ns_rf &lt;- sperrorest(data = iris, formula = myf,
                         model.fun = randomForest,
                         pred.fun = mypred.rf,
                         smp.fun = partition.cv, smp.args= list(repetition=1:5, nfold=10))
summary(eval_ns_rf$error)

#### SUMMARIES Mean Squared Errors(MSE)
tr_MSE_rpart&lt;-(summary(eval_ns_rpart$error)[3,1]) # MSE training error
# 0.08548725

t_MSE_rpart&lt;-(summary(eval_ns_rpart$error)[10,1]) # MSE test error
# 0.1445583

tr_MSE_RF&lt;-(summary(eval_ns_rf$error)[3,1]) # MSE training error
# 0.07241344 # 2nd run: 0.07266605

t_MSE_RF&lt;-(summary(eval_ns_rf$error)[10,1]) # MSE test error
# 0.1403778  # 2nd run: 0.1358957
</code></pre>
"
"0.0404556697031367","0.0406222231851194","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.10703564115707","0.107476300250388","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.131589800568843","0.132131547612928","192173","<p>I'm working on the similarity of categorical regression with exclusively  dummy variables and ANOVA. There are lots of references, like Gujarati &amp; Porter (2009), which have mentioned that those two are equivalent. Everything is okay when distribution of residuals is normal, variances are homogeneous and regression model is significant. My questions are there. We have a category with 3 levels (red,blue,green), a numeric variable ""allscore""( -5 &lt;= allscore &lt;= +5). I played with R and made data and ran models (regression and variance).</p>

<pre><code># creating data 
bluescore  &lt;- rnorm(n=100, mean=-1, sd=1)
redscore   &lt;- rnorm(n=100, mean=2,  sd=1)
greenscore &lt;- rnorm(n=100, mean=.1, sd=2)
for (i in 1:100) {
  if (bluescore[i] &lt; -5)  bluescore[i]  &lt;- -5
  if (bluescore[i] &gt; 5)   bluescore[i]  &lt;-  5
  if (redscore[i] &lt; -5)   redscore[i]   &lt;- -5
  if (redscore[i] &gt; 5)    redscore[i]   &lt;-  5
  if (greenscore[i] &lt; -5) greenscore[i] &lt;- -5
  if (greenscore[i] &gt; 5)  greenscore[i] &lt;-  5
}
color &lt;- as.factor(c(rep(1,100), rep(2,100), rep(3,100)))
allscore &lt;- c(bluescore, redscore, greenscore)
table &lt;- data.frame(color, allscore)
randtable &lt;- table[sample(nrow(table)),]
finaltable &lt;- data.frame(randtable$color, randtable$allscore)
colnames(finaltable) &lt;- c(""color"", ""score"")
# plot
plot(randtable$allscore ~ randtable$color, data=finaltable)
# saving data for SPSS
library(rio)
export(finaltable, ""dummy.sav"")
write.csv(finaltable, ""finaltable.csv"")
# making dummy variables
dummyred   &lt;- NULL
dummygreen &lt;- NULL
dummyblue  &lt;- NULL
for(i in 1:NROW(finaltable)) {
  if (randtable$color[i]==2) dummyred[i]=1 else dummyred[i]=0
      if (randtable$color[i]==3) dummygreen[i]=1 else dummygreen[i]=0
  if (randtable$color[i]==1) dummyblue[i]=1 else dummyblue[i]=0
}
t1 = cbind(randtable, dummyred, dummygreen)
# run regression model 
mosel.1 &lt;- lm(formula = allscore~dummyred + dummygreen + dummyblue -1, data=t1)
ttt &lt;- summary(mosel.1)
ttt

# **test of homogenity**
# Bartlettâ€™s test
bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)
# Leveneâ€™s test
library(car)

leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
# Fligner-Killeen test
fligner.test(randtable$allscore ~ randtable$color, data=finaltable)
# ANOVA mode
hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
hh
summary(hh)
# post hoc test
TukeyHSD(hh)
</code></pre>

<p>Output would be something like this:  </p>

<p><a href=""http://i.stack.imgur.com/v0o8x.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v0o8x.png"" alt=""enter image description here""></a></p>

<pre><code>Call:
lm(formula = allscore ~ dummyred + dummygreen + dummyblue - 1, 
    data = t1)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.0152 -0.7880  0.0043  0.8088  3.3731 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
dummyred    2.02102    0.13273  15.227  &lt; 2e-16 ***
dummygreen  0.01525    0.13273   0.115    0.909    
dummyblue  -1.04294    0.13273  -7.858 7.24e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.327 on 297 degrees of freedom
Multiple R-squared:  0.4971,    Adjusted R-squared:  0.4921 
F-statistic: 97.87 on 3 and 297 DF,  p-value: &lt; 2.2e-16

&gt;  
&gt; # test of homogenity
&gt; # Bartlettâ€™s test
&gt; bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)

    Bartlett test of homogeneity of variances

data:  randtable$allscore by randtable$color
Bartlett's K-squared = 94.825, df = 2, p-value &lt; 2.2e-16

&gt; # Leveneâ€™s test
&gt; library(car)
&gt; 
&gt; leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value    Pr(&gt;F)    
group   2  43.995 &lt; 2.2e-16 ***
      297                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # Fligner-Killeen test
&gt; fligner.test(randtable$allscore ~ randtable$color, data=finaltable)

    Fligner-Killeen test of homogeneity of variances

data:  randtable$allscore by randtable$color
Fligner-Killeen:med chi-squared = 66.204, df = 2, p-value = 4.207e-15

&gt; # ANOVA mode
&gt; hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
&gt; hh
Call:
   aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

Terms:
                randtable$color Residuals
Sum of Squares         484.3572  523.2176
Deg. of Freedom               2       297

Residual standard error: 1.327281
Estimated effects may be unbalanced
&gt; summary(hh)
                 Df Sum Sq Mean Sq F value Pr(&gt;F)    
randtable$color   2  484.4  242.18   137.5 &lt;2e-16 ***
Residuals       297  523.2    1.76                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # post hoc test
&gt; TukeyHSD(hh)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

$`randtable$color`
         diff        lwr       upr p adj
2-1  3.063958  2.6218117  3.506104 0e+00
3-1  1.058184  0.6160382  1.500330 1e-07
3-2 -2.005774 -2.4479195 -1.563628 0e+00
</code></pre>

<ul>
<li>Is variance homogeneity check essential for regression model as assumption (because it compares means and equivalent to ANOVA)?</li>
<li>What is assumption for this regression model?</li>
<li>How can I interpret ""greendummy"" variable insignificance? Can I omit it from model? What theory support this omission? Is it means green color has no effect on scores? Is it equivalent to heterogeneity of variances?</li>
<li>How about ANOVA model, what can I say about the results?  </li>
<li>Can I remove green level from ANOVA?</li>
</ul>

<blockquote>
  <p>Gujarati, Damodar N.; Porter, Dawn C. (2009): Basic econometrics. 5th
  ed. Boston: McGraw-Hill Irwin (The McGraw-Hill series, economics).</p>
</blockquote>
"
"0.0990957479752576","0.0912117424359157","192436","<p>I'm not a stats major and I'd appreciate any help I can get.
I've got data with each point defined by a score (between 0-1) and a frequency (0-100%). At 100%, the score is most reliable, at 1% the score is VERY UNRELIABLE. A score of 1 represents a very interesting case, 0.5 is not interesting. I realistically only care about scores between 0.5-1 (0 would technically be ""interesting"", but I'm keeping things one-tailed). This score is normally distributed, with mean centred around 0.5 (at least it should be). Ideally, a score of 1 and frequency of 100% would be the most interesting case. I'm trying to rank data from most interesting to least interesting. Standard score makes sense to me in order to rank this, but I'm having some trouble computing the variance.</p>

<p>Currently, I'm assuming the mean is consistent across all frequencies (0.5), and I know the observed score, so I only need to determine variance (which I'm assuming is different at any given frequency). The issue that is complicating this is that the frequencies are NOT normally distributed. The majority of the data (>99% of the data) falls below 5% frequency (and score at lower frequencies becomes increasingly quantized. At 0.006% frequency (the lowest frequency), scores are either 0 or 1. At 0.012% frequency, scores are either 0, 0.5 or 1. etc.. At most points above 15% frequency, only one data point exists, so I'm not sure at all how to calculate the variance given a single point and only an estimate of the mean based on normal distribution.</p>

<p>Is there a way to create a ranked list from most interesting to least? Is standard score even appropriate?</p>

<p>What I've tried so far on R (data represents the entire data set, col1 is a row.name, V2 (column2) is frequency, V3 (column3) is the score): </p>

<pre><code>    &gt; head(data)
                          V2  V3
    CREB3L1      0.013793103 1.0
    MMP2         0.006896552 0.0
    PCDHB15      0.020689655 1.0
    FEZF1        0.006896552 1.0
    TRAF3IP2-AS1 0.013793103 0.5
    PP12613      0.013793103 0.5
data$half &lt;- abs(data$v3-.5);
datalp &lt;- locpoly(data$V2, data$half, bandwidth=dpill(data$V2,data$half));
Error in if (!missing(bandwidth) &amp;&amp; bandwidth &lt;= 0) stop(""'bandwidth' must be strictly positive"") : 
  missing value where TRUE/FALSE needed
dpill(data$V2, data$half)
[1] NaN 
&gt; summary(data)
       V2                 V3        
 Min.   :0.006897   Min.   :0.0000  
 1st Qu.:0.006897   1st Qu.:0.0000  
 Median :0.006897   Median :0.5000  
 Mean   :0.010443   Mean   :0.5105  
 3rd Qu.:0.013793   3rd Qu.:1.0000  
 Max.   :0.868965   Max.   :1.0000  
plot(data)
</code></pre>

<p><a href=""http://i.stack.imgur.com/uiGr4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uiGr4.jpg"" alt=""enter image description here""></a></p>

<p>Above clearly does not work as I can't generate a regression estimate using dpill. I'm assuming it is because the data is so quantized and discrete at lower frequencies? Again, I am not limited to ranking based on standard score, if anyone has any better idea or a method to transform the data to a linear scale, the goal is just to rank using a single score the most to least interesting cases (where frequency determines reliability of score).</p>
"
"0.0990957479752576","0.0995037190209989","192464","<p>I have like 30k rows in training set and 60k in test set. Distribution of dependent variable (ascending order) looks like this:</p>

<p><a href=""http://i.stack.imgur.com/MOTXU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MOTXU.png"" alt=""A""></a></p>

<p>I believe I must use quantile regression here, as OLS regressions fit badly here.</p>

<p>I've got the following code:</p>

<pre><code>library(quantreg)
Y &lt;- cbind(A)
X &lt;- cbind(B,C,D,E,F,G)
quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
quantreg.plot &lt;- summary(quantreg.all)
plot(quantreg.plot)
</code></pre>

<p>At this point I have a plot of various distributions: 
<a href=""http://i.stack.imgur.com/LnHdK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LnHdK.png"" alt=""enter image description here""></a></p>

<p>As I understand that plot, tails (quantiles) of <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code> has different from OSL structure. I cant say anything about <code>F</code>.  Thus, I tried to use following code to get predicted values for the test set:</p>

<pre><code>P &lt;- predict.rq(quantreg25, newdata=test)
</code></pre>

<p>Which resulted in error message: </p>

<pre><code>Warning message:
'newdata' had 60000 rows but variables found have 30000 rows 
</code></pre>

<p>I get either vector or <code>DF</code> only for 30k rows (depending on number of quantiles). At the moment, I'm a bit stuck. In general: I need to get <code>A</code> vector using QRegression equation derived from the training set for test set independet variables.</p>

<p>1) Do I understand QRegression properly? E.g. Can I get a vector of predicted values like from OLS model, but with respect to quantiles distribution?</p>

<p>2) What can be said about <code>F</code> here?</p>

<p>3) How to omit error while fitting regression?</p>

<p>4) What quantiles and various equations here mean in general? I mean, I understand that they represent some law of distribution for <code>Nth</code> quantile of data, but how to apply these several equations to test data?</p>
"
"0.0872741054526671","0.0956000809414361","192714","<p>I want to estimate the current maximum capacity (in kWh) having the current power consumption (in kWh) and the state of charge of the battery (in %) available in a time series. </p>

<p>I do not have a full battery charge circle recorded but only a snippet with the state of charge going from ~95% to ~35% in a 1 second data recording interval.</p>

<p>At first, i cumulated the current power consumption and noticed that the progression between cumulated power consumption and state of charge was almost the same (see figure 1 and 2). </p>

<p><strong>progression of state of charge vs. cumulated power consumption</strong>
<a href=""http://i.stack.imgur.com/Y5tTp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Y5tTp.png"" alt=""enter image description here""></a></p>

<p>So i tried to use a linear regression model to predict two values of cumulated power consumption at 0% and 100% state of charge. In my assumption the delta of these two leaves me with the current maximum capacity of the battery.</p>

<p>Ideally i want to monitor the capacity during the data recording. Which means i only have data of a few percent of state of charge.
In figure 3 and 4 you can see my attempt of trying to create a linear regression model for every 2% state of charge. As you can see in the right plot, the estimated capacity has a very high variance.</p>

<p><strong>LEFT: regression lines (green) of every 2% state of charge; RIGHT: estimated capacity of every 2% state of charge</strong>
<a href=""http://i.stack.imgur.com/qOmlY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qOmlY.png"" alt=""enter image description here""></a></p>

<p>Since i am not an expert in neither the matter of regression nor batteries/physics, my questions are:</p>

<ul>
<li><p>Is there a much simple or more exact way (or both) to estimate the current maximum capacity of the battery?</p></li>
<li><p>If yes, is there a good way to estimate an sufficiently exact capacity with an even smaller snippet in an ongoing process, lets say every 2% state of charge? (maybe using a characteristic curve of the discharge process)</p></li>
</ul>
"
"0.0858194351535935","0.0861727484432139","192993","<p>I have a question about including both time based and individual fixed effects. (I am using within fixed effects.) They are both significant individually when I run an F-test on a basic OLS regression without the effects:</p>

<pre><code>F test for time effects
F = 22.593, df1 = 24, df2 = 699, p-value &lt; 2.2e-16
alternative hypothesis: significant effects

F test for individual effects
F = 2.1279, df1 = 29, df2 = 694, p-value = 0.0005739
alternative hypothesis: significant effects
</code></pre>

<p>However, when I combine them, they are less significant than just the time based fix effects. Does that mean that I should not include the individual fixed effects?</p>

<pre><code>F test for twoways effects
F = 13.614, df1 = 53, df2 = 670, p-value &lt; 2.2e-16
alternative hypothesis: significant effects
</code></pre>
"
"0.0948769553749019","0.0952675579132743","193000","<p>So I'm trying to fit a hurdle model with the count distribution as negative binomial.  I get the following outputs for assuming negative binomial and poisson:</p>

<pre><code>&gt; hurdle(degree ~ dc, data = data, dist = ""negbin"")

Call:
hurdle(formula = degree ~ dc, data = data, dist = ""negbin"")

Count model coefficients (truncated negbin with log link):
(Intercept)           dc  
     0.2428       0.1035  
Theta = 0.8815 

Zero hurdle model coefficients (binomial with logit link):
(Intercept)           dc  
    -0.8512       0.1649

&gt; hurdle(degree ~ dc, data = data, dist = ""poisson"")

Call:
hurdle(formula = degree ~ dc, data = data, dist = ""poisson"")

Count model coefficients (truncated poisson with log link):
(Intercept)           dc  
    0.68283      0.08584  

Zero hurdle model coefficients (binomial with logit link):
(Intercept)           dc  
    -0.8512       0.1649  
</code></pre>

<p>From a regression in python based on estimates of mean of non zero data vs. regressor, I get:</p>

<p>m = 0.08374289, b =  0.7132967</p>

<p>Which is far from what the negative binomial estimates, but the Poisson gets it pretty close.  However a vuong test tells me that the negative binomial is far better:</p>

<pre><code>&gt; vuong(mod_pois, mod_nb_hurdle)
Vuong Non-Nested Hypothesis Test-Statistic: -114.0873 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
in this case:
model2 &gt; model1, with p-value 0 
</code></pre>

<p>The non-zero data is overdispersed, but it looks like the variance is constant*mean, so I know Poisson shouldn't be used, but why is a Poisson hurdle so much better at predicting log(expected value)? </p>
"
"0.0429097175767967","0.043086374221607","193440","<p>I am estimating a gravity model of migration on cross-sectional data. The Moran I statistic indicates a positive and significant spatial autocorrelation in the residuals of the non-spatial model, and the Lagrange Multiplier test points to the Spatial Autoregressive (SAR) model as the preferred specification.</p>

<p>While I have no issue fitting a linear SAR, it does not accommodate the very large number of zeroes (> 90%) in my dependent variable. This clearly point to a Poisson process. </p>

<p>In short, I am having trouble fitting the SAR Poisson model. I had two questions:</p>

<ol>
<li>Is there a method to run a SAR Poisson GLM in R? (I searched quite a bit
before posting here)</li>
<li>If answer (1) is no, I should at least use a spatially filtered
Poisson GLM. Yet, both SptatialFiltering() and ME() methods crash even using
a very simple connectivity structure (symmetric knn = 5). I mean
that it did not give any message error but RStudio simply ""lost the
connection with the R session"". I suspect this is due to the large
number of observation (278 784). Any tip to increase computational
efficiency?</li>
</ol>

<p>I am aware this question focuses on programming but I posted it on CV rather than SO because it require a statistician rather than a programmer.</p>
"
"0.0429097175767967","0.0574484989621426","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"0.0404556697031367","0.0406222231851194","194470","<p>I am new to R and I am trying to find a relationship between Wing length (mm) and Weight (g) of black-capped chickadees using a data set of over 4000 data points. </p>

<p>I did a regression analysis and made a residual plot, and the result is attached. I'm new to R and new to statistics so I'm not sure what this means exactly...any insight would be helpful!</p>

<p>Also any tips on other ways to analyse this data would be great. </p>

<p><a href=""http://i.stack.imgur.com/4FQJW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4FQJW.png"" alt=""enter image description here""></a><a href=""http://i.stack.imgur.com/1fQ2u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1fQ2u.png"" alt=""enter image description here""></a></p>
"
"0.0330319159917525","0.0497518595104995","195021","<p>I am trying to test out a hand-rolled cross-validation procedure for ridge regression. </p>

<p>I've run the <code>glmnet</code> package which gives me an MSE bottoming out around log(lambda)=0.5. My hand-rolled one gives log(lambda) = 5. </p>

<p>I think I must've screwed up in the code somewhere, but I don't see it.</p>

<p>Using <code>glmnet</code>:</p>

<pre><code>require(glmnet)
X = scale(NIR)
L = 500 # lambda.max
V = 5 # number of folds
lSeq = seq(1, L, by=1)
K = 5 #folds

cvres = cv.glmnet(X, y, nfolds = K)
plot(cvres)
</code></pre>

<p>Hand-rolled:</p>

<pre><code>require(caret)
folds = createFolds( y = y, k=K)
l = seq(10,1000,10)

# For each of the folds folds[v]
# For each of the ridge parameter values l[i]

ridge_mse = vector()
for(i in 1:length(l)){
  beta = matrix(nrow=ncol(X), ncol=length(folds))
  fold_error = vector()
  for(v in 1:length(folds)){ #for each fold
    X_nv = X[-folds[[v]],]
    beta[,v] = solve(crossprod(X_nv) + l[i] * diag(nrow=ncol(X_nv))) %*% crossprod(X,y)
    beta_int = rbind(mean(y[-folds[[v]]]), beta) # with an intercept
    fold_error[v] = mean(  cbind(1, X[folds[[v]],]) %*% beta_int[,v] - y[folds[[v]]])^2
  }
  ridge_mse[i] = mean(fold_error) # mse for each ridge parameter value
}
plot(log(l),ridge_mse, type=""l"")
</code></pre>

<p>These are the plots of the MSE estimates (glmnet on the left). </p>

<p><a href=""http://i.stack.imgur.com/FEupE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FEupE.png"" alt=""enter image description here""></a></p>
"
"0.0990957479752576","0.0995037190209989","195120","<p>I recently ran a beta regression model in R using the <code>betareg</code> package. I am modeling a continuous dependent variable (a fraction out of 1) that is bound between 0 and 1, as a function of a continuous variable that only takes on positive values. Model code, and code to generate residual vs. fitted is here:</p>

<pre><code>fit &lt;- betareg(y ~ x, data=d)
plot(residuals(fit) ~ fitted(fit))
</code></pre>

<p>The residual vs. fitted plot looks like this:
<a href=""http://i.stack.imgur.com/5hVHy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5hVHy.png"" alt=""enter image description here""></a></p>

<p>So like... what is going on here. Is this normal for beta regression, or have I mis-specified my model somehow?</p>

<p>Histogram of dependent variable, <code>y</code>:
<a href=""http://i.stack.imgur.com/bB2wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bB2wK.png"" alt=""enter image description here""></a></p>

<p>Histogram of independent variable, <code>x</code>:
<a href=""http://i.stack.imgur.com/8T7gq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8T7gq.png"" alt=""enter image description here""></a></p>

<p>output from <code>summary(fit)</code></p>

<pre><code>Call:
betareg(formula = relEM ~ mat, data = d1)

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.0716 -0.3940 -0.1730  0.4468  2.0633 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.863468   0.062820  13.745   &lt;2e-16 ***
mat         -0.053734   0.005667  -9.482   &lt;2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi) 0.261182   0.005735   45.54   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 1.156e+04 on 3 Df
Pseudo R-squared: 0.03853
Number of iterations: 14 (BFGS) + 1 (Fisher scoring) 
</code></pre>
"
"0.0952081150392732","0.103566754353222","195306","<p>I am trying to compare and plot regression lines of an ANCOVA when their slopes are the same and no interaction effect </p>

<p>There are two sets of ANCOVAs plots that I am trying to create:</p>

<p>a) 1 categorical variable (2 levels) and 1 covariate and;
b) 1 categorical variable (2 levels) and 2 covariates</p>

<p>For a) I didn't have much of an issue plotting the data, the code I used was:</p>

<blockquote>
  <p>modSMR &lt;- lm(SMR ~ CROSS + BM, data = CTMB)</p>
  
  <p>predSMR &lt;- predict(modSMR) </p>
  
  <p>ggplot(data = cbind(CTMB, predSMR), aes(SMR, BM, shape=CROSS)) +    geom_line(aes(y=predSMR)</p>
</blockquote>

<p>This code more or less created what I was looking for, a plot with two, straight, parallel regression lines running through the data points.</p>

<p>However once I add in the second covariate everything seems to be turned upside down.</p>

<p>In attempt to plot b) I tried using this code:</p>

<blockquote>
  <p>modSMR.ET &lt;- lm(ET ~ CROSS + SMR +  BM, data = CTMB)</p>
  
  <p>predSMR.ET &lt;- predict(modSMR.ET) </p>
  
  <p>ggplot(data = cbind(CTMB, predSMR.ET), aes(SMR, ET, shape=CROSS)) +    geom_line(aes(y=predSMR.ET)</p>
</blockquote>

<p>With this code, the regression lines were all wonky. They seemed to be oriented in a parallel fashion but they were no where near being straight or displayed the same patterns.  </p>

<p>I have tried taking the residuals of SMR and BM as well as adjusting SMR to a mean BM and plugging in to the model so that there is only 1 covariate but neither method produces the straight parallel lines I am looking for. I am not sure why I was able to produce them in a) using 1 covariate but not in b) when using 1 covariate. </p>

<p>Ideally I would like to create the plot with the 2 covariates described above. </p>

<p>*Note: SMR and BM are correlated with one another so there is some collinearity there however I am not sure how it will effect the model. I did try taking the residuals of the two as well as adjusting SMR to a mean BM in attempt to solve this issue but I ended up with similar results.</p>

<p>I hope this all makes sense and I look forward to hearing some suggestions</p>

<p>Thanks in advance </p>
"
"0.131091352563232","0.131631047527805","195359","<p>I have a set of complex survey data with sampling weights. I am using the <code>svyglm()</code> function from the <code>survey</code> package in R to describe the relationship between 2 variables in a GLM. I am using the quasipoisson family because both variables are over-dispersed. </p>

<p>The GLM output is as follows:</p>

<pre><code>hlsereg &lt;- svyglm(formula = HLSEPALLACRESFIX ~ HLSE_ACRE, sbdiv, family = quasipoisson)

Survey design:
svydesign(id = ~1, weights = ~spwgtdividedby3, data = sportsbind)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.489465   0.414979  13.228   &lt;2e-16 ***
HLSE_ACRE   -0.002744   0.001118  -2.454   0.0144 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 2.601914e+15)

Number of Fisher Scoring iterations: 12
</code></pre>

<p>I have used the <code>predict()</code> and <code>lines()</code> function to plot this model output:</p>

<pre><code>acreaxis &lt;- seq(0,2000,.1)
hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis))
    plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,35), col=alpha(""red"",.35), font = 2, font.lab = 2)
    lines(acreaxis, hlse, lwd=4, col = ""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/3EUZ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3EUZ6.png"" alt=""enter image description here""></a></p>

<p>This plots a line given by the regression output of an intercept at 5.5 and a very slow negative slope of -.003, but I'm uncertain if this is a correct representation of the line.</p>

<p>I have found others using the <code>predict(..., type = ""response"")</code> option, which is shown in various plots of quasipoisson models, including the one found by @Glen_b at <a href=""http://stats.stackexchange.com/a/177926/45582"">this question</a> and for <a href=""http://stats.stackexchange.com/questions/38201/problems-plotting-glm-data-of-binomial-proportional-data?rq=1"">binomial GLMs here</a>. The <code>predict.glm()</code> help page notes for the <code>type</code> argument that: ""The default is on the scale of the linear predictors; the alternative ""response"" is on the scale of the response variable."" I just don't understand what that means.  The ""response"" type yields a very different prediction line, which is curved and at a much higher value (note the scale of the y-axis, with an intercept at ~250):</p>

<pre><code>hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis), type = ""response"")
plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,400), col=alpha(""red""),     font = 2, font.lab = 2)
lines(acreaxis, hlse, lwd=4, col = ""black"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/jnY9T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jnY9T.png"" alt=""enter image description here""></a></p>

<p>I have also tried to run a GLM using the negative binomial distribution, but despite inputting the quasipoisson coefficient values for starting values, the model can't find valid coefficients (I have purged all zeros from the data):</p>

<pre><code> hlsereg.nb &lt;- glm.nb(HLSEPALLACRESFIX~HLSE_ACRE,data = model.frame(sbdiv.scaledweights), start = c(5.45, -.003))
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning message:
glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>My questions:</p>

<p>1) What is the most appropriate illustration of the GLM output from a quasipoisson family?<br>
2) If the negative binomial is more appropriate to describe this relationship, why can't it find a coefficient? If I figure out how to get it to find a coefficient, how would I visualize that output?</p>
"
"0.0904616275314925","0.0817506471951855","197001","<p>I am trying to follow the procedure offered by <a href=""http://www.jstor.org/stable/2082979?seq=1#page_scan_tab_contents"" rel=""nofollow"">Beck and Katz 1995</a> in a way that I also have a TSCS data with $T=100$ (time dimension) and $N=12$ (unit dimension). My data is not balanced, which means that for some time periods, not all units have observations. </p>

<p>I am using R, and I found a <code>pcse</code> package that does what I need. It calculates panel corrected standard errors which accounts for contemporaneous correlation of errors across units and unit level heteroskedasity of errors. However, the steps I have to take to calculate panel robust standard errors for this type of regression start with the need to correct for serial correlation of errors, if I understand it well. Particularly, that is what is recommended in <code>pcse</code> package documentation:</p>

<p><a href=""http://i.stack.imgur.com/GNowz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GNowz.png"" alt=""enter image description here""></a></p>

<p>So, I am lost trying to understand what I need to do. My options how I see them:</p>

<ol>
<li>Run simple OLS regression on my pooled panel data. </li>
<li><p>Test for serial correlation of error term using Durbinâ€“Watson test and examining ACF/PACF. In most cases, I will have AR(1) in errors. </p>

<ul>
<li>Either compute clustered standard errors - it should account for the fact that errors should be clustered on the unit variable. After this step, I would get robust standard errors, but I cannot use it in pcse estimation - I don't need the VCV of errors as an input for the <code>pcse</code> function, but the OLS <code>lm</code> object itself.</li>
<li>Or use Cochraneâ€“Orcutt transformation first, and then use transformed  model as an input for pcse estimation. I started doing it, but realized that after CO transformation, error term became serially independent, but had the kurtosis of 20 (normality assumption fails).</li>
</ul></li>
</ol>

<p>So, my options are not so suitable. How do you think I should approach this situation?</p>
"
"0.0858194351535935","0.0765979986161901","197488","<p>How would you go about making an unbiased comparison of two interventions (old vs. New <em>prtcl.binary</em> (0 and 1; individual worksheets vs. group work).) when there's a negative longitudinal slope present (longitudinal meaning the data is measured over time, but not repeatedly since each student only had one measurement)?
<a href=""http://i.stack.imgur.com/jRP29.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jRP29.png"" alt=""enter image description here""></a></p>

<p>The switch happened in 2014 (NOTE: there's little data for 2015, ~5 cases, so you can pretty much group it with 2014) But the mean comparison seems a little questionable when the previous intervention was having an effect already, see the negative slope before 2014.</p>

<p>So if my outcome is a count (i.e., number of days student did Y behavior), that is overdispersed (so I'm using a negative binomial dist.)</p>

<p>I was concerned with just comparing the one protocol to another using <code>prtcl.binary</code> as a dummy variable, so I added <code>year</code> to adjust the means for the the linear effect (black line) and an interaction between <code>year</code> and <code>prtcl.binary</code>, the differential, to compare the slopes. But I'm not sure this fixes my problem with interpreting the main effect of <code>prtcl.binary</code>.</p>

<pre><code>MASS::glm.nb(formula = y.count ~ 1 + prtcl.binary + year + prtcl.binary : year, data = d0)
</code></pre>

<p>Someone told me this is a good case for detrending data (but I'm not fond of the idea of interpreting the regression coefficients of residuals; i.e., I'm dumb). Someone else suggested I used a mixed model and use year as a random effect.
I thought it might make sense to use the 2008 to 2013 data to predict a value for 2014 with a standard error and then compare it with the mean and standard error measured in just 2014.</p>

<p>Suggestions?</p>
"
"0.0495478739876288","0.0497518595104995","197634","<p>What is the correct way to compare correlation between 2 dependent variables in R?</p>

<p>Thanks</p>

<p>Edit:
Here is the edited question and apologize for not asking this correctly before:</p>

<p>I acquired 2 measures from 2 different experiments and I want to know whether these 2 measures are correlated. </p>

<p>The problem is Measure 1 is confounded by some other covariates. So, I went ahead and did multiple regression and found the coefficient of my main effect. Since this is an estimate it has a mean with a deviation.</p>

<p>I could do cor(mean(parameter_estimate, Measure_2)) but I need to know if this correlation is significant. Is this right as I dont incorporate the spread of the estimate (variance)? My guess is the mean may be significant but with the standard error of the estimate, the correlation may become insignificant.</p>

<p>Thank you for your help</p>

<p>Regards</p>
"
"0.0858194351535935","0.0861727484432139","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.10340625341847","0.111248539872496","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"0.0286064783845312","0.0287242494810713","198655","<p>Can anyone please explain the identity link and log link in Poisson regression with simple example? For example, I have run a script using the <code>mtcars</code> data set in R.  </p>

<pre><code>head(mtcars) 
glm(mpg~disp+hp+wt, data=mtcars, family=poisson(link=""â€Œâ€‹identity""))
</code></pre>

<p><code>mpg</code>, <code>disp</code>, and <code>wt</code> are variables in the data set. I got the results:  </p>

<pre><code>(Intercept)= 35.205669
Coefficient of disp=-0.004313
Coefficient of hp=-0.028214
Coefficient of wt=-3.102409
</code></pre>

<p>How do I interpret these coefficients? How does the model look like mathematically? What does <code>link=""identity""</code> mean in the model?</p>
"
"0.0990957479752576","0.0995037190209989","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.100122674345859","0.114896997924285","198844","<p>I'm trying to understand how <code>auto.arima</code> with covariates in the xreg parameter works. I'm familiar with regression and I'm starting to work on forecasting.</p>

<p>My understanding of forecasting is that you look for patterns in the past time series and then project those paterns onto the future.  </p>

<p>My uderstanding of regression is that you use predictors to try to generate an output value and minimize the difference between your created value and the real value.  </p>

<p>So how does forecasting <code>auto.arima</code> with <code>xreg</code> work? Do you create a forecast for a timeseries based on past data and regression model based on the input time series and input <code>xreg</code>, and then forecast each data point in the time series and for each forecasted data point use the regression model you built and future <code>xreg</code> values to adjust the forecasted values?</p>

<p>I'm a former physics grad student, so I'm not allergic to math but I'm just looking for a high level overview of the process here to understand how forecasting <code>auto.arima</code> works.  </p>

<p>For example like, </p>

<ul>
<li><p>step 1: build forecast model on input time series, and regression model on input time series and input <code>xreg</code> values</p></li>
<li><p>step 2: forecast model into future one step, and predict value with regression model and future <code>xreg</code> values</p></li>
<li><p>step 3: algorithm combines forecasted value and regression model prediction to get combined value</p></li>
</ul>

<p>This is just a guess at how it works, but it's an example of the kind of high level explanation I'm looking for.</p>

<p>I've included some code below that I've been working on trying to forecast time in to out <code>TiTo</code> for customers at a restaurant with predictor count of customers in the restaurant <code>CustCount</code>.</p>

<pre><code>OV&lt;-zoo(SampleData$TiTo, 
    order.by=SampleData$DateTime)


eDate &lt;- ts(OV, frequency = 24)

Train &lt;-eDate[1:15000]
Test &lt;- eDate[15001:22773]

xregTrain &lt;- SampleData[1:15000,]$CustCount
    xregTest &lt;- SampleData[15001:22773,]$CustCount

Arima.fit &lt;- auto.arima(Train, xreg = xregTrain)

Acast&lt;-forecast(Arima.fit, h=7772, xreg = xregTest)

accuracy(Acast$mean,Test)
</code></pre>
"
"0.0286064783845312","0.0287242494810713","199141","<p>I would like to perform an anomaly temperature trends analysis in R. I have temperature data from 1901 to 2012. My idea to compute anomaly temperature trends was as follows: 
1. Compute temperature means per month over this time period
2. Subtract temperature means from actual data, to get an anomaly in temperature per month. 
3. Perform a linear regression on this anomaly values to get the anomaly temperature trends.</p>

<p>However, I am not quite sure about the approach. Anyone has done this before and could give me some hints? </p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.143032391922656","0.132131547612928","200155","<p>Just writing with a question about fixed effects when you have panel data with</p>

<ul>
<li>More than two time periods</li>
<li>Clusters, as in individual students within schools</li>
<li>An intervention given to some of the clusters mid-way through panel</li>
</ul>

<p>In particular, I'm trying to understand the diff-in-diff model used in a particular paper, ""<em>The Effects of Targeted Recruitment and Comprehensive Supports for Low-Income High Achievers at Elite Universities: Evidence from Texas Flagships</em>,"" by Rodney Andrews, Scott Imberman, and Michael Lovenheim.
You can find a copy of their paper here: 
<a href=""https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf"" rel=""nofollow"">https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf</a></p>

<p>Their regression equation is at the very bottom of page 17 (which is what I'm curious about).</p>

<p>Here's a short, super simplified summary, for the sake of my question, which is really about diff-in-diff and fixed effects, rather than the particulars of the paper itself:</p>

<p>The authors have a bunch of high schools in Texas. Each of these schools has many students. The schools and students are observed for a few years. Then some of the schools receive an intervention, meant to increase students' college-going and eventually their earnings as adults. Pretend for this example it's just a college scholarship program.* We observe the students in all these schools as they complete high school (or don't), enter college (or don't), and hopefully earn adult incomes. </p>

<p>The diff-in-diff part is comparing the change in adult earnings across cohorts of students that went to schools that received the intervention, compared to the trend among student cohorts that didn't attend the intervention high schools. </p>

<p>I've created some pretend data that tries to mimic the authors' real data in greatly simplified form. You can find that here, along with my (probably wrong) R script:
<a href=""https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing"" rel=""nofollow"">https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing</a></p>

<p>(You'll need to paste the cells into excel, which I used to create the values, since google docs doesn't have the same formulas. Apologies for the inconvenience.)</p>

<p>It's got 8 variables</p>

<ul>
<li>""id"": a row index variable</li>
<li>""student"": indexes students within schools, across cohorts. So the index goes from 1 to 20 within each school. But there are only 5 students per cohort, or high school graduating class, because there are 4 time periods. (not sure if that was smart).</li>
<li>""random_uniform"": just a uniform random variable between 0.01 and 0.99. I just used this to create the next variable.</li>
<li>""test_score"": a covariate, student test score. All are normally distributed with a standard deviation of 4, and a mean that's specific to the school and graduating cohort. For interest, I made it so that some schools started with lower overall means, but each school's mean score improved a little over time (about 10 points). All the scores are around 40-60. </li>
<li>""school"": a factor variable that indicates the school. There are 4 schools.</li>
<li>""treat_indicator"": a factor variable that is 0 before the intervention, and 1 after the intervention at the schools that receive the intervention (schools 1 and 2).</li>
<li>""time_period"": a factor variable that denotes the graduating class cohort. There are 4. </li>
<li>""adult_earnings"": a numeric variable that's a function of the students' high school test score (""test_score""), plus a bunch of noise. For the kids that received the ""college scholarship"" intervention--kids in the latter 2 cohorts at schools 1 &amp; 2--I've also added an additional earnings bump between 1,000 and 2,000 dollars, to simulate a treatment effect. </li>
</ul>

<p><strong>So my question is, how do I find the true effect of the intervention, if I want to use both high school graduating cohort (time) and school (cluster) fixed effects?</strong> </p>

<p>My R script is in that shared folder, but I'm not sure it's correct. The regression equation I gave R was</p>

<pre><code>fixedreg &lt;- lm(adult_earnings ~ treat_indicator + test_score + school + time_period, 
                     data=mydata)
</code></pre>

<p>Does that model the time and cluster fixed effects correctly, and create an unbiased coefficient on the ""treat_indicator"" variable?</p>

<p>Any insight would be much appreciated. Thanks!</p>

<hr>

<p>*Or read the actual paper and laugh at my ridiculous attempt to simplify all this.</p>
"
"0.0862517776135472","0.0952675579132743","200304","<p>I am trying to understand if there is a way to approximate what portion of the variance explained is being contributed by each independent variable in a random forest model. Just for illustration, I am borrowing the following model from the Stanford StatLearning class notes. This builds a random forest model for predicting median housing prices in Boston using the dataset provided with the <code>MASS</code> package.</p>

<pre><code>require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
</code></pre>

<p>Fitting the model (just using a simple model here without any validation just for illustration)</p>

<pre><code>rf.boston=randomForest(medv~.,data=Boston,subset=train)
rf.boston
</code></pre>

<p>I get the following output</p>

<pre><code>Call:
 randomForest(formula = medv ~ ., data = Boston, subset = train) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 4

          Mean of squared residuals: 12.34243
                    % Var explained: 85.09
</code></pre>

<p>Now <code>R</code> tells me that this model explains 85.09% variance in median housing prices. Additionally, I can run the <code>importance</code> command to figure out what variables turned out to be ""significant"" in my model.</p>

<pre><code>importance(rf.boston)

        IncNodePurity
crim        1487.1777
zn           142.0280
indus        965.7756
chas         234.6918
nox         1741.9305
rm          7435.3378
age          655.6031
dis         1357.3411
rad          316.3278
tax          794.0953
ptratio     1858.7183
black        455.5382
lstat       6947.9121
</code></pre>

<p>Is there a way to use these two pieces of information (or using some other approach) to tell us what percentage of <code>85.09</code> was explained by <code>crim</code>, <code>zn</code> and so on. </p>

<p>My goal here is to show this as a 100% stacked bar graph ordered by variable importance illustrating major drivers of the dependent variable (median housing prices in this example). Overall, I want to see if we can get outputs akin to shapely value regression  as shown <a href=""http://www.predictiveanalyticsworld.com/sanfrancisco/2013/pdf/Day2_1550_Reno_Tuason_Rayner.pdf"" rel=""nofollow"">here</a> (esp slide 21) using random forests.</p>
"
"0.154050600652431","0.149350858195547","200598","<p>This is a follow up question <a href=""http://stats.stackexchange.com/questions/191851/var-forecasting-methodology"">the question that can be found here</a>, and is a result of me having implemented (after as careful evaluation as I'm capable of) the alterations and changes suggested.</p>

<p>Below is my method and should be replicable. </p>

<p>My question relates to the implementation of k-fold cross validation and whether the code produces a mean average error value that is reliable and whether there are some aspects of k-fold cross validation I may have neglected, thus skewing any results.</p>

<p>Otherwise any comments, both as to the method as it stands or the logic behind their inclusion (see above link) is welcome.</p>

<pre><code>library(plyr)
library(forecast)
library(vars)

#Read Data
da=read.table(""VARdata.txt"", header=T)
dac &lt;- c(2,3) # Select variables
x=da[,dac]

plot.ts(x)
summary(x)

#Run Augmented Dickey-Fuller tests to determine stationarity and
#differences to achieve stationarity.
adf1 &lt;- ur.df(x[,""VAR1""], type = ""drift"", lags = 10, selectlags = ""AIC"")
adf2 &lt;- ur.df(x[,""VAR2""], type = ""drift"", lags = 10, selectlags = ""AIC"")

summary(adf1)
summary(adf2)

#Difference to achieve stationarity
d.x1 = diff(x[, ""VAR1""], differences = 1)
d.x2 = diff(x[, ""VAR2""], differences = 1)


#Check if differenced variables are stationary
adf1b &lt;- ur.df(d.x1, type = ""drift"", lags = 10, selectlags = ""AIC"")
adf2b &lt;- ur.df(d.x2, type = ""drift"", lags = 10, selectlags = ""AIC"")

summary(adf1b)
summary(adf2b)

#If variable is stationary I(0), do not difference
#Shorten undifferenced variable by n, so as to make all variables same length
# d.x2 = (x[, ""VAR2""])
# d.x2 = d.x2[-c(1:1)]

#Bind variables in time series
dx = cbind(d.x1, d.x2)
dx = as.ts(dx)
plot.ts(dx)

summary(dx)

#Lag optimisation
VARselect(dx, lag.max = 10, type = ""both"")

#Run VAR 
var = VAR(dx, p=2)

#Test for serial autocorrelation using the Portmanteau test
#Rerun var model with other suggested lags if H0 can be rejected at 0.05
serial.test(var, lags.pt = 10, type = ""PT.asymptotic"")

#ARCH test (Autoregressive conditional heteroscedasdicity)
arch.test(var, lags.multi = 10)

summary(var)

#Forecasting
prd &lt;- forecast(var, h = 12)

print(prd)
plot(prd)

# Forecast Accuracy
data &lt;- as.data.frame(dx)

k = 10 #Folds

# sample from 1 to k, nrow times (the number of observations in the data)
data$id &lt;- sample(1:k, nrow(data), replace = TRUE)
list &lt;- 1:k

# prediction and testset data frames that we add to with each iteration over
# the folds

prediction &lt;- data.frame()
testsetCopy &lt;- data.frame()

#Creating a progress bar to know the status of CV
progress.bar &lt;- create_progress_bar(""text"")
progress.bar$init(k)

for (i in 1:k){
  # remove rows with id i from dataframe to create training set
  # select rows with id i to create test set
  trainingset &lt;- subset(data, id %in% list[-i])
  trainingset &lt;- as.ts(trainingset)
  testset &lt;- subset(data, id %in% c(i))

  # run a VAR model
  mymodel &lt;- VAR(trainingset, p = 2)

  # remove response column 1
  temp &lt;- forecast(mymodel, h = nrow(testset))
  temp &lt;- do.call('cbind', temp[['mean']])
  temp &lt;- as.data.frame(temp)

  # append this iteration's predictions to the end of the prediction data frame
  prediction &lt;- rbind(prediction, temp)

  # append this iteration's test set to the test set copy data frame
  # keep only the desired Column
  testsetCopy &lt;- rbind(testsetCopy, as.data.frame(testset[,1]))

  progress.bar$step()
}

# add predictions and actual values
result &lt;- cbind(prediction, testsetCopy[, 1])
names(result) &lt;- c(""Predicted"", ""Actual"")
result$Difference &lt;- abs(result$Actual - result$Predicted)

# As an example use Mean Absolute Error as Evalution 
summary(result$Difference)
result
</code></pre>

<p><strong>Edit based on answer below:</strong></p>

<p>As per the answer below I have changed the code for the cross validation to this (full test code included for ease):</p>

<pre><code>library(forecast)
library(vars)
library(plyr)

x &lt;- rnorm(70)
y &lt;- rnorm(70)

dx &lt;- cbind(x,y)
dx &lt;- as.ts(dx)

j = 12  #Forecast horizon
k = nrow(dx)-j #length of minimum training set

prediction &lt;- data.frame()
actual &lt;- data.frame()

for (i in j) { 
  trainingset &lt;- window(dx, end = k+i-1)
  testset &lt;- window(dx, start = k-j+i+1, end = k+j)
  fit &lt;- VAR(trainingset, p = 2)                       
  fcast &lt;- forecast(fit, h = j)
  fcastmean &lt;- do.call('cbind', fcast[['mean']])
  fcastmean &lt;- as.data.frame(fcastmean)

  prediction &lt;- rbind(prediction, fcastmean)
  actual &lt;- rbind(actual, as.data.frame(testset[,1]))
}

# add predictions and actual values
result &lt;- cbind(prediction, actual[, 1])
names(result) &lt;- c(""Predicted"", ""Actual"")
result$Difference &lt;- abs(result$Actual - result$Predicted)

# Use Mean Absolute Error as Evalution 
summary(result$Difference)
</code></pre>

<p>Would this be a better application of cross validation? I realize that it is no longer k-fold, but is based on the link provided in the answer.</p>
"
"0.0286064783845312","0.0287242494810713","200794","<p>I have a problem with outputting the terms for a logistic regression model in R. For a given list of independent values, say list l of terms {w,y,z} to determine dependent variable {x}, I want to find out what the biggest regressor is when we pair two terms together. I want to be able to group multiple independent variables together and say ""when a record has this combination of values, then they have a very strong chance of predicting X"". I tried to just add the interactions when calling the glm function like glm(x~y + w + z + w:z + y:z + y:w, data = l). But the results come out very hard to explain, because of how they are measured between themselves and not just measured against the mean. Does anyone know a way to do this?</p>
"
"0.0404556697031367","0.0406222231851194","201487","<p>I have two independent variable and one dependent variable. This is my summary when I use <code>lm(y~x_1+x_2)</code>:</p>

<pre><code>Residuals:
    Min      1Q  Median      3Q     Max 
-22.265  -9.563  -1.916   6.405  39.319 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  23.0107    18.2849   1.258  0.21407   
x_1          23.6386     6.8479   3.452  0.00114 **
x_2          -0.7147     0.3014  -2.371  0.02163 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 14.84 on 50 degrees of freedom
Multiple R-squared:  0.2018,    Adjusted R-squared:  0.1699 
F-statistic: 6.321 on 2 and 50 DF,  p-value: 0.00357
</code></pre>

<p>I got stuck because neither the F value and R squared are very significant. However the p-value is less than 0.05. Does it mean that y depends on both variables? What should I do next in my regression analysis?</p>
"
"0.118129972176712","0.12520610071704","201971","<p>I am challenged by a simple, but hopefully interesting, data set.</p>

<h2>Data</h2>

<p>The data are driving times of ambulances to the scene (<code>data$actual</code>) as well as the driving times I created by using a GIS to calculate the time (<code>data$simulation</code>). The times differ because the GIS does not take into account that the ambulance drives faster than a standard car. Both times are in seconds. The actual data was provided in minutes, thus the steps in the data. You will find the data a the end of this post.</p>

<h2>Goals</h2>

<p>In order to use the GIS to predict which area the ambulance is able to cover I like to create a model that predicts a simulation driving time based on the actual time which I will then feed into the GIS simulation. This is necessary since the GIS itself does not account for the fact that ambulances drive faster than standard cars. The goal then is to use a longer driving time for the simulation in order to take this fact into account.</p>

<h2>Approach</h2>

<p>My first approach was to build a simple linear regression model for the data:</p>

<pre><code>model1 &lt;- lm(simulation ~ actual, data)
</code></pre>

<p>This gives site a bad R2 and residual standard error. In addition, I took into account the fact that if there is 0 seconds of actual driving time, there should also be 0 seconds of simulation driving time, resulting in:</p>

<pre><code>model2 &lt;- lm(simulation ~ 0 + actual, data)
</code></pre>

<p>Now the R2 drastically increases but the residual standard error also increases. Another thought involves the fact that the ambulance should always be faster than the normal car. So I filtered the data for <code>simulation &gt; actual</code> and created a third model:</p>

<pre><code>newData &lt;- data[data$simulation &gt; data$actual,]
model3 &lt;- lm(simulation ~ 0 + actual, newData)
</code></pre>

<p>This again increases the R2 and now also reduces the error even below the value of <code>model1</code>.</p>

<h2>My question</h2>

<p>Is this a legitimate way to handle the data given what I try to create? I think reducing the amount of data will often yield better results since less data points need to be taken care of. In addition, if you look at the variation of simulation time for every value of the actual driving time one could also try to create a model involving just the means and medians of the simulation time per actual time value (which yields even better results!).</p>

<h2>The data</h2>

<pre><code>structure(list(actual = c(120, 60, 120, 120, 240, 60, 120, 180, 
120, 60, 180, 420, 420, 180, 300, 240, 60, 180, 180, 60, 300, 
180, 240, 180, 60, 180, 420, 240, 60, 360, 180, 60, 240, 180, 
60, 60, 780, 60, 180, 240, 480, 240, 180, 120, 660, 180, 60, 
300, 420, 180, 240, 360, 840, 180, 240, 600, 300, 120, 60, 180, 
120, 60, 60, 120, 60, 180, 180, 180, 120, 360, 300, 180, 60, 
180, 360, 180, 180, 180, 180, 180, 240, 300, 600, 60, 60, 180, 
180, 600, 300, 60, 120, 300, 180, 60, 120, 60, 120, 120, 180, 
120, 120, 120, 240, 120, 120, 600, 120, 120, 180, 360, 300, 240, 
60, 180, 120, 420, 120, 180, 60, 120, 180, 240, 360, 300, 240, 
120, 180, 180, 300, 240, 180, 120, 180, 120, 120, 120, 240, 120, 
180, 180, 180, 60, 120, 180, 120, 420, 60, 180, 180, 240, 180, 
300, 180, 180, 360, 240, 540, 240, 120, 60, 120, 120, 60, 60, 
180, 180, 60, 180, 360, 300, 180, 240, 180, 180, 120, 120, 180, 
60, 180, 180, 240, 240, 180, 180, 180, 180, 180, 240, 120, 180, 
120, 180), simulation = c(194.28940773, 212.275300026, 220.287079812, 
24.607690572, 407.197437288, 81.217067244, 24.607690572, 150.680236818, 
478.658294676, 136.179299352, 377.049865722, 194.28940773, 261.164245608, 
319.750185012, 220.287079812, 351.498241422, 8.703469632, 478.658294676, 
24.607690572, 173.848915098, 220.287079812, 81.217067244, 212.275300026, 
24.607690572, 136.179299352, 150.680236818, 220.287079812, 407.197437288, 
377.049865722, 204.83267784, 220.287079812, 173.848915098, 220.287079812, 
212.275300026, 136.179299352, 194.28940773, 351.498241422, 377.049865722, 
478.658294676, 407.197437288, 664.460391996, 659.49136734, 171.987490656, 
162.42626667, 485.496425628, 360.000858306, 121.588454244, 24.607690572, 
478.658294676, 171.987490656, 152.808523176, 664.460391996, 659.49136734, 
360.000858306, 485.496425628, 162.42626667, 24.607690572, 274.938783648, 
121.588454244, 115.878911016, 385.97213745, 94.89244938, 140.229663846, 
262.36567497, 94.89244938, 115.878911016, 115.878911016, 115.878911016, 
239.758086204, 303.008880618, 519.334259034, 68.913009168, 239.758086204, 
353.441877366, 303.008880618, 68.913009168, 68.913009168, 303.008880618, 
280.39235115, 428.468284608, 259.42299843, 182.360544204, 671.648883822, 
96.808075902, 96.598634718, 186.045684816, 369.657411576, 293.113288878, 
392.484369276, 56.862205266, 343.983478548, 369.657411576, 428.468284608, 
80.855455398, 144.722843172, 60.819990636, 157.677226068, 139.932003024, 
78.863933088, 212.355537414, 158.009676936, 243.857574462, 292.072420122, 
167.319359778, 158.009676936, 270.116386416, 158.009676936, 100.485241416, 
349.8108387, 194.206109046, 538.366470336, 174.882373812, 97.03774452, 
428.468284608, 20.02849281, 615.891094206, 169.016976354, 100.77576399, 
158.009676936, 78.04938555, 99.34376478, 226.997423172, 490.142440794, 
88.538596632, 243.464784624, 266.780548098, 212.355537414, 206.20563984, 
343.983478548, 428.468284608, 428.468284608, 158.009676936, 186.045684816, 
144.722843172, 157.677226068, 212.355537414, 428.468284608, 428.468284608, 
210.082454682, 243.857574462, 280.39235115, 96.808075902, 20.02849281, 
369.657411576, 169.016976354, 490.142440794, 80.855455398, 266.780548098, 
428.468284608, 226.997423172, 158.009676936, 343.983478548, 343.983478548, 
243.857574462, 490.142440794, 428.468284608, 671.648883822, 428.468284608, 
428.468284608, 169.016976354, 139.932003024, 78.863933088, 60.819990636, 
96.598634718, 99.34376478, 369.657411576, 80.855455398, 167.319359778, 
194.206109046, 369.657411576, 158.009676936, 212.355537414, 169.016976354, 
186.045684816, 210.082454682, 428.468284608, 144.722843172, 157.677226068, 
212.355537414, 158.009676936, 194.206109046, 158.009676936, 243.857574462, 
428.468284608, 428.468284608, 99.34376478, 428.468284608, 538.366470336, 
280.39235115, 164.87254143, 177.99147606, 99.029567244)), .Names = c(""actual"", 
""simulation""), row.names = c(NA, -192L), class = ""data.frame"")
</code></pre>
"
"0.051172824211752","0.0642293744423385","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0.0572129567690623","0.0574484989621426","202447","<p>I often come across a classification problem - where we have 0/1 binary outcome and several features. And the main goal is build a classifier on training set.</p>

<p>Now given several choices of algorithms - Random forests, logistic regression, SVM, etc., is there a scientific approach one can apply to choose one among the above algorithms just based on the data attributes. By attributes I mean number of features in dataset, no. of categorical variables, how many levels in categorical variables, etc.</p>

<p>In other words, you have dataset and based on it you take a call which method suits best.</p>

<p>The reason I ask is that I currently apply different methods and choose one with the best accuracy on cross validation set. But I think there is a way to narrow down on methods just based on dataset features.</p>

<p>Would appreciate any thoughts on this.</p>

<p>Thanks in advance!</p>
"
"NaN","NaN","203298","<p>I am implementing some machine learning algorithms on a  large data set (90K rows) with 274 different variables. I have to carry out Logistic Regression and Random Forest for this data set. meanwhile, I want to carry out feature selection to reduce the number of those variables drastically.</p>

<p>What would be an effective feature selection algorithm (in R) for classification use case?
Thanks,
Aman</p>
"
"0.0540611626362958","0.0542837290884934","203359","<p>Consider the following heteroscedastic model:
$$y_i = f(x_i, \beta) + g(x_i, \theta)\varepsilon_i, i = 1, \ldots, n, \tag{1}$$
where $f(\cdot, \beta)$ is the regression function and $g(\cdot, \theta)$
is the variance function. For simplicity, assume the errors $\{\varepsilon_i\}$ are i.i.d. with mean $0$ and variance $\sigma^2$.</p>

<p>Regarding model $(1)$, I understand (but I am not quite sure) that the <code>gls</code> function in <code>nlme</code> package can be used (at least when $f$ is linear) to implement the iteratively reweighted least squares algorithm (Carroll, Ruppert, <em>Transformation and Weighting in Regression</em>, pp. 69). When I read the manual of <code>nlme</code>, it looks to me that <code>gls</code> function restricts the forms of $g$ to a very small class of functional forms. For example, given observations $\{(y_i, x_{i1}, x_{i2}): i = 1, \ldots, n\}$, is it
possible to use <code>gls</code> to fit the following special case of $(1)$:
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \sqrt{\theta_0 + \theta_1 x_{i1}^2 + \theta_2 x_{i2}^2}\varepsilon_i, i = 1, \ldots, n$$
, where $\theta_0 &gt; 0, \theta_1 \geq 0, \theta_2 \geq 0$? If yes, how should I specify my own square-root variance functional form in <code>gls</code>? If
no, are there any other available R packages to implement IRLS algorithm?</p>
"
"0.0429097175767967","0.0574484989621426","203417","<p>Since my original question was to R-code-specific I'm trying to rewrite it:</p>

<p>I want to make a regression where my dependent variable <code>y</code> should follow a log-normal-distribution influenced by the explanatory variable <code>x</code> where the mean and variance changes across the observation.</p>

<p>Since log-normal doesn't belong to the exponential-family I can't try a glm.
Then I found gamlss which should exactly do the trick.</p>

<p>I was looking for some paper explaining the theory a little deeper - especially in my case of log-normal and where all the parameters of the distribution are  functions of the explanatory variables.</p>

<p>First of all I would like to know if there is a formula like the one below for an ordinary linear regression to calculate <code>y</code> after fitting:</p>

<p><a href=""http://i.stack.imgur.com/5Xqte.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Xqte.png"" alt=""enter image description here""></a></p>

<p>But my biggest problem is, that I have no idea on how to handle that the moments of <code>y</code> change across the observation of <code>x</code>.
So let's say I have the following:</p>

<p><a href=""http://i.stack.imgur.com/dv61k.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dv61k.png"" alt=""enter image description here""></a></p>

<p>I'm trying to achieve the regression via R and its <code>gamlss</code>-package.</p>

<p>There I start with <code>gamlss(y~x,familiy=LOGNO())</code> and then one has the possibility the make <code>sigma</code> depending on <code>x</code> via <code>sigma.formula=~</code> but no option for <code>mu</code>. So is it even possible in general?</p>

<p>Here is my code:</p>

<pre><code>library(gamlss)

y&lt;-c(1495418, 1684470, 1997120, 1901727, 2070008, 2213829, 2364602, 2333710, 2491570, 2540110, 2620947, 2761075, 2943475, 2854544)
x&lt;-c(3932300, 4119100, 4354400, 4483752, 4585303, 4803234, 4989701, 5177605, 5380031, 5494672, 5606376, 5783627, 6015992, 6171564)

fm&lt;-gamlss(y~x,familiy=LOGNO())
summary(fm)
fitted(fm)
residuals(fm)
y-fitted(fm) #How come this aren't the residuals?

fitted(fm,""mu"")
fitted(fm,""sigma"")
</code></pre>
"
"0.111567195944673","0.12520610071704","203454","<p>Although I have visited this site several times, this is the first time I make a question, so be kind if it is not in a appropriate form.</p>

<p>My problem is part statistical and part R. I am trying to build a Cox PH model in order to make prediction of unemployment. I have a big dataset, N=32538 with covariates p=37 . I split this sample in 3 parts according to Hastie &amp; Tibshirani, train =50%, test=25% and validation = 25%. So, I now have a training set of N=16270 cases. I would like to reduce the number of predictors, but from what I know and have read, it is not wise to do any kind of stepwise elimination. Therefore, I am trying to perform a penalized cox regression, especially with LASSO, using the R package 'penalized'. </p>

<pre><code>library( penalized )
</code></pre>

<p>However, it seems that it cannot run...I am not sure why, but I suppose that either my laptop is not very powerful, or that the respective functions are not very efficient for such a bog dataset. </p>

<pre><code>optL1( Surv ( time, status ) ~ . , minlambda=5, fold=3, data=mydata )
optL1( Surv ( time, status ) ~ . , minlambda=5,maxlamda=15, fold=3, data=mydata )
</code></pre>

<p>As you can see, I specify minlambda in the first case and both min and max lambda in the second. If I leave it unspecified, it just crushes my whole OS. Now, my pc runs veeeeeeery slow, and after 3 hours ( the most I left it running ), although it seems still running, nothing at all was produced. Those familiar with this function, know that while it is running, it produces in the console ""what is going on"". That is , for every lambda that it checks, it shows it in the consore along with the according cvl( log-lik ). </p>

<p>In some cases, but not always,  it produces the well-known irritating message of memory error....</p>

<pre><code> Error: cannot allocate vector of size xxx Mb
</code></pre>

<p>My details :</p>

<pre><code>session(info)
R version 3.2.4 Revised  ( 2016-03-16 r70336)
Platform: x86_64-w64-mingw32/x64 ( 64 bit)
Running under: Windows &gt;= 8 x64 ( build 9200)
</code></pre>

<p>For now, I tried to run the function in subsets of the full dataset, and I ""managed"""" to make it run until  N=8000( the half sample).</p>

<p><strong>Question</strong> 1:</p>

<p>Do you now if I am doing something wrong in running the specific function, or it is an unsolved problem and I have to find another way to proceed ?  </p>

<p><strong>Question 2</strong></p>

<p>Do you know if there are any other packages in R, that can accommodate more efficient the penalized cox regression, and also be capable of making predictions ?</p>

<p>Many thanks!!
Giannis</p>

<p><strong>EDIT</strong></p>

<p>actually, as you can see, I used the classic formula for regression. Meaning, I used the 'dot' in order to include all the predictors in the model. Moreover, as you maybe have guessed, I have many categorical predictors. Do you think that it is better to add the variables all by name in the model, and specify the factors with :</p>

<pre><code>factor(var1) 
</code></pre>

<p>????</p>

<p>Because by reading the vignette( penalized) , I realized that they use only continuous predictors, leaving out of the model the categorical ones! </p>
"
"0.145965139117999","0.146566068538932","203785","<p>I'm using the <code>tgp</code> package in R for fully Bayesian Gaussian Process Regression, and it's great! I'm currently performing regression for experimental data coming from turbomachinery testing, and I'm using the <code>bgp</code> function. This function uses a GP prior with either a <code>linear</code> mean or a <code>constant</code> mean (respectively, option <code>meanfn=""linear""</code> or <code>meanfn=""constant""</code>, which is the default). Note that <code>tgp</code> allows the use of treed Gaussian priors, but for now I'm staying simple, so I'm using the <code>bgp</code> function which doesn't use regression trees, just ordinary Gaussian Processes.</p>

<p>I would like my posterior predictive mean to go to zero away from the training set data, for physical reasons. How can I impose that? I was thinking to set the prior over $\beta_0$ to a Normal distribution centered at 0 and with an extremely small variance, but I'm not sure how to do that. From <code>help(btgp)</code></p>

<pre><code>bprior Linear (beta) prior, default is ""bflat""; alternates include ""b0"" hierarchical Normal
prior, ""bmle"" empirical Bayes Normal prior, ""b0not"" Bayesian treed LMstyle
prior from Chipman et al. (same as ""b0"" but without tau2), ""bmzt"" a independent
Normal prior (mean zero) with inverse-gamma variance (tau2), and
""bmznot"" is the same as ""bmznot"" without tau2. The default ""bflat"" gives
an â€œimproperâ€ prior which can perform badly when the signal-to-noise ratio is
low. In these cases the â€œproperâ€ hierarchical specification ""b0"" or independent
""bmzt"" or ""bmznot"" priors may perform better
</code></pre>

<p>Default is the improper prior <code>""bflat""</code>, which is not what I want. If I use the <code>""b0""</code> hierarchical Normal prior, I guess I cannot set the mean and the variance because they should become additional hyperparameters to be determined in the Bayesian paradigm. Thus, I may go for <code>""bmzt""</code>, the independent Normal prior with zero mean. However, with this prior I cannot set the variance, which is again an hyperparameter. Basically, I want my prior mean function to be zero, so that away from the data, also the posterior predictive mean will be zero. Is there a way to achieve that?</p>

<p>EDIT: nobody wants to have a try? :) As my actual case is quite complicated, I wrote a small test case which illustrates the main problem, with the help of the <code>tgp</code> package author. NOTE: unless you have an optimized version of R, you may want to set <code>BTE = c(1000,10000,2)</code> in the call to <code>bgp</code>, or you may have to wait for a very long time to get an answer.</p>

<pre><code># clear the workspace
rm(list=ls())
gc()
graphics.off()

# set seed for reproducibility
set.seed(825)

# load required packages
library(tgp)
library(ggplot2)

# simulated data
x &lt;- seq(-1,1,len=100)
eps &lt;- rnorm(n=100,mean=0,sd=0.5)
y &lt;- -5*x^2+eps
ymean &lt;- mean(y)

# prediction points
xpred &lt;- seq(-20,20,len=100)

# fit GP
GPModel &lt;- bgp(X=x,Z=y,XX=xpred,meanfn = ""constant"", bprior=""bmzt"", 
BTE = c(2000,52000,2), tau2.p=c(1,10000), tau2.lam=""fixed"")    
ypred &lt;- GPModel$ZZ.mean 

# plots
ymean_vector &lt;- rep(ymean,100)
df &lt;- data.frame(x,y,xpred,ypred,ymean_vector)
p &lt;- ggplot(data=df)
p &lt;- p + geom_point(aes(x=x,y=y)) + 
    geom_line(aes(x=xpred,y=ypred),col=""blue"") +
    geom_line(aes(x=xpred,y=ymean_vector),col=""red"") +
    geom_line(aes(x=xpred,y=GPModel$ZZ.q1), col=""green"") + 
        geom_line(aes(x=xpred,y=GPModel$ZZ.q2), col=""green"")
p
</code></pre>

<p>The resulting plot is</p>

<p><a href=""http://i.stack.imgur.com/VjePX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VjePX.png"" alt=""enter image description here""></a></p>

<p>The mean response is the red line: the blue line is the GP posterior predictive mean, and the green lines give the 90% credible interval.Thus, outside the training data range, the data mean is indeed included in the 90% credible interval, but I would like the predictive mean to converge to it...I think that if I could find a way to set the standard deviation of the prior for $\beta_0$ to some  extremely small value, I would achieve what I want, but I don't know how to do it.</p>

<p>EDIT2: I can use either a multiplicative (separable) squared exponential kernel
or an additive squared exponential kernel.</p>

<pre><code>sep_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    prod(sigma*exp(-0.5*(abs(x-y)/l)^2))
}    

add_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    sum(sigma*exp(-0.5*(abs(x-y)/l)^2))/length(x)
} 
</code></pre>
"
"0.0572129567690623","0.0574484989621426","204057","<p>I have a very large number of R-squared statistics to collect, basically I'm collecting a monthly R-squared series for a period of around 25 years for a large number of variables where I'm interested in how well each of the variables is explained by the mean of the others in the bunch. There are thousands of variables under examination.</p>

<p>I am currently simply doing it like this:</p>

<pre><code>fit &lt;- lm(df[,i] ~ me, na.action=na.omit);
rsqPeriod &lt;- summary(fit)$r.squared;
</code></pre>

<p>where <code>df[,i]</code> is the individual one-month series and <code>me</code> is the respective series of means of the others. But as there are thousands of variables and a few hundred months, this takes a very long time to calculate. In total, we're talking of several days on a high-end Xeon. </p>

<p>Now the question is, is there a considerably more efficient way of calculating the R-squared values? I do not need any other results from the regressions, and I am wondering if there is e.g. very much unnecessary calculation in the summary function, since I don't need any of the other values.</p>
"
"0.0495478739876288","0.0497518595104995","204678","<p>I have created a random walk model ARIMA(0,1,0) in R. The coefficients and R output is as shown below:</p>

<pre><code>arima(x = Y, order = c(0, 1, 0), xreg = Indp_varbl)
Coefficients:
          t2       t3
      9.1993  18.0351
s.e.  0.4921   7.7715
</code></pre>

<p>I wanted to ask how I can forecast points using these coefficients through an equation? I have gone through papers but was not able to find exact equation using regressors as well:</p>

<pre><code>Yt = Yt-1 + mean error 
</code></pre>

<p>is generally used. How can I get the equation for the same?</p>
"
"0.0756856276908142","0.0542837290884934","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.0904616275314925","0.090834052439095","205123","<p>To idetifying the important activity performed from users who have been converted in last N days. So, I have tried GLM, Rpart and Random forest models which can give me the impoprtant activities (in terms of Data Sciece its highly significant variables). Now If I want to extract the influencer counts for each important activity. i.e. count 5 for viewed_product activity means every user who converts into customer performs five product views. </p>

<p>I have tried my GLM with response variable as IsConverted and rest of the variables are frequencies of all activity performed at user level.  </p>

<pre><code>Call: 
glm(formula = regression_input2$IsConverted ~ ., family = binomial(), 
    data = regression_input2)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2285  -0.8820  -0.8245   1.3572   1.6479  

Coefficients: (1 not defined because of singularities)
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -0.904264   0.068017 -13.295  &lt; 2e-16 ***
view_product         0.029021   0.007867   3.689 0.000225 ***
view_collection     -0.034973   0.054757  -0.639 0.523018    
view_brand           0.047889   0.020289   2.360 0.018258 *  
search_category     -0.028920   0.032899  -0.879 0.379384    
search               0.172942   0.053855   3.211 0.001322 ** 
remov_product       -0.178905   0.151888  -1.178 0.238845    
payment                0.321474   1.054034   0.305 0.760371    
like_product          0.047789   0.035914   1.331 0.183305        
checkout_unsuccessful        NA         NA      NA       NA    
checkout_successful    0.397584   0.973795   0.408 0.683066    
added_product          0.179261   0.097749   1.834 0.066671 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2174.3  on 1668  degrees of freedom
Residual deviance: 2086.3  on 1657  degrees of freedom
AIC: 2110.3

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apart from these, I have also tried Random forest and Rpart models. 
Here is the output of variable importance of Random forest.</p>

<pre><code>                      IncNodePurity
view_product           51.6447716
view_collec            16.9695232
view_brands            31.8345159
search_category        20.6999952
search                 18.0962766
remov_product_cart     6.6511766
payment                2.2859159
like_product          14.4360793
checkout_unsuccessful  0.2139582
checkout_successful    2.6284091
added_product          14.7047717
</code></pre>

<p>So, with the above utilities how can I get that counts of influencer activity which affects user's convergence. </p>
"
"NaN","NaN","205227","<p>I'm a little new to R and I haven't done stats in a while. I know a one way ANOVA is the same as a linear regression, but is there a difference between a two way ANOVA and a linear regression with two covariates? And if they are different I'm not sure which one I performed. Below is my sample code:</p>

<pre><code>data.frame[[""Acute""]] = factor(data.frame[[""Acute""]])
data.frame[[""Frequency""]] = factor(data.frame[[""Frequency""]])
DishMortalityVsTime.Total.Acute.Freq = aov(Dish.Mortality ~ Time * Acute * Frequency, data=data.frame)
summary(DishMortalityVsTime.Total.Acute.Freq)
</code></pre>

<p>and the output</p>

<pre><code>                      Df Sum Sq Mean Sq F value               Pr(&gt;F)    
Days                   1  1.352  1.3524  65.189  0.00000000000000429 ***
Acute                  2  5.885  2.9423 141.822 &lt; 0.0000000000000002 ***
Frequency              3  0.539  0.1795   8.653  0.00001279126504853 ***
Days:Acute             2  1.672  0.8361  40.302 &lt; 0.0000000000000002 ***
Days:Frequency         3  0.050  0.0165   0.796                0.496    
Acute:Frequency        6  0.787  0.1311   6.320  0.00000192315201011 ***
Days:Acute:Frequency   6  0.038  0.0064   0.309                0.932    
Residuals            552 11.452  0.0207 
</code></pre>

<p>Any help would be appreciated, Thanks!</p>
"
"0.06396603026469","0.0642293744423385","205817","<p>Thanks in advance for someone who can help.</p>

<p>I realize that, in regression analysis, if there is an interaction term included,  it is recommended to center the variables  (subtracting variable's mean).</p>

<p>My query is, should we </p>

<p>1) centering variable first, then create the interaction term, </p>

<p>or </p>

<p>2ï¼‰create the interaction term first, then centering all variables including the interaction termï¼Ÿ</p>

<p>these two operations can give different resultï¼Œ I am wondering which one is correct.</p>

<p>For example, we have 2 independent variables v1 &amp; v2, and an interaction term v1 * v2,</p>

<p>let:
v1 &lt;- c(3,   5,   4,  2,    9)
v2 &lt;- c(1,   0,   1,  1,    0)</p>

<p>when creating the interaction term, I am confusing which one is correct: </p>

<p>method 1</p>

<blockquote>
  <p>v1*v2 - mean(v1*v2)<br/> 
  [1]  1.2 -1.8  2.2  0.2 -1.8</p>
</blockquote>

<p>or </p>

<p>method 2</p>

<blockquote>
  <p>(v1-mean(v1)) * (v2-mean(v2)) <br/> 
  [1] -0.64 -0.24 -0.24 -1.04 -2.64  <br/> </p>
</blockquote>
"
"0.0862517776135472","0.0866068708302494","206119","<p>I have two sets of data from two groups of participants that are identical in all but one way: one set of data were collected using software A and the other set was collected with software B. It's possible that the outcome variables are roughly equivalent across groups, and it's possible that they're not. (The groups are quite similar otherwise; I've run chi-square tests on gender and race and there are no differences by the latter and only minor differences by the former. Regardless, such differences should not be systematic.)</p>

<p>What I want to do is run some preliminary tests to see whether there are significant effects of software engine on my three primary outcome variables. If there are, I'd then treat these as two separate experiments. If there are not, I'd instead group them together. </p>

<p>What I had in mind was to use a straightforward linear regression, as so (I'm using R, but obviously this is not specific to the stats program):</p>

<pre><code>combined &lt;- rbind(data1, data2)
reg1 &lt;- lm(outcome1 ~ engine, data=combined)
summary(reg1)
# and repeat for outcome2 and outcome3
</code></pre>

<p>If this regression shows a significant effect of engine for any of these outcome variables, then that would mean that I should keep the groups separate. It also occurred to me to do this with other covariates in the model: </p>

<pre><code>combined &lt;- rbind(data1, data2)
reg1 &lt;- lm(outcome1 ~ engine + age + gender + race, data=combined)
summary(reg1)
# and repeat for outcome2 and outcome3
</code></pre>

<p>In this case, I'd be seeing if engine had a significant effect <em>in the presence of other relevant variables</em>. </p>

<p>Any thoughts on which method makes more sense, or if there's a better way?</p>
"
"0.06396603026469","0.0642293744423385","206454","<p>I wish to compute <code>MSE</code> of my models.  Say my data was generated from the following model:</p>

<p>$y_i=f(x_i)+e_i$ </p>

<p>where $e_i$ is some noise around the true relationship $f(x)$.  I estimated the function $f(x)$ as $f\hat(x)$, and now I'd like to compute the MSE.  </p>

<p>My professor often writes MSE as the following:</p>

<p>$1/n \sum_{i=1}^n (f(x_i)-f\hat(x_i))^2$</p>

<p>Let's say I know $f(x)$, the true function and I'm using it for simulation.</p>

<p>My question is, when I compute MSE, do I use my observations $y_i$?  or do I use the true function without the noise $f(x_i)$?  Because, the professor writes the true function in the formula above, but this means that computing MSE involves taking the difference between the functions at the $x_i$ value of each observation, without actually using the value $y_i$ of that observation?</p>

<p>This formulation seems much more intuitive to me:  </p>

<p>$1/n \sum_{i=1}^n (y_i-f\hat(x_i))^2$  , because this will actually capture the observations.</p>

<p>Which formulation is correct?  And when might one use one over the other?  Feel free to use linear regression as an example, since that will allow easy illustration.  </p>
"
"0.0756856276908142","0.0651404749061921","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.0495478739876288","0.0497518595104995","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.06396603026469","0.0642293744423385","208765","<p>I know its possible to extract r-squared values to quantify the 'goodness-of-fit' of regressions in R, with something to the effect of:</p>

<pre><code>fit &lt;- lm(y ~ x1 + x2 + x3, data=mydata)  # Not actual data
r-sq &lt;- summary(fit)$r.squared # or $adj.r.squared
</code></pre>

<p>I've recently been using the <code>cumSeg</code> package for step-function regressions, but it doesn't appear to offer this functionality, though it does provide residuals as a vector.</p>

<p>Is there some way to extract an r-squared (or adj. r squared) that I don't know about? Or can it be calculated 'de novo' with something that <code>cumSeg</code> does actually provide?</p>

<p><strong>EDIT</strong>
This is the output of <code>summary()</code> for my stepfunction created via <code>cumSeg</code>. Perhaps someone more mathematically versed with stepfunctions knows if the  nomenclature for an r-squared (or whatever the equivalent is) is just different and the data I'm looking for is actually there (or if it is even a legitimate question to ask for an R-squared for stepfunctions?! I'm assuming it should be calculable from any fitted model really.</p>

<pre><code>&gt; summary(stepfunc)
              Length Class  Mode   
coefficients   3     -none- numeric
residuals     16     -none- numeric
effects       16     -none- numeric
rank           1     -none- numeric
fitted.values 16     -none- numeric
assign         0     -none- NULL   
qr             5     qr     list   
df.residual    1     -none- numeric
epsilon        1     -none- numeric
it             1     -none- numeric
psi            1     -none- numeric
beta.c         1     -none- numeric
gamma.c        1     -none- numeric 
V             16     -none- numeric
y             16     -none- numeric
id.group      16     -none- numeric
est.means      2     -none- numeric
n.psi          1     -none- numeric
</code></pre>
"
"0.0990957479752576","0.0995037190209989","209412","<p>I'm trying to build a bivariate copula-based model of income and wealth in Italy and I'm having trouble handling weighted data. I have access to micro data, a survey of about 10,000 households that includes the corresponding sample weights.</p>

<p>When calculating basic statistics (like mean and median) and even when performing linear regressions it is pretty easy to account for weights, besides there are useful packages for that (e. g. survey). But what do I do when I want to fit a parametric model of the distribution to weighted data? Or to estimate its kernel density?</p>

<p>I have a few ideas, but they seem to be pretty crude. For one, I could inflate my sample to the size of the universe. That is, I could multiply all weights by 100 (which would turn them into integers) and then create a vector that repeats each value of income and wealth a given number of times. But that would lead to a very large sample (which I believe still wouldn't be a perfect representation of the population) and will certainly put some extra strain on my computer.</p>

<p>I could also just round the weights off instead of multiplying them by 100, but this would still make the sample noticeably bigger and will inevitably skew the real proportions.</p>

<p>Another approach I came up with would be to normalize the weights (so that they sum up to one) and then randomly sample with repetitions from my initial sample with the corresponding vector of probability weights. R doesn't allow to draw the samples that are larger in size than the one that they are being drawn from. But I think that drawing the sample of the same size as the initial one will lead to some loss of information about the observed proportions. So I could draw the samples of the initial size as described above several times (how would I know how many is though?) and then combine them into one sample. And again, I will have a larger sample with some of the information lost along the way.</p>

<p>So I was wondering if there is a better way to handle weighted data. In some cases I think I could technically introduce the weights into the formula for computing the maximum likelihood for fitting a particular model, although I certainly wouldn't like to code that from the ground up. I will have to fit a lot of models as part of my project, both univariate (e. g. Singh-Mandala) for income and wealth and bivariate for copulas. I don't think the built in functions in any of the copula-related packages that I'm aware of allow one to account for weights. So any advice would help!</p>
"
"0.12136700910941","0.121866669555358","209864","<p>This is a very simple exercise that I'm hoping may help people with limited knowledge in statistical analysis (like myself). 
I am having trouble deciding what statistical analysis I can perform (in R) to determine whether or not my data are closer to one linear model or another. </p>

<p>For example: I have measurements of sodium and chloride in various dilute solutions: </p>

<pre><code>#
Na &lt;- c(1.56, 1.00, 1.60, 3.23, 2.02, 2.81, 2.09, 26.24, 1.59, 0.42)
Cl &lt;- c(1.40, 0.91, 1.22, 2.67, 1.67, 3.01, 2.17, 27.42, 1.45, 0.51)
</code></pre>

<p>For simplicity, this solution is a dilution of either table salt dissolved in water or natural seawater. For each case, Cl/Na will be a specific ratio that reflects the composition of the original solution. We can visualize this by:</p>

<pre><code>plot(Na,Cl)
abline(0,1)    # expected slope for table salt dissolved in water
abline(0,1.16) # expected slope for natural seawater.
</code></pre>

<p>I want to know which model, table salt in water or seawater, is a more statistically accurate fit to the provided data. Linear regression analysis in R gives a line of best fit with a slope of 1.05 (<code>lm(Cl~Na)</code>), right in between the two models.</p>

<p>So, which solution do I more likely have and why? The line of best fit slope is closer to that of table salt dissolved in water, but that does not seem very statistically sound. Thoughts? </p>

<p>Edit: @whuber mentioned that there is one anomaly in the dataset - in reality, the provided data is just a subset of the original data. There are actually hundreds of data points in between the apparent outlier and the rest of the provided data.</p>

<p>Also, here is a <code>log(Na)-log(Cl)</code> summary of the complete dataset:</p>

<pre><code>    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's 
-0.46870 -0.06186  0.02654  0.02218  0.12780  0.47510      183 
</code></pre>

<p>Edit2: As for the ""true nature of my investigation"": The 'solution' in question is likely a mixture of both table salt water and natural seawater. What I'd like to do is find a definitive way (through statistical analysis) to show that I have more of one or the other. I had hoped that my simplified question/dataset would yield an answer from the community, but it seems I was off base. If it helps, a complete dataset is now hosted below:</p>

<p><a href=""http://www.filedropper.com/clna"" rel=""nofollow"">http://www.filedropper.com/clna</a></p>

<p>Looking at the distribution of the complete data shows I have more Cl/Na about 1.00, but this does not seem 'sound enough' to back up an argument. The probability that I have one solution or the other is unknown. I have the raw data and relevant models for Cl to Na to run with.</p>

<p>For clarification, the original question is still the one I'd like to solve.  An alternative question could be: Which solution do I have <em>more</em> of and what analysis did I use to come to that conclusion?</p>
"
"0.0404556697031367","0.0406222231851194","209950","<p>I am running a multinomial regression in R, and the p value of a lot of variables is 0. Just $0$, not $0.00000$ or anything. And we have 8 dependent variable choices, and the p value for a certain X variables is 0 for all of them. What does this mean? we also have a $0.00000e+00$ as the p value for a variable, does this mean the p value is 1?</p>
"
"0.0862517776135472","0.0866068708302494","209999","<p>Let's say I fit a regression model to standardized data and then use it to predict out-of-sample data. I evaluate model performance using mean squared error.</p>

<p>I compare this model to another model that simply selects a single predictor and 
calculates mean squared error as: <code>mean((DV - single predictor)^2)</code></p>

<p>The weird thing is that this single predictor model performs better than regression.</p>

<p>What am I missing?</p>

<pre><code>    #load data
    data(mtcars)

    #standardize data
    data &lt;- scale(mtcars)

    #matrix for storing results of a 100 replications
    MSE &lt;- matrix(0,ncol=2,nrow=100)

    #replicate procedure 100 times
    for(reps in 1:100){

    #divide data into training and test set
    train &lt;- sample(1:nrow(data),nrow(data)*0.5)
    test &lt;- setdiff(1:nrow(data),train)

    train.data &lt;- as.data.frame(data[train,])
    test.data &lt;- as.data.frame(data[test,])

    #fit linear regression and get predictions for test data
    fit &lt;- lm(train.data[,1]~.,data=train.data[,2:ncol(train.data)])
    preds &lt;- suppressWarnings(predict(fit,test.data))

    #calculate mean squared error for regression  
    MSE[reps,1] &lt;- mean((test.data[,1] - preds)^2)

    #calculate mean squared error for the single variable prediction
    MSE[reps,2] &lt;- mean((test.data[,1] - test.data[,5])^2)

    }

    #compare MSE for regression and single variable prediction.    
    colMeans(MSE)
    [1] 1.3268521 0.6020453
</code></pre>
"
"0.0286064783845312","0.0287242494810713","210012","<p>I have percentage areas values from 4 treatments (25 replicates per treatment). I would like to compare these percentages.</p>

<p>I am supposed to use a beta regression, because my response variable is a proportion (not resulting from a count). For other variables I use GLMM and then use the glht() function to get the significance of pairwise comparisons. This does not seem to work with my betareg model. <strong>Is there any way to do that ?</strong></p>

<p>Also I would like to include random effects because of nested sampling protocol. From this <a href=""http://stats.stackexchange.com/questions/167340/beta-regression-with-random-effect-of-source-plot-in-two-seasons"">answer</a> this does not seem very appropriate/easy to do this with beta regression. </p>

<p>Do you have any suggestion ?</p>

<hr>

<p>EDIT</p>

<hr>

<p>Thanks to @rvl's comment I could calculate pairwise comparisons and extract group letters.</p>

<pre><code>   library(lsmeans)
   library(betareg)       
   betalive = betareg(Live ~ Crop)
   live.rg &lt;- ref.grid(betalive)        
   live.lsm = lsmeans(live.rg, ""Crop"")
   cld(live.lsm, alpha =  0.05)$.group
   &gt; [1] "" 1 "" "" 1 "" "" 1 "" ""  2""
</code></pre>

<p>However I could not solve my problem of random effects yet. </p>
"
"0.0809113394062735","0.0710888905739589","210515","<p>Here are some sample data in R:</p>

<pre><code>set.seed(42)
df &lt;- data.frame(g = factor(rep(1:2, each= 50)), y = rnorm(100)+rep(0:1, each=50))
</code></pre>

<p>One can easily get group means using e.g. <code>with(df, tapply(y,g,mean))</code> but there is no such easy way to get the confidence intervals for group means. This is why I tried:</p>

<pre><code>lm(y ~ g-1, df)
# correct group means:
# -0.03567   1.10070 
confint(lm(y~g-1, df))
# too narrow CI's
#         2.5 %    97.5 %
# g1 -0.3287751 0.2574315
# g2  0.8075981 1.3938047
</code></pre>

<p>That is, one can estimate the group means using a linear model with dummy group indicators, omitting the overall intercept. But the confidence intervals of these regression parameters are narrower than the confidence intervals of group means. The latter could be found group by group with the same function:</p>

<pre><code>confint(lm(y~1, data=df, subset=g==1))
#                  2.5 %    97.5 %
# (Intercept) -0.3629177 0.2915742
</code></pre>

<p>Or a manual check using textbook formulae:</p>

<pre><code>ci.mean &lt;- function(x, alfa=0.05){
   n &lt;- length(x)
   a&lt;-qt(1-alfa/2, n-1)
   m&lt;-mean(x);          s&lt;-sd(x)
   se&lt;-s/sqrt(n)
   res &lt;- c(m, m-a*se,m+a*se)
   setNames(res, c(""mean"", paste(100*alfa/2, ""%""), paste(100*(1-alfa/2), ""%"")))
}
ci.mean(with(df, y[g==1])
#        mean       2.5 %      97.5 % 
# -0.03567178 -0.36291775  0.29157418 
</code></pre>

<p>There is probably an easy answer to the question of why the CIs of seemingly the same parameters are different. (The answer, obviously, has to start with difference in standard errors.) But I would be interested in the interpretation: why is that I can trust the group means found with <code>lm(y~g-1)</code> but I can't trust the confidence intervals around those ""means"" found with <code>confint(lm(y~g-1))</code>? And another naive question, why is the standard error for a group mean smaller if another group is present? That is:</p>

<pre><code>coef(summary(lm(y~g-1, df)))[1,2]
# [1] 0.1476987
coef(summary(lm(y~1, df, subset=g==1)))[1,2]
# [1] 0.1628433
</code></pre>

<p>Again, I am more interested in the substantial interpretation than  the formula showing why this is so. (I suppose after some sleep I could figure out the formula but would still be in trouble with interpretation).</p>

<p>Thanks in advance!</p>
"
"NaN","NaN","210651","<p>I would like to use the OOB cases from a random forest fit to estimate the mean squared prediction error so I don't have to cross-validate. I am using the randomForest package in R. It is clear from the documentation that OOB error is reported for classification, but I can't figure out how to get OOB MSPE for regression. Am I missing it or is it truly not reported, which seems odd?</p>
"
"0.0404556697031367","0.0203111115925597","211786","<p>My intuitive understanding is that if $x$ and $z$ are categorical factors, then each observation $y_i$ is given a mean value which is equal to the mean value given to $y_j$ if $y_i$ and $y_j$ belong to the same $(x,z)$ product group. </p>

<p>However, when I run the command in R and look at the model matrix, the parametrisation is clearly not the same. R seems to give each observation a mean value equal to a sum of a bunch of parameters. Why? What does it mean? </p>

<p>My only clue right now is that R maybe considers $x$ and $z$ to be numerical predictors, and thus the parametrisation is a regression?</p>
"
"0.0990957479752576","0.0995037190209989","211951","<p>Out of curiosity, I conducted the following simulation (code below). Why is it that when the variance of the error term is large coefficient associated with the intercept is biased? Can you recommend some reference that discusses this? Or better yet is there a formal proof of that?</p>

<pre><code>rm(list=ls())
set.seed(12345)
m  &lt;- 10000
x1 &lt;- runif(m,0,100)       # random numbers from uniform distribution
x2 &lt;- 1:10000
u  &lt;- rnorm(m,0,100)       # random numbers from standard normal distribution
y  &lt;- 5*x1 + 2*x2 + 10 + u # generating y series
data  &lt;- cbind(x1, x2, y)
beta1 &lt;- c()
beta2 &lt;- c()
beta3 &lt;- c()
R2    &lt;- c()
n       &lt;- 1000 # number of loops
ksubset &lt;- 100  # length of subset
for (i in 1:n){
  datam &lt;- data.frame(data[sample(nrow(data),ksubset), ])
  ols   &lt;- summary(lm(y~x1+x2, data=datam))
  beta1 &lt;- append(beta1, ols$coefficients[1])
  beta2 &lt;- append(beta2, ols$coefficients[2])
  beta3 &lt;- append(beta3, ols$coefficients[3])
  R2    &lt;- append(R2,    ols$r.squared)
}

results &lt;- c(mean(beta1), mean(beta2), mean(beta3))
results
## [1] 7.290909 5.027431 2.000345
</code></pre>

<p>The code takes thousand random samples of size 100 from the population and calculates a regression in each. Then I take the average of each of the estimated coefficients, which in theory should be equal to the original model. It works great for the slopes, but not for the intercept.</p>

<p><strong>Update</strong></p>

<p>I have noticed that with the population set at 10,000, the bias persists. If I increase the size of the population to 1,000,000, the bias disappears. The size of the target population was insufficient. Anyway, @Maarten's answer is a step in the right direction.</p>

<p><strong>Update2</strong></p>

<p>Entire population results:</p>

<pre><code>&gt; summary(lm(y~x1+x2))       
Call:
lm(formula = y ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-367.24  -66.37   -0.33   66.64  385.26 

Coefficients:
             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) 7.0388165  2.6626801    2.644  0.00822 ** 
x1          5.0296554  0.0348480  144.331  &lt; 2e-16 ***
x2          2.0002959  0.0003465 5772.764  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 100 on 9997 degrees of freedom
Multiple R-squared:  0.9997,  Adjusted R-squared:  0.9997 
F-statistic: 1.667e+07 on 2 and 9997 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I updated the question, including simulation results (result). I also present the model results with the entire population, which in effect is biased due to the huge variance. The simulation, as expected, reproduce those results correctly. I repeat, my mistake is in the size of the target population.</p>
"
"0.11794753195637","0.111466460825459","212026","<p>Lets say I have a multiple regression model, where I predict <code>y</code> from predictors <code>x1</code> and <code>x2</code>. Here are some example data and code for R.</p>

<pre><code>#generate data
x1 &lt;- seq(1,10,by=0.1)
x2 &lt;- rnorm(91, 100, sd=4)
y &lt;- x1*0.3 + x2 * 0.1
#add noise.
y&lt;- y + rnorm(length(y))

#model and summary
model &lt;- lm(y~x1 + x2)
summary(model)
Call:
lm(formula = y ~ x1 + x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.32345 -0.67679 -0.01036  0.77334  2.60149 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.15715    2.73185   1.522   0.1317    
x1           0.23871    0.03996   5.973 4.83e-08 ***
x2           0.06152    0.02721   2.261   0.0262 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.001 on 88 degrees of freedom
Multiple R-squared:  0.3214,    Adjusted R-squared:  0.306 
F-statistic: 20.84 on 2 and 88 DF,  p-value: 3.888e-08
</code></pre>

<p>Now, I want to plot how each predictor, <code>x1</code> and <code>x2</code>, affects <code>y</code>, over the range of <code>x1</code> and <code>x2</code> in the data set. To do this, I first extract model coefficients, and generate ranges and means of each predictor. I then calculate the values of <code>y</code> over the range of each predictor, holding the other predictor constant at its mean. For example:</p>

<pre><code>#grab model coefficients
p&lt;-coef(model)

#calculate effect sizes over the range of each predictor, at the mean of all other values
m.x1 &lt;- mean(x1)
m.x2 &lt;- mean(x2)

#get range of each predictor, subdivide into 100 increments for plotting.
range.x1 &lt;- seq(min(x1),max(x1), by = (max(x1) - min(x1)) / 99)
range.x2 &lt;- seq(min(x2),max(x2), by = (max(x2) - min(x2)) / 99)

effect.x1 &lt;- p[1] + p[2]*range.x1 + p[3]*m.x2
effect.x2 &lt;- p[1] + p[2]*m.x1     + p[3]*range.x2 

#plot effects over range
plot(effect.x1, lwd=0, ylim=c(10,13))
lines(smooth.spline(effect.x1), lwd = 2)
lines(smooth.spline(effect.x2), lwd = 2, lty = 2)
legend(5,13, legend = c('x1','x2'), lty = c(1,2),lwd=2, box.lwd=0)
</code></pre>

<p>Here is the plot:</p>

<p><a href=""http://i.stack.imgur.com/trncW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/trncW.png"" alt=""enter image description here""></a></p>

<p>My questions:</p>

<ol>
<li><p>I would like to shade a region of uncertainty around each of these effect lines. My first intuition would be to use the standard error estimate for each predictor from the multiple regression output. However, Each effect estimate also includes uncertainty associated with the estimate of the intercept and the estimate of the other predictor in the model. How can I incorporate all of these sources of uncertainty to generate uncertainty bounds for each of my effect estimates?</p></li>
<li><p>Presuming there is a good answer to question 1, how would I best go about implementing this in R? (I realize this question may be better posed on stackoverflow).</p></li>
</ol>
"
"0.0700712753800578","0.0703597544730292","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.0858194351535935","0.0765979986161901","212355","<p>I am using beta regression to model fulfillment ratio of a store (orders delivered/orders placed), which is between 0 and 1. I am using betareg package in RStudio to run this analysis. I have 24 predictors (many of these correlated with each other) with which i am trying to build the model. For certain variables i am getting this error:</p>

<pre><code>    Error in chol.default(K) : 
      the leading minor of order 18 is not positive definite
    In addition: Warning message:
    In sqrt(wpp) : NaNs produced
    Error in chol.default(K) : 
      the leading minor of order 18 is not positive definite
    In addition: Warning messages:
1: In betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control) :
  failed to invert the information matrix: iteration stopped prematurely
2: In sqrt(wpp) : NaNs produced
</code></pre>

<p>For a few other variables i am getting this error:</p>

<pre><code>Warning message:
In betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control) :
  optimization failed to converge
</code></pre>

<p>I do not what these errors mean and how to fix them. There are no missing data points in the dataset. Will be glad if someone could help me out here. Thanks.</p>

<hr>

<p>Thank you for letting me know my info is insufficient. Approximately i have 3,900 observations.</p>

<p>I am posting the summary of my dataset since i cannot post the actual data. Let me know if this provides insight into the data i am dealing with:</p>

<pre><code>   chain_id       timediff        ratio_store     fulfilled_store 
 0      :3781   Min.   :  0.000   Min.   :0.0000   Min.   :   0.0  
 11     :  37   1st Qu.:  0.000   1st Qu.:0.4138   1st Qu.:   8.0  
 3      :  25   Median :  7.771   Median :0.7349   Median :  39.0  
 13     :  18   Mean   : 31.179   Mean   :0.6331   Mean   : 134.7  
 2      :   9   3rd Qu.: 47.771   3rd Qu.:0.8889   3rd Qu.:  97.0  
 12     :   8   Max.   :153.771   Max.   :1.0000   Max.   :1294.0  
 (Other):  19                                                      
 pricecount_monthly store_pricecount  ratio_monthly    fulfilled_monthly
 Min.   :   0       Min.   :    2.0   Min.   :0.0000   Min.   :  0.00   
 1st Qu.:  16       1st Qu.:  199.0   1st Qu.:0.4000   1st Qu.:  2.00   
 Median :  79       Median :  479.0   Median :0.7727   Median : 11.00   
 Mean   : 193       Mean   :  829.7   Mean   :0.6347   Mean   : 36.53   
 3rd Qu.: 223       3rd Qu.: 1095.0   3rd Qu.:0.9130   3rd Qu.: 37.00   
 Max.   :3654       Max.   :13561.0   Max.   :1.0000   Max.   :379.00   

 pricecount_sp     allpricecount_product monthlypricecount_product
 Min.   :  1.000   Min.   :    1.0       Min.   :   0.0           
 1st Qu.:  1.000   1st Qu.:   26.0       1st Qu.:   4.0           
 Median :  2.000   Median :   80.0       Median :  15.0           
 Mean   :  4.027   Mean   :  533.2       Mean   : 115.9           
 3rd Qu.:  5.000   3rd Qu.:  375.0       3rd Qu.:  86.0           
 Max.   :116.000   Max.   :28907.0       Max.   :1412.0           

 ratio_product    total_fulfilledproduct monthlyratio_product
 Min.   :0.0000   Min.   :   0.00        Min.   :0.0000      
 1st Qu.:0.4167   1st Qu.:   2.00        1st Qu.:0.4000      
 Median :0.6667   Median :   6.00        Median :0.7692      
 Mean   :0.6130   Mean   :  70.67        Mean   :0.6442      
 3rd Qu.:0.8621   3rd Qu.:  40.00        3rd Qu.:1.0000      
 Max.   :1.0000   Max.   :1245.00        Max.   :1.0000      

 monthly_fulfilledproduct monthlyratio_sp  monthpricecount_sp
 Min.   :  0.00           Min.   :0.0000   Min.   : 0.000    
 1st Qu.:  1.00           1st Qu.:0.0000   1st Qu.: 0.000    
 Median :  2.00           Median :1.0000   Median : 1.000    
 Mean   : 23.55           Mean   :0.6174   Mean   : 1.374    
 3rd Qu.:  9.00           3rd Qu.:1.0000   3rd Qu.: 2.000    
 Max.   :414.00           Max.   :1.0000   Max.   :32.000    

 monthlystorecount_product storecount_product productcount_store
 Min.   :  0.00            Min.   :  1.0      Min.   :   1.0    
 1st Qu.:  3.00            1st Qu.: 13.0      1st Qu.: 111.0    
 Median :  8.00            Median : 37.0      Median : 213.0    
 Mean   : 45.92            Mean   :115.8      Mean   : 324.6    
 3rd Qu.: 37.00            3rd Qu.:131.0      3rd Qu.: 436.0    
 Max.   :330.00            Max.   :870.0      Max.   :2084.0    

 monthlyproductcount_store leafordersplaced_sp  leafratio_sp   
 Min.   :   0.00           Min.   :  1.00      Min.   :0.0000  
 1st Qu.:  12.00           1st Qu.:  3.00      1st Qu.:0.3571  
 Median :  50.00           Median : 11.00      Median :0.7692  
 Mean   :  96.38           Mean   : 46.99      Mean   :0.6345  
 3rd Qu.: 125.00           3rd Qu.: 39.00      3rd Qu.:0.9524  
 Max.   :1148.00           Max.   :620.00      Max.   :1.0000  

 leafmonthlycount_sp leafmonthlyfulfilled_sp averageprice_sp   
 Min.   :  1.00      Min.   :  0.00          Min.   :       5  
 1st Qu.:  2.00      1st Qu.:  1.00          1st Qu.:    1119  
 Median :  4.00      Median :  2.00          Median :    2253  
 Mean   : 14.89      Mean   : 12.31          Mean   :   10187  
 3rd Qu.: 13.00      3rd Qu.:  9.00          3rd Qu.:    8141  
 Max.   :268.00      Max.   :211.00          Max.   :14280251  

   dependent        
 Min.   :0.0001282  
 1st Qu.:0.0001282  
 Median :0.9998718  
 Mean   :0.6173614  
 3rd Qu.:0.9998718  
 Max.   :0.9998718  
</code></pre>
"
"0.0572129567690623","0.0574484989621426","212446","<p>I'd like to use elastic net regression for coefficient estimate and parameter selection on a data set that includes nested structure. I've been experimenting with lassop{MMS} to do so. I'm not a statistician by training, and I'm having a difficult time deciphering how to translate the example provided with the documentation to a real-data context.</p>

<pre><code>    require(lme4)
    require(lmerTest)
    require(MMS)
    data(grouseticks)
    ?grouseticks # sample data w/ multiple grouping levels
    n&lt;-length(grouseticks$TICKS)
#two dummy variables for additional fixed effects that we'll assume will be selected out
    dv1&lt;-rnorm(n, mean = 0, sd = 1)
    dv2&lt;-rnorm(n, mean=3, sd=2)
#sample saturated ME model, two terms for random intercept. I'm trying to write this in lassop syntax. 
sat_lmm&lt;- lmer(TICKS~YEAR+HEIGHT+YEAR+dv1+dv2+HEIGHT:dv1+(1|BROOD)+(1|LOCATION), data=grouseticks, REML=FALSE)
summary(sat_lmm)
</code></pre>

<p>How would set up the random effects and grouping matrices to mimic the above model formulation? Feel free to rip into this, I know my grouping and random effects matrices are desperately wrong.</p>

<pre><code>x&lt;-getME(sat_lmm,name = c( ""X""))
x&lt;-x[,c(""(Intercept)"" , ""HEIGHT"",""dv1"" ,""HEIGHT:dv1"",  ""dv2""  , ""YEAR96"" , ""YEAR97"")]
#rearrange variables so that first 3 collumns will be frozen in
y&lt;-as.numeric(getME(sat_lmm,name = c( ""y"")))

# this was my naive guess at handling  random effects
zlx&lt;-cbind( factor(grouseticks$BROOD, labels=seq(length(unique(grouseticks$BROOD)))),
            factor(grouseticks$LOCATION, labels=seq(length(unique(grouseticks$LOCATION)))))

#dummy grouping variable
gx&lt;-rbind(rep(1, length(n)) , 
          rep(1, length(n)))

require(glmnet)
lam&lt;-cv.glmnet(x, y, alpha=0.8, standardize=TRUE)
plot(lam)
#value of lambda that gives minimum cross-validation error
lammin&lt;-lam$lambda.min
lamlse&lt;-lam$lambda.lse
melasso.minlam&lt;-lassop(data=x,
                       Y=y,
                       z=zlx, 
                       mu=lammin,
                       fix=3,
                       D=TRUE,
                       alpha=0.8,
                       showit=F)
#as this stands, it won't run.
print(melasso.minlam)
</code></pre>
"
"0.0700712753800578","0.0703597544730292","213117","<p>My question concerns the calculation of sampling variance for studies using meta-regression or systematic review methods. I am using raw mean differences from different surveys conducted in the same country in the same year, the measurement scales are the same, so i am using raw mean differences. To see the variation in these survey, I intend to use meta data from the source surveys. </p>

<p>However, Running the model in R to see the confidence interval of the model coeffs shows me the following message <strong><em>""   Cannot compute confidence interval for the amount of (residual) heterogeneity with non-positive sampling variances in the data.""</em></strong></p>

<p>How can I find the confidence interval for the model coefficients to know which one to use as a moderator ? </p>
"
"0.0700712753800578","0.0703597544730292","213253","<p>In general, my question is how to estimate some prediction intervals in the case of penalized linear models (in particular, I think about the glmnet R package). I understood that the introduction of a penalization in the objective function generates a shrinkage effect, which is a bias on the estimated coefficients. 
I understand that in this case the calculation of the uncertainties is troublesome</p>

<p><a href=""https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf"" rel=""nofollow"">https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf</a></p>

<p>(see sections 3.2 and 3.3 the quoted papers)</p>

<p>Two bootstrap methods (random x vs fixed x) are discussed in the context of standard linear models here</p>

<p><a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients?rq=1"">Two ways of using bootstrap to estimate the confidence interval of coefficients in regression</a></p>

<p>but again the focus is on the beta coefficients.
However, I am not interested in the estimate of the confidence intervals on the beta coefficients, but only on the predicted values.
For instance, consider the following R code</p>

<pre><code>library(glmnet)


# Generate data
set.seed(19875)  # Set seed for reproducibility
n &lt;- 1000  # Number of observations
p &lt;- 5000  # Number of predictors included in model
real_p &lt;- 15  # Number of true predictors
x &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)
y &lt;- apply(x[,1:real_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows &lt;- sample(1:n, .66*n)
x.train &lt;- x[train_rows, ]
x.test &lt;- x[-train_rows, ]

y.train &lt;- y[train_rows]
y.test &lt;- y[-train_rows]



fit.elnet &lt;- glmnet(x.train, y.train, family=""gaussian"", alpha=.5)

yhat &lt;- predict(fit.elnet, s=fit.elnet$lambda, newx=x.test)
</code></pre>

<p>Does anybody know how to calculate a meaningful confidence interval for yhat?</p>

<p>Thanks!</p>
"
"0.0700712753800578","0.0703597544730292","213571","<p>I am trying to do L2-regularized MLR on a data set using caret. Following is what I have done so far to achieve this:</p>

<pre><code>r_squared &lt;-  function ( pred, actual){
    mean_actual = mean (actual)
    ss_e = sum ((pred - actual )^2)
    ss_total = sum ((actual-mean_actual)^2 )
    r_squared = 1 - (ss_e/ss_total)
}

df = as.data.frame(matrix(rnorm(10000, 10, 3), 1000))
colnames(df)[1] = ""response""
set.seed(753)
inTraining &lt;- createDataPartition(df[[""response""]], p = .75, list = FALSE)
training &lt;- df[inTraining,]
testing  &lt;- df[-inTraining,]
testing_response &lt;- base::subset(testing,
                                 select = c(paste (""response"")))
gridsearch_for_lambda =  data.frame (alpha = 0,
                                      lambda = c (2^c(-15:15), 3^c(-15:15)))
regression_formula = as.formula (paste (""response"", ""~ "", "" ."", sep = "" ""))
train_control = trainControl (method=""cv"", number =10,
                              savePredictions =TRUE , allowParallel = FALSE )
model = train (regression_formula,
                           data = training,
                           trControl = train_control,       
                           method = ""glmnet"",
                           tuneGrid =gridsearch_for_lambda,
                           preProcess = NULL
            )
prediction = predict (model, newdata = testing)
testing_response[[""predicted""]] = prediction
r_sq = round (r_squared(testing_response[[""predicted""]],
              testing_response[[""response""]] ),3)
</code></pre>

<p>Here I am concerned about assurance that the model I am using for prediction is the best one (the optimal tuned lambda value).</p>

<p>P.S.: The data is sampled from random normal distribution, which is not giving a good R^2 value, but I want to get the idea correctly</p>
"
"0.0286064783845312","0.0287242494810713","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"0.124692748408752","0.12520610071704","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"NaN","NaN","214017","<p>I have done a multi-level meta analysis with R, using <a href=""https://cran.r-project.org/web/packages/metafor/metafor.pdf"" rel=""nofollow"">metafor package</a>. My questions relates to funnel plot asymmetry. </p>

<p>(1) If a regression test suggested funnel asymmetry (i.e. suggested a publication bias is present), Does this mean that my meta-analysis model is bad? and my results of meta-analysis are bad?</p>

<p>(2) What are other possible reasons, besides publication bias, that could lead to asymmetry in the funnel plots? </p>
"
"0.0700712753800578","0.0703597544730292","214556","<p>I have difficulty interpreting some results. I am doing an hierarhical related regression with <code>ecoreg</code>. If I enter the code I receive output with oddsratio's, confidence ratio's and a 2x maximized log likelihood. </p>

<p>However, I do not fully understand how to interpreted the 2x maximized log likelihood. As far as I know log likelihood is used as a convenient way to calculate a likelihood and it calculates the value of the parameters based on the outcomes. But I do not understand if a higher or lower value is better. I looked at several online sources e.g. <a href=""http://stackoverflow.com/questions/2343093/what-is-log-likelihood"">http://stackoverflow.com/questions/2343093/what-is-log-likelihood</a>, but I am still stuck. </p>

<p>Below the outcome I receive:</p>

<p><code>Call:
eco(formula = cbind(y, N) ~ deprivation + meanIncome, binary = ~fracSmoke + 
    soclass, data = dfAggPlus, cross = cross)</code></p>

<p><code>Aggregate-level odds ratios: 
                   OR        l95        u95
(Intercept) 0.0510475 0.03837276 0.06790878
deprivation 0.9859936 0.88421991 1.09948134
meanIncome  1.0689951 0.95574925 1.19565924</code></p>

<p><code>Individual-level odds ratios:
                OR       l95      u95
fracSmoke 3.124053 2.0761956 4.700765
soclass   1.001050 0.9930815 1.009083</code></p>

<p><code>-2 x log-likelihood:  237.4882</code> </p>

<p>So, how should I interpreted a value of 237.4882 compared to an outcome of 206 or 1083? Help is much appreciated!</p>
"
"0.0495478739876288","0.033167906340333","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.0286064783845312","0.0287242494810713","214790","<p>My model is logistic regression. Is there a way to tune the parameter lambda of lasso or ridge based on cross-validated log-loss and brier(eg. proper scores?) in any R packages? </p>

<p>I'm using glmnet right now and the only measure available seems to be deviance, mean absolute error, misclassification error(is this based on 0.5 cut off?) and auc which are not proper scores and are therefore less desirable. </p>

<p>On a related note, is there a score like squared loss but penalize the deviation from one outcome more severely?</p>
"
"0.0809113394062735","0.0812444463702388","215256","<p>I wanted to do something equivalent to a PCA on a mixed data set containing categorical variables and continuous numerical predictor variables which are normally distributed but measured in very different units. The aims are (a) to explore/describe the variable relationships, and (b) hopefully to reduce the dimensions of the data set for predictive modelling. </p>

<p>Based on this <a href=""http://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont/5777#5777"">cross validated post</a> I have been using (and liking!) the Factor Analysis of Mixed Data function FAMD() of the FactoMineR package in R.  </p>

<p>But I can't work out if this analysis can be treated the same way as a PCA. Two specific questions:</p>

<ol>
<li>I understand the <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">need to scale</a> (ie subtract mean and divide by sd) such numerical variables in a PCA to stop some variables unhelpfully dominating the components. But is this necessary in factor analysis of mixed data as done by the FactoMineR package? Running with both scaled and unscaled as supplementary variables seems to show no difference. 

<ol start=""2"">
<li>Can the principle component dimensions of a mixed data analysis be extracted (from a training set) and applied to a test set, as we might with a true PCA? E.g. if I extract the coordinates of each individual in the training set of Dimension1, then run a regression using the original variables predicting Dimension1, and use the coefficients as the weights of each variable to make a new composite 'Dimension 1 variable' which can be applied to the test set variables - would that be valid?</li>
</ol></li>
</ol>
"
"0.0700712753800578","0.0703597544730292","215275","<p>I have difficulty interpreting some results. I am doing an hierarhical related regression with <code>ecoreg</code>. If I enter the code I receive output with oddsratio's, confidence ratio's and a 2x maximized log likelihood. </p>

<p>However, I do not fully understand how to interpreted the 2x maximized log likelihood. As far as I know log likelihood is used as a convenient way to calculate a likelihood and it calculates the value of the parameters based on the outcomes. But I do not understand if a higher or lower value is better. I looked at several online sources e.g. <a href=""http://stackoverflow.com/questions/2343093/what-is-log-likelihood"">What is log-likelihood?</a>, but I am still stuck. </p>

<p>Below the outcome I receive:</p>

<p><code>Call:
eco(formula = cbind(y, N) ~ deprivation + meanIncome, binary = ~fracSmoke + 
    soclass, data = dfAggPlus, cross = cross)</code></p>

<p><code>Aggregate-level odds ratios: 
                   OR        l95        u95
(Intercept) 0.0510475 0.03837276 0.06790878
deprivation 0.9859936 0.88421991 1.09948134
meanIncome  1.0689951 0.95574925 1.19565924</code></p>

<p><code>Individual-level odds ratios:
                OR       l95      u95
fracSmoke 3.124053 2.0761956 4.700765
soclass   1.001050 0.9930815 1.009083</code></p>

<p><code>-2 x log-likelihood:  237.4882</code> </p>

<p>So, how should I interpreted a value of 237.4882 compared to an outcome of 206 or 1083? Help is much appreciated!</p>
"
"0.143138302630928","0.138199609751387","215441","<p>I am new to R and analytics.
I am trying to create weekly forecasting model. Additionally , I have been asked to see if following components impacts product movement : </p>

<ol>
<li>Weather data ( Mean temperature,rain,snowfall,humidity and precipitation) </li>
<li>Holidays</li>
<li>Promotions</li>
</ol>

<p>Data can be downloaded from below link :
<a href=""https://www.dropbox.com/s/nudm3vs7eo837ml/Store%20Sales%20Data.xlsx?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/nudm3vs7eo837ml/Store%20Sales%20Data.xlsx?dl=0</a></p>

<p>The objective is to create and validate weekly forecast for each store and each individual product category.</p>

<p>Can anyone please validate my approach ? Also, do we have alternate efficient approach ?</p>

<p><strong><em>Code :</em></strong></p>

<p>for(pProduct in unique(df_sales$product_desc)) { </p>

<pre><code>## Fetch data for of the identified location
print (paste(paste("" Proceesing for"", pProductType),pProduct,sep = "" : "" ))

## Data cleansing for subclass long description
vProductDescription &lt;- gsub(""[^[:alnum:][:space:]-]"", """", pProduct)

## Create directory for Product if it does not exists
if (!file_test(""-d"", file.path(pRootDirectory, vProductDescription))){
   dir.create(file.path(pRootDirectory, vProductDescription), showWarnings = FALSE  ,recursive = FALSE, mode = ""0777"")
}else {
   print(paste(""Directory Already exists :"", paste(pRootDirectory, vProductDescription ,""\\"",sep = """")  ))
}

pDirectoryL1 = paste(pRootDirectory, vProductDescription , sep="""")

## Create product subset for processing
df_subset    &lt;- subset(df_sales,product_desc == pProduct
                      ,select=c(store,process_date,units,rain,snowfall,meantemp,promo_ind,humidity,precipitation,holiday_week,fiscal_year,fiscal_week_nbr))

## Calculate Product History
pMinDate       &lt;- as.Date(min(df_subset$process_date[df_subset$units &gt; 0]))
pMaxDate       &lt;- as.Date(max(df_subset$process_date) )
pHistoryLength &lt;- as.numeric(difftime(strptime(pMaxDate, format = ""%Y-%m-%d""),strptime(pMinDate, format = ""%Y-%m-%d""),units=""weeks""))

## Check if product needs to be evaluated
if (pHistoryLength &gt; 104 ) {

    ## Data Cleansing for the data for processing
    df_subset$process_date &lt;- as.Date(df_subset$process_date )
    df_subset &lt;- df_subset[(df_subset$process_date &gt;= pMinDate),]

    ## Processing individual location for forecasting
    for(pLocation in unique(df_subset$store))
    {

        print (paste(paste("" Proceesing for "", pLocationType),pLocation,sep = "" : "" ))

        ## Data Preparation for Location
        pLocationData           &lt;- subset(df_subset,store == pLocation )
        pLocationData           &lt;- pLocationData[order(pLocationData$process_date),]
        rownames(pLocationData) &lt;- rep(1:nrow(pLocationData))

        ## Create Directory Name
        pLocationDirectoryName &lt;- paste(pLocationType,  pLocation ,  sep=""_"")

        ## Create directory for Product if it does not exists
        if (!file_test(""-d"", file.path(pDirectoryL1, pLocationDirectoryName))){
           dir.create(file.path(pDirectoryL1, pLocationDirectoryName), showWarnings = FALSE  ,recursive = FALSE, mode = ""0777"")
        } else {
           print(paste(""Directory Already exists :"", paste(pDirectoryL1, pLocationDirectoryName ,""\\"",sep = """")  ))
        }

        pDirectoryL2 &lt;- paste(pDirectoryL1, pLocationDirectoryName, sep=""\\"")

        ## set the current working directory to Location Folder
        setwd(pDirectoryL2)

        if (mean(pLocationData$units) &gt; 20) ## Do not forecast product with very low sales
        {
            ## 
            pBreakPointDate     &lt;- as.Date(timeFirstDayInMonth(pMaxDate-89))

            if (pForecastType == ""W"") {pforecastPeriod  &lt;- ceiling(as.numeric((pMaxDate - pBreakPointDate)/7))}

            ## find the correlation of individual components with #Units sold
            cor_week     &lt;- cor(pLocationData$units, pLocationData$fiscal_week_nbr, use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_holiday  &lt;- cor(pLocationData$units, pLocationData$holiday_week   , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_promo    &lt;- cor(pLocationData$units, pLocationData$promo_ind      , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_temp     &lt;- cor(pLocationData$units, pLocationData$meantemp       , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_humid    &lt;- cor(pLocationData$units, pLocationData$humidity       , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_precip   &lt;- cor(pLocationData$units, pLocationData$precipitation  , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_rain     &lt;- cor(pLocationData$units, pLocationData$rain           , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))
            cor_snowfall &lt;- cor(pLocationData$units, pLocationData$snowfall       , use=""na.or.complete"" ,method = c(""pearson"", ""kendall"", ""spearman""))

            covariates  &lt;- c(if ((!is.na(cor_week))     &amp; abs(cor_week)     &gt; 0.4) {""fiscal_week_nbr""}
                            ,if ((!is.na(cor_holiday))  &amp; abs(cor_holiday)  &gt; 0.4) {""holiday_ind""}
                            ,if ((!is.na(cor_promo))    &amp; abs(cor_promo)    &gt; 0.4) {""promo_ind""}
                            ,if ((!is.na(cor_temp))     &amp; abs(cor_temp)     &gt; 0.4) {""meantemp""}
                            ,if ((!is.na(cor_humid))    &amp; abs(cor_humid)    &gt; 0.4) {""humidity""}
                            ,if ((!is.na(cor_precip))   &amp; abs(cor_precip)   &gt; 0.4) {""precipitation""}
                            ,if ((!is.na(cor_rain))     &amp; abs(cor_rain)     &gt; 0.4) {""rain""}
                            ,if ((!is.na(cor_snowfall)) &amp; abs(cor_snowfall) &gt; 0.4) {""snowfall""} )

            covariates_str &lt;- """"

            for (i in covariates) {covariates_str &lt;- paste(covariates_str,i[1], sep="" "")}

            ## Create time-series object required for forecasting
            xts_training &lt;- window(ts(pLocationData, start = 1 ), end   = (nrow(pLocationData) - pforecastPeriod  ))
            xts_test     &lt;- window(ts(pLocationData, start = 1 ), start = (nrow(pLocationData) - pforecastPeriod+1))

            ts_training  &lt;- ts(xts_training[,""units""] , start=c(year(pMinDate),month(pMinDate),day(pMinDate))                      , freq=  365.25/7)
            ts_test      &lt;- ts(xts_test[,""units""]     , start=c(year(pBreakPointDate),month(pBreakPointDate),day(pBreakPointDate)) , freq=  365.25/7)

            #================ AUTO ARIMA Model with regressors  ================#
            try(
            {
               print(paste('Starting AUTO ARIMA with regressor for - ',vProductDescription))

               ## forcasting using AUTO ARIMA  model
               forecastARIMAReg &lt;- forecast(auto.arima(xts_training[,""units""], xreg = xts_training[,covariates])
                                            , xreg = xts_test[, covariates], h = pforecastPeriod)

               ## Save the forecasted data using AUTO ARIMA model
               ForecastFileName = paste(pDirectoryL2,""forecast_data_"", ""ARIMA_Regressor"","".txt"" ,sep="""")
               write.csv(forecastARIMAReg,file=ForecastFileName,row.names = TRUE)

               ## PLOT ARIMA GRAPH
               graph_data &lt;- xts(zoo(cbind(training= xts_training[,""units""], actual= xts_test[,""units""], forecast = forecastARIMAReg$mean , temperature=pLocationData$meantemp))
                                 ,order.by = seq(min(pLocationData$process_date), max(pLocationData$process_date), by='weeks')  )

               ## Graph title              
               graph_title &lt;-  paste(vProductDescription , "" - Analysis using "", forecastARIMAReg$method , "" with regressor"" ,covariates_str)

               ## PLOT AUTO ARIMA w/ Regressor GRAPH
               graph_arima_reg &lt;- dygraph( graph_data, main= graph_title) %&gt;%
                                  dySeries(""training"" , label = ""History""   , strokeWidth = 1.5  ) %&gt;%
                                  dySeries(""actual""   , label = ""Actual""    , strokeWidth = 1.5  )  %&gt;%
                                  dySeries(""forecast"" , label = ""Predicted"" , strokeWidth = 1.75 )  %&gt;%
                                  dySeries(""temperature"", axis = ""y2"" , label=""Temperature"" , strokePattern=""dotted"")  %&gt;%
                                  dyRangeSelector(height = 35)  %&gt;%
                                  dyShading(from = as.yearmon(pBreakPointDate) , to = as.yearmon(pMaxDate), color = ""#E9FCE4"") %&gt;%
                                  dyOptions(axisLineWidth = 1.5,includeZero = TRUE, axisLineColor = ""black"", gridLineColor = ""lightblue"" )


               ## SAVE TO HTML File
               saveWidget(widget= graph_arima_reg, file=""graph_arima_reg.html"")

               ## Accruacy of Model against TEST data
               accuracyARIMA_reg   &lt;- accuracy(forecastARIMAReg  , xts_test[,""units""])

               print(paste('Finished AUTO ARIMA w/ regressor for - ',vProductDescription))
            }, silent=T)

 } else {
   vErrorMessage &lt;- paste(""Insignificant data for forecasting "", pLocationType ,pLocation, sep="" - "")
   print (vErrorMessage)
   write.csv(vErrorMessage,file=""ErrorFile.txt"",row.names = TRUE)
 }

    } ##  End - Store loop


} else {
        vErrorMessage &lt;- paste(""Insufficient history for"",vProductDescription,""hence no forecast"")
        print (vErrorMessage)
        write.csv(vErrorMessage,file=""ErrorFile.txt"",row.names = TRUE)

} ## IF condition for history validation
</code></pre>

<p>} ## End of Product Loop</p>
"
"0.0404556697031367","0.0406222231851194","215560","<p>I have three data sets that, when joined, have O(320) independent variables for a classification problem.  </p>

<p>Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous.</p>

<p>I'm at a loss as to how to proceed.  </p>

<p>How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable?  How do you decide between variables?  What calculations can you go on to supplement your gut and experience?  How do you avoid throwing away significant variables?</p>

<p>A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant.  How to accomplish that?</p>

<p>Should I just toss all of it into a logistic regression and see what happens, without any forethought?</p>

<p>More detail in response to comments:</p>

<ol>
<li>Classification. </li>
<li>Many more observations than columns. </li>
<li>Yes, big oh notation meaning approximately. </li>
<li>Linear model at first. Also interested in boosted models in addition to logistic regression. </li>
</ol>
"
"0.0286064783845312","0.0287242494810713","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.0429097175767967","0.0574484989621426","216310","<p>I do not know much about regression, and, in order to practise, I need to code this different regressions in R or Matlab, for the next function:</p>

<p>$$y=\alpha+\beta_1 x_1+\beta_2 x_2+\epsilon$$ where $x_1, x_2$ are uniform variables and $\epsilon$ is normally distributed with mean zero. </p>

<ul>
<li>OLS: $\displaystyle \min \sum_i\left(y_i - x_i^T \beta\right)^2 = ||y-X\beta||_2^2$</li>
<li>Least-absolute regression: $\displaystyle \min ||y-X\beta||_1 = \sum_i |y_i-x_i^T\beta|$</li>
<li>Chebyshev regression: $ \min ||y-X\beta||_{\inf} \equiv \min \max |y_i-x_i^T\beta|$</li>
<li>Ridge regression: $\min ||y-X\beta||_2^2 + \rho ||\beta||_2^2$. You can estimate $\rho$ by cross-validation.</li>
<li>Lasso regression: $||y-X\beta||_2^2 + \lambda ||\beta||_1$, You can estimate $\lambda$ by cross-validation.</li>
<li>Forward regression: you can estimate the final model by the BIC.</li>
<li>Backward regression: you can estimate the final model by the BIC. </li>
</ul>

<p>Thansk a lot. Any help will be very useful for me. </p>
"
"0.064873395163555","0.0759972207238908","217529","<p>I'm on my project to predict the amount of demand of products in a store. we have lot of 0 on the amount of sales of products so when I did multiple regression, predictions of the demand had negative number and I got weird residual plot.
<a href=""http://i.stack.imgur.com/hSqAE.png"" rel=""nofollow"">enter image description here</a></p>

<p>1.It has big difference between under 500 and over 500 in my plot. I want to use weight to make nice residual plot. but How can I decide weight?</p>

<p>And I don't want negative predicted amount of demand. so I used Negative binomial regression and poisson regression to prevent making negative number in prediction. and I got some crazy residual plot. 
<a href=""http://i.stack.imgur.com/0EswV.png"" rel=""nofollow"">enter image description here</a></p>

<p>2.Can I keep using this binomial regression for prediction?
3.Could you recommend any other useful regression model? 
4.If variance of data is bigger than mean of data, should I have to use quasi-poisson regression? </p>
"
"0.140254799517206","0.135198931031345","218085","<p>I have two questions concerning planned contrasts: </p>

<ol>
<li>I would like to know how factor-based contrasts (obtained through an interaction term) compare to model-paramter-based contrasts (obtained by specifying model parameters). </li>
<li>I would like to know this for a simple case of comparing one condition with another, but ultimately I am interested in comparing one condition vs all other conditions. </li>
</ol>

<p>Below are my attempts at understanding factor-based contrasts and model-parameter based contrasts:</p>

<pre><code>#create some dummy data
data &lt;- mtcars
#create interaction terms
data$interaction &lt;- interaction(mtcars$am, mtcars$vs, sep=""X"")
	data$interaction &lt;- gsub(""^0"", ""am0"", data$interaction)
	data$interaction &lt;- gsub(""^1"", ""am1"", data$interaction)
	data$interaction &lt;- gsub(""0$"", ""vs0"", data$interaction)
data$interaction &lt;- gsub(""1$"", ""vs1"", data$interaction)
	data$interaction &lt;- factor(data$interaction)
	levels(data$interaction)
#[1] ""am0Xvs0"" ""am0Xvs1"" ""am1Xvs0"" ""am1Xvs1""
</code></pre>

<p>From Eric Fuchs' <a href=""http://r-eco-evo.blogspot.nl/2007/10/one-of-most-neglected-topics-in_06.html"" rel=""nofollow"">blogpost</a> I think I figured out how to obtain factor-based contrasts. Let us assume for now that I am interested in the comparison of <code>am0Xvs0</code> vs. <code>am0Xvs1</code>. To this purpose, I create a contrast matrix where I assign equal weights with opposing signs to my two levels of interest, and 0 to the other two levels:</p>

<pre><code>#specify contrasts:
c.f &lt;- c(-1, 1, 0, 0) 
mat.f &lt;- cbind(c.f)
contrasts(data$interaction) &lt;- mat.f
#fit model
fit.f &lt;- aov(mpg~interaction, data)
#get coefficients for contrasts
summary(fit.f, split=list(interaction=list(""am0Xvs0 vs. am0Xvs1""=1)))
</code></pre>

<p>Output:</p>

<pre><code>                                       Df Sum Sq Mean Sq F value   Pr(&gt;F)    
interaction                         3  788.6  262.86   21.81 1.73e-07 ***
  interaction: am0Xvs0 vs. am0Xvs1  1  232.3  232.28   19.27 0.000147 ***
Residuals                          28  337.5   12.05                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<hr>

<p>For comparison, my attempt at model-parameter-based contrasts:</p>

<pre><code>fit.m &lt;- aov(mpg~am*vs, data)
</code></pre>

<p>with $E[mpg]=b_0 + b_1 \text{am} + b_2 \text{vs} + b3 (\text{am} \times \text{vs}).$ Based on Matt Blackwell's <a href=""http://stats.stackexchange.com/a/13168/79643"">answer</a> I think that a comparison of <code>am0Xvs0</code> vs. <code>am0Xvs1</code> means that $H_0: b_1 = 0$ (i.e., the regression weight associated with <code>am</code>).  Therefore:     </p>

<pre><code>## construct contrast matrices
mat.m &lt;- rbind(""am0:vs0 - vs1"" = c(0, 0, 1, 0))
library(car)
lht(fit.m, mat.m)
</code></pre>

<p>Output: </p>

<pre><code>Linear hypothesis test

Hypothesis:
am = 0

Model 1: restricted model
Model 2: mpg ~ am * vs

  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1     29 425.84                              
2     28 337.48  1     88.36 7.3311 0.01142 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The values differ, so one of the two solutions is not correct (and my suspicion is that I did not correctly translate Matt's answer to this case). My question now is: which one of the two approaches is correct and how can the other be rewritten correctly? </p>

<hr>

<p>Ultimately, I am interested in the comparison of 1 of the 4 levels of the interaction term vs. the other 3. Let's say I want to compare <code>am0Xvs0</code> vs. the other 3 conditions. The factor-based version is simply an extension of what I have written above (if what I wrote above was correct): </p>

<pre><code>#create contrast matrix
c.f2 &lt;- c(1, -1/3, -1/3, -1/3) 
mat.f2 &lt;- cbind(c.f2)
contrasts(data$interaction) &lt;- mat.f2
#fit model
fit.f2 &lt;- aov(mpg~interaction, data)
#get coefficients for contrasts
summary(fit.f2, split=list(interaction=list(""am0Xvs0 vs. rest""=1)))
</code></pre>

<p>Output: </p>

<pre><code>                                Df Sum Sq Mean Sq F value   Pr(&gt;F)    
interaction                      3  788.6   262.9   21.81 1.73e-07 ***
  interaction: am0Xvs0 vs. rest  1  487.8   487.8   40.48 6.95e-07 ***
Residuals                       28  337.5    12.1                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Unfortunately I would not know how to start with the parameters for the model-parameter-based version, which is what I am ultimately interested in. </p>
"
"0.0707974219804893","0.0710888905739589","218399","<p>I am working on a project where the explanatory variables include soil attributes, land use and land cover properties, stream flow and climate (precipitation, temperature etc) measurements recorded at multiple locations across a study area. I am proposing to use a random forest regression model to predict the response (an eco-indicator) at these locations.</p>

<ol>
<li>Soil attributes and land-use/land-cover data are available as single measurements at each location.</li>
<li>Stream flow and climate data are available as a time series. From these time series data I was hoping to extract long-term representative values such as mean daily stream flow, average annual precipitation total, average annual temperature and so on. </li>
</ol>

<p>However during exploratory data analysis I noticed that at several locations in the study area the annual time series of stream flow and climate variables are exhibiting non-stationarity. I am aware that it is possible stationarize these series using techniques such as detrending or differencing. However, I would like to know if random forests can handle non stationary inputs (without stationarizing)? Are there any best practices when dealing with such data sets?</p>

<p>Thanks!</p>
"
"0.0404556697031367","0.0406222231851194","218630","<p>What does it mean when you have a very low intercept e.g. <code>0.007466004</code> in regression model with a balanced group for a binary dependent variable.</p>

<p>Does it mean the changes of the dependent variable are equal? The other odds ratio's and confidence intervals for the variables make sense. See below: </p>

<pre><code>    Aggregate-level odds ratios:
                  OR            l95         u95
    (Intercept)   0.00467616 0.004038836 0.005414054
    meanA         1.01160918 0.979894450 1.044350381
    meanB         0.90497753 0.883284344 0.927203489

    Individual-level odds ratios:
                  OR            l95      u95
    pC            0.8012136 0.6068843 1.057769
    pD            2.9432395 2.4204315 3.578973
</code></pre>

<p>Any suggestions are appreciated!</p>
"
"0.0948769553749019","0.0866068708302494","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.0572129567690623","0.0574484989621426","218879","<p>I'm looking to model percent change in transaction year over year for sales people grouped in certain categories (7 categories total). The percent change in transactions would be Q1 of the current year divided by the number of transactions from Q1 of the previous year which gives me a percent change (positive or negative). </p>

<p>Since this is count data the correct model to use is the Poisson distribution and eventually I would like to do a Poisson regression to look at the effects of predictor variables on percent change in transactions.</p>

<p>But my question is, if I want to compare the means between the different groups what is the analogous version of the ANOVA for Poisson assumptions. (if that's the correct way to put it) </p>
"
"0.122624731915101","0.123129570327801","219288","<p>UPDATE: This problem was (embarrassingly) solved by specifying the intercept in the regression equation as shown below: </p>

<pre><code>lcs ~ 1 + 0*Y1_bl_ctr #gamma is set to 0 for equivilance with the t-test
</code></pre>

<hr>

<p>This question is distinct enough from (<a href=""http://stats.stackexchange.com/questions/219040/change-score-model-in-lavaan"">Change Score Model in lavaan</a>) that I am migrating it here, but I am including the link for reference purposes. </p>

<p>At the aforementioned post, I was attempting to calculate a latent change score for two waves of observation. As indicated there, I believe the model I specified was over-identified. </p>

<p>Having thought about this for a day and reading a little more, I think I may have been over-complicating the problem.</p>

<p>In a recent paper by Colman and colleagues (<a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794455/"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794455/</a>), it was shown how under certain conditions a latent change score model is equivalent to a simple t-test. </p>

<p>The authors specify a path model in the paper as follows: </p>

<p><a href=""http://i.stack.imgur.com/X6Z3U.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X6Z3U.jpg"" alt=""enter image description here""></a>
As noted in the path diagram, the authors state that the model is equivalent to a paired sample t-test when $\gamma=0$. </p>

<p>The authors additionally make the following constraints on the model: </p>

<ol>
<li>No $Y_2$ residual error; </li>
<li>Intercept of $Y_2$ is set to zero; </li>
<li>The auto-regressive path is set to 1; </li>
<li>The $LCS$ to $Y_2$ path is set to 1.</li>
</ol>

<p>NOTE: Within the body of the paper, the authors also specify that they are baseline-centering the $Y_1$ and $Y_2$ values. </p>

<p>The authors supply the data for their paper at (<a href=""http://trippcenter.uchc.edu/modeling"" rel=""nofollow"">http://trippcenter.uchc.edu/modeling</a>). For convenience, I provide the <code>dput</code> output of the data in the following code chunk and assign it to an object named <code>dat</code>.</p>

<pre><code>dat &lt;- structure(list(Y1 = c(7.6, 8.4, 8.4, 8.7, 9.7, 10.9, 9.7, 7.4, 
7.9, 9.3, 10.3, 11.3, 14, 8.6, 13.3, 10.9, 7.9, 9.5, 9.9, 11.5, 
11.8, 9.9, 8.8, 10.6, 11.3, 7.6, 8, 8.1, 8.6, 8.2, 7.4, 8.5, 
9.3, 9.4, 10.6, 10.3, 10.8, 8.2, 10.1, 7.8, 7.2, 7.8, 8.1, 8.1, 
8.2, 9.1, 8.7, 8.6, 7.6, 8.8, 10.3, 11.3, 7.7, 7.8, 8.5, 8.4, 
8.3, 9.2, 9.9, 9.8, 10.9, 7.8, 10.1, 9.5, 8, 8.3, 8.2, 7.8, 8.8, 
9.4, 8.7, 8.8, 10.8, 11.5, 7.6, 7.6, 7.9, 8.2, 9.7, 10, 8.8, 
10, 12.7, 7.5, 13.4, 8.3, 13.8, 14, 8.4, 14, 10, 9.5, 6.2, 11.7, 
9.7, 14, 7.3), Y2 = c(7.9, 7.8, 5.9, 7.5, 7.5, 9.1, 8, 7.9, 6.7, 
8.1, 8.5, 12, 14, 6.9, 10, 10.9, 7.2, 8.9, 10.8, 7.1, 7.4, 9.7, 
10.3, 9.3, 11.6, 8, 7.1, 6.7, 8.2, 9.3, 7.6, 9.9, 8.9, 8.8, 7.2, 
10.1, 6.7, 6.2, 8.9, 7.3, 7.6, 7.5, 7.3, 9.6, 8.1, 7.8, 8.7, 
8.4, 11.4, 9, 10.2, 12.5, 7, 8.7, 8, 7.2, 8.9, 10.4, 9.4, 10.8, 
9.9, 6.3, 5.7, 10.1, 7.8, 8.2, 7.4, 7.7, 11.8, 7.1, 6.8, 8.1, 
9.2, 10.2, 8.4, 7.1, 9, 6.9, 8.7, 8.8, 9.3, 8.6, 8.5, 7.7, 13.8, 
8.7, 10.5, 14, 10.1, 14, 14, 9, 14, 14, 10.3, 14, 8.4)), .Names = c(""Y1"", 
""Y2""), class = ""data.frame"", row.names = c(NA, -97L))
</code></pre>

<p>In <code>laavan</code> I am trying to implement the model as follows: </p>

<pre><code># baseline mean center Y1 and Y2
dat$Y1_bl_ctr = dat$Y1 - mean(dat$Y1)
dat$Y2_bl_ctr = dat$Y2 - mean(dat$Y1)

test &lt;- '
  # measurement model
    lcs =~ 1*Y2_bl_ctr #4 - the LCS to Y2 path is set to 1
  # regressions
    lcs ~ 0*Y1_bl_ctr #gamma is set to 0 for equivilance with the t-test
    Y2_bl_ctr ~ 1*Y1_bl_ctr #3 - The auto-regressive path is set to 1
  # residual error
    lcs ~~ 1*lcs 
    Y2_bl_ctr ~~ 0*Y2_bl_ctr #1 - No Y2 residual error
'


summary(test &lt;- lavaan(test
                       ,data=dat
                       ,int.lv.free = TRUE #intercepts of LCS is to be estimated
                       ,int.ov.free = FALSE #2- Intercept of Y2 is set to zero;
                       )
        )
</code></pre>

<p>As is probably obvious, this overly-restricted specification results in an unestimated model.</p>

<pre><code>#Error in lav_syntax_parse_rhs(rhs = rhs.formula[[2L]], op = op) : 
#  lavaan ERROR: I'm confused parsing this line: offsetY2_bl_ctr 
</code></pre>

<p>If I respecify the measurement model as <code>lcs =~ Y2_bl_ctr</code>, I get the following output from <code>laavan</code>.</p>

<pre><code>#lavaan (0.5-20) converged normally after   7 iterations
#
#  Number of observations                            97
#
#  Estimator                                         ML
#  Minimum Function Test Statistic               11.543
#  Degrees of freedom                                 1
#  P-value (Chi-square)                           0.001
#
#Parameter Estimates:
#
#  Information                                 Expected
#  Standard Errors                             Standard
#
#Latent Variables:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#  lcs =~                                              
#    Y2_bl_ctr         1.780    0.128   13.928    0.000
#
#Regressions:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#  lcs ~                                               
#    Y1_bl_ctr         0.000                           
#  Y2_bl_ctr ~                                         
#    Y1_bl_ctr         1.000                           
#
#Variances:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#    lcs               1.000                           
#    Y2_bl_ctr         0.000  
</code></pre>

<p>However, I still don't appear to get an intercept estimated - just the $Y_2$ to $LCS$ path coefficient. </p>

<pre><code>t.test(dat$Y1,dat$Y2,paired=TRUE)

#   Paired t-test
#
#data:  dat$Y1 and dat$Y2
#t = 2.1734, df = 96, p-value = 0.03221
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# 0.03423662 0.75545410
#sample estimates:
#mean of the differences 
#              0.3948454
</code></pre>

<p>The 0.395 value is the correct value based on the paper. </p>

<p>Any thoughts on how this model can be specified in <code>laavan</code> in order to produce equivalent results to the t-test?</p>
"
"0.0572129567690623","0.0574484989621426","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.0990957479752576","0.0995037190209989","219679","<p>I would like to know how to find out the analytical solution of a simple linear regression with fixed intercept = 0:</p>

<p>$$ s = e^{-ht}$$
$$ y = -ln(s)  = h\cdot t$$</p>

<p>Here ist the background: I have three survival probabilities $s$ at 30, 90 and 180 days. Obviously, I have at day = 0 100% survival, so I  include this <em>observation</em>. I know that this is contested (<a href=""http://stats.stackexchange.com/questions/102709/when-forcing-intercept-of-0-in-linear-regression-is-acceptable-advisable"" title=""here"">here</a>) but I think in this special case it makes sense. The data I use for fitting the linear regression:</p>

<pre><code>&gt;     obs
    t    s          y
1   0 1.00 0.00000000
2  30 0.98 0.02020271
3  90 0.90 0.10536052
4 180 0.80 0.22314355
</code></pre>

<p>If I fit with simple regression I get this:</p>

<pre><code>&gt;     (fit1 &lt;- lm(y~t, data=obs))

Call:
lm(formula = y ~ t, data = obs)

Coefficients:
(Intercept)            t  
  -0.008464     0.001275
</code></pre>

<p>This can be obtained analytically if the following function is derived:</p>

<p>$$f(h) = \sum (y_i - ht_i)^2$$</p>

<p>which gives:</p>

<p>$$ \frac{\sum (y_i-\bar{y})\cdot (t_i-\bar{t})}{\sum (t_i-\bar{t})^2}$$</p>

<hr>

<p>UPDATE 1: This is the result of the minimization of 
$$f(h) = \sum (y_i - c - ht_i)^2$$. The correct result (see answers):
$$ \frac{\sum (y_i\cdot t_i)}{\sum t_i^2}$$</p>

<hr>

<p>The analytical results is:</p>

<pre><code>yc &lt;- with(obs,y-mean(y))
tc &lt;- with(obs, t -  mean(t))
sum(yc*tc)/sum(tc^2)
[1] 0.001275204
</code></pre>

<p>The same as coefficient in the fit1. Now, if I fix intercept to intercept=0 I get this:</p>

<pre><code>&gt;     (fit2 &lt;- lm(y~0+t, data=obs))

Call:
lm(formula = y ~ 0 + t, data = obs)

Coefficients:
   t  
0.001214  
</code></pre>

<p>I'm wondering how I can get an analytical solution for this. How I have to consider the fix intercept in the function $f(h)$ above?</p>

<p>Any idea is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/ilwvG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ilwvG.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here is the way I constructed the data:</p>

<pre><code>set.seed(123)
# Hazard ratio
h &lt;- 0.7
# Number of observation
n &lt;- 50
# Model: exponential
t &lt;- rexp(n,h)
# scale to days
t &lt;- t*365.25
hist(t)
t &lt;- sort(t)
# Put data into a dataframe
df0 &lt;- data.frame(t=t)
head(df0)
# Compute probablities
df0$s &lt;- 1 - c(1:n)/n
head(df0)
# Extract survival probabilities at 30,90 and 180 days
df0$t2 &lt;- ceiling(df0$t/30)*30
# Select survival probablity 30, 90, 180 days
library(sqldf)
obs &lt;- sqldf(""SELECT t2 t, MAX(s) s FROM df0 WHERE t2 IN (30,90,180) GROUP BY t2"")
# Add survival probability=1 at day 0
obs &lt;- rbind(data.frame(t = 0, s = 1), obs)
# s = e^(-ht)  =&gt; y = -ln(s) = h*t
obs$y &lt;- -log(obs$s)
plot(y~t, data=obs)
fit1 &lt;- lm(y~t, data=obs)
abline(fit1,lty=2)
fit2 &lt;- lm(y~0+t, data=obs)
abline(fit2,lty=2, col=""red"")
legend(""topleft"", legend=c(""fit1"",""fit2""), col=c(1,2), lty=c(2,2))
</code></pre>
"
"0.0700712753800578","0.0703597544730292","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.0756856276908142","0.0759972207238908","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.06396603026469","0.0642293744423385","220429","<p>I'm running a meta-regression/multi-level analysis that contains only categorical variables. </p>

<p>The printout of the data is as follows:</p>

<blockquote>
  <p>res.fe</p>
</blockquote>

<pre><code>Multivariate Meta-Analysis Model (k = 19; method: REML)

Variance Components: none

Test for Residual Heterogeneity: 
QE(df = 7) = 5.9504, p-val = 0.5456

Test of Moderators (coefficient(s) 2,3,4,5,6,7,8,9,10,11,12): 
QM(df = 11) = 39.3316, p-val &lt; .0001

Model Results:

                             estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt                        0.8995  0.0807  11.1477  &lt;.0001   0.7414   1.0577  ***
as.factor(Z)2          -0.1090  0.0825  -1.3213  0.1864  -0.2708   0.0527     
as.factor(Z)3          -0.1299  0.1785  -0.7276  0.4668  -0.4797   0.2199     
as.factor(Z)4          -0.2180  0.2015  -1.0820  0.2793  -0.6128   0.1769     
as.factor(Z)5           0.4280  0.1510   2.8352  0.0046   0.1321   0.7240   **
as.factor(W)1            0.1059  0.1091   0.9707  0.3317  -0.1079   0.3196     
as.factor(W)2            0.1215  0.1584   0.7673  0.4429  -0.1889   0.4319     
as.factor(W)3            0.3696  0.1141   3.2381  0.0012   0.1459   0.5933   **
as.factor(U)2   -0.0575  0.1289  -0.4463  0.6554  -0.3101   0.1950     
as.factor(V)3             -0.1709  0.0981  -1.7417  0.0816  -0.3632   0.0214    .
as.factor(V)4             -0.2247  0.2495  -0.9007  0.3677  -0.7138   0.2643     
as.factor(V)5             -0.4078  0.1181  -3.4542  0.0006  -0.6391  -0.1764  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Variable Z has levels 1, 2, 3, 4, and 5, but the printout only shows levels 2-5; variable V has levels 0, 3, 4, and 5, but only prints levels 3-5;variable U has levels 1 and 2, but the printout only shows level 2.</p>

<p>My understanding is that the intercept is the value when all other variables are explanatory variables are equal to zero. </p>

<p>Does this mean that the intercept is equal to the omitted variables with their respective levels? (i.e. intercept = Z(level=1) + U(level=1)+ V(level=0))</p>

<p>If so, how would I obtain estimates for each of the omitted variables at specified levels?</p>

<p>Thanks for any information that you can provide me with. </p>
"
"0.06396603026469","0.0642293744423385","220614","<p>My problem is how to find the best decreasing 3rd degree polynomial regression in <code>R</code>.
I have data, lets say</p>

<pre><code>x &lt;- x &lt;- c(1:5, 17, 23, 30, 35, 36)
x &lt;- x/max(x)
y &lt;- c(600, 555, 400, 333, 332, 331, 330, 214, 210, 190)
</code></pre>

<p>Here I want to find best polynomial fitting these points, with a constraint, that polynomial must be in whole interval decreasing (here the is interval from 0 to 1). When I use simple <code>lm</code></p>

<pre><code>m &lt;- lm(y ~ I(x^3) +I(x^2) + x)
</code></pre>

<p>the polynomial is not strictly decreasing. </p>

<p>Simple math: I want to find a polynomial $$ y = ax^3 + bx^2 +cx + d $$ </p>

<p>This polynomial is decreasing when </p>

<p>$$ dy / dx &lt; 0 $$,</p>

<p>which means</p>

<p>$$ 3ax^2 + 2bx + c &lt; 0 $$.</p>

<p>From here I can find the constraints, that are</p>

<ol>
<li>$ a &lt; (-2bx - c)/3x^2 $,</li>
<li>$ b &lt; (-3x^2 - c)/2x $,</li>
<li>$ c &lt; -3ax^2 - 2bx $.</li>
</ol>

<p>Is there any way how to input these constraints into <code>lm</code> ? </p>

<p>PS: I found <a href=""http://stats.stackexchange.com/questions/61733/linear-regression-with-slope-constraint"">this link</a> with similar question, but my problem is little more complex. Hope somebody can help!</p>

<p>EDIT: I tested the current answer on this new data set and it doesn't work, why?</p>

<pre><code>x1 &lt;- c(0.01041667, 0.30208333, 0.61458333, 0.65625000, 0.83333333)
y1 &lt;- c(772, 607, 576, 567, 550)
</code></pre>
"
"0.0404556697031367","0.0406222231851194","220649","<p>What's the difference between</p>

<pre><code># Fit Model #
fit1 = lm(var$1 ~ var$2, data=data)
</code></pre>

<p>and</p>

<pre><code># Fit Model #
cof = data.frame(var$1,var$2)
fit1 = VAR(cof, p=lag, type=""both"")
</code></pre>

<p>I think that the first is a linear autoregressive model and that the latter is a vector autoregressive model, right? Meaning: The results should be the same, as long as the VAR does not use not more than two variables.</p>
"
"0.114425913538125","0.114896997924285","220868","<p>The goal of this regression is to determine whether the amount of leaf disk that an insect consumed varied by what tree the leaf material came from. I'll acknowledge upfront that my coding is rarely pretty/efficient, but hopefully it works (usually).</p>

<ul>
<li>Variables:

<ul>
<li>Response: pctrans; the percent of a 7 mm diameter leaf disk that was consumed.  Values have been transformed to fit (0,1).</li>
<li>Explanatory: tree; a categorical (factor) variable of six tree types.</li>
</ul></li>
</ul>

<p>When I use betareg(), which as I understand it, is best suited to data of this sort, I get no significance:</p>

<pre><code>model.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
modelnull.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
lrtest(model1.beta, modelnull.beta)
</code></pre>

<p>Results:</p>

<pre><code>Call:
betareg(formula = pctrans ~ tree, data = BT.data, link = ""logit"")

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.7716 -0.5800  0.0472  0.5351  3.5109 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.111504   0.069191 -16.064  &lt; 2e-16 ***
treeBC3F3   -0.050940   0.095889  -0.531  0.59525    
treeD54     -0.279927   0.096470  -2.902  0.00371 ** 
treeD58     -0.034000   0.095716  -0.355  0.72242    
treeEllis1  -0.006764   0.095175  -0.071  0.94334    
treeQing     0.785992   0.094003   8.361  &lt; 2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi)   3.5549     0.1352   26.29   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 529.8 on 7 Df
Pseudo R-squared: 0.1105
Number of iterations: 20 (BFGS) + 2 (Fisher scoring) 

Likelihood ratio test

Model 1: pctrans ~ tree
Model 2: pctrans ~ 1
  #Df LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 529.82                         
2   2 460.70 -5 138.25  &lt; 2.2e-16 ***
</code></pre>

<p>As I've been told, since the model is significantly worse than the null, no comparisons can be made between treatment means.</p>

<p>HOWEVER...
If I run the same model using glm the model is significantly better than the null.</p>

<pre><code>beta.glm &lt;- glm(pctrans ~ tree, data=BT.data, family=quasibinomial)
</code></pre>

<p>Results:</p>

<pre><code>Call:
glm(formula = pctrans ~ tree, family = quasibinomial, data = BT.data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.94474  -0.38492  -0.08785   0.22725   1.80291  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.22601    0.07643 -16.042  &lt; 2e-16 ***
treeBC3F3    0.06826    0.10660   0.640  0.52205    
treeD54     -0.33864    0.11312  -2.994  0.00281 ** 
treeD58     -0.19878    0.11062  -1.797  0.07260 .  
treeEllis1  -0.07763    0.10808  -0.718  0.47276    
treeQing     0.88596    0.09978   8.879  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2069603)

    Null deviance: 307.54  on 1240  degrees of freedom
Residual deviance: 267.59  on 1235  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4

Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: pctrans
Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                  1240     307.54              
tree  5   39.951      1235     267.59 &lt; 2.2e-16 ***
</code></pre>

<p>Where do I go from here?</p>
"
"0.0904616275314925","0.090834052439095","221046","<p>I have a small data set of cases from an outbreak. For each case I have collected data for a few variables (age, HIV status, hospitalisation, country of infection etc. ). There are only 28 cases and so I imagine that it won't be possible to do any meaningful inferential statistical analysis. I would however like to present the data in as scientific manner as possible. </p>

<p>I would like to, for example, tabulate the data showing the difference between the cases that are HIV positive and HIV negative. I've included some R script and the results of what I've managed to do so far: </p>

<pre><code>data &lt;- read.csv(""Shigella.csv"")
library(dplyr)
library(epiR)
data$Hospitalised &lt;- factor(data$Hospitalised, levels = c(""Yes"", ""No""), labels = c(""Hospitalised"", ""Not Hospitalised""))
data$HIV &lt;- factor(data$HIV, levels = c(""Positive"", ""Negative""))
ttab &lt;- table(data$Hospitalised, data$HIV)
ttab


                   Positive Negative
  Hospitalised            2        1
  Not Hospitalised       13       11


Odds_Ratio &lt;- epi.2by2(ttab, method = ""case.control"", conf.level = 0.95)
Odds_Ratio

&gt; Odds_Ratio
             Outcome +    Outcome -      Total        Prevalence *        Odds
Exposed +            2            1          3                66.7        2.00
Exposed -           13           11         24                54.2        1.18
Total               15           12         27                55.6        1.25
Point estimates and 95 % CIs:
-------------------------------------------------------------------
Odds ratio (W)                               1.69 (0.13, 21.27)
Attrib prevalence *                          12.50 (-44.45, 69.45)
Attrib prevalence in population *            1.39 (-25.97, 28.75)
Attrib fraction (est) in exposed  (%)        39.79 (-1206.69, 99.08)
Attrib fraction (est) in population (%)      5.45 (-22.83, 27.23)
-------------------------------------------------------------------
 X2 test statistic: 0.169 p-value: 0.681
 Wald confidence limits
 * Outcomes per 100 population units
</code></pre>

<p>From the above, my interpreting is that the odds of a hospitalised case being HIV postive are 1.69 that of a non-hospitalised case being HIV positive. The 95% CI is (0.13, 21.27). </p>

<p>I am not sure that the p value is however. I see a p value of 0.681 but that seems to be for the X2 value. </p>

<p>Here is what I'm stuck with: 
1) am I using the right statistical test (or should I be doing logistic regression or something else)? 
2) how do I get a p value for the odds ratio (or is the p value provided above actually for the odds ratio)? </p>

<p>With appreciation! </p>

<p>Greg</p>
"
"0.110792414376932","0.111248539872496","221161","<p>I have been having trouble with the predict function underestimating (or overestimating) the predictions from an lmer model with some polynomials. Hopefully my edits make it clearer. I have scaled data that looks like this:</p>

<pre><code>Terr      Date     Year            Age  
T.092     123      0.548425     -0.86392            
T.104     102      1.2072       -0.48185            
T.104     105      1.075445     -0.86392            
T.104     112      0.94369      -1.24599            
T.040     116     -0.2421        2.192652           
T.040     114     -0.37386       1.810581           
T.040     119     -0.50561       1.428509           
T.040     128      0.15316      -0.09978            
T.040     113      0.021405     -0.48185
</code></pre>

<p>Iâ€™m trying to determine how Year affects lay date after controlling for Age, with Terr (territory) as a random variable. I usually include polynomials and do model averaging, but whether I use a single model or do model averaging, the predict function gives predictions that are a bit lower or higher than they should be. I realize that the model below would not be a good model for this data, Iâ€™m just trying to provide a simplified example.  </p>

<p>Below is my code  </p>

<pre><code>library(lme4
m1 &lt;- lmer(Date ~ (1|Terr) + Year + Age + I(Age^2), data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictions=predict(m1, newdata = new.dat, re.form=NA)
pred.l&lt;-cbind(new.dat, Predictions)
pred.l  

      Year          Age Predictions
    1   -2 2.265676e-16    124.4439
    2   -1 2.265676e-16    123.2124
    3    0 2.265676e-16    121.9810
    4    1 2.265676e-16    120.7496
</code></pre>

<p>When plotted with the means, the graph looks like this:</p>

<p><a href=""http://i.stack.imgur.com/mwIpJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mwIpJ.jpg"" alt=""graph1""></a></p>

<p>When I use effects, I get a much better fit  </p>

<pre><code>library(effects)
ef.1c=effect(c(""Year""), m1, xlevels=list(Year=-2:1))
pred.lc=data.frame(ef.1c)
pred.lc

      Year      fit        se    lower    upper
    1   -2 126.0226 0.6186425 124.8089 127.2363
    2   -1 124.7911 0.4291211 123.9493 125.6330
    3    0 123.5597 0.3298340 122.9126 124.2068
    4    1 122.3283 0.3957970 121.5518 123.1048
</code></pre>

<p><a href=""http://i.stack.imgur.com/SvI3f.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SvI3f.jpg"" alt=""graph2""></a></p>

<p>After much trial and error, I have discovered that the problem is with the Age polynomial, because when the Age polynomial is not included, the predicted and fitted are equal and both fit well. Below is the same  model but with Age as a linear term.  </p>

<pre><code>m2 &lt;- lmer(Date ~ (1|Terr) + Year + Age, data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsd=predict(m2, newdata = new.dat, re.form=NA)  
pred.ld&lt;-cbind(new.dat, Predictionsd)
pred.ld

      Year          Age Predictionsd
    1   -2 2.265676e-16     125.9551
    2   -1 2.265676e-16     124.7653
    3    0 2.265676e-16     123.5755
    4    1 2.265676e-16     122.3857

library(effects)
ef.1e=effect(c(""Year""), m2, xlevels=list(Year=-2:1))
pred.le=data.frame(ef.1e)
pred.le

      Year      fit        se    lower    upper
    1   -2 125.9551 0.6401008 124.6993 127.2109
    2   -1 124.7653 0.4436129 123.8950 125.6356
    3    0 123.5755 0.3406741 122.9072 124.2439
    4    1 122.3857 0.4093021 121.5827 123.1887
</code></pre>

<p>I do many similar analyses, and this issue with the predictions being slightly lower (or higher) than they should be often happens when Age is included as a polynomial. When I include a polynomial for Year, there is no problem and the predicted and fitted are equal, so I know the problem is not with all polynomials.</p>

<pre><code>m3 &lt;- lmer(Date ~ (1|Terr) + Year + I(Year^2) + Age, data=data)

new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsf=predict(m3, newdata = new.dat, re.form=NA)  
pred.lf&lt;-cbind(new.dat, Predictionsf)
pred.lf

      Year          Age Predictionsf
    1   -2 2.265676e-16     125.6103
    2   -1 2.265676e-16     124.8494
    3    0 2.265676e-16     123.7483
    4    1 2.265676e-16     122.3070

library(effects)
ef.1g=effect(c(""Year""), m3, xlevels=list(Year=-2:1))
pred.lg=data.frame(ef.1g)
pred.lg

      Year      fit        se    lower    upper
    1   -2 125.6103 0.8206625 124.0003 127.2203
    2   -1 124.8494 0.4615719 123.9438 125.7549
    3    0 123.7483 0.4275858 122.9094 124.5871
    4    1 122.3070 0.4262110 121.4708 123.1431
</code></pre>

<p>I've looked for answers (e.g., <a href=""http://stats.stackexchange.com/questions/180010/overestimated-and-underestimated-predictions-in-regression"">here</a>) but haven't found anything that is directly helpful. I can provide the whole data set if needed. Does anyone have any insight?</p>
"
"0.111567195944673","0.118616305942459","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.0404556697031367","0.0406222231851194","221630","<p>Since standard error of a linear regression is usually given for the response variable, I'm wondering how to obtain confidence intervals in the other direction - e.g. for an x-intercept. I'm able to visualize what it might be, but I'm sure there must be a straightforward way to do this. Below is an example in R of how to visualize this:</p>

<pre><code>set.seed(1)
x &lt;- 1:10
a &lt;- 20
b &lt;- -2
y &lt;- a + b*x + rnorm(length(x), mean=0, sd=1)

fit &lt;- lm(y ~ x)
XINT &lt;- -coef(fit)[1]/coef(fit)[2]

plot(y ~ x, xlim=c(0, XINT*1.1), ylim=c(-2,max(y)))
abline(h=0, lty=2, col=8); abline(fit, col=2)
points(XINT, 0, col=4, pch=4)
newdat &lt;- data.frame(x=seq(-2,12,len=1000))

# CI
pred &lt;- predict(fit, newdata=newdat, se.fit = TRUE) 
newdat$yplus &lt;-pred$fit + 1.96*pred$se.fit 
newdat$yminus &lt;-pred$fit - 1.96*pred$se.fit 
lines(yplus ~ x, newdat, col=2, lty=2)
lines(yminus ~ x, newdat, col=2, lty=2)

# approximate CI of XINT
lwr &lt;- newdat$x[which.min((newdat$yminus-0)^2)]
upr &lt;- newdat$x[which.min((newdat$yplus-0)^2)]
abline(v=c(lwr, upr), lty=3, col=4)
</code></pre>

<p><a href=""http://i.stack.imgur.com/bgrM3.png""><img src=""http://i.stack.imgur.com/bgrM3.png"" alt=""enter image description here""></a></p>
"
"0.0908377689773195","0.0995037190209989","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.0572129567690623","0.0574484989621426","222777","<p>I am modeling forest inventory data. Forest plots are visited every 5-10 years. New trees that cross some threshold diameter between census intervals are classified as 'recruits' and counted. Generally, across many forest plots, there are many plots with 0 recruits, and some with some discrete number of recruits.</p>

<p>This seems like a straightforward, zero-inflated poisson problem. However, The re-measurement interval varies from plot to plot. This means plots with a longer re-measurement period will have more recruits, just because they have had more time to recruit.</p>

<p>What is the best way to go about modeling the number of recruits in a plot, given that the re-measurement interval varies from plot to plot? I have so far tried including the re-measurement period as a predictor in my model. It is significant, however I do not think this is the appropriate way to go about doing this.</p>

<pre><code>#R code to fit zero-inflated poisson regression model
library(pscl)
m1 &lt;- zeroinfl(recruits ~ x1 + x2 + remeasurement.interval, data=data)
</code></pre>
"
"0.154050600652431","0.154684817416817","223230","<p>I've inherited observations of a response variable ($y$) measured over time ($t$) during which the response increases and subsequently decreases.  The measurement of this response occurs repeatedly over ~ 50 replications ($rep$). The code below pulls in some data from six example $rep$s spanning the range of the replicates.  In this case, time $t$ has been centered across all $rep$s but it need not be.</p>

<p>Some complications of the data include, as the figure illustrates, that the response is usually not measured until after is underway and before the decline has concluded.  This is especially true in later replicates, which observe a smaller window and make fewer measurements of the response curve (e.g., see $rep$ 6).  </p>

<pre><code>tmp &lt;- tempfile()
download.file(""https://gist.githubusercontent.com/adamdsmith/cbb8fa530fa6f1f9c32e41be487ded63/raw/18d3ccf205df181a6343712798b384ecf71dbc27/data.R"", tmp)
source(tmp)
library(""ggplot2"")
ggplot(dat, aes(t,y,group = rep, colour = rep)) + geom_line(size=1) + 
  geom_point() + theme_classic()
</code></pre>

<p><a href=""http://i.stack.imgur.com/g12E8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/g12E8.jpg"" alt=""Six replicates""></a></p>

<p>The variability in the nature of the response curve most of interest is how the response function changes over replicates, most notably whether:</p>

<ol>
<li><p>the temporal location of the maximum response (i.e., the $t$ of peak $y$) changes consistently over $rep$s, and</p></li>
<li><p>the shape of the response curve, particularly its width at some standardized level of response $y$, also changes as a function of $rep$ (i.e., is the response curve becoming more protracted or abbreviated over $rep$s?).  This may be too simplistic, and other parameters describing the response curve may be worth considering (e.g., skew, kurtosis, etc.).</p></li>
</ol>

<p>What makes the most intuitive sense is to have a single estimated underlying response function that describes the mean change in $y$ over $t$ and then evaluate whether the parameters that describe this function vary over $rep$.  This ""hierarchical"" structure can then be evaluated readily in a Bayesian framework.  The actual height of the curve in each $rep$ is, for various reasons, not of interest and thus variability in any related parameter(s) is preferentially be captured with a random $rep$ effect.</p>

<p>There's no shortage of possible ways to model this response function and its change with $rep$, but each approach that I've considered has apparent shortcomings and I'm mentally stuck on a reasonable way forward:</p>

<ol>
<li><p><strong>Quadratic function</strong></p>

<p>This seems an easy way, I think, to get at the questions, particularly using the vertex form: $y = a(t - h)^2 + c$, where $a$ controls the width of the parabola (and the direction it opens), $h$ controls the horizontal position of the vertex, and both can be modeled as a function of $rep$. $c$ controls the vertical position of the vertex and is easily captured with a random effect.  Unfortunately, this approach assumes symmetry about the peak response, which doesn't hold as the decline after peak response seems to occur more quickly than the increase (see, e.g., $rep$s 2-4). </p></li>
<li><p><strong>Gaussian function</strong></p>

<p>Another relatively easy way, where $y = ae^{-\frac{(t - m)^2}{2s^2}}$ and $a$ controls the height of curve's peak, $m$ controls the horizontal ($t$) position of peak (mean), and $s$ controls width of curve (standard deviation).  But this form has the same symmetry problem as the quadratic function.  Certainly Guassian functions can incorporate various degrees of skew and kurtosis but I don't think there's an easy way to model these aspects without cumulative distribution functions/integration.</p></li>
<li><p><strong>Trigonometric regression (sine/cosine Fourier terms)</strong></p>

<p>Trigonometric functions based on $sin$/$cos$ pairs are another relatively easy option (see, e.g., <a href=""http://stats.stackexchange.com/a/60504/21074"">here</a>.  The first $sin$/$cos$ pair terms produces a symmetric sine wave, but additional $sin$/$cos$ pairs with different functional periods get past this limitation.  Unfortunately, what experimentation I've done suggests it takes several additional $sin$/$cos$ pairs of unknown period (i.e., need to be estimated) to adequately capture the asymmetry, which quickly gets unwieldy and yields inadequate fits in some $rep$s.</p></li>
<li><p><strong>Additive models/smoothing splines</strong></p>

<p>An additive model seems to get around many of these limitations, but do I not lose the seemingly preferable state of having a fixed underlying response function defined by parameters than I can allow to vary with increasing $rep$?  Would a factor-smooth interaction that forced the smooth for each $rep$ to have the same smoothing parameter be a reasonable compromise?  For example, something like:</p>

<pre><code>library(mgcv)
?factor.smooth.interaction
m &lt;- gam(y ~ s(t, rep, bs = ""fs""), data = dat)
plot(m)
</code></pre>

<p><a href=""http://i.stack.imgur.com/9lINC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9lINC.png"" alt=""GAM plot""></a></p>

<p>If this is a reasonable solution, how then can I relate the peak or the width of each smooth to $rep$? Finding the peak of fitted smooths is a relatively simple exercise (see <a href=""http://stats.stackexchange.com/a/191489/21074"">here</a> for example), but is it reasonable to then model these derived values (and uncertainty) as a function of $rep$ in a separate model? </p></li>
</ol>

<p>Thanks very much for considering. As is likely obvious, I'm most comfortable with R, but I'm open to any solutions.</p>
"
"0.12136700910941","0.121866669555358","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.0583927294833815","0.0703597544730292","223441","<p>I am running a multivariate binary logistic regression trying to predict political party affiliation using the variables: <strong>Age, Sex, Race, Level of Education, Church Attendance</strong></p>

<p>I have ran the regression using glm() in R and have got some pretty decent results. All of them being pretty significant with p-values less than .05.</p>

<p>I know that doesn't really speak to the validity of the regression but I am now trying to graph it and can't figure out if it's possible given how many ""values"" are within each category.</p>

<p>Although I am only using 5 variables I end up with about 15 values and I know that means a lot of potential graphs.</p>

<p>Is this possible and would anybody know how to set it up using R?</p>

<p>I have attempted to graph just the first few values thus far but it doesn't look correct and I'm thinking it would be impossible to have all the variable values in a single graph. </p>
"
"0.06396603026469","0.0642293744423385","223447","<p>Let's suppose I have <em>p</em> predictor variables. For those predictors, there exists a weight vector <em>w</em> of length <em>p</em> that, if multiplied by the predictors, will minimize an error function. This is not any different than what linear regression performs when the error metric is RMSE. The problem is that I am not using RMSE to determine performance. Instead, I must multiply my weights by my predictors, then plug them into a complex function that takes .5 seconds to compute, and only then do I know if my error improved or worsened. </p>

<p>Pseudo R Code:</p>

<pre><code>vec=rnorm(150,0,1)
p=matrix(unlist(split(vec, ceiling(seq_along(vec)/15))),ncol=10)
response=rnorm(15,0,1)
w=rnorm(15,0,1)

for(i in 1:500){
  #multiply predictors by weights to get predictions
  preds=colSums(t(p)*w)

  #complex error function that takes .5 seconds, e.g.:
  #this isn't the true error function, just an example:
  preds=ifelse(preds&gt;1,preds,ifelse(preds&lt;=1&amp;preds&gt;0,0,-1)) 
  error=mean(abs(response-preds))

  #update weight vector w to move in the most optimal pattern to minimize error
  w= ???
}
</code></pre>

<p>How to update <em>w</em> in the most efficient manner?</p>
"
"0.0572129567690623","0.0574484989621426","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.0404556697031367","0.0406222231851194","224153","<p>I am performing a mediation analysis to investigate the relationship between National Error Management Culture (error_is) on Entrepreneurial Activity (TEA) with the mediator Innovation Index (innov_perc). So first I ran these regressions:</p>

<pre><code>lm(TEA~error_is) --&gt; 5,085 / p=0,0053 
lm(innov_perc~error_is) --&gt; -0,313 / p=0,0001 
lm(TEA~error_is+innov_perc) --&gt; for error_is X: 0,25, p not significant / for innov_perc: -15,55, p=0,007
</code></pre>

<p>Am I correct to conclude, that there is a full mediation? If so, how can I explain it, because Y~X is positive, and M~X and Y~M is negative. What does this mean?</p>

<p>I then ran the mediate function:</p>

<pre><code>model.1 &lt;- lm(innov_perc~error_is, data)
model.2 &lt;- lm(TEA~ error_is+innov_perc, data)
out.1 &lt;- mediate(model.1, model.2, treat = ""error_is"", sims=1000, dropobs = TRUE, mediator = ""innov_perc"")
summary(out.1)
</code></pre>

<p>With these results:</p>

<pre><code>               Estimate 95% CI Lower 95% CI Upper p-value
ACME              4.723        1.518        8.661    0.00
ADE               0.270       -3.902        4.756    0.92
Total Effect      4.993        1.474        8.425    0.01
Prop. Mediated    0.954        0.278        2.784    0.01

Sample Size Used: 35 


Simulations: 1000 
</code></pre>

<p>Does this mean that Error_is has a negative influence on innoc_perc and therefore TEA increases because Entrepreneurial Activity is often not innovative?</p>

<p>Please help me to understand how I can interpret this!! 
Thanks a lot!!</p>
"
"0.0404556697031367","0.0406222231851194","224165","<p>I built a mixed linear regression model which includes a dependent variable 'dv', independent variable 'v1' &amp; 'v2', and subject ID 'subject'. </p>

<p>The R syntax is shown below:</p>

<p>output &lt;-lmer(dv ~ v1 + v2 + v1*v2 + (1|subject)+(0+ v1|subject)+(0+ v2 |subject), data=matrix, control=lmerControl(optimizer=""bobyqa"",optCtrl=list(maxfun=2e5)))</p>

<p>The dataset is here:
<a href=""https://1drv.ms/t/s!AitdBHtSjoIpiCXWmzLEnvqX1iuE"" rel=""nofollow"">dataset</a></p>

<p><em>My question:</em></p>

<p>There is a point very strange, the significance of interaction is not consistent with the data pattern (as shown on attached plot), the fixed effect of interaction shows a very small p-value, meaning the slope of two lines should be significantly different, however, the plot does not show an expected pattern. I am worrying there is something wrong, which one (mixed model output or plot result) is correct?</p>

<p><a href=""http://i.stack.imgur.com/IMWEm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMWEm.png"" alt=""enter image description here""></a></p>
"
"0.0960200924600074","0.111248539872496","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225908","<p>What is a goodness of fit measure for quantile regression? I'm not too familiar with the theory but in R the AIC and log-likelihood are given in <code>quantreg</code> package. So I can use that to do variable selection.</p>

<p>But if I want to compare two fitting methods, eg whether <code>quantreg</code> does better than <code>quantregForest</code>, what shall I do? Maybe trimmed residual mean square?</p>
"
"0.12136700910941","0.121866669555358","226073","<p>I have panel data that is structured like the example below only with more variables. I am using R and my goal is pretty straight forward - 
estimate the effect of the independent variables on my dependent variable.</p>

<pre><code>        country   date        dependent     independent type1  independent type2
        Germany   01/01/2006  70            30                 0.754
        Germany   01/02/2006  72            36                 0.821
        ...  
        Germany   12/31/2016  70            16                 1.214
        Italy     01/01/2006  54            30                 0.213
        Italy     01/02/2006  59            36                 0.343
        ...
</code></pre>

<p>I assume that there are country specific effects that probably also vary across time in the long run which is why I wanted to include fixed effects and then split my panel into 4 time periods and run a regression that is simply like this:</p>

<pre><code>time period 1 lm(dependent ~ independent1+independent2+independent3 ....+country)
time period 2 lm(dependent ~ independent1+independent2+independent3 ....+country)
...
</code></pre>

<p>However when skimming trough some books  I became increasingly unsure (and confused) if this is an adequate approach. Thus my two questions would be:</p>

<ol>
<li><p>Is what I intended to do the (or a) suitable approach to achieve my objective?             </p>

<ol start=""2"">
<li>Can you recommend some other ways to estimate this? I am also very interested in trying some ""creative"" things as long as they are not way over my head.</li>
</ol></li>
</ol>

<p>One more remark.
I indicated type 1 and type 2 for the independent variables. While both vary across time the first one does not vary across countries. I am not sure if that is important on the other hand I feel like I am not sure about anything anymore after looking trough those statistics books.</p>

<p>Thank you.</p>

<p><strong>EDIT:</strong> </p>

<p>What I mean with ""fixed effects change over time"" 
There are (for me) unobservable variables. The effect of these variables is of different magnitude for every country however it will also change to some extend over time. I thought that fixed effects might be able to improve my estimation in that they would capture something like ""the average"" effect of these variables for every country. </p>

<p>It might be difficult to explain without the economic context. I left it out before, because I thought it might be clearer when I express it in general terms but maybe this helps. </p>

<p>I look at the credit default swap (CDS) bond basis which is a spread so simply the difference between the two ""yields"". Now some of this spread I can explain with variables that I am able to proxy for like counter party risk that is involved in CDS etc. However some other parts like ""embedded options in CDS contracts"" I can not observe or proxy for. The impact of these variables will likely be large and also be different for every country. So for example the option value is connected to the default risk of the country thus it will vary over time (as the default probability will vary over time) but especially it will be different for say Greece and Germany.</p>
"
"0.0700712753800578","0.0586331287275243","226085","<p>I try to find additive and innovative outliers in the German Stock Index (DAX) using the method <a href=""https://www.nuffield.ox.ac.uk/economics/papers/2005/W24/Garchoutlier.pdf"" rel=""nofollow"">Doornik &amp; Ooms explained in 2002</a>:</p>

<ol>
<li>Estimate the baseline GARCH model to obtain log-likelihood ($lb$) and residuals.</li>
<li>Find the largest (in absolute value) standardized residual at $t=s$.<br>
Estimate the extended GARCH model with dummy $d_t=1$ if $t=s$ in the mean, and $d_{tâˆ’1}$ in the variance.<br>
This gives estimates for the added parameters and log-likelihood ($lm$).</li>
<li>If $2(lm-lb) &lt; C$ then terminate: no further outliers are present with.<br>
Here $C=5.66+1.88\log(T)$ and $T$ is the number of observations.</li>
</ol>

<p>The data is the DAX (Deutscher Aktienindex) from 2014-06-02 till 2016-01-01 and I got it via Datastream cause <code>pdfetch</code> did not work proper at that time.</p>

<p>My question is, how do I distinguish between the $d_t$ dummy in the mean model and the $d_{t-1}$ dummy in the variance model within the extended GARCH model?</p>

<p>My code so far:</p>

<pre><code>    # Preparation:
    library(""rugarch"")
    library(""tseries"")
    library(""xts"")

    dax &lt;-read.csv2(""~/Bachelorarbeit/Daten/DAXINDX_Time_Series_010114_010116_final.csv"", stringsAsFactors=FALSE)
    dax_xts&lt;-xts(dax, order.by=as.Date.character(dax$Date, format=""%Y-%m-%d"")) #Convert into xts-format
    dax_xts$Date=NULL #Remove ""Date""-Column
    storage.mode(dax_xts)&lt;- ""numeric""
    colnames(dax_xts)&lt;-c(""Dax"") #Rename Column-Names

    dax.logs.prep&lt;-diff(log(dax$Index), lag=1)
    dax.date&lt;-dax$Date[-1]
    dax.logs&lt;-data.frame(dax.date,dax.logs.prep)
    dax_ret&lt;-xts(dax.logs, order.by=as.Date.character(dax.logs$Date, format=""%Y-%m-%d"")) #Convert into xts-format
    dax_ret$Date=NULL #Remove ""Date""-Column
    storage.mode(dax_ret)&lt;- ""numeric""
    colnames(dax_ret)&lt;-c(""Index Returns"") #Rename Column-Names

    # Step 1: Estimate baseline GARCH model to obtain log-likelihood and residuals
    dax_mod&lt;-garch(dax_ret, order = c(1,1))
    l.b&lt;-dax_mod$n.likeli
    dax_mod.res&lt;-data.frame(dax.date, dax_mod$residuals)

    # Step 2: Find largest absolute standardized residual
    max(abs(dax_mod.res$dax_mod.residuals/sd(dax_mod.res$dax_mod.residuals,    na.rm = TRUE)), na.rm = TRUE)
    specgarch &lt;- ugarchspec(variance.model=list(model=""sGARCH"", external.regressors=dummy), mean.model=list(external.regressor=dummy), distribution=""norm"")
    garchfit &lt;- ugarchfit(data=dax_ret, spec=specgarch)
</code></pre>
"
"0.0858194351535935","0.0861727484432139","226803","<p>Is it right to use rfImpute to impute missing feature values on the whole data set and then use other regression/classification techniques on the new data set created? Or, is an rfImpute model intended to be fitted on a subset of the data and then that fitted model is somehow used to fill in missing values in the rest of the data?</p>

<p>To be clear, I only bring up these questions because rfImpute seems to require arguments for both X (features) and y (target variable). In addition, y cannot have any missing values. Does this mean that y gets used for the imputation of the features? Wouldn't this be harmful later when trying to fit models to the new imputed data set? Obviously the y values we are trying to predict in the future won't be known, so how could rfImpute fit into a machine learning pipeline?</p>

<p><strong>Link:</strong>
<a href=""http://math.furman.edu/~dcs/courses/math47/R/library/randomForest/html/rfImpute.html"" rel=""nofollow"">http://math.furman.edu/~dcs/courses/math47/R/library/randomForest/html/rfImpute.html</a></p>

<p>Thank you!</p>
"
"0.0404556697031367","0.0406222231851194","227073","<p>I have a small perplexity some of you might be able to help me with. 
I have fitted a linear model in R of the form</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
</code></pre>

<p>and I want to obtain Likelihood Ratio Tests on the regression coefficients for <code>X1</code> and <code>X2</code>. 
One way to get them is using:</p>

<pre><code>anova(fullmodel, test=""LRT"")
</code></pre>

<p>But, in my understanding, if I use <code>anova</code> on the full model it removes covariates and performs LRT sequentially, indeed results differed depending on ordering of predictors.
<code>drop1</code>, on the other hand, drops one covariate at a time and leaves the rest untouched; thus I could use:</p>

<pre><code>drop1(fullmodel, test=""Chisq"")
</code></pre>

<p>This should work. Yet, out of curiosity, I also tried the following:</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
reducedmodel1 = lm(Y ~ X1)
reducedmodel2 = lm(Y ~ X2)

anova(fullmodel, reducedmodel1, test=""LRT"")
anova(fullmodel, reducedmodel2, test=""LRT"")
</code></pre>

<p>In my understanding, the two procedures (<code>drop1</code> and the two separate <code>anova</code>) have identical meaning and should give exactly the same p-values. That's not the case, though; they differ already at the 3rd decimal number. 
Can anyone explain to me why this happens? Am I doing something wrong?</p>
"
"0.0990957479752576","0.0912117424359157","228238","<p>I'm having a strange problem running a meta-regression using the function <code>rma.mv()</code> in the 'metafor' package in R.</p>

<p>Since some of my data are from multiple-endpoint studies, I have calculated the variance-covariance matrix so that correlations between outcomes are taken into account. I'm also using random effects at study and treatment level. As far as I'm aware, I have now covered all issues with regard to dependent effect sizes.</p>

<p>The model looks like this:</p>

<pre><code>cov_mod &lt;- rma.mv(Hedges_g, cov, mods = ~ days, random = ~ treatment | study, data = rev)
</code></pre>

<p>When running the code, it gives this error message:</p>

<pre><code>Error in rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  : 
  Error during optimization.
In addition: Warning message:
In rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :
  V appears to be not positive definite.
</code></pre>

<p>I have discovered that the problem lies with one particular study (9 effect sizes in total, coming from 3 treatment groups that were each tested at 3 moments in time). When I remove this study from the data set, the code runs without problem.</p>

<p>Thus, apparently this particular study causes the matrix to be 'not positive definite'. I have read that this likely means that ""at least one of [the] variables can be expressed as a linear combination of the others"" (<a href=""http://stats.stackexchange.com/questions/30465/what-does-a-non-positive-definite-covariance-matrix-tell-me-about-my-data"">source</a>).</p>

<p>However, here comes the strange thing: I have replaced all values in the variance-covariance matrix relating to this particular study with random numbers between 0-1 (maintaining the symmetry), and the error message remains unchanged. I am puzzled, because the matrix can no longer be linearly predictable if it contains random numbers.</p>

<p>What could be the issue?</p>
"
"0.0286064783845312","0.0287242494810713","228351","<p>As the titles states, I would like to compare two coefficients in my multiple regression model but I'm not quite sure how.</p>

<pre><code>Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       68.9483    29.7439   2.318 0.024493 *  
Shots.PG          -0.5074     1.4696  -0.345 0.731334    
Shots.OT.PG        7.4992     3.1410   2.388 0.020707 *  
Dribbles.PG        0.6081     0.8121   0.749 0.457401    
Fouled.PG         -0.9856     0.8783  -1.122 0.267031    
Offsides.PG        1.0520     3.0728   0.342 0.733477    
Tackles.PG         0.2705     0.6721   0.402 0.689016    
Fouls.PG          -0.4230     0.7893  -0.536 0.594329    
Ints.PG            0.3414     0.5962   0.573 0.569451    
Shots.Allowed.PG  -3.3604     0.8063  -4.167 0.000119 ***
</code></pre>

<p>Above are the results I've obtained. At first glance I thought it was interesting Shots OT has double the impact of Shots Allowed but I see that their standard errors are significantly different so that worries me.</p>

<p>How would I go about comparing these two values?</p>

<p>Using linear.hypothesis() I get:</p>

<p>Linear hypothesis test</p>

<pre><code>Hypothesis:
Shots.OT.PG  + 2 Shots.Allowed.PG = 0

Model 1: restricted model
Model 2: Points ~ Shots.PG + Shots.OT.PG + Dribbles.PG + Fouled.PG + Offsides.PG + 
    Tackles.PG + Fouls.PG + Ints.PG + Shots.Allowed.PG

  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
1     52 4488.5                           
2     51 4484.2  1    4.2107 0.0479 0.8277
</code></pre>

<p>How do I interpret this? Does this mean they are not different due to its large P Value. I am trying to find out whether or not the Shots OT has a larger effect on the Points total than the Shots Allowed PG</p>
"
"0.0429097175767967","0.043086374221607","228679","<p>I understand that ""glmnet"" package has alpha and lambda regularization parameters which can be optimized by ""caret"" package's train function. Optimal lambda value and lambda values of trained model are in the image. </p>

<p><strong>Can some one please help me understand what these lambda values mean, do they mean while minimizing the criterion function of multinomial regression, the aforementioned lambda values represent values at each iteration?</strong></p>

<pre><code>library(caret)
library(nnet)
ctrl &lt;- trainControl(method = ""repeatedcv"", number = 10, savePredictions = TRUE)
model_train_glmnet &lt;- train(Class2 ~ ZCR + Energy + EntropyE + SpectralC + SpectralS + SpectralE + SpectralF + SpectralR + MFCC1 + MFCC2 + MFCC3 + MFCC4 + MFCC5 + MFCC6 + MFCC7 + MFCC8 + MFCC9 + MFCC10 + MFCC11 + MFCC12 + MFCC13, data = training, method=""glmnet"", trControl = ctrl, tuneLength = 5)

print(model_train_glmnet$finalModel$lambdaOpt)
[1] 0.007676627
&gt; 
&gt; print(model_train_glmnet$finalModel$lambda)
[1] 3.838314e-01 3.497328e-01 3.186635e-01 2.903543e-01 2.645601e-01
[6] 2.410573e-01 2.196424e-01 2.001300e-01 1.823510e-01 1.661514e-01
[11] 1.513910e-01 1.379418e-01 1.256875e-01 1.145217e-01 1.043479e-01
[16] 9.507796e-02 8.663150e-02 7.893539e-02 7.192299e-02 6.553355e-02
[21] 5.971173e-02 5.440710e-02 4.957373e-02 4.516973e-02 4.115698e-02
[26] 3.750071e-02 3.416925e-02 3.113375e-02 2.836791e-02 2.584778e-02
[31] 2.355154e-02 2.145928e-02 1.955290e-02 1.781587e-02 1.623316e-02
[36] 1.479105e-02 1.347706e-02 1.227979e-02 1.118889e-02 1.019490e-02
[41] 9.289211e-03 8.463983e-03 7.712066e-03 7.026948e-03 6.402693e-03
[46] 5.833895e-03 5.315628e-03 4.843402e-03 4.413128e-03 4.021078e-03
[51] 3.663856e-03 3.338369e-03 3.041798e-03 2.771573e-03 2.525354e-03
[56] 2.301009e-03 2.096593e-03 1.910338e-03 1.740629e-03 1.585996e-03
[61] 1.445100e-03 1.316722e-03 1.199748e-03 1.093165e-03 9.960517e-04
[66] 9.075652e-04 8.269396e-04 7.534766e-04 6.865398e-04 6.255495e-04
[71] 5.699774e-04 5.193422e-04 4.732052e-04 4.311670e-04 3.928633e-04
[76] 3.579624e-04 3.261620e-04 2.971867e-04 2.707854e-04 2.467296e-04
[81] 2.248108e-04 2.048393e-04 1.866419e-04 1.700611e-04 1.549534e-04
[86] 1.411878e-04 1.286450e-04 1.172166e-04 1.068034e-04 9.731524e-05
[91] 8.867002e-05 8.079282e-05 7.361541e-05 6.707562e-05 6.111681e-05
[96] 5.568736e-05 5.074025e-05 4.623262e-05 4.212544e-05 3.838314e-05
</code></pre>
"
"0.0286064783845312","0.0287242494810713","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.0756856276908142","0.0759972207238908","228981","<p>I am very new to R and the forecast package authored by Rob Hyndman.
I am working on a time series with 24 samples per hour. I trained a random forest regressor to forecast 6 hour ahead values and am using MAPE(Mean Absolute Percentage Error) on a held out duration as the accuracy metric. </p>

<p>I want to compare its accuracy with standard time series methods like ARMA and ARIMA models.</p>

<p>Time Series Sample</p>

<p><a href=""http://i.stack.imgur.com/H25Wt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H25Wt.png"" alt=""enter image description here""></a></p>

<p>Here is what I have currently. <a href=""https://gist.github.com/sivapvarma/aa277c81f7a617b1f584d0ea32856eb4"" rel=""nofollow"" title=""data.csv"">data.csv</a> has 576*16(16 days worth) samples and I wish to measure forecast accuracy on last 3 days.</p>

<pre><code>library(forecast)
pv_data = read.csv(""data.csv"", header=FALSE)
pv_ts &lt;- ts(pv_data$V2)
train_ts &lt;- window(pv_ts, end=576*13)
test_ts &lt;- window(pv_ts, start=576*13+1)
fit &lt;- auto.arima(train_ts)
accuracy(forecast(fit, h=576*3), test_ts)
</code></pre>

<p>This gives me MAPE  which is average of <code>h = 1 to 576*3</code> samples ahead point forecast absolute errors. </p>

<p><strong>Question:</strong> How to find the average of <code>h=144</code> ahead forecast absolute percent error of the estimates of samples in <code>test_ts</code>? Specifically, how to calculate $ \frac {\sum\limits_{T=576*13-h}^{576*16-h} \left\lvert \tfrac{\hat{e}_{T+h}}{y_{T+h}}\right\rvert }{576*3}$ with <code>h=144</code> where  $\hat{e}_{T+h}=\hat{y}_{T+h | T}-y_{T+h}$?</p>
"
"0.0809113394062735","0.0812444463702388","228985","<p>I have, as the title suggests, two heavily skewed, overdispersed histograms. The data ranges from 0 minutes to 85334 minutes. 90% of the data is below 15 minutes, and takes the form of a positive-skewed exponential/power distribution. Then, there's just a huge tail. There are two groups with similar data structuresâ€”one for <strong>Conversation A</strong> and <strong>Conversations B</strong>. </p>

<p>I'm solid enough with basic statistics to know that comparing the means, STD, p-values, etc. is pretty useless, but I'm not good enough to know <em>how</em> I can compare these two, or what metrics I can compare with one another to see if being in <strong>A</strong> or <strong>B</strong> has any significant effect on the data. I've done some research, and it looks like <em>negative binomial regression</em> fittings will suit my purposes best.</p>

<p>I'm using the <code>MASS</code> package in R, w/ the calls
<code>glm.nb(conversation$A_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    5.624

Degrees of Freedom: 1674 (i.e. Null); 1674 Residual
Null Deviance:      1850
Residual Deviance:  1850    AIC: 17130
</code></pre>

<p>and <code>glm.nb(conversation$B_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    4.768

Degrees of Freedom: 1072 (i.e. Null); 1072 Residual
Null Deviance:      1234
Residual Deviance:  1234    AIC: 12390
</code></pre>

<p>Now, I imagine that the goal here is to compare two coefficients (or sets thereof) for significant differences, but I'm not actually sure what to do with this info. What are some directions I can take to learn more and really figure out what I'm doing? </p>
"
"0.0404556697031367","0.0406222231851194","229286","<p>So, I did do a search already and came across <a href=""http://stats.stackexchange.com/questions/60777/what-are-the-assumptions-of-negative-binomial-regression"">this</a> response but, his explanation went over my head a bit. The research i've done online hasn't been any more helpful. I've used this code,</p>

<pre><code>m3 &lt;- glm(daysabs ~ math + prog, family = ""poisson"", data = dat)
X2 &lt;- 2 * (logLik(m1) - logLik(m3))
</code></pre>

<p>to find out whether or not the Poisson model is more applicable but i'm not sure what to do if the value of X2 is close to 0. Also, in the link, he mentions, </p>

<pre><code>Linearity: The model is still linear in the parameters 
(i.e. the linear predictor is XÎ²), but the expected response
is not linearly related to them (unless you use the identity link function!).
</code></pre>

<p>but I'm not sure what that means. Should I be looking at the linearity between Y vs. X? or the Residuals of the regression model vs. X?</p>

<p>Please advise.</p>
"
"0.118129972176712","0.12520610071704","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"NaN","NaN","229542","<p>Im doing a multiple imputation of a dataset using R's MICE package. </p>

<pre><code>imp &lt;- mice(nhanes, m=5, print = FALSE, seed = 55152)
</code></pre>

<p>I figured out that to pool regression coefficients you really only need to get the mean of the 5 regression coefficients for the 5 datasets. </p>

<p>But now i need to pool means, confidence intervals and standard deviation using Rubin's rules. </p>

<p>How do i do that? </p>

<p>/Kind regards</p>
"
"0.0776265998521925","0.0952675579132743","230201","<p>I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists. </p>

<pre><code> library(glmnet)
 set.seed(1)
make_dummies_out_of_factors&lt;- function(your_df, names_of_factor_variables) {
  indices&lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables
  model_matrices_list&lt;- lapply(indices, function(x) {
    model.matrix(~your_df[,x] - 1, your_df)
  })
  #create a model matrix for each factor variable, and stores each one as a list
  model_matrices_together&lt;- do.call(cbind, model_matrices_list)
  #Column bind all model matrices which are stored as lists
  final&lt;- cbind(your_df, model_matrices_together)
  #Column bind all the model matrices to the original data
  final&lt;- final[,-indices]
  #Get rid of the original factor variables

  names(final)&lt;- gsub(""your_df.*\\]"", ""dummy_"", names(final))
  #Give appropriate names to the dummies

  return(final)
}
test_df&lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000), 
                     state = rep(letters[1:4], 250), year = rep(c(""yr1"", ""yr2""), 500)) #This data frame has 2 factor variables
test_df&lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(""state"", ""year""))

linear_alldum&lt;- lm(test_df$numeric2 ~ test_df$numeric1 + test_df$dummy_yr1 + test_df$dummy_yr2 + test_df$dummy_a + 
                     test_df$dummy_b + test_df$dummy_c + test_df$dummy_d)


X_test&lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrix
y_test&lt;- test_df[,1] #This is the dependent variable

ridge_alldum&lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)


comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))
names(comparison)[1]&lt;- ""coefficients_ridge_l0""
names(comparison)[2]&lt;- ""coefficients_linear_reg""
#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. 

prediction_linear&lt;- predict(linear_alldum)
prediction_ridge&lt;- predict(ridge_alldum, newx = X_test, s = 0)
predictions&lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)
names(predictions)[2]&lt;- ""prediction_ridge""

#Note that the predictions using linear regression and ridge regression aren't the same. 

sapply(predictions, mean) #Means of predictions using linear and ridge.
sapply(predictions, sd) #SDs of predictions using linear and ridge. 
</code></pre>
"
"NaN","NaN","230417","<p>My R code:</p>

<pre><code>library(rugarch)
rtn&lt;-rnorm(500)
vxr&lt;-rnorm(500)
specgarch &lt;- ugarchspec(variance.model=list(model=""sGARCH"",
             external.regressors=matrix(vxr)),
             mean.model=list(armaOrder=c(0,0),external.regressors=matrix(vxr)),
             distribution=""norm"")

garchfit &lt;- ugarchfit(data=rtn, spec=specgarch)
garchfit
#result
Optimal Parameters

vxreg1  0.000000    0.013308   0.000000  1.00000
</code></pre>

<p>At any set of data I found <code>vxreg</code> provides the same result <code>0.0000</code> (p-value=1). Kindly help me regarding this problem.</p>
"
"0.0404556697031367","0.0203111115925597","230851","<p>When we specify the â€œfamily=â€ argument inside glm() in R, how is the distribution being used to regress between the dependent and independent.?</p>

<p>In simple linear regression( lm() in R ) ,we simply calculate the mean of Y for each X and that becomes the predicted value. How different is this when we mention a family? </p>

<p>I do know that each distribution has few paramters that describes it ( mean, shape, scale etc). So how are they used to get predictions?</p>

<p>Bascially I would like to learn what does it mean to fit a distribution to a data.</p>

<p>Edit : to clarify the question</p>

<p>Lets say I trained my model on a dataset of 1lac obs with 2 independent variables and the coefficients are 1,2. i.e. beta1 = 1 and beta2 = 2</p>

<pre><code>          Y    x1   x2
1st obs.  2    .5    1
2nd obs.  1    .25  .25
</code></pre>

<p>So if I choose Poisson distribution, then</p>

<pre><code>1st obs. mean(mu) = exp(.5*1+1*2) = 12.18
</code></pre>

<p>similarily we get a mean for the second obs and so on. 
Now how is this mean related to what you gonna predict? I am not able to connect this. </p>

<p>Another major concern is also how the shape/scale i.e.(sigma, nu, tau for gamlss) being modeled. However they are secondary and for now wanna focus on glm My questions may be stupid but even any resources/links will be helpful and I shall sincerely read them </p>
"
"0.10340625341847","0.111248539872496","231059","<p>So first of all I did some research on this forum, and I know <a href=""http://stats.stackexchange.com/questions/140991/comparing-difference-between-two-polynomial-regression-models-in-r"">extremely similar</a>  questions have been asked but they usually haven't been answered properly or sometimes the answer are simply not detailed enough for me to understand. So this time my question is : I have two sets of data, on each, I do a polynomial regression like so :</p>

<pre><code>Ratio&lt;-(mydata2[,c(2)])
Time_in_days&lt;-(mydata2[,c(1)])
fit3IRC &lt;- lm( Ratio~(poly(Time_in_days,2)) )
</code></pre>

<p>The polynomial regressions plots are:</p>

<p><a href=""http://i.stack.imgur.com/T7r3i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7r3i.png"" alt=""enter image description here""></a></p>

<p>The coefficients are :</p>

<pre><code>&gt; as.vector(coef(fit3CN))
[1] -0.9751726 -4.0876782  0.6860041
&gt; as.vector(coef(fit3IRC))
[1] -1.1446297 -5.4449486  0.5883757 
</code></pre>

<p>And now I want to know, if there is a way to use an R function to do a test that would tell me whether or not there is a statistical significance in the difference between the two polynomials regression knowing that the relevant interval of days is [1,100].</p>

<p>From what I understood I can not apply directly the anova test because the values come from two different sets of data nor the AIC, which is used to compare model/true data.</p>

<p>I tried to follow the instructions given by @Roland in the related question but I probably misunderstood something when looking at my results :</p>

<p>Here is what I did : </p>

<p>I combined both my datasets into one.</p>

<p><code>f</code> is the variable factor that @Roland talked about. I put 1s for the first set and 0s for the other one.</p>

<pre><code>y&lt;-(mydata2[,c(2)])
x&lt;-(mydata2[,c(1)])
f&lt;-(mydata2[,c(3)])

plot(x,y, xlim=c(1,nrow(mydata2)),type='p')

fit3ANOVA &lt;- lm( y~(poly(x,2)) )

fit3ANOVACN &lt;- lm( y~f*(poly(x,2)) )
</code></pre>

<p>My data looks like this now :</p>

<p><a href=""http://i.stack.imgur.com/dNpMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dNpMQ.png"" alt=""enter image description here""></a></p>

<p>The red one is <code>fit3ANOVA</code> which is still working but I have a problem with the blue one <code>fit3ANOVACN</code> the model has weird results. I don't know if the fit model is correct, I do not understand what @Roland meant exactly.</p>

<p>Considering @DeltaIV solution I suppose that in that case :
<a href=""http://i.stack.imgur.com/HLLp9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLLp9.png"" alt=""enter image description here""></a>
The models are significantly different even though they overlap. Am I right to assume so ?</p>
"
"0.124848907203078","0.131631047527805","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.0286064783845312","0.0287242494810713","231659","<p>I ran a plm two-way fixed effects regression and added a lagged dependent variable as i believe theoretically that the effects of a year are dependent on previous years. Namely i am looking at comprehensive and economic sanctions. However I am unsure how to interpret the regression output, because the only variable that is significant is the lagged variable? What does it mean exactly? Thanks</p>

<pre><code>===================================
                 Model 1   
-----------------------------------
lag(Years_of_schooling)    0.92 ***
                  (0.03)   
SanctionsYES              -0.82    
                  (3.09)   
polity                     0.01    
                  (0.01)   
civilwarYES                0.05    
                  (0.05)   
AID                       -0.00    
                  (0.00)   
GDP                        0.00    
                  (0.00)   
Population                -0.00    
                  (0.01)   
SanctionsNO:GNIpc         -0.00    
                  (0.00)   
SanctionsYES:GNIpc         0.00    
                  (0.01)   
GNIpc:impositionYES        0.00    
                  (0.00)   
-----------------------------------
R^2                        0.80    
Adj. R^2                   0.52    
Num. obs.                336       
===================================
*** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05
</code></pre>
"
"0.0404556697031367","0.0406222231851194","231763","<p>I have a set of $N$ models that have generated simulated values based on theoretical assumptions (not regressions) for times $t=1,2,\dots,T$. I want to check which of these models has produced the best fit over this period (i.e. backward looking not forward looking) compared with the series of actuals. Important to note is that I am interested in which series mimics the ""shape"" (i.e. upward and downward movements etc.) of the actuals series the best, however, I am not interested in which series is actually closest to the actuals in terms of absolute distance (like Mean Squared Error would give me). I.e., a series could be very far away from the actuals at times, but as long as it moves in approximately the same way it is considered ""good"".</p>

<p>Of course I can visually inspect this, but I want to be able to base it on numbers. Does anyone have any ideas on how I could do this in R?</p>

<p>Thanks in advance.</p>
"
"0.0286064783845312","0.0287242494810713","232161","<p>I am using randomForest package in R and getting these results</p>

<pre><code>enter Random Forest on Global Rating and Predicted Rating               
  |      Out-of-bag   |             
    Tree    MSE %Var(y) 
    100 0.4703  54.4    
    200 0.4758  55.04   
    300 0.4755  55.01   
    400 0.4732  54.75   
    500 0.472   54.6    
    600 0.4714  54.53   
    700 0.4695  54.31   
    800 0.4692  54.28   
    900 0.4681  54.16   
    1000    0.4669  54.02   

Call:               
 randomForest(formula = Rating ~ Prating + grating, data = mynewdata,      ntree = 1000, importance = T, do.trace = 100)                
               Type of random forest: regression                
                     Number of trees: 1000              
No. of variables tried at each split: 1             

          Mean of squared residuals: 0.4669436              
                    % Var explained: 45.98              
</code></pre>

<blockquote>
  <p>How to validate the model generated with these values is good or bad?
  Is %var explained similar to R^2 and why the %var(explained) is diff.
  from out of bag % var(target).</p>
</blockquote>
"
"0.140142550760116","0.128992883200554","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0809113394062735","0.0812444463702388","233007","<p>I'm currently reading the book <code>An R Companion to applied regression</code> and have started the section on effects plots which is a good method for seeing the effects of
independent variables on dependent variables.</p>

<p>The book explains the steps as follows</p>

<ul>
<li>Identify high order terms of a model (which seems to be when factors are multiplied by numeric vectors to produce interactions ~ the interactions are the high order terms)</li>
<li>It then seems that that all the other independent values are held constant while the value we are interested in seeing the effect for is varied. The constant seems to be mean</li>
</ul>

<p>I have three questions in relation to the plots</p>

<p>1) Although it is explained in the book, I'm having trouble getting my head around how the mean captured for factor variables when holding them constant</p>

<p>2) How is the Y value of the plot interpreted? Is it just a case of when the main line varies along the X-AXIS (independent variables), the Y Axis is the effect on the value to be predicted? If this is the case, how do we determine a large effect? I would assume a researcher would go back to the literature in whatever area they are studying to review previous researcher papers but effects seem to be rarely reported</p>

<p>3) How are interaction terms effects interpreted from the plot?</p>

<p>In essence, I'm trying to get a conceptual understanding to the plots. I assume that a plot with compact 95% confidence intervals and a steep slope is ideal scenario for the plot. </p>

<p>Taken from the <code>effects</code> package as an example</p>

<pre><code># Examples
summary(TitanicSurvival)
titanic &lt;- glm(survived ~ (passengerClass + sex + age)^2,
data=TitanicSurvival, family=binomial)
titanic.all &lt;- allEffects(titanic, typical=median,
given.values=c(passengerClass2nd=1/3, passengerClass3rd=1/3, sexmale=0.5))
plot(titanic.all, ticks=list(at=c(.01, .05, seq(.1, .9, by=.2), .95, .99)), ask=FALSE)

plot(effect(""passengerClass*sex*age"", titanic, xlevels=list(age=0:65)), ticks=list(at=c(.001, .005, .01, .05, seq(.1, .9, by=.2), .95, .99, .995)))
</code></pre>

<p>Thank you for your help</p>
"
"NaN","NaN","233256","<p>I did a toy experiment with linear regression, but getting different results for $R^2$, could any one help me?</p>

<pre><code>library(ggplot2)
fit=lm(price~carat+depth+table+x+y+z-1,data=diamonds)
summary(fit)
sse=crossprod(diamonds$price-fit$fitted.values)
sst=crossprod(diamonds$price-mean(diamonds$price))
1-sse/sst
</code></pre>

<p><a href=""http://i.stack.imgur.com/nBOd0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nBOd0.png"" alt=""enter image description here""></a></p>
"
"0.0700712753800578","0.0703597544730292","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.0286064783845312","0.0287242494810713","233607","<p>I am running logical regression on a model. The output consists of the below values. </p>

<pre><code>Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 459.44  on 396  degrees of freedom
</code></pre>

<p>I did a chisquared test on the residual deviance and it came to  P-value - 0.0151027. Which is pretty significant. So does this mean my model is good or not ?</p>
"
"0.0756856276908142","0.0759972207238908","233619","<p>I have data from an EEG experiment. Originally, the design was balanced, but because in EEG you loose a lot of data to noise and malfunctioning equipment, the end-result is unbalanced data. </p>

<p>I am working with R. In R, my data looks like this:</p>

<pre><code>          Name                 Condition Channel EpochCountStd EpochCountDeviant ErpMinTime ErpMinVoltage ErpMinAv Frequency
380   AY11042016B1-Deci.EEG      HFMM     AF3           423               103        203          -1.2     -1.1      High
388 AvdP23052016B1-Deci.EEG      HFMM     AF3           410               101        144          -3.7     -3.2      High
397   EW20042016B1-Deci.EEG      HFMM     AF3           457               118        123          -2.6     -2.3      High
413  IG160312016B1-Deci.EEG      HFMM     AF3           435               105        214          -2.2     -1.6      High
422  IJB18042016B1-Deci.EEG      HFMM     AF3           408               110        121          -1.3     -1.1      High
439   MC31032016B1-Deci.EEG      HFMM     AF3           438               101        116          -4.0     -3.3      High
        Hemisphere  Region      Lexicality Subject
380       Left AnterioFrontal       Word Subject08
388       Left AnterioFrontal       Word Subject14
397       Left AnterioFrontal       Word Subject12
413       Left AnterioFrontal       Word Subject02
422       Left AnterioFrontal       Word Subject10
439       Left AnterioFrontal       Word Subject05
</code></pre>

<p>For present purposes, my dependent variable is <code>ErpMinAv</code>, a continuus numerical variable and my independent variables are <code>Lexicality</code> (2 levels), <code>Frequency</code>(2 levels), <code>Hemisphere</code>(3 levels) and <code>Region</code> (5 levels).</p>

<p>Moreover, I have tested for sphericity and the result is highly significant, meaning my data violate the sphericity assumptions. </p>

<p>I have already run anovas and regressions on my data, but I am always onsure of some things:</p>

<ol>
<li>Should I be using anova if I am violating the sphericity assumption? I understand most statistical programs include corrections in the tests themselves, but from what I have read on the internet, this is not necessarily the case in R. I have seen people recommending the use of <code>lm()</code> or <code>lme4()</code>. </li>
<li><p>How should I order my independent variables in the R syntax? So far, I tried:</p>

<pre><code>MM_Model &lt;- aov(ErpMinAv ~ Lexicality*Frequency*Hemisphere*Region 
                + Error(Subject),data = MM_Table)
</code></pre></li>
</ol>

<p>But I have seen different ways of ordering the factors (e.g. with sums instead of multiplication signs) and many different ways of writing the <code>Error()</code> term (e.g. like this: <code>Error(Subject/(v1*v2*v3*v4</code>). I have not, however, been able to find explanations for a case where there is no sphericity and the data is unbalanced. </p>
"
"0.0700712753800578","0.0703597544730292","233858","<p>I am unable to interpret this graph. My dependent variable is total number of movie tickets that will be sold for a show. The independent variables are the number of days left before the show, seasonality dummy variables (day of week, month of year, holiday), price, tickets sold till date, movie rating, movie type (thriller, comedy, etc., as dummies). Also, please note that movie hall's capacity is fixed. That is, it can host maximum of x number of people only. I am creating a linear regression solution and it's not fitting my test data. So I thought of starting with regression diagnostics. The data are from a single movie hall for which I want to predict demand.</p>

<p>The is a multivariate dataset. For every date, there are 90 duplicate rows, representing days before the show. So, for 1 Jan 2016 there are 90 records. There is a 'lead_time' variable which gives me number of days before the show. So for 1 Jan 2016, if lead_time has value 5, it means it will have tickets sold until 5 days before the show date. In the dependent variable, total tickets sold, I will have the same value 90 times.</p>

<p>Also, as a side remark, is there any book that explains how to interpret residual plot and improve model afterwards?</p>

<p><a href=""http://i.stack.imgur.com/Q0ZuC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q0ZuC.png"" alt=""enter image description here""></a></p>
"
"0.0286064783845312","0.0287242494810713","234172","<p>I ran a plm two-way fixed effects regression and added a lagged dependent variable as i believe theoretically that the effects of a year are dependent on previous years. Namely i am looking at comprehensive and economic sanctions.
However I am unsure how to interpret the regression output, because the only variable that is significant is the lagged variable? What does it mean exactly? Thanks</p>

<pre><code>===================================
                     Model 1   
-----------------------------------
lag(Years_of_schooling)    0.92 ***
                      (0.03)   
SanctionsYES              -0.82    
                      (3.09)   
polity                     0.01    
                      (0.01)   
civilwarYES                0.05    
                      (0.05)   
AID                       -0.00    
                      (0.00)   
GDP                        0.00    
                      (0.00)   
Population                -0.00    
                      (0.01)   
SanctionsNO:GNIpc         -0.00    
                      (0.00)   
SanctionsYES:GNIpc         0.00    
                      (0.01)   
GNIpc:impositionYES        0.00    
                      (0.00)   
-----------------------------------
R^2                        0.80    
Adj. R^2                   0.52    
Num. obs.                336       
===================================
*** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05
</code></pre>
"
"0.0990957479752576","0.0912117424359157","234192","<p>I referred to <a href=""https://www.otexts.org/fpp/9/4"" rel=""nofollow"">this link</a> and I have the following questions regarding my data. Let me start by explaining the time series that I am dealing with.</p>

<p>I have <strong>daily</strong> hospital data with various <strong>departments</strong> and numerous <strong>doctors</strong> working in each department. I have several years of data and my forecast horizon is for the next 365 days. My data has weekly and annual seasonality. Moreover I intend to capture the effects of holidays and Sundays in my forecasts. As a result I have not created a hierarchical time series as suggested towards the end of the link(primarily because I am not sure whether we can pass a regressor to it and more so because I do not know how many doctors I end up predicting for in each department). </p>

<p>The reason for this is that some doctors do not have good data(short time series or sparse data). In this case I collect these doctors and aggregate them to form something I call ""OtherDocs"". Typically in <code>DeptXYZ -&gt; Doc1 , Doc2 , Doc3 , Doc4 , Doc5 and Doc6</code> I could end up creating forecasts for <code>DeptXYZ -&gt; Doc1 , Doc3 , Doc4 , Doc6 and OtherDocs</code>. If <code>OtherDocs</code> is still not predictable I generate a naive forecast. In this fashion I created <strong>base forecasts for every level in the hierarchy individually using <code>arima</code> and passing my <code>xreg</code> to it and selecting the best model on the basis of AIC</strong>.</p>

<p>Now, consider this example - </p>

<p><code>Total -&gt; DeptX and DeptY</code></p>

<p><code>DeptX -&gt; DocA and DocB</code></p>

<p><code>DeptY -&gt; Doc1 , Doc2 and Doc3</code></p>

<p>There are cases where <code>DocA</code> has a time series that starts from ""2011-03-11"" and ends on ""2016-09-07"" while <code>DocB</code> has a time series that starts from ""2011-05-17"" and ends on ""2016-09-07"". Generating the base forecasts for <code>DocA</code> and <code>DocB</code> results in the predicted values(<code>fit$mean</code>) being of a time series from ""2016-09-08"" to ""2017-09-07"". As long as the time series refers to the same dates within the Department I believe we are good to go.</p>

<p>In my attempt to reconcile the forecasts from each level I employed the forecasted proportions like so -</p>

<p>$\Largeá»¹_{DocA,365} = \frac{Å·_{DocA,365}*Å·_{DeptX,365}}{(Å·_{DocA,365}+Å·_{DocB,365})*(Å·_{DeptX,365}+Å·_{DeptY,365})}Å·_{Total,365}$</p>

<p><strong>1. Am I doing anything wrong in the above step?</strong></p>

<p><strong>2. Suppose for one moment that the topmost level forecasted values do not capture the low points of data in the case of Holidays and Sundays. Does that intuitively mean that revised forecasts for DocA might not correctly capture the same(being a proportion of $Å·_{Total,365}$)?</strong></p>

<p>Another query I have is to do with the Optimal Combination Approach -</p>

<p>$\Largeá»¹_h = S(Sâ€²S)^{-1}Sâ€²Å·_h$</p>

<p><strong>3. I am unfamiliar with this matrix notation $S'$. Is it the inverse of $S$? Could you shed some light on this? And how do you suggest I calculate the summing up matrix in my case?(Is it absolutely necessary to proceed with the exact knowledge of the number of doctors in each department?)</strong></p>
"
"0.0814154647783432","0.072667241951276","234470","<p>I have data from these set of experiments:</p>

<p>In each experiment I infect a neuron with a rabies virus. The virus climbs backwards across the dendrites of the infected neuron and jumps across the synapse to the input axons of that neuron. In the input neurons the rabies will then express a marker gene thereby labeling them. This allows me to see which neurons are inputs to the target neuron I infected and thus create a connectivity map of a certain region in the brain.</p>

<p>In each experiment I obtain counts of all the infected input neurons of the target neuron I infected.</p>

<p>Here's a simulation of the data: (3 targets and 5 inputs)</p>

<pre><code>set.seed(1)
probs &lt;- list(c(0.4,0.1,0.1,0.2,0.2),c(0.1,0.3,0.4,0.1,0.1),c(0.1,0.1,0.4,0.2,0.2))
mat &lt;- matrix(unlist(lapply(probs,function(p) rmultinom(1, as.integer(runif(1,50,150)), p))),ncol=3)
inputs &lt;- LETTERS[1:5]
targets &lt;- letters[1:3]
df &lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))
</code></pre>

<p>What I'd like to estimate is the effect of each target neuron on these counts, relative to the grand mean. I was thinking that a multinomial regression model is appropriate in this case, where the contrasts are set to the <code>contr.sum</code> option:</p>

<pre><code>library(foreign)
library(nnet)
library(reshape2)

df$input &lt;- factor(df$input,levels=inputs)
df$target &lt;- factor(df$target,levels=targets)
fit &lt;- multinom(input ~ target, data = df,contrasts = list(target = ""contr.sum""))
# weights:  20 (12 variable)
initial  value 505.363505 
iter  10 value 445.057386
final  value 441.645283 
converged
</code></pre>

<p>Which gives me:</p>

<pre><code>&gt; summary(fit)$coefficients
  (Intercept)   target1   target2
B  0.08556288 -1.743854 1.6062660
C  0.55375003 -2.094266 1.2616939
D -0.17624590 -1.364270 0.6284231
E -0.04091248 -1.617374 0.6601274
</code></pre>

<p>So the effects for <code>input A</code> are not reported and I would like to obtain both the effects of all <code>targets</code> on all <code>inputs</code>.</p>

<p>I'm wondering if adding a mean across <code>targets</code> and a mean across <code>inputs</code>, and setting them as baseline <code>dummy</code> variables is a good solution:</p>

<pre><code>#add target mean
mat &lt;- cbind(mat,round(apply(mat,1,mean)))
colnames(mat)[ncol(mat)] &lt;- ""x""
targets &lt;- c(targets,""x"")

#add input mean
mat &lt;- rbind(mat,round(apply(mat,2,mean)))
rownames(mat)[nrow(mat)] &lt;- ""X""
inputs &lt;- c(inputs,""X"")
</code></pre>

<p>So <code>x</code> and <code>X</code> represent the means of <code>targets</code> and <code>inputs</code>, respectively, and are rounded so that they are counts.</p>

<pre><code>df &lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))

df$input &lt;- factor(df$input,levels=rev(inputs))
df$target &lt;- factor(df$target,levels=rev(targets))
</code></pre>

<p>And then fit the <code>multinom</code> regression using <code>dummy coding</code>:</p>

<pre><code>fit &lt;- multinom(input ~ target, data = df)
</code></pre>

<p>Thanks</p>
"
"0.06396603026469","0.0642293744423385","234538","<p>I am trying to combine two linear models (one linear-quadratic and one linear) into one unified model by means of piecewise regression. The tail of the lhs (linear-quadratic part) should continue to be the asymptote for the rhs (linear part). Here's <a href=""http://www.intechopen.com/source/html/45395/media/image4.png"" rel=""nofollow"">a link</a>! The piecewise function is,</p>

<p>$$y = ax + bx^2,\ x \lt x_t$$ and</p>

<p>$$y = cx + d,\ x \ge x_t$$</p>

<p>where $a, b, c, d$ and $x_t$ (a breakpoint) are parameters to be determined. This unified model should be compared with the linear-quadratic model for the whole range of $x$ by R.squared.adjusted as a measure of goodness of fit.</p>

<pre><code>&gt; y
[1] 1.00000 0.59000 0.15000 0.07800 0.02000 0.00470 0.00190 1.00000 0.56000 0.13000 0.02500 0.00510 0.00160 0.00091 1.00000 0.61000 0.12000
[18] 0.02600 0.00670 0.00085 0.00040
&gt; x
[1] 0.00  5.53 12.92 16.61 20.30 23.07 24.92  0.00  5.53 12.92 16.61 20.30 23.07 24.92  0.00  5.53 12.92 16.61 20.30 23.07 24.92
</code></pre>

<p>I'm after continuity of the first derivative and to find the parameters, including determining the breakpoint. Since i want continuity at $x=x_t$, I have rewritten the piecewise function to</p>

<p>$$y = ax + bx^2,\ x \lt x_t$$ and</p>

<p>$$y = ax_t + bx_t^2 + k(x - x_t),\ x \ge x_t$$</p>

<p>where $k$ is a constant. So my attempt goes as follows (assuming I have derived $x_t$ theoretically):</p>

<pre><code>I = ifelse(x &lt; xt, 0, 1)*(x - xt)
x1 = ifelse(x &lt; xt, x, xt)
mod = lm(y ~  x1 + I(x1^2) + I)
</code></pre>

<p>But the tail (asymptote) doesn't seem to be parallel to the linear part in the upper range...</p>
"
"0.0495478739876288","0.0497518595104995","234998","<p>For testing purposes I made up some correlated data in R like this:</p>

<pre><code>mydata = data.frame(
  outcome   = c(1, 0, 1, 0, 0, 1, 1, 0, 1, 1),
  predictor = c(0.1, -0.2, 0, 0.1, -0.3, 0.3, 0.2, -0.1, 0.1, 0.1)
)
</code></pre>

<p>Then I did this in order to create a logistic model that modeled this data:</p>

<pre><code>model1 = glm(family = binomial, formula = outcome ~ predictor, data = mydata)
</code></pre>

<p>Running <code>plot(model1)</code> yields the following plots:</p>

<p><a href=""http://i.stack.imgur.com/tbdpD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tbdpD.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/4O3jW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4O3jW.png"" alt=""enter image description here""></a></p>

<p>I need answers to some questions in order to understand how to perform diagnostics on such a logistic model. As someone with only an introductory course in statistics I'm having trouble gathering knowledge on how to interpret the plots.</p>

<ol>
<li>What do the ""Predicted Values"" in the first plot represent?</li>
<li>What does residual mean in the context of logistic regression?</li>
<li>Which of these plots can in any way be useful for model diagnostics based on real data? How?</li>
</ol>
"
"0.06396603026469","0.0642293744423385","235402","<p>I am a newbie at R. I am trying to do some logistic regressions. My predictors are categorical, and most have more than two levels.</p>

<p>A couple questions:
1. It looks like R already creates the relevant contrasts for the categorical predictors (I am used to SAS where I need to specify all the contrasts). Is this correct for R? As in, I do <strong>not</strong> need to manually create the contrasts myself?</p>

<ol start=""2"">
<li><p>the ""family=binomial"" step in the glm syntax, will I always need to write this regardless of the number of predictors, and even if the categorical predictors have more than two levels (DV is always binary)?</p></li>
<li><p>For interpreting the results, if I use a categorical predictor (e.g., lettergroup) with different levels (a, b, c, d) as my predictor, with the binary DV as 1=yes and 0=no, if R created the lettergroupb contrast, and it is a significant, positive coefficient, this means b is more likely to say yes compared to a, c, and d? thank you!</p></li>
</ol>
"
