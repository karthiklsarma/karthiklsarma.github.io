"V1","V2","V3","V4"
"0.0588235294117647","0.0592348877759092","8750","<p>If I have an arima object like <code>a</code>:</p>

<pre><code>set.seed(100)
x1 &lt;- cumsum(runif(100))
x2 &lt;- c(rnorm(25, 20), rep(0, 75))
x3 &lt;- x1 + x2

dummy = c(rep(1, 25), rep(0, 75))

a &lt;- arima(x3, order=c(0, 1, 0), xreg=dummy)
print(a)
</code></pre>

<p>.</p>

<pre><code>Series: x3 
ARIMA(0,1,0)                    

Call: arima(x = x3, order = c(0, 1, 0), xreg = dummy) 

Coefficients:
        dummy
      17.7665
s.e.   1.1434

sigma^2 estimated as 1.307:  log likelihood = -153.74
AIC = 311.48   AICc = 311.6   BIC = 316.67
</code></pre>

<p>How do calculate the R squared of this regression?</p>
"
"0.0831890330807703","0.0837707816583391","12885","<p>I always believed that time should not be used as a predictor in regressions (incl. gam's) because, then, one would simply ""describe"" the trend itself.  If the aim of a study is to find environmental parameters like temperature etc. that explain the variance in, letÂ´s say, activity of an animal, then I wonder, how can time be of any use? as a proxy for unmeasured parameters? </p>

<p>Some trends in time on activity data of harbor porpoises can be seen here:
-> <a href=""http://stats.stackexchange.com/questions/12712/how-to-handle-gaps-in-a-time-series-when-doing-gamm"">How to handle gaps in a time series when doing GAMM?</a></p>

<p>my problem is: when I include time in my model (measured in julian days), then 90% of all other parameters become insignificant (ts-shrinkage smoother from mgcv kick them out). If I leave time out, then some of them are significant...</p>

<p>The question is: is time allowed as a predictor (maybe even needed?) or is it messing up my analysis?</p>

<p>many thanks in advance</p>
"
"0.0588235294117647","0.0592348877759092","14110","<p>Is there a R package that I can use to specify Smooth Transition Models. I'm looking specifically for something that allows me to specify a TAR model for a given time series.</p>

<p>In 2008, a package, <a href=""http://www.r-project.org/conferences/useR-2008/slides/Balcilar.pdf"" rel=""nofollow"">RSTAR</a>, was presented in the R User Conference by Mehmet Balcilar. However, the packages doesn't seems to be published and I was wondering if there is any other alternatives in R.</p>
"
"NaN","NaN","52171","<p>We consider a sparse autoregressive time series of length 1000 obeying the model</p>

<p>$$X(t)=0.2X(t-1)+0.1X(t-3)+0.2X(t-5)+0.3X(t-10)+0.1X(t-15)+Z(t)$$</p>

<p>with nonzero coefficients at lags 1,3,5,10 and 15,where the innovations Z(t) are i.i.d. Gaussians with mean zero and standard deviation 0.1.</p>

<p>The question is how to simulate 1000 time series from the model with R or SAS?</p>
"
"0.131533410441164","0.132453235706504","53336","<p>I've done some research and it doesn't seem this question has been directly answered. I have a situation where the outcome variable is measured monthly from 1975-current and then multiple predictors with varying start dates and end dates. My question is what is the best approach to take with this data? I'm working with another statistician who has more experience with time series data in these situations and he tells me the common practice is to delete the data up to the shortest predictor. For example, if predictor 1 is measured from 1975-current and predictor 2 is measured from 2001-current, he tells me the best approach and only option is to use data for both predictors from 2001-current. So, essentially deleting all the data for predictor 1 from 1975-2000. He also said this often results in better prediction. I'm a bit skeptical of all this and was wondering if an ML or Bayesian approach in R or other common methods in R that might be able to handle this situation and use all available data. One thought is to handle this as a ""variable occasion design"" (see Chapter on Longitudinal Data in Snijders &amp; Bosker, ""Multilevel Analysis"").</p>
"
"0.0831890330807703","0.0837707816583391","54683","<p>I am struggling with a linear regression model of the shape $y = a + b_1\text{month} + b_2\text{year}$. I have 12 months for each year and 10 years. My dependent variable is a log transformed ratio. I have understood that much that when setting such a model up in R, R automatically picks a level for each variable to go into the intercept.  March and 2005 goes into the intercept, in order to provide a baseline for comparison for the other factors.</p>

<p>As stated my problem is that i cannot really figure out what the intercept represents. Is it simply an average of the ratio of March and 2005 or what is it?</p>

<pre><code>Residuals:                  
Min 1Q  Median  3Q  Max 
-0.90339    -0.16789    -0.00373    0.15472 0.88338 

Coefficients:                   
    Estimate    Std. Error  t   value   Pr(&gt;|t|)
(Intercept) -3.586154   0.131642    -27.242 &lt;2.00E-16   ***
MONTHJan    0.381735    0.140731    2.713   0.007875    **
MONTHFeb    0.256457    0.140731    1.822   0.071426    .
MONTHApr    0.072824    0.140731    0.517   0.605981    
MONTHMay    0.207984    0.140731    1.478   0.142613    
MONTHJun    -0.008194   0.140731    -0.058  0.953686    
MONTHJul    0.363693    0.140731    2.584   0.011217    *
MONTHAug    0.195791    0.140731    1.391   0.16727 
MONTHSep    0.212562    0.140731    1.51    0.134124    
MONTHOct    0.124234    0.140731    0.883   0.379495    
MONTHNov    0.204009    0.140731    1.45    0.15032 
MONTHDec    0.175348    0.140731    1.246   0.215711    
YEAR1999    0.477663    0.128469    3.718   0.000333    ***
YEAR2000    -0.027343   0.128469    -0.213  0.83189 
YEAR2001    -0.166637   0.128469    -1.297  0.197612    
YEAR2002    -0.060508   0.128469    -0.471  0.638684    
YEAR2003    -0.173492   0.128469    -1.35   0.179948    
YEAR2004    0.003592    0.128469    0.028   0.977753    
YEAR2006    -0.283261   0.128469    -2.205  0.029776    *
YEAR2007    -0.267752   0.128469    -2.084  0.03972 *
YEAR2008    -0.240654   0.128469    -1.873  0.063985    .
---                 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1                  

Residual standard error: 0.3147 on 99 degrees of freedom                    
Multiple R-squared: 0.4167,                 
Adjusted R-squared: 0.2988                  
F-statistic: 3.536 on 20 and 99 DF,  p-value: 1.491e-05 
</code></pre>
"
"0.186016332951081","0.187317162316339","225578","<p>I have the following time series dataset (dependent | independent) :</p>

<pre><code>Sales | Income,Inflation, Interest Rates etc
</code></pre>

<p>All of this is dynamic data pertaining to each of 24 months (month:0 to month:24). For 25th month onward I have no data for the independent variables (Income,Inflation, Interest Rates etc), yet I want to be able to predict sales for month:25 +.</p>

<p>I have been trying to figure out models which I can used to implement this scenario including Dynamic Regression and ARMAX/ARIMAX models. However, it seems that to be able to predict sales for the 25th month, i need data for dependent variables (Income,Inflation, Interest Rates etc) for the month (25). </p>

<p>Can I create a model using lagged values of the dependent and independent variables, used together in a regression model? I'm not sure if that makes sense.</p>

<p>This is my first time series model and im not sure if i am on the right track. Please advise.</p>
"
"0.101885341621699","0.102597835208515","121408","<p>I have average life expectancy at birth data for an 8 year period and I would like to use that 8 year period to predict the trend for average life expectancy for the next 5 years. I would then like to ask whether this deviates significantly from the actual average life expectancy over the next 5 years.</p>"
"NaN","NaN","<ol>",""
"NaN","NaN","<li>What's the best regression model to fit to the observation base data in order to get predictions for next 5 years?</li>",""
"NaN","NaN","<li>How can I assess whether the difference between the predicted and observed trend is significant?</li>",""
"NaN","NaN","<li>How can I implement #1 and #2 in R?</li>",""
"NaN","NaN","</ol>",""
"NaN","NaN","","<r><regression><time-series><forecasting><life-expectancy>"
"0.131533410441164","0.132453235706504","225827","<p>G'day,</p>

<p>I have a R script below which I run twice.</p>

<pre><code>library(caret)
library(deepnet)

features &lt;- read.csv(""https://docs.google.com/spreadsheets/d/1vVVjvMLSBSHbky2C2eSw_NMZNjDB74Pzj-_JaggeVHw/export?format=csv"", na.strings=c(""."", ""NA"", """", ""?""), strip.white=TRUE, encoding=""UTF-8"") 

mySummary &lt;- function (data, lev = NULL, model = NULL) {
positions &lt;- ifelse(data[ , ""pred""] &gt; 0.0, 1, ifelse(data[, ""pred""] &lt; 0.0, -1, 0))
res &lt;- positions*data[, ""obs""]
criteria &lt;- mean(res)
names(criteria) &lt;- 'criteria'
return(criteria)
}

myTrainControl &lt;- trainControl(method = 'timeslice', fixedWindow = TRUE, initialWindow = 1000, horizon = 1, 
    summaryFunction=mySummary, selectionFunction = ""best"", returnResamp = 'final',  savePredictions = 'final')

myGrid &lt;- expand.grid(.layer1 = c(2:3), .layer2 = c(0:1), .layer3=c(0:1), .hidden_dropout=c(0, 0.1, 0.2), .visible_dropout=c(0, 0.1, 0.2))
#myGrid &lt;- expand.grid(.layer1 = 2, .layer2 = 0, .layer3=0, .hidden_dropout=0.2, .visible_dropout=0.0)

set.seed(503)
model &lt;- train( features[, c(1:5)], features[, 6], method = ""dnn"", trControl = myTrainControl, tuneGrid = myGrid )
criteria &lt;- model$resample$criteria
plot(cumsum(criteria), type = 'l', col = 'blue')
#end
</code></pre>

<p>First time I run it with</p>

<pre><code>myGrid &lt;- expand.grid(.layer1 = c(2:3), .layer2 = c(0:1), .layer3=c(0:1), .hidden_dropout=c(0, 0.1, 0.2), .visible_dropout=c(0, 0.1, 0.2))
</code></pre>

<p>It returns a growing chart which is good for me.
<a href=""http://i.stack.imgur.com/Ln2Nk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ln2Nk.png"" alt=""enter image description here""></a></p>

<p>For the second run I hardcode optimal model parameters returned by a model from first call (The final values used for the model were layer1 = 2, layer2 = 0, layer3 = 0, hidden_dropout = 0.2 and visible_dropout = 0)   </p>

<pre><code>myGrid &lt;- expand.grid(.layer1 = 2, .layer2 = 0, .layer3=0, .hidden_dropout=0.2, .visible_dropout=0.0):
</code></pre>

<p>This time I get completely different chart, not even close to the first. Both charts are consistent over runs.</p>

<p><a href=""http://i.stack.imgur.com/T4oKd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T4oKd.png"" alt=""enter image description here""></a></p>

<p>Shouldn't the charts be alike? What am I doing wrong?</p>

<p>Code is runnable. CSV dataset contains 1100 records (1000 for training and 100 for validation) and stored at google drive. First run takes a couple of minutes, second run takes a few seconds.</p>

<p>Thank you for your help!</p>
"
"0.101885341621699","0.102597835208515","185033","<p>I have a regression (time series) problem </p>

<p>$$y_t = w_1x_t+w_2z_t$$</p>

<p>Now, when I include a time-1 lag in this problem, so that the equation becomes </p>

<p>$$y_t = w_1x_t+w_2z_t+w_3x_{t-1}+w_4z_{t-1}$$</p>

<p>my $R^2$ values go up a little bit, but further including a time-2 lag:</p>

<p>$$y_t = w_1x_t+w_2z_t+w_3x_{t-1}+w_4z_{t-1}+w_5x_{t-2}+w_6z_{t-2}$$</p>

<p>completely ruins the model: Multiple $R^2$ is equal to $1$, but Adjusted $R^2$ is NaN (in R), all Std. Errors, etc. are NA. Also, when I use the learned models for prediction, the first model is slightly better than the lag-1 model, which is immensely better than the lag-2 model (the lag-2 model predicts nonsense).</p>

<p>What could be the reason for this (other than some implementation error on my side)? Too few data points to estimate the model (I have only 11 observations, but quite a few variables: The lag-0 model has about 20 variables, then there are 40 for the lag-1 model, and 60 for the lag-2 model)? Or is this an instance of multicollinearity? Does this indicate that the lag-0 model is most suitable? </p>
"
"0.0588235294117647","0.0592348877759092","103670","<p>I have data in the form of these columns:</p>

<pre><code>date, x coordinate, y coordinate, value A, value B, value C, value D, etc. 
</code></pre>

<p>(I don't see the possibility to copy an Excel table into this text so I show just the headers of the columns)</p>

<p>Now I have read and Googled a lot already but I can't find the code to make a regression model in R predict value A (numerical) based on the date, coordinates and the rest of the values (both numerical and text, no negative or complex numbers). Does anyone knows which code in R should work best? </p>

<p>I already get nice regressions with the simple <code>lm()</code> function if I stick to years as a label instead of dates but then I of course lose the time-connection. For background its maybe handy to know that it is real estate data.  </p>
"
"0.0588235294117647","0.0592348877759092","204132","<p>i'm using a selvaggio model to explain the behavior of deposits in a bank's data, and i need to use the estimated parameters, 
the problem is the heteroskedasticity that i detectect with breusch-pagan test 
        bptest(model)
and when i'm trying to use the correction with what ever methods, sandwick for example. i get the same estimated coefficients .
        coeftest(model, vcov = sandwich) 
what should i do ? </p>
"
"0.101885341621699","0.102597835208515","124129","<p>I would like to fit the following model <code>Y (t) = m (t) + b * t + g * C (t) + N (t)</code> with m (t) to be the long term mean monthly values (remove seasonal component), b the trend coefficient, C to be the matrix of explanatory variables, and N (t) the error term being AR (1).</p>

<p>I would like to ask you if this model is the same as the following:</p>

<p><code>Y (t) - m (t) = b * t + g * C (t) + N (t)</code>. Forced to have 0 intercept term, or I should also substract the mean from my regressors also.</p>

<p>Moreover, I would like to know if you can propose how this could be implemented, preferably in Matlab, or secondly in R.</p>

<p>I am not familiar with this kind of models yet, so thanks in advance, all help is very much appreciated.</p>
"
"0.0831890330807703","0.0837707816583391","124690","<p>Right now I am working with vector autoregressive models in order to make 3 months forecasts for a commodity good (sawlogs) y. I have several time-series of ""follow-up-products"" of sawlogs that should work as ""predictors"" for saw-log prices from a logical point of view. 
I encountered within the VAR-function from package ""vars"" (<a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a>), that one attribute called ""type"" has the following expressions: ""const"", ""both"", ""trend"", ""none"". I really don't know what this means from a statistical point of view.</p>

<p>Since neither the package-description nor other literature I've screened so far can give me an answer I actually understand I'd like to ask you guys the following:</p>

<p>How should I interpret/understand and use the argument ""type"" in R's VAR() Function?</p>

<p>What do those 4 different arguments really mean? ""both"", ""none"", ""trend"", ""constant""?
Could anyone explain this in a simple way and probably provide an example as well?</p>

<p>Does this mean that I can directly use non-stationary time series for my VAR-model since I can consider trend/season afterwards by setting the ""type-argument"" to both, or am I wrong here?</p>
"
"0.166378066161541","0.167541563316678","124700","<p>I am trying to create a linear regression model containing two predictors and 1 response variable. My response variable has a short term pattern, i.e. surge during weekdays and slump during weekends and I suspect this pattern is a result of two things: 
1) A natural trend - people are more active on weekdays and 
2) Partially related to my independent variables which follows a similar pattern.</p>

<p>There is also lagged cross-correlation between predictor and response.</p>

<p>Should I take some steps to normalize the data before running a linear regression? I've been reading about detrending time series, ARIMA, moving averages etc. but am a little lost on the right approach. Attached below are are time series plots of the predictor and response and the lagged cross correlation.</p>

<p><img src=""http://i.stack.imgur.com/NNkB1.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/cxYFk.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/zBzmC.jpg"" alt=""enter image description here""></p>
"
"0.275906809401378","0.252578410141361","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"0.0831890330807703","0.0837707816583391","80538","<p>I have data for 4 years with variables year, month and sales volume only. I want to find the base volume and incremental volume. I want to use Marketing Mix Modeling. Can anybody tell how to define model equation with these variables and how can I use SPSS/R for the same ?</p>

<p>Is it possible to build model with only these variables?</p>

<p>Is there any other way or statistical method to find these volume ?</p>

<p>Please reply...</p>

<p>Thanks in advance....</p>
"
"0.0831890330807703","0.0837707816583391","40749","<p>I have a linear model (with seasonal dummy variables) that produces monthly
forecasts. I'm using R together with the 'forecast' package:</p>

<pre><code>require(forecast)
model = tslm(waterflow ~ rainfall + season, data = model.df, lambda = lambda)
forec = forecast(model, newdata = rainfall.df, lambda = lambda)
</code></pre>

<p>I did a cross-validation and it looks great. Now, what i need is to generate
<em>weekly data points</em> from these month forecasts - in other words, i need to generate a synthetic time-series that have monthly means equal to the forecasts above. So my function would look like:</p>

<pre><code>generate.data = function(monthly.means, start.date, end.date)
{
   #code here
}
</code></pre>

<p>I'm not sure how to do this (interpolation?), so any help is welcome.
Thanks!</p>
"
"NaN","NaN","41319","<p>Suppose we are given a matrix of many rows (different genes for example) and few columns (different time points) and we want to identify the top rows (genes) that are following a trend, like monotonically increasing or decreasing. What is a good method to perform this in R?</p>

<p>The answers providing also a statistical significance result (something like p-value) are more than welcome. Finally the methods to indicate more complicated cases - such as up-then-down treands - are really perfect.</p>
"
"0.117647058823529","0.118469775551818","125152","<p>i would like your help to implement this model in R</p>

<p><img src=""http://i.stack.imgur.com/dGwpe.gif"" alt=""enter image description here""></p>

<p>or more explicity</p>

<p><img src=""http://i.stack.imgur.com/ff5iD.gif"" alt=""enter image description here""></p>

<p>where</p>

<ul>
<li>yt = monthly mean values</li>
<li>Î¼i = mean value in month i, i = 1 . . . 12 .</li>
<li>I1;t = Indicator series for month i of the year, i.e., 1 if the month
corresponds to month i of the year, and 0 otherwise.</li>
<li>bi = Trend in month i of the year.</li>
<li>Rt = t/12</li>
<li>Z1;t= Regressor 1, with c1 the associated coefficient.</li>
<li>Z2;t= Regressor 2, with c2 the associated coefficient.</li>
<li>Nt = Residual noise series, modeled as an autoregressive AR(1) series</li>
</ul>

<p>so i would like to get 12 coefficients for monthly and trend component and 1 for regressors 1 and 2.</p>

<p>Thanks in advance, all help is very much appreciated.</p>
"
"0.0831890330807703","0.0837707816583391","24193","<p>I am performing a returns analysis. The idea is to regress a time-series of returns on the returns of various asset classes. The beta coefficients must be constrained such that sum of the coefficients is 1 and no coefficient is less than 0 or greater than 1. These beta coefficients can then be interpreted as explaining what % of returns are explained by exposure to the various asset classes.</p>"
"NaN","NaN","<p>Are there any packages in R that let me setup the above regression and benefit from the attendant reporting on model fit statistics? Or do I need to do some homework on setting up constrained least squares optimization in R (please provide any references to recommended R packages)?</p>",""
"NaN","NaN","","<r><time-series><multiple-regression><optimization>"
"NaN","NaN","145113","<p>My plots of conditional weighted residuals (CWRES) plotted against time show some sort of time trend (image attached). The response variable is on a Box_cox scale. How could I solve this problem ? <img src=http://i.stack.imgur.com/kIepS.png alt=enter image description here></p>"
"NaN","NaN","","<r><regression><time-series><residuals><model>"
"0.0588235294117647","0.0592348877759092","24445","<p>I'm trying to estimate a multiple linear regression in R with an equation like this:</p>

<pre><code>regr &lt;- lm(rate ~ constant + askings + questions + 0)
</code></pre>

<p>askings and questions are quarterly data time-series, constructed with <code>askings &lt;- ts(...)</code>.</p>

<p>The problem now is that I got autocorrelated residuals. I know that it is possible to fit the regression using the gls function, but I don't know how to identify the correct AR or ARMA error structure which I have to implement in the gls function. </p>

<p>I would try to estimate again now with,</p>

<pre><code>gls(rate ~ constant + askings + questions + 0, correlation=corARMA(p=?,q=?))
</code></pre>

<p>but I'm unfortunately neither an R expert nor an statistical expert in general to identify p and q.</p>

<p>I would be pleased If someone could give me a useful hint.
Thank you very much in advance!</p>

<p>Jo</p>
"
"0.131533410441164","0.132453235706504","82509","<p>I have time series data on fish catches from 1950-2011. </p>

<p>I wish to implement a regression model with varying coefficients. I'm aware that cox models etc. exist and implementation via the <code>survival</code> package in R. My data is not survival data, it's just several variables with fish catches and year.</p>

<p>Is there a way in R to implement such models? I've yet to come across this but I don't think it's unreasonable to want to model such data without it being survival data.</p>

<p>I want to model <code>inlandfao</code> from <code>marinefao</code>. </p>

<p>Here is my data and some plots:</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)

require(reshape2)
require(ggplot2)
theme_set(theme_bw())
require(scales)

df2 &lt;- data.frame(cbind(year,totalmarinefao, totalinlandfao))
df2
dd &lt;- melt(df2, id.vars = ""year"")
dd
pp &lt;- ggplot(dd, aes(year, value, colour=variable)) + geom_point() + geom_line(size=1)
pp_final &lt;- pp +  xlab(""Year"") + ylab(""Catches (Tons)"") + ggtitle(""Time Series of Variables (1950-2011)"") 
pp_final
pp_final2 &lt;- pp_final +  scale_colour_discrete(name = ""Variable - Catches (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""),
                                               labels=c(""Marine"", ""Inland"")) + 
  scale_shape_discrete(name = ""Variable (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""), labels=c(""Marine"", ""Inland"")) + 
  scale_x_continuous(breaks=seq(1950,2011,10)) + scale_y_continuous(labels=comma)

pp_final2
pp_3 &lt;- pp_final2 + theme(axis.text.x  = element_text(vjust=1, size=16)) + theme(axis.title.x = element_text(size=20))
pp_4 &lt;- pp_3 + theme(axis.text.y = element_text(vjust=0, size=16)) + theme(axis.title.y = element_text(size=20, vjust=0.2))
pp_5 &lt;- pp_4 + theme(plot.title = element_text(lineheight=.8, face=""bold"", size=20))
pp_5

qplot(marinefao, inlandfao, data=fishdata, main=""Scatterplot of the Marine &amp; \n Inland 
fish Catches (Tons)"", xlab=""Marine Catches"", ylab=""Inland Catches"") + 
scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma)
</code></pre>

<p>From these plots, a linear model isn't appropriate. I have fitted GAMs etc. to these data. </p>

<p>Let me more if you require details.</p>
"
"0.0831890330807703","0.0837707816583391","153033","<p>I'm trying to create model for consumer loan defaults that incorporates individuals payment behavior as time series. Typically this kind of problem is modeled using Cox/Allen model.</p>

<p>Then, the other day, I came across this paper: <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F4210-learning-patient-specific-cancer-survival-distributions-as-a-sequence-of-dependent-regressors.pdf&amp;ei=ojhbVb7gHoe4ggTTwoGYBA&amp;usg=AFQjCNHVEg6lOEFIV4WaZEwqnXwlez_wpA&amp;sig2=vA-EFCEDOzx2m1BGI_PTyg&amp;bvm=bv.93564037,d.eXY"" rel=""nofollow"">Learning Patient-Specific Cancer Survival
Distributions as a Sequence of Dependent Regressors</a></p>

<p>Basically authors propose to fit multiple logits with their own parameters at each time step  while enforcing smoothness of regression parameters over time. Also maximum likelihood objective function is modified such that model naturally enforces consistency of predicted event(i.e there is no come back after fatal event)</p>

<p>I was wondering if anyone is aware of R implementation of similar problem?</p>
"
"0.144087631928422","0.145095250022002","60500","<p>I want to assume that the sea surface temperature of the Baltic Sea is the same year after year, and then describe that with a function / linear model. The idea I had was to just input year as a decimal number (or num_months/12) and get out what the temperature should be about that time. Throwing it into lm() function in R, it doesn't recognize sinusoidal data so it just produces a straight line. So I put the sin() function within a I() bracket and tried a few values to manually fit the function, and that gets close to what I want. But the sea is warming up faster in the summer and then cooling off slower in the fall... So the model is wrong the first year, then gets more correct after a couple of years, and then in the future I guess it becomes more and more wrong again. </p>

<p>How can I get R to estimate the model for me, so I don't have to guess numbers myself? The key here is that I want it to produce the same values year after year, not just be correct for one year. If I knew more about math, maybe I could guesstimate it as something like a Poisson or Gaussian instead of sin(), but I don't know how to do that either. Any help to get closer to a good answer would be greatly appreciated.</p>

<p>Here is the data I use, and the code to show results so far:</p>

<pre><code># SST from Bradtke et al 2010
ToY &lt;- c(1/12,2/12,3/12,4/12,5/12,6/12,7/12,8/12,9/12,10/12,11/12,12/12,13/12,14/12,15/12,16/12,17/12,18/12,19/12,20/12,21/12,22/12,23/12,24/12,25/12,26/12,27/12,28/12,29/12,30/12,31/12,32/12,33/12,34/12,35/12,36/12,37/12,38/12,39/12,40/12,41/12,42/12,43/12,44/12,45/12,46/12,47/12,48/12)
Degrees &lt;- c(3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5,3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5,3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5,3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5)
SST &lt;- data.frame(ToY, Degrees)
SSTlm &lt;- lm(SST$Degrees ~ I(sin(pi*2.07*SST$ToY)))
summary(SSTlm)
plot(SST,xlim=c(0,4),ylim=c(0,17))
par(new=T)
plot(data.frame(ToY=SST$ToY,Degrees=8.4418-6.9431*sin(2.07*pi*SST$ToY)),type=""l"",xlim=c(0,4),ylim=c(0,17))
</code></pre>
"
"0.144087631928422","0.145095250022002","25702","<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>

<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>

<p>(i) With an intercept only</p>

<p>$$\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\overline{Y}_{t-1} + d_i\Delta\overline{Y}_{t-1}+ e_i\Delta\overline{Y}_{t-2}+ f_i\Delta\overline{Y}_{i,t-1}+ g_i\Delta\overline{Y}_{i,t-2} + \varepsilon_{i,t}$$</p>

<p>where $\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>

<p>I constructed each of these variables in the following way:</p>

<p>$\Delta Y_t$</p>

<pre><code>dif.yt = diff(yt) 
## yt is the object containing all the observations for a specific country 
## (e.g. Australia)
</code></pre>

<p>$Y_{t-1}$</p>

<pre><code>yt.lag.1 = lag(yt, -1)
</code></pre>

<p>$\overline{Y}_{t-1}$</p>

<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) 
## x is the object containing my entire data frame
</code></pre>

<p>$\Delta \overline{Y}_{t-1}$</p>

<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)
</code></pre>

<p>$\Delta \overline{Y}_{t-2}$</p>

<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))
</code></pre>

<p>$\Delta Y_{t-1}$</p>

<pre><code>dif.yt.lag.1 = diff(yt.lag.1)
</code></pre>

<p>$\Delta Y_{t-2}$</p>

<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)
</code></pre>

<p>After constructing each variable individually, I then run the linear regression</p>

<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + 
                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)
summary(reg)
</code></pre>

<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>

<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>
"
"0.212091251497882","0.213574425172396","60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"NaN","NaN","109813","<p>I'm trying to analyze if there are correlations between binomial dataset. I have binomial data (presence/absence) of two variables in different periods and I need to know what is the best way to find some relationship. Data were taken annually from the same samples.</p>"
"NaN","NaN","","<r><regression><time-series><probability>"
"NaN","NaN","25316","<p>How would we measure the predictive power of predictors in time series models. For e.g. in linear regression we have the magnitude and direction of the regression co-efficients and their p-values.</p>"
"NaN","NaN","<p>Is there any measure like that to evaluate the performance of predictors in kalman filter?</p>",""
"NaN","NaN","","<r><regression><time-series><forecasting><kalman-filter>"
"0.0588235294117647","0.0592348877759092","168717","<p>this is my first post.  I have an irregular time series that exhibits large shifts in both mean and in the direction of the trend.  It looks something like this (though this is far cleaner than reality):</p>

<pre><code>set.seed(1)
x = sort(rep(seq(as.POSIXct('2015/01/01'),as.POSIXct('2015/01/10'),by='day'),100) + runif(1000,0,80000))
y = c(seq(300),seq(90,70,-.2),seq(20,50,.1),seq(238.5,90,-.5)) + runif(1000,50,80)
plot(x,y)
</code></pre>

<p>The task is to:
 1. accurately partition/segment the data
 2. extract the change point indices
 3. fit regression separately for each segment</p>

<p>I have tried several routes, including:
a) hierarchical clustering based on dissimilarity matrix
b) function cpt.mean/var/meanvar from package changepoint (does not seem to work well)
c) function 'breakpoints' from package strucchange (slow and often inaccurate)
d) various types of kmeans (inappropriate, I know)</p>

<p>I have also explored some other packages, such as TSclust, urca, and DTW, but these seem better suited to clustering <em>sets</em> of time-series, not individual values within a time series.</p>

<p>Can someone point me in the correct direction? Perhaps I am not considering the appropriate data model?</p>

<p><strong>UPDATE</strong>
Thank you all for your very considered responses.  I went back to the strucchange package, and after some fiddling, have gotten it to work quite well.  I had not initially appreciated the h 'minimal segment size' argument.
Finished product:
<a href=""http://i.stack.imgur.com/qBgeY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBgeY.jpg"" alt=""enter image description here""></a></p>
"
"NaN","NaN","137094","<p>I'm currently using Twitter's AnomalyDetection in R: <a href=https://github.com/twitter/AnomalyDetection>https://github.com/twitter/AnomalyDetection</a>.  This algorithm provides time series anomaly detection for data with seasonality.</p>"
"NaN","NaN","<p>Question: are there any other algorithms similar to this (controlling for seasonality doesn't matter)?</p>",""
"NaN","NaN","<p>I'm trying to score as many time series algorithms as possible on my data so that I can pick the best one / ensemble.</p>",""
"NaN","NaN","","<r><regression><time-series>"
"NaN","NaN","25889","<p>I have a few tens of thousands of observations that are in a time series but grouped by locations. For example:</p>

<pre><code>location date     observationA observationB
---------------------------------------
 A       1-2010   22           12
 A       2-2010   26           15
 A       3-2010   45           16
 A       4-2010   46           27
 B       1-2010   167          48
 B       2-2010   134          56
 B       3-2010   201          53
 B       4-2010   207          42
</code></pre>

<p>I want to see if month <em>x</em>'s <code>observationA</code> has any linear relationship with month <em>x</em>+1's <code>observationB</code>.</p>

<p>I did some research and found a <code>zoo</code> function, but it doesn't appear to have a way to limit the lag by group. So if I used zoo and lagged <code>observationB</code> by 1 row, I'd end up with the location A's last <code>observationB</code> as location B's first <code>observationB</code>. I'd rather have the first <code>observationB</code> of any location to be <code>NA</code> or some other obvious value to indicate ""don't touch this row"".</p>

<p>I guess what I'm getting at is whether there's a built-in way of doing this in R? If not, I imagine I can get this done with a standard loop construct. Or do I even need to manipulate the data?</p>
"
"0.101885341621699","0.102597835208515","153560","<p>I'm currently working on power plant time series data and my main objective is finding out the optimal combination of independent variables which would keep ""SO2 concentration (dependent variable) below a threshold of 200mg/m3. For some clarity, some of the independent variables I'm working on include: Power, Number of Pumps, Heat Value, Dust, % Calcium in Coal etc. So, a good insight for example would be the optimal amount of power (in combination with other variables) required to keep ""SO2 conc"" under the above threshold. </p>

<p>I've been working on fitting regression models to the data and examining variable importance to find out how influential the predictors are to ""SO2 conc"". However, this doesn't really address my objective.</p>

<p>I'm using R to work on this but I'm not really sure what statistical problem this is as it seems to require more than just regression modelling. Does anyone have any ideas on how to approach this?</p>
"
"NaN","NaN","56451","<p>How do I get better forecasts for my model?  Is the plot supposed to look like this? I am using the code:</p>

<pre><code>fit &lt;- auto.arima(blah)
fcast &lt;- forecast(fit,100)
</code></pre>

<p><img src=""http://i.stack.imgur.com/TeiGU.png"" alt=""enter image description here""></p>
"
"0.0831890330807703","0.0837707816583391","234220","<p>I am new to time series analysis. I am trying to use the R package dlm for time-varying regression with multiple regressors. I got the basic dlmModReg to work, so I can get the coefficients for all the regressors as a function of time. My question is, what's the correct way to estimate the relative importance of each regressor at each time point (or within some window)? Thank you!</p>
"
"0.0588235294117647","0.0592348877759092","234446","<p>I have a time series $Y_t$ that is stationary, and several explanatory variables $X$ .. $Z$ (stationary as well). </p>

<p>Is there an R package (or Python one) that can automatically fit all the possible predictive regressions of the form</p>

<p>$Y_t$ = a + b * L(X) + c * L(Y) + d * L(Z) + epsilon</p>

<p>where indicates the vector of lagged X valuesm ie L(X) = ($X_{t-1}$, $X_{t-2}$, ..). The package should tell me automatically (for a given max lag of course) which specification has the best in-sample prediction accuracy (by eventually dropping some of the predictive variables)?</p>

<p>Thanks!</p>
"
"0.0588235294117647","0.0592348877759092","193435","<p>I have a model that I need to estimate,</p>

<p><a href=""http://i.stack.imgur.com/WPJZK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WPJZK.jpg"" alt=""enter image description here""></a> 
where </p>

<p><a href=""http://i.stack.imgur.com/Qv7L1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qv7L1.jpg"" alt=""enter image description here""></a></p>

<p>I've seen a similar example  (<a href=""http://stats.stackexchange.com/questions/41168/constrained-regression-in-r-coefficients-positive-sum-to-1-and-non-zero-interc"">Constrained Regression in R: coefficients positive, sum to 1 and non-zero intercept</a>) but without the second part of the regression, i.e. the relationship with betas and theta0. I have not been able to modify that solution. </p>

<p>The model comes from MSCI Private and Public Real Estate - What's the link ( June 2010). My intention is to use this model for other illiquid assets as well.</p>
"
"0.117647058823529","0.118469775551818","187777","<p>I have a time series that count the number of ""type 1"" events in a city, for each day. The serie contains a lot of zeros because type 1 events are rare (about 80% of counts are zeros).
I'm using a Poisson Model but I don't know how to handle temporal dependencies. For example, I know that there are some other events (let say ""type 2"") which will increase the probability of an event of type 1 in the current day and/or in the next days. The Poisson parameter is not constant over time.</p>

<p>Do you know a good R package to handle this and a good way to model this situation ?</p>

<p>Thanks</p>
"
"0.186016332951081","0.187317162316339","209874","<p>I have a model fitted with <code>auto.arima</code>, the model is ARIMA(0,1,0)x(0,1,0)[6] with seasonal period 6. The data is bi-monthly so there is an annual seasonality. There is only one regressor indicating an intervention (dummy). </p>

<p>Then I used this model to old data to see what would have happened if the intervention would have done since and earlier period, using the model and forecast from an earlier data. <strong>The thing I do not understand yet</strong> is that if I suppose the intervention only occur in one period, the series only differ in this period. Therefore, there is no persistence on the intervention.</p>

<p>As I understand, the model has ARIMA errors. The error in the intervention period should change and so there should be an effect in the next periods when using forecast to predict futures values. If the intervention occurs in only one period, <strong>why</strong> in the forecast the intervention does not affect futures predictions?</p>

<hr>

<p>EDIT:</p>

<p>The code I am using is</p>

<pre><code>model1&lt;-auto.arima(ts,xreg = X.ts)
</code></pre>

<p>Where <code>X.ts</code> is a <code>ts</code> object with <code>0</code> and a period with intervention. </p>

<p>Then I used </p>

<pre><code>model2&lt;-Arima(Xold, xreg= X.ts.old, model=model1)
</code></pre>

<p>So I used the first model on earlier data to make the following</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>So I am trying to show what would have been expected from an earlier period (the forecast) if the intervention would have started earlier.</p>

<p>The thing I do not understand yet is that for instance</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,0...))
forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>only differ in the periods the <code>xreg</code> differ, with no persistence of these differences. I did not expect this, <strong>why is that?</strong></p>
"
"0.0831890330807703","0.0837707816583391","111103","<p>I'm trying to apply R output to generate a scenario using external data, I'm not sure how exactly to use the coefficients in each from the R output.</p>

<p>I have an ARMAX(1, 1) model</p>

<ul>
<li>Coefficient of AR1: $A$</li>
<li>Coefficient of MA1: $M$</li>
<li>Intercept = $k$</li>
<li>Coefficient of external regressors: $B_1$, $B_2$</li>
<li>External regressors: $X_1$, $X_2$</li>
<li>Y actual = $Y_a$</li>
<li>Y predicted = $Y_p$</li>
</ul>

<p>The formula I'm using is $Y_p = k + A*(Y_a(t-1)-k) - M*(Y_a(t-1) - Y_p(t-1)) + B_1*(X_1(t) - X_1(t-1)) + B_2*(X_2(t) - X_2(t-1))$</p>

<p>Is there anything wrong with the formula I'm using?
I'm asking because of two things, one is that while AIC is much better for this model, the sum of squared errors is actually higher than if I just used AR(1) model; another thing is that when I adjust the AR term, the sum of squared residuals gets reduced, but I thought the model is supposed to minimize the sum of squared residuals for each of the terms?</p>

<p>I'm using the same data set for generating the model and to fit the model, I need to test  to see how much improvement this model offers compared to simpler models. I'm doing the fitting in Excel.</p>

<p>Any help would be greatly appreciated, and if additional information is needed to answer this please ask.</p>

<p>This is my R output</p>

<pre><code>ARMA11R2R30


Call:
arima(x = YieldReOLD[, 5], order = c(1, 0, 1), xreg = cbind(YieldReOLD[, 2], 
    YieldReOLD[, 4]))

Coefficients:
         ar1      ma1  intercept  cbind(YieldReOLD[, 2], YieldReOLD[, 4])1
      0.9872  -0.1970    -6.2862                                   -0.1867
s.e.  0.0072   0.0489     0.3743                                    0.0566

      cbind(YieldReOLD[, 2], YieldReOLD[, 4])2
                                       -0.3999
s.e.                                    0.1140
</code></pre>
"
"NaN","NaN","157142","<p>A couple years ago I performed a linear regression on data that looked like this:</p>"
"NaN","NaN","<pre><code>   company year     y       x1      x2      x3      x4",""
"NaN","NaN","1        A 2012  1.83  34811.8 14755.5   278.2     0.0",""
"NaN","NaN","2        B 2012  3.87  10435.5  9692.6   522.2   317.9",""
"NaN","NaN","3        C 2012 19.76 199670.6 23428.7 10675.5  2815.8",""
"NaN","NaN","4        D 2012  1.22   3204.4  2087.5  2282.8  2804.1",""
"NaN","NaN","5        E 2012  0.00      5.2    53.5     0.2   193.8",""
"NaN","NaN","6        F 2012  0.81 161936.0 25777.9  2364.8   540.6",""
"NaN","NaN","7        G 2012  1.22   1479.3    28.6     0.4     3.9",""
"NaN","NaN","8        H 2012  2.24   9716.3   888.2  2073.9  1059.1",""
"NaN","NaN","9        I 2012 25.25 331396.9 15162.0 87062.1 32724.7",""
"NaN","NaN","10       J 2012  0.20   9812.0 10363.4    49.9 36664.9",""
"NaN","NaN","11       K 2012  1.02  62715.3  5746.5  1007.7   866.3",""
"NaN","NaN","12       L 2012  3.87 121397.5  5842.2  1481.6   621.0",""
"NaN","NaN","13       M 2012 12.22 243189.5 50370.8 16747.1 23025.8",""
"NaN","NaN","14       N 2012 18.33 147305.6 87916.3 15098.3 16449.7",""
"NaN","NaN","15       O 2012  0.61  20699.1  8345.6     0.0    26.4",""
"NaN","NaN","16       P 2012  2.44  30735.1  1840.6  4900.1     0.0",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>Each row is a different company and the main objective was to interpret the coefficients. Its been 3 years since that regression and I want to look at it again with data from each year so the dataset would look like this:</p>",""
"NaN","NaN","<pre><code>   company year     y       x1      x2       x3      x4",""
"NaN","NaN","1        A 2012  1.83  34811.8 14755.5    278.2     0.0",""
"NaN","NaN","2        B 2012  3.87  10435.5  9692.6    522.2   317.9",""
"NaN","NaN","3        C 2012 19.76 199670.6 23428.7  10675.5  2815.8",""
"NaN","NaN","4        D 2012  1.22   3204.4  2087.5   2282.8  2804.1",""
"NaN","NaN","5        E 2012  0.00      5.2    53.5      0.2   193.8",""
"NaN","NaN","6        F 2012  0.81 161936.0 25777.9   2364.8   540.6",""
"NaN","NaN","7        G 2012  1.22   1479.3    28.6      0.4     3.9",""
"NaN","NaN","8        H 2012  2.24   9716.3   888.2   2073.9  1059.1",""
"NaN","NaN","9        I 2012 25.25 331396.9 15162.0  87062.1 32724.7",""
"NaN","NaN","10       J 2012  0.20   9812.0 10363.4     49.9 36664.9",""
"NaN","NaN","11       K 2012  1.02  62715.3  5746.5   1007.7   866.3",""
"NaN","NaN","12       L 2012  3.87 121397.5  5842.2   1481.6   621.0",""
"NaN","NaN","13       M 2012 12.22 243189.5 50370.8  16747.1 23025.8",""
"NaN","NaN","14       N 2012 18.33 147305.6 87916.3  15098.3 16449.7",""
"NaN","NaN","15       O 2012  0.61  20699.1  8345.6      0.0    26.4",""
"NaN","NaN","16       P 2012  2.44  30735.1  1840.6   4900.1     0.0",""
"NaN","NaN","17       A 2013  0.20   4832.1 10691.6      0.6     0.0",""
"NaN","NaN","18       B 2013  3.02  12575.8  1270.3    106.6   368.0",""
"NaN","NaN","19       C 2013 16.00 184628.5 38269.7   5343.1  4645.6",""
"NaN","NaN","20       D 2013  1.76   4684.6  1445.2   2150.1  1727.0",""
"NaN","NaN","21       E 2013  1.27      4.3    22.9     38.3   314.6",""
"NaN","NaN","22       F 2013  0.39 141808.6 26368.8    673.6  2259.2",""
"NaN","NaN","23       G 2013  0.59    986.3    38.6      7.0     5.8",""
"NaN","NaN","24       H 2013  2.83  20111.4  3518.3    549.5    59.6",""
"NaN","NaN","25       I 2013 21.17 303925.9 20248.0 107366.7 19979.1",""
"NaN","NaN","26       J 2013  1.37   7792.8 16000.7     33.5 39541.7",""
"NaN","NaN","27       K 2013  1.66 141071.9 11136.1    162.2     0.0",""
"NaN","NaN","28       L 2013  3.80 130359.7  8882.5     40.5   520.8",""
"NaN","NaN","29       M 2013 10.63 280250.3 39029.7  16208.6 29284.3",""
"NaN","NaN","30       N 2013 19.41 145278.1 55141.6  14115.5  1783.4",""
"NaN","NaN","31       O 2013  0.98   1517.6  3610.4      0.0   547.3",""
"NaN","NaN","32       P 2013  3.32 101484.2  1140.5   5489.9     0.0",""
"NaN","NaN","33       A 2014  0.10      0.0  9520.7      0.9     0.0",""
"NaN","NaN","34       B 2014  4.02  14886.8  2331.5      0.0   631.8",""
"NaN","NaN","35       C 2014 14.22 143760.9 50222.1   6118.1  4342.1",""
"NaN","NaN","36       D 2014  0.88    936.1  1802.7   1273.6  4394.3",""
"NaN","NaN","37       E 2014  0.78    231.5    15.8     64.1   291.9",""
"NaN","NaN","38       F 2014  0.78 244303.2 29148.3   3161.4  4908.1",""
"NaN","NaN","39       G 2014  0.78   1032.6    30.3      1.3     7.8",""
"NaN","NaN","40       H 2014  2.55  26322.6 11726.1   2859.2     0.0",""
"NaN","NaN","41       I 2014 21.96 614241.5  9138.2  94273.7 17702.0",""
"NaN","NaN","42       J 2014  1.27   8946.5 13853.7    693.9 19672.0",""
"NaN","NaN","43       K 2014  1.18 164269.7  7088.1     29.7   825.0",""
"NaN","NaN","44       L 2014  2.35 107152.3  3275.2     94.7   490.9",""
"NaN","NaN","45       M 2014  8.73 284267.4 51896.4  12838.1 28019.5",""
"NaN","NaN","46       N 2014 20.69  84554.6 32341.0  11408.2   624.9",""
"NaN","NaN","47       O 2014  1.08      0.0  7663.2      0.0     0.0",""
"NaN","NaN","48       P 2014  3.63 109392.9  5229.2   4691.0    11.1",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>When I think about this dataset I don't immediately think its a time series but I also don't think I should be ignoring year all together and regressing it like so in R:</p>",""
"NaN","NaN","<pre><code>lm(y ~ x1 + x2 + x3 + x4)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>So I'm wondering how I should model this dataset. Should I just include dummy variables for year or are there better approaches here?</p>",""
"NaN","NaN","","<r><regression><time-series><categorical-data>"
"0.0831890330807703","0.0837707816583391","62597","<p>I am performing distributed non-linear lag models in R.</p>

<p>I got the figure result of <code>dlnm</code> as shown in the vignette (<a href=""http://cran.r-project.org/web/packages/dlnm/vignettes/dlnmOverview.pdf"" rel=""nofollow"">pdf</a>) on page 13:  </p>

<p><img src=""http://i.stack.imgur.com/Qjf7B.png"" alt=""enter image description here""></p>

<p>The X-axis is lag, which I can understand. However, I cannot understand what the label for the Y-axis means. For the figure on page 13, the Y-axis is called ""RR"" (relative risk) which is positive; however, I got a negative RR when I input my data. My $X$ is temperature, my $Y$ is pollutant concentration. Can I still use the model?</p>

<p>Could anybody explain it to me a little bit? </p>

<p>The R code that I am using is:</p>

<pre><code>OrigData &lt;- read.csv(""Hourly data for use SHULE-2.csv"", na.strings = ""."", 
                     header=T, sep="","")
CO2H6 = OrigData$CO2H6      # $
OutT  = OrigData$OutT       # $
Year  = OrigData$Year       # $
Month = OrigData$Month

argvar &lt;- list(type=""ns"", df=11)
arglag &lt;- list(type=""ns"", df=5)

suppressWarnings(cb1 &lt;-crossbasis(OutT, lag=12, argvar=argvar, arglag=arglag))
model1 &lt;- glm(CO2H6~ cb1 +as.factor(Month))
summary(model1)

cp &lt;- crosspred(cb1,model1,-22.83:34.79,by=1, cumul=TRUE)
par(cex.axis=0.7,cex.lab=0.7)
d3 &lt;- plot(cp, xlab=""Delay Effects of OutT in IN2H-H6"", zlab=""CO2 con."", r=90, 
           d=0.3, col=""red"" , xlab=""Out T"")

plot(cp, ""slices"", var=10, ci=""n"", ylim= c(-1000,1000), ylab=""CO2 con."", lwd=1.0, 
     xlim=c(0,10))
for(i in 4:6){
  lines(cp, ""slices"", var=c(-20,-10,0,10,20,30)[i], col=i+1, lwd=1.5)
}
legend(7,1000, col=5:7, lwd=0.9, pch=1:1, legend=c(""OutT =10â„ƒ"", ""OutT =20â„ƒ"", 
                                                   ""OutT =30â„ƒ""), cex=0.7)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ZBJ8K.jpg"" alt=""enter image description here""></p>

<p>Since RR cannot be negative, could there be a way for me to transfer my x(Temperature) and y (Pollutant con.) to make it fitting the requirement of the model?</p>

<p>Thanks for all your help!!</p>
"
"0.159623653011757","0.178599906592273","62646","<p>I've got data on mail volume sent by household for seven age groups, with 12 years of data for each age group. I originally ran a simple regression on each age group individually and realized I needed to dig deeper. My aim now is to pool the data (giving me 84 observations) and try to identify some period effects (or year effects, whichever you prefer). My pooled data are currently organized like this (PPHPY stands for Pieces per Household Per Year):</p>

<pre><code>Age Group    Year   PPHPY
1            2001   127.62
1            2002   144.47
1            2003   111.70
1            2004   95.96
1            2005   96.46
1            2006   139.91
1            2007   85.52
1            2008   75.43
1            2009   109.34
1            2010   53.16
1            2011   64.09
1            2012   50.94        
2            2001   176.48
2            2002   172.86
2            2003   137.79
.              .      .
.              .      .
.              .      .
7            2012   163.39
</code></pre>

<p>I first regressed PPHPY on year and year dummies (leaving the intercept as 0 to avoid perfect multicollinearity). This gave me period effects for the aggregated data (ie something like a period effect across all age groups, I think). This looked like the following:</p>

<pre><code>&gt; ## Generate YearDummy using factor()
&gt;
&gt; YearDummy &lt;- factor(YearVar)
&gt;
&gt; ## Check to see that YearDummy is indeed a factor variable
&gt;
&gt; is.factor(YearDummy)
[1] TRUE
&gt;
&gt; ## (...+0) ensures intercept is left out and thus YearDummy1 remains in.
    ## One or the other must be subtracted out to avoid perfect mutlicollinearity
&gt;
&gt; LSDVYear &lt;- lm(PPHPY ~ YearVar + YearDummy + 0, data=maildatapooled)
&gt; summary(LSDVYear)
Call:
lm(formula = PPHPY ~ YearVar + YearDummy + 0, data = maildatapooled)
Residuals:
Min 1Q Median 3Q Max
-99.658 -39.038 8.814 43.670 82.300
Coefficients: (1 not defined because of singularities)
Estimate Std. Error t value Pr(&gt;|t|)
YearVar 5.743e-02 9.851e-03 5.830 1.45e-07 ***
YearDummy2001 1.099e+02 2.795e+01 3.930 0.000193 ***
YearDummy2002 1.209e+02 2.796e+01 4.324 4.85e-05 ***
YearDummy2003 7.791e+01 2.797e+01 2.786 0.006819 **
YearDummy2004 8.053e+01 2.797e+01 2.879 0.005251 **
YearDummy2005 6.887e+01 2.798e+01 2.461 0.016236 *
YearDummy2006 6.572e+01 2.799e+01 2.348 0.021618 *
YearDummy2007 5.975e+01 2.799e+01 2.134 0.036210 *
YearDummy2008 5.836e+01 2.800e+01 2.084 0.040696 *
YearDummy2009 4.119e+01 2.801e+01 1.471 0.145745
YearDummy2010 3.056e+01 2.801e+01 1.091 0.278990
YearDummy2011 1.472e+01 2.802e+01 0.525 0.600951
YearDummy2012 NA NA NA NA
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 52.44 on 72 degrees of freedom
Multiple R-squared: 0.9316, Adjusted R-squared: 0.9202
F-statistic: 81.71 on 12 and 72 DF, p-value: &lt; 2.2e-16
</code></pre>

<p>What I want, however, is to tease out period effects for each age group individually. This is what I'm not sure how to set up. I was hoping someone might help me devise some code in R that would kick out those period effects for <em>each</em> of the seven age groups using the pooled data, as well as help me understand the problem conceptually. </p>

<p>EDIT: I forgot to mention that I see I must include an interaction term involving the time dummies to allow the coefficients to vary across age groups. I'm just having difficulty constructing the proper interaction term and resulting regression equation.</p>

<p>EDIT 2: I came up with two models and ran them. I felt like the question had evolved at this point and might merit a new post, which is can be found <a href=""http://stats.stackexchange.com/questions/62755/period-effects-in-pooled-time-series-data-in-r"">here</a>.</p>
"
"0.233045521593201","0.247026558326606","62755","<p>This is closely related to a question I asked yesterday but I've now got a much more complete answer on which I was hoping to get feedback. The previous question was just looking for conceptual advice and was very helpful. You can find the relevant data and introduction <a href=""http://stats.stackexchange.com/questions/62646/pooled-time-series-regression-in-r"">here</a>.</p>

<p>I wanted to find period effects for each age group. I've run two regressions using dummies as part of an interaction term. I'm hoping to see if my method is flawed and if my interpretation of the results is correct or not. They first regression is as follows:</p>

<pre><code>&gt; ## Generate YearDummy and AgeGroupDummy using factor()
&gt; 
&gt; YearDummy &lt;- factor(YearVar)
&gt; AgeGroupDummy &lt;- factor(AgeGroup)
&gt; 
&gt; ## Check to see that YearDummy and CohortDummy are indeed factor variables
&gt; 
&gt; is.factor(YearDummy)
[1] TRUE
&gt; is.factor(AgeGroupDummy)
[1] TRUE
&gt; ## Regress on AgeGroup and include AgeGroup*YearDummy interaction terms
&gt; 
&gt; PooledOLS1 &lt;- lm(PPHPY ~ AgeGroup + AgeGroup*YearDummy + 0, 
data=maildatapooled)
&gt; summary(PooledOLS1)
Call:
lm(formula = PPHPY ~ AgeGroup + AgeGroup * YearDummy + 0, data = 
maildatapooled)
Residuals:
 Min 1Q Median 3Q Max 
-38.852 -10.632 3.298 11.275 26.481 
Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|) 
AgeGroup 26.2212 3.5070 7.477 3.84e-10 ***
YearDummy1 119.8836 15.6840 7.644 1.99e-10 ***
YearDummy2 123.7458 15.6840 7.890 7.55e-11 ***
YearDummy3 103.2660 15.6840 6.584 1.28e-08 ***
YearDummy4 97.7102 15.6840 6.230 5.06e-08 ***
YearDummy5 103.3295 15.6840 6.588 1.26e-08 ***
YearDummy6 103.2330 15.6840 6.582 1.29e-08 ***
YearDummy7 84.8291 15.6840 5.409 1.16e-06 ***
YearDummy8 70.7114 15.6840 4.509 3.09e-05 ***
YearDummy9 90.9566 15.6840 5.799 2.65e-07 ***
YearDummy10 50.0885 15.6840 3.194 0.00224 ** 
YearDummy11 37.7004 15.6840 2.404 0.01933 * 
YearDummy12 33.1947 15.6840 2.116 0.03846 * 
AgeGroup:YearDummy2 1.8066 4.9597 0.364 0.71695 
AgeGroup:YearDummy3 -3.8022 4.9597 -0.767 0.44632 
AgeGroup:YearDummy4 -1.7436 4.9597 -0.352 0.72640 
AgeGroup:YearDummy5 -6.0494 4.9597 -1.220 0.22735 
AgeGroup:YearDummy6 -6.7992 4.9597 -1.371 0.17552 
AgeGroup:YearDummy7 -3.6752 4.9597 -0.741 0.46158 
AgeGroup:YearDummy8 -0.4799 4.9597 -0.097 0.92323 
AgeGroup:YearDummy9 -9.8190 4.9597 -1.980 0.05232 . 
AgeGroup:YearDummy10 -2.2452 4.9597 -0.453 0.65241 
</code></pre>

<p>My interpretation of the interaction term coefficients is that they represent the difference in slope of AgeGroup between the period of the corresponding YearDummy and the AgeGroup slope at the top of the results. This is kind of like the AgeGroup effect across different periods.</p>

<p>My second regression is as follows:</p>

<pre><code>&gt; ## Regress YearVar and Include YearVar*AgeGroupDUmmy
&gt; 
&gt; PooledOLS2 &lt;- lm(PPHPY ~ YearVar + YearVar*AgeGroupDummy + 0, 
data=maildatapooled)
&gt; summary(PooledOLS2)
Call:
lm(formula = PPHPY ~ YearVar + YearVar * AgeGroupDummy + 0, data = 
maildatapooled)
Residuals:
 Min 1Q Median 3Q Max 
-29.345 -9.325 -0.915 8.540 40.150 
Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|) 
YearVar -7.089 1.252 -5.664 3.04e-07 ***
AgeGroupDummy1 142.292 9.211 15.447 &lt; 2e-16 ***
AgeGroupDummy2 185.508 9.211 20.139 &lt; 2e-16 ***
AgeGroupDummy3 218.170 9.211 23.685 &lt; 2e-16 ***
AgeGroupDummy4 255.733 9.211 27.763 &lt; 2e-16 ***
AgeGroupDummy5 278.180 9.211 30.200 &lt; 2e-16 ***
AgeGroupDummy6 300.910 9.211 32.667 &lt; 2e-16 ***
AgeGroupDummy7 282.325 9.211 30.650 &lt; 2e-16 ***
YearVar:AgeGroupDummy2 -1.737 1.770 -0.981 0.3298 
YearVar:AgeGroupDummy3 -2.401 1.770 -1.357 0.1792 
YearVar:AgeGroupDummy4 -3.772 1.770 -2.131 0.0366 * 
YearVar:AgeGroupDummy5 -2.915 1.770 -1.647 0.1040 
YearVar:AgeGroupDummy6 -3.587 1.770 -2.026 0.0465 * 
YearVar:AgeGroupDummy7 -2.372 1.770 -1.340 0.1846 
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 14.97 on 70 degrees of freedom
Multiple R-squared: 0.9946, Adjusted R-squared: 0.9935 
F-statistic: 917.9 on 14 and 70 DF, p-value: &lt; 2.2e-16
</code></pre>

<p>My interpretation of the interaction term coefficients here is that they represent the difference in slope of YearVar between the corresponding AgeGroup in the interaction term and the YearVar result at the very top. That is, they are something like a period effect across the different age groups.</p>

<p>Can anyone see a problem with what I've done here or with my interpretation? This second regression is the closest thing to period effects across distinct age groups that I've been able to muster. Any critiques/new ideas are welcome.</p>
"
"0.0588235294117647","0.0592348877759092","157622","<p><img src=""http://i.stack.imgur.com/3HyvV.png"" alt=""data"">
I have data set (download from <a href=""https://dl.dropboxusercontent.com/u/15562376/test.csv"" rel=""nofollow"">here</a>), this data set is occupancy level in an office building within 30 days. There is a seasonal pattern daily as you may easily understand.   I tried <code>diff(t, lag=1440)</code> but it is still non-stationary. What methods should I use to predict this pattern? </p>

<p>I would like to predict the next few days based on the historical data. I tried different methods in R:</p>

<pre><code>a = read.table(""test.csv"", sep="",""); # read data
b = a[,2]  
t = ts(b, frequency = 1440)  # convert to time series 
plot(t)

d = decompose(t)  
plot(d)
acf(d$random,na.action = na.pass)   # non-stationary
    Box.test(d$random)
</code></pre>
"
"0.0789200462646985","0.0794719414239026","6513","<p>I want to predict inter-day electricity load. My data are electricity loads for 11 months, sampled in 30 minute intervals. I also got the weather-specific data from a meteorological station (temperature, relative humidity, wind direction, wind speed, sunlight). From this, I want to predict the electricity load until the end of the day. </p>

<p>I can run my algorithm until 10:00 of the present day and after that it should give the prediction of loads in 30 minute intervals. So, it should tell the load at 10:30, 11:00, 11:30 and so on until 24:00.</p>

<p>My first attempt was to create a <strong>linear model</strong> in R.</p>

<pre><code>BP.TS &lt;- ts(Buying.power, frequency = 48)
a &lt;- data.frame(
Time, BP.TS, Weekday, Pressure, Temperature, RelHumidity, AvgWindSpeed, AvgWindDirection, MaxWindSpeed, MaxWindDirection, SunLightTime,
m, Buying.2dayago, AfterHolidayAndBPYesterday8, MovingAvgLast7DaysMidnightTemp
)
a &lt;- a[(6*48+1):nrow(a),]

start = 9716
steps.ahead = 21
par(mfrow=c(5,2))
for (i in 1:10) {
    train &lt;- a[1:(start+(i-1)*48),]
    test &lt;- a[((i-1)*48+start+1):((i-1)*48+start+steps.ahead),]
    summary(reg &lt;- lm(log(BP.TS)~., data=train, na.action=NULL))
    pred &lt;- exp(predict(reg, test))

    plot(test$BP.TS, type=""o"")
    lines(pred, col=2)
    cat(""MAE"", mean(abs(test$BP.TS - pred)), ""\n"")
}
</code></pre>

<p>This is not very succesful. Now I try to model the data with ARIMA. I used auto.arima() from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"">forecast package</a>. These are the results I got:</p>

<pre><code>&gt; auto.arima(BP.TS)
Series: BP.TS 
ARIMA(2,0,1)(1,1,2)[48]                    

Call: auto.arima(x = BP.TS) 

Coefficients:
         ar1      ar2     ma1    sar1     sma1    sma2
      1.1816  -0.2627  -0.554  0.4381  -1.2415  0.3051
s.e.  0.0356   0.0286   0.033  0.0952   0.0982  0.0863

sigma^2 estimated as 256118:  log likelihood = -118939.7
AIC = 237893.5   AICc = 237893.5   BIC = 237947
</code></pre>

<p>Now if I try something like:</p>

<pre><code>reg = arima(train$BP.TS, order=c(2,0,1), xreg=cbind(
train$Time, 
train$Weekday, 
train$Pressure, 
train$Temperature, 
train$RelHumidity, 
train$AvgWindSpeed, 
train$AvgWindDirection, 
train$MaxWindSpeed, 
train$MaxWindDirection, 
train$SunLightTime,
train$Buying.2dayago,
train$MovingAvgLastNDaysLoad,
train$X1, train$X2, train$X3, train$X4, train$X5, train$X6, train$X7, train$X8, train$X9, 
train$X11, train$X12, train$X13, train$X14, train$X15, train$X16, train$X17, train$X18, 
train$MovingAvgLast7DaysMidnightTemp
))

p &lt;- predict(reg, n.ahead=21, newxreg=cbind(
test$Time, 
test$Weekday, 
test$Pressure, 
test$Temperature, 
test$RelHumidity, 
test$AvgWindSpeed, 
test$AvgWindDirection, 
test$MaxWindSpeed, 
test$MaxWindDirection, 
test$SunLightTime,
test$Buying.2dayago,
test$MovingAvgLastNDaysLoad,
test$X1, test$X2, test$X3, test$X4, test$X5, test$X6, test$X7, test$X8, test$X9, 
test$X11, test$X12, test$X13, test$X14, test$X15, test$X16, test$X17, test$X18, 
test$MovingAvgLast7DaysMidnightTemp
))

plot(test$BP.TS, type=""o"", ylim=c(6300,8300))
par(new=T)
plot(p$pred, col=2, ylim=c(6300,8300))
cat(""MAE"", mean(p$se), ""\n"")
</code></pre>

<p>I get even worse results. Why? I ran out of ideas, so please help. If there is additional information I need to give, please ask.</p>
"
"0.144087631928422","0.145095250022002","6544","<p>I have a dataset that I want to fit a simple linear model to, but I want to include the lag of the dependent variable as one of the regressors. Then I want to predict future values of this time series using forecasts I already have for the independent variables. The catch is: how do I incorporate the lag into my forecast?</p>

<p>Here's an example:</p>

<pre><code>#A function to calculate lags
lagmatrix &lt;- function(x,max.lag){embed(c(rep(NA,max.lag),x),max.lag)}
lag &lt;- function(x,lag) {
 out&lt;-lagmatrix(x,lag+1)[,lag]
 return(out[1:length(out)-1])
}

y&lt;-arima.sim(model=list(ar=c(.9)),n=1000) #Create AR(1) dependant variable
A&lt;-rnorm(1000) #Create independant variables
B&lt;-rnorm(1000)
C&lt;-rnorm(1000)
Error&lt;-rnorm(1000)
y&lt;-y+.5*A+.2*B-.3*C+.1*Error #Add relationship to independant variables 

#Fit linear model
lag1&lt;-lag(y,1)
model&lt;-lm(y~A+B+C+lag1)
summary(model)

#Forecast linear model
A&lt;-rnorm(50) #Assume we know 50 future values of A, B, C
B&lt;-rnorm(50)
C&lt;-rnorm(50)
lag1&lt;-  #################This is where I'm stuck##################

newdata&lt;-as.data.frame(cbind(A,B,C,lag1))
predict.lm(model,newdata=newdata)
</code></pre>
"
"0.169808902702831","0.170996392014192","192714","<p>I want to estimate the current maximum capacity (in kWh) having the current power consumption (in kWh) and the state of charge of the battery (in %) available in a time series. </p>

<p>I do not have a full battery charge circle recorded but only a snippet with the state of charge going from ~95% to ~35% in a 1 second data recording interval.</p>

<p>At first, i cumulated the current power consumption and noticed that the progression between cumulated power consumption and state of charge was almost the same (see figure 1 and 2). </p>

<p><strong>progression of state of charge vs. cumulated power consumption</strong>
<a href=""http://i.stack.imgur.com/Y5tTp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Y5tTp.png"" alt=""enter image description here""></a></p>

<p>So i tried to use a linear regression model to predict two values of cumulated power consumption at 0% and 100% state of charge. In my assumption the delta of these two leaves me with the current maximum capacity of the battery.</p>

<p>Ideally i want to monitor the capacity during the data recording. Which means i only have data of a few percent of state of charge.
In figure 3 and 4 you can see my attempt of trying to create a linear regression model for every 2% state of charge. As you can see in the right plot, the estimated capacity has a very high variance.</p>

<p><strong>LEFT: regression lines (green) of every 2% state of charge; RIGHT: estimated capacity of every 2% state of charge</strong>
<a href=""http://i.stack.imgur.com/qOmlY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qOmlY.png"" alt=""enter image description here""></a></p>

<p>Since i am not an expert in neither the matter of regression nor batteries/physics, my questions are:</p>

<ul>
<li><p>Is there a much simple or more exact way (or both) to estimate the current maximum capacity of the battery?</p></li>
<li><p>If yes, is there a good way to estimate an sufficiently exact capacity with an even smaller snippet in an ongoing process, lets say every 2% state of charge? (maybe using a characteristic curve of the discharge process)</p></li>
</ul>
"
"NaN","NaN","112550","<p>I am currently working with the Hodrick-Prescott filter. </p>"
"NaN","NaN","<p>I would like to understand the equation in lay terms. </p>",""
"NaN","NaN","","<r><regression><time-series><mathematical-statistics>"
"0.0588235294117647","0.0592348877759092","65900","<p>I'm not used to using variables in the date format in R. I'm just wondering if it is possible to add a date variable as an explanatory variable in a linear regression model. If it's possible, how can we interpret the coefficient? Is it the effect of one day on the outcome variable? </p>

<p>See my <a href=""https://gist.github.com/pachevalier/5966314"" rel=""nofollow"">gist</a> with an example what I'm trying to do. </p>
"
"0.0831890330807703","0.0837707816583391","26469","<p>In my current regression model, one of the explanatory variables I need is ""capital"" (K). The country under consideration here is CÃ´te d'Ivoire (West Africa) and, as you may already know, finding data for small open economies is very often an extremely difficult task.
I was able to find the data for ""capital formation"", which is essentially the first differences of the actual capital values (dK), but I am required to show the plot on levels (and not only on differences).</p>

<p>It is obvious that it is impossible to find the actual values from just the first differences, but there may be a way to have an exact similar plot using the differences as a basis.</p>

<p>How should I go about it using R?</p>

<p>Your answer will be much appreciated.</p>
"
"0.166378066161541","0.167541563316678","26432","<p>I am a web developer and novice statistician.</p>

<p>My data looks something like this</p>

<pre><code>Subject  Week   x1  x2  x3  x4  x5  y1
A        1      .5  .6  .7  .8  .7  10
B        1      .3  .6  .2  .1  .3  8
C        1      .3  .1  .2  .3  .2  6  
A        2      .1  .9  1.5 .8  .7  5
B        2      .3  .6  .3  .1  .3  2
D        2      .3  .1  .4  .3  .5  10  
</code></pre>

<p>I am trying to predict y1 as a product of the x variables. However, I have reason to believe that there may be a lag in the effect of the multiple x variables on y1, i.e the x variables from week 1 for subject A influence y1 for subject A in week 2. </p>

<p>Note that not all subjects will have data points for every week (in fact most won't). Subjects will tend to have data points for say week 1, 2, 3, 4 then drop off and not show up again until week 7,8,9. I am willing to restrict my analysis to data points where we have data for the previous N weeks given my hypothesis about lag.</p>

<p>Like I said, I am a novice and am unsure of the best way to deal with a dataset of this form. I am hoping to carry out this analysis either in R, Python, or some combination of the two.
I don't think that the current week's x variables will have no effect. I think they will have some effect, perhaps greater than previous weeks. I just believe that previous weeks will have some effect.</p>

<p>I am expecting there to be two to three weeks of lag. To give a little context, the analysis that I am attempting here relates to judging the quality of online traffic. Every week I get a score rating the quality of a certain stream of users I send to a given website. I am trying to find secondary metrics, such as browser distribution, percent duplicate clickouts, etc. that will allow me to predict what that score will be ahead of time. </p>
"
"0.144087631928422","0.145095250022002","65437","<p>I've got a data set which has volume of mail sent from 2001-2012, recorded by age group. That is, for seven different age groups, I have each age group's annual volume of mail sent over that time period.</p>

<p>I've run seven regressions in R--one for each age group--which simply regress volume of mail sent on time. My hypothesis is that these seven relationships are structurally similar (that is, while the coefficients may be different due to the initial volume of mail having been different for each age group in 2001, the <em>relationship</em> between volume of mail sent and time is not significantly different since 2001), or that there are no significant ""age group effects.""</p>

<p>I don't think this can be accomplished through a Chow Test, since the dependent variables are different in all seven regressions (maybe there's a way to run a Chow Test on these data if they are pooled?). Does anybody know of a way I might be able to do this, particularly in R?</p>
"
"NaN","NaN","159257","<p>I am working on a marketing data which is a time series data with marketing spend done through different channels and revenue generated.</p>"
"NaN","NaN","<p>The data looks like this :",""
"NaN","NaN","<img src=http://i.stack.imgur.com/k3yCE.png alt=DataSet Sample></p>",""
"NaN","NaN","<p>My data contains too many zeros (no spend at that particular time) and my spend vs channel1 looks like this :</p>",""
"NaN","NaN","<p><img src=http://i.stack.imgur.com/NnrNo.png alt=Spend Vs Channel1(Catalog)></p>",""
"NaN","NaN","<p>I want to develop a marketing mix model on this data set to understand the impact of various marketing spend on Sales and to predict future Sales and to optimize spend on different channels. How can I proceed analyzing the same. Can I transform variables and do a regression?</p>",""
"NaN","NaN","","<r><time-series><multiple-regression><zero-inflation><marketing>"
"0.159623653011757","0.160739915933046","193611","<p>The maximum capacity of a battery during usage can vary from manufacturer's specifications due to factors like temperatures and battery health. Because of that i want to estimate the real maximum capacity (in kWh) of a battery using the percentage of the battery's remaining capacity (in %) and the current power consumption (in kW).</p>

<p>I will get the above mentioned measurements in real time, in a time interval of every second, i.e., in a time series. During data recording, i will never see a full battery charge cycle but instead the battery will be between ~95% and ~35% percent.</p>

<p>In my data exploration, i cumulated the current power consumption, visualized it together with the battery percentage and noticed that they share a linear dependency, as you can see in figure 1.</p>

<p><strong>Figure 1: progression of battery capacity percentage and cumulated power consumption</strong>
<a href=""http://i.stack.imgur.com/Y5tTp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Y5tTp.png"" alt=""enter image description here""></a></p>

<p>My approach of holding measurements in a bin for every 2% lost battery capacity and performing on that bin a linear regression to predict the cumulated power consumption at zero percent battery capacity is giving me rather unstable results. (see figure 2)</p>

<p><strong>Figure 2: estimated capacity of every two percent battery capacity</strong>
<a href=""http://i.stack.imgur.com/CB2jq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CB2jq.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>Is there a way to get the results in a more stable (less varying) state?</p></li>
<li><p>Or is there even a more exact way to estimate the current maximum capacity of the battery using a different method?</p></li>
</ul>
"
"0.0588235294117647","0.0592348877759092","88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"0.0588235294117647","0.0592348877759092","27000","<p>Does anyone understand how the argument <strong>""span""</strong> is used in  <strong>spec.pgram</strong> in <strong>R</strong>? From the help menu, it provides the following examples:  </p>

<pre><code>spectrum(ldeaths)
spectrum(ldeaths, spans = c(3,5)) gives bandwidth= 0.241
spectrum(ldeaths, spans = c(5,7)) gives bandwidth= 0.363
spectrum(ldeaths, spans = 3)      gives bandwidth= 0.127
</code></pre>

<ol>
<li><p>What do <strong>c(3,5)</strong> and <strong>c(3,7)</strong> or <strong>spans=3</strong> represent? </p></li>
<li><p>How the bandwidths are calculated?</p></li>
</ol>

<p>Thanks!</p>
"
"NaN","NaN","212110","<p><code>auto.arima</code> gives me that the best model is <code>arima(0,1,0)</code>. But using Arima and fitting <code>(0,1,0)(0,1,0)[6]</code> I have a better fit according to BIC, AIC, AICc. <code>auto.arima</code> is not evaluating that model</p>

<p>Here is the code</p>

<pre><code>model1&lt;-auto.arima(data.set,xreg = X1,trace = T,
                                   stepwise=F,approximation = F,
                                   parallel = F, num.cores = 2,
                   allowdrift = T,allowmean = T)
</code></pre>

<p>Why is <code>auto.arima</code> not evaluating <code>(0,1,0)(0,1,0)[6]</code>?</p>
"
"0.242535625036333","0.244231699021682","89531","<p>I'm expanding a question I posed earlier because I think it was lacking detail. </p>

<p>I'm attempting to forecast daily demand for a restaurant that sells take away food, primarily to office workers on their lunch breaks. They are located in the downtown core of a major city.</p>

<p>They are only open on workdays - no holidays, no weekends. I'm familiar with models that take into account seasonality and trend - Holt-Winters triple exponential smoothing, for example. I'm also familiar with models that take into account complex seasonality and trend - the TBATS package for R, for example.</p>

<p>My problem is that I've identified 8 components that determine sales on a given day:</p>

<ol>
<li>The yearly seasonal component. Sales are lower in the summer, for example, when many office workers are on vacation.</li>
<li>The weekly component. Sales very obviously peak on Thursdays (in the absence of other effects - see below)</li>
<li>The <em>Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the coming Friday is a holiday. Wednesday will typically have higher sales, for example.</li>
<li>The <em>Post-Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week before was shortened due to the Friday being a holiday.</li>
<li>The <em>Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes in week $t$ if the Monday in week $t+1$ is a holiday. For example, sales are much lower on Fridays preceding Monday-Long-Weekends. Presumably people are leaving the office early and skipping lunch.</li>
<li>The <em>Post-Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week is shortened due to the Monday being a holiday.</li>
<li>The trend component. </li>
<li>The noise component.</li>
</ol>

<p>If holidays fell on the same date every year, then the ""long-weekend-effects"" would be captured in the yearly seasonal component. However, they don't. </p>

<p>My first thought was to include dummy variables. For example, let $X_{M+1}$ be the ""Monday-long-weekend-effect"" component, and $\beta_{M+1}$ be the associated coefficient, for a given day. Then for the Friday preceding a Monday-Long-Weekend, $X_{M+1}=1$, and for a Friday not preceding a Monday-Long-Weekend, $X=0$.</p>

<p>I'm only using three years of data, so it would be easy for me to change the $X_{M+1}$ values to 0 or 1 by hand for each year. However, I don't know how to include such dummy variables in models like those that I've mentioned.</p>

<p>Any input as to a model that can take into account the components I've mentioned would be greatly appreciated. It seems like I need to capture moving-holiday-effects, day-of-the-week effects, seasonal patterns, and trend, all in one.</p>

<p><strong>Question: Is there a model I can use that can be implemented in R and take into account the components I've listed?</strong></p>

<p><em>My background: I'm a forth year mathematics and economics student. I've also taken statistics classes, and I'm using R to perform my analysis. This is for a final report for a forth year data analysis class.</em></p>
"
"NaN","NaN","89749","<p>I was wondering if someone knows a R-package or function library for the topic of shrinkage for regression with ARMA errors.</p>

<p>Please let me know if you came across something related.</p>

<p>Thank you!</p>

<p>Regards,<br>
Patricia Tencaliec</p>
"
"0.0882352941176471","0.118469775551818","212623","<p>I am monitoring tree death caused by insects and potential impact of human treatment on yearly amount of tree mortality in areas with and without human intervention. My data are recorded by remote sensing, thus my results represents the area of trees killed per year. </p>

<p>Please, is there a way how to evaluate the effect of human intervention? My data contain one observation per year per category intervention/non-intervention. Is this the case of dynamic regression? </p>

<p>Dataset: two areas, differing by management or non-management approaches - one without chemical treatment (A), another with chemical treatment (B). Both contain the area of killed trees in yearly time step. </p>

<p>How can I decide if these two types of management differs, or if one potentially leads to decreased tree mortality? Is there a way how can I include in my analysis the amount of treatment applied each year and its impact in subsequent year? The insects develop within a year, and the treatment is applied by aerial treatment, not on specific tree thus I don't observe the immediate effect on chemical on the insect individual, as the treatment can stay on leafs of bark.</p>

<p>Thank you a lot for your suggestions !</p>

<p><a href=""http://i.stack.imgur.com/rSHAt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rSHAt.png"" alt=""enter image description here""></a></p>

<p>sample data:</p>

<pre><code>library(ggplot2)
library(gridExtra)

set.seed(100)
df&lt;-data.frame(year = c(2001:2010),
               A = floor(runif(10, min=0, max=200)),
               B = floor(runif(10, min=0, max=60)))

a&lt;- ggplot(df, aes(factor(year), y = A)) + ggtitle(""Non Management"") +
geom_bar(stat = ""identity"") + ylim(0,200) + ylab(""area in ha"") +
theme_classic() + theme(axis.text.x = element_text(angle = 90))

b &lt;- ggplot(df, aes(factor(year), y =B)) + ggtitle(""Management"") +
  geom_bar(stat = ""identity"") + ylim(0,200) + ylab(""area in ha"") +
  theme_classic() + theme(axis.text.x = element_text(angle = 90))

grid.arrange(a,b, ncol = 2)
</code></pre>
"
"NaN","NaN","116842","<p>I have a SarimaX model with three regressor variables:</p>

<pre><code>ARIMA(1,0,0)(0,1,1)[7]                    

Coefficients:
          ar1       sma1   C1 (for xreg1)   C2 (for xreg2)   C3 (for xreg3)
      -0.0260    -0.9216          -0.0354           0.0316           0.9404
s.e.   0.0291     0.0350           0.0016           0.0017           0.0128
</code></pre>

<p>I would like to know how to use these coefficients to obtain the actual equation, like:</p>

<pre><code>y[t] = f(ar1, sma1, C1|xreg1[t], C2|xreg2[t], C3|xreg3[t])
</code></pre>

<p>I have read the following:</p>

<p><a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">https://www.otexts.org/fpp/8/9</a> - I'm using the forecast package in R, so I'm quite grateful for Mr. Hyndman's work,</p>

<p><a href=""http://people.duke.edu/~rnau/arimreg.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/arimreg.htm</a></p>

<p>and others, and I devised some formulas, but they generated values less acurate than those from the R forecast. Somehow, my error-related terms are probably wrong.</p>

<hr>

<p><strong>EDIT</strong>: This is what I have so far:</p>

<p>$$ \ (1-ar1*B)*(1-B^7)*y_t=$$
$$ = (1-ar1*B)*(1-B^7)*(C1*xreg1_t + C2*xreg2_t+C3*xreg3_t)+ $$
$$ + e_t + sma1*e_{t-7}$$</p>

<p>I would like to know if this formula is correct, could anyone please help? Thank you.</p>
"
"0.0588235294117647","0.0592348877759092","11929","<p>I am starting to use R's <a href=http://cran.r-project.org/web/packages/dynlm/index.html rel=nofollow>dynlm</a> package. Currently I am just looking at the fit and eyeball which choice of lags might be the best. Is there a standard way or a strategy to determine the best k parameters for <code>L()</code>. What I often see is ridiculously high lags like k=10 in a quarterly series delivering the best fit. What could be the reason for that?</p>"
"NaN","NaN","","<r><regression><time-series>"
"0.227822549776907","0.229415733870562","11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.227822549776907","0.214121351612524","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.0588235294117647","0.0592348877759092","6967","<p>/edit: To clarify: The mtable function from the <a href=""http://cran.r-project.org/web/packages/memisc/index.html"" rel=""nofollow"">memisc</a> package does exactly what I need, but unfortunately does not work with arima models.</p>

<p>Similar to <a href=""http://stats.stackexchange.com/questions/6856/aggregating-results-from-linear-model-runs-r"">this question</a>: I have multiple Arima models, some of which I've also fit with dependent variables. I'd like an easy way to make a table/graph of the coefficients in each model, as well as summary statistics about each model.</p>

<p>Here is some example code:</p>

<pre><code>sim &lt;- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 200)

ar1&lt;-arima(sim,order=c(1,1,0))
ar2&lt;-arima(sim,order=c(2,1,0))
ar3&lt;-arima(sim,order=c(3,1,0))
ar4&lt;-arima(sim,order=c(2,2,1))

#Try mtable
library(memisc)
mtable(""Model 1""=ar1,""Model 2""=ar2,""Model 3""=ar3,""Model 4""=ar4)
#&gt;Error in UseMethod(""getSummary"") : 
#  no applicable method for 'getSummary' applied to an object of class ""Arima""

#Try  apsrtable
library(apsrtable)
apsrtable(""Model 1""=ar1,""Model 2""=ar2,""Model 3""=ar3,""Model 4""=ar4)
#&gt;Error in est/x$se : non-numeric argument to binary operator
</code></pre>
"
"0.155632430062623","0.156720781993876","118114","<p>Suppose we have collected a set of data points $\{a_{t}\}$ at time $t = 1, 2, ..., t', ..., n$. Each data point consists of the following attributes: <code>Average Citizen Income</code>, <code>GDP</code>, <code>Education Expense</code>.</p>

<p>At time $t'$, the government introduces certain policies, and I am interested in testing whether these policies has effect on <code>Average Citizen Income</code>.</p>

<p>Ideally, I want to introduce an extra dummy attribute <code>Policy</code> for these data points, in which the data points collected before time $t'$ will take <code>Policy = 0</code>, and the remaining data points will take <code>Policy = 1</code>. Then I will regress <code>Average Citizen Income</code> on <code>GDP</code>, <code>Education Expense</code>, and <code>Policy</code>, and test whether the coefficient of <code>Policy</code> is 0.</p>

<p>However, in this case the data points have time dependence, so I don't think I can directly adapt this approach. I wonder whether there are similar approaches (or alternative approaches) for testing whether <em>the introduction of the government policies has effect on the average citizen income</em>.</p>
"
"0.144087631928422","0.145095250022002","176361","<p>I got a time series dataset from the lab and would like to fit my data to a curve (using R package): 
$$
P_{1}(t)=y_{0}-a_{1}e^{-b_{1}t}-a_{2}e^{-b_{2}t}, b_{2}&gt;b_{1}  \tag{A}
$$</p>

<p>I plotted the data:  </p>

<p><a href=""http://i.stack.imgur.com/MZlY9.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MZlY9.jpg"" alt=""enter image description here""></a></p>

<p>I think I need to start with good initial values to get a better estimate when fitting to the curve.So I start with a simpler curve:
$$
P_{2}(t)=a_{1}e^{-b_{1}t}  \tag{B}
$$</p>

<p>and I got: </p>

<pre><code>Call:
lm(formula = log(P1_1) ~ time)

Coefficients:

(Intercept)        time  
   4.084148        0.002703 
</code></pre>

<p>That is, $a_{1}=54, b_{1}=0.003$ (roughly).</p>

<p>So I set the starting value $y_{0}=30, a_{1}=20, b_{1}=0.002, b_{1}=30, b_{2}=0.003$ and then did:</p>

<pre><code>nls_fit &lt;- nls(X1r_1_green~-y0+A1*exp(tau1*Time)+A2*exp(tau2*Time), 
               start=list(y0=35, A1=20, A2=30, tau1=.02, tau2=.03), data=data1, trace=T)
</code></pre>

<p>However, it returns errors that I could not figure out how to fix, such as:</p>

<blockquote>
  <p>step factor 0.000488281 reduced below 'minFactor' of 0.000976562<br>
  singular gradient matrix at initial parameter estimates</p>
</blockquote>

<p>After digging into some posts online, the error might be the poor initial values. After many times attempts at different initial values I've still had no luck, though. Do you have any suggestions?    </p>

<p>The data is as follows:</p>

<pre><code>       Time X1r_1_green
1    11.333     14.1060
2    12.443     21.6894
3    13.450     26.5231
4    14.457     33.0356
5    15.463     36.4517
6    16.471     37.6585
7    17.478     44.5417
8    18.485     46.3301
9    19.492     51.2111
10   20.499     51.2094
11   21.505     51.3405
12   22.513     54.6848
13   23.520     55.8172
14   24.527     61.5436
15   25.534     62.5158
16   26.541     62.5279
17   27.547     62.8395
18   28.555     61.9790
19   29.562     65.7806
20   30.569     66.7353
21   31.576     64.3837
22   32.583     64.7834
23   33.589     68.8058
24   34.597     70.6975
25   35.604     73.2809
26   37.597     74.2578
27   38.707     69.5155
28   39.714     75.3023
29   40.721     76.5360
30   41.727     74.0348
31   42.735     76.6561
32   43.742     79.1025
33   44.749     79.2463
34   45.756     79.4334
35   46.763     79.4644
36   47.769     81.9903
37   48.776     78.4627
38   49.784     81.7913
39   50.791     84.2073
40   51.798     82.0027
41   52.805     83.6003
42   53.811     82.0862
43   54.818     80.9991
44   55.826     81.0088
45   56.833     80.6877
46   57.840     84.4431
47   58.847     81.8827
48   59.853     82.3165
49   60.861     84.3172
50   61.868     83.6584
51   63.862     81.0175
52   64.972     83.9112
53   65.979     84.7511
54   66.986     83.3383
55   67.993     85.2289
56   68.999     84.1657
57   70.007     83.5959
58   71.014     86.6437
59   72.021     86.1192
60   73.028     86.5437
61   74.034     89.5566
62   75.041     86.0854
63   76.049     85.1923
64   77.056     86.5120
65   78.063     87.7273
66   79.070     84.8408
67   80.076     87.7706
68   81.083     89.3330
69   82.091     86.4411
70   83.098     88.4944
71   84.105     86.3283
72   85.112     85.8619
73   86.118     85.1758
74   87.125     88.8349
75   88.133     86.4028
76   89.140     85.1487
77   90.147     85.0808
78   91.154     84.1891
79   92.160     79.2782
80   93.167     83.9142
81   94.175     77.9845
82   95.182     76.1993
83   96.189     76.3565
84   97.196     79.3567
85   98.202     78.5809
86   99.210     78.2514
87  100.217     78.5353
88  101.224     82.9957
89  102.231     79.0445
90  103.237     80.5854
91  104.244     80.3490
92  105.252     77.5888
93  106.259     78.3460
94  107.266     80.4607
95  108.273     79.6868
96  109.279     82.5708
97  110.287     81.7843
98  111.294     81.2645
99  112.301     84.0419
100 113.308     83.5896
101 114.315     87.6291
102 115.321     86.3707
103 116.329     84.7563
104 117.336     86.4180
105 118.343     81.4442
106 119.350     83.6793
107 120.357     83.8090
108 121.363     82.8488
109 122.371     86.8810
110 123.378     83.4640
111 124.385     86.9706
112 125.392     86.5779
113 126.398     85.8226
114 127.406     89.9185
115 128.413     88.6702
116 129.420     87.5920
117 130.427     91.5657
118 131.434     90.5003
119 132.440     88.6852
120 133.448     88.8704
121 134.455     94.9899
122 135.462     89.8719
123 136.469     89.0472
124 137.477     85.9579
125 138.483     89.7549
126 139.490     88.1925
127 140.497     90.3598
128 141.504     87.4038
129 142.511     88.8794
130 143.518     89.6318
131 144.525     87.6712
132 145.532     91.8890
133 146.539     87.8800
134 147.546     91.4838
135 148.553     88.9816
136 149.560     87.4804
137 150.567     87.5171
138 151.574     86.1898
139 152.581     85.5146
140 153.588     86.2422
141 154.595     90.9029
142 155.602     88.2005
143 156.609     84.7370
144 157.616     90.0859
145 158.623     83.5787
146 159.630     87.1796
147 160.637     90.1887
148 161.644     82.7800
149 162.651     82.8887
150 163.658     83.5638
151 164.665     82.8934
152 165.672     81.3959
153 166.679     83.2340
154 167.686     80.5834
155 168.693     81.9224
156 169.700     85.8687
157 170.707     82.0637
158 171.714     85.9917
159 172.722     78.8532
160 173.728     83.0037
161 174.735     84.7516
162 175.742     84.4288
163 176.749     90.4443
164 177.757     83.3640
165 178.764     86.1808
166 179.770     84.2887
167 180.777     87.9906
168 181.784     84.4754
169 182.791     85.7398
170 183.798     87.0640
171 184.805     86.6323
172 185.812     83.0898
173 186.819     86.8609
174 187.826     87.7454
175 188.833     86.8763
</code></pre>
"
"0.117647058823529","0.118469775551818","68493","<p>I'm having a hard time understanding something. Let's say that I have 36 months of data (36 observations) regarding consumer behavior on a website. I constructed a model regressing $y$ on a number of predictors, and I get the desired coefficients. However, I'm interested in knowing how well my model did in predicting the response variable in month $x$. Given my model and the training set that it was run on, I want to be able to determine how well it did in predicting the response variable in a given month (say April or March). I have the traditional stats on the overall 'goodness of fit' of the model, but I really want to understand how well my model predict the data for month $x$.</p>

<p>Does that make sense? How would I go about getting that information? How would I perform this task in R?</p>

<p>What about predicting future months? The next three months?</p>

<p>I am running a <code>glm</code> in R (Poisson).</p>
"
"0.0831890330807703","0.0837707816583391","28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.0831890330807703","0.0837707816583391","119946","<p>I have some time series data where I'm modelling temperature as a function of various predictors. On physical grounds, I can expect that</p>

<p>$$\frac{dT}{dt} \propto T_a - T$$</p>

<p>where $T_a$ is the ambient temperature (which can vary over time, but whose values are known). I thus fit models of the form</p>

<p>$$\Delta T(t) \sim \alpha + \beta \left[ T_a(t) -T(t) \right] + \gamma X(t)$$</p>

<p>with $X$ being the other covariates, and $\alpha$, $\beta$ and $\gamma$ are the regression parameters. I can fit these easily enough in R:</p>

<pre><code>lm(diff(T) ~ I(Ta - T) + x, data=df)
</code></pre>

<p>and I can get predictions for the change in $T$. However, what I really want are predictions for $T$ itself. At the moment I'm calculating these via a loop, where I plug $\hat{T}(t)$ into the regression equation to obtain $\hat{\Delta T}(t+1)$.</p>

<p>Is there any R package, probably time series-related, that will do these calculations automatically?</p>

<p>Also, if there are any issues with this approach, I'd be happy to know about them.</p>
"
"0.131533410441164","0.132453235706504","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.101885341621699","0.102597835208515","125308","<p>I just fit a model to a time series. I am now required to generate a 10-year extrapolation forecast of my model. My model includes a time term, a time^2 term, 12 seasonal dummies, and 4 lagged dependent variables. If I am correct, I thought that I could not do an h step ahead forecast because of the lags, so I am trying to run 120 1-step ahead forecasts. I am using the predict() function and trying to make a for-loop for times i=313:432 (that is 120 months following our initial 312 observations).</p>

<p>I am confused with the predict () function and with for loops, and any guidance on how to generate this loop would be extremely appreciated!</p>
"
"0.305656024865096","0.307793505625546","29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.131533410441164","0.132453235706504","68802","<p>I'm trying to understand some concepts related to predictive modeling. So let's say that I have the following data sample and am trying to regress <code>sales</code> on <code>clicks</code> and <code>calls</code>. Ultimately, I'm interested in predicting sales figures for any given month (e.g., <code>Jan</code>, <code>Feb</code>, etc.) given a change in clicks or calls. <code>Month</code> is not included in the model, but I'm not sure how I would go about building a model which would allow me to predict sales figures for <code>Feb</code> given in a change in <code>clicks</code> or <code>calls</code>.</p>

<ol>
<li><p>My data  </p>

<pre><code>df = data.frame(month_main=c(""Jan"",""Feb"",""Mar"",""Apr"",""May""),
                sales=c(50,35,60,20,50),
                month_attr=c(""Jan"",""Feb"",""Mar"",""Apr"",""May""),
                clicks=c(300,350,500,550,250),
                calls=c(100,150,200,150,150))
</code></pre></li>
<li><p>Regression Model</p>

<pre><code>m1 = lm(sales ~ clicks + calls, data=df)
summary(m1)
</code></pre></li>
</ol>

<p></p>

<ul>
<li><p>I want to build a model to predict sales for <code>Feb</code> given a change in either predictor</p></li>
<li><p>Model doesn't include <code>month</code>, but I want to predict for a given month.</p></li>
<li><p>How should I be thinking about this problem?</p></li>
<li><p>How would I perform this task in R?</p></li>
</ul>
"
"0.144087631928422","0.145095250022002","160178","<p>I have two I(1) time series and I regressed one against the other and found that it had low to moderate R-squared but my DW statistic is about 0.015.  I know the literature says this is the case of spurious regression?  Now, upon running co-integration tests on the residuals (I ran an ADF test using updated MacKinnon's p table, used Phillips Ouliaris test, Johansen test and Elliott Rothenberg and stock test).  Now, all my tests pass except for Phillips Ouliaris and Johansen test.  These are the only tests where I am not getting the residuals from the data.  I believe the PO test automatically runs an regression of y against x and uses the Phillips Ouliaris distribution rather than the ADF distribution on residuals.  </p>

<p>My main question is, which test do I trust and whether these tests even make any sense considering I have a spurious regression phenomena?  I believe if the two series are co-integrated, then the residuals won't be spurious correct?  So my main questions are the follows:</p>

<ol>
<li>Can you have co-integration even with spurious regression?</li>
<li>which test do I ultimately have to chosen from?  PO test, Johansen test (both these tests accept the series are not co-integrated).  ERS test passes on residuals and so does the R functions <code>adf.test</code>, <code>adfTest</code>, &amp; <code>ur.df</code>.  </li>
<li>My time series is from 1998 to 2015.  Sometimes, daily gives me co-integration, but monthly doesn't.  What time frame is most acceptable?  </li>
</ol>
"
"0.195095575903259","0.196459897251501","30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"0.166378066161541","0.167541563316678","138847","<p>Working with a panel data set with a daily time series structure I was told to include a lagged dependent variable.
The dependent variable is daily electricity consumption of a medium size sample (>200) over a metering period of about 1 year.</p>

<p>In my existing pooled OLS model, outdoor temperature and some consumer specific variables like floor space or number of residents are the most important variables. $R^2$ is around 0.7.</p>

<p>Including yesterday's consumption yields an $R^2$ of about 0.9. However, I am not sure if including the lagged value is meaningful.
Today's consumption $y_{t}$ is highly correlated with yesterday's consumption $y_{t-1}$ but this is probably because $y_{t}$ and $y_{t-1}$ are influenced by the same variables (temperature, dwelling size,...). Thus, $y_{t-1}$ and the other independent variables are also correlated which violates some OLS assumptions (if I remember correctly...).
<strong>Is it useful and in accordance with OLS assumptions to include a lagged value in pooled OLS?</strong></p>

<p>Further, as I have never worked with lagged values before I really have some problems understanding its practical use. There is a <a href=""http://stats.stackexchange.com/questions/52458/inclusion-of-lagged-dependent-variable-in-regression"">similar question</a> in SE but still I don't understand...</p>

<p>When I want to model/predict consumption of day $t$, I will probably not have access to consumption of day $t-1$. <strong>So how do I use a model with lagged DVs in practice? Do I just use modelled values so that every $\hat{y}_{t}$ relies on a previously estimated value $\hat{y}_{t-1}$? If so, do I need some kind of starting value for the first day of the time series?</strong></p>

<p>Many thanks in advance for any help in understanding this!</p>
"
"0.0831890330807703","0.0837707816583391","94406","<p>I am working on a project, and I am totally new to statistics. I have sales data for last two years at week level, along with other variables like temperature, holiday (TRUE/FALSE), where holiday are nominal variables. I have to do forecasting for the next 52 weeks. I have the following questions:</p>

<ol>
<li>Can I use time series regression model where sales would be dependent, and temperature and<br>
holiday would be independent variables?</li>
<li>How to decide which independent variable would have more impact on the sales? </li>
<li>Can we do forecasting using nominal variables? Will dummy coding work?</li>
<li>Can we do it in R/SPSS?</li>
</ol>

<p>I would appreciate any kind of help. Thanks in advance.</p>
"
"0.0588235294117647","0.0592348877759092","31846","<p>I'm trying to model AR1 using <code>dlmModReg()</code>. The main purpose is to keep phi a variable so that if phi >1, I know that mean reversion is not occurring.</p>

<p>Below is my code:</p>

<pre><code>buildFun &lt;- function(x) {dlmModReg(e1, addInt = FALSE, dV = exp(x[1]), dW = exp(x[2]))}
fit      &lt;- dlmMLE(e0, parm = c(0,0), buildFun)
dlmRes   &lt;- buildFun(fit$par)  
ResFilt  &lt;- dlmFilter(e, dlmRes)
R        &lt;- unlist(dlmSvd2var(ResFilt$U.R, ResFilt$D.R))
Q        &lt;- R + as.vector(V(dlmRes)) 
</code></pre>

<p>""e is some residues from OLS and I'm trying to model e using AR1""</p>

<p>My Question:</p>

<ul>
<li>Is my Q calculated wrongly as I try to built a confidence interval for <code>ResFilt$f</code>  and the Q values are at least order 20 bigger than <code>ResFilt$f</code>?</li>
</ul>
"
"0.101885341621699","0.102597835208515","94062","<p>I am trying to NOT use packages for the estimation of models in order to have a deeper understanding of how things work. Currently, I am trying to estimate a VAR(1) (vector autoregression of first order) and my question is about how to construct the matrix of dependent and independent variables.</p>

<p>Theoretically, my model is:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cbinom%7Bx_t%7D%7By_t%7D%20%3D%20%5Cbinom%7Bc_1%7D%7Bc_2%7D%20&plus;%20%5Cbegin%7Bpmatrix%7D%20a_%7B11%7D%20%26%20a_%7B12%7D%5C%5C%20a_%7B21%7D%20%26%20a_%7B22%7D%20%5Cend%7Bpmatrix%7D%5Cbinom%7Bx_%7Bt-1%7D%7D%7Bx_%7Bt-2%7D%7D%20&plus;%20%5Cbinom%7Bu_%7B1%2Ct%7D%7D%7Bu_%7B2%2Ct%7D%7D"" alt=""model""></p>

<p>As for the matrix of dependent variables, it should be easy enough. I just need to stack <code>xt</code> and <code>zt</code>. However, I don't know how to construct the matrix of independent variables. I read that VAR is very similar to SUR (Seemingly Unrelated Regressions) so I think that the matrix of independent variables should be a block diagonal matrix, but what should it look like? Especially if I want to add a constant and a time trend to the model?</p>

<p>Thank you.</p>
"
"NaN","NaN","74975","<p>I have time series, for example, gdp and unemployment(<code>unemp</code>), freq= 4. </p>

<p>What if I interpret it as cross-sectional data and do cross-sectional analysis instead of time series? </p>

<p>My task is to test how unemployment affects gdp. </p>

<p>Is it allowed to do that kind of analysis? </p>

<p>Do the coefficients in the model <code>lm(gdp~unemp)</code> have an economic explanation?</p>
"
"NaN","NaN","92602","<p>Let's say I have two regression models </p>

<p>(I) $y_t=\beta_1+\beta_2 x_2+u_t$</p>

<p>(II)  $y_t=\beta_1+\beta_2 x_2+\beta_3 x_3 + u_t$</p>

<p>How the omission of relevant variable (not irrelevant variable) affects adjusted $R^2$? </p>

<p>That's when I compare Adj-$R^2$ for two models, what can I say? Which one less? </p>

<p>I see this while studying lecture notes. So I Try to understand the topic. Please explain this.  Thank you:) </p>
"
"0.212091251497882","0.213574425172396","141423","<p>I use the <code>decompose</code> function in <code>R</code> and come up with the 3 components of my monthly time series (trend, seasonal and random). If I plot the chart or look at the table, I can clearly see that the time series is affected by seasonality.</p>

<p>However, when I regress the time series onto the 11 seasonal dummy variables, all the coefficients are not statistically significant, suggesting there is no seasonality.</p>

<p>I don't understand why I come up with two very different results. Did this happen to anybody? Am I doing something wrong?</p>

<hr>

<p>I add here some useful details.</p>

<p>This is my time series and the corresponding monthly change. In both charts, you can see there is seasonality (or this is what I would like to assess). Especially, in the second chart (which is the monthly change of the series) I can see a recurrent pattern (high points and low points in the same months of the year).</p>

<p><img src=""http://i.stack.imgur.com/rILAU.jpg"" alt=""TimeSeries""></p>

<p><img src=""http://i.stack.imgur.com/LdVnv.jpg"" alt=""MonthlyChange""></p>

<p>Below is the output of the <code>decompose</code> function. I appreciate that, as @RichardHardy said, the function does not test whether there is actual seasonality. But the decomposition seems to confirm what I think.</p>

<p><img src=""http://i.stack.imgur.com/ZaVRV.jpg"" alt=""Decompose""></p>

<p>However, when I regress the time series on 11 seasonal dummy variables (January to November, excluding December) I find the following:</p>

<pre><code>    Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 5144454056  372840549  13.798   &lt;2e-16 ***
    Jan     -616669492  527276161  -1.170    0.248    
    Feb     -586884419  527276161  -1.113    0.271    
    Mar     -461990149  527276161  -0.876    0.385    
    Apr     -407860396  527276161  -0.774    0.443    
    May     -395942771  527276161  -0.751    0.456    
    Jun     -382312331  527276161  -0.725    0.472    
    Jul     -342137426  527276161  -0.649    0.520    
    Aug     -308931830  527276161  -0.586    0.561    
    Sep     -275129629  527276161  -0.522    0.604    
    Oct     -218035419  527276161  -0.414    0.681    
    Nov     -159814080  527276161  -0.303    0.763
</code></pre>

<p>Basically, all the seasonality coefficients are not statistically significant.</p>

<p>To run linear regression I use the following function:</p>

<p><code>lm.r = lm(Yvar~Var$Jan+Var$Feb+Var$Mar+Var$Apr+Var$May+Var$Jun+Var$Jul+Var$Aug+Var$Sep+Var$Oct+Var$Nov)</code></p>

<p>where I set up Yvar as a time series variable with monthly frequency (frequency = 12).</p>

<p>I also try to take into account the trending component of the time series including a trend variable to the regression. However, the result does not change.</p>

<pre><code>                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 3600646404   96286811  37.395   &lt;2e-16 ***
    Jan     -144950487  117138294  -1.237    0.222    
    Feb     -158048960  116963281  -1.351    0.183    
    Mar      -76038236  116804709  -0.651    0.518    
    Apr      -64792029  116662646  -0.555    0.581    
    May      -95757949  116537153  -0.822    0.415    
    Jun     -125011055  116428283  -1.074    0.288    
    Jul     -127719697  116336082  -1.098    0.278    
    Aug     -137397646  116260591  -1.182    0.243    
    Sep     -146478991  116201842  -1.261    0.214    
    Oct     -132268327  116159860  -1.139    0.261    
    Nov     -116930534  116134664  -1.007    0.319    
    trend     42883546    1396782  30.702   &lt;2e-16 ***
</code></pre>

<p>Hence my question is: am I doing something wrong in the regression analysis?</p>
"
"0.166378066161541","0.167541563316678","32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.0588235294117647","0.0592348877759092","201440","<p>I have two time series and I want to check the relationship between them. I would like to use vector autoregression (VAR) model to do this. </p>

<p>I'd like to specify the model so that both variables will be explained by the lagged values of both itself and the other variable.
Moreover, (and here I encountered the actual trouble,) one of the time series variables, x, will need to depend on the <strong>same period</strong> value of the other variable y. </p>

<p>In R, can I somehow use the package ""vars"" to do this?
I tried to look at the documentation but I could only figure out how to include lagged variables in the model.</p>

<p>Or is there some other (easy) way to do this? Preferably in R.</p>
"
"0.0588235294117647","0.0592348877759092","91972","<p>Let us say I have two sets of time varying series as shown below:</p>

<pre><code>a &lt;- c(895,0,0,0,0,832,0,3084,0,434,0,0,0,0,853,0,3727,0,513,0,0,0,0,1248,0,3704,0,646,0,0,0,0,2066,0,5500,0,424,0,0,0,0,1069,0)
b &lt;- c(234,0,0,0,0,521,0,1683,0,152,0,0,0,0,740,0,3242,0,355,0,0,0,0,1117,0,3047,0,443,0,0,0,0,832,0,3213,0,326,0,0,0,0,1235,0)
</code></pre>

<p>Here, each value represents the number of times a person has listened to a song. Also, the data is daily which means if the first value (a[1] = 895) is for 10th Feb then the last value will be for 25th Mar. </p>

<p>I want to find out whether a or b is trending more and by what margin, so if I have let say another series c then I can arrange them in the trending order for example c > a > b . Is there a statistical approach to do this.</p>
"
"NaN","NaN","202092","<p>I am trying to analyze correlations between traffic speeds on city roads, and make forecasts for the speed on a road based on its speed and that of correlated roads. The speeds are recorded irregularly, about every 5 minutes, and different roads are recorded at different times. There are occasional larger gaps of a few hours.</p>

<p>I am considering interpolating the data onto regular time series and running a VAR model. However, I'm not sure if the interpolation would cause some problems, especially given the longer gaps. </p>

<p>Can anyone suggest a good approach in Python or R?</p>
"
"0.131533410441164","0.132453235706504","202181","<p>I'm getting some odd coefficients when I apply <code>lm</code> to dates that have been processed and rounded using the <code>lubridate</code> package.  MWE:  </p>

<pre><code>library(ggplot2)
library(lubridate)
library(dplyr)

lakers$month &lt;- ymd(lakers$date) %&gt;% round_date(unit = 'month')
items_by_month &lt;- lakers %&gt;% group_by(month) %&gt;% summarize(count = n()) %&gt;%
    mutate(count = count / 1000)

ggplot(data = items_by_month, aes(x = month, y = count)) + 
    geom_line() +
    stat_smooth(method = 'lm', data = items_by_month)

model &lt;- lm(data = items_by_month, count ~ month)
summary(model)
time &lt;- max(items_by_month$month) - min(items_by_month$month)
coef(model)['month'] * as.numeric(time)
</code></pre>

<p>The plot indicates that <code>ggplot</code>, at least, understands what's going on with the regression model.<br>
<a href=""http://i.stack.imgur.com/HYTks.png""><img src=""http://i.stack.imgur.com/HYTks.png"" alt=""Plot with monthly totals and regression line""></a></p>

<p>But in <code>summary(model)</code> the coefficient on <code>month</code> is on the order of 10^-7, which is about 5 orders of magnitude too small:  the plot shows an increase of about 2.5 between the first and last dates, but the last line shows an increase of about 2.5 * 10^-5.  </p>

<p>Note that I've divided the <code>count</code> column by 10^3, in order to get values that are easier to read (and closer to my actual use case).  But that shouldn't effect either the plot or <code>lm</code>.  Also, I know there are more sophisticated techniques than linear regression for analyzing time series data; but I'm just looking at gross trends over time, not factor out seasonal patterns, etc.  </p>
"
"0.117647058823529","0.118469775551818","219775","<p>I am trying to build a regression model using two time series data in R. There is not much correlation between the two time series, so I am using trend part of both time series(using STL decomposition) for the model. The maximum correlation between these two is about 0.6 at a fixed lag. Is this sufficient to establish that one time series affects the other and build a regression model upon them.</p>
"
"0.155632430062623","0.156720781993876","17328","<p>I need to know the types of tests I need to be thinking about for this stats project. I will then go and look up the specific tests you recommend. The data is from all 50 states for the years 1960-2010.</p>

<p>I think the best way to illustrate the data is to show an example (I just made up numbers):</p>

<pre>
State           Condition       Score
Alabama1995     Condition 1     .534
Alabama1996     Condition 1     .343
Alabama1997     Condition 1     .545
Alabama1998     Condition 2      NA
Alabama1999     Condition 2     .344
....
Alaska1995      Condition 2     .434
Alaska1996      Condition 2     .444
Alaska1997      Condition 2      NA
Alaska1998      Condition 3     .456
Alaska1999      Condition 3     .678
Alaska2000      Condition 3     .437
....
Maine1995       Condition 3     .343
Maine1996       Condition 3     .342
Maine1997       Condition 3     .543
Maine1998       Condition 1     .439
</pre>

<p>So what I want to know is if the conditions and scores are statistically different. Do states when under condition 3 have a significant difference from states when under 2 or 1?</p>

<p>Let's say a state in a given year was either condition 1, 2, 3.</p>

<p>Not all states could have a score computed for them, but most did.</p>

<p>I want to evaluate the following types of things:</p>

<ul>
<li>What is the relationship between conditions and scores?</li>
<li>What is the relationship between time and conditions and scores?</li>
<li>When a state changed from condition 1 to condition 2, was the change in score significant?</li>
<li>Does condition 1 overall have a higher/lower score than other conditions?</li>
</ul>

<p>Are there other things besides simple significance tests I should be thinking about? </p>

<p>Ideally something that R (Rcmdr!) can do nicely. Stata, SPSS are also available to me.</p>
"
"0.144087631928422","0.145095250022002","165214","<p>I am attempting to conduct a dynamic or time series regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. 
For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>Numerous matches, for each I have:</p>

<ul>
<li>No. of service points played </li>
<li>No. of service points Nadal won </li>
<li>(Thus) Nadal's Service point win %</li>
<li>The court surface the match was played on (independent variable)</li>
<li><p>Nadal, and his opponent's world ranking points at time of match </p>

<p>I will be using a model like this:</p></li>
</ul>

<p><em>Nadal serve win % = surface + (Nadal rank points - opponent rank points)</em> </p>

<p>However I would like to include an independent variable that accounts for ""form"" or ""hot hands"" in tennis. So I want to include an independent variable : </p>

<ul>
<li>Xi = Avg. Serve % of Last 5 matches </li>
</ul>

<p>i.e. a moving average of sorts </p>

<p>Is this a good idea? Does anyone have any suggestions how this could be implemented in R specifically? </p>

<p>Lastly, since my dependent variable data is binary, and can be used in a logistic regression in R.... Can I run a dynamic logistic regression/logistic time series for the model I discuss above? </p>

<p>Any advice on how I can account for this form/trend would be massively appreciated</p>
"
"NaN","NaN","127441","<p>Hi I am currently trying to simulate an AR(4) process $y_t=0.67y_{t-1}-0.51y_{t-4}+\epsilon_t$ given that the initial value $y_1=1,y_2=2,y_3=3,y_4=4$ and $\epsilon_t\sim N(0,1)$. My code is given as below</p>

<pre><code>&gt; y &lt;- arima.sim(34,model=list(ar=c(0.6,0,0,-0.5)),
start.innov=c(1,2,3,4),n.start=4,innov=c(0,0,0,0,rnorm(30,mean=0,sd=1)))

&gt; y
Time Series:
Start = 1 
End = 34 
Frequency = 1 
 [1]  3.541600000  0.824960000 -1.785024000 -4.439014400 -5.947049868 -4.222318979 -2.326217678  1.144088356  5.150246659  4.159775404  5.767793661
[12]  0.967415814 -1.472489589 -3.311269208 -3.235976179 -1.787193697  0.855102209  1.619000292  3.488545520  2.288206839  1.247018920 -0.197260086
[23] -1.554881352 -2.202977785 -1.038750927 -1.255710320 -0.006073465  0.080668247  0.461419926 -0.274830999  0.806213750  0.216685532 -0.398132839
[34]  0.122593066
</code></pre>

<p>While what I am expecting is that the first four values of $y$ to be 1,2,3,4 say since I've set the first four entries of innov to be zero. Can someone help me with this problem? Thanks in advance. </p>
"
"0.131533410441164","0.132453235706504","167044","<p>I am interested in analyzing the correlation between nationwide home prices and nationwide unemployment rates, both of which are leading economic indicators. I have data on nationwide home prices by using the Case-Shiller nationwide home price index (found here: <a href=""http://us.spindices.com/indices/real-estate/sp-case-shiller-us-national-home-price-index"" rel=""nofollow"">http://us.spindices.com/indices/real-estate/sp-case-shiller-us-national-home-price-index</a>), and I have data on nationwide unemployment rates from the Bureau of Labor Statistics. </p>

<p>Preliminary hypothesis/background info: Home prices are high when economy is doing well, and unemployment rates are low when the economy is doing well. So common sense tells me that as the unemployment rate rises, then the Case Shiller home price index decreases, which means there should be a negative correlation. But I don't know how to prove this. Here is a summary of the data I have:</p>

<p>I have the data for the Case-Shiller nationwide Home Price index for every month over the last ten years (1/1/2005-12/31/2014) which means 120 data points. I also have all the data for the nationwide Unemployment Rate over the same time period (1/1/2005-12/31/2014), which also means 120 data points. Both data are collected for the end of the month over the same time period, which means there is zero lag in the data sets.</p>

<p>What kind of correlation analysis do I need to do to determine if there is any correlation between these two data sets? Cross-correlation? Time-series analysis?</p>

<p>Thank you so much for any advice on how to start this research! Any help on what direction I should go would be incredibly appreciate. </p>

<p>Thank you!</p>
"
"0.117647058823529","0.118469775551818","20542","<p>I am using the lm() and princomp() functions in R to perform regressions on foreign exchange time series. I would like to weight the regressions (and PCA) such that 50% of the influence on the regression comes from the past 3 months, 25% from the previous 3 months, etc, but in a smooth fashion. Both functions take such a weighting series. What is a simple formula for generating a decaying weights series with a half life of 3 months (or any other period, for that matter), in R?</p>

<p>Ideally the sum of the weights will equal 1, although I don't think this is mandatory in lm().</p>
"
"0.0831890330807703","0.0837707816583391","130116","<p>I am a total beginner in using the bootstrap method...
At first I calculated the 95% confidence interval myself using R but it is not normally distributed so this would only be a veery rough estimate...</p>

<p>Therefore, I heard about bootstrapping, but it does not seem to work the way I tried but I don't understand it completely either...</p>

<p>I calculated the mean already, my table looks something like this: </p>

<p>Year   mean_N0     mean_N1      mean_N2     mean_Shannon
1998   13.4        0.563        0.459       0.643
...    ...         ...          ...         ...
2013   11          0.314        0.243       0.423</p>

<p>I used something like: </p>

<p>n = nrow(IBTS_A_mean2$IBTS_mean_N0)
boot.samples = matrix(sample(IBTS_A_mean2$IBTS_mean_N0, size = B * n, replace = TRUE),
B, n)</p>

<p>And I get the answer: </p>

<p>Error in sample.int(length(x), size, replace, prob) : 
  invalid 'size' argument</p>

<p>Can anyone help me with this? Is there a simple R routine available somewhere? </p>

<p>Thank you!</p>
"
"0.0588235294117647","0.0592348877759092","183245","<p>I'm new to regression and I am trying to perform regression analysis on two time series <code>Price A</code> (x-variable) and <code>Price B</code> (y-variable). When doing LASSO regression, the R-squared score is extremely low at 0.01. Log transformations made it worse.</p>

<p>Plotting out the scatter plot, we can see 2 different clusters. How should we handle such a data set? And how do we interpret such a scatter plot?</p>

<pre><code>plt.scatter(df['priceA'], df['priceB'])
plt.xlabel('Price A')
plt.ylabel('Price B')
</code></pre>

<p><a href=""http://i.stack.imgur.com/MZM1Z.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MZM1Z.png"" alt=""enter image description here""></a></p>
"
"0.0831890330807703","0.0837707816583391","189220","<p>I am looking to determine if there are regional differences in single common measurement across time, using R.</p>

<p>The data is of annual frequency for 51 years (1961 to 2011) for seven regions.</p>

<p>The variable measured is per capita emissions.</p>

<p>Plotting the data visually suggests that not all regions exhibit the same trend (some appear constant, others increase/decrease while yet others look like their trends may be increasing - getting steeper).</p>

<p>What statistical test(s) should be used to determine if the slopes of these regional time-series trends are indeed different from each other?  The measurements are likely not independent of each other within Region (behaviour is sticky), but we might presume that they are between Region...</p>

<p>If it helps, my dataset (in wide <code>mts</code> R format) is below.</p>

<p>Thanks!</p>

<pre><code>     Year    Europe Indus.Asia    Lat.Am NAfr.WCAsia NAm.Oceania       SSA  SSE.Asia
1961    1 0.3330287 0.08342768 0.2988371   0.1970832   0.6036110 0.2771904 0.1491002
1962    2 0.3399356 0.08739431 0.3050448   0.1957801   0.6072381 0.2774883 0.1519685
1963    3 0.3497737 0.09234678 0.3064860   0.1967561   0.6188448 0.2808335 0.1513993
1964    4 0.3506815 0.09567696 0.2994094   0.1964786   0.6400042 0.2846973 0.1527038
1965    5 0.3517937 0.10081783 0.2971080   0.1959702   0.6371043 0.2882228 0.1495615
1966    6 0.3655056 0.10409945 0.3022665   0.1959077   0.6513748 0.2845789 0.1506697
1967    7 0.3751051 0.10359561 0.3085563   0.1962348   0.6527076 0.2934402 0.1529256
1968    8 0.3847365 0.10244660 0.3181694   0.2015809   0.6682721 0.2886078 0.1558793
1969    9 0.3838484 0.10135446 0.3214369   0.2004127   0.6670239 0.2939225 0.1563187
1970   10 0.3936929 0.10550931 0.3190227   0.2011312   0.6736572 0.3029824 0.1582456
1971   11 0.3911788 0.10782480 0.3005018   0.2061686   0.6810935 0.3015153 0.1555705
1972   12 0.3888467 0.10796591 0.2995851   0.2042940   0.6862982 0.2964260 0.1543937
1973   13 0.4042535 0.11137749 0.2966723   0.1998279   0.6666261 0.2920530 0.1576311
1974   14 0.4114716 0.11116291 0.3081587   0.2041539   0.6770527 0.2975302 0.1573771
1975   15 0.4088594 0.11213601 0.3195318   0.2093902   0.6990652 0.2992955 0.1605997
1976   16 0.4124370 0.11137034 0.3329371   0.2110148   0.7338768 0.2997060 0.1594188
1977   17 0.4125538 0.11234512 0.3337914   0.2126674   0.7226528 0.3039447 0.1620297
1978   18 0.4198469 0.11851035 0.3397066   0.2194563   0.7101107 0.3064783 0.1637319
1979   19 0.4232864 0.12057965 0.3380299   0.2209554   0.6810473 0.3040716 0.1602157
1980   20 0.4218926 0.12148329 0.3505814   0.2280130   0.6721993 0.3029758 0.1621418
1981   21 0.4172976 0.12375568 0.3494269   0.2311256   0.6807635 0.3051217 0.1660875
1982   22 0.4223521 0.13089340 0.3373198   0.2317372   0.6819238 0.3058660 0.1661771
1983   23 0.4261567 0.13512950 0.3316797   0.2311859   0.6781721 0.2947787 0.1713330
1984   24 0.4327464 0.13905190 0.3364816   0.2352414   0.6853792 0.2891287 0.1726075
1985   25 0.4327769 0.14014033 0.3448070   0.2415244   0.6955721 0.2943122 0.1725749
1986   26 0.4384473 0.14360094 0.3596229   0.2449454   0.6966196 0.2941324 0.1753695
1987   27 0.4409924 0.14773305 0.3481004   0.2412722   0.6998172 0.2942876 0.1750773
1988   28 0.4418265 0.14960314 0.3485329   0.2437859   0.6900106 0.2968633 0.1778406
1989   29 0.4447421 0.15081434 0.3546949   0.2384495   0.6866791 0.3024815 0.1810750
1990   30 0.4304627 0.15639769 0.3570101   0.2413544   0.6841630 0.3049107 0.1816244
1991   31 0.4142964 0.15557267 0.3595744   0.2363494   0.6824669 0.3054693 0.1803897
1992   32 0.4099102 0.16369167 0.3620947   0.2492692   0.6920663 0.3002969 0.1849306
1993   33 0.3970080 0.17536967 0.3649345   0.2463124   0.6811545 0.3011804 0.1872747
1994   34 0.3827137 0.18347220 0.3754718   0.2482677   0.7008975 0.2940523 0.1884567
1995   35 0.3823503 0.19545531 0.3861121   0.2491750   0.6987574 0.2967518 0.1917361
1996   36 0.3844043 0.20075046 0.3952523   0.2507883   0.6990538 0.3023953 0.1937828
1997   37 0.3812992 0.20906418 0.3934072   0.2484025   0.6972838 0.3041432 0.1930203
1998   38 0.3798254 0.21622077 0.3870048   0.2545955   0.7007982 0.3047103 0.1946998
1999   39 0.3773650 0.22288219 0.4024141   0.2563572   0.7167966 0.3103474 0.1975481
2000   40 0.3792981 0.23270136 0.4036371   0.2594711   0.7132213 0.3080760 0.1984123
2001   41 0.3740450 0.23275714 0.4023274   0.2531616   0.7031493 0.3120869 0.1994249
2002   42 0.3778445 0.23755244 0.4036734   0.2609872   0.7121247 0.3163197 0.1958088
2003   43 0.3830538 0.24061360 0.4048166   0.2666359   0.7032099 0.3208936 0.2019877
2004   44 0.3879519 0.24239370 0.4106120   0.2687964   0.7123248 0.3294516 0.2035080
2005   45 0.3927197 0.24813350 0.4215930   0.2742297   0.7061623 0.3348791 0.2080581
2006   46 0.3947798 0.25229643 0.4313497   0.2810117   0.7046364 0.3409603 0.2142793
2007   47 0.3980420 0.25871431 0.4414887   0.2843196   0.7053338 0.3409665 0.2222694
2008   48 0.3975002 0.26601418 0.4477263   0.2842824   0.6871963 0.3479022 0.2245092
2009   49 0.4016810 0.27165581 0.4399089   0.2869924   0.6803058 0.3448820 0.2265546
2010   50 0.3940681 0.27902452 0.4439119   0.2939435   0.6725924 0.3563978 0.2337000
2011   51 0.3887767 0.28328307 0.4481589   0.3020620   0.6633836 0.3586240 0.2358033
</code></pre>
"
"0.0588235294117647","0.0592348877759092","77617","<p>I have done a cross-sectional regression of time-series average returns on estimated Betas (over the same time horizon) to determine average premiums. So far so good. But I was told that the standard t-statistics can be biased, due to the fact that betas are estimated.</p>

<p>There is a solution by:</p>

<blockquote>
  <p>Shanken (1992) - On the estimation of beta-pricing models [Review of Financial Studies].  </p>
</blockquote>

<p>It does some small adjustments to the formula of the estimated covariance matrix. However I don't understand how to implement this in R. The paper is also very mathematical, but the solutions are supposed to be easy if you look e.g. at Cochrane Asset Pricing, chap 12 or <a href=""http://www.uv.es/qf/06006.pdf"" rel=""nofollow"">http://www.uv.es/qf/06006.pdf</a>. I cannot find anything close to that in the original paper though. I think the notation is very different.</p>

<p>Does anyone know how to do it, or has done it already? I needed the adjusted formula in my context (in-sample regression), or even better the R-code.</p>
"
"0.0831890330807703","0.0837707816583391","104541","<p>I have a fairly basic statistics application question. Lets say I have a set of four fold-change values, representing the abundance of a factor as it passes through four consecutive time points:</p>

<pre><code>x&lt;-c(1.0, 1.2, 15.3, 0.2)
</code></pre>

<p>And I want to define its ""trend"" ie, a single-number representation of how it acts during the entirety of the time course. </p>

<p>In the example given, x has a general increasing trend. </p>

<p>I have tried using trendlines, but I get a lot of over-generalization of the trend, and my info is lost. Is there a more-informative solution to defining a ""trend"" of values as they pass through a time series?</p>
"
"0.0831890330807703","0.0837707816583391","183451","<p>I have yearly rainfall projections from five climate models for the period of 2010 through 2099.</p>

<p>This is a sample of my data:</p>

<pre><code>df=structure(list(Year = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 
2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 
2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 
2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 
2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 
2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 
2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 
2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 
2094, 2095, 2096, 2097, 2098, 2099), CanESM2 = c(1163.46560706632, 
1045.27764563553, 1192.99035592859, 1039.18159594737, 1069.85056057463, 
1109.61718257189, 1080.22225446686, 996.465673784495, 1330.31482267773, 
1135.09951956191, 1036.5620174695, 1171.19645849667, 1230.07980354375, 
1224.51936031341, 1059.0667847652, 1119.61709915399, 1093.01684435802, 
1123.72623649933, 1088.75970830321, 1096.55713940808, 1136.73460669118, 
1225.56589926383, 1230.47636335948, 971.878201373981, 1077.94938659653, 
1067.66509425384, 1278.93601510725, 1132.26048108291, 1172.28768446317, 
1152.31800688181, 1226.66419877102, 1284.17386265492, 1065.46324990181, 
1126.16088253523, 1199.68965697911, 1048.76918572934, 1067.22448843151, 
1233.49853962937, 1125.31148343304, 1110.58047100806, 1096.19567721952, 
1277.141766876, 1024.98721582214, 1065.74405206603, 1067.80045192583, 
1115.30598493354, 1189.9948818424, 1437.13931627192, 1157.00834795935, 
976.858312813719, 1116.52499339511, 1111.09759483922, 1199.08438212517, 
1071.78135303469, 1007.25547265899, 1071.39018135202, 1089.81301384961, 
1312.96135982164, 1119.14850815955, 1128.62351901672, 1085.83469993455, 
1490.14291868226, 1082.82332451136, 1094.93028550704, 1138.81957874767, 
1097.4652709511, 1106.83810592802, 1229.91613882742, 1133.7894640904, 
1138.07334287779, 1281.84520786833, 1044.82250530464, 1187.35200775616, 
1156.74028624375, 1128.97358661251, 1168.80857077186, 1029.65489771463, 
1024.77267482949, 1085.6743046951, 1400.35428529776, 1165.82818082067, 
1117.43356446918, 1158.87599998443, 1188.6860869167, 985.854151901199, 
1147.12318390124, 1077.81579069868, 1154.53800683884, 1129.23840124991, 
1230.68311935564), `GFDL-ESM2M` = c(1110.40656291483, 1165.52810385097, 
986.538514416152, 1095.38727970136, 908.291964523058, 1015.48130275974, 
910.223751648018, 1037.17545203031, 1064.46407778575, 821.925055711163, 
1096.30232777925, 1145.56041166767, 994.615426183943, 1017.5660122588, 
1019.44059497268, 1074.6107636677, 1199.64822769138, 965.649952638057, 
947.58734190787, 946.097731390942, 1001.45702762066, 1115.81807500611, 
1037.58905387885, 917.925884051693, 974.408569840561, 1065.98187555499, 
988.032759990321, 1156.4922081412, 1053.23214676772, 868.078070630601, 
1117.82655721168, 1129.78430893212, 1078.10041660786, 792.460976651464, 
913.253156177221, 919.321633035608, 1040.56072059552, 1090.87082960431, 
1071.27625924979, 1033.46091240528, 883.895086023128, 939.489773000334, 
1102.2485093512, 779.851230298964, 1100.51135836672, 966.685826958901, 
961.744760284696, 989.753864405085, 826.629851232945, 909.284738712619, 
873.476834043498, 940.715939174158, 1005.83652099677, 1146.69688740834, 
978.082249009606, 1075.71089568088, 1101.2751541009, 962.851445193924, 
1024.69987402044, 1114.93474064477, 1024.47604232346, 1056.49825285734, 
975.492763800606, 1101.02607222684, 920.749421732859, 903.596908739447, 
912.489759611543, 987.64092666866, 808.10762021923, 973.408506903507, 
969.489829593018, 968.373783576046, 993.01049783438, 1080.98673871791, 
869.923091297771, 934.285784415424, 956.3081757549, 1013.38311159891, 
856.448744053944, 874.516625599295, 1061.51117387629, 1128.54448558007, 
1009.21257737855, 1017.16456465614, 833.802010570892, 927.740880449226, 
1001.38592283682, 952.145679780795, 996.893937239244, 931.957115223409
), inmcm4 = c(997.516654764994, 1145.13308691203, 959.542879520275, 
903.786765066992, 937.432661124838, 1007.46247945661, 1013.04583737278, 
1061.26827856464, 1056.61600975265, 892.433893874083, 1061.97862849247, 
978.554696188197, 939.55857188082, 996.416292720483, 1079.06762584418, 
1068.98156370385, 822.342905469512, 1019.26614712021, 1116.36245976214, 
1008.22817419423, 902.855504214986, 1087.38893040866, 885.956566984022, 
1016.16044809847, 1230.18564397572, 870.800309733513, 1037.21587559441, 
861.236791929213, 948.771432189766, 871.382698104435, 1089.3915218932, 
1054.49129365067, 941.903074357385, 973.558427724714, 938.638805666033, 
1006.66304703962, 863.28436030157, 806.021669160746, 869.369012721652, 
934.149986789314, 974.449974533922, 953.915114968656, 998.865234642777, 
949.027492106268, 967.701742440291, 952.306409621697, 959.027780253679, 
1033.69516116182, 992.987130941704, 1132.85542518598, 857.240436050392, 
1125.7240784834, 905.912164925948, 791.911242057151, 950.50688942409, 
684.62210670821, 928.505401464914, 898.455471168554, 881.937660493059, 
1043.17046368419, 990.173635925243, 872.642891720416, 968.112830276678, 
1156.70704372798, 954.681404468287, 1208.90589934053, 717.191066272881, 
985.50086154963, 1019.84388106248, 836.40573448035, 923.512492484388, 
942.453644704655, 1297.40333367214, 993.491117038875, 1035.26544859565, 
1056.57888641097, 968.592730428887, 887.591583109264, 1109.2935814163, 
923.189139410888, 972.33707020049, 1071.81484320842, 921.974872423456, 
956.677679924482, 886.258785523372, 903.500244794601, 1006.38055244068, 
914.283438551077, 779.704380831126, 1121.49522817412), 
`MRI-CGCM3` = c(848.301135629757, 
1065.98505740902, 888.813467571221, 1070.05226905271, 929.86581326247, 
949.338629810498, 1055.34828003874, 950.605580125944, 1025.98651471033, 
1062.58098758771, 1129.04220061327, 1113.69640112993, 983.264122816401, 
909.350171139101, 1049.76982306699, 991.477943714084, 1050.37713605355, 
957.954418661432, 1198.88425343781, 1019.03177011263, 892.820372968912, 
1117.83057226823, 1172.12350726162, 1045.54420748303, 1021.44374290702, 
1226.69766824788, 1076.26278716493, 892.835182446986, 1081.36552627508, 
1119.61825607435, 1101.7977981361, 1166.29589473449, 1114.24627378901, 
1111.95127710174, 1188.29593429323, 1005.43226899398, 1062.11294060983, 
1177.29197380333, 1233.89949211259, 949.362448506262, 1080.82477119637, 
848.404135782853, 1173.85210197539, 983.618625752799, 1023.98178052282, 
1174.67507789645, 1061.16686611069, 1145.73728230606, 1109.37233672472, 
1131.39932373277, 1192.52567101682, 1035.72075610234, 1170.69030772479, 
1402.30470791808, 828.360972784472, 1119.41148932651, 1418.65415788168, 
1111.1623167992, 1031.72445008109, 1133.74439397242, 1256.44423788796, 
1205.53255936418, 1246.028794886, 1353.06641467104, 1203.75084317503, 
1260.7607967303, 1133.17128359068, 1128.75225848513, 1248.50107501562, 
1125.34125579299, 1107.47674713103, 1286.39203071485, 1134.21641721783, 
1368.91059626765, 1249.62807137382, 900.188446846731, 1215.72863596362, 
1217.36216136844, 1239.46334238843, 1225.6431042983, 1293.28424474932, 
1316.91205393836, 1344.5824880094, 1431.35684547467, 1228.35587386956, 
1063.69382941901, 1366.01835107431, 1178.54993691379, 1425.59197731446, 
1439.18433794687), `NorESM1-M` = c(1104.36330130108, 1143.59864836685, 
1156.96992286211, 1225.47647032022, 1051.47222270552, 1225.96709059087, 
1169.02846946487, 1008.33133254077, 1041.4377020821, 1094.28488840048, 
1055.7942901173, 1182.85110699334, 1074.64700888046, 1220.40875347693, 
974.413995298429, 1082.02850933558, 1149.38849910412, 1177.55048525853, 
1159.36173358997, 1159.47946095381, 1013.84251995501, 1039.38081678502, 
1175.99482174589, 1075.9123379481, 1124.78923639009, 1072.4595220739, 
1059.99103769952, 1147.12456096131, 1054.66087104772, 1176.13768054148, 
1051.63089069775, 1124.1299767682, 958.783755377516, 1117.04595412512, 
1187.68311854194, 922.314885386565, 1118.97706673769, 1089.94177070272, 
1104.64546357211, 1129.91397601017, 1076.23273471979, 1117.36435679852, 
950.901478997885, 1171.61731400339, 1161.05025755828, 1122.1168542936, 
1132.68254406784, 1116.39483899695, 1052.38202713855, 1136.25445489531, 
1212.37181415053, 1113.58170133876, 988.657267351285, 1077.72113312282, 
1132.82563231238, 1060.24278563685, 1054.61879253374, 1138.19823195196, 
1086.79680899531, 1089.53650740066, 1101.56581663204, 1163.2291892284, 
1139.8293826996, 1125.56954148811, 1024.34059527431, 1152.0981675668, 
1064.32755358318, 1009.13933210229, 1210.55906508268, 1136.30532890842, 
1167.56870811327, 1163.3444730351, 1030.3320225021, 1334.11561872902, 
1186.00627022209, 1208.89776922561, 1172.03331588195, 1118.47337542874, 
1087.92709804022, 942.085245232004, 1159.78077235011, 1112.90807832063, 
1105.76965865985, 1104.31469614124, 1231.97413518222, 1228.57728152883, 
1208.44334175124, 1120.57185259478, 1014.7309899663, 1144.9384886849
)), .Names = c(""Year"", ""CanESM2"", ""GFDL-ESM2M"", ""inmcm4"", ""MRI-CGCM3"", 
""NorESM1-M""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>

<p>Based on visual analysis, I suspect that there is a increasing <code>precip</code> trend in all models. But I need some kind of metric that can confirm an quantify this trend.</p>

<ul>
<li><strong>For each model in this dataset, how can I calculate the rainfall trends for every year and for decades (2010-2019, 2020-2029 and so on)?</strong>  </li>
<li><strong>Is it a simple linear regression of year against <code>precip</code> sufficient to show the trend?</strong></li>
</ul>
"
"0.117647058823529","0.0592348877759092","106038","<p>I know that this is probably a question that's been asked plenty of times, but i haven't seen an answer that's both accurate and simple. How do you estimate the appropriate forecast model for a time series by visual inspection of the ACF and PACF plots? Which one, ACF or PACF, tells the AR or the MA (or do they both?) Which part of the graphs tell you the seasonal and non seasonal part for a seasonal ARIMA?</p>

<p>Take for instance these functions:</p>

<p><img src=""https://i.imgur.com/E64Sd7p.png"" alt=""enter image description here""></p>

<p>They show the ACF and PCF of a log transformed series that's been differenced twice, one simple difference and one seasonal.</p>

<p>How would you caracterize it? What model best fits it?</p>

<p>Thanks in advance!</p>

<p><strong>EDIT:</strong> Added raw data</p>

<p>Original data: <a href=""http://pastebin.com/KRJnXzXp"">here</a></p>

<p>Log transformed data: <a href=""http://pastebin.com/JR3bkctv"">here</a></p>

<p><strong>EDIT:</strong> Corrected ACF and PACF functions (previous ones were overdifferentiated)</p>
"
"0.155632430062623","0.156720781993876","106183","<p>I want to run a regression in R with different datasets.
The question is whether stock performance (daily log return) is influenced by factors like interest rates (the one set by fed or ECB), size of the board of directors (e.g. has been long 7 and then switches to 8) and whether there is an audit committee or not (binary, <code>1</code> or <code>0</code>).</p>

<p>I guess in theory I could just add the interest rate, size of board, etc. to the daily log returns â€“ meaning the interest rate stays fixed for 80 days and then suddenly switches to a new value. However, because this is a large dataset of 10y daily log returns, and I just cannot figure out how to do it automatically in Excel, I am wondering whether it is possible to do that in R or to run a regression on the different data sets. Note: every dataset has a corresponding date.</p>

<p>Since 2 days ago, I've been searching the web but cannot find anything which solves my problem, so your help is really appreciated. If you have any links to papers or the like, it would be totally sufficient :) Although I did find a few papers on that topic, they were cutting-edge theory, which is way too complicated for me.
Thank you!</p>
"
"0.0588235294117647","0.0592348877759092","107823","<p>I am working with time series values which are all in the closed interval [0, 1]; these values represent relative frequencies, i.e., empirical probabilities. I would like to create a model such that all forecasted values are within [0, 1], but it would also be fine if the model's output was strictly within the open interval (0, 1).</p>

<p>This answered question tackles the lower bound aspect of my question, but not the upper bound aspect: <a href=""http://stats.stackexchange.com/questions/80859/how-to-achieve-strictly-positive-forecasts"">How to achieve strictly positive forecasts?</a></p>

<p>I'd like to use the R <code>forecast</code> package if possible to achieve this, but I am open to other suggestions.</p>
"
"0.256405820208275","0.258198889747161","35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.145580807891348","0.125656172487509","147619","<p>I'm trying to build a model that can predict streamflow for an alpine (snowmelt-fed) watershed using snow albedo (roughly, the energy reflectance of the snow) data. Albedo controls the melt of the snowpack, and higher albedo means slower melt, and vice versa. I have daily time-series data for both the snow albedo and streamflow, for 12 years from 2002-2013. The albedo time-series was obtained by spatially-averaging albedo data (raster files) from NASA's MODIS satellite.</p>

<p>I have tried various methods (simple regression, GLMs, GAMs, decision trees and random forests) to build the flow prediction model, but all of them fail because of the autocorrelated relationship between albedo and flow. Since the albedo is a snowpack property, there is a lag between it and the flow (related to snowmelt).</p>

<p>The Cross correlation function (CCF) between albedo and flow is shown below:</p>

<p><a href=""http://imgur.com/PpW1Kpy"" rel=""nofollow""><img src=""http://i.imgur.com/PpW1Kpy.png"" title=""source: imgur.com"" /></a></p>

<p>I have tried to include albedo lags of various days into the models, but I'm not able to mimic the distributed lag relationship between albedo and flow. I have tried to add precipitation, temperature and other climatic data to the predictors, but they don't seem to help. There are similar lagged and cross-correlation problems between these other predictors and flow.</p>

<p>The albedo, flow, precipitation and air temperature time-series are shown below:</p>

<p><a href=""http://imgur.com/Kb8Ta6q"" rel=""nofollow""><img src=""http://i.imgur.com/Kb8Ta6q.png"" title=""source: imgur.com"" /></a></p>

<p>Is there a statistical or machine learning technique in R that I can explore to build the albedo-streamflow model?</p>

<p>Thank you.</p>
"
"0.0831890330807703","0.0837707816583391","63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.131533410441164","0.132453235706504","108666","<p>Can anyone give me some advice on how to fit a model with linear (some categorical), non-linear and time series components in R? I don't want to use a non-parametric model like a Loess smooth or similar, as I want a simple and well defined confidence interval and the ability to extrapolate. I also don't have sufficient data to justify a non-parametric model. The model form should be as below, note that there are time series components that exhibit seasonal behavior (and my notation probably doesn't express this well!):</p>

<p>$$\hat{Y} = \beta_0 +\beta_1 X_1 + e^{\beta_2X_{1}}+\beta_3 f_{harmonic}(X_t) + \epsilon_i 
$$ 
Where $\epsilon_i \sim N(\mu, \sigma) $.</p>

<p>I know that the <code>nls</code> package can fit non-linear models, and I could use that to fit a harmonic function to get the time series part of the model. But I understand that it doesn't work for factor type variables, so I'm reluctant to use it. </p>

<p>Equally, I understand that the <code>dynlm</code> package can be used to fit dynamic linear models (and could therefore account for seasonality), but it cannot fit dynamic models of the form specified above to the best of my knowledge. </p>

<p>Would the preferred approach be to simply perform each model sequentially on the residuals of the previous procedure? If I did this, how would I ultimately combine them together to create predictions on new data?</p>
"
"NaN","NaN","209173","<p>I am working on a project, and I am absolutely new to forecasting and not so strong in statistics. I have an employee data for the last 7 years, along with the other variables like economic growth, employee turnover, vacancies, and some other economical factors. 
I have to do forecasting for the next 5 years. I have some questions: </p>

<ul>
<li>Is it possible to do a time series analysis with more than one explanatory variable? Can this be done in R? </li>
</ul>

<p>I would appreciate any kind of help. Thanks in advance!! </p>
"
"0.176470588235294","0.177704663327728","63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.195095575903259","0.178599906592273","132845","<p>I have a problem in interpreting what the <code>arima</code> function in R is doing.  I have the following code:</p>

<pre><code>x &lt;- 1000*.8^(0:100)
arima(x, order = c(1,0,0), include.mean = F)
</code></pre>

<p>The resulting coefficient is ""0.9988"".  But I would think the coefficient should be exactly ""0.8"", since <code>x[t] = 0.8 * x[t-1]</code>.</p>

<p>I must be missing something that R is doing in processing the data.</p>

<p>Any help would be appreciated.</p>

<p><strong>New Info I:</strong>
If I change the function to </p>

<pre><code>arima(x, order = c(1,0,0), include.mean = F, method=""CSS"")
</code></pre>

<p>then it solves for the correct coefficient of 0.8.</p>

<p>My problem that I am generally finding that the function <code>arima</code> with <code>order = c(1,0,0)</code> is often producing different results to:</p>

<pre><code>lm(x[-1] ~ I(x[-length(x)]))
</code></pre>

<p>for all sorts of different time series that I am reviewing. </p>

<p><strong>New Info II:</strong>
Some of the comments and answers below are concerned that there is no random fluctuation in my data.  I did this to make the problem as simple as possible, but even if you add in random fluctuation, <code>arima</code> still produces the same wrong results in the default case.  To add insult to injury, <code>arima</code> will sometimes get the right answer if you change <code>method = ""CSS""</code>. This suggest that there is perhaps a computational issue with <code>arima</code> and not my misunderstanding the statistical model. Here is an extended set of two examples that highlight the problem (I ignore the intercept difference between <code>lm</code> and <code>arima</code> as these two items are not the same thing, but the coefficients should be.</p>

<pre><code>set.seed(1)
# Example 2
x &lt;- rep(1000,200)
for (i in 2:200) x[i]=x[i-1]*.8 + runif(1)*100 
plot(x,type=""l"")
arima(x, order = c(1,0,0))  #Incorrect answer:  coefficient = 0.9944
arima(x, order = c(1,0,0), method=""CSS"") # correct answer: coefficient = 0.7915
lm(x[-1] ~ I(x[-length(x)])) # correct answer: coefficient = 0.7914

# Example 3
x &lt;- rep(0,200)
for (i in 2:200) x[i]=x[i-1]*.8 + runif(1)*.2 
plot(x,type=""l"")
arima(x, order = c(1,0,0))  #Incorrect answer:  coefficient = 0.8836
arima(x, order = c(1,0,0), method=""CSS"") # correct answer: coefficient = 0.8158
lm(x[-1] ~ I(x[-length(x)])) # correct answer: coefficient = 0.8158
</code></pre>
"
"0.117647058823529","0.118469775551818","149322","<p>I am trying to use multiple regression for a time series dataset. I have values corresponding to a variable measured by 24 hrs for 4 months. Since there was a pattern which repeated every 24 hours I used 23 dummy variables for the hourly variations in values.</p>

<p>I used log transformation of the dependent variable before performing multiple regression. The fitted coefficients were highly significant and the R-squared was around 0.99.
However, when I look at the Residuals vs fitted plot, it seems sort of weird. According to the plots <a href=""http://www.r-bloggers.com/model-validation-interpreting-residual-plots/"" rel=""nofollow"">here</a>, my plot is neither biased nor heteroskedastic, but it also doesn't look like random noise. Can someone help me find the issue here? 
<img src=""http://i.stack.imgur.com/sCGPx.png"" alt=""enter image description here"">
Also please find below a plot of the observed and fitted model for first 500 hrs<img src=""http://i.stack.imgur.com/jiTts.png"" alt=""Observed Values VS Time in hours overlaid by fitted model in red""></p>
"
"0.0588235294117647","0.0592348877759092","208870","<p>I am trying to study the effects of foreign direct investment (FDI) in growth of gross domestic product (GDP). It's considered that FDI positively impacts GDP growth and it makes sense to assume that FDI in a particular year will cause GDP growth in following years as the investment produces goods and services, provides jobs and pays taxes not just in the year of investment but also in years following. So does it make sense to use lagged independent FDI variable? And brief instructions regarding doing the analysis in <code>R</code> would be much appreciated.
TIA </p>
"
"0.117647058823529","0.118469775551818","134644","<p>Do you see trends in my residual plots? These residuals plot show the standardized residuals against fitted values, origin period, calendar period, and development period. The patterns in any direction can be result of trends. There seems to be a trend on my bottom left plot, do you agree? </p>

<p><img src=""http://i.stack.imgur.com/LceG5.png"" alt=""1"">
This is a research project. I am asking your inputs to confirm my assumptions. </p>
"
"0.117647058823529","0.118469775551818","135068","<pre><code>  I have a time series of log returns (from 01-01-2007 to 31-12-2012).  I need to predict the t+1,t+2, ....... t+1510  log stock returns. 

basically what i need is to predict the 1 day ahead return for every single day. so i     would like to predict the return on the 02-01-2007, the 03-01-2007........till the 01-01-  2013

          I have therefore done the following coding using fGARCH in `R`.

      AIG &lt;- garchFit(formula = ~arma(1, 1) + garch(1, 1), data =R.AXP, cond.dist = ""std"")
      Predict_AIG &lt;- predict(AIG, n.ahead = 1510)
      But this gives me the prediction on a single forecast horizon(so it gives me the forecast 1, 2,3.....1510 days from the 01-01-2007) not the forecast on the 02-01-2007, 03-01-2007,...........01-01-2013.

    Please can anyone help me to find a different way of coding.
    Thank you.
</code></pre>
"
"0.144087631928422","0.145095250022002","85913","<p>I want to fit a DLM with time-varying coefficients, i.e. an extension to the usual linear regression,</p>

<p>$y_t = \theta_1 + \theta_2x_2$.</p>

<p>I have a predictor ($x_2$) and a response variable ($y_t$), marine &amp; inland annual fish catches respectively from 1950 - 2011. I want the DLM regression model to follow,</p>

<p>$y_t = \theta_{t,1} + \theta_{t,2}x_t$</p>

<p>where the system evolution equation is</p>

<p>$\theta_t = G_t \theta_{t-1}$</p>

<p>from page 43 of Dynamic Linear Models With R by Petris et al.</p>

<p>Some coding here,</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- fishdata$marinefao
    y &lt;- fishdata$inlandfao

lmodel &lt;- lm(y ~ x)
summary(lmodel)
plot(x, y)
abline(lmodel)
</code></pre>

<p>Clearly time-varying coefficients of the regression model are more appropriate here. I follow his example from pages 121 - 125 and want to apply this to my own data. This is the coding from the example</p>

<pre><code>############ PAGE 123
require(dlm)

capm &lt;- read.table(""http://shazam.econ.ubc.ca/intro/P.txt"", header=T)
capm.ts &lt;- ts(capm, start = c(1978, 1), frequency = 12)
colnames(capm)
plot(capm.ts)
IBM &lt;- capm.ts[, ""IBM""]  - capm.ts[, ""RKFREE""]
x &lt;- capm.ts[, ""MARKET""] - capm.ts[, ""RKFREE""]
x
plot(x)
outLM &lt;- lm(IBM ~ x)
outLM$coef
    acf(outLM$res)
qqnorm(outLM$res)
    sig &lt;- var(outLM$res)
sig

mod &lt;- dlmModReg(x,dV = sig, m0 = c(0, 1.5), C0 = diag(c(1e+07, 1)))
outF &lt;- dlmFilter(IBM, mod)
outF$m
    plot(outF$m)
outF$m[ 1 + length(IBM), ]

########## PAGES 124-125
buildCapm &lt;- function(u){
  dlmModReg(x, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(IBM, parm = rep(0,3), buildCapm)
exp(outMLE$par)
    outMLE
    outMLE$value
mod &lt;- buildCapm(outMLE$par)
    outS &lt;- dlmSmooth(IBM, mod)
    plot(dropFirst(outS$s))
outS$s
</code></pre>

<p>I want to be able to plot the smoothing estimates <code>plot(dropFirst(outS$s))</code> for my own data, which I'm having trouble executing. </p>

<p><strong>UPDATE</strong></p>

<p>I can now produce these plots but I don't think they are correct.</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- as.numeric(fishdata$marinefao)
    y &lt;- as.numeric(fishdata$inlandfao)
xts &lt;- ts(x, start=c(1950,1), frequency=1)
xts
yts &lt;- ts(y, start=c(1950,1), frequency=1)
yts

lmodel &lt;- lm(yts ~ xts)
#################################################
require(dlm)
    buildCapm &lt;- function(u){
  dlmModReg(xts, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(yts, parm = rep(0,3), buildCapm)
exp(outMLE$par)
        outMLE$value
mod &lt;- buildCapm(outMLE$par)
        outS &lt;- dlmSmooth(yts, mod)
        plot(dropFirst(outS$s))

&gt; summary(outS$s); lmodel$coef
       V1              V2       
 Min.   :87.67   Min.   :1.445  
 1st Qu.:87.67   1st Qu.:1.924  
 Median :87.67   Median :3.803  
 Mean   :87.67   Mean   :4.084  
 3rd Qu.:87.67   3rd Qu.:6.244  
 Max.   :87.67   Max.   :7.853  
 (Intercept)          xts 
273858.30308      1.22505 
</code></pre>

<p>The intercept smoothing estimate (V1) is far from the lm regression coefficient. I assume they should be nearer to each other. </p>
"
"0.0588235294117647","0.0592348877759092","135651","<p>I've created an Arima model based on past forex closing prices using auto arima, which has generated a (0,1,0) ARIMA model.</p>

<pre><code>&gt; auto.arima(ma5)
Series: ma5 
ARIMA(0,1,0)                    

sigma^2 estimated as 5.506e-07:  log likelihood=11111.42
AIC=-22220.83   AICc=-22220.83   BIC=-22215.27
</code></pre>

<p>I next tried to plot the forecasted values, but as you can see all predictions are constant. Anyone know what I'm doing wrong?</p>

<p><img src=""http://i.stack.imgur.com/Er1k5.png"" alt=""enter image description here""></p>
"
"0.155632430062623","0.156720781993876","135847","<p>After you decompose a univariate time series with stl() function in R you are left with the trend, seasonal and random components of the time series. Is it valid to use those components to then model the original timer series with additional other variables?</p>

<p>For example:</p>

<pre><code>&gt; tsData
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

&gt; stl(tsData, s.window = ""periodic"")
 Call:
 stl(x = tsData, s.window = ""periodic"")

Components
            seasonal     trend   remainder
Jan 2012 -24.0219753  36.19189   9.8300831
Feb 2012 -20.2516062  37.82808   8.4235219
Mar 2012  -0.4812396  39.46428  -4.9830367
Apr 2012 -10.1034302  41.32047   1.7829612
...
Sep 2014   2.2193527 165.55136  -1.7707170
Oct 2014   7.3239448 169.33893  -0.6628760
Nov 2014  18.4285405 173.12650   1.4449614
Dec 2014  30.5244146 176.84390  -3.3683103
</code></pre>

<p>Now if I wanted to model the time series with a linear model with some other variables is it valid to do so?</p>

<pre><code>lm(index ~ trend + seasonal + s1 + s2, data)
</code></pre>

<p>When I run that model I get an R-squared = .98 which make sense considering that the original time series index is just the sum of trend + season + error. I guess what I'm concerned about is using a linear model with time series data I want to make sure I'm not violating some major rules of linear regression. I figure since I have the seasonal variable I'm essentially controlling for that element and hopefully reducing the auto correlation or am I since the R-squared is so high? Any help is appreciated!</p>
"
"NaN","NaN","183854","<p>I want to estimate a vector-valued model</p>

<p>$$\mathbf{y}_t = a\mathbf{y}_{t-1}+b\mathbf{y}_{t-2}+\cdots$$</p>

<p>Here, each $\mathbf{y}_t\in\mathbb{R}^n$ and the coefficients $a,b,\dotsc$ are real numbers (unlike in VAR, where they are matrices). I wonder what the best way to do this in R is. Applying the function <code>ar</code> doesn't seem to work for me ([i] <strong>is this only for series that lie in $\mathbb{R}^1$</strong>?), and when I apply standard linear regression, I'm not sure how I should test for the order of the model, etc. [ii] <strong>Should I go for the $R^2$ value and significance of the coefficients?</strong></p>

<p>Also, I have my data in a matrix $Y$ where each column corresponds to one time point. Currently, I regress as follows </p>

<pre><code>Y = c(Y[,10], Y[,9],..., Y[,2])
Y.l1 = c(Y[,9],..., Y[,1])
fit = lm(Y ~ 0 + Y.l1)
</code></pre>

<p>[iii] <strong>Is there a more elegant way?</strong></p>
"
"0.144087631928422","0.145095250022002","188510","<p>I have developed a model (TSM) which is very good at forecasting daily revenue, however it is very black box. The TSM is univariate, whereas the regression models are multivariate. My goal here is to identify the causal factors of revenue for policy recommendations using regression analysis in R. The models that I am using are decision tree, random forest, linear model (some variables), linear model 2 (all variables), xgboost, and lasso regression. All variables are numeric. </p>

<p>Looking at random forest vs linear regression, (both rmse and mae are similar for these two), we see something odd: Looking at the variable importance plot for random forest, variable x is the least important. However, for the regression model, it is highly significant. Furthermore, lasso regression also indicates that the variable is very important. How can this be so? </p>

<p>This variable has a important policy implication to the business, so which model is right - is variable x the most important or is it the least important?  I vaguely recall Breiman writing a paper some time ago discussing something along the lines of this. </p>

<p>The below chart is the out of sample accuracy of the models over a 4 month time period. </p>

<pre><code>&gt; a
    tsm      tree        rf       lm       lm2      xgb      lasso
 0.9715964 0.9854246 0.9904363 0.981333 0.9817757 0.974603  0.997324
</code></pre>

<p>PS, I am sorry I cannot disclose the variables themselves since they are confidential to the company.</p>

<p>Thank you for your assistance!</p>
"
"0.166378066161541","0.167541563316678","196653","<p>How do I assign more weight to more recent observations in R?</p>

<p>I assume this as a commonly asked question or desire but I have a hard time figuring out exactly how to implement this.  I have tried to search alot for this but I am unable to find a good practical example.</p>

<p>In my example I would have a large dataset over time.  I want to say apply some sort of exponential weighting of the rows of data that are more recent.  So I would have some sort of exponential function saying observations in 2015 are ___ more important to training the model than observations in 2012.</p>

<p>My dataset variables contain a mix of categorical and numerical values and my target is a numerical value - if that matters.  </p>

<p>I would want to test/try this out using models such as GBM/Random Forest, ideally in the CARET package.</p>

<h2>update-question</h2>

<p>I appreciate response given below on how to exponentially decay the weight by the date distance between two points.</p>

<p>However, when it comes to training this model in caret, how exactly do the weights factor in?  The weight value in each of the training rows is the distance between some point in the future and when that point historically occured.</p>

<p>Do the weights come into play only during the prediction?  Because if they come into play during the training, wouldn't that cause all sorts of problems as various cross-folds would have varying weights, trying to predict something that may have actually at a point in time before it?</p>
"
"0.0588235294117647","0.0592348877759092","85592","<p>I'm having issues forecasting a model of the following form.</p>

<pre><code>y1 &lt;- tslm(data_ts~ season+t+I(t^2)+I(t^3)+0)
</code></pre>

<p>It fits my data very well, but I run into a problem when attempting to do this:</p>

<pre><code>forecast(y1,h=72)
</code></pre>

<p>This is the error that R gives me.</p>

<pre><code>""Error in model.frame.default(Terms, newdata, na.action = na.action,  
 xlev = object$xlevels) : 
   variable lengths differ (found for 't')
In addition: Warning message:
'newdata' had 72 rows but variables found have 1000 rows"" 
</code></pre>

<p>As far as I can tell, this has something do with using <code>tslm</code> and having the cubic function in it. If I just use <code>tslm(data_ds~season+trend)</code> everything works out fine, but I specifically need the model mentioned earlier. How can I forecast my model?</p>
"
"0.166378066161541","0.167541563316678","44617","<p>I'm trying to measure the impact that rainfall causes in the number of incoming calls in a insurance-company. I have 4 years of daily data.</p>

<p>The plots below shown the correlation plot for each year:
<img src=""http://i.stack.imgur.com/KTdSX.png"" alt=""enter image description here""></p>

<p>The same plots as above, but now taken the weekly-mean for each variable:
<img src=""http://i.stack.imgur.com/0iZnQ.png"" alt=""enter image description here""></p>

<p>The rainfall has a <strong>yearly</strong> seasonality and the call-center data has a <strong>weekly pattern</strong>. The idea is to come up with a <em>weekly-based model</em>, so that i can measure the impact that a <em>weekly mean rainfall forecast</em> will cause.</p>

<p>Plotting the whole dataset, taken weekly-means (image below):
<img src=""http://i.stack.imgur.com/CrSb9.png"" alt=""enter image description here""></p>

<p>I'd like some suggestions on how to measure this variable 'impact' - i can try to split the rainfall data into 3 categories (low, normal, high), then build some model.</p>

<p>Thanks for any help! (i'm using R for analysis).</p>
"
"0.101885341621699","0.102597835208515","190586","<p>I am using cross correlation to demonstrate a potential link between two time series (ext &amp; co). Both series are strongly autocorrelated, so it is difficult to assess the dependence between the two series. For a quick preliminary analysis, the cross correlation shows a clear (somehow delayed) link between the two time series, although it might spurious. <a href=""http://i.stack.imgur.com/eHUnj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eHUnj.jpg"" alt=""CCF""></a>. Prewhitening seems to be the best option; I will prewhiten my x variable by fitting an ARIMA process and then use the coefficients to filter my variable y. My question is if I should estimate the coefficients of the ARIMA process (for example using <code>auto.arima</code>) using my series x or by using the residuals of the OLS regression of x on y.</p>
"
"0.0588235294117647","0.0592348877759092","191156","<p>I know that $Y_{t} = a + bY_{t-1} + \epsilon$ is named as autoregression model. I am dealing with the model like: $Y_{t} = a + bY_{t-1} + cX_{t} + dX_{t-1} + \epsilon$. I could not find any useful information when I searched multivariate autoregression. Could anyone tell me the model name and how to do it in R?</p>"
"NaN","NaN","","<r><regression><time-series><terminology>"
"NaN","NaN","71038","<p>I have a dataset that contains a few hundred transactions from a three suppliers operating in 100+ countries over a three year period. </p>"
"NaN","NaN","<p>We've found that the country of sales is not a significant factor in the prices achieved (the products are more or less global commodities). All of the prices have declined significantly over time. Any one day can have multiple transactions at different prices from the same supplier (i.e. in different countries). </p>",""
"NaN","NaN","<p>I would like to test whether there is a statistically significant difference in the prices charged by the different suppliers.</p>",""
"NaN","NaN","<p>The data look something like this:</p>",""
"NaN","NaN","<pre><code>    Country X  1/1/2010  $200 Supplier A",""
"NaN","NaN","    Country Y  1/1/2010  $209 Supplier A",""
"NaN","NaN","    Country Z  1/1/2010  $187 Supplier A",""
"NaN","NaN","    Country A  1/1/2010  $200 Supplier B",""
"NaN","NaN","    Country X  1/2/2010  $188 Supplier B",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>Any ideas on how to do this?.....</p>",""
"NaN","NaN","","<r><time-series><multiple-regression>"
"0.117647058823529","0.118469775551818","112860","<p>I am doing a time series regression between 2 variables. I used the dynlm library in R. I'm trying to understand how to interpret the results. </p>

<p>Could you please point out where I am getting it wrong: </p>

<p>1) The R squared seems very low -- this indicates a weak linear relationship. Or too many outliers. 
2) Normal QQ plot shows that there are a significant number of outliers -- at a later date in the time series? 
3) Does the 'Residuals vs Fitted' plot show that the model is a pretty good fit for most of the data, except for those outliers? </p>

<p>Any suggested readings (esp open-sourced materials available online) also appreciated. </p>

<hr>

<pre><code>Call:
dynlm(formula = P ~ L(B))
Residuals:
    Min      1Q  Median      3Q     Max 
-63.711 -27.687 -14.907   2.364 146.157 
Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 17.485051  13.500833   1.295    0.197     
L(B)         0.019422   0.002384   8.146 5.84e-14
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
Residual standard error: 48.17 on 182 degrees of freedom
Multiple R-squared:  0.2672,    Adjusted R-squared:  0.2632 
F-statistic: 66.36 on 1 and 182 DF,  p-value: 5.838e-14
</code></pre>

<p><img src=""http://i.stack.imgur.com/xJbHF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/6cpT2.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/JZbmX.png"" alt=""enter image description here""></p>
"
"0.0831890330807703","0.0837707816583391","112899","<p>I have Beta as my independent variable and Economic value added (EVA) as my dependent variable. 
To calculate EVA I need to use Cost of capital and to calculate that I have to use Beta, so is it possible to use EVA as the dependent variable. thank you </p>

<blockquote>
  <p>EVA= Net Operating Profit After Taxes (NOPAT) - (Capital * Cost of Capital) </p>
</blockquote>
"
"NaN","NaN","214308","<p>I am studying time-series econometrics and in particular Dynamic Linear Models for multivariate time-series. </p>"
"NaN","NaN","<p>Someone can help me in understanding which is the difference between SUTSE (Seemingly Unrelated Time Series Equations) and SUR (Seemingly Unrelated Regression) Dynamic Linear Models?</p>",""
"NaN","NaN","<p>How can I specify a SUTSE and a SUR model on <code>r</code>? I think <code>dlmModPoly</code> may be of help but I am a bit stuck</p>",""
"NaN","NaN","","<r><time-series><dynamic-regression><sur>"
"NaN","NaN","113413","<p>I have around 800 companies for only two years period. However, around 200 of them have only one year observation. Is it still possible to conduct panel data analysis with such data 
Thank you </p>
"
"0.0831890330807703","0.0837707816583391","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.155632430062623","0.156720781993876","113598","<p>This question is about three-way interaction and the possibility of applying without second lower terms with keeping the main variables in the equation not like the other questions. In fact the other answers suggest there is possibility of applying . I am not here to find the best solution because I know it and I already included in my question, but to know whether is it possible regardless if it is preferable or not. thank you and please open my question for discussion </p>

<p>The widely known regression equation for assessing the three-way interaction is</p>

<p>$$ Y= B_1 X+B_2 Z+B_3 W +B_4XZ+B_5XW+B_6ZW+B_7XZW+B_0 $$</p>

<p>All lower order terms is included in the regression  equation for the B7 coefficient  to
represent  the  effect  of the  three-way  interaction  on  Y. </p>

<p>Is there possible way to skip the lower order terms and include only the higher term? as in:</p>

<p>$$ Y= B_1 X+B_2 Z+B_3 W +B_4XZW+B_0 $$</p>

<p>And how many observations do I need to perform such equation if X &amp; Z are continuous variables and W is dummy variable ?</p>

<p>I will be thankful if anyone can provide me with any suggestions </p>
"
"0.155632430062623","0.156720781993876","45482","<p>What I'm trying to do is estimate the following GARCH(1,1) model in <code>R</code> with the <a href=""http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fGarch/html/garchFit.html"" rel=""nofollow"">garchFit</a> function from the <code>fGarch</code> package:</p>

<hr>

<p>Mean equation:</p>

<p>$Y_t = a + bX_t + e_t$</p>

<p>$e_t = z_th_t^{0.5}$</p>

<p>$e_t \sim N(0,h_t)$, $z_t \sim N(0,1)$</p>

<p>Variance equation:</p>

<p>$h_t = \omega + k_0e^2_{t-1} + k_1h_{t-1}$</p>

<hr>

<p>In the above, $Y_t$ is the response of my mean equation and $X_t$ is the predictor. I will estimate this equation within the GARCH framework because of heteroscedasticity of residuals. </p>

<p>I want estimates of both the mean equation and the variance equation (similar to what <code>EViews</code> would give). However, the <a href=""http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fGarch/html/garchFit.html"" rel=""nofollow"">garchFit</a> function has two inputs that I'm concerned about:</p>

<ul>
<li><em>formula - ARMA(m,n) + GARCH/APARCH(p,q) mean and variance specification</em></li>
</ul>

<p>As you can see in my equations, my mean equation is <em>not</em> of the ARMA(m,n) form. </p>

<ul>
<li>data - any univariate time series which can be converted into a timeSeries using the generic function as.timeSeries</li>
</ul>

<p>I'm modeling both $Y_t$ and $X_t$ together; thus it shouldn't be univariate. </p>

<p>If I put this model through <code>EViews</code>, It'll give me parameter estimates of $a$ and $b$. This is what I want, but in <code>R</code>. In <code>R</code>, I can do <code>garchFit(data=Y)</code> (where <code>Y</code> is the column of $Y_t$), and it'll do a GARCH(1,1) over $Y_t = \mu(Y) + e_t$; but it doesn't give me $a$ or $b$ in the mean equation that I want!</p>
"
"0.0831890330807703","0.0837707816583391","147417","<p>I ran the following regression using R. </p>

<pre><code>libor &lt;- ts(diff(Libor))
ois &lt;- ts(diff(OIS))
x &lt;- ts(diff(Repo-OIS))
vix &lt;- ts(diff(VIX))
cds &lt;- ts(diff(CDS))
treasury &lt;- ts(diff(log(P_treasury)))
mbs &lt;- ts(diff(log(P_MBS)))
rrp &lt;- ts(diff(log(RRP)))
axx &lt;- ts.intersect(mbs, treasury, rrp, cds, libor, ois, x, vix)
reg3 &lt;- lm(libor~ois+x+vix+cds+treasury+mbs+rrp, data=axx, subset=418:521)
</code></pre>

<p>which gave me the following output:</p>

<pre><code>t test of coefficients:

               Estimate  Std. Error t value Pr(&gt;|t|)  
(Intercept)  8.6279e-04  8.0537e-04  1.0713  0.28675  
ois          1.1427e-01  5.5089e-02  2.0742  0.04076 *
x           -1.0914e-02  2.0758e-02 -0.5258  0.60028  
vix         -1.3155e-04  1.8298e-04 -0.7190  0.47394  
cds          7.9692e-05  1.0590e-04  0.7525  0.45358  
treasury    -3.3914e-01  1.9171e-01 -1.7690  0.08010 .
mbs         -4.0022e-03  1.3883e-02 -0.2883  0.77376  
rrp          1.6299e-05  1.5772e-04  0.1033  0.91791
</code></pre>

<p>While the variables <code>libor, ois, x ,vix</code> are in percent the variables <code>treasury, mbs and rrp</code> are in log and in million of USD. So it is a lin- log model. The variable treasury increased during the analysed time period by 47% (from 1671382 mio USD to 2461389 mio USD.). Now I want to calculate the impact of the variable <code>treasury</code> on my dependent variable (<code>libor</code>). following my book this is done: Î”y=(Î²1/100)%Î”x  --> = (-0.33914/100)*47=0.1593958. So I can say, that if the variable treasury is increased by 47% my dependent variable will decline by 0.1593958% or 15.94 basispoints. Is this interpretation correct?</p>

<p>My question arises because in a working paper they calculated (B1*100*47). I couldn't figure out why they multiply their B1 with 100 instead of divide it by 100. Any ideas? Many thanks!</p>
"
"0.0588235294117647","0.0592348877759092","108228","<p>I am working in R with daily time series data and have daily observations of two variables. The first is continuous.  The second is zero for every day except one, in which it is a number (<em>I'm not sure what to call it, so I referred to it as â€œalmost-Booleanâ€ in my title</em>).  Both variables are stationary.  I have hundreds of observations to work with, but I have put together a month of sample data:</p>

<pre><code>d &lt;- (as.Date(""2000-01-01""):as.Date(""2000-01-30""))
x &lt;- c(19,31,47,65,42,55,43,69,38,57,85,70,38,42,50,40,35,69,48,37,41,32,39,41,58,43,
     32,25,22,30)
z &lt;- c( 0,0,0,26,0,0,0,0,0,0,28,0,0,0,0,0,0,19,0,0,0,0,0,0,22,0,0,0,0,0)
data &lt;- data.frame(date=as.Date(d),x = as.integer(x),z = as.integer(z))
</code></pre>

<p>For a given vector y(t), what is the best approach/package to test whether variables x(t) and z(t)  (and their lagged values) are predictive of y(t)?  Thanks in advance for your suggestions.</p>
"
"0.0588235294117647","0.0592348877759092","108255","<p>I am using <code>auto.arima</code> for forecasting. I have more than one categorical variables having more than one level. </p>

<p>My questions are :</p>

<ol>
<li><p>Do I need to do dummy coding ?</p></li>
<li><p>If I do dummy coding with my categorical variables, this will result into 20 variables. Is it good to have such number of variables in time series model.</p></li>
<li><p>How can I do variable selections?</p></li>
</ol>

<p>I would appreciate any kind of help. </p>
"
"0.101885341621699","0.102597835208515","154627","<p>I'm pretty new to including time in any sort of modeling, so forgive me if any of my questions are basic.</p>

<p>I'm trying to predict a final sales number over a certain period of days using the incremental sales and how far out from the final date I am. Here's a sample of my data:</p>

<pre><code>      EVENT.DATE       SALES.DATE SALES.TO.DATE DAYS.OUT FINAL.QUANTITY
1       4/2/2014        3/18/2014             0       15             42
2       4/2/2014        3/19/2014             2       14             42
3       4/2/2014        3/20/2014             4       13             42 
4       4/2/2014        3/21/2014             4       12             42
5       4/2/2014        3/22/2014             4       11             42
6       4/2/2014        3/23/2014             4       10             42
7       4/2/2014        3/24/2014             4        9             42
8       4/2/2014        3/25/2014             6        8             42
9       4/2/2014        3/26/2014             6        7             42
10      4/2/2014        3/27/2014             6        6             42
11      4/2/2014        3/28/2014             8        5             42
12      4/2/2014        3/29/2014             8        4             42
13      4/2/2014        3/30/2014            13        3             42
14      4/2/2014        3/31/2014            13        2             42
15      4/2/2014         4/1/2014            15        1             42
16      4/2/2014         4/2/2014            26        0             42
17      5/3/2014        4/18/2014            10       15            412
18      5/3/2014        4/19/2014            12       14            412
19      5/3/2014        4/20/2014            20       13            412
20      5/3/2014        4/21/2014            36       12            412
21      5/3/2014        4/22/2014            44       11            412
22      5/3/2014        4/23/2014            47       10            412
23      5/3/2014        4/24/2014            60        9            412
24      5/3/2014        4/25/2014            66        8            412
25      5/3/2014        4/26/2014            75        7            412
26      5/3/2014        4/27/2014            95        6            412
27      5/3/2014        4/28/2014           111        5            412
28      5/3/2014        4/29/2014           120        4            412
29      5/3/2014        4/30/2014           128        3            412
30      5/3/2014         5/1/2014           174        2            412
31      5/3/2014         5/2/2014           207        1            412
32      5/3/2014         5/3/2014           263        0            412
</code></pre>

<p>I'm assuming that I can't just use OLS because the 16 observations for each event date are not completely unrelated from one another. I've been looking into modeling that involves panel data, but I'm not sure if that is applicable. I've looked into time series as well, but I can't figure out if I can use that because my dates are often irregular. I'm willing to do the research myself, I'd really just like some jumping off points. </p>

<p>If anything is unclear, please let me know. </p>
"
"0.166378066161541","0.167541563316678","155121","<p>First cross-validated question so please be gentle :o)</p>

<p>I have two datasets all gathered and managed in '<strong>R</strong>'... </p>

<p><strong>Dataset 1</strong> - News Corpus. Contains 3,270 entries from the period 1/Apr/13 to 31/Mar/14.  There are often multiple stories on any one day, and indeed days with no stories at all (which I believe makes for an incomplete time series and problems).  The dataset structure is;</p>

<pre><code>Date - (a date)
Domain - (a string) with 8 levels i.e. there are 8 web domains
DomainType - (a string) with 4 levels e.g. ""other news"" or ""technology news""
Sentiment_Title - (a numeric) a score that currently sits in range -4:4
Sentiment_Description - (a numeric) a score that currently sits in range -6:7
Sentiment_Body - (a numeric) a score that currently sits in range -53:146
CCAT - (logical)
ECAT - (logical)
GCAT - (logical)
MCAT - (logical)
</code></pre>

<p><a href=""https://mega.co.nz/#!HVQTkCJJ!UUFJMzN6i0xI_GKDEtzVV1WfUzkphYCEiB36oMsOINo"" rel=""nofollow"">DOWNLOAD corpusData.csv from Mega</a></p>

<p><strong>Dataset 2</strong> - Bitcoin Market Data. 365 day time series of weighted price, volume and intra-day spread for four different exchanges.  </p>

<p><a href=""https://mega.co.nz/#!3RokVLKB!8rsEBIL8N-F-SXP2lucsjnUfo40MBfN13YRFPGAMSlQ"" rel=""nofollow"">DOWNLOAD finData.csv from Mega</a> </p>

<p><strong>The Problem</strong>
What I really want to know is which features (if any) of dataset 1 (the corpus) are significantly related to the time series and how.  I guess the time series also needs leads and lags applied to know which direction any relationship goes and how far away from the story publication date that relationship lays.</p>

<p>I have spent a couple of weeks applying the very basic stats knowledge I have to the task and have spent a couple of hours with a post-grad stats support group who also proved unable to find a method that could be readily applied.</p>

<p>I (we) looked at basic Pearson's and Spearman's, moved on to look at linear regression and generalised linear models and so far there appears to be issues with the residuals that makes the output bunkum apparently.  I believe vector-autoregression could also be applied but we are way off into realms I just don't understand yet.</p>

<p><strong>The Question</strong> Given the datasets (and, ideally R) can anyone suggest or indeed offer up an approach to solving my problem?  Even better some simple explanation of how to interpret the results of any such approach.</p>
"
"NaN","NaN","46246","<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days."
"NaN","NaN","If data.ts is my time series then I would like to use something like</p>",""
"NaN","NaN","<pre><code>tslm(data.ts~season|businesss.dummy)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>Thus I want to model season given that the dummy for this hour is True or False.",""
"NaN","NaN","I don't want</p>",""
"NaN","NaN","<pre><code>tslm(data.ts~season + businesss.dummy)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>as this would just give a parallel shift on business days.",""
"NaN","NaN","I know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?",""
"NaN","NaN","Thanks!</p>",""
"NaN","NaN","","<r><regression><time-series><forecasting>"
"0.101885341621699","0.102597835208515","114399","<p>I have a theoretical growth function that can be perturbed by events, and I'd like to estimate the growth parameters as well as the perturbation, and the rate of falloff after that perturbation.</p>

<p>I'm thinking of using a logistic function to model the effect of the event and the falloff of that effect (if any).</p>

<p>To ground this, $x$ is time, and $t$ is the time the event occurs. Before time $t$, or if the event never occurs, we have a simple linear regression. After the event occurs, I model the contribution of the event with magnitude controlled by $\beta_2$ and rate of falloff by $\beta_3$.</p>

<p>$y_i=\left\{x_{i}&lt;t:\beta_{0}+\beta_1x_i+\epsilon_i,x_i&gt;t:\beta_0+\beta_1x_i+2\beta_2\frac{1}{\left(1+e^{\beta_3\left(x_i-t\right)}\right)}+\epsilon_i\right\}$</p>

<p>(<em>edited to add the error term</em>)</p>

<p>Here's a <a href=""https://www.desmos.com/calculator/nzmusqqosq"" rel=""nofollow"">Desmos graph</a> if it helps.</p>

<p>I'm really not sure how to estimate parameters for this model in any of the stats packages I'm familiar with in R. Do I need to turn to Bayesian methods?</p>
"
"0.166378066161541","0.167541563316678","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.166378066161541","0.167541563316678","48357","<p>I'm currently trying to use a variable x (and others) to explain a dependent variable y in a distributed lag model (with the long term goal of predicting variable y). The plot of variable x shows an evident seasonality at the end of the year: See <a href=""http://i.imgur.com/8gGtaVS.png"" rel=""nofollow"">http://i.imgur.com/8gGtaVS.png</a></p>

<p>After having deseasonalized the data with decompose (with multiplicative components), adf and kpss tests indicate, that the seasonal adjusted data is still not stationary. Because there are more independent variables and I don't want to look deeper into investigation a cointegration relationship between those series, I thought it would be the most usual way to take the difference of both series (with the diff() function).</p>

<p>Now there are 2 alternatives:</p>

<ol>
<li>Take the diff of the already seasonal adjusted data. The problem with this approach is, that I'm not sure if this a good idea because I don't see how you can reseasonalize the time series for a forecast result in an easy manner.</li>
<li>Take the diff of the raw series. This leads to the following graph. See <a href=""http://i.imgur.com/loDU2IE.png"" rel=""nofollow"">http://i.imgur.com/loDU2IE.png</a>
The time series is now stationary regarding adf and kpss tests, however there is still the seasonal pattern visible. Now I'm not sure if it is recommended to use decompose (with the multiplicative) method, to deseasonalize the diff of the time series, especially because there are zero values which have no effect when calculating the seasonal adjusted time series in the following way:</li>
</ol>

<pre class=""lang-R prettyprint-override""><code>decompose(x_diff, ""mult"")$x / decompose(x_diff, ""mult"")$season
</code></pre>

<p>So, how should I proceed when I want to include (the diff of) x as a independent (and lagged) variable in a distributed lag model?</p>
"
"0.0831890330807703","0.0418853908291695","167859","<p>I wish to predict variable $y$, and so I am tempted to estimate</p>

<p>$$
y_t = \beta_0 + \beta_1 x_t+ u_t
$$</p>

<p>Looking at a plot of $y$, the series does not seem stationary.
Instead I regress like so:</p>

<p>$$
y_t - y_{t-1} = \gamma ( x_t - x_{t-1} ) + u_t
$$</p>

<p>Now the purpose of this time series transformation is to make the series stationary - lets assume that the series is stationary in the changes.</p>

<p>How do then back track, and calculate the actual level predictions?</p>

<p>Bonus points for R code.</p>
"
"0.0588235294117647","0.0592348877759092","102902","<p>i have data that includes clicks, spend, signups and date. </p>

<p>for 1 week, i turn off advertising spend to see what clicks and signups are.
the next week, i turn advertising back on to see what the new clicks or signups are.</p>

<p>Given this 2 sets of data, how can i run regression analysis to see how impactful advertising is?</p>

<p>Should i run a regression analysis on y=(signups_week2-signups_week1) and x=(spend_week2-spend_week1) ?</p>

<p>Thanks,
J </p>
"
"0.269563276173873","0.232670020298844","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.0588235294117647","0.0592348877759092","143838","<p>I have several time series and want to regress the dependent variable on the explanatory variables. My question is: Because of structural breaks in my series I do not want to include all the observations in my regression. How can I select the a range of observations for my regression using R?</p>"
"NaN","NaN","<ol>",""
"NaN","NaN","<li>5 </li>",""
"NaN","NaN","<li>6  </li>",""
"NaN","NaN","<li>8 </li>",""
"NaN","NaN","<li>20</li>",""
"NaN","NaN","<li>25</li>",""
"NaN","NaN","<li>28</li>",""
"NaN","NaN","</ol>",""
"NaN","NaN","<p>For example: Suppose this is my data set. Now for my first regression I just want to use the obeservations 1 to 3 and for the second regression I want to use the observations 4 to 6. How can this be done without opening my dataset in excel and then rearrange my data?</p>",""
"NaN","NaN","","<r><regression><time-series>"
"0.0831890330807703","0.0837707816583391","175035","<p>I'm using ARIMA models to estimate sales forecast for a company. The company's sales channel is broken down into 4 sales channels and I'm running 4 different models to estimate the sales for each channel. Eventually, I'm going to aggregate the sales of these channels to find the total forecasted sales for the whole company. My questions is, how should i go about finding the confidence interval for the overall forecast? Adding up the confidence intervals of each channel is not correct since that will give me a very large interval.</p>

<p>I'd really appreciate if anyone can give me some idea on how to approach this sort of issue. Thanks in advance!</p>
"
"0.117647058823529","0.118469775551818","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.101885341621699","0.102597835208515","196839","<p>From a data stream i'm receiving a pair of measurements consisting of a current consumption and a current percentage every second. By accumulating the consumption over time it will represent eventually the maximum capacity when the percentage reaches from 100% to 0%.</p>

<p>I want to predict the maximum capacity in (almost) real time using linear regression with a small sample size window of two percent. However, when i compare the models of these local regressions of every two percent with the model of the whole data regression, i get very different results due to perhaps local fluctuation. (see figure)</p>

<p>Is there a way to bring the local regression models closer to the whole data model? (in a way that i can see the differences due to fluctuation but overall closer predictions to the whole data model)</p>

<p><a href=""http://i.stack.imgur.com/e1c8w.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/e1c8w.png"" alt=""enter image description here""></a></p>
"
"0.166378066161541","0.167541563316678","197488","<p>How would you go about making an unbiased comparison of two interventions (old vs. New <em>prtcl.binary</em> (0 and 1; individual worksheets vs. group work).) when there's a negative longitudinal slope present (longitudinal meaning the data is measured over time, but not repeatedly since each student only had one measurement)?
<a href=""http://i.stack.imgur.com/jRP29.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jRP29.png"" alt=""enter image description here""></a></p>

<p>The switch happened in 2014 (NOTE: there's little data for 2015, ~5 cases, so you can pretty much group it with 2014) But the mean comparison seems a little questionable when the previous intervention was having an effect already, see the negative slope before 2014.</p>

<p>So if my outcome is a count (i.e., number of days student did Y behavior), that is overdispersed (so I'm using a negative binomial dist.)</p>

<p>I was concerned with just comparing the one protocol to another using <code>prtcl.binary</code> as a dummy variable, so I added <code>year</code> to adjust the means for the the linear effect (black line) and an interaction between <code>year</code> and <code>prtcl.binary</code>, the differential, to compare the slopes. But I'm not sure this fixes my problem with interpreting the main effect of <code>prtcl.binary</code>.</p>

<pre><code>MASS::glm.nb(formula = y.count ~ 1 + prtcl.binary + year + prtcl.binary : year, data = d0)
</code></pre>

<p>Someone told me this is a good case for detrending data (but I'm not fond of the idea of interpreting the regression coefficients of residuals; i.e., I'm dumb). Someone else suggested I used a mixed model and use year as a random effect.
I thought it might make sense to use the 2008 to 2013 data to predict a value for 2014 with a standard error and then compare it with the mean and standard error measured in just 2014.</p>

<p>Suggestions?</p>
"
"0.144087631928422","0.145095250022002","197337","<p>I have monthly temperature averages for a weather station across 100 years.
I am wondering how I should analyze this data in a regression. </p>

<p>The data are set up in the following fashion:</p>

<pre><code>year  month  temp.avg
1900    11      9 
1900    12      6       
1901    01      5 
1901    02      4 
....
2015    12      7 
</code></pre>

<p>My question is: <strong>how do I go about accounting for the time and incorporating it into my model?</strong></p>

<p>Here are my <strong>4 proposed methods:</strong></p>

<ol>
<li><p>Should I add a ""time"" variable which essentially counts my months from 1 through <em>n</em> rows of data? </p>

<ul>
<li>Ex: Using the example above, I would have a ""time"" column of 1,2,3,4,etc.</li>
<li>I essentially lose the actual year &amp; month data, but does this matter? -- can I just add back, for example, ""Dec 1900"" for time 1?</li>
</ul></li>
<li><p>Given a month <em>i</em> in year <em>j</em>, should I create a continuous time by adding 0.0833 (1/12) to each year for each i-1 month? </p>

<ul>
<li><p>Ex. Again using the above example, I would have a ""time"" column consisting of values 1900, 1900.8333, 1900.9167, 1901.0000, 1901.0833, etc...</p></li>
<li><p>The linear regression model for the prior two methods would essentially be: <code>lm(temp.avg ~ time)</code></p></li>
</ul></li>
<li><p>Do I just incorporate year and month (or perhaps more usefully <em>season</em>) in the model together?</p>

<ul>
<li>This would result in: <code>lm(temp.avg ~ year + month)</code></li>
</ul></li>
<li><p>Or is 3 wrong and instead I'd have to create a dummy variable for each month (or season)?</p>

<ul>
<li><code>lm(temp.avg ~ year + jan + feb + mar + apr ...)</code></li>
</ul></li>
</ol>

<p>So <strong>which is correct?</strong> 
I assume perhaps the questions I'm asking would dictate this to some degree. But perhaps someone could describe simply the validity of each method and when to apply each?</p>

<p>Note: I understand that I will have to account for temporal autocorrelation, but I'm wondering how I incorporate time data <em>prior</em> to worrying about that.</p>

<p>I will note that I perform my analyses in <code>R</code>.</p>
"
"0.195095575903259","0.196459897251501","161929","<p>I've recently jumped into the deep end of statistical analysis of revenue. I've learned a ton about statistics, probability, decomposition (stl), and the Python and R languages. I feel like I'm climbing the learning curve well, even though it's a steep one. Many of my largest hurdles are simply discovering the correct terms to search for on Google in this new field to me.</p>

<p>TL;DR: <strong>What search terms, statistical concepts, or R packages would you use to find the best method of filtering revenue caused by marketing campaigns out of the baseline seasonality of a business?</strong></p>

<p>The end goal that I have now is to filter out marketing effects on historical data to gain a base and to then run some predictive analysis on  our gross revenue prior to future marketing efforts. </p>

<p>I currently have about 4 years of monthly sales revenue and have been able to successfully decompose it with the stl() function in R, do some prediction with various methods including Holt-Winters. </p>

<p>My problem is that I believe some aggressive marketing campaigns in our past is severely corrupting my seasonal vector (and therefore my predictions).</p>

<p>In addition to the 4 years of sales data I have 4 years of monthly data about our email campaigns and have been able to determine a strong correlation between emails sent per month and gross revenue (.74) I was hoping that I might be able to use that data to filter out the marketing effects on gross_revenue and find a better representation of our seasonal trends without marketing effects. </p>

<p>I've looked a little into Marketing Mix Modeling but I wasn't convinced that was the correct tree to bark up and if i could correlate it with my predictive efforts anyway.</p>

<p>I know that pictures and data often speak better than i do, so here's some data I've generated to better communicate my current problem. In both cases I feel like the spikes where we pushed marketing in the top bars have too much influence on the seasonality in the 3rd bars.</p>

<p>Spikes show marketing efforts and are distorting seasonality</p>

<p><img src=""http://i.stack.imgur.com/lHlZS.png"" alt=""Decomposition of time series""></p>

<p>Code for above plot:</p>

<pre><code>ds2 &lt;- ts(ds$gross, start=c(2011,7), frequency=12)
fit &lt;- decompose(ds2, type=""multiplicative"")
plot(fit)
</code></pre>

<p>This one is decomposed with stl()
<img src=""http://i.stack.imgur.com/Dm8u6.png"" alt=""Decomposed with stl() ""></p>

<p>Code used to generate above plot</p>

<pre><code>ds2 &lt;- ts(ds$gross, start=c(2011,7), frequency=12)
fit &lt;- decompose(ds2, type=""multiplicative"")
plot(fit)
</code></pre>
"
"NaN","NaN","198761","<p>I have this data with 3 variables 60 observations for continues three days period, in which two nurses work in irregular shifts to cover this whole 60 hours:</p>

<p>Hour-Productivity-Nurse, 1-50%-A, 2-40%-A, 3-20%-B, 4-10%-A, 5-60%-B, 6-15%-B, 7-15%-B, . . .,. . .,. . ., 60-5%-A.</p>

<p>I have been asked to build a time series model for the Productivity and decide whether there a strong Productivity difference between nurse A and nurse B. what is the right statistical approach to handle this question? thanks for the kind help in advance.</p>
"
"0.0831890330807703","0.0837707816583391","223917","<p>I have multiple time series of air passenger demand with specific classification data. Data looks like this (some rows may lack some data):</p>

<pre><code>Origin  Destination Time  {more classifiers}  Apr. 1st   May 2nd   Jun. 20th {more dates} 
------------------------------------------------------------------------------------------
Madrid  London      early ...                       10        15          20  ...  
{more rows}
London  Rio         late  ...                       12        10          15  ...   
</code></pre>

<p>The task: The target is to predict values for the next month/day(s). We made an approach with regression trees (ctree, R package partykit) but this results in quite inaccurate results. That is why we want to give time series analysis a try.</p>

<p>My Question: Which algorithm/method would you suggest for this problem? I think it should be a mixture between time series analysis and classification. Any hints/suggestions are highly appreciated. Thanks!</p>
"
"0.0831890330807703","0.0837707816583391","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.166378066161541","0.167541563316678","180992","<p>I'm building a machine learning (random forest) regression model to predict flow in a river, using rainfall, relative humidity, air temperature and certain other climatic variables. Since flow on a particular day (<code>flow_t</code>) is highly correlated with flow on previous day (<code>flow_t_1</code>), I want to include lagged flow in the model formulation.</p>

<p>In case I build the model this way:</p>

<pre><code>require(randomForest)
flow.rf=randomForest(flow_t~flow_t_1+temp+humidity..........)
</code></pre>

<p>How can I use the above model for predictions? 
Since the input dataset for prediction will not have the flow variable, I cannot include its lagged version in the prediction call. I know that the <code>dynlm</code> package can be used to perform 'autoregressive distributed lag modeling' to include lagged dependent variables, but how can this be done for machine learning models? Or even for other statistical modeling techniques, like GLMs and GAMs?</p>
"
