"V1","V2","V3","V4"
"0.120714328056691","0.11724207635211","  6513","<p>I want to predict inter-day electricity load. My data are electricity loads for 11 months, sampled in 30 minute intervals. I also got the weather-specific data from a meteorological station (temperature, relative humidity, wind direction, wind speed, sunlight). From this, I want to predict the electricity load until the end of the day. </p>

<p>I can run my algorithm until 10:00 of the present day and after that it should give the prediction of loads in 30 minute intervals. So, it should tell the load at 10:30, 11:00, 11:30 and so on until 24:00.</p>

<p>My first attempt was to create a <strong>linear model</strong> in R.</p>

<pre><code>BP.TS &lt;- ts(Buying.power, frequency = 48)
a &lt;- data.frame(
Time, BP.TS, Weekday, Pressure, Temperature, RelHumidity, AvgWindSpeed, AvgWindDirection, MaxWindSpeed, MaxWindDirection, SunLightTime,
m, Buying.2dayago, AfterHolidayAndBPYesterday8, MovingAvgLast7DaysMidnightTemp
)
a &lt;- a[(6*48+1):nrow(a),]

start = 9716
steps.ahead = 21
par(mfrow=c(5,2))
for (i in 1:10) {
    train &lt;- a[1:(start+(i-1)*48),]
    test &lt;- a[((i-1)*48+start+1):((i-1)*48+start+steps.ahead),]
    summary(reg &lt;- lm(log(BP.TS)~., data=train, na.action=NULL))
    pred &lt;- exp(predict(reg, test))

    plot(test$BP.TS, type=""o"")
    lines(pred, col=2)
    cat(""MAE"", mean(abs(test$BP.TS - pred)), ""\n"")
}
</code></pre>

<p>This is not very succesful. Now I try to model the data with ARIMA. I used auto.arima() from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"">forecast package</a>. These are the results I got:</p>

<pre><code>&gt; auto.arima(BP.TS)
Series: BP.TS 
ARIMA(2,0,1)(1,1,2)[48]                    

Call: auto.arima(x = BP.TS) 

Coefficients:
         ar1      ar2     ma1    sar1     sma1    sma2
      1.1816  -0.2627  -0.554  0.4381  -1.2415  0.3051
s.e.  0.0356   0.0286   0.033  0.0952   0.0982  0.0863

sigma^2 estimated as 256118:  log likelihood = -118939.7
AIC = 237893.5   AICc = 237893.5   BIC = 237947
</code></pre>

<p>Now if I try something like:</p>

<pre><code>reg = arima(train$BP.TS, order=c(2,0,1), xreg=cbind(
train$Time, 
train$Weekday, 
train$Pressure, 
train$Temperature, 
train$RelHumidity, 
train$AvgWindSpeed, 
train$AvgWindDirection, 
train$MaxWindSpeed, 
train$MaxWindDirection, 
train$SunLightTime,
train$Buying.2dayago,
train$MovingAvgLastNDaysLoad,
train$X1, train$X2, train$X3, train$X4, train$X5, train$X6, train$X7, train$X8, train$X9, 
train$X11, train$X12, train$X13, train$X14, train$X15, train$X16, train$X17, train$X18, 
train$MovingAvgLast7DaysMidnightTemp
))

p &lt;- predict(reg, n.ahead=21, newxreg=cbind(
test$Time, 
test$Weekday, 
test$Pressure, 
test$Temperature, 
test$RelHumidity, 
test$AvgWindSpeed, 
test$AvgWindDirection, 
test$MaxWindSpeed, 
test$MaxWindDirection, 
test$SunLightTime,
test$Buying.2dayago,
test$MovingAvgLastNDaysLoad,
test$X1, test$X2, test$X3, test$X4, test$X5, test$X6, test$X7, test$X8, test$X9, 
test$X11, test$X12, test$X13, test$X14, test$X15, test$X16, test$X17, test$X18, 
test$MovingAvgLast7DaysMidnightTemp
))

plot(test$BP.TS, type=""o"", ylim=c(6300,8300))
par(new=T)
plot(p$pred, col=2, ylim=c(6300,8300))
cat(""MAE"", mean(p$se), ""\n"")
</code></pre>

<p>I get even worse results. Why? I ran out of ideas, so please help. If there is additional information I need to give, please ask.</p>
"
"0.104541674697863","0.101534616513362"," 26183","<p>I would like to convert an ARIMA model developed in R using the <code>forecast</code> library to Java code. Note that I need to implement only the forecasting part. The fitting can be done in R itself. I am going to look at the <code>predict</code> function and translate it to Java code. I was just wondering if anyone else had been in a similar situation before and managed to successfully use a Java library for the same. </p>

<p>Along similar lines, and perhaps this is a more general question without a concrete answer; What is the best way to deal with situations where in model building can be done in Matlab/R but the prediction/forecasting needs to be done in Java/C++? Increasingly, I have been encountering such a situation over and over again. I guess you have to bite the bullet and write the code yourself and this is not generally as hard as writing the fitting/estimation yourself. Any advice on the topic would be helpful. </p>
"
"0.347053693162988","0.35172622905633"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.181071492085037","0.175863114528165"," 29424","<p>I'm looking for some forecasting advice when dealing with seasonal time series data that has a large number of observations.  By ""large"" I only mean a few thousand --- I'm used to such sizes in Data Mining being considered pretty small, but it seems that in time series modeling that's pretty unwieldy for many of the tools I've tried.</p>

<p>For example, here's a toy data set that records an observation once per minute, for five days:</p>

<pre><code>set.seed(123)
t &lt;- 1:(5*24*60)
x &lt;- ts(15 + 0.001*t + 10*sin(2*pi*t/(length(t)/5)) + rnorm(length(t)), freq=length(t)/5)
plot(x, type='l')
</code></pre>

<p><img src=""http://i.stack.imgur.com/xVSCN.png"" alt=""time series plot""></p>

<p>(In my real operational data set, the values are observed at irregular intervals, but I've regularized them by doing something like <code>x &lt;- approx(d$t, d$x, xout=1:(5*24*60))</code> first.  Advice on whether that's advisable, or alternative approaches, is welcome too.)</p>

<p>So the seasonality in this data set has a lag of 1,440 observations, which seems to be way outside the range that things like <code>auto.arima()</code> (in the <code>forecast</code> package) will find:</p>

<pre><code>m1 &lt;- auto.arima(x)
plot(forecast(m1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/ccnGc.png"" alt=""prediction plot""></p>

<p>And I'm not quite sure how to interpret the <code>ets()</code> function here, but it doesn't seem to be able to handle this size data, and it didn't seem to pick up on the seasonality:</p>

<pre><code>&gt; m2 &lt;- ets(x, 'MAZ')
&gt; plot(forecast(m2))
Error in forecast.ets(m2) : Forecast horizon out of bounds
&gt; m2$method
[1] ""ETS(M,A,N)""
</code></pre>

<p>Where to go from here?  Any suggestions?  Thanks.</p>
"
"0.314060312330961","0.321080649533968"," 31374","<p>Motivation: I was hired as an intern a few weeks ago to figure out if my company needed to buy new machines six months in advance. Database machines take up to 4 months to install and there is a 2 month grace period.</p>

<p>I signed an NDA, so I don't think I can give any actual data.</p>

<p>The only reliable information I have now, is information on the number of logins and registrations for an education company from 2002 to 2011. I think I can get more recent information on registrations, and people are working on getting login information. We stopped logging login information in 2011 so there will be a gap of no data when I try to forecast :(</p>

<p>The information is collected daily.</p>

<p>I've created a time series forecast of the data using R. I used this tutorial
<a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html#arima-models"" rel=""nofollow"">http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html#arima-models</a> To make a holt winters exponential model with daily frequency (frequency = 365). I've removed February 29 from the data. Unfortunately the gap in login data means I will have to try a more specific ARIMA right? Will I be able to use arima if there are long gaps in the data? Also, the arima function in R doesn't allow for frequencies greater than 350, and it runs out of memory quickly, so I'd have to use a monthly model (freq = 12). I have tried using fourier but the predictions didn't look right intuitively. Since I want to know what the peak usages are though, I think I might want to be more specific. Is it ok to use a weekly frequency (freq = 52) and just remove Dec 31?</p>

<p>Is daily frequency allowable? Like can I use exponential smoothing with daily frequency even though Sept 7, 2012 might fall on a Sunday, whereas Sept 7, 2011 and 2010 and 2009 might all be weekdays. There is a daily, weekly, and yearly seasonality in demand and number of logins. Eg. 6pm, and Monday, and September are more loaded in general than 4am, and Saturday, and May. There is a yearly seasonality in number of registrations.</p>

<p>I've been having some issues with the login predictions
The problem is that variability increases too much before 6 months have even passed. At the 80% confidence interval. The projection line extends into 2012 and the orange area is the 80% confidence interval. Logging and using additive exponential smoothing gave me much more variability than multiplicative exponential smoothing.</p>

<p>It's not useful to the company to say that ""well you might have 8 jillion logins sometime in the next 6 months and you might have 20% more than you had last year."" How do I reduce the variance in the projection?</p>

<p><a href=""http://img836.imageshack.us/img836/8460/holtwintersloginmultipl.png"" rel=""nofollow"">http://img836.imageshack.us/img836/8460/holtwintersloginmultipl.png</a></p>

<p>Finally, I was thinking that after I got accurate projections, I'd put logins and registrations in a neural network, and I'd put something like average wait time on a few machines as the ouput variable, and I'd forecast peak projected processing power demand in 6 months. There are other variables to consider, like software releases that change cpu demand per user, but I'm hoping the neural network will learn when these happen, or that they are easy to detect and account for. I don't have any good data on average wait time yet, but assuming I find some, is this a good plan?</p>
"
"0.147844254190915","0.143591631723548"," 32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"0.132235920981457","0.160540324766984"," 32634","<p>Is it better to difference a series (assuming it needs it) before using an Arima OR better to use the d parameter within Arima?</p>

<p>I was surprised how different the fitted values are depending on which route is taken with the same model and data. Or am I doing something incorrectly?</p>

<pre><code>install.packages(""forecast"")
library(forecast)

wineindT&lt;-window(wineind, start=c(1987,1), end=c(1994,8))
wineindT_diff &lt;-diff(wineindT)

#coefficients and other measures are similar
modA&lt;-Arima(wineindT,order=c(1,1,0))
summary(modA)
modB&lt;-Arima(wineindT_diff,order=c(1,0,0))
summary(modB)

#fitted values from modA
A&lt;-forecast.Arima(modA,1)$fitted

#fitted from modB, setting initial value to the first value in the original series
B&lt;-diffinv(forecast.Arima(modB,1)$fitted,xi=wineindT[1])


plot(A, col=""red"")
lines(B, col=""blue"")
</code></pre>

<p><strong>ADD:</strong></p>

<p>Please note I am differencing the series once and fitting arima (1,0,0) then I am fitting arima (1,1,0) to the original series. I am (I think) reversing the differencing on the fitted values for the arima(1,0,0) on the differenced file. </p>

<p>I am comparing the fitted values - not the predictions.</p>

<p>Here is the plot (red is arima(1,1,0) and blue is the arima (1,0,0) on the differenced series after changing back to the original scale)  :</p>

<p><img src=""http://i.stack.imgur.com/mQjAb.jpg"" alt=""enter image description here""></p>

<p><strong>Response to Dr. Hyndman's Answer:</strong></p>

<p>1) Can you illustrate in R code what I would need to do in order to get the two fitted values (and presumably forecasts) to match (allowing for small difference due to your first point in your answer) between Arima (1,1,0) and Arima(1,0,0) on the manually differenced series? I assume this has to do with the mean not being included in modA, but I am not entirely sure how to proceed.</p>

<p>2) Regarding your #3. I know I am missing the obvious, but are not $\hat{X}_t = X_{t-1} + \phi(X_{t-1}-X_{t-2}) $ and $\hat{Y}_t = \phi (X_{t-1}-X_{t-2})$ the same when $\hat{Y}_t$ is defined as $\hat{X}_t - X_{t-1}$? Are you saying I am ""undifferencing"" incorrectly?</p>
"
"0.330965259476921","0.306138387461313"," 41622","<p>I am aware that the ""prediction interval"", as defined in most textbooks on linear models, is focused on the uncertainty in the model being fit and is used to estimate an output prediction range for exact inputs.    However, as is normally the case, what about situations where the input isn't exact?   How do you calculate a ""prediction interval"" that accounts for both the uncertainty in the model and the uncertainty in the input data?</p>

<p>For example,</p>

<p>Assume the model is:   <code>y = a0 + (a1 * x1) + (a2 * x2)</code></p>

<p>Where <code>y</code>, <code>x1</code>, and <code>x2</code> are time series vectors.     </p>

<p>I only have one observation of the <code>y</code> time series with its associated <code>x1</code> and <code>x2</code> time series.  That data is used to fit the model.   However, I also have 1000 additional observations for <code>x1</code> and <code>x2</code>.   I can easily calculate individual ""prediction intervals"", using the model and each <code>x1</code> <code>x2</code> pair, however I want to estimate a ""prediction interval"" that allows for all <code>x1</code> and <code>x2</code> observations.   To be more specific, I generated the example below.   The questionable code is below the <code>#======</code> comment line.</p>

<p>Basically, I used <code>lm(..)</code> to fit the <code>y</code> vector to its associated <code>x1</code> and <code>x2</code> vectors.  Next, I used <code>predict.lm(...interval=""prediction"", level=alpha)</code> to generate its typical <code>fit</code>, <code>lwr</code>, and <code>upr</code> vectors for each additional <code>x1</code> <code>x2</code> pair (see Graph 1, <code>fit</code> is the solid line, <code>upr</code> and <code>lwr</code> are the dashed lines for two observations).  I then collected these vectors for all <code>x1</code> <code>x2</code> pairs and used <code>alpha</code> to extract the upper and lower tails of all of the <code>lwr</code> and <code>upr</code> vectors (see Graph 2).</p>

<p>Is this scheme valid?   Does the <code>alpha</code> that was used in <code>predict.lm(... level=alpha)</code> apply directly to counting-up/sorting the results to generate a ""prediction interval"" that allows for all <code>x1</code> <code>x2</code> pairs?   Can a 5% range from a competing model (for example an ARIMA model) be compared to this 5% ""prediction interval""?</p>

<p>I'm fairly sure that the following scheme isn't right, but so far, I haven't figured out what I need to fix.</p>

<pre><code>set.seed(1)

numpoi &lt;- 10 #Number of data points in a time series vector

#First independent variable ""x1"", first observation
x1mea &lt;- 0.03
x1sta &lt;- 0.05
x1 &lt;- cumsum(rnorm(numpoi, mean=x1mea, sd=x1sta))

#Second independent variable ""x2"", first observation
x2mea &lt;- -0.01
x2sta &lt;- 0.1
x2 &lt;- cumsum(rnorm(numpoi, mean=x2mea, sd=x2sta))

#Dependent variable ""y"", first observation
a0 &lt;- 3
a1 &lt;- 2
a2 &lt;- 1
noimea &lt;- 0.0
noista &lt;- 0.1
y &lt;- a0 + (a1 * x1) + (a2 * x2) + rnorm(numpoi, mean=noimea, sd=noista)

#Build a data frame of the ""first observation"" data
datfra &lt;- data.frame(y=y, x1=x1, x2=x2)

#Fit the model for the ""first observation""
mod &lt;- lm(y ~ x1 + x2, data=datfra)
summary(mod)

#Set up desired alpha value
alpha &lt;- 0.95
onetai &lt;- (1 - alpha)/2 #Convert the two tail ""alpha"" to a one tail value for use later

#Generate some new data ""a"" for a second observation of ""x1"" and ""x2"".
x1a &lt;- cumsum(rnorm(numpoi, mean=x1mea, sd=x1sta))
x2a &lt;- cumsum(rnorm(numpoi, mean=x2mea, sd=x2sta))
datfraa &lt;- data.frame(y=rep(NA, numpoi), x1=x1a, x2=x2a)
modprea &lt;- predict(mod, newdata=datfraa, interval=""prediction"", level=alpha)

#Generate some new data ""b"" for a third observation of ""x1"" and ""x2"".
x1b &lt;- cumsum(rnorm(numpoi, mean=x1mea, sd=x1sta))
x2b &lt;- cumsum(rnorm(numpoi, mean=x2mea, sd=x2sta))
datfrab &lt;- data.frame(y=rep(NA, numpoi), x1=x1b, x2=x2b)
modpreb &lt;- predict(mod, newdata=datfrab, interval=""prediction"", level=alpha)

#Plot the results for both new data ""a"" and ""b""
plot(modprea[, 1], type=""l"", ylim=c(min(modprea, modpreb), max(modprea, modpreb)), main=""Graph 1 - Second and Third Observations for x1 and x2"", lwd=2, col=""red"")
lines(modprea[, 2], lwd=2, lty=2, col=""red"")
lines(modprea[, 3], lwd=2, lty=2, col=""red"")
lines(modpreb[, 1], lwd=2,  col=""green"")
lines(modpreb[, 2], lwd=2, lty=2, col=""green"")
lines(modpreb[, 3], lwd=2, lty=2, col=""green"")

#===========================================================================
#The code below is where my question lies.    Is this the appropriate method
#to account for all observations?

#Run the above calculation scheme on ""all"" observations.
numtri &lt;- 1000 #All observations
modprecfit &lt;- matrix(0, nrow=numpoi, ncol = numtri) #Matrix to hold all ""fit"" vectors
modpreclwr &lt;- matrix(0, nrow=numpoi, ncol = numtri) #Matrix to hold all ""lwr"" vectors
modprecupr &lt;- matrix(0, nrow=numpoi, ncol = numtri) #Matrix to hold all ""upr"" vectors

#Fill up the modprecfit, modpreclwr, and modprecupr matricies.
for (i in 1:numtri) {

  #Generate the new data and put it in a dataframe
  x1c &lt;- cumsum(rnorm(numpoi, mean=x1mea, sd=x1sta))
  x2c &lt;- cumsum(rnorm(numpoi, mean=x2mea, sd=x2sta))
  datfrac &lt;- data.frame(y=rep(NA, numpoi), x1=x1c, x2=x2c)

  #Predict the new ""y"" for new input data
  modprec &lt;- predict(mod, newdata=datfrac, interval=""prediction"", level=alpha)

  #Store the ""ft"", ""lwr"", and ""upr"" vectors so they can be processed later
  modprecfit[, i] &lt;- modprec[, 1]
  modpreclwr[, i] &lt;- modprec[, 2]
  modprecupr[, i] &lt;- modprec[, 3]

}

#Extract the average ""fit"", the lower quantile ""lwr"", and the upper quantile ""upr""
modprecfitfin &lt;- apply(modprecfit, 1, quantile, 0.5)
modpreclwrfin &lt;- apply(modpreclwr, 1, quantile, onetai)
modprecuprfin &lt;- apply(modprecupr, 1, quantile, (1 - onetai))

plot(modprecfitfin, type=""l"", ylim=c(min(modpreclwrfin), max(modprecuprfin)), main=""Graph 2 - All Observations for x1 and x2"", lwd=2, col=""red"")
lines(modpreclwrfin, lwd=2, lty=2, col=""red"")
lines(modprecuprfin, lwd=2, lty=2, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/dHaNa.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/oMcGL.png"" alt=""enter image description here""></p>
"
"0.209083349395727","0.203069233026724"," 58101","<p>I am doing predictions on monthly temperature data for 100 years, from 1901 to 2000 (i.e 1200 data points). I want to know if the method I follow is correct because in my output, I do not see the requisite ""randomness"" of temperature being reproduced in the prediction.  </p>

<p>Here is a link to the plot of the prediction (in red)<br>
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing</a>  </p>

<p>EDIT: added the ACF and PACF of the detrended and de-seasonalised time series:
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing</a></p>

<p>Below is the dput() of my data:</p>

<pre><code>&gt; dput(fr.monthly.temp.ts)
structure(c(2.7, 0.4, 4.7, 10, 13, 16.9, 19.2, 18.3, 15.7, 10.6,   
4.9, 3.5, 4.1, 3.2, 7.5, 10.3, 10, 15.1, 18.2, 17.4, 15, 10.2, 
6.3, 3.5, 3.8, 5.9, 7.6, 7.1, 12.9, 14.9, 17.6, 17.3, 15.5, 12.1, 
6.9, 2.7, 3, 4.6, 5.5, 10.3, 13.6, 16.3, 20.2, 18.5, 13.9, 11.2, 
5.4, 4.8, 1.7, 4, 7.4, 9.3, 11.9, 16.5, 20, 17.6, 14.7, 8.4, 
5.5, 3.8, 4.3, 3.1, 5.6, 8.5, 12.6, 16.1, 18.2, 18.9, 16, 12.7, 
7.4, 2.3, 2.5, 2.1, 6.3, 8.4, 12.7, 15.1, 16.5, 17.9, 16.2, 11.6, 
7.6, 5.6, 1.7, 4.8, 5, 7.7, 14.2, 16.8, 17.9, 17.1, 14.8, 12.1, 
6.5, 3.6, 2.2, 2, 4.7, 10.4, 12.8, 14.2, 16.3, 18, 14.2, 12.2, 
5, 4.9, 4, 5.4, 6.6, 8.5, 11.9, 16.1, 16.4, 17.3, 14.2, 11.9, 
5.9, 6, 1.6, 4.5, 6.4, 8.3, 13.6, 16.1, 20.8, 20.7, 17.5, 11.3, 
7.3, 6.6, 4.6, 6.8, 8.4, 9.2, 13.8, 15.5, 17.9, 15.5, 12.5, 10, 
5.5, 5.8, 5.4, 4.7, 7.9, 9.1, 13, 15.8, 16.5, 17.6, 15.4, 12.3, 
9.2, 4, 0.7, 6.5, 7.4, 11.2, 12.2, 15.3, 17.3, 18.2, 15.3, 10.6, 
6.3, 5.7, 3.5, 4.3, 5.7, 8.5, 14.2, 17, 17.2, 17.5, 14.7, 9.6, 
4.6, 7, 6.4, 4.8, 5.9, 9.5, 13.8, 14, 17.4, 18.4, 14.5, 11.5, 
7, 4.3, 1.1, 1.4, 4.4, 6.7, 15.1, 17.6, 18.3, 17.2, 16.4, 9.4, 
7.3, 1.4, 3.7, 5.4, 6.5, 8.4, 14.2, 15, 18, 18.1, 15.4, 9.7, 
6.4, 6.9, 3.3, 3.7, 6.2, 7.8, 13.8, 16.3, 15.9, 18.9, 16.2, 8.8, 
4.6, 5.5, 5, 6.4, 8.2, 9.9, 14.4, 16, 17.4, 16.5, 15.2, 11.5, 
6, 4, 6.4, 4.2, 7.2, 8.9, 13.7, 16.9, 20.6, 18, 17, 14.1, 4.7, 
4.5, 3.4, 4.7, 6.6, 8, 14.8, 16.3, 16.7, 16.9, 13.7, 9.2, 5.4, 
4.5, 3.7, 6.3, 7.6, 9.4, 12.2, 14.1, 19.9, 18.8, 15.1, 12.3, 
5.3, 3.8, 3.8, 2.4, 6.4, 9.2, 14.1, 16.2, 18, 15.9, 15.2, 11.7, 
7.1, 4.5, 4.8, 5.6, 4.3, 9.1, 12.9, 17, 18, 17.6, 13.3, 11.8, 
4.9, 3.9, 4.1, 8.3, 7.2, 10.3, 11.6, 14.5, 18.2, 18.7, 17.3, 
11.5, 8.3, 2.5, 4.3, 4.5, 7.7, 9.8, 13.7, 15.7, 18, 17.8, 15.2, 
11.3, 6.7, 2.9, 5, 6.4, 7.1, 9.3, 11.8, 16.1, 20.5, 19.3, 15.8, 
11.5, 8.2, 3.7, 0.3, -0.2, 6.7, 7.8, 13.2, 16.3, 19.1, 18.1, 
18.4, 11.4, 7.3, 6.4, 5.8, 3.3, 7, 9.7, 12.1, 17.7, 17.3, 18.2, 
15.9, 11.9, 8.6, 4.5, 3.7, 3.3, 5.8, 8.8, 13.8, 17.5, 17.7, 17, 
12.8, 10.6, 8.2, 3.2, 4.8, 1.4, 5.5, 8, 12.1, 15.8, 17.4, 20.4, 
17.2, 11, 7.4, 5, 1.8, 4.3, 7.8, 10.1, 13.1, 15.4, 19.5, 20.1, 
16.7, 12, 5.5, 0.3, 3.3, 3.1, 6.3, 10.4, 13.8, 17.2, 20, 17.5, 
17.1, 11.9, 5.8, 7.6, 2.6, 5.1, 6.2, 9.1, 11.6, 17.2, 19.5, 18.1, 
16.1, 10.7, 7, 3.9, 6.5, 4.6, 7.9, 8.3, 13.4, 16.1, 17.2, 18, 
16, 9.1, 6.6, 4.2, 5.3, 6.9, 5.6, 9.9, 14.2, 16.6, 18.6, 19.1, 
15.5, 11.7, 6.3, 3.2, 4.4, 3.9, 8.8, 7.7, 11.7, 16.8, 17.5, 18.2, 
15.6, 11.3, 9.3, 2.5, 5.3, 4.7, 5.4, 10.2, 11.5, 16.4, 17.3, 
18.1, 15.2, 10.3, 8.7, 2.6, -0.9, 4.5, 7.1, 9.6, 13.5, 17.1, 
17.1, 17.5, 15.6, 10.6, 7.6, 1.1, 0.7, 4.5, 7.3, 8.2, 10.3, 16.8, 
19.3, 16.9, 15.5, 10.8, 6.6, 3.7, -0.2, -0.1, 7.7, 10.6, 13.1, 
16.7, 18.1, 18.7, 16.7, 13.2, 5.5, 4.8, 4.8, 5.3, 8, 11.5, 14.2, 
16.4, 19.2, 19.2, 16, 12.4, 5.9, 3.4, 5.1, 2.2, 5.1, 11.1, 13.4, 
16, 18.6, 20.6, 15.2, 10.1, 7.1, 3.4, -1, 7.1, 8.4, 11.9, 14.8, 
17.8, 20, 18.1, 16.7, 12.3, 6.5, 4.8, 1.7, 6.4, 6.7, 11.2, 13.1, 
15.7, 18.9, 17.9, 16.2, 11.3, 7.1, 2.1, 1, 1.3, 7.3, 11.3, 14.8, 
17.9, 20.4, 20.9, 17.6, 12.1, 8.3, 3.8, 5.7, 4.5, 9.5, 10.4, 
14, 15.8, 17, 17.8, 15.5, 11.4, 7.2, 4.6, 4.5, 5.4, 5.7, 11.7, 
12.2, 16.8, 20.6, 19.8, 18.6, 13.4, 6.4, 5.1, 3, 6.4, 8, 8.7, 
14.2, 18.3, 20.2, 18.6, 15.2, 11.4, 7.4, 1.1, 4.6, 4.7, 5.8, 
9.1, 11.8, 16.1, 18.7, 17.5, 16.5, 10.5, 8.7, 4.9, 2.7, 2.8, 
8.1, 11.2, 14.5, 17.9, 20.2, 18.9, 13.1, 10.9, 5.5, 3.5, 1.1, 
3, 7.5, 10.1, 14.8, 15.4, 18, 18.8, 16.2, 12.1, 7, 6.8, 1.7, 
2.3, 7.5, 8.6, 12.6, 16, 16.4, 16.9, 15.5, 12.4, 8, 6.2, 4.4, 
3.6, 4.6, 10.3, 12.5, 16.4, 19.1, 19.2, 15.7, 10.4, 6.7, 6.4, 
4.4, -1.8, 6.7, 8.1, 13.8, 14.4, 17.8, 16.4, 16.4, 10.6, 5.3, 
5.2, 3.1, 6.9, 9.8, 9.6, 11.5, 17, 18.5, 17.6, 15.1, 11.8, 6.8, 
3.6, 3.7, 6.2, 4.9, 7.9, 13.9, 15.6, 17.9, 18.4, 17.3, 11.4, 
6.7, 5.1, 3.4, 4.5, 8.6, 10.2, 13.8, 17, 20.3, 18.9, 17.2, 12.2, 
6.8, 5.7, 3.5, 5, 8, 9.6, 14.5, 17.6, 16.8, 17.3, 14.5, 11.1, 
8.4, 3.5, 3.6, 7.6, 8.3, 11.7, 12.5, 16.6, 17.7, 18, 18.5, 12.3, 
6.4, 4.5, 4.8, 3.7, 3.9, 9.1, 11.5, 15.8, 17.6, 18.6, 15.5, 11.9, 
5.4, 1.3, -1.6, -0.3, 6.5, 9.6, 12.2, 15.8, 18.5, 16.5, 15.2, 
11.5, 9.3, 1.3, 1.5, 5.2, 5.6, 9.6, 14.5, 16.8, 19.6, 18.2, 16.7, 
9.6, 7.2, 3.2, 3.6, 1.7, 6.6, 8.7, 12.7, 16.1, 16.7, 17.1, 13.7, 
12.2, 6.3, 5.7, 2.6, 7.9, 6.2, 10.5, 13.2, 17, 16.8, 17.2, 16.6, 
12.7, 5, 5.3, 3.5, 5.5, 7.7, 8.8, 12.5, 15.6, 19.8, 18.1, 15.3, 
13.2, 7.1, 3, 3.3, 4.3, 6.8, 9.9, 11.8, 15.9, 17.8, 17.2, 15.1, 
13.5, 6.8, 3, 4.8, 2.1, 6.2, 9.2, 13.2, 15, 19.1, 18.1, 15.9, 
13.1, 7.1, 1.4, 4.1, 4.3, 4.4, 7.6, 12.8, 17.6, 17.8, 18.3, 16.6, 
11.3, 8.7, 2.6, 3.1, 4.2, 3.8, 10.5, 13.7, 14.8, 19.7, 18.7, 
15.7, 12.3, 5.8, 4.9, 3.2, 5.5, 7.9, 8.9, 11.7, 14.3, 18, 17.1, 
13.3, 10.9, 7.3, 4.5, 3, 3.4, 6.1, 7.6, 13.5, 17, 18.1, 19.9, 
16.7, 10.6, 6.8, 3.7, 6.2, 5.5, 7.3, 9.4, 12.5, 15.9, 17.7, 18.6, 
14.5, 8.2, 7.4, 6.8, 6.4, 5.5, 5.3, 9, 12.1, 15.9, 19.1, 19.8, 
16.1, 10.4, 6.7, 3.1, 4.1, 4.8, 6, 8.9, 14, 18.8, 20.1, 19, 14.8, 
11.8, 6.6, 3.1, 3.7, 6.6, 8.3, 8.3, 12.1, 14.8, 17.8, 16.9, 14.7, 
12.9, 7, 5.3, 3.3, 4, 7.2, 7.8, 12.3, 15.2, 17.3, 17.2, 15.6, 
11.8, 6.7, 5.1, 1.3, 4, 6.6, 8.2, 12.3, 16.5, 18.5, 17.1, 15.7, 
12.4, 6.7, 5.7, 2.2, 6.3, 6.2, 8.4, 11.9, 15, 16.4, 18.6, 16.5, 
10.8, 5.8, 3.1, 3.3, 2.9, 9.2, 10, 12.6, 16, 17.5, 18.8, 16.2, 
11.2, 7.2, 3.8, 4.6, 5, 6.3, 9.3, 13.4, 17.4, 20.1, 18, 17.4, 
11.4, 8.3, 4.9, 5.5, 2.5, 7, 8.9, 11.5, 17.1, 22.2, 19.3, 16.3, 
11.9, 7.6, 4.5, 4.2, 3.5, 5.2, 9.6, 10.4, 15.8, 18.8, 18.4, 14.7, 
11.9, 9, 4.5, -1, 3.8, 5.2, 9.7, 12.5, 15.3, 19.4, 17.6, 17.3, 
12.3, 4.4, 5.6, 3.9, -0.6, 5.9, 6.9, 13.7, 16.9, 18.7, 17.6, 
14.9, 13.1, 7.9, 5, -0.8, 3.7, 4.8, 10.9, 11.4, 15, 18.6, 18.6, 
17.8, 12.4, 7.1, 5.2, 6.4, 4.9, 6.5, 10.1, 13.8, 16.2, 17.8, 
18.7, 15.7, 12.9, 6.3, 6, 4.2, 5.6, 9.3, 8.2, 15.3, 16.9, 20.2, 
19.5, 16.5, 13.2, 7, 5.6, 4.8, 8.8, 8.7, 8.9, 15.3, 16, 19.7, 
20.4, 15.9, 13.3, 7.2, 3.1, 3.9, 1.9, 9, 8.7, 11.7, 14.9, 19.6, 
20.7, 17.9, 10.9, 6.9, 3.6, 2.8, 4.9, 7.6, 9.5, 15.3, 16.1, 19.1, 
19.9, 15.5, 9.6, 9, 4.8, 5.9, 3.5, 7, 10.4, 14.1, 17.3, 17.8, 
18.7, 14.7, 10.4, 4.8, 6.2, 5.2, 5.1, 9.4, 8.7, 13.6, 17.1, 21.4, 
19.9, 15, 12, 10.2, 6.5, 4.5, 7.5, 6.5, 9.9, 13.6, 16.1, 21.1, 
20.2, 14.5, 14.6, 7.5, 3.8, 5, 2.9, 6, 10, 12.2, 17.5, 18.7, 
18.2, 14.2, 11.9, 6.9, 3.4, 2.3, 6.9, 9.3, 10, 14.2, 16.3, 18.6, 
21, 17, 12.4, 8.4, 5.5, 5, 5.9, 8.1, 9, 14.9, 17, 18.5, 19.4, 
16.1, 11.6, 5.2, 4.5, 5.3, 4.3, 8, 10, 15.2, 16.3, 20.2, 19.4, 
17.9, 12.2, 6.4, 5, 3.7, 6.6, 7.5, 9.9, 15, 17.8, 17.5, 19.6, 
16.9, 12.2, 8.2, 7.1), .Tsp = c(1901, 2000.91666666667, 12), class = ""ts"")  
</code></pre>

<p>I run <code>stl()</code> on it to remove the seasonality:  </p>

<pre><code># calculate and remove the seasonality  
fr.monthly.temp.ts.stl &lt;- stl(fr.monthly.temp.ts, s.window=""periodic"")    # get the    components  
fr.monthly.temp.seas &lt;- fr.monthly.temp.ts.stl$time.series[,""seasonal""]  
#plot(fr.monthly.temp.seas)  

fr.monthly.temp.ts.noseas &lt;- fr.monthly.temp.ts - fr.monthly.temp.seas  
#plot(fr.monthly.temp.ts.noseas)  
</code></pre>

<p>Then remove the trend with a regression:</p>

<pre><code>fr.mtrend.noseas &lt;- lm(fr.monthly.temp.ts.noseas~t)  
summary(fr.mtrend.noseas)  
</code></pre>

<p>and then use the residuals of this model to fit an ARIMA model (after checking the ACF and PACF for which one is appropriate):</p>

<pre><code># create time series of residuals..this is our ""detrended"" series..for now use only linear trend result  
fr.monthly.temp.ts.new &lt;- ts(fr.mtrend.noseas$resid, start=c(1901,1), frequency=12)
#plot.ts(fr.monthly.temp.ts.new, main=""Detrended and de-seasonalized time series"")

# ARIMA 1,1,1  
fit6 &lt;- arima(fr.monthly.temp.ts.new,order=c(1,1,1))  
fit6  
tsdiag(fit6)  
</code></pre>

<p>I then make a prediction on the stationary time series:</p>

<pre><code>#forecast for the stationary TS, for next 50 yrs months  
forecast &lt;- predict(fit6,n.ahead=600)  
</code></pre>

<p>And then add back the trend and seasonality:</p>

<pre><code>t.new &lt;- (n+1):(n+600)  

#initial time series = stationaryTS + seasonality + trend  
fr.monthly.temp.ts.init &lt;- fr.monthly.temp.ts.new + fr.monthly.temp.seas +
                            fr.mtrend.noseas$coefficients[1] + t * fr.mtrend.noseas$coefficients[2]  

#same for the prediction: we need to add seasonality and trend  
pred.Xt &lt;- forecast$pred + fr.monthly.temp.seas[1:(1+50*12 - 1)] + 
                                fr.mtrend.noseas$coefficients[1] + t.new * fr.mtrend.noseas$coefficients[2]  

plot(fr.monthly.temp.ts.init,type=""l"",xlim=c(1940,2060))  
lines(pred.Xt,col=""red"",lwd=2)  
</code></pre>

<p>So going back to my question: Do I need to add some white noise to the prediction to be able to realistically predict temperature? And more generally, is my method correct?</p>
"
"0.104541674697863","0.101534616513362"," 59065","<p>I have a monthly time series (for 2009-2012 non-stationary, with seasonality). I can use ARIMA (or ETS) to obtain point and interval forecasts for each month of 2013, but I am interested in forecasting the total for the whole year, including prediction intervals. Is there an easy way in R to obtain interval forecasts for the total for 2013?</p>
"
"0.132235920981457","0.160540324766984"," 59305","<p>So I remember reading somewhere that when we have external regressors,  <code>auto.arima</code> cannot make correct predictions for the order of difference for either seasonality or the main time series itself (correct me if I'm wrong!)</p>

<p>Now, I'd like to know whether we'd need to difference the external regressors as well?  Also, in the case of having external regressors (a few time series and a few dummies for seasonal patterns in those time series), can <code>auto.arima</code> even calculate the optimal MA and AR?</p>

<p>Also, I have weekly seasonality as well as quarterly and yearly seasonality; since I can't specify that many seasonalities in auto.arima, I'm inputting a lot of dummy variables for quarters and months; will that yield mathematically correct results?</p>

<p>Further, for those of you who have worked with SAS, when using the forecast procedure and estimating the input variables (the external regressors), does it automatically calculate the MA and AR for each external regressor?</p>
"
"0.128036879932896","0.124354000843452"," 62237","<p>I am working on a data set. After using some model identification techniques, I came out with an ARIMA(0,2,1) model. </p>

<p>I used the <code>detectIO</code> function in the package <code>TSA</code> in R to detect an <em>innovative</em> outlier (IO) at the 48th observation of my original data set. </p>

<p>How do I incorporate this outlier into my model so I can use it for forecasting purposes? I don't want to use the ARIMAX model since I might not be able to make any predictions from that in R. Are there any other ways I could do this?  </p>

<p>Here are my values in order:</p>

<pre><code>VALUE &lt;- scan()
  4.6  4.5  4.4  4.5  4.4  4.6  4.7  4.6  4.7  4.7  4.7  5.0  5.0  4.9  5.1  5.0  5.4
  5.6  5.8  6.1  6.1  6.5  6.8  7.3  7.8  8.3  8.7  9.0  9.4  9.5  9.5  9.6  9.8 10.0
  9.9  9.9  9.8  9.8  9.9  9.9  9.6  9.4  9.5  9.5  9.5  9.5  9.8  9.3  9.1  9.0  8.9
  9.0  9.0  9.1  9.0  9.0  9.0  8.9  8.6  8.5  8.3  8.3  8.2  8.1  8.2  8.2  8.2  8.1
  7.8  7.9  7.8  7.8
</code></pre>

<p>That is actually my data. They are unemployment rates over a period of 6 years. There are 72 observations then . Each value is to at most one decimal place</p>
"
"NaN","NaN"," 62810","<p>I got some users' history data and generated some sequences of real numbers. The length of each sequence is between 15 and 25. What's more, I do not know whether these sequences have patterns and the frequency is not known as well.</p>

<p>My goal is using each sequence to predict its next value, and then I use auto.arima in R to do this. However, the accuracy of the prediction is low.</p>

<p>Anyone have any good ideas to improve the accuracy?</p>

<p>One of these sequences is:</p>

<pre><code>    1.5959709882736206  
    0.7300914525985718  
    2.0011744499206543  
    3.6755871772766113  
    0.8066112399101257  
    1.3413848876953125  
    3.371157646179199  
    0.4400146007537842  
    2.637667655944824  
    2.1453769207000732  
    2.341433048248291  
    2.3429665565490723  
    1.1187453269958496  
    1.4169363975524902  
    3.328829050064087  
    4.157748699188232  
    3.9255290031433105  
    2.7843635082244873 
</code></pre>
"
"0.221766381286372","0.215387447585321"," 63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.165294901226822","0.160540324766984"," 64621","<p>I am new to time series modeling in R. I have sales data of one year and three months only. I am trying to do sales forecasting at the day level or max at the week level. Following is the step I intend to follow</p>

<ol>
<li>Convert it into time series object using <code>ts(data$qty, frequency= ??)</code>. Here I am very confused about frequency. I can see in data that there is some seasonality like sales is picking up in May, June, July and then again in festival seasons. I guess I cannot use 365 as I have only one year data. Please suggest what should be the frequency</li>
<li>Decompose the time series. Subtract the seasonality and trend from the actual time series model </li>
<li>Fit ARIMA to get a prediction</li>
<li>Again add seasonality and trend to output the final forecast</li>
</ol>

<p>Please provide feedback on this if its correct approach or not or if there is any other better way to handle it.</p>
"
"0.147844254190915","0.143591631723548"," 66927","<p>I need to take the output parameters from an ARIMA model fitted in R from the following set (1,0,1), (0,1,0), (1,1,0), (0,1,1), (1,1,1) of models and implement the prediction function in C. I DO NOT HAVE THE OPTION of calling predict or any other R package for that step. </p>

<p>Obviously, I can eventually track down all the source code in predict and figure it out. But I was hoping there is somewhere that will walk me through a simple example of how to map the various output parameters of Arima() with X, Y, a, b, E, t (no upper and lower case thetas and B^t's) since every paper loves to include those already. </p>

<p>I think this request is slightly duplicative except in previous versions the question was retired without an answer or a link.</p>

<p>UPDATE: So, first, I HIGHLY second all recommendations for <a href=""http://otexts.com/fpp/"" rel=""nofollow"">Forecasting: Principles and Practice by Hyndman&amp;Athanasopoulos</a>. </p>

<p>I think what I've been missing is that ""d"" isn't a model parameter -- it changes what is being modeled. So while I'm not all the way to where I want to be, I'm starting to be able to write predictive equations based on R output. I will update with my eventual findings if nobody else posts something better. </p>
"
"0.209083349395727","0.203069233026724"," 68131","<p>Just asking if someone knows why the prediction intervals are quite different when one uses a time series analytic method of estimation <em>versus</em> when one simulates such time series. </p>

<p>For example, I used the forecast package's <code>auto.arima</code> function to get the best fit to my data, say it was an ARIMA(1,1,1), and then, on the one hand, I simulated such process doing around 10 thousand simulations and then calculating the 95% percentile with ""quantile"" function, and on the other hand, I used R's <code>forecast</code> package to do it. So I realized that these different approaches gave prediction intervals with different width (actually, those related with simulation approach are closer than those obtained with forecast package). The way I simulated such time series process is simulating the parameters as random variables distributed normally with mean equal to its estimated value and standard deviation equal to its related standard error. The ""white noise"" variables related with the Moving Average (MA) part of the process were simulated as normally distributed with mean zero and variance equal to the variance of the residuals.</p>

<p>Thanks in advance for your help.</p>
"
"0.3556579998136","0.373062002530355"," 68812","<p>I'm really new to ARIMA methods and am trying to forecast electricity load. I've integrated: electricity load, temperature, weekday (dummy), public holidays, and school holidays. My model tries to perform a non seasonal ARIMA with linear regression for each hour of the day.</p>

<p>Here is my code for an example of one of the 24 hours (6 AM):</p>

<pre><code># ElecLoad contains hourly loads and other data for 2005 and 2006 (=2*365*24 entries):
# 1. Electricity load in MW
# 2. day of weak: sunday=0, monday=1, etc 
# 3. Hour of the day 0 -&gt; 23
# 4. Public Holiday: 1 if Public Holiday, 0 otherwise
# 5. Scool vacation: 1 if no scool
# 6. Temperature in Â°F

# Create the weak matrix = dumy variables for the weakdays
weakmatrix&lt;-model.matrix(~as.factor(ElecLoad[,2]))
#Remove intercept
weakmatrix&lt;-weakmatrix[,-1]

#Generate FullTable
FullTable&lt;-cbind(load=ElecLoad[,1], weakmatrix, ElecLoad[,4],
                 ElecLoad[,3],ElecLoad[,5],ElecLoad[,5]^2, ElecLoad[,6])
colnames(FullTable)&lt;-c(""Load"",""mon"",""tue"",""wed"",""thu"",""fri"",""sat"",
                       ""ScoolHol"",""PubHol"",""Temp"",""Temp2"",""Hour"")

#Create the xreg = substed for a specific hour of the day (column 12 = Hour)
xreg&lt;-subset(FullTable[,2:11], FullTable[,12] == 7)

#Create the Load time serie, also a subset of the full table
LoadTs&lt;-ts(subset(FullTable[,1], FullTable[,12] == 7),start=1,frequency=1)

#Launch of auto.arima
ArimaLoad&lt;-auto.arima(LoadTs, xreg=xreg, lambda=0)
</code></pre>

<p>When I try to forecast with the same 2 years data as <code>xreg</code>, here is my output</p>

<pre><code>plot(forecast(ArimaLoad,xreg=xreg), include=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>While when I try to plot the fitted it looks identical to my original Load</p>

<pre><code>plot(fitted(ArimaLoad))
</code></pre>

<p><img src=""http://i.stack.imgur.com/zsw81.png"" alt=""enter image description here""></p>

<p>I don't understand why the <code>prediction()</code> is so much different than the <code>fitted()</code> with the same <code>xreg</code> matrix. Is this a normal behaviour, how can I improve my model to better fit with the real situation?</p>

<hr>

<p>Thank you so much for your support.</p>

<p>I'm not sure I understood everything from what you propose.</p>

<p>You mean that I should build a first model to forecast the daily average load (I prefer the average than the sum because due to DST, some days don't have 24 hours...). This model would be deterministic, but I don't see what kind of model you're thinking off? Is a multilinear regression ok? I prefer to consider the log(load) to make the different parameters multiplicative which I think is better fit to the reality.
Then I should have 24 hourly models, taking the daily average then split with a sort seasonal effect?
Should I use somewhere an ARIMA model?
I'm not convince of considering the month as having an effect, in my opinion there is no reason that consumption is more important in January than August except if we consider the Temperature and Holidays effects. The hour of the day is related to the activity that's the reason why I'm considering the specific model for each hour. The same way each day of the weak is different.</p>

<p>I've tried a multilinear regression for the same hour (7:00 AM) and the result looks not so bad.</p>

<pre><code>#Create the frame.data
Load&lt;-subset(FullTable[,1], FullTable[,12] == 7)
FullData&lt;-cbind(LogLoad=log(Load), xreg)
FrameData&lt;-data.frame(FullData)

# multilinear regression
mlin&lt;-lm(LogLoad ~ mon+tue+wed+thu+fri+sat+ScoolHol+PubHol+Temp+`Temp2`, FrameData)
plot(exp(mlin$model$LogLoad), type=""l"",col=""blue"")
lines(exp(fitted(mlin)), col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>fitted() in red which is now exactly the same as predict() if I re-use the same data entry (2005-2006) and looks not so far from the original load in blue (no so bad for a simple model). I still don't fully understand why it did not work with ARIMA as it also takes into consideration multilinear regression.</p>

<p>Now my ""simple"" model already takes into account several parameters, like the temperature, the holiday, the school vacations the day of the weak and the hour of the day (local time, not UCT).
How can I improve my model further more? How can I make sure that the parameters are invariant? Is there a specific method?</p>
"
"0.0522708373489317","0.101534616513362"," 83433","<p>I would like to ask how the long-term (multiple step ahead) prediction intervals are calculated by function <code>predict.Arima</code> in R. I am particularly interested in ARIMA models, SARIMA models and in ARIMA models with external regressors (include argument xreg => regression with ARIMA errors) </p>
"
"0.085357919955264","0.124354000843452"," 84255","<p>I am new to R and the ARIMA model and I am attemping to forecast 1440 values into the future using a base of roughly 5000 numbers. It is data extracted roughly every minute from a machine log(performance values). The intend to forecast 1 day into the future, which explains the 1440 values(as they are minutes).</p>

<p>Here is my result using the following commands:
    datats&lt;-c(data);
    arima&lt;-auto.arima(datats);
    fcast&lt;-forecast(arima, h=1440);</p>

<p><img src=""http://i.imgur.com/N7aCqvx.png"" alt=""forecast""> </p>

<p>The prediction begins at the flat line on the right hand side.</p>

<p>Forecast method: ARIMA(0,1,1)                   </p>

<p>Model Information:
Series: datats 
ARIMA(0,1,1)                    </p>

<p>Coefficients:
          ma1
      -0.9373
s.e.   0.0071</p>

<p>sigma^2 estimated as 86737:  log likelihood=-21221.46
AIC=42446.93   AICc=42446.93   BIC=42458.93</p>

<p>Error measures:
                    ME     RMSE      MAE       MPE     MAPE      MASE
Training set 0.6506441 294.4619 196.7211 -59.85254 85.45473 0.7637028
                   ACF1
Training set 0.01519673</p>

<p>Dataset here: <a href=""http://pastebin.com/92ssDExn"" rel=""nofollow"">http://pastebin.com/92ssDExn</a></p>

<p>Is the issue too little past values?
To many values to be predicted?</p>

<p>Any information or advice would be extremely welcomed, any other information required will be provided. </p>
"
"0.233762291106092","0.22703830459325"," 86211","<p>I have about 64000 music Charts ranked by their usage frequency. 
I want to have a future two-day prediction frequency and eventually its rank for each music chart using its past 21 days usage frequencies but  the frequencies are obviously correlated but because of scale, I dont want to use ARIMA related models. I am not sure what would be the effect of correlated residuals on my predictions.
Constraints:<br>
1-Even though  the frequencies are correlated (time series data), I don't want to use ARIMA related models as for a production scale, they aren't stable and break a lot (Matrix singularities,..)    </p>

<p>2- Because of the scale, it is very much preferable to just use the 21 data points for each record, independent of other records (no Mixed effect model)    </p>

<p>3- The model must outperform the current method ( Using today's record rank as the predicted rank for two days later (base model). The base model for the top rank records is fairly good and hard to beat as the daily ranks don't fluctuate much.  </p>

<p>This is what I have been doing.  </p>

<p>1- Use this model: freq = b0 + b1 Day(-2) i.e. I use days 1:21 to predict the frequencies of  days 3:23 (day1 for freq on day3,...).  A weighted least square approach<br>
 lm(freq ~ poly(day,2),data=df,weights=wgh) is used to counter the non-constant variance.  </p>

<p>2- Since this was not good enough, I used a weighted average of predicted frequency and the base frequency. For example  if  ( 1198,1234) are the predicted frequencies of day 23 and the actual frequency on day 21 (two days earlier(the base frequency)) respectively.<br>
 My final predicted freq for day 23 will be<br>
 ** w freq_pred[23] + (1-w) freq[21] =  w * 1198 + (1-w) 1234** for some w.    </p>

<p>That is the only way I can beat the base prediction for top records.<br>
How can I improve my model? How will the unaccounted serial correlation will affect my results?<br>
Are there other suggestions? Is there a big problem with this approach?  </p>
"
"NaN","NaN"," 89316","<p>I am using <code>auto.arima()</code> for prediction, and getting the following warning message. I want to know if I can ignore this warning message or if I should be worried.</p>

<p><code>Warning message:</code><br>
<code>In auto.arima(forecast_data_ts) :</code><br>
<code>Unable to fit final model using maximum likelihood. AIC value approximated</code></p>
"
"0.150892910070864","0.175863114528165"," 92743","<p>I have an example of call center data for 2013. There are 261 days of data (excluding weekends).<br>
For 2013, I have included a holiday dummy variable (<code>holiday</code>) for the days where there were no stats.<br>
For 2014, I have also included a future holiday dummy variable (<code>holidayf</code>).<br>
<strong>My objective is to assess how accurate this code is in making predictions for 2014.</strong></p>

<p>I tried this code below but when looking at <code>fc$fitted</code>, the forecasts don't seem to be correct. For the first 8 days of January 2014, it forecasts the exact number of calls that were received in the first 8 days of January 2013, which seems wrong. Also, where there is a public holiday in 2014, the future forecast for that day predicts a normal to high volume of calls, so it seems that the forecast is using the <code>holiday</code> variable and not the <code>holidayf</code> variable.</p>

<pre><code>library(forecast)
y &lt;- ts(calls,frequency=5)
z &lt;- fourier(ts(calls,frequency=261),K=12)
zf &lt;- fourier(ts(calls,frequency=261),K=12,h=261)
fit &lt;- auto.arima(y,xreg=cbind(z,holiday))
fc &lt;- forecast(fit,xreg=cbind(zf,holidayf),h=261)
plot(fc)
</code></pre>

<p>Data:</p>

<pre><code>calls &lt;- 
  c(0,145,175,129,266,219,156,184,167,241,218,194,192,162,236,219,212,191,162,216,
  235, 218,180,150,245,209,210,211,151,236,197,217,140,164,200,156,152,153,141,224,178,
  159,153,137,207,173,197,213,206,305,284,248,289,269,359,333,257,0,244,325,292,267,
  206,0,0,360,261,327,284,385,377,317,327,271,372,191,320,268,261,376,320,280,251,200,
  200,200,0,236,161,259,200,190,166,174,225,228,202,201,155,241,207,199,179,178,249,
  243,230,177,181,264,250,219,204,178,244,249,185,184,164,0,253,216,217,165,170,185,
  175,160,148,231,223,196,162,149,228,213,190,177,139,212,205,221,190,170,196,210,
  198,192,131,220,185,199,153,166,240,176,200,145,0,255,202,220,220,181,250,171,164,
  142,118,179,197,167,130,124,180,214,203,153,140,161,200,191,159,141,227,170,166,
  166,106,131,0,176,156,109,196,175,175,174,161,230,191,159,150,91,180,188,173,157,
  107,193,172,172,172,116,195,183,169,146,125,208,160,160,177,128,191,176,149,175,
  136,217,162,178,130,99,158,154,135,146,106,155,148,119,137,96,161,106,114,139,84,
  0,97,95,82,65,59,23,0,0,48,83,48)



holiday &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,
             0,0,0,1,1,1,0,0,0)

 holidayf &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,
             0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
             0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,
             0,0,0,0,1,1,0,0,0)
</code></pre>
"
"0.128036879932896","0.124354000843452"," 92935","<p>I'm making a project connected with identifying the dynamics of sales. My database concerns 26 weeks (so equally in 26 time-series observations) after launching the product.</p>

<p>This is what my database looks like: <a href=""https://imageshack.com/i/0yyh6ij"" rel=""nofollow"">https://imageshack.com/i/0yyh6ij</a> </p>

<p>I want to make forecast based on S-curve for clusters of time-series. The main aim was to compare two methods of forecasting:</p>

<ol>
<li>based on parameters of logistic curve</li>
<li>based on ARIMA</li>
</ol>

<p>However, I do not know how to compare these two methods = measure their performance.</p>

<p>That's a plot with prediction based on S-curve</p>

<p><a href=""http://imageshack.com/a/img850/6600/rzkp.jpg"" rel=""nofollow"">http://imageshack.com/a/img850/6600/rzkp.jpg</a></p>

<p>So my questions are:</p>

<ol>
<li>How to measure performance=forecast errors based on logistic curve?</li>
<li>How to compare forecasting based on logistic curve and ARIMA - what is the main difference between these two approaches if I base on one variable - units_sold_that_week?</li>
</ol>

<p>I would be grateful for any explanation.</p>
"
"0.267212556013138","0.259526399216023"," 94774","<p>I'm new in the page and pretty new in statistics and R. I'm working on a project for college with the objective of finding the correlation between rain and water flow level in rivers. Once the correlation is proved I want to forecast/predict it.</p>

<p><strong>The data</strong>
I have a set of data of several years(taken every 5 minutes) for a particular rivers containing: </p>

<ul>
<li>Rainfall in millimetres</li>
<li>River flow in cubic meters per second</li>
</ul>

<p>This river doesn't have snow, so the model is just based on rain and time. There are occasionally freezing temperatures, but I'm thinking on removing those periods out of the data as outliers as that situation is out of scope for my project.</p>

<p><strong>Examples</strong>
Here you have a couple of plots of sample data the from a rain and the rise of water a few hours later.</p>

<p><img src=""http://i.stack.imgur.com/ssmtM.jpg"" alt=""Bigger example a few days""></p>

<p><img src=""http://i.stack.imgur.com/XSkvv.jpg"" alt=""Shorter example just one rainfall period""></p>

<p>The red line is the river flow. The orange is the rain. You can see it always rains before water raises in river. There is some rain starting again at the end of the time series, but it will affect the river flow later.</p>

<p>The correlation is there. Here is what I've done in R to prove the correlation using ccf in R: </p>

<ul>
<li>the cross-correlation</li>
<li>the leading variable</li>
<li>the lag</li>
</ul>

<p>This is my R line used for the second example (one rainfall period):</p>

<pre><code>ccf(arnoiaex1$Caudal, arnoiaex1$Precip, lag.max=1000, plot=TRUE, main=""Flow &amp; Rain"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/IT62e.jpg"" alt=""ccf result for small example 2""></p>

<p>My interpretation is: </p>

<ul>
<li>that the rain leads (happens first),</li>
<li>there is a significant correlation that peaks at a lag of $\approx 450$ (I can check the exact number, I know that part). </li>
<li>I don't know how to find out the time that correlation affects the river flow, I think the name is â€œretentionâ€. What I see is the graph follows the same shape of the first graph, when the river losing the water after the rain. I don't if based on that I can say the retention lasts from $\approx 450$ when it peaks to $\approx 800$ (I can check this in the object created in the dataframe returned by <code>ccf</code> and see when the water level comes back to the value of â€œbefore rainâ€. Is that right? Is there a better way to find the retention?</li>
</ul>

<p>Am I right?</p>

<p><strong>About the time series</strong>.
This time series doesn't have periodicity or seasonality. Rain can come any time and cause an effect. It does reduce in summer, but it still happens, it's an area with a lot of rain all year around.</p>

<p><strong>Model and forecast.</strong>
I don't know how to create a model to be able to do a forecast that tells me how much is a river going to increase the volume after a period of rain. I've been trying some <code>arima</code>, <code>auto arima</code> but haven't been very successful. Should I use <code>Arima</code>, <code>vars</code> or other different multivariate model? Any link to a example would be of great help.</p>

<p>Please, let me know if you know the best way to create this prediction, what model should I use. There are a few other things I'm considering doing but taken them out of this explanation for simplicity.
I can share some data if required.</p>
"
"0.165294901226822","0.160540324766984","103775","<p>For work, I'm working on an app where you essentially forecast the failure rate of the overall machine through different factors such as the historical failure rates for the components used to build it or the failure rates of the factories that manufacture it, or even the historical rate for the machine itself. The idea is that for any machine you can make a solid prediction, so I need some algorithm to self-build a good model for each of the 1000s of machines.</p>

<p>I've been able to implement this using ARIMAX models, but I just don't feel good about using auto.arima and then just cross-validating to see how many external regressors to add in. I've also tried SVM, but what seemed to happen was that the model was not good at dropping irrelevant factors, and therefore the prediction was a flat line.</p>

<p>I feel like boosting would be a promising area, but I was wondering if anyone had other options and could more importantly, point me to examples of how the specific algorithm was implemented in R? I'm actually an undergrad intern majoring in statistics, so I'm not too strong in the actual programming side of things, so am not very good at implementing the theory I read about into R code.</p>

<p>Also, would a normal GLM be good enough? I used ARIMAX because I wanted to correct for autocorrelation.</p>
"
"0.295688508381829","0.287183263447095","104558","<p>I am really new to R and to time series. My field of studies is in the field of Networks and Telecommunication, but my summer internship is about trying to find a statistical model for some sets of data.</p>

<p>The data consists of what is called ""10-minutes-points"", recorded over a year and which represent power consuption of a source substation. It means I have 6 * 24 * 365 = 52 560 points of data to process, one set for each source substation.</p>

<p>It's been about a week I'm trying to found information about ARIMA models. <a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">This website</a> and the report of my predecessor quite helped me getting in the subject, by I still encountered many problems.</p>

<p>I found one might be due to the large size of the data set <a href=""http://stats.stackexchange.com/questions/27313/how-would-you-fit-arima-model-with-lots-of-autocorrelations"">as explained here</a>, the second one to the existence of exogenous data as <a href=""http://stats.stackexchange.com/questions/25780/what-is-the-purpose-of-and-how-to-use-the-xreg-argument-when-fitting-arima-model"">mentioned there</a>.</p>

<p>My predecessor found the ARIMA model to be effective for short term predictions (up to 20-ish hours), and the SARIMAX for mid-term predictions (around a dozen days). I guess it is cause exogeneous data doesn't affect as much the core data on such short periods of time.</p>

<p>I found <a href=""http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r"">this thread</a> to be very interesting but I'm not sure I understand everything.</p>

<p>In a first time I would like to know if my understanding of the general method to evaluate a model is correct :</p>

<ol>
<li><p>first you plot your data and try to look for any trend/seasonality (the data I have showed to have a daily seasonality and a yearly one)</p></li>
<li><p>you use log in order to reduce the trend, and maybe differentiate to eliminate the seasonality (so I should use something like : <code>diff(data.ts, 144)</code> in my case to get rid of the daily seasonality (6*24 points a day) ?)</p></li>
<li><p>plot the acf/pcf of the differentiated time series and try to estimate a model from there</p></li>
<li><p>try to fit the model to my data with <code>fit &lt;- Arima(data, order=c(p,d,q), seasonal=c(P,D,Q))</code> but I don't where the seasonality (144) would appear in this function ?</p></li>
<li><p>study the residuals of fit to see if the model is correct (looking at the acf/pacf)</p></li>
<li><p>use fitted or forecast (I don't know which one is better) to predict future values</p></li>
</ol>

<p>Thing is, since the data set is huge, I always get significant spikes at many lags in the acf/pacf and I don't feel I can judge if a model is correct or not.</p>

<p>Here is an example :</p>

<p><code>data = scan(""auch.txt"", skip=1)
plot.ts(data)</code></p>

<p><img src=""http://i.stack.imgur.com/weVCX.png"" alt=""Data""></p>

<p><code>data.ts = ts(log(data)
data.diff = diff(data.ts, 144)
plot.ts(data.diff)</code></p>

<p><img src=""http://i.stack.imgur.com/Ck7mu.png"" alt=""Datadiff""></p>

<p>Which seems somehow stationary to me. I then proceed to look at the acf/pacf, and had to differentiate once more because it wasn't stationary in fact :</p>

<p><code>tsdisplay(data.diff, lag.max=150)
tsdisplay(diff(data.diff), lag.max=150)</code></p>

<p><img src=""http://i.stack.imgur.com/dgqG6.png"" alt=""Tsdisplay"">
<img src=""http://i.stack.imgur.com/P7Kj7.png"" alt=""Tsdisplaydiff""></p>

<p>And I really don't know how to handle these results, so I hoped I could find some help here, because I came across the website a lot during my researchs.</p>

<p>Thanks in advance, and I apologies for any grammatical mistakes or vocabulary error ; English is not my native language.</p>

<p><strong>Edit :</strong> does anyone know why my pictures won't appear ?</p>

<p><strong>Edit bis :</strong> nvm in fact it might be me, because imgur is blocked on my work computer</p>
"
"0.246027710431419","0.258863495453396","104977","<p>I understand we should use ARIMA for modelling a non-stationary time series. Also, everything I read says ARMA should only be used for stationary time series.</p>

<p>What I'm trying to understand is, what happens in practice when misclassifying a model, and assuming <code>d = 0</code> for a time series that's non-stationary? For example: </p>

<pre><code>controlData &lt;- arima.sim(list(order = c(1,1,1), ar = .5, ma = .5), n = 44)
</code></pre>

<p>control data looks like this:</p>

<pre><code> [1]   0.0000000   0.1240838  -1.4544087  -3.1943094  -5.6205257
 [6]  -8.5636126 -10.1573548  -9.2822666 -10.0174493 -11.0105225
[11] -11.4726127 -13.8827001 -16.6040541 -19.1966633 -22.0543414
[16] -24.8542959 -25.2883155 -23.6519271 -21.8270981 -21.4351267
[21] -22.6155812 -21.9189036 -20.2064343 -18.2516852 -15.5822178
[26] -13.2248230 -13.4220158 -13.8823855 -14.6122867 -16.4143756
[31] -16.8726071 -15.8499558 -14.0805114 -11.4016515  -9.3330560
[36]  -7.5676563  -6.3691600  -6.8471371  -7.5982880  -8.9692152
[41] -10.6733419 -11.6865440 -12.2503202 -13.5314306 -13.4654890
</code></pre>

<p>Assuming I didn't know the data was <code>ARIMA(1,1,1)</code>, I might have a look at <code>pacf(controlData)</code>.</p>

<p><img src=""http://i.stack.imgur.com/IOXJf.jpg"" alt=""pacf(controlData)""></p>

<p>Then I use Dickey-Fuller to see if the data is non-stationary:</p>

<pre><code>require('tseries')
adf.test(controlData)

# Augmented Dickey-Fuller Test
#
# data:  controlData
# Dickey-Fuller = -2.4133, Lag order = 3, p-value = 0.4099
# alternative hypothesis: stationary

adf.test(controlData, k = 1)

# Augmented Dickey-Fuller Test
#
#data:  controlData
# Dickey-Fuller = -3.1469, Lag order = 1, p-value = 0.1188
# alternative hypothesis: stationary
</code></pre>

<p>So, I might assume the data is ARIMA(2,0,*) Then use <code>auto.arima(controlData)</code> to try to get a best fit?</p>

<pre><code>require('forecast')
naiveFit &lt;- auto.arima(controlData)
navifeFit
# Series: controlData 
# ARIMA(2,0,1) with non-zero mean 
# 
# Coefficients:
#          ar1      ar2     ma1  intercept
#      1.4985  -0.5637  0.6427   -11.8690
# s.e.  0.1508   0.1546  0.1912     3.2647
#
# sigma^2 estimated as 0.8936:  log likelihood=-64.01
# AIC=138.02   AICc=139.56   BIC=147.05
</code></pre>

<p>So, even though the past and future data is ARIMA(1,1,1), I might be tempted to classify it as ARIMA(2,0,1). <code>tsdata(auto.arima(controlData))</code> looks good too.</p>

<p>Here is what an informed modeler would find:</p>

<pre><code>informedFit &lt;- arima(controlData, order = c(1,1,1))
# informedFit
# Series: controlData 
# ARIMA(1,1,1)                    
#
# Coefficients:
#          ar1     ma1
#       0.4936  0.6859
# s.e.  0.1564  0.1764
#
# sigma^2 estimated as 0.9571:  log likelihood=-62.22
# AIC=130.44   AICc=131.04   BIC=135.79
</code></pre>

<p>1) Why are these information criterion better than the model selected by <code>auto.arima(controlData)</code>?</p>

<p>Now, I just graphically compare the real data, and the 2 models:</p>

<pre><code>plot(controlData)
lines(fitted(naiveFit), col = ""red"")
lines(fitted(informedFit), col = ""blue"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sy3YR.jpg"" alt=""tsPlots""></p>

<p>2) Playing devil's advocate, what kind of consequences would I pay by using an ARIMA(2, 0, 1) as a model? What are the risks of this error? </p>

<p>3) I'm mostly concerned about any implications for multi-period forward predictions. I assume they would be less accurate? I'm just looking for some proof.</p>

<p>4) Would you suggest an alternative method for model selection? Are there any problems with my reasoning as an ""uninformed"" modeler?</p>

<p>I'm really curious what are the other consequences of this kind of misclassification. I've been looking for some sources and just couldn't find anything. All the literature I could find only touches on this subject, instead just stating the data should be stationary before performing ARMA, and if it's non-stationary, then it needs to be differenced d times.</p>

<p>Thanks!</p>
"
"NaN","NaN","108551","<p>I have daily sales data from 2011 to 2013. I have to do prediction for 2014.I have used arima and exponential method to predict the daily sale, but it is not giving the better result. MAPE is around 25%. </p>

<p><code>y=ts(x,frequency=7)</code></p>

<p><code>fit &lt;- auto.arima(y)</code></p>

<p><code>fc &lt;- forecast(fit, h=265)</code></p>

<p><code>plot(fc)</code></p>

<p><code>fit &lt;- ets(y)</code></p>

<p><code>fc &lt;- forecast(fit,h=265)</code></p>

<p>Below is the link for data:</p>

<p><a href=""https://drive.google.com/file/d/0B5en_TDcZWi3QWNjMzd3ZkdIcW8/edit?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B5en_TDcZWi3QWNjMzd3ZkdIcW8/edit?usp=sharing</a></p>

<p>Is there a way to improve the MAPE? I am new to time series,I would appreciate any kind of help.</p>
"
"0.150892910070864","0.175863114528165","110798","<p>I have a problem with the <code>forecast</code> function for ARIMA models in R. It calls <code>predict</code> that calls <code>KalmanForecast</code>. Ok...here's the deal.</p>

<p>the mean one-step forecast of the Arima object produced by this call</p>

<pre><code>forecast(Arima, h=1)$mean[[1]]
</code></pre>

<p>is often significantly different from the result of a manual forecast by conditional expected value (best linear predictor). </p>

<p>For example a non seasonal Arima(1,1,1) without drift has of course the structure</p>

<pre><code>y[t] = y[t-1] + AR1*(y[t-1] - y[t-2]) + MA1*epsilon[t-1] + epsilon[t]
</code></pre>

<p>so the one-step prediction is very straightforward </p>

<pre><code>y[t] = y[t-1] + AR1*(y[t-1] - y[t-2]) + MA1*epsilon[t-1]
</code></pre>

<p>but this result is always different from the result of the forecast function call.
Is it due to approximation errors in the Kalman recursion?</p>

<p>Try yourself with this code, it only needs the <code>forecast</code> package</p>

<pre><code>  x = arima.sim(n = 1000, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),sd=sqrt(0.1796))
  t = length(x) + 1
 Arimafit = Arima(x = x, order = c(1,1,1), seasonal = list(order = c(0,0,0), period =     1), include.mean = FALSE,include.drift = FALSE)
 manualforecast = x[t-1] + coef(Arimafit)[[""ar1""]]*(x[t-1] - x[t-2]) + coef(Arimafit)    [[""ma1""]]*Arimafit$residuals[t-1]
     autoforecast = forecast(Arimafit, h = 1)$mean[[1]]
</code></pre>

<p><code>autoforecast</code> is always different from <code>manualforecast</code>, sometimes significantly.</p>
"
"0.181071492085037","0.175863114528165","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.0739221270954573","0.0717958158617738","114815","<p>I have seen many prediction of time series using ARIMA model in R forecast package, and the result always have some initial wiggle before reaching the stable level. What causes the initial wiggle?</p>

<p>I can't attach a image because I need at least 10 reputation to post images. Basically the predicted value for the first few steps forecast is different from the long stable level, but it converges to it after a few steps.</p>

<pre><code>install.packages(""forecast"")
library(forecast)
install.packages(""Ecdat"")
data(Tbrate,package=""Ecdat"")
attach(as.list(Tbrate))
auto.arima(pi,max.P=0,max.Q=0,ic=""bic"")
fit = arima(pi,order=c(1,1,1))
forecasts = predict(fit,36)
plot(pi,xlim=c(1980,2006),ylim=c(-7,12))
lines(seq(from=1997,by=.25,length=36), forecasts$pred,col=""red"")
lines(seq(from=1997,by=.25,length=36), forecasts$pred + 1.96*forecasts$se, col=""blue"")
lines(seq(from=1997,by=.25,length=36), forecasts$pred - 1.96*forecasts$se, col=""blue"")
</code></pre>
"
"0.195579564679489","0.18995387394524","116339","<p>I have a set of 3 years of daily data. I saw weekly and annual seasonality in the data so I used <code>msts</code> time series and <code>tbats</code> (from the <code>forecast</code> package in R) to fit the best fitted model. </p>

<p>The predicted values for weekdays are with 5% of the actual data but it has very off predictions for weekend. I did not expect that as I included daily seasonality in a week (different weekday and weekend patterns) in my time series which I though will consider the seasonality correctly. I wonder if anybody have any idea whats going wrong with my data. </p>

<p>I also used ts with single seasonality of frequency 7 and again used <code>tbats</code> to fit a model. Th new model has better predictions for weekend but worse predictions for weekday. I also tried <code>auto.arima</code> (also from the <code>forecast</code> package) but as I have a huge number of data points, arima was not able to find a good model. </p>
"
"0.085357919955264","0.124354000843452","116896","<p>I used tbats to fit a model for a 3 years of historic data and the values work fine but as I did not include holidays, holiday predictions are really off. I used arima with regressor (holidays at regressors) and the predictions for holidays are much better than the one by tbats but tbats got more accurate results for normal days. I know it sounds un reasonable but is this ok that I use the arima model for holidays and the tbats model for normal days?</p>
"
"NaN","NaN","125909","<p>I am trying to predict values using arima(0,1,1).
After doing <code>predict(mod,n.ahead=5)</code> (in <code>R</code>) am getting the same value for all the predictions: </p>

<pre><code>5947.681 5947.681 5947.681 5947.681 5947.681 
</code></pre>

<p>Is it correct?</p>
"
"0.209083349395727","0.203069233026724","126196","<p>I'm developing an app in C# (WPF) that amongst other things, it makes a time-series based forecast of sales (4-5 months into the future). I'm an industrial engineer so I'm not pro in statistics nor in programming (basic knowledge of both).</p>

<p>What I'm doing right now is to aggregate my daily data into monthly data, then I test for monthly seasonality, and then either go for a <strong>Holt</strong>'s exponential smoothing or for a <strong>Holt-Winters</strong>'s one depending on the result. </p>

<p>For determining the <strong>smoothing parameters</strong> I'm using <strong>brute force</strong> (i.e. testing a lot of possible combinations) and keeping the one that would have predict the past year (backtesting) with minimum <a href=""http://en.wikipedia.org/wiki/Mean_absolute_error"" rel=""nofollow"">MAE</a>.</p>

<p>A <strong>problem</strong> arises: this method is SLOW (obviously, as always with brute force). It takes about 0,5s only trying the smoothing parameters in 0.05 intervals which doesn't give much accuracy. I need to do this with 1000+ items so it goes over 8 minutes (too much).</p>

<p>So I have a few <strong>questions</strong>:</p>

<ul>
<li>Is there any method to determine optimal smoothing parameters without testing all of them?</li>
<li>Using <em>R.NET</em> to use the forecast package of R will be faster?</li>
<li><p>If so, should I:</p>

<ul>
<li>Use daily or monthly data?</li>
<li>Make also an auto.arima? How to determine which model is better?</li>
</ul></li>
<li><p>Is my method of backtesting (make a model only with data previous to that point) valid to determine if a model is better than another?</p></li>
</ul>

<p><strong><em>EDIT:</em></strong> I have tried implementing R.NET. Time for <code>ets</code> is about 0,1s if I set which model to use and use only mae as <code>opt.crit</code> (if not, it goes up to 5s). </p>

<p>This is good enough <strong>IF</strong> I could get the same out-of-sample predictions I mention in the comment. If it's not possible then I would have to run it 12 times, adding up to 1,2s which is not fast enough.</p>

<ul>
<li>How can I do that (get predictions over the last 12 data without considering them in the model) in R?</li>
</ul>
"
"0.085357919955264","0.124354000843452","131393","<p>I would like to have the best ARIMA model prediction that has the lowest MAPE or lowest AIC/BIC. For example, I would want to change the Arima order automatically with loop or some other way and want to test with all possible combinations like below</p>

<pre><code>c(1,0,0)
c(1,1,0)
.
.
c(x,y,z)
</code></pre>

<p>Below is the reproducible example code but I do not know how to go with multiple order execution and comparison of MAPE/AIC/BIC.</p>

<pre><code>set.seed(1)
tsdata &lt;- ts(rnorm(50), start = c(1980,1), frequency = 12)
myts &lt;- tsdata

fit &lt;- Arima(myts,order=c(2,1,0))
forecast(fit, 3)
plot(forecast(fit, 3))
fit
accuracy(fit)
</code></pre>

<p>Is it possible to save all the Accuracy measures (<code>MAPE, AIC, BIC</code>) in a data frame or in a list then select the best order to execute a Arima model? I tested with <code>auto.arima</code> in my real data but it did not give me the best order. Thanks in advance for your help !</p>
"
"0","0.0717958158617738","135651","<p>I've created an Arima model based on past forex closing prices using auto arima, which has generated a (0,1,0) ARIMA model.</p>

<pre><code>&gt; auto.arima(ma5)
Series: ma5 
ARIMA(0,1,0)                    

sigma^2 estimated as 5.506e-07:  log likelihood=11111.42
AIC=-22220.83   AICc=-22220.83   BIC=-22215.27
</code></pre>

<p>I next tried to plot the forecasted values, but as you can see all predictions are constant. Anyone know what I'm doing wrong?</p>

<p><img src=""http://i.stack.imgur.com/Er1k5.png"" alt=""enter image description here""></p>
"
"0.181071492085037","0.175863114528165","139164","<p>I'm trying to find out how to do forecasting with a mixture model (averaging the forecasts of an <code>ets</code>, an <code>arima</code> and an <code>stlf</code> model). I do not have a huge amount of statistics experience and so I'm struggling with finding out how to do it.</p>

<p>The point forecasts will just be the average of the point forecasts of the three methods, no problem.</p>

<p>The problem is how to calculate the prediction intervals. </p>

<p>I have found an R script with an attempt to do it, but the mixture prediction intervals are just calculated as an average of the prediction intervals of the models, and I am pretty sceptical about this approach - is it really that easy?</p>

<p>If not, how do I go about calculating them?</p>
"
"0.233762291106092","0.22703830459325","140163","<p>I am working on a small project where we are trying to predict the prices of commodities (Oil, Aluminium, Tin, etc.) for the next 6 months. I have 12 such variables to predict and I have data from Apr, 2008 - May, 2013.</p>

<p>How should I go about prediction? I have done the following:</p>

<ul>
<li>Imported data as a Timeseries dataset </li>
<li>All variable's seasonality tends to vary with Trend, so I am going to multiplicative model. </li>
<li>I took log of the variable to convert into additive model </li>
<li>For each variable decomposed the data using STL</li>
</ul>

<p>I am planning to use Holt Winters exponential smoothing, ARIMA and neural net to forecast. I split the data as training and testing (80, 20). Planning to choose the model with less MAE, MPE, MAPE and MASE.</p>

<p>Am I doing it right?</p>

<p>Also one question I had was, before passing to ARIMA or neural net should I smooth the data? If yes, using what? The data shows both Seasonality and trend.</p>

<p>EDIT:</p>

<p>Attaching the timeseries plot and data
<img src=""http://i.stack.imgur.com/V0wes.png"" alt=""enter image description here""></p>

<pre><code>Year  &lt;- c(2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 
           2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 
           2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 
           2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 
           2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 
           2012, 2012, 2013, 2013)
Month &lt;- c(4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
           12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 
           8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2) 
Coil  &lt;- c(44000, 44500, 42000, 45000, 42500, 41000, 39000, 35000, 34000, 
           29700, 29700, 29000, 30000, 30000, 31000, 31000, 33500, 33500, 
           33000, 31500, 34000, 35000, 35000, 36000, 38500, 38500, 35500, 
           33500, 34500, 36000, 35500, 34500, 35500, 38500, 44500, 40700, 
           40500, 39100, 39100, 39100, 38600, 39500, 39500, 38500, 39500, 
           40000, 40000, 40500, 41000, 41000, 41000, 40500, 40000, 39300, 
           39300, 39300, 39300, 39300, 39800)
coil &lt;- data.frame(Year = Year, Month = Month, Coil = Coil)
</code></pre>

<p><strong>EDIT 2:</strong>
One question, can you please tell me if my data has any seasonality or trend? And also please give me some tips on how to identify them.
<img src=""http://i.stack.imgur.com/Hg1yp.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/PdNwJ.png"" alt=""enter image description here""></p>
"
"0.128036879932896","0.124354000843452","143367","<p>I got a question about modeling time series in R.
my data consist of the following matrix:</p>

<pre><code>1   0.03333333 0.01111111 0.9555556
2   0.03810624 0.02309469 0.9387991
3   0.00000000 0.03846154 0.9615385
4   0.03776683 0.03119869 0.9310345
5   0.06606607 0.01201201 0.9219219
6   0.03900325 0.02058505 0.9404117
7   0.03125000 0.01562500 0.9531250
8   0.00000000 0.00000000 1.0000000
9   0.04927885 0.01802885 0.9326923
10  0.06106870 0.02290076 0.9160305
11  0.03846154 0.00000000 0.9615385
12  0.00000000 0.00000000 1.0000000
13  0.06028636 0.03843256 0.9012811
14  0.09646302 0.05144695 0.8520900
15  0.04444444 0.06666667 0.8888889
</code></pre>

<p>these matrix has in total 200 rows.</p>

<p>as you can see in each situation the sum of each row is 1, that becomes because the values are the percentage of a whole. for example row 1 contains 3.33% of variable a, 1.11% of variable 2 and 95.5% of veriable 3.
the first collomn indicates the year that the values are measured.</p>

<p>my target is to make a prediction for the next 5 years, so from year 200 to 205.</p>

<p>I can doe that by making three normal time series forecast. But for that forecast the total sum is never equal to 1, which is very important. 
Normaly is use techniques like arima and exponential smoothing.</p>

<p>Does somebody know a method to make a forecast for such a problem?</p>
"
"0.0739221270954573","0.0717958158617738","151739","<p>I am not even sure how to even phrase this question so if anyone could help that would be great.</p>

<p>I am analyzing facebook activity and I wish to predict a particular activity (comments, for instance). Doing this with R, and using the package 'forecast' my prediction for the future periods with 80 and 95% yield lower boundaries of negative numbers - this can't be. The minimum possible comments that any activity could have is 0. What can I do to restrict this with a valid statistical background instead of running some code to limit these values?</p>

<p>EDIT:</p>

<p>Here is a MRE, with data that very closely resembles the parameters of the data I am working with (which unfortunately I can't share), but in reality, anything that has a sd larger than a mean would yield similar results:</p>

<pre><code> library(forecast)
 TEST &lt;- rnorm(150,mean=87,sd=140)
 # change negatives to 0
 TEST[TEST &lt; 0] &lt;- 0
 #It's crucial that the sd is larger than the mean, if not, repeat first commands
 mean(TEST)
 sd(TEST)
 arima.TEST &lt;- arima(TEST, order = c(1,0,0))
 forecast.Arima(arima.TEST, h =12)
 #Lo 80 and Lo 95 are predicted in the negatives
</code></pre>
"
"0.128036879932896","0.124354000843452","153492","<p>I had 4 groups of data (in color 1 to 4) and one group is the data for one day, so I had 4 days of data. I was trying to fit a line which describes the pattern of theses lines (oscillating pattern) and do some prediction for the behavior on the fifth day. What I have in mind is lm(~poly(x,,)) or auto.arima(), which only needs one day of data. Is there any way to use all my data for fitting? What would be a more appropriate statistical model to use? Any help would be appreciated. Thanks.</p>

<p>Sophie 
<img src=""http://i.stack.imgur.com/eyV11.png"" alt=""enter image description here""></p>
"
"0.147844254190915","0.143591631723548","155305","<p>I have a large dataset with different factors that I want to forecast to the future. These forecasts I will then later on use as inputs for a Monte Carlo simulation. My idea would be to use arima forecasting on the different variables. Subsequently, I would use the resulting prediction interval as inputs for the Monte Carlo simulation.</p>

<p>Using R, (I think) I get what I want by using the following.</p>

<p>First, I set some parameters FC_years &lt;- 4 FC_boundaries &lt;- 68 # This would be 1 sd</p>

<p>Next, the forecasting is done by: <code>fit &lt;- auto.arima(POP) forecast &lt;- data.frame(forecast(fit,FC_years,level=FC_boundaries))</code></p>

<p>What is now very important for me in order to use these results for a MC simulation, is to know how R calculates the prediction interval (I have been searching for quite a while now, but I can't get a clear answer), and how this interval is distributed (I assume it is normal, but again, I can't get a clear answer).</p>

<p>Just as an example, the following dataset could be used:</p>

<pre><code>1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 68235 72498 76700 80326 83195 85447 87276 89004 90858 92894 94995 97015 98742 100031 100830 101219 101344 101418 101597 101932 102384 102911
</code></pre>

<p>Can anybody give me any hints?</p>
"
"0.104541674697863","0.101534616513362","157378","<p>My issue is really simple: I need to compute a seasonal arima model on traffic data (5 min frequency). The data exhibits daily seasonality (288 observations).</p>

<p>This is causing me issues in computing the model using R. (SO question: <a href=""http://stackoverflow.com/questions/30804281/r-arima-method-blocks-when-adding-seasonality"">http://stackoverflow.com/questions/30804281/r-arima-method-blocks-when-adding-seasonality</a>)</p>

<p>I know that seasonal arima models are not particulary suited for long seasonality periods, however I read tons of articles regarding traffic forecasting that exploits them in order to make predictions.</p>

<p>I will appreciate any suggestion regarding software tools (in addition to R) that would do the trick. Thank you. </p>
"
"0.128036879932896","0.124354000843452","158701","<p>I use the svm function (for regression) to make forecast like I would with for exemple the arima function:<br>
<code>fit&lt;-auto.arima(ts)</code><br>
<code>prediction&lt;-forecast(fit,h=20)</code><br>
which returns different attributes : </p>

<blockquote>
  <ol>
  <li><code>prediction$mean</code> which is the actual prediction  </li>
  <li><code>prediction$lower</code> and <code>prediction$upper</code> which are the   <strong>boundaries of the confidence intervals</strong> on each points of the   <code>prediction$mean</code>.  </li>
  </ol>
</blockquote>

<p>I would like the <code>svm</code> function (from <em>e1071</em> package) to return a more detailed answer than just the value (like the <code>forecast()</code> would).<br>
 But I guess it is not implemented in the function yet.
Is there another function to do it ? Or should I use <strong>bootstrap</strong> methods to try to estimate those boundaries? And if I should use this are they pre-implemented version of them instead of using sample over a for loop which is very time-consuming ?</p>
"
"0.110883190643186","0.143591631723548","161182","<p>I am trying to fit and forecast log returns of a price data using ARIMA model in R. For reproducibility, data is provided <a href=""https://docs.google.com/spreadsheets/d/1U619rL30yGcNRWxoiiIsfy-C-VOH2W2tnEivuIUOVq4/edit?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p><strong>Steps Followed, Code and Results obtained</strong> </p>

<ol>
<li><p>Check for outliers (Package: <code>forecast</code>) - No outliers detected. </p>

<pre><code>outliers &lt;- tsoutliers(log.rtn)
</code></pre></li>
<li><p>Stationarity Check using ADF test (Package: fUnitRoots) - Series found to be stationary</p>

<pre><code>stationary &lt;- adfTest(log.rtn, lags = m1$order, type = c(""c""))
</code></pre></li>
<li><p>Determination of p,d,q using ACF and PACF (Package: astsa) - Based on my understanding, p = 2, d = 0, q = 2</p>

<pre><code>acf2(log.rtn, lags = 20)
</code></pre></li>
<li><p>Fitting ARIMA (Package: forecast)</p>

<pre><code>fit &lt;- auto.arima(log.rtn, stepwise=FALSE, trace=TRUE, approximation=FALSE)
</code></pre>

<p>Model obtained : ARIMA(2,0,1)</p>

<pre><code>Series: log.rtn 

  ARIMA(2,0,1) with zero mean     

Coefficients:
          ar1     ar2     ma1
      -0.5705  0.1557  0.6025
s.e.   0.1549  0.0532  0.1519

sigma^2 estimated as 0.001086:  log likelihood=775.57
AIC=-1543.14   AICc=-1543.04   BIC=-1527.29
</code></pre></li>
<li><p>Prediction (Package:forecast)</p>

<pre><code>fcast &lt;- forecast(fit, n.ahead=5)
plot(fcast)

    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95
390   1.416920e-03 -0.04080849 0.04364233 -0.06316127 0.06599511
391   8.228924e-04 -0.04142414 0.04306993 -0.06378837 0.06543416
392  -2.488236e-04 -0.04289257 0.04239493 -0.06546681 0.06496917
393   2.700663e-04 -0.04248622 0.04302635 -0.06512003 0.06566016
394  -1.928045e-04 -0.04303250 0.04264690 -0.06571047 0.06532486
395   1.520366e-04 -0.04273465 0.04303872 -0.06543749 0.06574156
396  -1.167506e-04 -0.04303183 0.04279833 -0.06574971 0.06551621
397   9.027370e-05 -0.04284167 0.04302221 -0.06556846 0.06574901
398  -6.967566e-05 -0.04301167 0.04287232 -0.06574379 0.06560444
399   5.380284e-05 -0.04289419 0.04300179 -0.06562948 0.06573708
</code></pre></li>
</ol>

<p>I am quite confused why the model is predicting so badly.</p>
"
"0.209083349395727","0.203069233026724","164067","<p>For each day, I observe my variable, <code>y(t)</code>, for a period of 12 hours. In order to understand the data and make predictions, I want to put together these data and make a long timeseries data. Now, if I fit an <code>AR(1)</code> model to the data, or even do kalman filtering on data as explained later,it would mean that my first reading on a day depends on the last reading of the previous day, which is meaningless in my context. </p>

<p><strong>Question</strong> how can I stop the last value of previous day to influence the first observation of new day?</p>

<p>What I've done:</p>

<p>So I have put <code>NA</code> s of the same length of each observation data (12) in between any two days to preserve the seasonality. But I have trouble understanding the consequences of this:</p>

<ol>
<li>upon seeing these NA's, will <code>arima()</code> ignore the NA's, practically doing the same thing I'm trying to avoid?</li>
<li>will it try to interpolate the values, which again means constructing the same series?</li>
</ol>

<p>Now if it is represented in state-space format, the system would just update the state equation (interpolating 'NA's?). But after updating it for 12 times before reaching the next non-missing value, would the state vector be a reasonable value?</p>

<p>All the computations are done in R. </p>

<p><strong>EDIT, clarification:</strong> Assume for each day, I'm collecting data from 1 pm to 12 am. Ideally, for each day, what I want to get at is a model of type $y(t) = \phi y(t-1) + \beta y(t-12) + v(t)$. now If I have a continuous timeseries, then $y(13) = \phi y(12) + ...$ where $y(12)$ is the last value from previous day. Now, in my context, it is meaningless for data on 1pm to depend on 12 am value of last day, but it's reasonable for 3pm data to be dependent on 2 or 1pm, or on 3pm data from previous day(s). </p>

<p>Also assume that  for the first 2 hours of each day, I don't need to forecast them.</p>
"
"0.195579564679489","0.18995387394524","165004","<p>I'm trying to find the best fit line for this data below but no matter what I try, the fit line seems to never be able to account for the lower values as shown below.</p>

<p>The x-values are just dates from 1/1/2014 to 7/20/2015 (566 values), but I don't know how to give you guys the y-values. I have it in my Environment but I don't know how to give you that without copying and pasting from the Console output.</p>

<p><a href=""http://i.stack.imgur.com/KA2LC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KA2LC.png"" alt=""Data with sinusodial fit""></a></p>

<p>This is the code that I'm using to get that fit line:</p>

<pre><code>wb.loglik=function(theta,y,x,null=NA)
{
  a=theta[1]
  b=theta[2]
  c=theta[3]
  d=theta[4]

  if(!is.na(null))
  {
    d=null
  }

  s2=theta[5]
  n=length(y)
  return((-n/2)*log(s2)-1/(2*s2)*sum((y-(a+b*cos(2*pi*((x-c)/d))))^2))
}
result=optim(par=c(mean(wbbcf),sd(wbbcf),1,365.25,var(wbbcf)/2),
fn=wb.loglik,x=Time,y=wbbcf,control=list(fnscale=-1))
theta=result$par
theta
value=result$value
value
</code></pre>

<p>This is the code to get the plot above:</p>

<pre><code>plot(date,wbbcf,xlim=c(as.Date(""2014-01-01""),as.Date(""2015-07-
    20"")),ylim=range(c(-3.2,0)),xlab=""Date (1/1/2014 to
    7/20/2015)"",ylab=""Total Net with Storage (bcf)"",main=""Total Burn with
    Model"")
par(new=T)
curve(-0.9740582-0.7857229*cos(2*pi*(x-5.9582996)/385.1581090),1,566,
    ylim=range(c(-3.2,0)),col=""blue"",xlab="""",ylab="""",xaxt='n',yaxt='n')
</code></pre>

<p>What else can I do to generate a better fit line for this data? Also, if there's a good way to predict future data, I would appreciate help with that as well.</p>

<p>Sorry in advance, if I'm not giving enough information. Please feel free to ask for any information you need and I will promptly edit the post.</p>

<p>EDIT: I have added the work I did with ARIMA below.</p>

<p>I inputted the following code and got the following results:</p>

<pre><code>forecast::auto.arima(wbbcf)
fit.arima = arima(wbbcf,order=c(0,1,2))
pred.arima = predict(fit.arima,n.ahead=500)
plot(wbbcf,xlim=c(1,800),ylim=range(c(-3.2,1)))
lines(pred.arima$pred,col=""red"")
lines(pred.arima$pred+1.96*pred.arima$se,col=""blue"",lty=3)
lines(pred.arima$pred-1.96*pred.arima$se,col=""green"",lty=3)
</code></pre>

<p><a href=""http://i.stack.imgur.com/W8Hqt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/W8Hqt.png"" alt=""ARIMA predictions""></a></p>

<p>Here's the <code>auto.arima()</code> output:</p>

<pre><code>Series: wbbcf 
ARIMA(0,1,2)                    

Coefficients:
          ma1      ma2
      -0.3023  -0.3188
s.e.   0.0397   0.0396

sigma^2 estimated as 0.05788:  log likelihood=3.02
AIC=-0.04   AICc=0.01   BIC=12.98
</code></pre>

<p>Is something wrong with what I'm doing with ARIMA?</p>
"
"0.297530822208279","0.321080649533968","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.104541674697863","0.101534616513362","171618","<p>I'm having trouble changing what prediction/confidence intervals/bands are plotted when plotting a forecasted arima object generated.</p>

<blockquote>
  <p>model &lt;- auto.arima(x)</p>
  
  <p>forecast &lt;- forecast(model, h=29)</p>
  
  <p>plot(forecast)</p>
</blockquote>

<p>I see the call automagically chooses some standard intervals (maybe 90% and 95%?), but the documentation for forecast.Arima() and plot.forecast() don't seem to mention which they are and how to alter which are plotted. The closest I can see to it is in the plot.forecast() help page, you can change the colors the prediction intervals are shaded with.</p>

<p>Ideally, I'd like to see 70% and 90% bands on my 29 step-ahead forecast plot.</p>

<p>thanks!</p>
"
"0.197125672254553","0.215387447585321","174692","<p>I have around 10000 time series showing one particular metric over 5 hours. </p>

<p>I used <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">auto.arima function</a> </p>

<p>In my previous question, people suggested that I have to use auto.arima for each time series, hold off some of data points and test the prediction with my hold off points.</p>

<p>I am holding off 20% of data points (if you see sample out of 40 I will hold off 8) and then let auto.arima predict. Then I can compare generated 8 values with actual 8 values.
But is there a formal way to test accuracy in ARIMA model? Is my approach correcT?</p>

<p>Is there a prebuilt function to test the accuracy of Arima.</p>

<p>Here is the code, I can always use x as my time series.</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>

<p>Both have 40 points. I can hold off 20% of them (8) and compare after auto.arima predicts.   But is there a simpler way I can test accuracy?</p>
"
"0.362433046678868","0.366088266073499","177262","<p>I have daily data points of the number of sales, but I am not looking at historic data only. My system delivers a new data point every day and in the evening I want to predict the number of sales tomorrow. The sales are typically pretty constant, but they might increase or decrease from time to time. After a change they will be constant for another reasonable amount of time. </p>

<p>Furthermore there might be seasonality which increases the sales by a certain amount for the duration of the seasonal event. I cannot be sure of it, but there might be weekly, monthly or yearly (but no other than that) seasonality, a combination of those or no seasonality at all. (Public) holidays are not considered. </p>

<p>I am using R and was looking at arima and triple exponential smoothing (Holt-Winters) models to use in order to predict the number of sales for the next day. I have to predict the number of sales for tomorrow for around 1000 different data sets: that's why I can not look at all of them by hand, plot acf or pacf or fine tune the models manually. The 1000 data sets share all the characteristics described above.</p>

<p>The Holt-Winters models are working fine until the constant value changes. This somehow messes up the predicted data. The problem with arima models is the computation time which is much higher compared with Holt-Winters. For a data set with data points of two years it takes around 2 seconds. Since the system should work with data sets that hold data for at least 10 years and I have a thousand of those this might not be acceptable, but it might be possible with a reasonable amount of parallelization. I think I will not be able to use auto.arima since it takes simply too much time.</p>

<p>I have a few questions:</p>

<ol>
<li>Is it possible to choose the same arima model that seems to work fine for one or two of my data sets, that I tested manually, for all 1000 data sets if they share the same characteristics?</li>
<li>If I look for yearly seasonality the model catches weekly and monthly seasonality as well. Does it do that in a worse/better way than pure weekly or monthly models? Should I combine different models for different seasonalities in my case?</li>
<li>Which model would you recommend using: Is arima the right way? How would you determine the parameters for the model?</li>
<li>Is there any guide or best practice I can follow for such a problem where the data set grows every day and cannot be investigated manually?</li>
<li>In general, independent of my problem: Do I get more accurate results if I predict every day using a model for yearly seasonality combined to once every year with the same model?</li>
<li>Related to 5.: How long does it take until a change in the above explained constant data affects the predictions for tomorrow in a yearly/monthly/weekly model?</li>
</ol>

<p>If you have any other hints apart from direct answers I would be very pleased as well. </p>

<p>Thank you so much for your help.</p>

<p><strong>Update</strong>:
It seems that one reason for my problems to get accurate results is the huge difference of sales between non-seasonal and seasonal days. It might happen that the number of sales is around 1 on days which are not affected by any seasonal events and bigger than 1000 on seasonally affected days.</p>
"
"0.181071492085037","0.175863114528165","183944","<p>I have a weird problems with producing good forecasts with R and the forecast package.</p>

<p>The data I use can be found here: <a href=""http://pastebin.com/2gwyVYrj"" rel=""nofollow"">http://pastebin.com/2gwyVYrj</a></p>

<p>Sample code I used:</p>

<pre><code>library(forecast)
library(lubridate)
library(xts)

#TIME and VALUE fields (UNORDERED and few missing values). TIME is in YYYYMMDDHH format as character
data &lt;- read.csv(""data.csv"", sep=""|"", header=TRUE)

#Gets the values
vals &lt;- data$VALUE

#Parses time to POSIXct format
time &lt;- parse_date_time(data$TIME, orders=""ymdh"")

#Create time series
timeSeries &lt;- xts(vals,time)

#Fit ets model to data

model &lt;- ets(timeSeries)

#I want 168h step ahead prediction so

forecasted_values &lt;- forecast(model,h=168)

#My goal is to write the predicted values back to file so I need the      

#corresponding time stamps for the prediction

#I found this method somewhat works last timestamp (max + 3600) to 168h ahead (max + 3600*168)

prediction_time &lt;- seq(from=(parse_date_time(max(subset_data$TIME_ID), orders=""ymdh"")+3600),
                         to=(parse_date_time(max(subset_data$TIME_ID), orders = ""ymdh"")+3600*168),
                     by=""hour"")
</code></pre>

<p>I think the predicted values are in <code>forecasted_values$mean</code> because that has the correct length (168 as I wanted). But it has a constant value... </p>

<p>Any ideas what went wrong and how could I get new dataframe with correct forecasted values and their corresponding (future) timestamps in the same ""yyymmddhh"" format?</p>

<p>EDIT:</p>

<p>Changing the model to ARIMA didn't help either.</p>

<pre><code>model &lt;- auto.arima(timeSeries)

#168h prediction

forecasted_values &lt;- forecast(model,h=168)

#not a valid forecast!
plot(forecasted_values$mean)
</code></pre>
"
"0.210386061995483","0.22703830459325","184880","<p>So I wanted to generate $500$ data points from an $ARMA(1,1)$ distribution in R, use the first $400$ as my training data and use the training data and the <code>predict</code> function to both see if I could obtain the correct model via AIC and then plot my prediction. I wanted to generate a whole bunch of different $ARMA$ models to use for the AIC which is why I basically built a grid, however I get a bunch of warnings and errors with my current method, despite the fact it seems to work. Note it is the estimation and not the actual prediction that raises the problems. Can I use a grid like so to produce AICs for various model types?</p>

<pre><code>####### ARMA(1,1) ############
# Causal stationary - root of the function phi(z) &gt; 1
theta1&lt;- 0.5
phi1&lt;- 0.6

arma11&lt;- arima.sim(n = 500,list(ar = c(phi1), ma = c(theta1)), sd = sqrt(1))
tr.data&lt;-arma11[1:400]
AIC&lt;-c()
for (i in 0:3){
  for (j in 0:3){
    aic.ij&lt;- arima(tr.data,order = c(i,0,j))$aic
    AIC&lt;- c(AIC,aic.ij) 
  }
}
AIC&lt;- matrix(AIC, ncol=4, byrow =T)
colnames(AIC)&lt;- c(""q=0"",""q=1"",""q=2"",""q=3"")
rownames(AIC)&lt;- c(""p=0"",""p=1"",""p=2"",""p=3"")
AIC
index&lt;-which(AIC==min(AIC),arr.ind=T)
index
prd&lt;-predict(arima(tr.data, order = c(index[1]-1,0,index[2]-1)),n.ahead =100)
vld.data&lt;-arma11[401:500]

# plot the training data with the next 100 predictions and the 95% confidence intervals
plot.ts(arma11, xlim=c(0,500),ylim=c(floor(min(arma11)),ceiling(max(arma11)))) 
lines(prd$pred, col='blue')
lines(prd$pred-(1.96*prd$se), col='red')
lines(prd$pred+(1.96*prd$se), col='red')
</code></pre>

<p>For this particular $ARMA$ I am error free but receive various size mismatch errors with other models under the same method, I receive this warning though. </p>

<pre><code>Warning message:
In arima(tr.data, order = c(i, 0, j)): possible convergence problem: optim gave code = 1Warning message:
In arima(tr.data, order = c(i, 0, j)): possible convergence problem: optim gave code = 1         q=0      q=1      q=2      q=3
p=0 1608.907 1296.137 1207.432 1187.231
p=1 1244.182 1178.820 1180.004 1181.993
p=2 1190.716 1180.072 1182.002 1183.776
p=3 1179.731 1181.688 1183.767 1181.305
    row col
p=1   2   2
</code></pre>
"
"0.205023092026182","0.258863495453396","186164","<p>I am new to time series and am trying to fit some time series data.</p>

<p>I understand the general concept of ARIMA model. However, as I read more textbooks and articles from Rob Hyndman, I realized I could put some regressors using the <code>xreg</code> argument for the functions <code>auto.arima</code> or <code>arima</code> in R to get an ARMAX model. Therefore, I wonder if it is still necessary to include seasonality in <code>ts(...,frequency)</code> as everything can be specified as dummy variable within the <code>xreg</code> matrix and a more complicated seasonality structure (e.g. monthly seasonality) can be specified. </p>

<p>In addition, what would be a good way to check the accuracy of the forecast? I am fitting multiple time series data with a hierarchical structure. Using <code>auto.arima</code>, I am able to select the best model and validate the model by looking at the residuals (check whether they are white noise). However, is there a way to even improve on the model if the prediction is still far from the actual data?</p>

<p>To sum up, </p>

<ol>
<li>Is the <code>frequency</code> argument in <code>ts</code> function really necessary?  Can I just specify everything in the <code>xreg</code> matrix?</li>
<li>What would be a normal routine to improve on model after selecting the appropriate ARIMA model with the lowest AIC?</li>
</ol>

<p>Updates (Dec 17):</p>

<p>I am now able to fit an ARIMA model with SARIMA error by specifying <code>xreg</code> argument and <code>seasonal=F</code>. One issue that I have with that is, my <code>xreg</code> matrix is not invertible (I assumed) and its not due to the presence of intercept term. Thus <code>auto.arima()</code> only fit a <code>c(0,0,0)</code> model.</p>

<p>I then tried using <code>Arima()</code> to manually select model and it outputted the following error</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
non-finite value supplied by optim
</code></pre>

<p>I check the <code>xreg</code> matrix and it turns out column 48 (Day) and column 52 (2015) is causing the issue. Could you check if there's something wrong with my <a href=""https://drive.google.com/file/d/0B-b9YsAB5mpnam1oN0hYcFRwLXM/view?usp=sharing"" rel=""nofollow"">matrix structure</a> ? </p>

<p>If you think this additional updates should be asked in stack overflow or additional question, I will move it.</p>
"
"0.165294901226822","0.160540324766984","186265","<p>I am currently working on some research and we are trying to do some Time-Series prediction using neural networks. To get started, I was using the paper published by G. Peter Zhang (<a href=""http://cs.uni-muenster.de/Professoren/Lippe/diplomarbeiten/html/eisenbach/Untersuchte%20Artikel/Zhan03.pdf"" rel=""nofollow"">Time Series forcasting using a hybrid ARIMA and NN model</a>) since I am no expert in either R or statistics, I could really do with some help. </p>

<p>I got R and the neuralnet lib setup and then took the Lynx dataset, then created a data-frame with the data long with the lags to set as input. My data now looks something like this (this is only for t, t-1, and t-2 lags) </p>

<pre><code>     x     x1    x2
1   269    NA    NA
2   321   269    NA
3   585   321    269
</code></pre>

<p>Now I want to train a NN with input x1 and x2 and get output at x.</p>

<p>I do the training with the following code </p>

<pre><code>nn &lt;- neuralnet(x~x1+x2, data=dat, hidden = 2, linear.output = T) # I am using t-1 ... t-4 so using hidden layer of 2
</code></pre>

<p>This does train the model, but the error is really high, and when I use it to do any computation the results of the second layer neuron is alway 1. I was discussing with some freinds and they said that its because I am maybe using the wrong activation function. I looked in the help for the act.fct and tried with both <code>logistic</code> and <code>tanh</code> but the results remain the same. </p>

<p>I have been stuck on this for a few days now, so could really use some help. May I am doing something wrong? Or missing something? </p>

<p>Thanks</p>
"
"0.234734279876976","0.248708001686903","192739","<p>I have been working with the forecast package in R a lot, recently. And my question might seem trivial (or not, maybe I'm missing something), but for the life of me I can't seem to find a way to fit an Arima model with exogenous variables (<code>xreg</code> argument) that has been computed by the <code>auto.arima</code> function to previously unseen test data.</p>

<p>So, I'm basically trying to do the following:</p>

<pre><code>library(forecast)
fit &lt;- auto.arima(trainingdata, xreg = trainingvariables)
</code></pre>

<p>...and then I would like to ""apply"" the model to new test data, for which I also have new exogenous variables available. I can see the following methods:</p>

<pre><code>fitted(fit)
</code></pre>

<p>That returns one-step in-sample forecasts, so, in effect, that's exactly what I want. Except that it's in-sample. However, I would like to calculate <strong>one-step out-of-sample forecasts</strong> (with <strong>new exogenous variables</strong> that I have available). Another method:</p>

<pre><code>forecast(fit, xreg = newvariables, h = ...)
</code></pre>

<p>That works for exactly one step, but then seems to merely forecast the trainingdata stored in the model fit. But I don't think I can use new testdata here? (So, I can't use this method for testing one-step prediction accuracy.) One more idea:</p>

<pre><code>fit2 &lt;- Arima(testdata, model = fit)
</code></pre>

<p>According to the manual, if the <code>model</code> parameter is used, ""this same model is
fitted to [testdata] without re-estimating any parameters"". Great, but I don't think I can supply any new exogenous variables, can I?</p>

<p>I really think, I must be missing something simple. Any help would be much appreciated.</p>
"
"0","0.0717958158617738","193550","<p>How can we decide the size or portion of the data given to get the ARIMA that has the best forecasting properties?</p>

<p>I mean, for example, we have a hourly series with over 28.000 elements.</p>

<p>Which is the criteria that tells us: do ARIMA over last 100 elements, or 250 last elements, so the ARIMA we get is better for forecasting?
I am interested in short time prediction, like for 24 hours.</p>

<p>I read everywhere but found no criterion yet.</p>
"
"0.240246913060236","0.287183263447095","198844","<p>I'm trying to understand how <code>auto.arima</code> with covariates in the xreg parameter works. I'm familiar with regression and I'm starting to work on forecasting.</p>

<p>My understanding of forecasting is that you look for patterns in the past time series and then project those paterns onto the future.  </p>

<p>My uderstanding of regression is that you use predictors to try to generate an output value and minimize the difference between your created value and the real value.  </p>

<p>So how does forecasting <code>auto.arima</code> with <code>xreg</code> work? Do you create a forecast for a timeseries based on past data and regression model based on the input time series and input <code>xreg</code>, and then forecast each data point in the time series and for each forecasted data point use the regression model you built and future <code>xreg</code> values to adjust the forecasted values?</p>

<p>I'm a former physics grad student, so I'm not allergic to math but I'm just looking for a high level overview of the process here to understand how forecasting <code>auto.arima</code> works.  </p>

<p>For example like, </p>

<ul>
<li><p>step 1: build forecast model on input time series, and regression model on input time series and input <code>xreg</code> values</p></li>
<li><p>step 2: forecast model into future one step, and predict value with regression model and future <code>xreg</code> values</p></li>
<li><p>step 3: algorithm combines forecasted value and regression model prediction to get combined value</p></li>
</ul>

<p>This is just a guess at how it works, but it's an example of the kind of high level explanation I'm looking for.</p>

<p>I've included some code below that I've been working on trying to forecast time in to out <code>TiTo</code> for customers at a restaurant with predictor count of customers in the restaurant <code>CustCount</code>.</p>

<pre><code>OV&lt;-zoo(SampleData$TiTo, 
    order.by=SampleData$DateTime)


eDate &lt;- ts(OV, frequency = 24)

Train &lt;-eDate[1:15000]
Test &lt;- eDate[15001:22773]

xregTrain &lt;- SampleData[1:15000,]$CustCount
    xregTest &lt;- SampleData[15001:22773,]$CustCount

Arima.fit &lt;- auto.arima(Train, xreg = xregTrain)

Acast&lt;-forecast(Arima.fit, h=7772, xreg = xregTest)

accuracy(Acast$mean,Test)
</code></pre>
"
"0.182947930721261","0.203069233026724","204763","<p>Using linear regression as an equation for prediction is straightforward with,</p>

<p>$$ Y_i = \beta_0 + \beta_1 X_i. $$</p>

<p>Once the betas are estimated I can insert different values of $X$ to use as a what-if analysis for different scenarios. </p>

<p>But trying to do the same with ARIMA models is proving difficult to translate. For example with an ARIMA(2,1,1) model, how do I create an equation where I can try out different scenarios to see how the projection changes? </p>

<p>Below I have the output for a projection of sales based on past sales and extra regressors. I see that a unit change in <code>poc0_3_PER</code> results in a <code>135.2229</code> change in sales. But how do I account for the moving average and auto-regression components?</p>

<pre><code>arima(ts.count, order=c(2,1,1), xreg=df.back[3:4])

Call:
arima(x = ts.count, order = c(2, 1, 1), xreg = df.back[3:4])

Coefficients:
          ar1     ar2     ma1  poc0_3_PER
      -0.4569  0.2458  0.9455    135.2229
</code></pre>

<p>I have <code>ar1</code> and <code>ar2</code> estimates along with <code>ma1</code> and the extra regressors. How do I convert this into a working equation wherein I can try out different scenarios for the extra regressors to see how the prediction is affected?</p>

<p>I'm hoping that the solution is not an equation like <a href=""http://stats.stackexchange.com/questions/69407/how-do-i-write-a-mathematical-equation-for-arima-2-1-0-x-0-2-2-period-12?rq=1"">this post here</a>. I do have SARIMA models at times with orders like <code>SARIMA(2,0,1)(1,0,1)[12]</code>.</p>
"
"0.104541674697863","0.101534616513362","207473","<p>I am new to R and statistics. I have a problem related to the prediction:
I am not able to plot the real value together with the predicted value. 
PROBLEM: I want to feed first 16 values into the ARIMA and then I want ARIMA should predict the next 3 values. I used both forecast and predict function but not sure which one is good for my case (please tell me).
After prediction, I only can plot the green line which is the prediction values but not able to add the real values in the same line.</p>

<p>Bonus: How to get the MAPE error measure when I use predict function in R.</p>

<p>Thanks..</p>

<p>SAMPLE CODE:</p>

<pre><code>x=file$Cost
    k&lt;-auto.arima(x[1:16]) 
    m=forecast(k,h=3) ## I tried both of them
    m=predict(k, n.ahead = 3)
    j=(m$fitted)
a=j[17:19]
b=x[17:19]
plot(a, col=""green"",type=""l"") # predicted
lines(b) # real
summary(m)
</code></pre>

<p>SAMPLE DATA:</p>

<pre><code>Timestamp   Cost
2010-09-21T00:00:00+00:00   5
2010-09-21T00:01:00+00:00   6
2010-09-21T00:02:00+00:00   6
2010-09-21T00:03:00+00:00   6
2010-09-21T00:04:00+00:00   6
2010-09-21T00:05:00+00:00   6
2010-09-21T00:06:00+00:00   6
2010-09-21T00:07:00+00:00   5
2010-09-21T00:08:00+00:00   6
2010-09-21T00:09:00+00:00   5
2010-09-21T01:10:00+00:00   5
2010-09-21T01:11:00+00:00   6
2010-09-21T01:12:00+00:00   6 
2010-09-21T01:13:00+00:00   6
2010-09-21T01:14:00+00:00   6
2010-09-21T01:15:00+00:00   6
2010-09-21T01:16:00+00:00   6
2010-09-21T01:17:00+00:00   5
2010-09-21T01:18:00+00:00   6
</code></pre>
"
"NaN","NaN","207987","<p>I have fit an ARIMA model to a time series with function <code>auto.arima</code> from ""forecast"" package in R. I wanted to check prediction intervals for robustness by changing the ARIMA terms. </p>

<p>Here is my R code:</p>

<pre><code>library(""forecast"", lib.loc=""~/R/win-library/3.2"")
library(""tseries"", lib.loc=""~/R/win-library/3.2"")

price = c(256, 223, 190, 170 ,140, 123, 133, 133, 125, 120, 125, 140, 166, 186, 206, 206, 206, 206, 206, 206,
       229, 263, 273, 273 ,273 ,273 ,258, 239, 233, 226, 226, 226, 249, 249, 249, 249, 249, 269, 279, 279,
       279, 279, 299, 316, 316, 316, 316, 316, 316, 316, 299, 299, 299 ,319, 319, 339 ,339, 356 ,356, 356,
       343, 343, 333 ,343 ,442 ,599, 599, 599, 599, 549, 516, 336, 336, 336, 309, 309 ,319, 565, 665, 832,
       832, 698, 632, 532, 499, 526, 526, 526, 526, 499, 466, 333 ,233, 233, 216, 200, 200, 200, 226, 239,
       279, 316, 333 ,366 ,366 ,366, 366 ,366 ,333 ,349 ,349, 349 ,359 ,359, 442 ,459 ,449 ,449, 449, 449,
       449, 449 ,449 ,459, 459 ,459, 459, 459, 446, 446, 446, 446, 459, 459, 439, 439, 439, 439, 482, 482,
       482, 482 ,516,516, 532, 532, 532 ,532 ,532 ,549, 599, 632 ,632 ,632, 632, 599 ,565 ,532, 482, 482,
       482, 482, 499 ,475 ,449, 416)

ts.plot(price)

auto.arima(price)

arima.fit&lt;-Arima(price, c(2,1,4), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))

arima.fit&lt;-Arima(price, c(2,1,3), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))
</code></pre>

<p>What I saw surprised me quite a bit:</p>

<p><a href=""http://i.stack.imgur.com/SHPAE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SHPAE.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/9pNVK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pNVK.jpg"" alt=""enter image description here""></a></p>

<p>Why do the prediction intervals widen in the MA(3) case and hardly so in the MA(4) case? </p>
"
"0.256073759865792","0.248708001686903","209426","<p>After reading <a href=""http://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/"" rel=""nofollow"">this blog post</a> about Bayesian structural time series models, I wanted to look at implementing this in the context of a problem I'd previously used ARIMA for.</p>

<p>I have some data with some known (but noisy) seasonal components - there are definitely an annual, monthly and weekly components to this, and also some effects due to special days (such as federal or religious holidays).</p>

<p>I have used the <code>bsts</code> package to implement this and as far as I can tell I haven't done anything wrong, although the components and prediction simply don't look as I'd expect. It isn't clear to me if my implementation is wrong, incomplete or has some other problem.</p>

<p>The full time series looks like this: </p>

<p><a href=""http://i.stack.imgur.com/Oy1ci.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oy1ci.png"" alt=""Full data""></a></p>

<p>I can train the model on some subset of the data, and the model generally looks good in terms of the fit (plot is below). The code I am using to do this is here:</p>

<pre><code>library(bsts)

predict_length = 90
training_cut_date &lt;- '2015-05-01'
test_cut_date &lt;- as.Date(training_cut_date) + predict_length

df = read.csv('input.tsv', sep ='\t')

df$date &lt;- as.Date(as.character(df$date),format=""%Y-%m-%d"")
df_train = df[df$date &lt; training_cut_date,]

yts &lt;- xts(log10(df_train$count), order.by=df_train$date)

ss &lt;- AddLocalLinearTrend(list(), yts)
ss &lt;- AddSeasonal(ss, yts, nseasons = 7)
ss &lt;- AddSeasonal(ss, yts, nseasons = 12)
ss &lt;- AddNamedHolidays(ss, named.holidays = NamedHolidays(), yts)

model &lt;- bsts(yts, state.specification = ss, niter = 500, seed=2016)
</code></pre>

<p>The model looks reasonable:</p>

<p><a href=""http://i.stack.imgur.com/kinlf.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kinlf.png"" alt=""Model Plot""></a></p>

<p>But if I plot the prediction then firstly the trend is completely wrong, and secondly the uncertainty grows VERY quickly - to the point where I can't show the uncertainty band on the same plot as the predictions without making the y axis on a log-scale.  The code for this part is here:</p>

<pre><code>burn &lt;- SuggestBurn(0.1, model)
pred &lt;- predict(model, horizon = predict_length, burn = burn, quantiles = c(.025, .975))
</code></pre>

<p>The pure prediction looks like this:</p>

<p><a href=""http://i.stack.imgur.com/PQl7t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PQl7t.png"" alt=""pure prediction""></a></p>

<p>And then when scaled back to the initial distribution (with the dotted line showing the transition from training to prediction, the problems are obvious:</p>

<p><a href=""http://i.stack.imgur.com/YolqM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YolqM.png"" alt=""full distro""></a></p>

<p>I have tried adding more seasonal trends, removing seasonal trends, adding an AR term, changing the AddLocalLinearModel to AddGeneralizedLocalLinearTrend and several other things concerning tweaking the model, but nothing has resolved the issues and made the predictions more meaningful. In some cases the direction changes, so rather than dropping to 0 the prediction just continues to increase as a function of time. I definitely don't understand why the model is breaking down in this way. Any suggestions would be very welcome.  </p>
"
"0.221766381286372","0.215387447585321","209874","<p>I have a model fitted with <code>auto.arima</code>, the model is ARIMA(0,1,0)x(0,1,0)[6] with seasonal period 6. The data is bi-monthly so there is an annual seasonality. There is only one regressor indicating an intervention (dummy). </p>

<p>Then I used this model to old data to see what would have happened if the intervention would have done since and earlier period, using the model and forecast from an earlier data. <strong>The thing I do not understand yet</strong> is that if I suppose the intervention only occur in one period, the series only differ in this period. Therefore, there is no persistence on the intervention.</p>

<p>As I understand, the model has ARIMA errors. The error in the intervention period should change and so there should be an effect in the next periods when using forecast to predict futures values. If the intervention occurs in only one period, <strong>why</strong> in the forecast the intervention does not affect futures predictions?</p>

<hr>

<p>EDIT:</p>

<p>The code I am using is</p>

<pre><code>model1&lt;-auto.arima(ts,xreg = X.ts)
</code></pre>

<p>Where <code>X.ts</code> is a <code>ts</code> object with <code>0</code> and a period with intervention. </p>

<p>Then I used </p>

<pre><code>model2&lt;-Arima(Xold, xreg= X.ts.old, model=model1)
</code></pre>

<p>So I used the first model on earlier data to make the following</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>So I am trying to show what would have been expected from an earlier period (the forecast) if the intervention would have started earlier.</p>

<p>The thing I do not understand yet is that for instance</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,0...))
forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>only differ in the periods the <code>xreg</code> differ, with no persistence of these differences. I did not expect this, <strong>why is that?</strong></p>
"
"0.147844254190915","0.143591631723548","211022","<p>I have 288 data points of the Wolf's sunspot data for the years 1700 to 1987. 
I need to predict one step ahead forecasts for a forecast horizon of 25.
I kept the last 25 data points of the time series to test against the predictions. </p>

<p>Will fitting an Arima to (288-25 =) 263 data points like suggested here <a href=""http://stats.stackexchange.com/questions/55168/one-step-ahead-forecast-with-new-data-collected-sequentially?rq=1"">One step ahead forecast with new data collected sequentially</a> work?
Or do I need to iteratively increase the size of training data by 1 and then predict the next value?</p>
"
"0.295688508381829","0.287183263447095","211079","<p>This question is also linked to <a href=""http://stats.stackexchange.com/questions/209790/how-to-detect-a-relatively-small-level-shiftleakage-in-an-hourly-water-flux-ti"">How to detect a relatively small level shift(leakage) in an hourly water flux time series in an area?</a> which I asked a week ago...</p>

<h3>Background</h3>

<p>I've got a series of water flux data among about four month. The data is hourly collected, and I'm trying to develop an approach to justify whether there is a leakage or not. In the end, I want to implement my approach in R.<br>
As the other post mentioned, since I'm kind of new to the field of time series analysis, I've already tried some approaches or black boxes to solve the problem, but the result seems not good. And in the course of digging deep into the problem, I start to suspect the approaches I used.<br>
<strong>So I'm here to ask for help, is there any advices/procedures/references which I could refer to?</strong></p>

<h3>What I've tried</h3>

<ol>
<li>I used ARIMA,ETS,TBATS,STLM models provided by the forecests package to directly get a model to predict, and I try to using the model to predict, and then compare the prediction with the test value. The accuracy test showed it's not a good idea. Since non of the prediction is better than the snaive, which also got a MASE > 1.  </li>
<li>I detected the single outliers, and replaced it with a moving average of nearest 7 days. Then using the tsoutliers::tso with the only type of ""LS"", but the outcomes even failed at the manually modificated data I created.  </li>
<li>I seperated the data at zero and other points(totally 24 groups), and within each group I used the same idea as the first to find a model, make a prediction, and then check the residuals in series. This time with the cross validation, results showed an ARIMA model fits the best averagely. But then I got lost the model I got returns an intercept which is constantly increasing. Definitely, there exists other effects such as temperature which could interpret this variation. This somehow leads me crestfallen, since I've only got four month data, without comparing with the last year data, <strong>is it possible to estimate the appropriate coefficient with the temperature, and solve the problem in the meantime???</strong>   

<blockquote>
  <p>mod_arima  0.369<br>
  naive     0.725<br>
  mod_exp   0.891<br>
  mod_stl   0.913<br>
  mod_tbats 1.067  </p>
</blockquote></li>
</ol>

<h3>Characteristics of the data</h3>

<p>I think there are some characteristics to help you have a better understanding of my data.  </p>

<ol>
<li><p>In my view, the leackage I want to detect is relatively small(I doubt, there's only about 5%/10% of the mean value).  </p>

<blockquote>
  <p>train_h &lt;- ts(data_h$Navigator[1393:(1392+14*24)], frequency = 24)<br>
  excess &lt;- ts(c(rep(0,279),rep(mean(train_h,na.rm = T)*0.1,57)),frequency = 24)<br>
  plot(train_h)<br>
  lines(train_h+excess,col = ""red"")<br>
  <a href=""http://i.stack.imgur.com/Qcm3g.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qcm3g.jpg"" alt=""enter image description here""></a>
  As you could see there isn't so much difference...</p>
</blockquote></li>
<li><p>There's a huge calendar effect in my data during the Chinese New Year, So all the approaches I explored above only used the time span I think could ignore this effect.  </p>

<blockquote>
  <p>tsdata_d &lt;- ts(data_d$Navigator[1:114],frequency = 7)<br>
  plot(tsdata_d)
  <a href=""http://i.stack.imgur.com/gl6BY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gl6BY.jpg"" alt=""enter image description here""></a>
  And I used the second half.</p>
</blockquote></li>
<li><p>There are other effect could not be interpreted simply by the two-weeks or one-month prediciton model, but it's crutial. I personally think it's the varied temperature along the whole year.</p></li>
</ol>
"
"NaN","NaN","211670","<p>I want to make predictions use ARIMA in forecast package. I find that basically the prediction is just a lag of the actuals. Is there any way that I can better fit the model or any other approach available? (I find the ARIMA parameters through function <code>auto.arima</code> in ""forecast"" package in R.)</p>

<pre><code>Model and Plot:
        fit &lt;- arima(dataf$actuals, xreg=dataf$regressor,order=c(0,1,0))
        labDates &lt;- seq(as.Date(""2016-01-01"", format = ""%Y-%m-%d""),as.Date(""2016-01-16"", format = ""%Y-%m-%d""),by = ""day"")
        plot(labDates, dataf$actuals,col=""red"",type='l')
        lines(labDates,fitted(fit),col=""blue"")
        legend('topleft',c(""Actual Number"",""Predicted Number""),pch=c(20,20,20),col=c(""red"",""blue""))

Data:
    actuals&lt;-c(26952, 38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738,73834, 82813) 
    actuals_next&lt;-c(38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738, 73834,82813,NA)  
    regressor&lt;-c(519020, 671049 ,501083 ,288259 ,290899 ,260817, 276166, 274859 ,279405, 286689, 234050,95562,15138,  16401,  27145,  53717)    
    dataf&lt;-as.data.frame(cbind(actuals, actuals_next, regressor))
</code></pre>
"
"0.26893123948668","0.296021732275489","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.195579564679489","0.18995387394524","218525","<p>Let say that one wants to fit a model to a daily financial time series for prediction (e.g. ARIMA, SVM). If data are stationary, ideally the longer the time series, the better. In practice, I don't feel comfortable in blindly trusting stationarity tests (e.g. KPSS, ADF). For example, a 90% KPSS and ADF confirm that the following time series is stationary when it qualitatively doesn't seem to be homoscedastic.
<a href=""http://i.stack.imgur.com/Qv8x2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qv8x2.png"" alt=""enter image description here""></a>
Which quantitative methods exist to identify a reasonable starting date of the time series in terms of quality of the prediction (i.e. minimum test error, low variance of the prediction)? Please refer to R packages when possible.</p>

<p>My attempts:</p>

<p>(i) A brute force approach could consist in repeating the fitting for any length of the time series of interest (e.g. 1y, 1y+1d, ..., 5y). Anyway, this approach is too expensive.</p>

<p>(ii) Perform stationarity tests (ADF, KPSS) to the time series of minimum allowed length and extend the length until the tests reject the stationarity. The problem of this approach are multiple:
  (a) extremely dependent to the confidence of the test (e.g. 95% or 80%).
  (b) stationarity tests are not able to identify change of regime that may occurs for long financial time series. </p>

<p>Strictly related topic, but it doesn't provides automatic/quantitative procedures:
<a href=""http://stats.stackexchange.com/questions/188868/length-of-time-series-for-forecasting-modeling"">Length of Time-Series for Forecasting Modeling</a></p>

<p>EDIT (2/Jul/2016): After further thoughts, I think that an optimal approach could be to follow the principle ""the larger the dataset, the better"". After all, a model that is highly dependent on the length of the time series I guess that it could be considered a ""bad"" model. Rather than focusing on the selection of an optimal length, one could focus on the identification of features that are able to work well under different regimes of the time series.</p>
"
"0.0522708373489317","0.101534616513362","220973","<p><a href=""http://i.stack.imgur.com/27CVA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/27CVA.png"" alt=""enter image description here""></a>I'm using the R function <code>auto.arima</code> to fit an arima model for a time series, 
the result is an ARIMA(2,1,1). After that I apply the <code>forecast</code> function to predict some futur values. My question is Should I do the transformation (""un-differentiate"" the predicted values) or is it done by <code>forecast</code> automatically ? 
edit : here is what i get when i execute the code : </p>

<pre><code>arimaf = auto.arima(timeseries)
pred = forecast(arimaf, h = 10)
plot(pred, main = ""PREDICTION USING ARIMA(2,1,1)"")
</code></pre>
"
"0.304788738084904","0.296021732275489","221028","<p>I have a dataset containing the prevalence rate of Malaria in Botswana, starting in 1990 and ending in 2014. My task is to verify whether these data can be used in order to make predictions on the future Malaria prevalance rate. I know that 24 data points is probably not enough, but I decided to give it a try.</p>

<pre><code>#plot data
Mal.TS &lt;- ts(Mal$Value, start=1990, end=2014, freq=1) 
plot.ts(Mal.TS)
</code></pre>

<p><a href=""http://i.stack.imgur.com/KJkE7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KJkE7.png"" alt=""enter image description here""></a></p>

<p>Now it would have been nice if there was a completely increasing/decreasing trend, but unfortunately the trend was first increasing and later decreasing.</p>

<pre><code>#Test for stationarity
adf.test(Mal.TS)
</code></pre>

<p><a href=""http://i.stack.imgur.com/NDrI5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NDrI5.png"" alt=""enter image description here""></a></p>

<p>Because of the first increasing and later decreasing trend and the absence of variance as there is only one data point per year in a limited dataset, the Dickey-Fuller test suggests that the data are stationary.</p>

<pre><code>#Test AFC and PACF
acf(Mal.TS)
pacf(Mal.TS)
</code></pre>

<p><a href=""http://i.stack.imgur.com/NltIG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NltIG.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/6p9TP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6p9TP.png"" alt=""enter image description here""></a></p>

<pre><code>#Do  arima
fit &lt;- arima(Mal.TS, order=c(2,0,1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/04331.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/04331.png"" alt=""enter image description here""></a></p>

<pre><code>#Predict
pred &lt;- predict(fit, n.ahead = 5)
ts.plot(Mal.TS,pred$pred, lty = c(1,3))
</code></pre>

<p><a href=""http://i.stack.imgur.com/qmOgu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qmOgu.png"" alt=""enter image description here""></a></p>

<p>I know this is a poor analysis because of the lack of datapoints. However, I would like some suggestions on how to interpret the results that I found and reasons why this cannot work. The plot shows that the Malaria Prevalence in Botswana has been decreasing over the last 10 years, so one would expect the data to suggest that in the future the prevalence will keep decreasing. Yet, the model predicts the prevalance to increase. Why is this?</p>

<p>One way to possibly address this contra-intuitive result is by adding exogeneous data using the <code>xreg</code> argument. If I could include time series data on GDP, Health care expenditures,... which probably correlate with the Malaria prevalence data and thus also show a decreasing trend, I might be able to predict the future prevalence to be decreasing. Is this correct?</p>

<p>If the task of predicting the future malaria prevalence rate using the 24 earlier data points is not possible, could you give a clear reasoning why this is the case?</p>

<p>So in short I have following questions:</p>

<ol>
<li>How come the arima model predicts the prevalance rate to increase in the future</li>
<li>Can I cause the predictions to be decreasing using exogeneous data like GDP and health care expenditures?</li>
<li>If the analysis is really hopeless, could you give a clear reasoning why this is the case?</li>
</ol>
"
"0.167639626868134","0.18995387394524","222832","<p>We have data values pertaining to BPS (bits per second) traffic on a networking device. We have data from for a particular month (October) from the past 4 years. The data points are available in a 1 minute interval.</p>

<p>So we have 31 days * 24 hours * 60 minutes = 44640 values for each year. Multiply that by 4 and we get ~180,000 data points in our CSV file. We have tried several model including TBATS, ARIMA etc. to make future predictions. For example, we need to predict 44640 values for the october of next year. Problem is that so many data values means that fitting a model takes forever and it's not worth waiting an entire day of processing just to find that model is predicting a straight line trend around the average of previous values.</p>

<p>We are looking for possibly reducing this data using something like exponential smoothing but we do not know how. The past data values are as follows:</p>

<pre><code>8839
29191985
3825997
439694949
5186727
5747251
4814919
489752985
481456366
53712118
51364413
57449919
48123322
473151317
529185483
51284866
528115232
597333333
535883672
594275668
549679615
589267353
54916916
756419719
65492594
587599734
616325563
68434481
63351749
624134894
61665387
697646113
61722678
689499647
6884953
618223888
67283625
7451432
773956231
72682555
748525567
682498934
71892441
8527712
752342356
68912676
746693391
7241629
712685465
748971655
74339677
773571787
81173992
9369364
885665416
969439265
99578482
13281261
127297176
1577597
129853832
13882798
1184388675
115559261
118735937
121685158
1128946618
1157798227
1143165165
11632918
122479785
11341628
116385628
12621439
1172845976
1214564385
1795176
12957522
1183316274
12619916
12519533
135765784
1399453354
1399224864
1372868436
1331569834
137852813
128497677
1297789678
135918171
1294935824
1384582825
1362893276
145228865
142459451
1523728929
1553973554
155186563
149211641
159253766
141712263
14764913
148991924
1562214535
164371933
1546871
163188462
168156746
168938876
16835799
171595761
1663196329
1692558573
17636281
18258581
177213887
1652531676
19852331
1789876462
1789629233
1748867173
181994385
176165681
1969791999
19861387
1947295162
287128848
235583965
1968433253
217279852
2212697598
1953822855
2212983294
2166245238
29418584
28276258
22111712
21361513
2114169137
2314153846
216195463
1948538537
2131395686
22873135
2121744212
2261766416
1952463426
231837712
211836243
21321957
231673786
23586221
237934824
23857991
226835959
22527878
229163528
223611724
242565252
2451523242
242146954
22592296
2524295439
24288788
239426786
2488167389
23614618
2387528327
224687321
2352352153
219349398
211514732
223242859
2114838493
2275546998
224398369
2271632485
2237118326
231972341
223867472
232943687
2616184865
2264386319
241637212
2577586277
2473823845
247444156
261553512
264819946
2643896421
274781277
26189985
272488724
2727773421
269784662
2923184161
2835866726
29476972
313529872
3899199
2979386981
37853218
38881954
297738289
3113766229
32723531
2773715317
3137525998
367757942
36456197
297769411
2882461831
342295362
344496963
39439679
3136141447
3324496997
3253434742
338259
2895698259
31183592
374252594
38459536
312441788
3239434239
3161928
3166617496
31916915
3162371549
319837457
3141362857
32638795
3157587728
3281771348
3142241484
3368347612
327583987
3241925869
33183412
32491351
3383213
3573783926
3299445212
3268651
33138667
329333539
329314786
343676884
342544137
3286497278
34854846
31553839
3553121791
3295782535
333871824
3357511167
364861848
3412626294
33294747
348641163
327124157
3392738132
325626931
33883856
334594742
32942374
31897973
3834926556
365132813
35475637
336384187
366552233
36141892
34887985
34695147
3576451651
3335458644
3326826563
3341539
339894997
337912327
336649223
3555534642
329266359
356461957
3439773899
328435177
3758339514
346635125
361774558
335482465
3486783351
349275
341392357
37215
3497621877
364242974
3624311875
377361582
367461755
363526377
34877241
47832182
371281677
376216515
3741615717
3695335388
3628351931
372717255
3792287845
36549945
372238998
3869247316
3822289851
386173797
372368834
345429379
417153116
38749739
395119594
3746367111
383839372
391378292
367872746
372373178
3625754
379946415
37778181
3746261571
3918932444
386892529
3695653853
3959862748
415346593
42977194
412162553
41582129
41732773
471311973
4415543
3838746827
43135679
41259122
416451147
382524677
3829914759
396922256
392669399
383285533
3915829759
497197157
471337265
494296438
395495
41562899
3973355519
398198495
376359951
397532419
438115941
383579951
39116435
425944659
3961366459
3997619677
4575215
415522986
3947337112
394636114
392714147
385221299
47237153
...and so on
</code></pre>

<p>Keeping in mind that we have 
~180,000 such past data values, and we wish to predict the next 44640 future values, how do we go about making such a prediction in R?</p>

<p>We are new at R so actual code rather than abstract concepts would help a lot!</p>

<p><strong>EDIT to show how out ARIMA model got stuck in computations:</strong></p>

<p>This is the code we used for auto-arima fitting that got stuck for hours until we had to abort:</p>

<pre><code>mydata &lt;- load.csv(""2.csv"")
mydata &lt;- ts(mydata, start = c(2012,1), frequency = 44640)
require(forecast)
arimafit &lt;- auto.arima(mydata)
</code></pre>

<p>What are we doing wrong in the ARIMA model that's taking so long? </p>
"
"0.234734279876976","0.248708001686903","222914","<p>I have a time series object <code>calc_visit_ts</code>. <strong>I want to apply the best fit time series model based on the MAPE value for each model.</strong> The issue I face is that the MAPE value HOLT-WINTER multiplicative model cannot be calculated in the same way as the other models(as it gives me a different MAPE value when compared to <code>summary(visit_model_Hw_M)</code>).</p>

<pre><code>#### AUTO-ARIMA
visit_model_Arima &lt;- auto.arima(calc_visit_ts)
# summary(visit_model_Arima)

#### HOLT-WINTER ADDITIVE
visit_model_Hw_A &lt;- hw(calc_visit_ts,h=monthly_prediction,seasonal = ""additive"")
# summary(visit_model_Hw_A)

#### HOLT-WINTER MULTIPLICATIVE
visit_model_Hw_M &lt;- hw(calc_visit_ts,h=monthly_prediction,seasonal = ""multiplicative"")
# summary(visit_model_Hw_M)


#### Calculating MAPE on models for best suit
model_Mape&lt;- c( MAPE_model(visit_model_Arima)
                ,MAPE_model(visit_model_Hw_A))
                #,MAPE_model(visit_model_Hw_M))  this is not accurate

model_Mape=na.omit(model_Mape)
token&lt;-which(min(model_Mape)==model_Mape)

if(length(token)&gt;0)
{
  if(token==1)
    {visit_model&lt;-visit_model_Arima
  }else if(token==2)
    {visit_model&lt;-visit_model_Hw_A
  }else if(token==3)
    {visit_model&lt;-visit_model_Hw_M
  }else 
  {
    ##EXCEPTION HANDLING  
  }
}

summary(visit_model)
</code></pre>

<p>And here is the <strong>function I use to perform MAPE calculation</strong> on the models - </p>

<pre><code>MAPE_model &lt;- function(visit_model) {
 #CHECK FOR ZERO CONDIITION  if(visit_model$x!=0)
 mape = mean(abs(visit_model$residuals)/visit_model$x)
 return(mape)
}
</code></pre>

<p><strong>Data</strong> for time series -</p>

<pre><code>calc_visit_ts
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012          35  53  65  60  64  49  63  55  59  66
2013  62  54  77  67  84  62  82  65  59  67  60  67
2014  73  75  55  76  93  96  89  76  88  65  83  82
2015  76  72  75  94  91  83  72  73  80  83  81  81
2016  97  91  90  80 101  98  

dput(calc_visit_ts)
structure(c(35, 53, 65, 60, 64, 49, 63, 55, 59, 66, 62, 54, 77, 
67, 84, 62, 82, 65, 59, 67, 60, 67, 73, 75, 55, 76, 93, 96, 89, 
76, 88, 65, 83, 82, 76, 72, 75, 94, 91, 83, 72, 73, 80, 83, 81, 
81, 97, 91, 90, 80, 101, 98), .Tsp = c(2012.16666666667, 2016.41666666667, 
12), class = ""ts"")
</code></pre>

<p>To show exactly what I mean -</p>

<p><strong>Holt-Winter Additive Plot</strong></p>

<p><a href=""http://i.stack.imgur.com/i0c0K.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i0c0K.jpg"" alt=""Holt-Winter Additive Plot""></a></p>

<p><strong>Holt-Winter Multiplicative Plot</strong>
<a href=""http://i.stack.imgur.com/rgvFt.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rgvFt.jpg"" alt=""Holt-Winter Multiplicative Plot""></a></p>

<p>The <strong>issue</strong> is <code>summary(visit_model_Hw_M)</code> gives <code>MAPE = 9.075097</code>
whereas, <code>MAPE_model(visit_model_Hw_M)</code> gives <code>0.001273087</code> because the multiplicative model fits the curve(data points) therefore using <code>visit_model_Hw_M$residuals</code> isn't an appropriate way to calculate the MAPE(as the function tries to fit the curve).</p>

<p>Is there a way I can fetch the MAPE value for HOLT-WINTER multiplicative from the summary itself? OR a way to correctly estimate the MAPE value for the HOLT-WINTER multiplicative model?</p>
"
"0.181071492085037","0.175863114528165","223457","<p>Suppose I have a training dataset, I use <code>auto.arima</code> (from ""forecast"" package in R) to fit the training data. As a result I get the lag and integration orders $(p, d, q)$ and the corresponding coefficients $\psi_i$ and $\theta_i$.</p>

<pre><code>ytrain = c(0.435477843, 0.435394762, 0.195528995, 1.451623315, 1.740084831 2.379904714, 1.092366508, 0.001031411, 0.592164090, 0.670323418)

fit &lt;- auto.arima(ytrain)
</code></pre>

<p>Now I have new data </p>

<pre><code>ytest = c(-0.1349199  0.9001208 -0.5171740 -0.9958452  0.4125953 -0.3320575  0.1633313  0.2890109 -0.4284824  0.7902680)
</code></pre>

<p>I want to fit this new data by using the model from training data (using the same $(p, d, q)$ and also the same corresponding coefficients). I.e. I want to use the model I have from <code>ytrain</code> to make prediction based on <code>ytest</code>. As a result I can know if there are any points in the new data looking like anomaly points (compared to the training data)</p>

<p>I have searched long time and haven't find a R function to implement it. I know I can compute this by hand, e.g. for ARMA(1,2):</p>

<p>$\hat{Y}_n =  \hat{\mu} + \hat{\psi}_1 Y_{n-1} - \hat{\theta}_{1} \epsilon_{n-1} - \hat{\theta}_2 \epsilon_{n-2} $</p>

<p>But if I do this, I am not sure how to start to get $\epsilon_1 = Y_1 - \hat{Y}_1$ and $\epsilon_2 = Y_2 - \hat{Y}_2$ to start since I don't have $\hat{Y}_1$ and $\hat{Y}_2$. </p>

<ul>
<li>Could anyone suggest an R function for doing this? Or if not,  </li>
<li>Could anyone help me with this question if there is no R function doing this?</li>
</ul>
"
"0.181071492085037","0.175863114528165","223872","<p>Data consisting of 30 values is stored in a time series <code>time</code>.<br>
After applying ARIMA modelling on <code>time</code>, I used <code>forecast</code> function to predict future values:</p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
prediction step is not working and showing error 
Error in ts(x) : object is not a matrix
</code></pre>

<p>As you see above, I am getting an error message. But if I do</p>

<pre><code>model = arima(time[1:25], order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
</code></pre>

<p>it works. Why is it so?</p>

<p>When I used the <code>predict</code> function </p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction=predict(model,n.ahead=10)
</code></pre>

<p>it also works.</p>

<p><strong>Which</strong> function would be better to use, <code>predict</code> or <code>forecast</code>, for ARIMA models in R, and <strong>why</strong>?</p>
"
"0.132235920981457","0.128432259813587","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.165294901226822","0.160540324766984","224380","<p>I have a dataset which contains data from a sensor for every 5 minutes and am trying to predict for example 10 future values based on the first 500 values. My data looks like the following and could be downloaded <a href=""https://github.com/numenta/NAB/blob/master/data/artificialWithAnomaly/art_daily_flatmiddle.csv"" rel=""nofollow"">here</a>:</p>

<pre><code>timestamp,value
2014-04-01 00:00:00,-21.0483826823
2014-04-01 00:05:00,-20.2954768676
2014-04-01 00:10:00,-18.127229468299998
2014-04-01 00:15:00,-20.1716653997
2014-04-01 00:20:00,-21.223761612
2014-04-01 00:25:00,-19.1044911334
</code></pre>

<p><a href=""http://i.stack.imgur.com/iw6O3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iw6O3.png"" alt=""enter image description here""></a></p>

<p>I am taking the following steps:</p>

<pre><code># Read data from file and create time series    
myData &lt;- read.zoo(file=""filePath"", sep = "","", header = TRUE,index = 1, tz = """", format = ""%Y-%m-%d %H:%M:%S"", nrows=500)

# Fit ARIMA model to the data
fit &lt;- auto.arima(z, stepwise=FALSE, trace=TRUE, approximation=FALSE)

# Predict 10 timesteps ahead
pred &lt;- predict(fit, n.ahead = 10)
</code></pre>

<p>But when I print the prediction results they do not seem promising and model always converges to a single value:</p>

<pre><code>$pred
Time Series:
Start = 1396474800 
End = 1396477500 
Frequency = 0.00333333333333333 
 [1] 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789

$se
Time Series:
Start = 1396474800 
End = 1396477500 
Frequency = 0.00333333333333333 
 [1]  7.136100  9.728122 11.762177 13.493007 15.025767 16.416032 17.697417 18.892088 20.015580 21.079276
</code></pre>

<p>And here is the summary of fit:</p>

<pre><code>&gt; summary(fit)
Series: z 
ARIMA(0,1,1)                    

Coefficients:
          ma1
      -0.0735
s.e.   0.0463

sigma^2 estimated as 50.92:  log likelihood=-1688.17
AIC=3380.34   AICc=3380.37   BIC=3388.77

Training set error measures:
                    ME     RMSE      MAE     MPE    MAPE       MASE        ACF1
Training set 0.2215984 7.121813 3.141386 1592726 1592732 0.07197436 0.001426353
</code></pre>

<p>This is my first day with R and I think I might be doing something wrong. Any help would be much appreciated.</p>

<p>Thanks</p>
"
"0.0739221270954573","0.0717958158617738","228364","<p>On the plot black is the data and red are the fitted values obtained from <code>fitted</code> i.e. one step forecast, I am using 365 days for training and then 3000+ days for testing, I choose value of k using cross-validation on 365 data points. Following is the model I used:</p>

<pre><code>Arima(data, order=c(2,0,2),xreg=forecast::fourier(min_temp_aus,57))
</code></pre>

<p><a href=""http://i.stack.imgur.com/lFhrN.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lFhrN.jpg"" alt=""enter image description here""></a></p>

<p>How can I improve the fit on both extremes?</p>

<p>PS: Square loss is of 17524 considering I am predicting 3000+ data points. The way I am looking at it is, if I am off by 1 with every prediction still it makes a loss of 3000. I thought it is good, but maybe I am wrong.</p>
"
"0.104541674697863","0.101534616513362","230269","<p>I am sitting with a couple of time-series that I am analysing using ARIMA models. I have a question regarding prediction intervals. When predicting using a model that takes a first difference (a SARIMA(1,1,0)x(1,0,0) model), I get an increasing size of the prediction interval. Without I get a very constant and narrow band (see below):</p>

<p><a href=""http://i.stack.imgur.com/UaHX6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UaHX6.png"" alt=""Graphs""></a></p>

<p>The corresponding results are as follows:</p>

<p><a href=""http://i.stack.imgur.com/Fu2nU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Fu2nU.png"" alt=""Results""></a></p>

<p>Can anyone explain why the band is so constant? First I thought it was because of a large significant MA coefficient. This, however, I removed and the ""problem"" persisted. Then I though it was because the ARIMA without differencing automatically included an intercept. However, again, when I specified <code>include.mean = FALSE</code>, nothing changed.</p>

<p>Any help would be appreciated.</p>
"
"0.165294901226822","0.160540324766984","232590","<p>I have a number of groups with monthly data from 2010 to 2016. It's over 80 groups. I succesfully ran an ARIMA model with the montly data but with the sales data summed up (without groups). </p>

<p>Now I'd like to compare the performance with a per group model that runs an ARIMA model for each group and maybe later consider another type of grouping (geographical location, clustering, etc.)</p>

<p>I ran my original model with the following code:</p>

<pre><code>        Datos &lt;- read.csv(""C:/Users/borja.sanz/Desktop/Borja/Forecasting/V`enter code here`entas/Datos para Forecast.csv"")
        options(scipen=999)
        library(lubridate)
        Datos$Fecha = dmy(Datos$Fecha)

        #Declare time series
        tsDatos&lt;-ts(Datos$VentaLocal,start = c(2010,1),frequency = 12)
        plot(tsDatos)
        library(forecast)
        library(dplyr)

        #AutoArima Model
        m_aa = auto.arima(tsDatos)
        f_aa = forecast(m_aa, h=36)
        plot(f_aa)

#Create the forecasts along with the lower and upper bound
    forecast_df = data.frame(prediction=f_aa$mean,
                             abajo=f_aa$lower[,2],
                             arriba=f_aa$upper[,2],
                             date=last_date + seq(1/12, 3, by=1/12))
    forecast_df
</code></pre>

<p>This is how my data looks like:</p>

<pre><code>       Group    Year    Month   Date    Sales
1   2010    1   1/01/2010   134536.625
1   2010    2   1/02/2010   117506.625
1   2010    3   1/03/2010   132153.75
1   2010    4   1/04/2010   129723.125
1   2010    5   1/05/2010   135834.5
1   2010    6   1/06/2010   130115.375
1   2010    7   1/07/2010   144716
1   2010    8   1/08/2010   137195
1   2010    9   1/09/2010   137522.875
1   2010    10  1/10/2010   187063
1   2010    11  1/11/2010   162002.75
1   2010    12  1/12/2010   262297.375
1   2011    1   1/01/2011   177291.25
1   2011    2   1/02/2011   154816
1   2011    3   1/03/2011   171231.125
1   2011    4   1/04/2011   217717
1   2011    5   1/05/2011   178767.75
1   2011    6   1/06/2011   180817.75
1   2011    7   1/07/2011   216927.125
1   2011    8   1/08/2011   204509.125
1   2011    9   1/09/2011   199449.5
1   2011    10  1/10/2011   243812.125
1   2011    11  1/11/2011   232135.875
1   2011    12  1/12/2011   330854.75
1   2012    1   1/01/2012   217123.875
1   2012    2   1/02/2012   200558
1   2012    3   1/03/2012   215689.5
1   2012    4   1/04/2012   245500.25
1   2012    5   1/05/2012   219687.25
1   2012    6   1/06/2012   243345.625
1   2012    7   1/07/2012   249042
1   2012    8   1/08/2012   198443.75
1   2012    9   1/09/2012   209157.375
1   2012    10  1/10/2012   234089
1   2012    11  1/11/2012   237531
1   2012    12  1/12/2012   365301.25
1   2013    1   1/01/2013   211129.375
1   2013    2   1/02/2013   185249.625
1   2013    3   1/03/2013   256565.625
1   2013    4   1/04/2013   183549.5
1   2013    5   1/05/2013   189698.25
1   2013    6   1/06/2013   207955.625
1   2013    7   1/07/2013   230764.125
1   2013    8   1/08/2013   212551.625
1   2013    9   1/09/2013   201329.5
1   2013    10  1/10/2013   242745.125
1   2013    11  1/11/2013   261893.375
1   2013    12  1/12/2013   418313.25
1   2014    1   1/01/2014   205532.75
1   2014    2   1/02/2014   170487.75
1   2014    3   1/03/2014   196077
1   2014    4   1/04/2014   221760.875
1   2014    5   1/05/2014   198185
1   2014    6   1/06/2014   204919.25
1   2014    7   1/07/2014   218972.75
1   2014    8   1/08/2014   221439.875
1   2014    9   1/09/2014   195888.375
1   2014    10  1/10/2014   234595.75
1   2014    11  1/11/2014   259712.875
1   2014    12  1/12/2014   355691.875
1   2015    1   1/01/2015   205156.25
1   2015    2   1/02/2015   185358.875
1   2015    3   1/03/2015   218555.75
1   2015    4   1/04/2015   204233.625
1   2015    5   1/05/2015   212160.625
1   2015    6   1/06/2015   207217.25
1   2015    7   1/07/2015   225723.75
1   2015    8   1/08/2015   205902.625
1   2015    9   1/09/2015   196940.625
1   2015    10  1/10/2015   250916
1   2015    11  1/11/2015   236835.125
1   2015    12  1/12/2015   358327.625
2   2010    1   1/01/2010   227175.875
2   2010    2   1/02/2010   205042
2   2010    3   1/03/2010   239206.375
2   2010    4   1/04/2010   212059.875
2   2010    5   1/05/2010   232789
2   2010    6   1/06/2010   247876.125
2   2010    7   1/07/2010   278557
2   2010    8   1/08/2010   270410.125
2   2010    9   1/09/2010   251060.375
2   2010    10  1/10/2010   302738.625
2   2010    11  1/11/2010   266869.75
2   2010    12  1/12/2010   272978.75
2   2011    1   1/01/2011   238614.5
2   2011    2   1/02/2011   224240.375
2   2011    3   1/03/2011   245457.375
2   2011    4   1/04/2011   238583.5
2   2011    5   1/05/2011   252392.75
2   2011    6   1/06/2011   256749.5
2   2011    7   1/07/2011   264736.125
2   2011    8   1/08/2011   256414
2   2011    9   1/09/2011   242335.125
2   2011    10  1/10/2011   305224.75
2   2011    11  1/11/2011   289199.875
2   2011    12  1/12/2011   281807.75
2   2012    1   1/01/2012   244886.125
2   2012    2   1/02/2012   232062.375
2   2012    3   1/03/2012   264991.75
2   2012    4   1/04/2012   232750.5
2   2012    5   1/05/2012   248498.375
2   2012    6   1/06/2012   264290.875
2   2012    7   1/07/2012   272689.75
2   2012    8   1/08/2012   260441.25
2   2012    9   1/09/2012   251852.375
2   2012    10  1/10/2012   305929.625
2   2012    11  1/11/2012   276711.625
2   2012    12  1/12/2012   278672.875
2   2013    1   1/01/2013   242613.875
2   2013    2   1/02/2013   227575.75
2   2013    3   1/03/2013   250318.875
2   2013    4   1/04/2013   250150.375
2   2013    5   1/05/2013   258467.25
2   2013    6   1/06/2013   261359.25
2   2013    7   1/07/2013   279113.75
2   2013    8   1/08/2013   258699
2   2013    9   1/09/2013   244841.375
2   2013    10  1/10/2013   308197.25
2   2013    11  1/11/2013   284195.5
2   2013    12  1/12/2013   287718.75
2   2014    1   1/01/2014   239510.375
2   2014    2   1/02/2014   216338.125
2   2014    3   1/03/2014   245626.75
2   2014    4   1/04/2014   230619.875
2   2014    5   1/05/2014   251758.875
2   2014    6   1/06/2014   254946.75
2   2014    7   1/07/2014   276268.75
2   2014    8   1/08/2014   266151.75
2   2014    9   1/09/2014   245859.375
2   2014    10  1/10/2014   317797.5
2   2014    11  1/11/2014   283786.625
2   2014    12  1/12/2014   289767.875
2   2015    1   1/01/2015   244008
2   2015    2   1/02/2015   228638
2   2015    3   1/03/2015   260056
2   2015    4   1/04/2015   232560.875
2   2015    5   1/05/2015   252642.125
2   2015    6   1/06/2015   249018.5
2   2015    7   1/07/2015   278113.125
2   2015    8   1/08/2015   255851
2   2015    9   1/09/2015   263046.625
2   2015    10  1/10/2015   344240.75
2   2015    11  1/11/2015   295486.125
2   2015    12  1/12/2015   293499.375
</code></pre>

<p>I only included two groups in the sample. I would like to use a function like one of the apply (tapply, lapply, sapply, etc.) that can run an AUTO.ARIMA model per group. Then I would like to obtain the forecast for each group for x number of months and also if I could visualize the model coefficients.</p>
"
"0.197125672254553","0.167523570344139","233036","<p>I'm finding some odd behaviour in Google's CausalImpact R package and wondered if anyone has found the same and knows the cause. If you feed the package a certain length time series, the model snaps to a perfect historical fit, no matter what explanatory variables you use.</p>

<p>Using code from Google's own toy example, I set up a model, which works fine</p>

<pre><code>library(CausalImpact)

total.points &lt;- 300
marketing.starts &lt;- 270

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = total.points)
y &lt;- 1.2 * x1 + rnorm(total.points)

y[marketing.starts:total.points] &lt;- y[marketing.starts:total.points] + 10
data &lt;- cbind(y, x1)

pre.period &lt;- c(1, marketing.starts-1)
post.period &lt;- c(marketing.starts, total.points)

impact &lt;- CausalImpact(data, pre.period, post.period)

plot(impact)
</code></pre>

<p>This site won't let me post more than two image links as I'm new, but the above produces a regular CausalImpact example.</p>

<p>Now switch the explanatory variable <code>X1</code> for a nonsense variable <code>X2</code> (different seed) that doesn't explain y at all. The result is as you'd expect and the model no longer fits.</p>

<pre><code>set.seed(10)
x2 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = total.points)

data &lt;- cbind(y, x2)

impact &lt;- CausalImpact(data, pre.period, post.period)

plot(impact)
</code></pre>

<p><a href=""http://i.stack.imgur.com/APOjj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/APOjj.png"" alt=""enter image description here""></a></p>

<p>Finally, change the historic and predicted periods, so that there is a bit more history and a shorter prediction. Still using only the nonsense <code>X2</code> variable as explanatory.</p>

<pre><code>total.points &lt;- 300
marketing.starts &lt;- 289

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = total.points)
y &lt;- 1.2 * x1 + rnorm(total.points)

y[marketing.starts:total.points] &lt;- y[marketing.starts:total.points] + 10
data &lt;- cbind(y, x2)

pre.period &lt;- c(1, marketing.starts-1)
post.period &lt;- c(marketing.starts, total.points)

impact &lt;- CausalImpact(data, pre.period, post.period)

plot(impact)
</code></pre>

<p><a href=""http://i.stack.imgur.com/2kfuO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2kfuO.png"" alt=""enter image description here""></a></p>

<p>The model suddenly has an almost perfect historical fit, even though I haven't given it anything useful to explain the past. It does it suddenly - if you use observation 288 as marketing start in the example above, it won't do it. I'm a newbie to the site, but would really appreciate any clues about what it's doing!</p>
"
