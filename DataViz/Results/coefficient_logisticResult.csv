"V1","V2","V3","V4"
"0.116472706986951","0.121395395733377","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.0984374038697697","0.102597835208515","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.132067635948844","0.137649440322337","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"0.108950241113821","0.12977713690461","  8511","<p>Christopher Manning's <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">writeup on logistic regression in R</a> shows a logistic regression in R as follows:</p>

<pre><code>ced.logr &lt;- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)
</code></pre>

<p>Some output:</p>

<pre><code>&gt; summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -1.31827    0.12221 -10.787 &lt; 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 &lt; 2e-16
followsV       0.53408    0.05660   9.436 &lt; 2e-16
factor(class)2 1.27045    0.10320  12.310 &lt; 2e-16
factor(class)3 1.04805    0.10355  10.122 &lt; 2e-16
factor(class)4 1.37425    0.10155  13.532 &lt; 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4
</code></pre>

<p>He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.</p>

<p>However, how much variance does the model account for?  A <a href=""http://www.ats.ucla.edu/stat/stata/output/old/lognoframe.htm"" rel=""nofollow"">Stata page on logistic regression</a> says:</p>

<blockquote>
  <p>Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the ""constant-only"" model and L1 is the log likelihood for the full model with constant and predictors. </p>
</blockquote>

<p>I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.</p>

<p>Can someone verify how one actually computes the pseudo-R^2 in R using this example?</p>
"
"0.0508328567775349","0.0794719414239026","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.0440225453162812","0.0458831467741124","  9027","<p>I have two logistic regression models in R made with <code>glm()</code>.  They both use the same variables, but were made using different subsets of a matrix.  Is there an easy way to get an average model which gives the means of the coefficients and then use this with the predict() function?</p>

<p>[ sorry if this type of question should be posted on a programming site let me know and I'll post it there ]</p>

<p>Thanks</p>
"
"0.0984374038697697","0.102597835208515"," 10316","<p>I'm working on a multiple logistic regression in R using <code>glm</code>. The predictor variables are continuous and categorical. An extract of the summary of the model shows the following:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   2.451e+00  2.439e+00   1.005   0.3150
Age           5.747e-02  3.466e-02   1.658   0.0973 .
BMI          -7.750e-02  7.090e-02  -1.093   0.2743
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Confidence intervals:</p>

<pre><code>                  2.5 %       97.5 %
(Intercept)  0.10969506 1.863217e+03
Age          0.99565783 1.142627e+00
BMI          0.80089276 1.064256e+00
...
</code></pre>

<p>Odd ratios:</p>

<pre><code>                 Estimate Std. Error   z value Pr(&gt;|z|)
(Intercept)  1.159642e+01  11.464683 2.7310435 1.370327
Age          1.059155e+00   1.035269 5.2491658 1.102195
B            9.254228e-01   1.073477 0.3351730 1.315670
...
</code></pre>

<p>The first output shows that $Age$ is significant. However, the confidence interval for $Age$ includes the value 1 and the odds ratio for $Age$ is very close to 1. What does the significant p-value from the first output mean? Is $Age$ a predictor of the outcome or not?</p>
"
"0.125032295569587","0.106622797996637"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0590624423218618","0.0615587011251092"," 11178","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio"">Logistic Regression in R (Odds Ratio)</a>  </p>
</blockquote>



<p>I need to do a logistic regression in R. My response variable is <code>surv=0</code>; <code>surv=1</code> and I have about 18 predictor variables.</p>

<p>After reading my model, I got the table of Coefficients below and I need to go through some steps, which I am not familiar with, until I get to the odds ratios.</p>

<p>This is my first time to do a logistic regression in R and your help would be appreciated.</p>

<pre><code>Call:
glm(formula = surv ~ as.factor(tdate) + as.factor(line) + as.factor(wt) + 
    as.factor(crump) + as.factor(pind) + as.factor(pcscore) + 
    as.factor(ptem) + as.factor(pshiv) + as.factor(pincis) + 
    as.factor(presp) + as.factor(pmtone) + as.factor(pscolor) + 
    as.factor(ppscore) + as.factor(pmstain) + as.factor(pbse) + 
    as.factor(psex) + as.factor(pgf), family = binomial(link = ""logit""), 
    data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9772  -0.5896  -0.4419  -0.3154   2.8264  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.59796    0.27024  -2.213 0.026918 *  
as.factor(tdate)2009-09-08  0.43918    0.19876   2.210 0.027130 *  
as.factor(tdate)2009-09-11  0.27613    0.20289   1.361 0.173514    
as.factor(tdate)2009-09-15  0.58733    0.19232   3.054 0.002259 ** 
as.factor(tdate)2009-09-18  0.52823    0.20605   2.564 0.010360 *  
as.factor(tdate)2009-09-22  0.45661    0.19929   2.291 0.021954 *  
as.factor(tdate)2009-09-25 -0.09189    0.21740  -0.423 0.672526    
as.factor(tdate)2009-09-29 -0.15696    0.28369  -0.553 0.580076    
as.factor(tdate)2010-01-26  1.39260    0.21049   6.616 3.69e-11 ***
as.factor(tdate)2010-01-29  1.67827    0.21099   7.954 1.80e-15 ***
as.factor(tdate)2010-02-02  1.35442    0.21292   6.361 2.00e-10 ***
as.factor(tdate)2010-02-05  1.36856    0.21439   6.383 1.73e-10 ***
as.factor(tdate)2010-02-09  1.18159    0.21951   5.383 7.33e-08 ***
as.factor(tdate)2010-02-12  1.40457    0.22001   6.384 1.73e-10 ***
as.factor(tdate)2010-02-16  1.01063    0.21783   4.639 3.49e-06 ***
as.factor(tdate)2010-02-19  1.54992    0.21535   7.197 6.14e-13 ***
as.factor(tdate)2010-02-23  0.85695    0.33968   2.523 0.011641 *  
as.factor(line)2           -0.26311    0.07257  -3.625 0.000288 ***
as.factor(line)5            0.06766    0.11162   0.606 0.544387    
as.factor(line)6           -0.30409    0.12130  -2.507 0.012176 *  
as.factor(wt)2             -0.33904    0.10708  -3.166 0.001544 ** 
as.factor(wt)3             -0.28976    0.13217  -2.192 0.028359 *  
as.factor(wt)4             -0.50470    0.16264  -3.103 0.001915 ** 
as.factor(wt)5             -0.74870    0.20067  -3.731 0.000191 ***
as.factor(crump)2           0.07537    0.10751   0.701 0.483280    
as.factor(crump)3          -0.14050    0.13217  -1.063 0.287768    
as.factor(crump)4          -0.20131    0.16689  -1.206 0.227724    
as.factor(crump)5          -0.23963    0.20778  -1.153 0.248803    
as.factor(pind)2           -0.29893    0.10752  -2.780 0.005434 ** 
as.factor(pind)3           -0.40828    0.12436  -3.283 0.001027 ** 
as.factor(pind)4           -0.73021    0.14947  -4.885 1.03e-06 ***
as.factor(pind)5           -0.68878    0.17650  -3.902 9.52e-05 ***
as.factor(pcscore)2        -0.52667    0.13606  -3.871 0.000108 ***
as.factor(ptem)2           -0.72600    0.08964  -8.099 5.52e-16 ***
as.factor(ptem)3           -0.79145    0.10503  -7.536 4.86e-14 ***
as.factor(ptem)4           -0.89956    0.10331  -8.707  &lt; 2e-16 ***
as.factor(ptem)5           -0.90181    0.10721  -8.412  &lt; 2e-16 ***
as.factor(pshiv)2           0.25236    0.07713   3.272 0.001068 ** 
as.factor(pincis)2          0.02327    0.07216   0.323 0.747041    
as.factor(presp)2           0.43746    0.11598   3.772 0.000162 ***
as.factor(pmtone)2          0.34515    0.11178   3.088 0.002016 ** 
as.factor(pscolor)2         0.53469    0.26851   1.991 0.046443 *  
as.factor(ppscore)2         0.25664    0.08751   2.933 0.003361 ** 
as.factor(pmstain)2        -0.48619    0.84408  -0.576 0.564611    
as.factor(pbse)2           -0.28248    0.07335  -3.851 0.000117 ***
as.factor(psex)2           -0.18240    0.06385  -2.857 0.004280 ** 
as.factor(pgf)12            0.10329    0.14314   0.722 0.470554    
as.factor(pgf)21           -0.06481    0.10772  -0.602 0.547388    
as.factor(pgf)22            0.39584    0.12740   3.107 0.001890 ** 
as.factor(pgf)31            0.18820    0.10082   1.867 0.061936 .  
as.factor(pgf)32            0.39662    0.13963   2.841 0.004504 ** 
as.factor(pgf)41            0.09178    0.10413   0.881 0.378106    
as.factor(pgf)42            0.21056    0.14906   1.413 0.157787    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7812.9  on 8714  degrees of freedom
Residual deviance: 6797.4  on 8662  degrees of freedom
  (418 observations deleted due to missingness)
AIC: 6903.4

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.062257280636469","0.064888568452305"," 14546","<p>I have run the following logistic regression:</p>

<pre><code>glm(formula = DecisionasReceiver ~ L1 + L2 + L3, 
  family = binomial(""logit""), data = lue)
</code></pre>

<p>where L1 L2 and L3 code for differences in condition of no.GREEN.
L1: 1,-1,0,0 : is there a difference in DecisionasReceiver as no.GREEN changes from 1 to 2?</p>

<p>L2: 0,1,-1,0: is there a difference in DecisionasReceiver as no.GREEN changes from 2 to 3?</p>

<p>L2: 0,0,1,-1: is there a difference in DecisionasReceiver as no.GREEN changes from 3 to 4?</p>

<p>And I'm running this regression both for the cases where MessageReceived is BLUE and for MessageReceived RED. </p>

<p>I have the following output:</p>

<pre><code>   Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9535     0.3659  -2.606  0.00916 ** 
L1            2.2753     0.5406   4.209 2.56e-05 ***
L2            3.1234     0.7318   4.268 1.97e-05 ***
L3            1.9369     0.8134   2.381  0.01726 *  
</code></pre>

<p>Looking at my graph it seems strange that the coefficients are positive and that the intercept is -0.953. How exactly should I interpret these results in the light of the graph? </p>

<p><img src=""http://dl.dropbox.com/u/22681355/graph.png"" alt=""""></p>
"
"0.062257280636469","0.064888568452305"," 15307","<p>I am curious about the consequences of changing the order of the explanatory variables in a binary logistic regression. In a recent series of logistic regressions I ran in SPSS, I found that changing the order of the explanatory variables ($z, y, x$ instead of $x, y, z$) resulted in different coefficient values and significance levels. Investigating further, I got the same results in <code>R</code> using the same orders. Clearly, shifting the predictors around matters in terms of the resultsâ€”my question is how? (And yes, I checked to make sure I wasn't entering the variables stepwise.)</p>
"
"0.139211511597426","0.145095250022002"," 15577","<p>I'm trying to run a zero-inflated regression for a continuous response variable in R. I'm aware of a gamlss implementation, but I'd really like to try out this algorithm by Dale McLerran that is conceptually a bit more straightforward. Unfortunately, the code is in SAS and I'm not sure how to re-write it for something like nlme. </p>

<p>The code is as follows:</p>

<pre><code>proc nlmixed data=mydata;
  parms b0_f=0 b1_f=0 
        b0_h=0 b1_h=0 
        log_theta=0;


  eta_f = b0_f + b1_f*x1 ;
  p_yEQ0 = 1 / (1 + exp(-eta_f));


  eta_h = b0_h + b1_h*x1;
  mu    = exp(eta_h);
  theta = exp(log_theta);
  r = mu/theta;


  if y=0 then
     ll = log(p_yEQ0);
  else
     ll = log(1 - p_yEQ0)
          - lgamma(theta) + (theta-1)*log(y) - theta*log(r) - y/r;


  model y ~ general(ll);
  predict (1 - p_yEQ0)*mu out=expect_zig;
  predict r out=shape;
  estimate ""scale"" theta;
run;
</code></pre>

<p>From: <a href=""http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779"" rel=""nofollow"">http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779</a></p>

<p><strong>ADD:</strong></p>

<p>Note: There are no mixed effects present here - only fixed.</p>

<p>The advantage to this fitting is that (even though the coefficients are the same as if you separately fit a logistic regression to P(y=0) and a gamma error regression with log link to E(y | y>0)) you can estimate the combined function E(y) which includes the zeroes. One can predict this value in SAS (with a CI) using the line <code>predict (1 - p_yEQ0)*mu</code> .</p>

<p>Further, one is able to write custom contrast statements to test the significance of predictor variables on E(y). For example, here is another version of the SAS code I have used:</p>

<pre><code>proc nlmixed data=TestZIG;
      parms b0_f=0 b1_f=0 b2_f=0 b3_f=0
            b0_h=0 b1_h=0 b2_h=0 b3_h=0
            log_theta=0;


        if gifts = 1 then x1=1; else x1 =0;
        if gifts = 2 then x2=1; else x2 =0;
        if gifts = 3 then x3=1; else x3 =0;


      eta_f = b0_f + b1_f*x1 + b2_f*x2 + b3_f*x3;
      p_yEQ0 = 1 / (1 + exp(-eta_f));

      eta_h = b0_h + b1_h*x1 + b2_h*x2 + b3_h*x3;
      mu    = exp(eta_h);
      theta = exp(log_theta);
      r = mu/theta;

      if amount=0 then
         ll = log(p_yEQ0);
      else
         ll = log(1 - p_yEQ0)
              - lgamma(theta) + (theta-1)*log(amount) -                      theta*log(r) - amount/r;

      model amount ~ general(ll);
      predict (1 - p_yEQ0)*mu out=expect_zig;
      estimate ""scale"" theta;
    run; 
</code></pre>

<p>Then to estimate ""gift1"" versus ""gift2"" (b1 versus b2) we can write this estimate statement:</p>

<pre><code>estimate ""gift1 versus gift 2"" 
 (1-(1 / (1 + exp(-b0_f -b1_f))))*(exp(b0_h + b1_h)) - (1-(1 / (1 + exp(-b0_f -b2_f))))*(exp(b0_h + b2_h)) ; 
</code></pre>

<p>Can R do this?</p>
"
"0.0984374038697697","0.0820782681668123"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.0984374038697697","0.0820782681668123"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.062257280636469","0.064888568452305"," 22462","<p>Let's say that I am putting together a logistic regression model where I am predicting 
something (y) based on the day of the week. However, the model needs to account for each single day.</p>

<p>Therefore, instead of:</p>

<pre><code>y = B0 + B1*(day)
</code></pre>

<p>Where day is a categorical variable with 7 levels.</p>

<p>It would be:</p>

<pre><code>y = B0 + B1*(monday) + B2*(tuesday) + B3*(wednesday) + ... + B7*(sunday)
</code></pre>

<p>I'm basically thinking that each day needs a separate coefficient because each 
has a different affect on y. However, I think each will need to be a dummy variable 
so that for monday, 1 is for monday, and 0 for not monday, and so forth. </p>

<p>I'm just curious if there is a statistical logic to doing it the second way 
with separate days? What's the best way to do this?</p>
"
"0.0787499230958158","0.0820782681668123"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.170498584867618","0.165857685772546"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.0880450906325624","0.0917662935482247"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.118124884643724","0.123117402250218"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.181509604247567","0.189181060585383"," 29690","<p>I would like to construct predictions for a mixed model (logistic via glmer) on a new data set using only the fixed effects, holding the random effects to 0.    But I am having trouble setting up the model matrix to be able to calculate them.</p>

<p>Since the mer class doesn't have a predict method, and since I want to omit the random effects for predictions on the new data set, I think I need to construct a model matrix for the fixed effects of the same structure used in the original model, but using the new data. Then multiply by the fixed effect coefficients in the model.  </p>

<p>The fixed effect portion of my model formula contains factors and interaction terms between numeric fixed effects, so it's a little more complicated than just extracting the fixed variables from the matrix.  e.g. I need to ensure the factor contrast expansion is the same as the original, interaction terms are properly listed, etc.</p>

<p>So my question is: what is the more straightforward general approach for constructing a new model matrix that mimics the structure of the original model matrix used in creating the model?</p>

<p>I've tried model.matrix(my.model, data=newdata) but that seems to return the original model matrix, not one based on newdata.</p>

<p>Sample code:</p>

<pre><code>library(lme4)

cake2 &lt;- head(cake) # cake2 is ""new"" data frame for future predictions

# recipe is a fixed effect factor, temp is fixed effect numeric, replicate is random effect
m &lt;- lmer(angle ~ temp + recipe + (1 | replicate), data=cake)
summary(m)

nrow(cake2)         # but new data frame has 6 rows
nrow(cake)          # original data frame has 270 rows

# attempt to make new model matrix using different data frame
mod.mat.cake2 &lt;- model.matrix(m, data=cake2)
nrow(mod.mat.cake2) # 270 rows, same as orig data frame
</code></pre>

<p>I tried other methods like extracting the terms from the formula and building a new formula from that, but it seemed overly convoluted, and brittle in handling factors and interaction terms.</p>

<p>How can I get mod.mat.cake2 to be a fixed effect model matrix based on the formula in m, but using values from cake2?  Or is there an easier way to go about getting fixed-effect only predictions from an lmer model?</p>

<p>All help is appreciated.  Thank you.</p>
"
"0.186771841909407","0.151406659722045"," 31592","<p>I am a bit puzzled about the behavior of uncorrelated predictors in logistic regression. 
As in OLS, I thought that if two predictors (<code>rv1</code> and <code>rv2</code>) are uncorrelated, then the regression weights of <code>rv1</code> will not change from a regression that only includes <code>rv1</code> to one that includes <code>rv1</code> and <code>rv2</code>. 
However, it seems to be the case that this is not true in logistic regression and coefficients change between the two regression models, even if the predictors are uncorrelated.</p>

<p>I have pasted some R syntax below that demonstrates this behavior.</p>

<p>Why is this the case and how do the regression weights from the two regressions (the one with only rv1 and the other one with rv1 and rv2) relate to each other? Is there a way to know what the regression weight of rv1 will be if one knows the regression weight of rv1 in the regression that includes both predictors?</p>

<p>Thanks!
P.S. This post is crossposted at another unrelated stat answer site.</p>

<pre><code>library(MASS)

#generate lots of data (a little bit weird data handling, I know)
n &lt;- 10000
rdta &lt;- as.data.frame(mvrnorm(n=n,c(0,0),matrix(c(1,0,0,1),2,2),empirical=TRUE))
names(rdta) &lt;- c(""rv1"",""rv2"")

#confirm that preds are uncorrelated
cov(rdta$rv1,rdta$rv2)

rv1 &lt;- rdta$rv1
    rv2 &lt;- rdta$rv2

rv1ry &lt;- 1
rv2ry &lt;- 1

#generate binary data from known regression coefficients
ylinp &lt;- (1 / (1+exp(-(-1 + rv1*rv1ry + rv2*rv2ry))))
y &lt;- rbinom(n,1,ylinp) 
glm(y~rv1+rv2,family=binomial(link='logit'))
glm(y~rv1,family=binomial(link='logit'))
glm(y~rv2,family=binomial(link='logit'))

#confirm that OLS regression works as expected (regression weights do not change)
rv1y &lt;- .222
rv2y &lt;- .333
y &lt;- rv1y * rv1 + rv2y * rv2 + rnorm(n,0,.5)
lm(y~rv1+rv2)
lm(y~rv1)
lm(y~rv2)   
</code></pre>

<p>I am not sure if it is expected to also paste relevant output here, but here goes:
OLS results</p>

<blockquote>
  <p>lm(y~rv1+rv2)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1 + rv2)

Coefficients:
(Intercept)          rv1          rv2  
 0.001096     0.220051     0.333072  
</code></pre>

<blockquote>
  <p>lm(y~rv1)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1)

Coefficients:
(Intercept)          rv1  
 0.001096     0.220051  
</code></pre>

<blockquote>
  <p>lm(y~rv2)  </p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv2)

Coefficients:
(Intercept)          rv2  
 0.001096     0.333072  
</code></pre>

<p>Logistic regression results</p>

<blockquote>
  <p>glm(ry~rv1+rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1 + rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1          rv2  
     -1.001        1.916        2.469  
</code></pre>

<blockquote>
  <p>glm(ry~rv1,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1  
    -0.5495       1.0535  
</code></pre>

<blockquote>
  <p>glm(ry~rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv2  
-0.6538       1.6140  
</code></pre>
"
"0.0762492851663023","0.0794719414239026"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.0762492851663023","0.0529812942826018"," 32839","<p>I have a response variable with 2 categories and $500$ predictor variables. The $500$ coefficient $a_1, a_2, a_3, \ldots, a_{500}$ ranges $(-1, 1)$. Positive $a_i$ indicates category A; negative $a_i$ indicates category B. The larger the coefficient, the stronger it indicates its correspondent category. I get the coefficient from a researcher, who score each coefficient from -1 to 1 based on the importance and influence in classification.</p>

<p>To classify an object that have attributes $x_1, x_2, x_3,\ldots,x_{500}$, I am thinking of using logistic regression. But I do not know how to deal with continuous data (the range of the coefficient is continuous from -1 to 1). Is logistic regression viable? </p>

<p>If not, will someone help with other methods and post your code? I prefer using R.</p>
"
"0.0660338179744218","0.0917662935482247"," 33857","<p>I'm trying to calculate logistic regression coefficients by defining the log-likelihood function and using maximum likelihood.</p>

<p>In some cases when the initial (start) values I gave to the maximum likelihood were not correct I got wrong results for the logistic regression (different from the ones I get when using <code>glm</code> for example).</p>

<p>Given the input data and y values, what should be the optimum initial values for logistic regression (or, in other words, what are the values that are being used in <code>glm</code>)?</p>
"
"0.107832773203438","0.0936585811581694"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.132067635948844","0.1223550580643"," 34319","<p>Let's say I have the following logistic regression models:</p>

<pre><code> df=data.frame(income=c(5,5,3,3,6,5),
                  won=c(0,0,1,1,1,0),
                  age=c(18,18,23,50,19,39),
                  home=c(0,0,1,0,0,1))

&gt; md1 = glm(factor(won) ~ income + age + home, 
+           data=df, family=binomial(link=""logit""))
&gt; md2 = glm(factor(won) ~ factor(income) + factor(age) + factor(home), 
+           data=df, family=binomial(link=""logit""))
&gt; summary(md1)

Call:
glm(formula = factor(won) ~ income + age + home, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
      1        2        3        4        5        6  
-1.0845  -1.0845   0.8017   0.4901   1.7298  -0.8017  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.784832   6.326264   0.756    0.449
income      -1.027049   1.056031  -0.973    0.331
age          0.007102   0.097759   0.073    0.942
home        -0.896802   2.252894  -0.398    0.691

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178  on 5  degrees of freedom
Residual deviance: 6.8700  on 2  degrees of freedom
AIC: 14.87

Number of Fisher Scoring iterations: 4

&gt; summary(md2)

Call:
glm(formula = factor(won) ~ factor(income) + factor(age) + factor(home), 
    family = binomial(link = ""logit""), data = df)

Deviance Residuals: 
         1           2           3           4           5           6  
-6.547e-06  -6.547e-06   6.547e-06   6.547e-06   6.547e-06  -6.547e-06  

Coefficients: (3 not defined because of singularities)
                  Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      2.457e+01  1.310e+05       0        1
factor(income)5 -4.913e+01  1.605e+05       0        1
factor(income)6 -2.573e-30  1.853e+05       0        1
factor(age)19           NA         NA      NA       NA
factor(age)23   -1.383e-30  1.853e+05       0        1
factor(age)39   -3.479e-14  1.605e+05       0        1
factor(age)50           NA         NA      NA       NA
factor(home)1           NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178e+00  on 5  degrees of freedom
Residual deviance: 2.5720e-10  on 1  degrees of freedom
AIC: 10
</code></pre>

<p>So depending on the mode of the predictors, R produced different outputs. For factors, R splits out the coefficients into separate categories for the levels, but not for the model with numeric predictors. I'm wondering about a couple things.</p>

<ol>
<li><p>Is it ever useful to have the response categories expressed as individual rows?</p></li>
<li><p>To express the general regression equation, how does one go from a model with the categories expressed in an individual equation to an equation with a single B_i. So, for example, if gender has two coefficients, 3.5 for Male and 2.3 for Female, how does one use that in an equation such that (besides converting them into numeric values):</p></li>
</ol>

<p>Y = B0 + B1 (Gender)</p>
"
"0.0880450906325624","0.0917662935482247"," 34997","<p>I want to do an ordinal logistic regression in R without the proportionality odds assumption. I know this can be done directly using <code>vglm()</code> function in <code>R</code> by setting <code>parallel=FALSE</code>.</p>

<p>But my problem is how to fix a particular set of coefficients in this regression setup? For example, say the dependent variable $Y$ is discrete and ordinal and can take values $Y = 1$, $2$, or $3$. If the regressors are $X_{1}$ and $X_{2}$, then the regression equations are</p>

<p>$$ \begin{aligned} 
{\rm logit} \big( P(Y \leq 1) \big) &amp;= \alpha_{1} + \beta_{11}X_{1} + \beta_{12}X_{2} \\
{\rm logit}\big(P(Y \leq 2) \big) &amp;= \alpha_{2} + \beta_{21}X_{1} + \beta_{22}X_{2} 
\end{aligned} $$</p>

<p>I want to set $\beta_{11}$ and $\beta_{22}$ to $1$. Please let me know how can I achieve this. Also if <code>R</code> can't do this, could you also please let me know if I can achieve this in any other statistical software?</p>
"
"0.107832773203438","0.0936585811581694"," 38500","<p>I am currently working with a logistic semi-parametric model in R using the mgcv package.  The output from the model gives the standard log-odds coefficients; however, reviewers have requested marginal effects (like the ones in Stata using the margins command).  I would like to do average marginal effects (though, marginal effects at the means--modes for categorical covariates--would at least be a start).</p>

<p>I was wondering if anyone has an implementation for this in R for models that use the gam() function.  I have a rather large data set (1.2 million observations, a large number of discrete and continuous covariates, and fixed effects for 2,000 individuals).  Are these estimates even tractable, given the non-parametric treatment of a couple of the continuous covariates?  Any information would be helpful.</p>

<p>Here is what I am working with, though I get errors when attempting to do the bootstrapped SEs for the effects (comes from this helpful site probitlogit-marginal-effects-in-r-2/).  I am not sure how this treats the non-parametrically estimated smooths (but maybe these don't matter, since it is using the ""predict"" function?):</p>

<pre><code>mfxboot &lt;- function(modform,dist,data,boot=1000,digits=3){ #dist is the distribution choice of logit or probit
      require(mgcv)

x &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",data)
      # get marginal effects

pdf &lt;- ifelse(dist==""probit"",               
 mean(dnorm(predict(x, type = ""link"")))            
 mean(dlogis(predict(x, type = ""link"")))
 marginal.effects &lt;- pdf*coef(x)


bootvals &lt;- matrix(rep(NA,boot*length(coef(x))), nrow=boot)  
set.seed(1111)  
for(i in 1:boot){    
samp1 &lt;- data[sample(1:dim(data)[1],replace=T,dim(data)[1]),]    
x1 &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",samp1)    
pdf1 &lt;- ifelse(dist==""probit"",                   
mean(dnorm(predict(x1, type = ""link""))),                   
mean(dlogis(predict(x1, type = ""link""))))       
bootvals[i,] &lt;- pdf1*coef(x1)     
}

res &lt;- cbind(marginal.effects,apply(bootvals,2,sd),marginal.effects/apply(bootvals,2,sd))     
if(names(x$coefficients[1])==""(Intercept)""){        
res1 &lt;- res[2:nrow(res),]    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep=""""),res1)),nrow=dim(res1)[1])         
rownames(res2) &lt;- rownames(res1)        
} else {    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep="""")),nrow=dim(res)[1]))       
rownames(res2) &lt;- rownames(res)    
}     
colnames(res2) &lt;- c(""marginal.effect"",""standard.error"",""z.ratio"") 
      return(res2)
}
</code></pre>
"
"0.158725544414289","0.165434038373702"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.0984374038697697","0.102597835208515"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.107832773203438","0.0936585811581694"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.0311286403182345","0.0324442842261525"," 45754","<p>I have the following output from a logistic regression model.</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode10000:14849  0.0019334  0.0009006   2.147 0.031807 *  
our_bid:zipcode14850:19699  0.0022905  0.0009514   2.407 0.016064 *  
our_bid:zipcode19700:29999 -0.0009483  0.0008583  -1.105 0.269231    
our_bid:zipcode30000:31999 -0.0016309  0.0011028  -1.479 0.139161    
our_bid:zipcode32000:34999  0.0016241  0.0007856   2.067 0.038688 *  
our_bid:zipcode35000:42999  0.0023549  0.0008541   2.757 0.005831 ** 
our_bid:zipcode43000:49999  0.0007096  0.0008104   0.876 0.381286    
our_bid:zipcode50000:59999  0.0006533  0.0009269   0.705 0.480942    
our_bid:zipcode60000:69999  0.0030564  0.0008169   3.742 0.000183 ***
our_bid:zipcode7000:9999   -0.0027419  0.0012699  -2.159 0.030847 *  
our_bid:zipcode70000:79999  0.0013243  0.0007809   1.696 0.089921 .  
our_bid:zipcode80000:89999  0.0038726  0.0008006   4.837 1.32e-06 ***
our_bid:zipcode90000:96999  0.0038746  0.0007817   4.957 7.18e-07 ***
our_bid:zipcode97000:99820  0.0009085  0.0010044   0.905 0.365726    
---
</code></pre>

<p>I am using these coefficients to draw the predicted probabilities such that.</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>I realize that interpreting these interaction terms can be challenging. However, I generate the main regression equation and use that to formulate the probability curve. However, I'm not sure how to make sense of any of the ""our_bid:zipcode"" variables? </p>

<p>What about if my model output was: (instead saving zipcode as a factor, I make it a continuous variable)</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode             0.0019334  0.0009006   2.147 0.031807 *  
</code></pre>

<p>Would interpretation being easier with this approach? Keeping with the log-odds, how can I make sense of the log-odds effect that this model expresses for the interaction term?</p>
"
"0.0880450906325624","0.0688247201611685"," 46205","<p>I have a large amount of vegetation data that has been broken down into 13 habitat classes. I am trying to determine which vegetation tends to fall into or is absent from which habitat with any sort of significance. I have been put onto running a multinomial logistic regression, specifically using glmnet (as I have approximately 200 variables, and only about 260 observations).</p>

<p>Running cv.glmnet using the code:</p>

<pre><code>cv&lt;-cv.glmnet(data,Class,family=""multinomial"",nfolds=50,standardize=FALSE)
</code></pre>

<p>I get a list of numbers that I am struggling to understand, however I found the code:</p>

<pre><code>coef(cv, s=cv$lambda.1se)
</code></pre>

<p>Which returns the coefficients for each variable for each habitat class for the lambda that is 1 SE larger than the minimum Lambda value (which as far as I can tell the generally accepted lambda value).</p>

<pre><code>(Intercept)                                              0.7914263664   
Salix                                                    0.0000000000  
Mash                                                     0.0000000000   
Pin                                                      0.0000000000   
Choke                                                    .          
Betula                                                   0.0025260258   
Ideae                                                    0.0000000000   
Leather                                                  0.0000000000
</code></pre>

<p>What I'm wondering, using these coefficients, is it possible to state that those values with the largest magnitude (either closest to -1 and +1) are the most important in defining that class, which those close to 0 are unimportant, and those with periods were removed during the cv.glmnet. So in this case the plant ""Betula"" would be more influential than all others, and ""Choke"" was so uninfluential that it was removed? Also, no idea what intercept means, but I imagine I can find that one on my own.</p>
"
"0.0762492851663023","0.0529812942826018"," 47795","<p>I am currently carrying out an investigation to find if certain factors such as playing home or away or position of a footballer affects overall pass completion using logistic regression. I am using R to compute my data. In my current section in which I am trying to analyse uses the data of every player to convey a general conclusion to whether or not the position of a player affects the successfulness of pass completion. </p>

<p>so far I have computed:</p>

<pre><code>test.logit &lt;- glm( cbind(Total.Successful.Passes.All,Total.Unsuccessful.Passes.All) ~
                   as.factor(Position.Id), data=passes.data, family = ""binomial"")

summary(test.logit)
</code></pre>

<p>and my output was:</p>

<pre><code>Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    

(Intercept)              0.28482    0.01256   22.67   &lt;2e-16 

as.factor(Position.Id)2  0.99768    0.01438   69.38   &lt;2e-16 

as.factor(Position.Id)4  1.06679    0.01398   76.29   &lt;2e-16 

as.factor(Position.Id)6  0.68090    0.01652   41.23   &lt;2e-16 

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 32638  on 10269  degrees of freedom
Residual deviance: 26499  on 10266  degrees of freedom

AIC: 60422

Number of Fisher Scoring iterations: 4
</code></pre>

<p>the intercept is goalkeepers,position.Id 2 is for a defender, 4 = midfielder and 6 = striker</p>

<p>Is this a good set of results to come to a conclusion? and with the large deviances?</p>
"
"0.152498570332605","0.158943882847805"," 48410","<p>I'm modeling the effect of a categorical predictor on a binary dependent variable using logistic regression. I'm comparing models with/without the predictor using a likelihood-ratio test.</p>

<p>Two categories of the predictor are associated with values of 1 only (no 0s) for the dependent variable. Regression coefficients for these categories (expressed as changes in log(odds) compared to a reference category) are very large and highly suspicious, as this reference category is always associated with response values of 1 (but for one case), and I would thus expect regression coefficients close to 0 for these two categories. Comparisons between the reference category and other categories having more balanced distribution of 1 and 0s matches what I'm expecting from visual inspection of the data. 
Removing cases associated with these two 'problematic' categories does not change the logLikelihood of the models, but because it changes the number of parameters it affects the results of the likelihood ratio test.</p>

<p>Models are fitted using the glm function with binomial family and logit link in R.</p>

<p>My question therefore is: what model (or procedure) should I use to: </p>

<p>(1) test the global significance of the effect of the predictor on the dependent variable? Should I keep data from the 'problematic' categories in the model or not before conducted the likelihood ratio test?</p>

<p>(2) compare these two 'problematic' categories with others?</p>

<p>Any hint appreciated,</p>
"
"0.152498570332605","0.145698559277155"," 48766","<p>My logistic model <a href=""http://stats.stackexchange.com/q/48739/5509"">has been suspicious</a> due to enormous coefficients, so I tried to do a crossvalidation, and also do a crossvalidation of simplified model, to confirm the fact that the original model is overspecified, as <a href=""http://stats.stackexchange.com/a/48741/5509"">James suggested</a>. However, I don't know how to interpret the result (this is the model from the linked question):</p>

<pre><code>&gt; summary(m5)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj + resid_usili2 + 
    rok:obdobi + rok:kraj + obdobi:kraj + kraj:resid_usili2 + 
    rok:obdobi:kraj, family = ""quasibinomial"")
[... see http://stats.stackexchange.com/q/48739/5509 for complete summary output ]

&gt; cv.glm(na.omit(data.frame(orel, resid_usili2)), m5, K = 10)
$call
cv.glm(data = na.omit(data.frame(orel, resid_usili2)), glmfit = m5, 
    K = 10)

$K
[1] 10

$delta
[1] 0.2415355 0.2151626

$seed
  [1]         403         271  1234892862 -1124595763  -489713400  1566924080   147612843
  [8]  1879282918  -694084381  1171051622  2063023839 -1307030905  -477709428  1248673977
 [15]  -746898494   420363755  -890078828   460552896  -758793089  -913500073  -882355605
[....]
Warning message:
glm.fit: algorithm did not converge
</code></pre>

<p>I guess the delta is the mean fitting error, but how to interpret it? Is it a good or bad fit? BTW, the algorithm did not converge, maybe due to the enormous coefficients (?)</p>

<p>I tried a simplified model:</p>

<pre><code>&gt; summary(m)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj, family = ""quasibinomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7335  -1.2324  -0.1666   1.0866   3.1788  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -107.60761   48.06535  -2.239 0.025335 *  
rok            0.05381    0.02393   2.249 0.024683 *  
obdobinehn    -0.26962    0.10372  -2.599 0.009441 ** 
krajJHC        0.68869    0.27617   2.494 0.012761 *  
krajJHM       -0.26607    0.28647  -0.929 0.353169    
krajLBK       -1.11305    0.55165  -2.018 0.043828 *  
krajMSK       -0.61390    0.37252  -1.648 0.099593 .  
krajOLK       -0.49704    0.32935  -1.509 0.131501    
krajPAK       -1.18444    0.35090  -3.375 0.000758 ***
krajPLK       -1.28668    0.44238  -2.909 0.003691 ** 
krajSTC        0.01872    0.27806   0.067 0.946322    
krajULKV      -0.41950    0.61647  -0.680 0.496315    
krajVYS       -1.17290    0.39733  -2.952 0.003213 ** 
krajZLK       -0.38170    0.36487  -1.046 0.295698    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for quasibinomial family taken to be 1.304775)

    Null deviance: 2396.8  on 1343  degrees of freedom
Residual deviance: 2198.6  on 1330  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and it's crossvalidation:</p>

<pre><code>&gt; cv.glm(orel, m, K = 10)
$call
cv.glm(data = orel, glmfit = m, K = 10)

$K
[1] 10

$delta
[1] 0.2156313 0.2154078

$seed
  [1]         403         526   300751243  -244464717  1066448079  1971573706 -1154513152
  [8]   634841816 -1521293072 -1040655077   505710009  -323431793 -1218609191  1060964279
 [15]  1349082996   -32847357 -1387496845   821178952  -971482876  1295018851  1380491861
</code></pre>

<p>Now it converged. But the delta seems more or less the same, despite of the fact that this model looks much more sane! I'm confused by the crossvalidation now... please give me a hint on how interpret it.</p>
"
"0.062257280636469","0.064888568452305"," 51412","<p>I'm performing a multiple logistic regression with a large amount of columns. I want to set all the variables over a certain p value to have 0 coefficients, then test the model on the test data. </p>

<p>Since there's so many variables I'm using, it's impractical to manually go in and set the coefficients. So I'm looking for something like the following.</p>

<pre><code>log_results &lt;- glm(formula, data, family);
log_results_sig &lt;- get_sig_only(log_results, p value threshold);
</code></pre>

<p>This way I can use the results with the predict() function, and export the results easily for use in another program. </p>

<p>Additionally, if anyone knows a way to automatically extract the significant variables and refit with them, I would appreciate that too.</p>

<p>Thanks.</p>
"
"0.0984374038697697","0.0820782681668123"," 52475","<p>I have been running some binomial logistic regressions in R on a data set and I realised that the p-values of the estimated coefficients are not computed based upon a Normal distribution. For e.g. I have the following result from the glm() function:</p>

<pre><code>    Coefficients:

                                   Estimate Std. Error z value Pr(&gt;|z|) 
    (Intercept)                    -1.6127     0.1124 -14.347  &lt; 2e-16 ***
    relevel(Sex, ""M"")F             -0.3126     0.1410  -2.216   0.0267 *  
    relevel(Feed, ""Bottle"")Both    -0.1725     0.2056  -0.839   0.4013    
    relevel(Feed, ""Bottle"")Breast  -0.6693     0.1530  -4.374 1.22e-05 ***
</code></pre>

<p>I previously thought that the beta hats (estimated coefficients) are asymptotically normal and the p-value should be calculated using the normal distribution. But it seems from the p-values here that a t-distribution is used to compute them (I think). </p>

<p>I know that the asymptotic condition does not hold when sample size is small but if so, what is the process that causes the estimator to be distributed with a Students distribution? And how do I find out the degrees of freedom for such a distribution?</p>
"
"0.0762492851663023","0.0794719414239026"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"NaN","NaN"," 58152","<p>Suppose that we have a two-column data set. One column consists of a hundred x=0 and a hundred x=1, whereas the other one consists of y's (1 or 0 response). Besides, suppose that the P(Y=1|X=0) = 0.001 and P(Y=1|X=1) = 0.05. When fitting a logistic model (glm(y ~ x)) to this data set, why are we having a quasi separation problem here?</p>

<p>My doubts are as follows:</p>

<p>Since P(Y=1|X=0) = 0.001, we have P(Y=0|X=0) = 0.999. On the other hand, P(Y=1|X=1) = 0.05, so P(Y=0|X=1) = 0.95. As a result, we would see Y=0 with a very high chance when x=0 or 1. Why does this situation imply a quasi separation problem?</p>

<p>Next, why log[(0.05/0.95)/(0.001/0.999)] = 4 is the theoretical coefficient of x in the model?</p>

<p>Thanks.</p>
"
"0.132067635948844","0.137649440322337"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.062257280636469","0.064888568452305"," 60958","<p>it happened to me that in a logistic regression in R with <code>glm</code> the Fisher scoring iterations in the output are less than the iterations selected with the argument <code>control=glm.control(maxit=25)</code> in <code>glm</code> itself.</p>

<p>I see this as the effect of divergence in the iteratively reweighted least
squares algorithm behind <code>glm</code>. </p>

<p>My question is: under which criteria does <code>glm</code> stop the iterations and provides with a partial output? I was thinking about something like ""when the new coefficients-old coefficients &lt; epsilon, then STOP"". Is this the case? If not, what does make <code>glm</code> stop?
Thanks,
Avitus</p>
"
"0.0762492851663023","0.0529812942826018"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.165084544936054","0.172061800402921"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0984374038697697","0.0820782681668123"," 63436","<p>I sometimes have to vectorise the Huber weights from a robust regression and use them in a lm.
Recently I've had to do something similar for a logistic model but I'm slightly worried because I don't get very similar results</p>

<pre><code>library(robustbase)
data(vaso)
ROB &lt;- glmrob(Y ~Volume+Rate, family=binomial(""logit""), data=vaso)
ROB
glm(Y ~Volume+Rate,data=vaso,family=binomial(""logit""))
glm(Y ~Volume+Rate,data=vaso,weights=ROB$w.r,family=binomial(""logit""))
</code></pre>

<p>The coefficients from the weighted glm are more similar to the robust regression than the unweighted glm, but is there a way to make them the same? I can get the same results with a robust (rlm) and weighted lm but this doesn't seem to be the case with glm. I haven't looked at the glm robust regression in detail so what I'm asking may be impossible...</p>

<p>Thanks for your help</p>
"
"0.062257280636469","0.064888568452305"," 63494","<p>In <code>R</code> I have a categorical variable that I performed logistic regression on and got the following result:</p>

<pre><code>glm(formula = mortality ~ SMOKE, family = binomial, data = c.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2155  -0.2155  -0.2155  -0.1860   2.8515  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.0483     0.3189 -12.694   &lt;2e-16 ***
SMOKEN        0.2968     0.3559   0.834    0.404    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 492.45  on 2369  degrees of freedom
Residual deviance: 491.72  on 2368  degrees of freedom
AIC: 495.72

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Is the value for the intercept the same as <code>SMOKEY</code> (has a history of smoking)?</p>
"
"0.124514561272938","0.12977713690461"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.0880450906325624","0.0917662935482247"," 64242","<p>I have the following toy data:</p>

<pre><code>x &lt;- structure(c(2L, 2L, 3L, 1L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 2L, 3L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 3L, 3L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")

y &lt;- structure(c(2L, 2L, 3L, 1L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
2L, 3L, 2L, 1L, 2L, 2L, 3L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 1L, 2L, 
2L, 3L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 3L, 3L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 3L, 3L, 1L, 3L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 2L, 1L, 
3L, 2L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 1L, 3L, 
3L, 3L, 2L, 2L, 3L, 3L, 2L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")

z &lt;- structure(c(1L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 3L, 1L, 1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 1L, 1L, 
1L, 3L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 3L, 1L, 3L, 
3L, 1L, 1L, 1L, 3L, 1L, 3L, 3L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 3L, 
3L, 1L, 1L, 1L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")
</code></pre>

<p>I have replaced the 1's in <code>x</code> with 2's in <code>z</code> and vice versa. Now when I do an ordered logistic regression in R with <code>polr</code> (library MASS), I get the following coefficients: </p>

<pre><code>f1 &lt;- polr(y~x,Hess=TRUE)
f2 &lt;- polr(y~z,Hess=TRUE)

coef(summary(f1))
       Value Std. Error  t value
x2  25.95727  0.3028808 85.70127
x3  30.21524  0.5463144 55.30742
1|2 24.02167  0.3480269 69.02246
2|3 27.77068  0.3432316 80.90944

coef(summary(f2))
         Value   Std. Error       t value
z2  -21.495979 6.530398e-10 -3.291680e+10
z3    4.257964 8.119540e-01  5.244095e+00
1|2  -1.935599 3.567345e-01 -5.425880e+00
2|3   1.813399 3.411874e-01  5.314964e+00
</code></pre>

<p>It seems that something is not correct. Why relabeling the levels is changing dramatically the estimates for the SEs?</p>
"
"0.116472706986951","0.121395395733377"," 64788","<p>I performed multivariate logistic regression with the dependent variable <code>Y</code> being death at a nursing home within a certain period of entry and got the following results (note if the variables starts in <code>A</code> it is a continuous value while those starting in <code>B</code> are categorical):</p>

<pre><code>Call:
glm(Y ~ A1 + B2 + B3 + B4 + B5 + A6 + A7 + A8 + A9, data=mydata, family=binomial)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0728  -0.2167  -0.1588  -0.1193   3.7788  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  20.048631  6.036637   3.321 0.000896 ***
A1           0.051167   0.016942   3.020 0.002527 ** 
B2          -0.664940   0.304299  -2.185 0.028878 *  
B3          -2.825281   0.633072  -4.463 8.09e-06 ***
B4          -2.547931   0.957784  -2.660 0.007809 ** 
B5          -2.862460   1.385118  -2.067 0.038774 *  
A6          -0.129808   0.041286  -3.144 0.001666 ** 
A7           0.020016   0.009456   2.117 0.034276 *  
A8          -0.707924   0.253396  -2.794 0.005210 ** 
A9           0.003453   0.001549   2.229 0.025837 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 485.10  on 2206  degrees of freedom
Residual deviance: 417.28  on 2197  degrees of freedom
AIC: 437.28

Number of Fisher Scoring iterations: 7

 (Intercept)           A1           B2           B3           B4           B5           A6           A7           A8           A9 
5.093426e+08 1.052499e+00 5.143045e-01 5.929197e-02 7.824340e-02 5.712806e-02 8.782641e-01 1.020218e+00 4.926657e-01 1.003459e+00 

                   2.5 %       97.5 %
(Intercept) 3.703525e+03 7.004944e+13
A1          1.018123e+00 1.088035e+00
B2          2.832698e-01 9.337710e-01
B3          1.714448e-02 2.050537e-01
B4          1.197238e-02 5.113460e-01
B5          3.782990e-03 8.627079e-01
A6          8.099945e-01 9.522876e-01
A7          1.001484e+00 1.039302e+00
A8          2.998207e-01 8.095488e-01
A9          1.000416e+00 1.006510e+00
</code></pre>

<p>As you can see, all of the variables are ""significant"" in that their p values are below the usual threshold of 0.05. However looking at the coefficients, I'm not quite sure what to make of these results. It seems that although these variables contribute to the model, looking at the odds ratios, they don't seem to really seem to have much predictive power. Of note, when I calculated the AUC, I got approximately 0.8. </p>

<p>Can I say that this model is better at predicting against mortality (e.g. predicting that seniors will live past the prescribed period) compared to predicting for mortality?</p>
"
"0.062257280636469","0.0324442842261525"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.132067635948844","0.137649440322337"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"NaN","NaN"," 66946","<p>When you predict a fitted value from a logistic regression model, how are standard errors computed?  I mean for the <em>fitted values</em>, not for the coefficients (which involves Fishers information matrix).</p>

<p>I only found out how to get the numbers with <code>R</code> (e.g., <a href=""https://stat.ethz.ch/pipermail/r-help/2010-August/248241.html"">here</a> on r-help, or <a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">here</a> on Stack Overflow), but I cannot find the formula.</p>

<pre><code>pred &lt;- predict(y.glm, newdata= something, se.fit=TRUE)
</code></pre>

<p>If you could provide online source (preferably on a university website), that would be fantastic.</p>
"
"0.0984374038697697","0.102597835208515"," 67460","<p>I fitted the following multinomial regression:</p>

<pre><code>library(car)
p1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,2,2,3,4,3,3,4,3,4)

d1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,1,2,3,4,3,2,2,2,1)

d1&lt;-as.ordered(d1)

library(nnet)
test&lt;-multinom(p1~d1)
predi&lt;-expand.grid(d1=c(""1"",""2"",""3"",""4""))

pre&lt;-predict(test,predi,type=""probs"")
</code></pre>

<p>The output is a table of the predicted probabilities for every coefficient. I can also order the results for the confidence interval of the coefficents with:</p>

<pre><code>confint(test)
</code></pre>

<p>My question is: is it possible to get the results for the confidence interval for the predicted probabilities? It means for every amount in the ""pre"" output! 
PS: I found a similar question here in 
[""plotting confidence intervals""][1]<a href=""http://stats.stackexchange.com/questions/29044/plotting-confidence-intervals-for-the-predicted-probabilities-from-a-logistic-re"">Plotting confidence intervals for the predicted probabilities from a logistic regression</a></p>

<p>The main answer is perfect for my question, but I do not know how to combine with multinomial regression. 
I hope you understand my bad english :) Thank you for your help</p>
"
"0.0880450906325624","0.0917662935482247"," 69088","<p>For those of you familiar with <a href=""http://en.wikipedia.org/wiki/Exploratory_factor_analysis"" rel=""nofollow"">Exploratory Factor Analysis</a> (EFA) and <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> (RF), I have recently had an idea of combining these two methods to reduce the number of potential predictor variables for use in a parsimonious binary logistic regression model. For the purposes of this post, assume large <em>n</em> (200k or more) and 1000 potential predictor variables.</p>

<p>To employ this idea, the first step would be to perform an EFA with all potential predictor variables using <code>proc varclus</code>. Additionally, using <code>randomForest</code> to rank all potential predictor variables by <code>IncNodePurity</code> (<a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a>). </p>

<p>After these two methods are independently used, I propose retaining the variable with the largest <code>IncNodePurity</code> (from RF) within each factor (from EFA).</p>

<p>Does anyone have any thoughts/concerns with this methodology (or lack thereof) for feature selection? I am aware that this ""picking and choosing"" of methods may be complete garbage, but I had this random thought and wanted to share. Thanks!</p>
"
"0.062257280636469","0.064888568452305"," 69208","<p>After creating a <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> object using <code>randomForest</code> with around 500 candidate variables, I used <code>importance(object)</code> to display <em>IncNodePurity</em> for each of the candidate variables in relation to the binary outcome of interest (Payment/No Payment). </p>

<p>I am aware that <em>IncNodePurity</em> is the total decrease in node impurities, measured by the <a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a> from splitting on the variable, averaged over all trees. What I don't know is what should be the cutoff for candidate variables to be retained after making use of <code>randomForest</code> for feature selection in regards to binary logistic regression models. For example, the smallest <em>IncNodePurity</em> among my 498 variables is 0.03, whereas the largest <em>IncNodePurity</em> is 96.68.
In summary, I have one main question:</p>

<p>Is there a cutoff for <em>IncNodePurity</em>? If yes, what is it?</p>

<p>If no, how do you determine the cutoff? Do you simply take 10 candidate variables with the largest <em>IncNodePurity</em> if you want a model with only 10 predictor variables?</p>

<p>Any thoughts or references are greatly appreciated. Thanks!</p>
"
"0.116472706986951","0.121395395733377"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0762492851663023","0.0529812942826018"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.139211511597426","0.145095250022002"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.062257280636469","0.064888568452305"," 76850","<p>I am trying to do a multinomial logistic regression on some data that I generated. I am using R and the package mlogit. My data looks like the following:</p>

<pre><code>Class X1 X2  X3
V +0.0655197 +0.6418541 +1.8110291
V-0.6713268 -0.0262458 -0.3602958
V +0.2357610 -0.3602958 -0.6943458
M +0.3900129 +0.5583416 -1.7800082
M +0.5714871 -0.2767833 +0.5583416
M +1.0732807 -1.7800082 -0.3602958
S +0.9553640 -0.3602958 +0.6418541
S +0.1139899 +0.1356030 +0.3889280
S +0.4717283 -0.2852090 -1.1229880
</code></pre>

<p>My model is</p>

<pre><code>Class = B1*X1 + B2*X2 + B3*X3
</code></pre>

<p>So far, I have:</p>

<pre><code>library(mlogit);
allData &lt;- read.table(""Features/AllFeatures.dat"", header=TRUE);
allData$Class&lt;-as.factor(allData$Class);
mlData&lt;-mlogit.data(allData, choice=""Class"");
myData&lt;- mlogit(Class~1|X1 + X2 + x3, data = mlData);
print(summary(myData));
</code></pre>

<p>Which gives me:</p>

<pre><code>Coefficients :
                           Estimate Std. Error t-value  Pr(&gt;|t|)    
S:(intercept)                -0.6832392  0.0951834 -7.1781 7.068e-13 ***
V:(intercept)                -0.6696254  0.0943282 -7.0989 1.258e-12 ***
S:X1                         -0.1362492  0.1134039 -1.2015    0.2296    
V:X1                         -0.0052649  0.1128722 -0.0466    0.9628    
S:X2                         -0.0198451  0.0973608 -0.2038    0.8385    
V:X2                          0.0183261  0.0974789  0.1880    0.8509    
S:X3                          0.1728694  0.1110473  1.5567    0.1195    
V:X3                          0.0230260  0.1101147  0.2091    0.8344
</code></pre>

<p>However in the following: <a href=""http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf</a></p>

<p>The author gets:</p>

<pre><code>Coefficients :
Estimate Std. Error t-value Pr(&gt;|t|)
ic -0.00623187 0.00035277 -17.665 &lt; 2.2e-16 ***
oc -0.00458008 0.00032216 -14.217 &lt; 2.2e-16 ***
</code></pre>

<p>How can I change my function call to get the same? I want the actual coefficients for the model, not for the comparisons.</p>

<p>Also: What is the difference between 'wide' and 'long'? What form is my data in?</p>

<p>Also: Do you have some mathematical references for this type of multinomial logistic regression aimed at engineers?</p>

<p>Thanks!</p>
"
"0.0984374038697697","0.0820782681668123"," 80880","<p>I have seen several articles and CrossValidated questions on bootstrapping ( <a href=""http://stats.stackexchange.com/questions/41625/can-i-use-boostrapping-why-or-why-not"">this</a>, <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">this</a> or <a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients"">this</a> for example); there are a lot of theoretical and statistical explanations, however since they are so theory based, I am afraid I might be understanding the use wrongly. Hence my questions:</p>

<p>1) When I make a non-parametric bootstrapping (changing the sample for every run) with logistic regression on my data, I basically will end up with several different coefficients for each predictor for each run. Eventually I'll have the confidence interval for each predictor as well. I understand until that point. My question is; assuming that the distribution is normal, when I want to come up with a final model on practice, can I just take the mean of the confidence intervals for each predictor and consider this as my final model coefficient?</p>

<p>2) If the answer to question #1 is yes, is this the only way of choosing coefficients while bootstrapping? If not, what else? I encountered in a few more articles a method called ""bagging"". This seems to be my main purpose. </p>

<p>3) This one is more of a curiosity question: Can above methodology be applied to the categorical predictors when they are assigned with Weight Of Evidences? I know we can split the  categorical predictors into dummy variables; but how would I treat each coefficient if I want to use WOE methodology?</p>
"
"0.0440225453162812","0.0458831467741124"," 83364","<p>I have been reading a number of papers where researchers have created risk scores based on logistic regression models. Often they refer to ""<a href=""http://www.ncbi.nlm.nih.gov/pubmed/15122742"" rel=""nofollow"">Sullivan's method</a>"" but I have no access to this paper and the explanations provided are far from clear. I noticed that Dr. Harrell's excellent RMS package provides a nomogram function which is in a way similar to creating a risk score (albeit with a very pretty graphical output).</p>

<p>It seems after tinkering around with it that the way it works is by dividing the beta coefficients by the smallest beta coefficient and then multiplying a constant to create points for categorical variables. However I cannot for the life of me figure out what is going on with continuous variables. I've spent hours searching google without much luck, and I would appreciate if someone could shed some light on this for me. Thanks!</p>
"
"0.146006265130522","0.138342892773215"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.116472706986951","0.104053196342894"," 87872","<p>When I do a (logistic) regression in R, I run something like this:</p>

<pre><code>mydata &lt;- read.csv(""data.csv"")
mylogit &lt;- glm(a ~ c+d, data = mydata, family=""binomial"")
summary(mylogit)
</code></pre>

<p>As of a few months ago, the output for the coefficients might look like this:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6476     0.1898  -8.680  &lt; 2e-16 ***
c             2.4558     0.3414   7.194 6.29e-13 ***
d             2.3783     0.4466   5.326 1.01e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Trying it today (with a newer version of R), the output looks like the following:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6709     0.1924  -8.683  &lt; 2e-16 ***
c1            2.4961     0.3476   7.181 6.94e-13 ***
cc           18.2370   979.6100   0.019    0.985    
d1            2.4524     0.4630   5.296 1.18e-07 ***
dd                NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the ""c1"", ""cc"", etc fields mean?  I can't seem to find this any documentation, but perhaps I am looking in the wrong places?</p>
"
"0.0440225453162812","0.0458831467741124"," 89130","<p>Consider this logistic regression:</p>

<pre><code>mtcars$vs &lt;- as.factor(mtcars$vs)
log_reg_mtcars &lt;- glm(am ~ vs*wt +vs*mpg, family = ""binomial"", mtcars)
</code></pre>

<p>I tried using the effects package to extract some coefficients from the model:</p>

<pre><code>as.data.frame(effect(""vs"", log_reg_mtcars)) 

NOTE: vs is not a high-order term in the model
  vs       fit       se       lower     upper
1  0 0.3957869 2.916877 0.002150238 0.9950031
2  1 0.0336822 2.181009 0.000484832 0.7146704
</code></pre>

<p>Could anyone explain why the coefficients/standard error for <code>vs</code> given by this code are different. Or in other words, what is the effects package doing?</p>

<pre><code>as.data.frame(summary(log_reg_mtcars)$coef)[2,1:2]

     Estimate Std. Error
vs1 -40.70047   40.53197
</code></pre>
"
"0.107832773203438","0.112390297389803"," 89474","<p>I ran this ordinal logistic regression in R:</p>

<pre><code>mtcars_ordinal &lt;- polr(as.factor(carb) ~ mpg, mtcars)
</code></pre>

<p>I got this summary of the model:</p>

<pre><code>summary(mtcars_ordinal)

Re-fitting to get Hessian

Call:
polr(formula = as.factor(carb) ~ mpg, data = mtcars)

Coefficients:
      Value Std. Error t value
mpg -0.2335    0.06855  -3.406

Intercepts:
    Value   Std. Error t value
1|2 -6.4706  1.6443    -3.9352
2|3 -4.4158  1.3634    -3.2388
3|4 -3.8508  1.3087    -2.9425
4|6 -1.2829  1.3254    -0.9679
6|8 -0.5544  1.5018    -0.3692

Residual Deviance: 81.36633 
AIC: 93.36633 
</code></pre>

<p>I can get the log odds of the coefficient for <code>mpg</code> like this:</p>

<pre><code>exp(coef(mtcars_ordinal))
 mpg 
0.7917679 
</code></pre>

<p>And the the log odds of the thresholds like:</p>

<pre><code>exp(mtcars_ordinal$zeta)

       1|2         2|3         3|4         4|6         6|8 
0.001548286 0.012084834 0.021262900 0.277242397 0.574406353 
</code></pre>

<p>Could someone tell me if my interpretation of this model is correct:</p>

<blockquote>
  <p>As <code>mpg</code> increases by one unit, the odds of moving from category 1 of <code>carb</code> into any of the other 5 categories, decreases by -0.23. If the log odds crosses the threshold of 0.0015, then the predicted value for a car will be category 2 of <code>carb</code>.  If the log odds crosses the threshold of 0.0121, then the predicted value for a car will be category 3 of <code>carb</code>, and so on.</p>
</blockquote>
"
"0.139211511597426","0.145095250022002"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.158725544414289","0.139982647854671"," 90906","<p>I'm new to R and logistic regression and have to admit that I don't really know how to interpret the result. I'm trying to compute a pretty simple model with 2 predictors (A and B). When I first try to compute models with the predictors one by one they are both significant. When I put them together and add an interaction term they lose their significance (but the interaction term is weakly significant). I interpret this as A and B are overlapping and no longer significant when the oter parameter is hold constant. Right?</p>

<p>But now to the part I don't know how to interpret. I make predictions from my models (see code below) and then run t-tests for the predictions vs. the depending variable. I think this should give a hint on how good the model is (is there a better way?). When I do it this way I get a much lower p-value for the model with both A and B. I think this is contradictory. The first part tells me that A doesn't provide any significant information to the model when combined with B, but on the other hand I get much better predictions. I guess something is really wrong, but I can't figure out what. Can you help me?</p>

<pre><code>model1=glm(f~A, , family=binomial(link=""logit""))
model2=glm(f~B,   family=binomial(link=""logit""))
model3=glm(f~A*B, family=binomial(link=""logit""))
summary(model1)
summary(model2)
summary(model3)
p1=predict(model1, newdata=data, type=""response"", na.rm=TRUE)
p2=predict(model2, newdata=data, type=""response"", na.rm=TRUE)
p3=predict(model3, newdata=data, type=""response"", na.rm=TRUE)
t.test(p1~f)
t.test(p2~f)
t.test(p3~f)
</code></pre>

<p>Part of the output:  </p>

<pre><code>&gt; summary(model1)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.9756     0.3499  -5.647 1.64e-08 ***
A            -0.5898     0.2119  -2.784  0.00537 ** 

&gt; summary(model2)
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  8.354e-01  1.309e+00   0.638   0.5234  
B           -1.028e-04  5.122e-05  -2.007   0.0447 *

&gt; summary(model3)
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  1.254e+00  1.705e+00   0.735    0.462  
A            1.589e+00  9.743e-01   1.631    0.103  
B           -1.324e-04  7.333e-05  -1.805    0.071 .
A:B         -9.418e-05  4.632e-05  -2.033    0.042 *

&gt; t.test(p1~f)
t = -2.614, df = 11.83, p-value = 0.02286

&gt; t.test(p2~f)
t = -1.8702, df = 15.679, p-value = 0.08024

&gt; t.test(p3~f)
t = -4.9777, df = 17.344, p-value = 0.0001084
</code></pre>
"
"0.062257280636469","0.064888568452305"," 91747","<p>I have a logistic regression with data that are kind of like this:</p>

<pre><code>y &lt;- rep(c(""A"", ""A"", ""B""), each = 30)
x &lt;- c(     rep(1, 12), rep(2, 18), rep(3, 16), rep(4, 12), rep(5, 2),
            rep(1, 3), rep(2, 5), rep(3, 8), rep(4, 10), rep(5, 4)  )

da &lt;- data.frame(y = y, x = x)
table(da)
   x
y    1  2  3  4  5
  A 12 18 16 12  2
  B  3  5  8 10  4
</code></pre>

<p>I'd like to show that there are more A's than B's in <code>y</code> (instead of a Bernoulli with <code>p</code> = 0.5), even after controlling for <code>x</code>, so I fitted two logistic regression models and used an ANOVA to compare them.</p>

<pre><code>mlogis.x              &lt;- glm(y ~ x,     family = binomial, da)
mlogis.x.no_intercept &lt;- glm(y ~ x + 0, family = binomial, da)

summary(mlogis.x.no_intercept)
summary(mlogis.x)

anova(mlogis.x.no_intercept, mlogis.x, test = ""Chisq"")
</code></pre>

<p>I have a few questions:</p>

<ul>
<li>Does what I did make sense overall?</li>
<li>Is it okay to not have an intercept in the more basic model and then add it to the full(er) model?</li>
<li>The coefficient for <code>x</code> changes sign between the two models, how should I interpret this?</li>
</ul>
"
"0.108950241113821","0.12977713690461"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.181509604247567","0.17805276290389"," 92737","<p>In my data, I have two treatment conditions with repeated measures for each subject. I would like to run a mixed logistic regression separately for each of my two conditions where my binary outcome DV (dependent variable) is regressed on my IV (independent variable) and also have a random slope and intercept fitted for each subject.</p>

<p>So, I run the following:</p>

<pre><code>modelT0 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D0, family = binomial)
modelT1 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D1, family = binomial)
</code></pre>

<p>In the above, D0 and D1 are data sets restricted to treatment conditions 0 and 1, respectively. What I would like to do is compare the estimated fixed effects coefficient on IV across conditions to see if it significantly changes. To do this, I pool D0 and D1 into a single data set, D, and create a treatment indicator that takes value 0 in D0 and 1 in D1. I then run:</p>

<pre><code>model &lt;- glmer(DV ~ IV + treatment + treatment:IV + (1 + treatment|subject:treatment) 
               + (0 + IV + treatment:IV|subject:treatment), data = D, family = binomial)
</code></pre>

<p>I should be able to look at the fixed effects coefficient on treatment:IV to get my answer, but the issue is that for whatever combination of random effects I seem to specify, the coefficients from the pooled regression are slightly different from the regressions specified by treatment. So for instance, the fixed effect coefficient on treatment:IV plus the one on IV in model is not equal to the coefficient on IV in model1.</p>

<p>Any idea about what I might be doing wrong or how to answer the question I have? Thanks!</p>

<p>EDIT:</p>

<p>As per Henrik's suggestion, I'm copying the random effects output of the models below:</p>

<p>summary(modelT0):</p>

<pre><code>    Random effects:
    Groups    Name        Variance  Std.Dev. 
    subject   (Intercept) 1.412e-07 0.0003758
    subject.1 IV          1.650e+00 1.2844341
</code></pre>

<p>summary(modelT1):</p>

<pre><code>    Random effects:
    Groups    Name        Variance Std.Dev.
    subject   (Intercept) 0.00378  0.06148 
    subject.1 IV          0.26398  0.51379 
</code></pre>

<p>summary(model):</p>

<pre><code>    Random effects:
    Groups              Name         Variance  Std.Dev. Corr 
    subject.treatment   (Intercept)  0.0005554 0.02357       
                        treatment    0.0066042 0.08127  -0.88
    subject.treatment.1 IV           1.6500112 1.28453       
                        IV:treatment 1.0278663 1.01384  -0.93
</code></pre>
"
"0.062257280636469","0.0324442842261525"," 93390","<p>This question is about understanding the logistic regression output using R.  Here is my sample data frame:</p>

<pre><code>    Drugpairs             AdverseEvent  Y    N
1   Rebetol + Pegintron       Nausea   29 1006
2   Rebetol + Pegintron      Anaemia   21 1014
3   Rebetol + Pegintron     Vomiting   14 1021
4   Ribavirin + Pegasys       Nausea    5  238
5   Ribavirin + Pegasys      Anaemia   12  231
6   Ribavirin + Pegasys     Vomiting    1  242
7 Ribavirin + Pegintron       Nausea   15  479
8 Ribavirin + Pegintron      Anaemia    7  487
9 Ribavirin + Pegintron     Vomiting    9  485
</code></pre>

<p>This basically describes the number of times a particular drug pair has caused a medically adverse event. (<code>Y=yes, N=no</code>). I ran a logistic regression on this dataset in R using the following commands:</p>

<pre><code>mod.form    = ""cbind(Y,N) ~ Drugpairs * AdverseEvent""
glmhepa.out = glm(mod.form, family=binomial(logit), data=hepatitis.df)
</code></pre>

<p>The summary output was as follows (only showing the coefficients table):  </p>

<pre><code>                                                      Estimate Std. Error z value
(Intercept)                                          -3.8771     0.2205 -17.586
DrugpairsRibavirin + Pegasys                          0.9196     0.3691   2.491
DrugpairsRibavirin + Pegintron                       -0.3652     0.4399  -0.830
AdverseEventNausea                                    0.3307     0.2900   1.140
AdverseEventVomiting                                 -0.4123     0.3479  -1.185
DrugpairsRibavirin + Pegasys:AdverseEventNausea      -1.2360     0.6131  -2.016
DrugpairsRibavirin + Pegintron:AdverseEventNausea     0.4480     0.5457   0.821
DrugpairsRibavirin + Pegasys:AdverseEventVomiting    -2.1191     1.1013  -1.924
DrugpairsRibavirin + Pegintron:AdverseEventVomiting   0.6678     0.6157   1.085
</code></pre>

<p>I understand that the coefficients give probabilistic odds. I am curious however, as to why there are no coefficients for the <code>AdverseEventAnaemea</code> and also why is there no coefficient for any combination of the drugs and the adverse event anaemea? (The last 4 rows are the combination effects of drugs and adverse events)</p>
"
"0.139211511597426","0.130585725019802"," 93454","<p><strong>Base Data</strong>: I have ~1,000 people marked with assessments: '1,' [good] '2,' [middle] or '3' [bad] -- these are the values I'm trying to predict for people in the future. In addition to that, I have some demographic information: gender (categorical: M / F), age (numerical: 17-80), and race (categorical: black / caucasian / latino).</p>

<p><strong>I mainly have four questions:</strong></p>

<ol>
<li><p>I was initially trying to run the dataset described above as a multiple regression analysis. But I recently learned that since my dependent is an ordered factor and not a continuous variable, I should use ordinal logistic regression for something like this. I was initially using something like <code>mod &lt;- lm(assessment ~ age + gender + race, data = dataset)</code>, can anybody point me in the right direction?</p></li>
<li><p>From there, assuming I get coefficients I feel comfortable with, I understand how to plug solely numerical values in for x1, x2, etc. -- but how would I deal with race, for example, where there are multiple responses: black / caucasian / latino? So if it tells me the caucasian coefficient is 0.289 and somebody I'm trying to predict is caucasian, how do I plug that back in since the value's not numerical?</p></li>
<li><p>I also have random values that are missing -- some for race, some for gender, etc. Do I have to do anything additional to make sure this isn't skewing anything? (I noticed when my dataset gets loaded into R-Studio, when the missing data gets loaded as <code>NA</code>, R says something like <code>(162 observations deleted due to missingness)</code> -- but if they get loaded as blanks, it does nothing.)</p></li>
<li><p>Assuming all of this works out and I have new data with gender, age, and race that I want to predict on -- is there an easier way in R to run all of that through whatever my formula with new coefficients turns out to be, rather than doing it manually? (If this question isn't appropriate here, I can take it back to the R forum.)</p></li>
</ol>
"
"0.158725544414289","0.152708343114187"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.0440225453162812","0.0458831467741124"," 95795","<p>from what I have studied in the data mining course (please correct me if I'm wrong) - in logistic regression, when the response variable is binary, then from the ROC curve we can determine the threshold.</p>

<p>Now I'm trying to apply the logistic regression for an ordinal categorical response variable with  more than two categories (4).
I used the function <code>polr</code> in r:</p>

<pre><code>&gt; polr1&lt;-polr(Category~Division+ST.Density,data=Versions.data)
&gt; summary(polr1)

Re-fitting to get Hessian

Call:
polr(formula = Category ~ Division + ST.Density, data = Versions.data)

Coefficients:
               Value Std. Error t value
DivisionAP   -0.8237     0.5195  -1.586
DivisionAT   -0.8989     0.5060  -1.776
DivisionBC   -1.5395     0.5712  -2.695
DivisionCA   -1.8102     0.5240  -3.455
DivisionEM   -0.5580     0.4607  -1.211
DivisionNA   -1.7568     0.4704  -3.734
ST.Density    0.3444     0.0750   4.592

Intercepts:
    Value   Std. Error t value
1|2 -1.3581  0.4387    -3.0957
2|3 -0.5624  0.4328    -1.2994
3|4  1.2661  0.4390     2.8839

Residual Deviance: 707.8457 
AIC: 727.8457  
</code></pre>

<p>How should I interpret the Intercepts?
and how can I determine the threshold for each group?</p>

<p>Thanks</p>
"
"0.107832773203438","0.112390297389803"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.139211511597426","0.145095250022002"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"0.0998337488459583","0.121395395733377"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.0984374038697697","0.102597835208515"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0762492851663023","0.0794719414239026"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"0.107832773203438","0.0749268649265355","102695","<p>I'm using R to run some logistic regression. My variables were continuous, but I used cut to bucket the data. Some particular buckets for these variables always result in dependent variable being equal to 1. As expcted, the coefficient estimate for this bucket is very high, but the p-value is also high. There are about ~90 observations in either these buckets, and around 800 total observations, so I don't think it's a problem of sample size. Also, this variable should not be related to other variables, which would naturally reduce their p-values.</p>

<p>Are there any other plausible explanations for the high p-value?</p>

<p>Example:</p>

<pre><code>myData &lt;- read.csv(""application.csv"", header = TRUE)
myData$FICO &lt;- cut(myData$FICO, c(0, 660, 680, 700, 720, 740, 780, Inf), right = FALSE)
myData$CLTV &lt;- cut(myData$CLTV, c(0, 70, 80, 90, 95, 100, 125, Inf), right = FALSE)
fit &lt;- glm(Denied ~ CLTV + FICO, data = myData, family=binomial())
</code></pre>

<p>Results are something like this:</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.53831  -0.77944  -0.62487   0.00027   2.09771  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -1.33630    0.23250  -5.747 9.06e-09 ***
CLTV(70,80]     -0.54961    0.34864  -1.576 0.114930    
CLTV(80,90]     -0.51413    0.31230  -1.646 0.099715 .  
CLTV(90,95]     -0.74648    0.37221  -2.006 0.044904 *  
CLTV(95,100]     0.38370    0.37709   1.018 0.308906    
CLTV(100,125]   -0.01554    0.25187  -0.062 0.950792    
CLTV(125,Inf]   18.49557  443.55550   0.042 0.966739    
FICO[0,660)     19.64884 3956.18034   0.005 0.996037    
FICO[660,680)    1.77008    0.47653   3.715 0.000204 ***
FICO[680,700)    0.98575    0.30859   3.194 0.001402 ** 
FICO[700,720)    1.31767    0.27166   4.850 1.23e-06 ***
FICO[720,740)    0.62720    0.29819   2.103 0.035434 *  
FICO[740,780)    0.31605    0.23369   1.352 0.176236    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1037.43  on 810  degrees of freedom
Residual deviance:  803.88  on 798  degrees of freedom
AIC: 829.88

Number of Fisher Scoring iterations: 16
</code></pre>

<p>FICO in the range [0, 660) and CLTV in the range (125, Inf] indeed always results in Denial = 1, so their coefficients are very large, but why are they also ""insignificant""?</p>
"
"0.127082141943837","0.145698559277155","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.0880450906325624","0.0917662935482247","105633","<p>There are times when one might want to estimate a prevalence ratio or relative risk, in preference to an odds ratio, for data with binary outcomes - say, if the outcome in question isn't rare, so the RR ~ OR relationship doesn't hold.</p>

<p>I've implemented a model in R to do that, as follows:</p>

<pre><code>uni.out &lt;- glm(Death ~ onset, family= binomial(link=log), data=data)
</code></pre>

<p>But I'm continually getting convergence issues, even when providing starting values (such as the coefficient estimates pulled from a logistic regression), or turning up the number of allowed iterations. I've also tried <code>glm2</code> without any success.</p>

<p>The two ideas I have from here are to either fit a poisson model to the same data using a sandwich estimator for the variance, or fitting the model using MCMC and taking the standard error of the posterior (this is being used alongside multiple imputation, so I can't just report the posterior). The problem is, I have no idea how to implement either one of these in <code>R</code>, nor if they're the best solution.</p>

<p>Additionally, while using a model like:</p>

<pre><code>glm(Death ~ age, family= binomial(link=log),start=c(-3.15,0.03),data=data)
</code></pre>

<p>I'm regularly get an error message ""Error: cannot find valid starting values: please specify some"", but not always. What is generating this message?</p>
"
"0.176395628469996","0.194665705356915","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.176395628469996","0.17303618253948","106347","<p>I am somewhat familiar with various ways of testing mediation for factors in different types of regression analysis.  (I'm using R and currently working with a multilevel binary logistic regression.)  But now I have a situation in which I'd like to test whether one interaction between factors mediates another, and I'm not sure how this could be done properly.  </p>

<p>To give a simplified example of what I am interested in doing:</p>

<p>I have a multilevel binary model using student characteristics to predict pass/fail for students who are in a control versus experimental group.  </p>

<p>Let's say that the ""intervention"" appears to affect women more strongly than men, because the interaction gender*intervention is significant.  But then adding in a number of co-variates (and their interactions with the intervention),  results in a decrease in the magnitude of the coefficient and the significance of the fenale*intervention interaction, suggesting that once we control for these co-variates and their interactions with the intervention, differences between how the intervention ""affects"" men and women are no longer significantly different.  </p>

<p>I would like to be able to say something about mediation, and I understand how to test the individual factors for mediation of the gender*intervention interaction, but what if there is another interaction, such as (hours spent on childcare)*intervention, which I think may mediate the gender*intervention interaction?  Is there a way to test whether the first interaction mediates the second one?</p>

<p>EDIT:</p>

<p>As requested, here is a simple example equation which I think explains what I want to do.  For the purposes of simplicity, I am specifying this as a simple binary logistic regression model instead of a multilevel binary logistic regression model.  </p>

<p>Let's suppose there are three IV being used as factors:
intervention = whether student was in control or experimental group
gender
GPA</p>

<p>And the DV is whether the student passed or failed.  </p>

<p>And let's suppose we consider the following models (I'm just listing the factors here, without coefficients or error terms, for ease of readability):</p>

<p>M1: OUTCOME = INTERVENTION + GENDER + INTERVENTION*GENDER</p>

<p>M2: OUTCOME = INTERVENTION + GENDER + GPA + INTERVENTION*GENDER + INTERVENTION*GPA</p>

<p>Suppose in M1 that INTERVENTION*GENDER was significant, so that women benefited significantly more than men from the intervention.  </p>

<p>Then supposed that INTERVENTION*GENDER was not significant (and the female*experimental coefficient had a smaller magnitude) in M2, and we suspect that this is because INTERVENTION*GPA mediates INTERVENTION*GENDER.   </p>

<p>What I would like to know is if there is a way to test whether or not INTERVENTION*GPA mediates INTERVENTION*GENDER for these two models....</p>
"
"0.186771841909407","0.17303618253948","108315","<p>I am running multinomial logistic regression analysis on my data.  The response variable is the number of calves produced each year (0,1, or 2).  I am trying to evaluate the influence of the <em>X</em> variables on the odds of producing a calf.  My <em>X</em> variables are predation risk (WR; continuous), age of mother (age; categorical or continuous), time (wolf; categorical).</p>

<p>First, I have 4 different age classification schemes (i only show 2) - I want to know which one of the age of mother would be ""best"" to use. I could use it as continuous variable - or as categories based on biological reasoning for senescence in older moose (old ladies don't invest in reproduction as much).  So, I thought I would use a likelihood ratio test.</p>

<pre><code>library(mlogit)

modata.model1 &lt;- mlogit(no.C ~ 1 | 1, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model2 &lt;- mlogit(no.C ~ 1 | age, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model3 &lt;- mlogit(no.C ~ 1 | age2, data=modata, reflevel=""1"", na.action = na.omit)  
</code></pre>

<hr>

<pre><code> lrtest(modata.model2,modata.model3)

Likelihood ratio test

Model 1: no.C ~ 1 | age
Model 2: no.C ~ 1 | age2
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   4 -213.22                         
2   4 -207.57  0 11.309  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>QUESTION: to interpret this output - there was a significant difference in the loglikelihood when we comparing the continuous age to a categorical age with 2 classes.  The loglik is smaller for model 1 and therefore it would be better to use? Or do I have that backwards? </p>

<p>Next I was going to use the Walds test to evaluate nested models.  To see if the addition of a variable was worth it.</p>

<pre><code>modata.model7 &lt;- mlogit(no.C ~ 1 | age+WR, data=modata, reflevel=""1"", na.action = na.omit) 

modata.model8 &lt;- mlogit(no.C ~ 1 | age+WR+wolf, data=modata, reflevel=""1"", na.action = na.omit) 
</code></pre>

<hr>

<pre><code>Wald test

Model 1: no.C ~ 1 | age + WR
Model 2: no.C ~ 1 | age + WR + wolf
  Res.Df Df  Chisq Pr(&gt;Chisq)
1    244                     
2    242  2 0.5828     0.7472
</code></pre>

<p>QUESTION: this tells me that there is no significant improvement when there is an additional variable of wolf added??  So, then I can use the smaller model or do I use the one with the smaller Res.DF?</p>

<p>In addition to confirming my interpretations of the results I have 2 side questions...  </p>

<p>1)to get the null model for <code>mlogit</code> library - is my <code>modata.model1</code> correct?  I want the intercept only model to compare against.</p>

<p>2) Hosmer and Lemshow suggest by getting Wald values to get significance levels for each coefficient - in mlogit, thats the same as using <code>summary(model)</code> and there they provide the t-values with p instead of needing to do an additional Walds test? (NOTE in the below model i use 2 category age class instead of continuous)</p>

<pre><code>summary(modata.model8)

Call:
mlogit(formula = no.C ~ 1 | age2 + WR + wolf, data = modata, 
    na.action = na.omit, reflevel = ""1"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
    1     0     2 
0.652 0.244 0.104 

nr method
6 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.4E-06 
successive function values within tolerance limits 

Coefficients :
              Estimate Std. Error t-value Pr(&gt;|t|)   
0:(intercept)  0.38730    0.40144  0.9648 0.334657   
2:(intercept) -2.40317    1.04546 -2.2987 0.021523 * 
0:age21       -1.39607    0.43989 -3.1737 0.001505 **
2:age21        0.53952    1.07275  0.5029 0.615012   
0:WR          -1.46584    0.64797 -2.2622 0.023686 * 
2:WR          -0.19214    0.60856 -0.3157 0.752206   
0:wolf1        0.56055    0.63292  0.8857 0.375797   
2:wolf1        0.42642    0.72375  0.5892 0.555744   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -203.11
McFadden R^2:  0.053606 
Likelihood ratio test : chisq = 23.009 (p.value = 0.00079359)
</code></pre>
"
"0.139211511597426","0.145095250022002","110136","<p>I'm working on a prediction model for a continuous variable (amount of medicine injected) .I use R for modeling.My project flow is to multiply the prediction of a glm (logistic regression) model that is used to predict 0/1 if a medicine was injected at all with an lm (linear regression) model that is used predict amount of medicine injected - this model works rather good In R .My problem is that when I move this model to MSSQL I get different values for the prediction (i.e. for a random row the value in the R is 400 and in SQL the value for the same row is 640.The model in SQL is made by attaching the models coefficiants from the glm model to produce the glm prediction values and then multiplying it with the lm model prediction values. I don't understand why there is a difference if I use the same coefficients?</p>

<p>Here is the code for the lm and glm models in r:</p>

<pre><code>d7_lm&lt;-lm(Ttl_Inject~UserSource+IsNewIndividual+IsCross,data=train)
d7_glm&lt;-glm(Is_Injected~UserSource+IsNewIndividual+IsCross,data=train)
</code></pre>

<p>Here is a part of the r code for the prediction:</p>

<pre><code>demo$d7_lm_pred&lt;-predict(d7_lm,newdata=demo,type='response')
    demo$d7_glm_pred_response&lt;-ifelse(predict(d7_glm,newdata=demo,type='response')&gt;0.5,1,0)
demo$glm01_lm_response&lt;-demo$d7_lm_pred*demo$d7_glm_pred_response # this is used for a container of the prediction model's values.
</code></pre>

<p>Here is a part of the SQL code : </p>

<pre><code>select TOP 1000*, InjectionAmount_pred= (-2.213e -1.180e+00*(case when User='IAF' then 1 else 0 end)-1.665e+00*(case when UserSource='Viral' then 1 else 0 end)
+IsNewIndividual  *  1.167e+00+IsCross )

* IIF((1 / (1 + EXP(-(-1.346e-03+1.140e-02*(case when UserSource='IAF' then 1 else 0 end) -2.975e-03*(case when UserSource='Viral' then 1 else 0 end)
-IsNewIndividual  * 1.503e-04 +IsCross ))))&gt;0.5,1,0) 
</code></pre>
"
"0.0440225453162812","0.0458831467741124","110148","<p>I have run a <code>multinomial logistic regression</code> test for the interaction between species of deer, days a camera trap was in the field and type of reaction. </p>

<p>The model with the best AIC value was: </p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following </p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone knows why this is considered the best model when neither days nor deer react (r) or strongly react (sr) higher than chance?</p>
"
"0.0440225453162812","0.0458831467741124","110155","<p>I have run a multinomial logistic regression test for the interaction between species of deer, days a camera trap was in the field and type of reaction.</p>

<p>The model with the best AIC value was:</p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following</p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone would know what the interaction between species and days is in relation to r (reaction) and sr (strong reaction)? I know I can't reject the null hypothesis that there is no effect of species or days on r or sr but beyond that I'm lost!</p>
"
"0.152951761733879","0.147153441476392","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.21130821751815","0.220239104515739","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.158725544414289","0.165434038373702","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.170832568703593","0.166924465222397","112670","<p>I'm working on a dataset with the following variables:</p>

<pre><code>Y:  a boolean telling whether the subject has experienced a seizure
ID:  the id of the subject
Sess:  the subject's session number (a subject has been observed multiple times)
X3:  numeric measurements of the subject's behavior during the session 
X4:  ""
X6:  ""
</code></pre>

<p>The idea is to see if the behavior measurements(X3,X4,X6) can be used predict the Seizure status (Y) in the population.  If there were just one session per subject I'd model the data with a logistic regression, but since each subject has multiple sessions the observations cannot be assumed independent.</p>

<p>It seems a GEE logistic model makes the most sense for this dataset, but I'm having trouble understanding the results when compared to a regular glm.  When introducing correlation structure to the GEE equation the signs of the coefficients are reversed, and the predicted values are opposite of what they would be with an independent correlation structure.</p>

<pre><code>library(""geepack"")
#regular logistic model
mod0 = glm(Y~X3+X4+X6, family=binomial(""logit""), data=mice)
#gee logistic model, independent correlation structure
mod1 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""indep"", scale.fix=T, waves=Sess, data=mice)
#gee logistic model, exchangeable correlation structure
mod2 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""exchangeable"", scale.fix=T, waves=Sess, data=mice)
</code></pre>

<p>As expected, the parameter estimates of the glm model(mod0) and the independent correlation gee model(mod1) are the same.  But when an exchangeable correlation structure(mod2) is introduced, the estimates are completely different and change sign.</p>

<pre><code>&gt; 
&gt; summary(mod1)                    

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""indep"", scale.fix = T)

 Coefficients:
            Estimate Std.err  Wald Pr(&gt;|W|)    
(Intercept)   -1.494   0.459 10.59  0.00114 ** 
X3            -2.377   0.626 14.42  0.00015 ***
X4             1.090   0.398  7.50  0.00619 ** 
X6             1.233   0.403  9.36  0.00222 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = independenceNumber of clusters:   28   Maximum cluster size: 4 
&gt; summary(mod2)

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""exchangeable"", 
    scale.fix = T)

 Coefficients:
            Estimate Std.err Wald Pr(&gt;|W|)   
(Intercept)  -0.8928  0.4255 4.40   0.0359 * 
X3            0.1059  0.0383 7.64   0.0057 **
X4           -0.0427  0.0299 2.04   0.1535   
X6           -0.0528  0.0213 6.16   0.0131 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
Number of clusters:   28   Maximum cluster size: 4 
</code></pre>

<p>My biggest concern is that the predicted Y's of the two GEE models show opposite trends, i.e. behaviors that would predict a positive seizure status in mod1, predict a negative seizure status in mod2.    </p>

<pre><code>plot(mod1$fitted, mod2$fitted)
</code></pre>

<p><img src=""http://i.stack.imgur.com/nAogH.png"" alt=""scatter plot of fitted values from mod1, mod2""></p>

<p>Also, what's with the estimated correlation parameter of alpha = 1.09 in mod2?  Unless I'm interpreting this wrong, shouldn't this always fall between -1 and 1?</p>

<pre><code>Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
</code></pre>

<p>It seems odd to me that the results are completely flipped depending on whether or not the observations are assumed to have dependence structure.  Can anyone else offer insight on this behavior?</p>

<p>Here is the data:</p>

<pre><code>&gt; dput(mice)
structure(list(Y = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), ID = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 
5L, 5L, 6L, 6L, 6L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 
11L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 
19L, 19L, 19L, 20L, 20L, 20L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 
23L, 23L, 23L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L, 26L, 
26L, 26L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L), Sess = c(2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 
4L, 3L, 4L, 3L, 4L, 3L, 4L, 3L, 4L, 1L, 3L, 4L, 1L, 3L, 4L, 1L, 
2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 1L, 2L, 3L, 1L, 2L, 
3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 
4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L), X3 = c(0.511015060794264, 
0.898356533693696, 0.798280430157052, 1.31144372617517, 0.829923189452201, 
0.289089506643144, -0.763028944257538, -0.944459588217789, -1.16474609928919, 
-0.182524267014845, -0.338967193889711, -0.896037509887988, 0.00426073081308205, 
0.0576592603165749, -1.4984737260339, 1.34752684212464, 0.106461438449047, 
-0.108424579472268, -2.85991432039569, -0.230115838261355, -1.54479536845993, 
-1.23693649938367, -1.53704616612456, -1.04825100254239, 0.142768659484482, 
0.28358135516745, -0.236302896321009, 0.708743856986942, -0.507503006972081, 
0.401550711842527, -0.16928449007327, 0.867816722958898, 0.487459858572122, 
1.35172112260613, 0.14742652989871, 0.742155288774287, 0.348552056119878, 
-0.82489952485408, 0.0366834636917457, -0.731010479377091, 0.979544093857171, 
1.4161996712129, 0.661035838980077, 0.600235250313596, -1.10872641912335, 
-0.212101744145196, -0.919575135240643, -0.813993077336991, -0.547068540188791, 
-0.0260198210967738, -0.0962240349391501, -0.251025721625606, 
0.894913664382802, -0.21993004239326, 0.0628839847717805, 1.77763503559622, 
0.718459471596243, 0.984412886705251, 0.77603470471174, 0.486187732642953, 
1.78012655684609, 1.31622243756713, 1.29635178661133, 0.427995111986702, 
0.993748401511881, 0.387623239882247, 0.42006794384777, -0.815889182132972, 
-0.897540332229183, -1.041943103505, 0.379425827374942, -1.00707718576756, 
-0.889182530787803, 0.148432805676879, -0.287928359114935, -0.747152636892815, 
-1.41003790431546, -0.611571256991109, -1.02569548477235, -1.02700056733181, 
-1.45808867127733, -1.47973458605138, 2.23643966561508, 2.69397876103083, 
0.81841473415516, 2.12167589051282, 0.267133799544379, -0.326215175076418, 
-1.08788244901967, -1.18733017947214), X4 = c(0.050598970482242, 
-0.0279694583060402, 0.999225143631274, 0.199872317584803, 0.779316284168575, 
-0.3552692229881, -0.232161792808608, -0.333479851296274, -0.748169603107953, 
-0.57785843363913, -0.480747933235349, -0.740466500603612, -0.618559437949564, 
-0.591541699294345, -0.538855647639331, 0.431376763414175, -0.327931008191724, 
-0.469416282917978, -0.659224551441466, -0.55285236403596, -0.637082867133913, 
-0.780321541069982, -0.40539035027884, -0.54024676972473, -0.185562290173831, 
0.054439450703482, 0.624097793456316, 0.24018937319873, -0.264194638773171, 
-0.389590537012038, -0.42771343162755, -0.738790918078674, -0.122411831542788, 
0.600119921164627, 0.0442597161778152, -0.0955011351192086, -0.521259643827527, 
-0.550050365103255, -0.504566887441653, -0.506571005423286, 0.523650149759566, 
0.341920916685254, -0.396343801985993, -0.366532239883921, -0.739276449002057, 
-0.56054127343218, -0.587601788901296, -0.56798329186843, -0.454937006653748, 
-0.672730639942183, -0.564864467446687, -0.678853515419629, 0.573072971483937, 
0.596973680548765, 0.0403978228634349, 1.93617633381248, 2.54301964691615, 
0.363075891004736, 0.0205658396444095, 0.560923287570261, 1.24212005971229, 
2.32518793880728, 2.69979166871713, 0.626868716830008, 0.219463581391793, 
0.236477261174534, -0.115429539698909, -0.49754151674106, -0.40827433350644, 
-0.0433283703798658, -0.578451015506926, -0.714208713291922, 
-0.802387726290423, -0.836794085697031, -0.471800405613954, -0.668030208971065, 
-0.610945789491312, -0.780838257914176, -0.411360572155088, -0.494388869332376, 
-0.63231547268951, -0.743853022088574, 4.90627675753856, 3.38455016460328, 
0.859445571488139, 2.42212262705776, -0.324759764820016, -0.541581784452693, 
-0.485324968098865, -0.770539730529603), X6 = c(0.0150287583709043, 
-0.151984283645294, -0.347950002037732, 0.379891135882966, 0.129107019894704, 
-0.314047917638528, -0.516381047940779, -0.751192211830495, -0.884460389494645, 
-0.462363867892961, -0.397583161539858, -0.559528880497725, -0.842987555132397, 
-0.922797893301111, -1.01175722882932, 0.32346425626624, -0.610909601293237, 
-0.605155952259822, -1.29840867980623, 0.0793710626694382, -0.806959976634144, 
-0.674523251142452, -0.960113466801064, -0.783836535852452, -0.0665321645536412, 
0.482235339656537, -0.319499220427413, -0.115345965733089, -0.30806448545927, 
0.251747727063608, -0.305013811851957, -0.931916656036151, 0.415032839884745, 
0.337184728843034, 0.0584335852357015, -0.0712185313438638, 0.78632612201797, 
0.490831043388539, 0.8902425262631, 0.160088439571744, 0.90343086944952, 
0.928495373121098, -0.389259569427933, -0.304578433259833, -0.593364448723133, 
-0.411333868741105, -0.882691663964141, -0.91208274239495, -0.708633954450382, 
-0.339396626779965, -0.420927315080057, -0.421383857909298, 0.407474183771483, 
0.629710767351175, -0.438726438495567, 2.40977730548689, 2.47250810430208, 
0.783562677342961, 0.781304150896319, 0.563221804716475, 1.85514126067038, 
1.30723846671955, 1.94869625911545, 0.876751836832149, 0.626629859119409, 
0.067113945916172, 3.54280776301513, 0.0082773667305384, -0.311414481848668, 
-0.732779325538588, -0.594477082903005, -1.0385239418576, -1.04141739541776, 
-0.99472304141247, -0.599659297534257, -0.804801224448196, -1.13096932958525, 
-0.641957537144073, -0.722959119516237, -0.671146043591047, -0.714432955420477, 
-0.766750949574034, 1.20993739830475, 2.79011376379402, 2.64532317075082, 
2.54251033029822, -0.539516582572252, -0.6419726544563, -0.663768795224503, 
-0.644829826467113)), .Names = c(""Y"", ""ID"", ""Sess"", ""X3"", ""X4"", 
""X6""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>
"
"0.164717281867254","0.17167901505579","112760","<p>I have used <code>mlogit</code> package and I am trying to summarize the results I have from my model.  I have a question regarding the reference value and will get to that in a moment.</p>

<pre><code>redata.full &lt;- mlogit(no.C~ 1| WR+age+age2+BP+noC.1yr, data=redata, reflevel=""0"", na.action=na.fail)

no.C = number of offspring    
WR = risk
age+age2 = the non-linear relationship that as an individual ages their production decreases
BP = browsing pressure
noC.1yr = number of offspring produced the year before
</code></pre>

<p>I recognize that my data is ordinal in nature, but Im following other people's methods who have done this and used the reference based approach rather than ordinal logistic regression.  However, I am still shakey on justification other than citing the other person and saying ""he did it too!""  If anyone has a suggestion I would appreciate it.</p>

<p>My results for this model are: </p>

<pre><code>Call:
mlogit(formula = no.C ~ 1 | WR + age + age2 + BP + noC.1yr, data = redata, 
    na.action = na.fail, reflevel = ""0"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
       0        1        2 
0.233766 0.675325 0.090909 

nr method
5 iterations, 0h:0m:0s 
g'(-H)^-1g = 2.16E-07 
gradient close to zero 

Coefficients :
               Estimate Std. Error t-value Pr(&gt;|t|)  
1:(intercept) -0.281226   1.225763 -0.2294  0.81854  
2:(intercept) -0.605312   1.997179 -0.3031  0.76183  
1:WR           0.847273   0.518854  1.6330  0.10248  
2:WR           1.347976   0.689916  1.9538  0.05072 .
1:age          0.314075   0.275486  1.1401  0.25425  
2:age         -0.422368   0.395240 -1.0686  0.28523  
1:age2        -0.018998   0.014446 -1.3151  0.18847  
2:age2         0.022572   0.018949  1.1912  0.23359  
1:BP          -0.143720   0.173585 -0.8280  0.40770  
2:BP          -0.074553   0.331108 -0.2252  0.82185  
1:noC.1yr      0.574304   0.377821  1.5200  0.12850  
2:noC.1yr      1.251673   0.626033  1.9994  0.04557 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -116.6
McFadden R^2:  0.079844 
Likelihood ratio test : chisq = 20.236 (p.value = 0.0271)

exp(cbind(OddsRatio = coef(redata.full), ci))
              OddsRatio      2.5 %    97.5 %
1:(intercept) 0.7548580 0.06831155  8.341351
2:(intercept) 0.5459038 0.01089217 27.360107
1:WR          2.3332750 0.84394900  6.450831
2:WR          3.8496270 0.99577472 14.882511
1:age         1.3689929 0.79782462  2.349065
2:age         0.6554925 0.30209181  1.422317
1:age2        0.9811815 0.95379086  1.009359
2:age2        1.0228284 0.98553735  1.061530
1:BP          0.8661299 0.61634947  1.217136
2:BP          0.9281585 0.48504538  1.776078
1:noC.1yr     1.7758933 0.84686698  3.724076
2:noC.1yr     3.4961862 1.02497823 11.925441
</code></pre>

<p>I would like confirmation of my interpretations:
The model is better than a null - obtained from the likelihood ratio test.</p>

<p>Question: How do I test how well the model is actually working (i.e., goodness of fit)?  Hosmer-Lemshow test? Ive read warnings about using the McFaddin's Pseudo R where they really aren't applicable to multinomial regressions.  Ive found a HL test with <code>ResourceSelection</code> library and it says my model is NOT doing well at all.  Now what?</p>

<p>Interpretation:
WR and noC.1yr are the only variables that are coming out as slightly significant.  But this is only between the reference value of 0 and production of 2 calves.  It is not significantly different between 0 or 1 for these variables.  </p>

<p>Question: Ive been trying to find somewhere in the vignette what the t-value is - it is just a t-test?  How would I refer to the estimate as being significant?  ""The estimated odds for 2-offspring being produced versus 0 were 3.85 (95% CI = 1.0-14.88) which was significant (t= 1.99, P=0.05)""</p>

<p>Referring to my statement regarding setting the reference value.  When I run this exact same model using my other options of 0 or 1 offspring - I get completely different results of which variables are significant.  If I use 2 as the reference value then Age+WR+noC.yr are significant.  If I use 1, then Age only is sig.  So, which one to use?  I have read you want to pick one that is most relevant to your hypothesis, but in this case I could motivate any of the 3 levels.  </p>
"
"0.0440225453162812","0.0458831467741124","114583","<p>I want to simulate a binary response variable which depends on two normally distributed continuous variables, and I want to have more 1s than 0s in the response variable. I wonder how this can be done such that a logistic regression will not identify a significant interaction term.</p>

<p>My current approach in R looks like this:</p>

<pre><code>n = 1e5
x1 = rnorm(n)
x2 = rnorm(n)
y = x1+x2+rnorm(n)
y = ifelse(y &gt; 2, 1, 0)
df=data.frame(x1=x1, x2=x2, y=y)
summary(glm(y ~ x1*x2, df, family=binomial(logit)))$coefficients
</code></pre>

<p>This usually results in a highly significant interaction term, even though the y is just the sum of x1 and x2.
So how can I simulate a y which depends on both x1 and x2, but not on their interaction?</p>
"
"0.152498570332605","0.158943882847805","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.146006265130522","0.152177182050536","115188","<p>I am trying to look at whether 2 variables (one dichotomous categorical and one continuous) predict the occurrence of a dichotomous categorical dependent variable.</p>

<pre><code>dependent variable is LENIpos - 0 = no event, 1 = event
predictor variables are Hip.Prox.Femur - 0 = no hip fracture, 1 = hip fracture
                and     age (continuous)
</code></pre>

<p>Both predictor variables have significant p values in separate chi square test and Mann Whitney U test respectively.</p>

<p>When I run a logistic regression <code>glm(LENIpos ~ age + Hip.Prox.Femur, family = ""binomial)</code>, the variables come out as not significant. (1)</p>

<p>However, when I run the logistic regression with interactions <code>glm(LENIpos ~ age * Hip.Prox.Femur...)</code> (2), they are no both significant.  How is this to be interpreted?</p>

<p>Example R outputs:</p>

<p>(1)</p>

<pre><code>Call: glm(formula = LENIpos ~ age + Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -0.9346  -0.7826  -0.4952  -0.3374   2.1897  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)              -3.46888    1.00693  -3.445 0.000571 ***
age                       0.02122    0.01519   1.397 0.162535  
Hip.Prox.Femhip fracture  0.72410    0.57790   1.253 0.210212    

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 135.48  on 149  degrees of freedom
AIC: 141.48

Number of Fisher Scoring iterations: 5
</code></pre>

<p>(2)</p>

<pre><code>glm(formula = LENIpos ~ age * Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
        Min       1Q   Median       3Q      Max  
    -1.0364  -0.7815  -0.5373  -0.1761   2.3443  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)                  -5.89984    1.98289  -2.975  0.00293 **
age                           0.05851    0.02818   2.076  0.03788 * 
Hip.Prox.Femhip fracture      5.04990    2.46269   2.051  0.04031 * 
age:Hip.Prox.Femhip fracture -0.06058    0.03339  -1.814  0.06965 . 


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 131.82  on 148  degrees of freedom
AIC: 139.82

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0880450906325624","0.0688247201611685","117340","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 50</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Following a suggestion to a previous <a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">question</a> of mine, I have run LASSO (using R's glmnet package) in order to select the subset of exaplanatory variables that best explain variations in my binary outcome variable.</p>

<p>I have calculated lambda.min through cross-validation (cv.glmnet command) and got the correspondent coefficients for my explanatory variables. For 6 of my total 50 explanatory variables, the coefficients were non-zero. Are those coefficients comparable, i.e. can I say that the variables with the highest ones are the most important? If they are not comparable, can I run logistic regression (using glm) with those 6 variables and then compare them in terms of coefficients and p-values?</p>
"
"0.0933859209547035","0.0973328526784575","117450","<p>I have a confusing situation where I have strongly conflicting results from two ways of analyzing my simple data. I measure two binary variables from each participant, AestheticOnly and ChoiceVA. I want to know if AestheticOnly depends on ChoiceVA and whether this relation is different in two different experiments. Here is my participant count data:</p>

<pre><code>Experiment 1
                 AestheticOnly
                 0   1  All
ChoiceVA A      35   6   41
         V      20  13   33
         All    55  19   74

Experiment 2
                 AestheticOnly
                 0   1  All
ChoiceVA A      12  10   22
         V      31  11   42
         All    43  21   64
</code></pre>

<p>I run a logistic regression where AestheticOnly is modelled by ChoiceVA, Experiment, and the interaction:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ ChoiceVA*Experiment, data = d, family=binomial)
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ ChoiceVA * Experiment, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1010  -0.7793  -0.5625   1.2557   1.9605  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -3.3449     0.9820  -3.406 0.000659 ***
ChoiceVAV              3.5194     1.2630   2.787 0.005327 ** 
Experiment             1.5813     0.6153   2.570 0.010170 *  
ChoiceVAV:Experiment  -2.1866     0.7929  -2.758 0.005820 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apparently all factors are significant. But, this just doesn't make sense to me. For example, looking at the main effect of experiment should be equivalent to performing a Fisher's Exact test comparing 55 and 19 with 43 and 21 (bottom lines of each table). This is obviously not significant (p=.452). So why does the regression model give such a different result? Any help much appreciated.</p>
"
"0.139790356138221","0.145698559277155","117593","<p>This is a really simple problem I am having, yet for the life of me I can't find a solution searching around. In theory I can simply recode the data, but that is an extreme solution I would rather not use if I don't have to. </p>

<p>I am simply trying to do a logistic regression with an ordered factor as my predictor. For a toy data set, consider:</p>

<pre><code>  radiation leukemia other total
1         0       13   378   391
2       1-9        5   200   205
3     10-49        5   151   156
4     50-99        3    47    50
5   100-199        4    31    35
6       200       18    33    51
</code></pre>

<p>I want to execute the following:</p>

<pre><code>glm(cbind(leukemia,other)~radiation,data=leuk,family=binomial(""logit""))
</code></pre>

<p>That is, leukemia are the ""successes"" and other are the ""failures"". Basically, trying to predict dose-response relationship between radiation and the proportional mortality rates for leukemia. However, this model is oversaturated:</p>

<pre><code>Call:  glm(formula = cbind(leukemia, other) ~ radiation, family = binomial(""logit""), 
    data = leuk)

Coefficients:
     (Intercept)      radiation1-9    radiation10-49  radiation100-199  
         -3.3699           -0.3189           -0.0379            1.3223  
    radiation200    radiation50-99  
          2.7638            0.6184  

Degrees of Freedom: 5 Total (i.e. Null);  0 Residual
Null Deviance:      54.35 
Residual Deviance: -3.331e-15   AIC: 33.67
</code></pre>

<p>I don't want each level of radiation as a factor to be its own predictor variable; that makes no sense, especially when you only have a small number of data points (note, this isn't actually the real data I am using, this is just a toy example that is similar). In any case, how do I force R to simply consider the factor radiation as a single variable with multiple levels? For example, if I do the following:</p>

<pre><code>x&lt;-c(0,1,2,3,4,5)
glm(cbind(leukemia,other)~x,data=leuk,family=binomial(""logit""))

Call:  glm(formula = cbind(leukemia, other) ~ x, family = binomial(""logit""), 
    data = leuk)

Coefficients:
(Intercept)            x  
    -3.9116       0.5731  

Degrees of Freedom: 5 Total (i.e. Null);  4 Residual
Null Deviance:      54.35 
Residual Deviance: 10.18        AIC: 35.84
</code></pre>

<p>This is more in line with what I want. But I am nervous about using that x variable in the regression for fear of changing the interpretation of the results. Similarly, I'd prefer to avoid an irritating system of dummy variables. </p>

<p>How do I go about doing this? Or is there a better workaround altogether for studying this type of relationship that I am not considering?</p>
"
"0.134306229889014","0.139982647854671","117631","<p>I measure two binary responses from each participant (ChoiceVA = V or A, AestheticOnly = 0 or 1). There are two experiments (between-participant). I want to test the following hypotheses:</p>

<p>AestheticOnly depends on Experiment (main effect)
AestheticOnly depends on ChoiceVA (main effect)
The way AestheticOnly depends on Experiment depends on ChoiceVA (interaction)</p>

<p>Here is my data. The first number in each cell is the proportion of participants scoring 1 for AestheticOnly, and the second number is the n for participants in that cell.</p>

<pre><code>                         ChoiceVA               
                        A       V     All

Experiment  1      0.1463  0.3939  0.2568
                       41      33      74

            2      0.4545  0.2619  0.3281
                       22      42      64

            All    0.2540  0.3200  0.2899
                       63      75     138
</code></pre>

<p>Just from looking at the data it is pretty obvious that neither main effect is significant (e.g. for ChoiceVA, bottom row, .25 of 63 participants is not significantly different from .32 of 75 participants). In my naivity I thought perhaps I could test these hypotheses with a straightforward binary logistic regression:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ Experiment+ChoiceVA+Experiment*ChoiceVA, data = d, family=binomial )
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ Experiment + ChoiceVA + Experiment *
    ChoiceVA, family = binomial, data = d)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.1010  -0.7793  -0.5625   1.2557   1.9605 

Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)            -1.7636     0.4419  -3.991 6.57e-05 ***
Experiment2             1.5813     0.6153   2.570  0.01017 * 
ChoiceVAV               1.3328     0.5676   2.348  0.01887 * 
Experiment2:ChoiceVAV  -2.1866     0.7929  -2.758  0.00582 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Clearly, the main effects are not being tested here in the way I hoped. I believe that this model, in testing main effects, rather than testing e.g. ChoiceVA=A against ChoiceVA=V across both levels of Experiment, is confining itself to that comparison only when Experiment=1. Can a model be constructed that instead tests the main effects in the way I would like?</p>

<p>This is related to a previous question (<a href=""http://stats.stackexchange.com/questions/117450/logistic-regression-gives-very-different-result-to-fishers-exact-test-why"">Logistic regression gives very different result to Fisher&#39;s exact test - why?</a>), but when I asked it I understand this even worse than I do now and consequently the question was so unclear that I need to start again.</p>
"
"0.181509604247567","0.189181060585383","118141","<p>I'm trying to fit a logistic curve to cumulative data, derived from satellite imagery. Previously, I have point observation data which were either 0s or 1s. Os being 'forest' and 1s being 'non-forest'. These point observations existed for multiple images/dates. So I had one csv file with 'observation date' in once column and 'state' in another. Weights were also included, but these aren't relevant here. </p>

<p>This was the code I used:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Owner\\Desktop\\BV\\Deforestation_Analysis\\Ambaro_Ambanja\\Analysis\\CDM_Table_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%m/%d/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,weights=Weight,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>Now, rather than having multiple 0s and 1s for each date, I have simply number of non-forest pixels (and no weight). Here is the data I have:</p>

<p><img src=""http://i.stack.imgur.com/oFFIe.jpg"" alt=""enter image description here""></p>

<p>The 'State' field is literally the portion of non-forest pixels (18.6% of pixels were non-forest in 2014)</p>

<p>I tried runnning the following adaption of the original script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>I fully expected it to fail, because the data is no longer binomial. But while it did throw up a warning ('In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!'), the coefficients it spat out formed a curve which looked perfect. But I wasn't overly confident in this hash.</p>

<p>I've read that you can still use the binomial glm family if you feed R with a table containing successes (non-forest) and failures. So I came up with this adapted data:</p>

<p><img src=""http://i.stack.imgur.com/CMSd3.jpg"" alt=""enter image description here""></p>

<p>and the following adapted script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=cbind(Deforested, Total-Deforested)~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>It ran fine with no errors, but the trend it generated doesn't fit the data anywhere near as well as the hashed version:</p>

<p><img src=""http://i.stack.imgur.com/FXL6I.jpg"" alt=""Blue line is hased version; Grey line is adapted script; Red points are the data I&#39;m using to fit the model""></p>

<p>The blue line is hased version; grey line is adapted script; red points are the data I'm using to fit the model.</p>

<p>Why does the adapted version fit the point worse than the hashed version? Is R so clever that it just uses my fractional values how I want it to in the glm(family=binomial)?</p>

<p>Any advice greatly appreciated! Am not happy with how I got the blue trend and this work is very important to our study.</p>

<p>THANK YOU!!</p>
"
"0.274920707384579","0.279192976180845","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.062257280636469","0.064888568452305","121823","<p>I am quite new in the R universe, so please excuse me if the question is too simple..</p>

<p>I would like to perform a logistic regression on a marketing data set (only categorical variables), of the form [outcome, X1,X2,X3,X4,X5,X6]</p>

<p>I split the data set into a training set and a validation set.</p>

<p>My problem: Predictor X1 has originally 3 levels. The model using glm retains only 2 of these 3 levels.</p>

<p>When I try to run the model on the validation set (where X1 still has 3 levels) I get an error message stating that the factor X1 has now a new level. </p>

<p>How can I prevent the glm function from excluding factor levels? I don't mind if their coefficients are set to zero. </p>

<p>Thanks for any help on this. Tried all sites, but to no avail. </p>
"
"0.124514561272938","0.113554994791534","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"0.062257280636469","0.064888568452305","122055","<p>I am working on Sales data. i have binary variable win/loss the opportunities and rest are the activities done by sales force (sales guys) with 40+ variables (different types of activities done for the Opportunity)</p>

<p>I build the logistic model on the available data-set, and i found huge VIF value for different Xi's, then i perform stepwise variable reduction procedure for getting less variable in my model. At the end of this process i got 15 indep variable with dependent variable </p>

<p>Again i build same model on new data-set and again i m getting high VIF around(5610,3374.020669,3270.561737,2.512324,9.922235,...... etc.) for each variable </p>

<p>If u will look at the pairs graph and coefficient result please refer attached pic<img src=""http://i.stack.imgur.com/ZprNu.png"" alt=""Paire Graph""> </p>

<p><img src=""http://i.stack.imgur.com/slpvm.png"" alt=""Result""></p>

<p>Please suggest me what should I do further and how to come with my actual model with less error?</p>

<p>I am really stuck for further conclusion.</p>
"
"0.0762492851663023","0.0529812942826018","122580","<p>I'm running a logistic regression with backard selection method. I get coefficients with p-values>.10. Here's an example:</p>

<pre><code>    DF   Estimate   Error   Chi-Square  Pr &gt; ChiSq  Estimate    Exp(Est)
Intercept   1   -30,32       11,48       6,97        0,01            -     
v1  1    0,001       0,00        9,70        0,00        0,10        1,00   
v2  1   -0,001       0,00        2,84        0,09       -0,07        1,00   
v3  1    0,000       0,00        0,12        0,73        0,01        1,00   
v4  1   -0,000       0,00        0,11        0,74       -0,01        1,00   
v5  1   -0,000       0,00        0,74        0,39       -0,03        1,00   
v6  1    0,000       0,00        0,58        0,45        0,02        1,00   
v7  1   -0,005       0,00        3,98        0,05       -0,07        1,00   
v8  1    0,002       0,01        0,04        0,84        0,01        1,00   
v9  1   -0,016       0,05        0,09        0,76       -0,02        0,98   
v10 1    0,014       0,03        0,29        0,59        0,03        1,01   
v11 1    0,102       0,03        14,77       0,00        0,09        1,11   
v12 1    0,009       0,01        1,27        0,26        0,05        1,01   
v13 1   -0,017       0,01        2,39        0,12       -0,05        0,98   
v14 1   -0,005       0,01        0,48        0,49       -0,03        1,00   
</code></pre>

<p>My question is, if the algorithm selects best variables, how is it be possible that keeps the variables that have p-values greater than 0.1? I know that the effect is reflected in the value of the coefficient but the pvalue shows the probability that having that value in that coefficient is only a coincidence, and the coefficient is 0 (considering all the other variables). So why is still keeping those?</p>
"
"0.116472706986951","0.104053196342894","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.139211511597426","0.145095250022002","125130","<p>I realize that a similar question to this has been asked, but it was not ultimately resolved. I have tried the suggestions posted to that question <a href=""https://stackoverflow.com/questions/23347467/is-there-any-way-to-fit-a-glm-so-that-all-levels-are-included-i-e-no-refer"">here</a>, but have had no success. I am using the following code:       </p>

<pre><code>allinfa4.exp = glm(survive ~ year + julianvisit + class + sitedist + roaddist
+ ngwdist, family = binomial(logexp(alldata$expos)), data=alldata)
summary(allinfa4.exp)

 Call:
glm(formula = survive ~ year + julianvisit + class + sitedist + 
roaddist + ngwdist, family = binomial(logexp(alldata$expos)), 
data = alldata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.6435   0.3477   0.4164   0.4960   0.9488  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  4.458e+00  7.117e-01   6.265 3.74e-10 ***
year2013     3.680e-01  1.862e-01   1.976  0.04819 *  
year2014     2.136e-02  1.802e-01   0.119  0.90564    
julianvisit -5.714e-03  3.890e-03  -1.469  0.14192    
classb       2.863e-02  2.194e-01   0.131  0.89615    
classc      -2.394e-01  2.277e-01  -1.051  0.29304    
classd      -1.868e-01  2.479e-01  -0.754  0.45109    
classe      -4.500e-01  2.076e-01  -2.167  0.03021 *  
classf      -5.728e-01  2.005e-01  -2.858  0.00427 ** 
classg      -8.495e-01  3.554e-01  -2.390  0.01684 *  
classh      -1.858e-01  2.224e-01  -0.835  0.40351    
classi      -3.196e-01  4.417e-01  -0.724  0.46932    
sitedist    -2.607e-04  5.043e-04  -0.517  0.60520    
roaddist     6.768e-05  4.311e-04   0.157  0.87525    
ngwdist     -5.751e-05  9.456e-05  -0.608  0.54306
</code></pre>

<p>The main thing to note here is that I have two categorical variables, <code>year</code> and <code>class</code>, and R has combined the first level of each (2012 and class a) into a reference level intercept term. Not only do I need to know the intercept term for these levels individually, but I also need to know the base intercept terms itself (beta0), just as SAS produces. </p>

<p>I have tried changing the contrasts and deviation coding to accomplish this, but although doing so allows me to extract different levels, it changes the way they are calculated and still does not produce beta0. I've also tried adding +0 and -1, but this also does not provide what I need. Is what I'm trying to do simply impossible in R? It may seem like a strange request, but beta0 is necessary to convert the results of logistic exposure (special kind of logistic regression for nest-survival data) to daily survival rates. Any help would be hugely appreciated. Thanks!</p>

<p>Here is an example of SAS output I want to emulate (taken from a similar analysis done by my lab mate) :</p>

<pre><code>Parameter Estimates
Parameter   Estimate    Standard Error  DF  t Value Pr &gt; |t|              
beta0       7.8404      2.8479          19  2.75    0.0127  
NT         -3.8786      1.8831          19  -2.06   0.0534  
bgdensity  -0.1127      0.1614          19  -0.70   0.4935  
nwh         1.3466      1.4625          19  0.92    0.3687      
NRD        -2.6981      1.9496          19  -1.38   0.1824      
NAGW       -0.4898      2.2518          19  -0.22   0.8301      
</code></pre>
"
"0.0440225453162812","0.0458831467741124","125211","<p>I am new to machine learning/statistical modelling.</p>

<p>I am trying to run a classification on a highly sparse dataset with 100 features, most of which are categorical (TRUE/FALSE) with the remaining values missing. To handle missing values, I filled the missing spots with the text 'Nothing', thereby creating a new level.</p>

<p>Next, I am trying to run a logistic regression using a penalty (glmnet package). When I check the coefficients, I see dummy variables corresponding to 'Nothing' having the higher coefficients.</p>

<p>How should I remove these coefficients? What would be a better approach to this?</p>

<p>Or should I just use trees? Please suggest the best way forward.</p>

<p>Thanks!</p>
"
"0.21130821751815","0.220239104515739","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.139211511597426","0.116076200017602","126768","<hr>

<h2>Original</h2>

<p>I have fitted an ordered logistic regression in R using the <code>polr</code> function, but I am having some trouble bringing the model coefficients into Excel and getting the probabilities there. </p>

<p>For explanatory variables <code>FlowMonth2, Orders_Apt, GeoUnits, HomeOwner, Platform, CreditScore</code>, my coefficients for the model are as follows: </p>

<pre><code>                                Value Std. Error  t value
FlowMonth2Aug                 0.12321    0.03852   3.1990
FlowMonth2Dec                 0.31092    0.03854   8.0672
FlowMonth2Feb                 0.02497    0.03873   0.6447
FlowMonth2Jan                -0.01874    0.03940  -0.4757
FlowMonth2Jul                 0.02924    0.03886   0.7525
FlowMonth2Jun                -0.02618    0.04054  -0.6456
FlowMonth2Mar                 0.09369    0.03739   2.5054
FlowMonth2May                -0.08169    0.03581  -2.2811
FlowMonth2Nov                 0.32610    0.03889   8.3841
FlowMonth2Oct                 0.45240    0.03708  12.2009
FlowMonth2Sep                 0.22771    0.04015   5.6711
Orders_Apty                   0.03786    0.02206   1.7160
GeoUnits1                    -0.04070    0.03260  -1.2487
GeoUnits2                     0.11923    0.03735   3.1920
GeoUnitsOther                 0.30464    0.20803   1.4644
GeoUnits5                    -0.19669    0.01892 -10.3942
HomeOwnery                    0.16577    0.02828   5.8624
PlatformMobile               -0.32933    0.01631 -20.1882
CreditScore525 - 600          1.01909    0.02937  34.7036
CreditScore600 - 700          1.12578    0.02953  38.1284
CreditScore700 - 800          1.29098    0.03091  41.7694
CreditScore800 - 900          1.43500    0.03085  46.5179
CreditScore900+               1.33816    0.02851  46.9414
CreditScoreHit with No Score  0.33832    0.03424   9.8812
CreditScoreNo Hit             0.37199    0.06443   5.7737
</code></pre>

<p>The intercepts are </p>

<pre><code>Intercepts:
                Value    Std. Error t value 
0|1              -1.2788   0.0377   -33.9349
1|2              -0.6609   0.0371   -17.8175
2|3              -0.1683   0.0369    -4.5571
3|4               0.1520   0.0369     4.1159
4|5               0.3813   0.0370    10.3163
5|6               0.5615   0.0370    15.1714
6|7               0.7314   0.0371    19.7357
7|8               0.8551   0.0371    23.0486
8|9               0.9608   0.0371    25.8740
9|10              1.0510   0.0372    28.2760
10|11             1.1342   0.0372    30.4826
11|12             1.2607   0.0373    33.8295
12|13             1.4770   0.0374    39.5140
13|14             1.5414   0.0374    41.1957
14|15             1.5827   0.0374    42.2710
15|16             1.6127   0.0375    43.0505
16|Still Active   1.6358   0.0375    43.6499
</code></pre>

<hr>

<p>Now when I bring this into Excel, I bring in the coefficients, select certain values to add together, say <code>FlowMonth2 = ""Aug"", Orders_Apt = ""n"", GeoUnits = ""5"", HomeOwner = ""y"", Platform = ""Desktop"", CreditScore = ""800 - 900""</code>. </p>

<p>I add these values together to get my logit statistic, $T = \mathbf{x}\mathbf{\beta}$, and then I add this $T$ to each different intercept to get $\beta_{0, i} - T$ for $1 \leq i \leq 17$ where the $17$th stage is transition from 16 to Still Active. </p>

<p>I then take $\mathrm{logit}(\beta_{0, i} - T)$ or ${1 \over 1 + \exp(-[\beta_{0, i} - T])} = \Pr(\text{being in the $i$th stage})$</p>

<p>But when I try to do this in Excel, and compare it to the output of <code>predict</code> in R, then I can't get these values to match up? What am I doing wrong in Excel? </p>

<hr>

<h2>Edit</h2>

<p>To compare the values from R and Excel, it's by more than a rounding error that they differ: </p>

<p>R: </p>

<pre><code>0                                                0.048650293
1                                                0.037989009
2                                                0.047738406
3                                                0.041799312
4                                                0.035787006
5                                                0.031644868
6                                                0.032650167
7                                                0.025400235
8                                                0.022740618
9                                                0.020074660
10                                               0.019006405
11                                               0.029758088
12                                               0.052613086
13                                               0.015949280
14                                               0.010274309
15                                               0.007485204
16                                               0.005777627
Still Active                                     0.514661427
</code></pre>

<p>Excel: </p>

<pre><code>0|1 0.048648622
1|2 0.086640673
2|3 0.134381676
3|4 0.176177948
4|5 0.211958542
5|6 0.243615257
6|7 0.27626595
7|8 0.301669593
8|9 0.32439208
9|10    0.344464821
10|11   0.363487303
11|12   0.393228837
12|13   0.44584823
13|14   0.461809529
14|15   0.472089045
15|16   0.479571379
16|Still Active 0.485339204
</code></pre>

<hr>

<h2>Edit 2</h2>

<p>Why are there only 17 intercepts in Excel, but 18 predicted points in R? </p>
"
"0.256693344027883","0.259673526870329","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.0984374038697697","0.102597835208515","129657","<p>What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? </p>

<p>Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or ""with shrinkage"".</p>

<p>I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge.</p>

<p>There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential.</p>

<p>This could be done via <code>lme4</code> in <code>R</code>. However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model?</p>

<p>EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc.</p>

<p>EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts.</p>

<p>EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).</p>
"
"0.176090181265125","0.172061800402921","131456","<p>I'm exploring the effects of removing the intercept in a logistic regression model.</p>

<p>Assume a model:</p>

<p>$$logit(Y = 1) = \beta_1 x + \beta_2z + 0$$</p>

<p>with $x$ and $z$ being categorical variables with 2 levels each and no intercept.</p>

<p>I understood that having no intercept with categorical predictors produce coefficients that compare the $P(Y = 1)$ in each level of the two predictor against a null case where $P(Y=1) = 0.5$ or $logit(Y=1) = 0$.</p>

<p>I noticed a phenomenon that can understand. Using glm() function in R if you change the order of the variable in the right hand part of the formula, the coefficients change too. But even more oddly, the coefficient of the first variable is always the same.</p>

<p>Here's an <code>R</code> demo:</p>

<pre><code>y &lt;- as.factor(sample(rep(1:2), 30, T))
x &lt;- as.factor(sample(rep(1:2), 30, T))
z &lt;- as.factor(sample(rep(1:2), 30, T))

coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ z + x - 1, binomial))
#        z1         z2         x2 
#-0.1764783 -0.3099976  0.5025523 
</code></pre>

<p>As you can see the first predictors have the same coefficient while the other are different in the two models.</p>

<p>Here is what I expected and instead behave differently than what I though:</p>

<ol>
<li>Since every level of the two predictors is compared to the same null case, I expected to have the same coefficients in the two models, independently from the order in which I use them.</li>
<li>I expected to see the coefficients of every level of every predictor, instead the coefficient for the 1 level of the second predictor is not shown.</li>
<li>I therefore assume that only the first variable is compared against the null case, while the second is compared against a reference level; but what is this level? Is it $P(Y = 1 | X = 1 \cap Z = 1)$? Reproducing one of the models WITH the intercept we get:</li>
</ol>

<p>`(for some reason stackexchange don't understand the following is code without the tick)   </p>

<pre><code>coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ x + z, binomial))
#(Intercept)         x2          z2 
#-0.1764783   0.5025523  -0.1335192
</code></pre>

<p>As expected x1 become the intercept, and x2 is likely relative to x1. z1 is missing also in this case and z2 is the same as in the model without intercept.</p>

<p>Thus should I assume that the comparison against the null case $P(Y = 1) = 0.5$ is made only for the first variable in a formula, while the other are compared against the usual intercept?
Is this behavior normal?
What about the fact that the first coefficient has the same value whichever the order of the predictors in the formula?
What if I want to compare all level of each predictor against the null case and have a coefficient for all levels?
Or it's theoretically impossible for some reason I don't get?</p>
"
"0.186771841909407","0.183850943948198","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.0984374038697697","0.102597835208515","136040","<p>I'm implementing a logistic regression model in R and I have 80 variables to chose from. I need to automatize the process of variable selection of the model so I'm using the step function.</p>

<p>I've no problem using the function or finding the model, but when I look at the final model I find that some of the variables chosen by the step function are not significant (I look at this using the summary function and looking at the fourth column in $coef, this is the Wald Test). This is a problem because I need all the variables included in the model to be significant.</p>

<p>Is there any function or any way to get the best model based on AIC or BIC methods but that also consider that all the coefficients must be significant?
Thanks</p>
"
"0.107832773203438","0.0749268649265355","136094","<p>I was trying to get an intuition for the interpretation of the coefficients in a logistic regression that was intended to reproduce to some extent that presented in a youtube video (<a href=""http://youtu.be/vq-_4kWmzTo"" rel=""nofollow"">http://youtu.be/vq-_4kWmzTo</a>). So I created a fictitious data set reflecting the chances of getting accepted (Accepted: int: 0 0 ... 1 0) into a college as related to SAT scores (int 1136 1347 1504) and family/ethnic background (categories = ""red"" vs ""blue""). </p>

<pre><code>fit &lt;- glm(Accepted ~ Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded:</p>

<pre><code>                      OR     2.5 %    97.5 %
Backgroundblue 0.7088608 0.5553459 0.9017961
Backgroundred  1.7352941 1.3632702 2.2206569
</code></pre>

<p>The interpretation seemed easy: Red applicants have 1.7 times more chances of getting in over the rest; blue applicants were at a disadvantage, and had  7 over 10 odds of getting in.</p>

<p>However, the more complete model,</p>

<pre><code>fit &lt;- glm(Accepted ~ SAT.scores + Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded coefficients for background that are difficult to reconcile or interpret:</p>

<pre><code>                         OR        2.5 %       97.5 %
SAT.scores     1.008558e+00 1.006940e+00 1.010297e+00
Backgroundblue 8.730056e-06 8.459031e-07 7.634723e-05
Backgroundred  2.329513e-05 2.426748e-06 1.929259e-04
</code></pre>

<p>Can you help point out what I am missing? Thank you.</p>

<p>Thanks to the enlightening answer from Maarten below, I was able to make some progress, and obtain the correct Odds Ratios without and with the SAT confounder:</p>

<p>Here is just regressing to Background (""Red"" versus ""Blue""):</p>

<pre><code>fit &lt;- glm(Accepted ~ Background, data = dat, family = ""binomial"")
exp(cbind(Odds_Ratio_RedvBlue = coef(fit), confint(fit)))

                        Odds_Ratio_RedvBlue             2.5 %       97.5 %
(Intercept)             0.7088608                     0.5553459   0.9017961
Backgroundred           2.4480042                     1.7397640   3.4595454
</code></pre>

<p>Which brought up a couple of additional questions (probably very basic): 1. Why is the Odds Ratio of the (Intercept) - 0.7088608 - the same for this model as for the <em>Odds</em> - as opposed to <em>Odds Ratio</em> - of the Background Blue in the model without the intercept above? And 2. Shouldn't the OddsRatio of Blue and Red be the reciprocal of each other $OddsRatioBlue = 1 / OddsRatioRed$?</p>
"
"0.15913201254311","0.154010708217364","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.116472706986951","0.121395395733377","139988","<p>For example, if you have a logistic regression on certain dataset:</p>

<pre><code>fit &lt;- glm(y ~ x, data = test, family = ""binomial"")
</code></pre>

<p>If you do <code>predict(fit, newdata, type = ""link"", se = TRUE)</code>, you will get a column named <code>se.fit</code>, which is the standard error for each predicted y value.</p>

<p>My questions are:  </p>

<ol>
<li><p>How is the MSE value for the link function is computed here?  </p>

<p>The variance of the fitting coefficients are basically the MSE times the variance-covariance matrix, there should be a way to compute the MSE value first. But for response variables that have 0 and 1 values, the link function corresponds to 0 and infinity. In this case, how does the model compute this value? Is there any way I can get the MSE value for the <code>glm</code> fitting in R?</p></li>
<li><p>Is <code>se.fit</code> the standard error for the link function value of the fitted line at point <code>x0</code>, or the standard error for the predicted link function value of <code>y</code> at point <code>x0</code>?</p></li>
</ol>
"
"0.0440225453162812","0.0458831467741124","140509","<p>I used logistic regression and found that my model fits well: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6434  -1.4623   0.8704   0.9013   1.0066  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.41595    0.02115   19.67   &lt;2e-16 ***
init_att_cnt  0.02115    0.00146   14.48   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 154956  on 122239  degrees of freedom
Residual deviance: 154746  on 122238  degrees of freedom
AIC: 154750
</code></pre>

<p>The chi-squared test is hightly statisticaly significant: <code>p = 9.642755e-48</code>. I decided to check the Nagelkerke $R^2$ statistic, </p>

<pre><code>R2 &lt;- R2/(1-exp((-mylogit$null.deviance)/n))
</code></pre>

<p>but it was $R^2 = 0.001350927$. This is unbelievable, why is $R^2$ so small, if my model fits well?</p>
"
"0.187712763944569","0.176081755696027","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.116472706986951","0.121395395733377","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"0.170832568703593","0.17805276290389","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.0880450906325624","0.0917662935482247","143943","<p>I have a need to do realtime predictions for individual rows of data based on a previously computed randomForest algorithm.  How can I run the ""predict"" command without recomputing ""fit"" on the entire training data set each time?  </p>

<p>I am using R and here's the line of code that computes ""fit"" by applying the randomForest algorithm on the training set.</p>

<pre><code>fit &lt;- randomForest(formula2, data=training, importance=TRUE, ntree=2000, na.action = na.omit)
</code></pre>

<p>And here's the predict command - I want to be able to run this without having to recompute fit every time.  Is this possible?</p>

<pre><code>outp_rf &lt;- predict(fit, testing)
</code></pre>

<p>For LogisticRegression, I know the coefficients so I can rerun the logistic function to compute the outcome.  However not sure how I can do it for RandomForest.</p>
"
"0.0440225453162812","0.0458831467741124","144152","<p>While I am reasonably comfortable with performing and interpreting the output from logistic regression using glm in R, I had a question about the mechanics of the calculation to better understand what is going on.</p>

<p>I am trying to fit a logistic model,</p>

<p>$\log \large( \frac{p}{1-p} \large) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$</p>

<p>where the input data consists of single rows for each observation with success/failure coded as 0/1 and the dependent numerical values $x_1,x_2$.</p>

<p>How are the individual 0/1 values converted into the LHS of equation above and used for the fitting of the coefficients?</p>
"
"0.0440225453162812","0.0458831467741124","144603","<p>I have built a logistic regression where the outcome variable is being cured after receiving treatment (<code>Cure</code> vs. <code>No Cure</code>). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. </p>

<p>In R my logistic regression output looks as follows: </p>

<pre><code>Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.2735     0.1306   9.749   &lt;2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75
</code></pre>

<p>However, the confidence interval for the odds ratio <strong>includes 1</strong>:</p>

<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167
</code></pre>

<p>When I do a chi-squared test on these data I get the following:</p>

<pre><code>data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365
</code></pre>

<p>If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:</p>

<pre><code>Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)
</code></pre>

<p>My question is: Why don't the p-values and the confidence interval including 1 agree? </p>
"
"0.0762492851663023","0.0794719414239026","145315","<p>I have age as a covariate in my material. A continuous variable. The age varies between 18-70 years.</p>

<p>I'm into a logistic regression and do not really know how to treat the variable. As a linear effect or as a polynomial?</p>

<pre><code>   gender       passinggrade age    prog
1    man          FALSE      69     FRIST
2    man             NA      70     FRIST
3 woman             NA       65     FRIST
4 woman           TRUE       68      FRIST
5 woman             NA       65     NMFIK
6    man          FALSE      70     FRIST
</code></pre>

<p>my model;</p>

<pre><code>mod.fit&lt;-glm(passinggrade ~prog+gender+age,family=binomial,data=both)
</code></pre>

<p>summary(mod.fit)</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.42653    0.28096   8.636  &lt; 2e-16 ***
progLARAA    0.44931    0.25643   1.752 0.079746 .  
progNASTK   -0.15524    0.26472  -0.586 0.557597    
progNBFFK    0.12091    0.65460   0.185 0.853462    
progNBIBK   -0.18850    0.37656  -0.501 0.616659    
progNDATK   -2.84617    0.73077  -3.895 9.83e-05 ***
progNFYSK    0.64391    0.19634   3.280 0.001040 ** 
progNMATK    0.18424    0.16451   1.120 0.262733    
progNMETK    0.22433    0.29086   0.771 0.440554    
progNMFIK    0.38877    0.42152   0.922 0.356373    
progNSFYY    0.97205    0.29320   3.315 0.000915 ***
progSMEKK   -0.58043    0.18185  -3.192 0.001414 ** 
genderman   -0.05623    0.10477  -0.537 0.591496        
age         -0.11780    0.01028 -11.462  &lt; 2e-16 ***
</code></pre>

<p>how would you treat the variable age?
and how should I interpret the results for age?</p>
"
"0.132067635948844","0.107060675806262","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.165084544936054","0.172061800402921","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"NaN","NaN","155459","<p>I'm trying to predict the outcome ""Decision"" in the function of Age, Gender, Occupation, .... </p>

<p>The independent variable ""Occupation"" is known to be significant. But when I do the logistic model, each sub-group (modality) of it is not.</p>

<p>Should I regroup the levels having the same value of estimated coefficient? (which I guess doesn't make many sense because the levels are not statistically significant)</p>

<p>The variable Occupation has 74 different sub-groups.</p>

<p>And another problem is that when checking the multicollinearity, the function VIF in R doest work, it produces the NaN value, may be its due to the large number of sub-groups of Occupation.</p>

<p><img src=""http://i.stack.imgur.com/ruScu.png"" alt=""Summary(Logistic Regression)""></p>
"
"0.062257280636469","0.064888568452305","156564","<p>I just developed a logistic regression model predicting customer churn (i.e how likely is a customer to leave us in the future?)</p>

<p>To understand the impact of my independent variables I calculated Odds ratio using the following function. </p>

<p><code>exp(trainingmodel$coefficients)</code></p>

<p>Where trainingmodel is the name of my model.</p>

<p>And I get the following results:</p>

<pre><code>AIRTIME 
9.789127e-01

Site.Report.By.Vehicle1
1.241823e+00
</code></pre>

<p>Both AIRTIME and Site Report are a feature of product we offer. In my dataset, AIRTIME is a continuous variable whereas Site.Report.By.Vehicle1 is a categorical variable with just two levels, ie. someone using the Site Report or not?</p>

<p>Can someone please help me to understand how to interpret the above number for AIRTIME and Site Report?</p>
"
"0.062257280636469","0.064888568452305","156780","<p>I am running a logistic regression with 5 continuous independent variables (IV). The problem is that IV4 when taken alone has a positive correlation with outcome (coeff > 0), and when taken with the other variables has a negative correlation (coeff &lt; 0). I evaluated correlation between IV4 and the other variables, and the results are: 
IV4 vs. IV1 (-0.51), IV4 vs. IV2 (-0.48), IV4 vs. IV3 (0.61) and IV4 vs. IV5 (0.73).</p>

<p>I ran other logistic regressions <em>eliminating one at a time all the other variables</em> to look if one of them was responsible for the sign change, and I noticed that when eliminating IV1, the sign of V4 coefficient became positive.</p>

<p>Thus, it seems that IV1 changes the sign of the coefficient of IV4. 
Is there someone who knows what might be the cause and (possibly) the solution?</p>

<p>Practically, do I have to eliminate the IV4 (or IV1) from the model and explain why?</p>

<p>Thanks a lot for answering</p>

<p>Leonardo Frazzoni, MD</p>
"
"0.0660338179744218","0.0917662935482247","159316","<p>I have been running logistic regression in R, and have been having an issue where as I include more predictors the z-scores and respective p-values approach 0 and 1 respectively.  For example if have few predictors:</p>

<pre><code>&gt; model1
b17 ~ i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -6.9461     1.8953  -3.665 0.000247 ***
i74           0.6842     0.9543   0.717 0.473384    
i73           1.7691     4.8008   0.368 0.712502    
i72           0.5134     2.0142   0.255 0.798812    
i71          -0.6753     4.9173  -0.137 0.890771    
</code></pre>

<p>The results appear to be fairly reasonable; however, if I have more predictors:</p>

<pre><code> &gt; model1
b17 ~ i90 + i89 + i88 + i87 + i86 + i85 + i84 + i83 + i82 + i81 + 
i80 + i79 + i78 + i77 + i76 + i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.887e+02  3.503e+05  -0.001    0.999
i90          1.431e-01  1.009e+04   0.000    1.000
i89          8.062e+01  1.027e+05   0.001    0.999
i88          9.738e+01  7.398e+04   0.001    0.999
i87         -1.980e+01  9.469e+03  -0.002    0.998
i86          9.829e+00  1.098e+05   0.000    1.000
i85          5.917e+01  3.074e+04   0.002    0.998
i84         -2.373e+01  1.378e+05   0.000    1.000
i83          7.257e+00  2.173e+05   0.000    1.000
i82         -1.397e+01  1.894e+05   0.000    1.000
i81          6.503e+01  1.373e+05   0.000    1.000
i80          3.728e+01  4.904e+04   0.001    0.999
i79          1.010e+02  5.556e+04   0.002    0.999
i78         -2.628e+01  1.546e+05   0.000    1.000
i77          4.725e+01  3.027e+05   0.000    1.000
i76         -6.517e+01  1.509e+05   0.000    1.000
i74          1.267e+01  1.175e+05   0.000    1.000
i73          2.796e+02  5.280e+05   0.001    1.000
i72         -2.533e+02  4.412e+05  -0.001    1.000
i71         -1.240e+02  4.387e+05   0.000    1.000
</code></pre>

<p>I know it is hard to say exactly what is going on without seeing the data, but the predictors are all 5-point Likert Scale items.  However, are there any thoughts to what is occurring here?  I don't have much experience with logistic regression, so I apologize if the question seems naive, but is there a certain threshold of predictors where logistic regression falls apart due to having such a large amount of predictors what is ultimately a very small amount of variance?  Is the potentially a multi-co-linearity issue?  Finally, when I run OLS regression on the data I get results that make more sense (or at least appear to), is it okay/what are the consequences of running OLS regression on a binary outcome?  Thank you!</p>
"
"0.132067635948844","0.137649440322337","160109","<p>I'm trying to fit a (logistic) regression model to predict the successful funding of crowdfunding ventures (0/1) based on a series of IV with different level of measurement. One of these IVs is a categorial variable that indicates the nature of the venture (like technology, media, etc.). From visually inspecting the IVs I can see that the other IVs take different values relative to the venture category.</p>

<p><strong>My question is, how do I properly incorporate the categorial variable into a regression model (using R)?</strong></p>

<p>I do realize I could simply add the categorial variable with all the other IVs like this:</p>

<pre><code>glm(success ~ IV1 + IV2 + IV3 + ... + ventureCategory +, data=data, family=""binomial"")
</code></pre>

<p>From what I understand this would only return the overall influence of the venture categories on successful funding, but not the potential interaction between venture category and IV1 to IVn.</p>

<p>If I was to include interactions, how would I know which interactions to add?
And if I assume the venture category to be influential on all the IVs do I include separate interactions for every IV with the ventureCategory like this</p>

<pre><code>glm(success ~ IV1 + Iv1:ventureCategory + IV2 + Iv2:ventureCategory + ... + ventureCategory, data=data, family=""binomial"")
</code></pre>

<p>It seems that model quickly becomes quiet messy and I got something mixed up here.</p>

<p>Finally, I read two studies, who include this exact variable differently.
Here is a link to one of them: <a href=""https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44"" rel=""nofollow"">https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44</a></p>

<p>Their model simply states they controled for categories (see table p.12) without providing any coefficients or p values. I read this in other papers too, but do not understand how the control variable actually contributes to the model in this case.</p>

<p>I also read through some of the threads here, but couldn't find anything that really helped me understand the underlying rationale.</p>

<p>Update: I have 5 IV in total, excluding the categorial one.</p>

<p>Best</p>
"
"0.0984374038697697","0.102597835208515","160545","<p>I recently ran two tests in R - one using glm() and one using lm() with the goal being to test the relationship between a binary response and binary predictor.  I ran glm() first and got an estimate of -0.68 for the predictor coefficient which I thought was pretty good.  P&lt;.05 and AIC of 653.  </p>

<p>When I ran lm() however I got an estimate of -.14, a multiple r-squared of .008, P&lt;.05.  </p>

<p>My understanding is that linear regression is usually a poor choice for a categorical response compared with logistic regression, but when is this not the case? I noticed in this post <a href=""http://statisticalhorizons.com/linear-vs-logistic"" rel=""nofollow"">http://statisticalhorizons.com/linear-vs-logistic</a> that the author states there's middle ground where it does make sense to use linear regression.  Are there any common rules (or rules of thumb you personally use) that determine when to try out linear regression on a categorical response?  Do any of these differ from the author's cases?</p>
"
"0.152951761733879","0.159416228266091","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.116472706986951","0.121395395733377","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.11739345417675","0.137649440322337","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.0984374038697697","0.0820782681668123","167440","<p>I am trying to get the bootstrapped confidence intervals of the coefficients for an ordinal logistic regression. 
Here below, my R code on fake data (reproducible example here below). This one does not work properly.</p>

<p>I suppose I need to enter a list of data with one line for each of the 20 subjects (this is the most simple way to proceed). Then the bootstrap with randomly select 20 rows (using sampling with replacement) to generate a new data set with 20 rows. That data set is converted into a new table of counts and a coefficient value is computed from that new â€œbootstrappedâ€ table.  This is repeated for each bootstrap sample. I can't get it! Thanks for your help.</p>

<pre><code>####################
library(rms)
x=c(1,2,3,2,3,1,2,3,3,3,2,2,1,2,1,2,3,2,1,2)
y=c(""math"",""eco"",""eco"",""lit"",""lit"",""eco"",""eco"",""math"",""math"",""lit"",""lit"",""math"",""eco"",""eco"",""math"",""lit"",""lit"",""math"",""eco"",""math"")
Dataset&lt;-data.frame(x,y)
h &lt;- orm(x ~ y)
h

# calculate coefficients using bootstrap
library(boot)
logit.bootstrap &lt;- function(data, indices) {
d&lt;-data[indices,]
fit&lt;-orm(x ~ y, data=data[indices,])
return(coefficients(fit))
}

# bootstrapping with 1000 replications
logit.boot &lt;- boot(data=Dataset, statistic=logit.bootstrap,R=1000)

# view results
logit.boot
plot(logit.boot)

# get 95% confidence interval
boot.ci(logit.boot, type=""all"")
############################
</code></pre>
"
"NaN","NaN","167794","<p>I wrote a script that create a logistic model, for Email opening probability, for each user name.</p>

<pre><code>form&lt;-formula(OpenOrNor~as.factor(TimeSend)
              +OpenWithSmartphoneind)
models&lt;- dlply(Data, ""User_id"", 
               function(df) {
                 model&lt;-glm(formula = form,family = binomial(""logit""),data = df,control = glm.control(epsilon = 1e-9, maxit = 500))
                 return(model)})
</code></pre>

<p>for some users it return me </p>

<pre><code>Call:  glm(formula = form, family = binomial(""logit""), data = df, weights = HistoryWeights, 
    control = glm.control(epsilon = 0.000000001, maxit = 500))

Coefficients:
              (Intercept)            as.factor(TimeSend)2      as.factor(TimeSend)3   as.factor(TimeSend)4  
                -22.61106                   20.21853                    0.07738                   20.54737  
          as.factor(TimeSend)5       as.factor(TimeSend)6      as.factor(TimeSend)7   as.factor(TimeSend)9  
                  0.19292                   -0.03624                    0.22013                    0.11837  
          OpenWithSmartphoneind  
                       NA  

Degrees of Freedom: 83 Total (i.e. Null);  76 Residual
Null Deviance:      190.6 
Residual Deviance: 166.8    AIC: 182.8
</code></pre>

<p>We can see that for OpenWithSmartphoneind their  is NA. and this is because their are no Opening With Smartphone at all In this user history.</p>

<p>My question is how it will impact on predict?<br>
And doe's it make different if OpenWithSmartphoneind in the formula will be a factor type or not?</p>
"
"0.0984374038697697","0.102597835208515","169291","<p>I have a logistic regression model below, predicting a dichotomous variable <em>type</em> from a single continuous predictor <em>fatigue</em>. Using the coefficients below I can obtain the increase in the odds of a positive <em>type</em> from a 1 unit increase in fatigue.</p>

<p>Also I believe by forming the model expression</p>

<pre><code>logit(type) = 0.3134 - 91.1171 * fatigue 
</code></pre>

<p>I can obtain the odds of a positive <em>type</em> for a given value of <em>fatigue</em> by plugging it in, say for a value <em>fatigue</em> = 1.</p>

<p><strong>However</strong>, what I want to do is to obtain the odds of a positive <em>type</em> for a range of <em>fatigue</em> values, i.e. <strong>&lt;= 0</strong>. Is this possible?</p>

<pre><code>## Call:
## glm(formula = type ~ fatigue, family = binomial(), data = myData)

## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6703 -1.3104 0.8369 1.0049 1.4695
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.3134 0.1496 2.095 0.0362 *
## fatigue -91.1171 36.3785 -2.505 0.0123 *
## ---
## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 282.84 on 210 degrees of freedom
## Residual deviance: 276.03 on 209 degrees of freedom
## AIC: 280.03
##
## Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.108950241113821","0.113554994791534","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.0762492851663023","0.0794719414239026","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.165084544936054","0.160591013709393","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.124514561272938","0.12977713690461","173076","<p>I was following the procedure in a statistics textbook to run a multinomial logistic regresion using <code>mlogit</code>. However, the Odds Ratios calculated seemed too high for some of the variables (>1000). Can someone take a look at this and check wether I am doing everything correctly? The data can be downloaded from <a href=""https://dl.dropboxusercontent.com/u/14303378/LogisticRegressionSample.csv"" rel=""nofollow"">here</a>. I prepared the data with the following commands:</p>

<pre><code>#read in the data
test&lt;-read.csv(file=""LogisticRegressionSample.csv"",sep="","")
#trasnform data into the correct form for mlogit
mlogitData&lt;-mlogit.data(test,choice=""Outcome"",shape=""wide"")
#build model
MLogitFit&lt;-mlogit(Outcome~1|V1+V2+V3+V4+V5+V6+V7+V8,reflevel=3,data=mlogitData)
#summary of the model
summary(MLogitFit)
#OddsRatios
data.frame(exp(MLogitFit$coefficients))
# confidence Interval of the odds Ratios
exp(confint(MLogitFit))
</code></pre>

<p>The summary of mlogit gives me:</p>

<pre><code>    Call:
mlogit(formula = Outcome ~ 1 | V1 + V2 + V3 + V4 + V5 + V6 + 
    V7 + V8, data = mlogitData, reflevel = 3, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
      Z       A       B 
0.43333 0.25556 0.31111 

nr method
7 iterations, 0h:0m:0s 
g'(-H)^-1g = 1.56E-06 
successive function values within tolerance limits 

Coefficients :
               Estimate Std. Error t-value  Pr(&gt;|t|)    
A:(intercept)  -6.74640    5.97451 -1.1292 0.2588147    
B:(intercept)  -7.12401    4.50350 -1.5819 0.1136759    
A:V1            3.65979    3.90808  0.9365 0.3490331    
B:V1            4.24363    3.25687  1.3030 0.1925822    
A:V2          -15.11554    6.92901 -2.1815 0.0291475 *  
B:V2           -4.88778    3.65249 -1.3382 0.1808302    
A:V3            1.71465    6.57907  0.2606 0.7943839    
B:V3            2.94335    3.96557  0.7422 0.4579497    
A:V4           -1.70660    1.58849 -1.0744 0.2826633    
B:V4           -1.67210    1.17575 -1.4222 0.1549820    
A:V5            1.18494    1.60760  0.7371 0.4610682    
B:V5            1.03084    1.25573  0.8209 0.4116971    
A:V6            8.28902    2.51631  3.2941 0.0009873 ***
B:V6            3.44578    1.91844  1.7961 0.0724727 .  
A:V7           -1.34395    2.67943 -0.5016 0.6159612    
B:V7            1.04803    1.95147  0.5370 0.5912343    
A:V8           -7.46263    4.12978 -1.8070 0.0707577 .  
B:V8            0.21861    2.13596  0.1023 0.9184810    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -64.636
McFadden R^2:  0.33149 
Likelihood ratio test : chisq = 64.1 (p.value = 1.0515e-07)
</code></pre>

<p>Running <code>data.frame(exp(MLogitFit$coefficients))</code> to calculate the odds ratios gives:</p>

<pre><code>              exp.MLogitFit.coefficients.
A:(intercept)                1.175103e-03
B:(intercept)                8.055280e-04
A:V1                         3.885310e+01
B:V1                         6.966040e+01
A:V2                         2.725226e-07
B:V2                         7.538147e-03
A:V3                         5.554743e+00
B:V3                         1.897938e+01
A:V4                         1.814819e-01
B:V4                         1.878524e-01
A:V5                         3.270504e+00
B:V5                         2.803423e+00
A:V6                         3.979917e+03
B:V6                         3.136764e+01
A:V7                         2.608125e-01
B:V7                         2.852036e+00
A:V8                         5.741439e-04
B:V8                         1.244345e+00
</code></pre>

<p>I obtained the confidence interavls with: <code>exp(confint(MLogitFit))</code>:</p>

<pre><code>                     2.5 %       97.5 %
A:(intercept) 9.650816e-09 1.430830e+02
B:(intercept) 1.182216e-07 5.488637e+00
A:V1          1.831725e-02 8.241213e+04
B:V1          1.176881e-01 4.123248e+04
A:V2          3.446800e-13 2.154711e-01
B:V2          5.864847e-06 9.688857e+00
A:V3          1.394913e-05 2.211978e+06
B:V3          7.994348e-03 4.505896e+04
A:V4          8.066986e-03 4.082774e+00
B:V4          1.875058e-02 1.881996e+00
A:V5          1.400307e-01 7.638467e+01
B:V5          2.392271e-01 3.285238e+01
A:V6          2.870699e+01 5.517731e+05
B:V6          7.303065e-01 1.347282e+03
A:V7          1.366460e-03 4.978060e+01
B:V7          6.223884e-02 1.306918e+02
A:V8          1.752860e-07 1.880591e+00
B:V8          1.891518e-02 8.185990e+01
</code></pre>

<p>The predicted Probabilities are as following:</p>

<pre><code>fitted(MLogitFit, outcome=FALSE)
                 Z            A          B
 [1,] 0.2790108926 3.880184e-01 0.33297074
 [2,] 0.5191458618 2.900625e-01 0.19079169
 [3,] 0.7263001933 1.633014e-02 0.25736966
 [4,] 0.8386056883 3.700203e-03 0.15769411
 [5,] 0.8050365007 7.487290e-03 0.18747621
 [6,] 0.7855655154 3.860347e-02 0.17583101
 [7,] 0.7878404896 7.992930e-03 0.20416658
 [8,] 0.8386056883 3.700203e-03 0.15769411
 [9,] 0.7878404896 7.992930e-03 0.20416658
[10,] 0.4363708036 2.827104e-01 0.28091885
[11,] 0.6126060746 3.320075e-02 0.35419317
[12,] 0.0274357267 8.418204e-01 0.13074390
[13,] 0.1438998597 5.869087e-01 0.26919146
[14,] 0.1850027820 2.105586e-01 0.60443858
[15,] 0.8427092407 5.933393e-03 0.15135737
[16,] 0.1537160539 4.929905e-01 0.35329341
[17,] 0.0434283140 6.358897e-01 0.32068201
[18,] 0.1868202029 1.141679e-01 0.69901186
[19,] 0.3064594418 1.156597e-01 0.57788084
[20,] 0.5737141160 6.734724e-02 0.35893865
[21,] 0.5841338911 1.374758e-01 0.27839031
[22,] 0.0866451414 4.019366e-01 0.51141821
[23,] 0.2794060013 9.964607e-02 0.62094793
[24,] 0.0252343516 7.343045e-01 0.24046118
[25,] 0.1314775919 4.602643e-01 0.40825811
[26,] 0.0274357267 8.418204e-01 0.13074390
[27,] 0.1303195991 6.649645e-01 0.20471586
[28,] 0.2818251202 4.896734e-01 0.22850146
[29,] 0.0063990341 8.874618e-01 0.10613917
[30,] 0.0002408527 9.742025e-01 0.02555668
[31,] 0.0523052465 7.073015e-01 0.24039322
[32,] 0.3287956423 2.756959e-01 0.39550841
[33,] 0.0419093705 7.521689e-01 0.20592173
[34,] 0.0523052465 7.073015e-01 0.24039322
[35,] 0.3287956423 2.756959e-01 0.39550841
[36,] 0.0100998700 7.475180e-01 0.24238212
[37,] 0.1609808596 2.268570e-01 0.61216212
[38,] 0.0119603037 8.065964e-01 0.18144331
[39,] 0.0697132279 4.549378e-01 0.47534896
[40,] 0.5756435353 6.315652e-02 0.36119994
[41,] 0.4689676672 6.796615e-02 0.46306619
[42,] 0.2652679745 6.358962e-02 0.67114240
[43,] 0.7870195702 2.038999e-03 0.21094143
[44,] 0.6438437943 9.222002e-03 0.34693420
[45,] 0.7462282258 5.881047e-04 0.25318367
[46,] 0.3532662528 2.193975e-01 0.42733620
[47,] 0.9563852795 4.133754e-05 0.04357338
[48,] 0.9079031419 2.786314e-03 0.08931054
[49,] 0.0220230156 8.017508e-01 0.17622619
[50,] 0.2268852285 1.745210e-01 0.59859376
[51,] 0.2268852285 1.745210e-01 0.59859376
[52,] 0.0751929214 6.261548e-01 0.29865225
[53,] 0.9426667411 4.520877e-06 0.05732874
[54,] 0.0212631471 6.729961e-01 0.30574075
[55,] 0.0212631471 6.729961e-01 0.30574075
[56,] 0.9218535421 1.166953e-02 0.06647693
[57,] 0.6374868816 3.856300e-02 0.32395012
[58,] 0.2920703240 2.410709e-01 0.46685876
[59,] 0.7047942848 1.728601e-02 0.27791970
[60,] 0.1850395244 5.297673e-01 0.28519316
[61,] 0.4402296785 8.870861e-03 0.55089946
[62,] 0.6781988218 3.852569e-04 0.32141592
[63,] 0.9889453179 4.036588e-05 0.01101432
[64,] 0.1618635354 8.011851e-02 0.75801796
[65,] 0.3008372801 9.835522e-02 0.60080750
[66,] 0.0740319347 4.284039e-01 0.49756417
[67,] 0.5529727485 1.768537e-01 0.27017351
[68,] 0.7824740564 5.001713e-03 0.21252423
[69,] 0.5343045050 5.865850e-02 0.40703700
[70,] 0.4564647083 1.733995e-01 0.37013579
[71,] 0.4711837972 8.449081e-03 0.52036712
[72,] 0.9154349308 2.364316e-02 0.06092191
[73,] 0.1858643216 2.217595e-01 0.59237621
[74,] 0.3770813535 9.943397e-02 0.52348468
[75,] 0.8124141650 3.243679e-04 0.18726147
[76,] 0.3195206223 2.932236e-01 0.38725578
[77,] 0.8615871019 5.063299e-04 0.13790657
[78,] 0.8615871019 5.063299e-04 0.13790657
[79,] 0.8254986241 2.059378e-03 0.17244200
[80,] 0.1208591778 4.615235e-01 0.41761730
[81,] 0.0035765650 9.093754e-01 0.08704806
[82,] 0.7583239965 3.544345e-02 0.20623255
[83,] 0.8141948591 5.016280e-03 0.18078886
[84,] 0.1204323818 2.545405e-01 0.62502710
[85,] 0.9594950290 3.694056e-05 0.04046803
[86,] 0.6858228916 1.691396e-01 0.14503752
[87,] 0.8254986241 2.059378e-03 0.17244200
[88,] 0.8254986241 2.059378e-03 0.17244200
[89,] 0.2463233530 2.793410e-01 0.47433568
[90,] 0.5674338104 1.448538e-02 0.41808081
</code></pre>

<p>To assess multicolinearity I calculated the VIF statistic but using the a glm model of the same dataset.</p>

<pre><code>fullmod&lt;-glm(as.factor(Outcome)~.,data=test,family=binomial())
vif(fullmod)
      V1       V2       V3       V4       V5       V6       V7       V8 
1.789116 1.822252 2.216444 1.320244 1.821820 1.439183 1.512865 1.121805 
</code></pre>
"
"0.0440225453162812","0.0458831467741124","173568","<p>In helping us understand how to fit a logistic regression in <code>R</code>, we are told to first replace 0 and 1 in the response variable by 0.05 and 0.95, respectively and second to take the logit transform of the resulting response variable. Last we fit these data using iterative re-weighted least squares method. </p>

<p>Then we are asked to use 0.005 and 0.995 instead of 0.05 and 0.95. Then the resulting coefficients are quite <strong>different</strong>.</p>

<p>My question is in <code>glm</code> function, how are 0 and 1 dealt with? Are they replaced by some numbers as above? What numbers are used by default and why are they used? How sensitive is the choice of these numbers?</p>
"
"0.146006265130522","0.138342892773215","173625","<p>I am new to bootstrapping. Let me explain my case, I ran logistic model (binomial) with 9 parameters. I divided complete dataset into train(70%) and test(30%). I checked stats for the model validity and then did bootstrapping over complete dataset. Now I have 3 sets of coefficients one from modeled train set, one from bootstrap original set (that ran on complete dataset) and last one bootstrapped coefficient. Now I am not sure which set of coefficients should I use while predicting in future? Below are all coefficient results where <code>prob_model$coefficient</code> is list fo coefficients from model trained on training set. In boot model result original is list of coefficient on complete data set and original + bias would give me list of bootstrapped coefficients. Now I am not sure which one should I use for future prediction. Please suggest.</p>

<pre><code>Bootstrap Statistics :
          original        bias     std. error
t1*   0.9974316479  3.619136e-03 3.772206e-03
t2*  -0.0001680308  1.082013e-06 1.595247e-05
t3*  -0.0122041526 -2.240127e-05 6.932449e-05
t4*  -0.2103373656 -1.739921e-03 1.102777e-02
t5*  -0.0065004644 -2.184167e-05 1.525726e-04
t6*   0.2189768454 -1.910363e-03 1.564095e-02
t7*   0.0794606315 -5.810673e-04 1.066958e-03
t8*   0.0015241943  3.244818e-06 5.442563e-05
t9*   0.1165591650  7.599514e-04 5.028385e-03
t10*  0.0010037383  2.001973e-05 1.115370e-04



prob_model$coefficients
           (Intercept)                    a b       c                   d 
          0.9903145633          -0.0001745036          -0.0122231856          -0.2017792147          -0.0064298957 
            e       f            g       h       i 
          0.2032220963           0.0795136175           0.0015688575           0.1149821030           0.0011356770 
</code></pre>
"
"0.0762492851663023","0.0794719414239026","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.0880450906325624","0.0917662935482247","175203","<p>I'm reading <a href=""https://onlinecourses.science.psu.edu/stat504/node/177"" rel=""nofollow"">this tutorial</a> to understand how to interpret the coefficients of an ordinal logistic regression which assumes the proportional odds. </p>

<p>They use a dataset about a cheese tasting experiment. Subjects were randomly assigned to taste one of four different cheeses (A,B,C,D). Response categories are 1 = strong dislike to 9 = excellent taste.</p>

<pre><code>m1=polr(response~cheese,weights=N,data=dati)
summary(m1)

Re-fitting to get Hessian

Call:
polr(formula = response ~ cheese, data = dati, weights = N)

Coefficients:
         Value Std. Error t value
cheeseB -3.352     0.4287  -7.819
cheeseC -1.710     0.3715  -4.603
cheeseD  1.613     0.3805   4.238

Intercepts:
    Value    Std. Error t value 
1|2  -5.4674   0.5236   -10.4413
2|3  -4.4122   0.4278   -10.3148
3|4  -3.3126   0.3700    -8.9522
4|5  -2.2440   0.3267    -6.8680
5|6  -0.9078   0.2833    -3.2037
6|7   0.0443   0.2646     0.1673
7|8   1.5459   0.3017     5.1244
8|9   3.1058   0.4057     7.6547

Residual Deviance: 711.3479 
AIC: 733.3479 
</code></pre>

<p>The tutorial's author writes:</p>

<blockquote>
  <p>we see that the implied ordering of cheeses in terms of quality is D >
  A > C > B. Furthermore, D is significantly better preferred than A,
  but A is not significantly better than C.</p>
</blockquote>

<p>Is this correct? 
I do agree that cheese B and C are significantly worse than A, and that D is significantly better than A, but I don't understand why cheese A should not be significantly better than C, as the author claims.</p>

<p>This are instead my conclusions:<br>
Since $\beta_B \neq 0$ and $\beta_B &lt; 0$, then $B&lt;A$.<br>
Since $\beta_C \neq 0$ and $\beta_C &lt; 0$, then $C&lt;A$.<br>
Since $\beta_D \neq 0$ and $\beta_D &gt; 0$, then $D&gt;A$.<br>
So, $D&gt;A&gt;B$ and $D&gt;A&gt;B$.
But since $\beta_B &lt; \beta_C$, then $D&gt;A&gt;B&gt;C$.<br>
So, I would say instead that cheese A is significantly better than C.</p>
"
"NaN","NaN","175216","<p>I have a binary dependent variable (<code>R</code>) and four numeric independent variables (<code>Q, M, S, T</code>) and want to examine coefficients for them.</p>

<p>Here is my glm code in R:</p>

<pre><code>fit = glm(R ~ Q + M + S + T, data=data, family=binomial())
</code></pre>

<p>When I run <code>predict(fit)</code>, I get a lot of predicted values greater than one (but none below 0 so far as I can tell). I have tried bayesglm and glmnet per suggestions to similar questions but both are a little over my head and the output I did get didn't seem to fix my problem.</p>

<p>I want to know:
A) Is this typical of logistic regression?
B) If not, how do I fix it?</p>
"
"0.0898606443361987","0.112390297389803","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.152498570332605","0.158943882847805","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.062257280636469","0.064888568452305","175956","<p>For a (fictional) <strong>multiple logistic regression</strong>, let's consinder a DV 'hired' (0,1) and <strong>three dichotomous IVs</strong> 'college_degree' (0,1), 'affluent' (0,1) and 'recommendated' (0,1) for <em>N</em> = 1,000 participants.</p>

<p>Running a logistic regression and generating predicted probabilities of being hired using the <code>predict</code> function for a <code>glm</code> object works well. For every respondent I have a probability value ranging continuously from 0 to 1.</p>

<p>Since I do have a base distribution of all three IVs and the DV, I want a kind of simulator that predicts the <strong>percentage/proportion</strong> of the DV using each indivduals predicted probability.</p>

<p>Let's say in the sample 20% are hired, 50% have a college degree, 10% are affluent and 35% are recommendated. I want to use the predicted values to see how much would the <strong>proportion of 'hired' goes</strong> up, when I, e.g., <strong>change the proportion of recommendations to 50%</strong>. I guess, I could also use the equation with the coefficients of the logit model, but would need to run it for every individual.</p>

<p>Is there any way to implement this in R (well or Excel, if that is easier)?</p>
"
"0.0880450906325624","0.0917662935482247","177650","<p>I have a binary response variable and a categorical predictor variable. If I test for associations between the 2 variables using chi-square test , it turns out to be significant. However, if I do a logistic regression with the same set of variables, the predictor is not significant. Why does this happen?</p>

<pre><code>  table(Data1$pred,Data1$target)

                            0    1
  Level1                    1    0
  Level2                    4    0
  Level3                   98    1
  Level4                 2056   22
  Level5                    1    0
  Level6                    2    0
  Level7                  311    0
  Level8                    6    1
  Level9                  131    7
  Level10                  49    2

  chisq.test(table(Data1$pred,Data1$target))

  Pearson's Chi-squared test

  data:  tabletable(Data1$pred,Data1$target)
  X-squared = 34.2614, df = 9, p-value = 8.037e-05
</code></pre>

<p>Logistic Regression on the same</p>

<pre><code>  logit.glm &lt;- glm(as.factor(target) ~ pred,                  
               data=Data1, family=binomial(link=""logit"")
  summary(logit.glm)
  Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5553  -0.1459  -0.1459  -0.1459   3.0315  

  Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)
  (Intercept)   -2.057e+01  1.773e+04  -0.001    0.999
  Data1Level2   -6.313e-06  1.982e+04   0.000    1.000
  Data1Level3    1.598e+01  1.773e+04   0.001    0.999
  Data1Level4    1.603e+01  1.773e+04   0.001    0.999
  Data1Level5   -6.312e-06  2.507e+04   0.000    1.000
  Data1Level6   -6.312e-06  2.172e+04   0.000    1.000
  Data1Level7   -6.312e-06  1.776e+04   0.000    1.000
  Data1Level8    1.877e+01  1.773e+04   0.001    0.999
  Data1Level9    1.764e+01  1.773e+04   0.001    0.999
  Data1Level10   1.737e+01  1.773e+04   0.001    0.999

  (Dispersion parameter for binomial family taken to be 1)

   Null deviance: 356.09  on 2691  degrees of freedom
   Residual deviance: 333.06  on 2682  degrees of freedom
   AIC: 353.06

   Number of Fisher Scoring iterations: 19
</code></pre>
"
"0.132732968300475","0.152177182050536","177654","<p>When running an ordered logistic regression using the <code>polr</code> function of the <code>MASS</code>package (DV is low, medium, high) and have a look at the summary I get Î²s for every IV and the intercepts for low|medium and medium|high.</p>

<p>The <code>predict</code>function for assessing the probabilities (<code>type='p'</code>) or the classes (<code>type='class'</code>) also works just fine.</p>

<p>However I want to calculate the probabilities myself in order to use them with different data sets.</p>

<p>If I use the following code for a <em>logistic model with a binary (!) dependent variable</em>, I can exactly replicate the <code>predict</code> - outcome:</p>

<p><code>log_pred &lt;- (logit_model$coefficients[1] + logit_model$coefficients[2]*IV_1 + logit_model$coefficients[3]*IV_2)</code></p>

<ul>
<li><code>logit_model</code> is my <code>glm</code>-object</li>
<li><code>logit_model$zeta[1]</code> is the first intercept</li>
<li><code>logit_model$zeta[2]</code> is the second intercept</li>
<li><code>logit_model$coefficients[1]</code> is the Î² of IV_1</li>
<li><code>logit_model$coefficients[2]</code> is the Î² of IV_2</li>
</ul>

<p>the only thing I have to do now, to get the predicted probabilities is:</p>

<p><code>log_pred_probs &lt;- exp(log_pred)/(1+exp(log_pred))</code></p>

<p>If I understand all the posts on ordered logistic regression I read correctly, the only thing I have to change with a <code>polr</code> object with the 3 ""groups"" of low, medium, and high would be to:</p>

<ul>
<li>run the <code>log-pred</code>part for each group using their own intercepts, let's call them <code>log_pred1</code> and <code>log_pred2</code></li>
<li>and to, then, run the following code (similar to the logistic model above):
<code>log_pred_probs1 &lt;- exp(log_pred1)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""low""
<code>log_pred_probs2 &lt;- exp(log_pred2)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""medium""
<code>log_pred_probs3 &lt;- 1/(1+exp(log_pred1)+exp(log_pred2))</code> for ""high""</li>
</ul>

<p>I think there are at least two problems ('cause this doesn't work at all):</p>

<ol>
<li>I need the Î²-coefficients for every level of the dependent variable, and <code>summary(polr-object)</code>does only show the Î²s for the first group (so does <code>$coefficients</code>)</li>
<li>and I am not sure about the computation of the predicted probabilities for group 3, ""high"".</li>
</ol>

<p>So these are the questions in short: <strong>How do I assess the Î²-coefficients for every level of the DV in a <code>polr</code>object?</strong></p>

<p>And</p>

<p><strong>How do I compute the predicted probabilities for every level of the DV myself?</strong></p>
"
"0.062257280636469","0.064888568452305","177903","<p>I fitted an ordinal logistic regression but I'm unable to interpret the coefficients. Can anyone assist in this regard? Here is the output generated: </p>

<pre><code>Call:
polr(formula = factor(grade) ~ factor(Month) + Day, data = myData, 
    Hess = TRUE)

Coefficients:
                  Value Std. Error t value
factor(Month)4 1.405114    0.51547  2.7259
Day            0.007672    0.01944  0.3947

Intercepts:
    Value   Std. Error t value
1|2 -0.6785  0.7019    -0.9667
2|3  1.6767  0.7162     2.3412

Residual Deviance: 333.602 
AIC: 341.602 
</code></pre>

<p>The grade is factored: </p>

<p>1 = good<br>
2 = very good<br>
3 = excellent</p>

<p>Month is factored:</p>

<p>3 = March<br>
4 = April</p>

<p>The grade is the response while month and day are my explanatory variables.</p>
"
"0.139790356138221","0.158943882847805","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"NaN","NaN","179329","<p>If your logistic regression fit has coefficients with the following attributes, do you look at the values of <code>Pr(Z&gt;|z|)</code> are smaller than 0.95 to determine whether that variable is needed at a 5% level of significance? </p>

<p>ie. If <code>Pr(&gt;|z|)</code> is 0.964, this variable is not needed at 5% significance.</p>

<p><a href=""http://i.stack.imgur.com/6UTBa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6UTBa.png"" alt=""enter image description here""></a></p>
"
"0.146006265130522","0.152177182050536","180302","<p>IÂ´m fairly new to the tools of statistics and I need some help.
IÂ´m working in R.
I have a list of students, their age, sex, test results(continuous variable) and education (a categorical variable. 0 if student has no education, 1 if student has education). This is stored in the data frame df.</p>

<p>I have assigned each student an age group depending on their age. The groups are 5-10,11-15, 16-20, 21-25. So now I have a new categorical variable with 4 levels.</p>

<p>I have done logistic regression for each age group. I did the test in R like this:</p>

<pre><code>model1 = glm(education ~ test results + sex + age, data = df, family = binomial())
</code></pre>

<p>From the logistic regression result I compute the odds ratio (exp(coefficient)) for the test results variable. For each age group I got an odds ratio and confidence interval. There are some differences in the odds ratio between age groups. The odd ratio gets closer to 1 with higher age group. I interpret this in the following way; as the age of students is higher there is less effect from education on test result.
My question is how can I test if the difference in odds ratios is significant, i.e. what test can I use in R ?</p>

<p>Here is a subset of the data:</p>

<pre><code>student sex age testScore education ageGroup

1        1   10   0.12       1        1

2        1    8   0.08       1        1

3        2   16   0.85       0        3

4        2   20   0.12       0        3

5        1   22   1.02       0        4

6        2   18   0.98       1        3

7        1   19   0.46       1        3

8        1   16   0.83       0        3

9        2   12   0.26       1        2

10       2   14   0.46       0        2
</code></pre>

<p>I have been searching books and the web without success, canÂ´t seem to find an example that I can relate to.
Any suggestions would be appreciated.</p>
"
"0.170498584867618","0.177704663327728","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.0440225453162812","0.0458831467741124","181695","<p>In linear regression, if I have a model,</p>

<pre><code>b0 + b1x1 + b2x2 + b3x3 + b4x4 = y
</code></pre>

<p>and I want to fix some of the coefficients ,say b1 = 1 and b3 = 2, I could just do the following</p>

<pre><code>b0 + b2x2 + b4x4 = y - x1 - 2x3
</code></pre>

<p>and just fit a linear regression on the other three parameters on the new y. Is there a way to do this for logistic regression? The sigmoid function seems to complicate things. Im looking to do this in r, so if theres an easy way to do it in r, that would be very appreciated.</p>
"
"0.124514561272938","0.12977713690461","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.164717281867254","0.159416228266091","182582","<p>This link from UCLA.edu (<a href=""http://www.ats.ucla.edu/stat/r/dae/mlogit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/mlogit.htm</a>) provides a useful example for a multinomial logistic case.
However, I wonder how can I correctly interpret the coefficients in case of grouped data. Here is an example following the ucla.edu exercise:</p>

<pre><code># Load packages
require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)
# Read data from ucla.edu
ml &lt;- read.dta(""http://www.ats.ucla.edu/stat/data/hsbdemo.dta"")
str(ml)
# $ prog   : Factor w/ 3 levels ""general"",""academic"",""vocation"": 3 1 3 3 3 1 3 3 3 3 ...
# $ write  : num  35 33 39 37 31 36 36 31 41 37 ...
# $ ses    : Factor w/ 3 levels ""low"",""middle"",""high"": 1 2 3 1 2 3 2 2 2 2 ...
</code></pre>

<p>NOTE: I wish to predict variable <code>prog</code> using the numeric variable <code>write</code> and the 3 level factor <code>ses</code>. I don't care if the 3 levels, ""low"", ""middle"", ""high"", are ordered! As far as I am concerned this factor can be gender or country or whatever.</p>

<pre><code># Choose the baseline level of 'prog' dependent variable
ml$prog2 &lt;- relevel(ml$prog, ref = ""academic"")
</code></pre>

<p>I run a simple model based only on 'write' independent variable (I.V.)</p>

<pre><code>test1 &lt;- multinom(prog2 ~ write, data = ml)
summary(test1)
# I get the following coefficients
Coefficients:
         (Intercept)       write
general     2.712485 -0.06600782
vocation    5.359000 -0.11780909
</code></pre>

<p>I run the complex model, including 'ses' factor as the second I.V.</p>

<pre><code>test2 &lt;- multinom(prog2 ~ write + ses, data = ml)
summary(test2)
# I get the following coefficients
Coefficients:
         (Intercept)      write  sesmiddle    seshigh
general     2.852198 -0.0579287 -0.5332810 -1.1628226
vocation    5.218260 -0.1136037  0.2913859 -0.9826649
</code></pre>

<p><strong>So, how can I directly get the intercepts for each level of <code>ses</code>?</strong> I am not interested in the change of intercept from one level to another.</p>

<p>I tried this:</p>

<pre><code>test3 &lt;- multinom(prog2 ~ write + ses -1, data = ml)
summary(test3)
</code></pre>

<p>which is interpreted by R as omitting the 'main' intercept</p>

<pre><code>Coefficients:
               write   seslow sesmiddle  seshigh
general  -0.05792787 2.852149  2.318866 1.689318
vocation -0.11360180 5.218146  5.509562 4.235464
</code></pre>

<p>As I see, the intercept of model <code>test2</code> is actually the coefficient of level <code>low</code> of factor <code>ses</code> in model <code>test3</code> (the first level in alphabetical order).</p>

<p>So, now, can I make the interpretations like this:?</p>

<p>The log odds of being in general program vs. in academic program will increase by 2.85 for <code>ses=low</code>, by 2.32 for <code>ses=middle</code> and by 1.69 for <code>ses=high</code>. In a similar way for the log odds of being in vocation program vs. in academic program: the increase will be by 5.22 for <code>ses=low</code>, by 5.51 for <code>ses=middle</code> and by 4.24 for <code>ses=high</code>.</p>

<p>Sorry if my question might look naive.</p>
"
"0.186771841909407","0.194665705356915","183320","<p>I have the following dataframe on which I did logistic regression with response as outcome. There are some good predictors in these variables so I expected significant variables.</p>

<pre><code>structure(list(response = c(0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 
    0L, 0L, 1L, 0L, 1L, 0L), HIST1H3F_rna = c(1.09861228866811, 0.693147180559945, 
    2.07944154167984, 1.09861228866811, 1.79175946922805, 0, 0, 0, 
    2.39789527279837, 1.38629436111989, 1.6094379124341, 1.6094379124341, 
    0.693147180559945, 1.79175946922805, 0), NCF1_rna = c(2.77258872223978, 
    3.09104245335832, 2.63905732961526, 2.19722457733622, 2.30258509299405, 
    2.56494935746154, 3.09104245335832, 3.98898404656427, 2.56494935746154, 
    4.06044301054642, 3.87120101090789, 2.07944154167984, 3.49650756146648, 
    3.17805383034795, 3.95124371858143), WDR66_rna = c(5.06890420222023, 
    4.49980967033027, 5.11799381241676, 3.40119738166216, 3.25809653802148, 
    4.02535169073515, 5.8348107370626, 5.89440283426485, 3.87120101090789, 
    5.67675380226828, 5.35185813347607, 4.15888308335967, 6.23441072571837, 
    5.91889385427315, 3.68887945411394), PTH2R_rna = c(0.693147180559945, 
    5.08759633523238, 0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 
    6.56526497003536, 5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 
    1.09861228866811, 3.49650756146648, 1.38629436111989, 5.93753620508243
    ), HAVCR2_rna = c(4.48863636973214, 3.40119738166216, 3.09104245335832, 
    2.94443897916644, 3.2188758248682, 3.76120011569356, 3.95124371858143, 
    2.83321334405622, 2.07944154167984, 4.36944785246702, 3.58351893845611, 
    1.94591014905531, 4.23410650459726, 3.43398720448515, 2.56494935746154
    ), CD200R1_rna = c(2.484906649788, 2.94443897916644, 0.693147180559945, 
    1.94591014905531, 0.693147180559945, 2.89037175789616, 2.56494935746154, 
    1.6094379124341, 1.6094379124341, 1.94591014905531, 2.19722457733622, 
    0.693147180559945, 4.26267987704132, 1.6094379124341, 0.693147180559945
    )), .Names = c(""response"", ""HIST1H3F_rna"", ""NCF1_rna"", ""WDR66_rna"", 
    ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna""), row.names = c(NA, 
    -15L), class = ""data.frame"")
</code></pre>

<p>However, running the following lines and getting a summary of the model I find that all variables have a p-value of 1 and the standard errors seem so high. What's going on here?</p>

<pre><code>fullmod &lt;- glm(response ~ ., data=final_model,family='binomial')
summary(fullmod)
Call:
glm(formula = response ~ ., family = ""binomial"", data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-6.515e-06  -2.404e-06  -2.110e-08   2.110e-08   7.470e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   1.460e+02  5.598e+05       0        1
HIST1H3F_rna  2.135e+01  5.145e+05       0        1
NCF1_rna     -4.133e+01  3.388e+05       0        1
WDR66_rna     1.296e+01  6.739e+05       0        1
PTH2R_rna     1.975e+00  3.775e+05       0        1
HAVCR2_rna   -2.477e+01  1.191e+06       0        1
CD200R1_rna  -1.420e+01  1.315e+06       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0190e+01  on 14  degrees of freedom
Residual deviance: 2.2042e-10  on  8  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25
</code></pre>

<hr>

<p>In response to your comments I'll show the feature selection step (and the complete dataframe I'm working with below that).  </p>

<pre><code># forward  feature selection 
library('boot')
z = c()
nullmod &lt;- glm(response ~ 1, data=final_model, family='binomial') ## â€˜emptyâ€™ 
fullmod &lt;- glm(response ~ ., data=final_model, family='binomial') ## Full model
first = T
for(x in 1:ncol(final_model)){
  stepmod &lt;- step(nullmod, scope=list(lower=formula(nullmod), upper=formula(fullmod)),
                  direction=""forward"", data=final_model, steps=x, trace=F)
  cv.err  &lt;- cv.glm(data=final_model, glmfit=stepmod, K=nrow(final_model))$delta[1]
  if (first == T){
    first=F
    final_features &lt;- stepmod
  }else{
    if (cv.err &lt; min(z)){ final_features &lt;- stepmod }
  }
  z[x] &lt;- cv.err
  print(paste(x,cv.err))
  print(colnames(final_features$model))
}

plot(z, main='Forward Feature Selection GLM Final Model', 
     xlab='Number of Steps', ylab='LOOCV-error', col='red', type='l')
points(z)
colnames(final_features$model)
summary(final_features)

structure(list(response = c(0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 0L, 1L, 1L, 1L), HIST1H3F_rna = c(1.09861228866811, 2.07944154167984, 
1.09861228866811, 1.79175946922805, 0, 0, 0, 2.39789527279837, 
1.38629436111989, 1.6094379124341, 1.6094379124341, 0.693147180559945, 
2.19722457733622, 2.39789527279837, 2.89037175789616), NCF1_rna = c(2.77258872223978, 
2.63905732961526, 2.19722457733622, 2.30258509299405, 2.56494935746154, 
3.09104245335832, 3.98898404656427, 2.56494935746154, 4.06044301054642, 
3.87120101090789, 2.07944154167984, 3.49650756146648, 2.07944154167984, 
2.07944154167984, 1.09861228866811), WDR66_rna = c(5.06890420222023, 
5.11799381241676, 3.40119738166216, 3.25809653802148, 4.02535169073515, 
5.8348107370626, 5.89440283426485, 3.87120101090789, 5.67675380226828, 
5.35185813347607, 4.15888308335967, 6.23441072571837, 4.0943445622221, 
4.21950770517611, 3.95124371858143), PTH2R_rna = c(0.693147180559945, 
0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 6.56526497003536, 
5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 1.09861228866811, 
3.49650756146648, 0, 0.693147180559945, 1.38629436111989), 
HAVCR2_rna = c(4.48863636973214, 
3.09104245335832, 2.94443897916644, 3.2188758248682, 3.76120011569356, 
3.95124371858143, 2.83321334405622, 2.07944154167984, 4.36944785246702, 
3.58351893845611, 1.94591014905531, 4.23410650459726, 1.38629436111989, 
1.09861228866811, 1.38629436111989), CD200R1_rna = c(2.484906649788, 
0.693147180559945, 1.94591014905531, 0.693147180559945, 2.89037175789616, 
2.56494935746154, 1.6094379124341, 1.6094379124341, 1.94591014905531, 
2.19722457733622, 0.693147180559945, 4.26267987704132, 1.94591014905531, 
0, 0.693147180559945), GDF7 = c(0.2232, -0.7281, 0.0655, -0.7919, 
0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 0.7371, 
-0.4978, -0.5096, -0.0827), HS1BP3 = c(0.2232, -0.7281, 0.0655, 
-0.7919, 0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 
0.7371, -0.4978, -0.5096, -0.0827), NKAIN3 = c(0.4072, 0.3216, 
-0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 0.6331, 
-0.135, 1.3532, -0.503, -0.1241, 0.2061), UG0898H09 = c(0.4072, 
0.3216, -0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 
0.6331, -0.135, 1.3532, -0.503, -0.1241, 0.2061), C15orf41 = c(0.122, 
-0.7519, -1.1267, -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, 
-0.5944, 0.0714, -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), 
    FAM98B = c(-0.1871, -0.7519, -1.1267, -0.7882, -0.1117, -0.5105, 
    -0.3905, -0.6834, -0.5944, 0.0714, -0.8134, -0.0115, -1.1112, 
    -1.1488, -0.4878), SPRED1 = c(-0.1871, -0.7519, -1.1267, 
    -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, -0.5944, 0.0714, 
    -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), MPDZ_ex = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0), TPR_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BUB1B_ex = c(0, 
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), APC_ex = c(0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ATM_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0), DYNC1LI1_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), TTK_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), PSMG2_ex = c(1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), NegRegMitosis = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0), brca1ness = c(0.037719, 
    0.900878, 0.013261, 0.900878, 0.659963, 0.005629, 9.8e-05, 
    0.996336, 0.910072, 0.850776, 0.000613, 0.104428, 0.978114, 
    0.938767, 0.041696), Methylation = c(0L, 0L, 0L, 1L, 1L, 
    1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L), LinoleicAcid_Metab = structure(c(2L, 
    2L, 2L, 2L, 1L, 3L, 2L, 2L, 1L, 5L, 2L, 5L, 1L, 2L, 2L), .Label = c(""CYP2E1_high"", 
    ""CYP2E1_med"", ""high"", ""low"", ""PLA2G2A_high""), class = ""factor""), 
    Neuro_lr = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 3L, 1L, 3L, 
    1L, 1L, 3L, 3L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor""), 
    NOX_signalling = structure(c(2L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L), .Label = c(""high"", ""low""
    ), class = ""factor"")), .Names = c(""response"", ""HIST1H3F_rna"", 
""NCF1_rna"", ""WDR66_rna"", ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna"", 
""GDF7"", ""HS1BP3"", ""NKAIN3"", ""UG0898H09"", ""C15orf41"", ""FAM98B"", 
""SPRED1"", ""MPDZ_ex"", ""TPR_ex"", ""BUB1B_ex"", ""APC_ex"", ""ATM_ex"", 
""DYNC1LI1_ex"", ""TTK_ex"", ""PSMG2_ex"", ""NegRegMitosis"", ""brca1ness"", 
""Methylation"", ""LinoleicAcid_Metab"", ""Neuro_lr"", ""NOX_signalling""
), row.names = c(NA, -15L), class = ""data.frame"")
</code></pre>

<p>Summary now gives the following:</p>

<pre><code>Call:
glm(formula = response ~ NegRegMitosis, family = ""binomial"", 
    data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-3.971e-06  -3.971e-06   3.971e-06   3.971e-06   3.971e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       25.57   76367.61       0        1
NegRegMitosis    -51.13  111790.71       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0728e+01  on 14  degrees of freedom
Residual deviance: 2.3655e-10  on 13  degrees of freedom
AIC: 4

Number of Fisher Scoring iterations: 24
</code></pre>

<p>Again even in a single predictor model, my p-value is 1. The predictor in this case is equal to the response, so it should predict perfectly. Then why is my pvalue 1?</p>
"
"0.107832773203438","0.112390297389803","183528","<p>I have three questions about the assumptions of the logistic regression:</p>

<ol>
<li><p>I read that the percentages of zeros and ones should be equal. If there's a data set where one of them is abundand, i.e. there are 80% zeros and 20% ones can I somehow put different weights in my glm? There's also the weight-function, but I don't understand what it's exactly for... </p></li>
<li><p>I didn't really get what the pseudo-coefficients of determination tell me - Do this, i.e. the Nagelkerke's index tell me something about the assumptions of my model, how much they are fullfilled or just how much my predicted model differ from the data points I've observed. </p></li>
<li><p>I also red for the assumptions that there should be at least 25 data points / group, what exactly is my group? When I i.e. have the mtcars-dataset in R </p>

<p>data(""mtcars"") </p></li>
</ol>

<p>and I want to look</p>

<pre><code>glm(vs ~ carp + disp, family = binomial) 
</code></pre>

<p>what are my groups? (maybe this is also a false point of view, but I'm really irritated...)</p>

<p>Best wishes </p>

<p>Marry</p>
"
"0.134306229889014","0.152708343114187","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.0440225453162812","0","183845","<p>I'm going crazy, because I can't find a simple description how the coefficients are calculated in R statistics in the multivariable logistic regression (and I'm not a mathematician). 
Are they standardised? So i.e. when I have x ~ y1 + y2 and the coefficient for y1 = 0.2, is this the coefficient in the model when the parameter y2 is 0, the mean of y2 or somehow all the parameters of y2? </p>

<p>Sorry, I'm stuck on this simple question... </p>

<p>p.s.: I also have an interaction y1:y2 if this changes anything...</p>
"
"0.139211511597426","0.130585725019802","183908","<p>I have a binary logistic regression with just one binary fixed factor predictor. The reason I don't do it as a Chi square or Fisher's exact test is that I also have a number of random factors (there are multiple data points per individual and individuals are in groups, although I don't care about coefficients or significances for those random variables). I do this with R glmer.</p>

<p>I would like to be able to express the coefficient and associated confidence interval for the predictor as a risk ratio rather than an odds ratio. This is because (maybe not for you but for my audience) risk ratio is much easier to understand. Risk ratio here is the relative increase in chance of the outcome being 1 rather than 0 if the predictor is 1 rather than 0.</p>

<p>The odds ratio is trivial to get from the coefficient and associated CI using exp(). To convert an odds ratio to a risk ratio, you can use ""RR = OR / (1 â€“ p + (p x OR)), where p is the risk in the control group"" (source: <a href=""http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/"" rel=""nofollow"">http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/</a>). But, you need the risk in the control group, which in my case means the chance that the outcome is 1 if the predictor is 0. I believe the intercept coefficient from the model is in fact the odds for this chance, so I can use prob=odds/(odds+1) to get that. I'm pretty much so-far-so-good on this as far as the central estimate for the risk ratio goes. But what worries me is the associated confidence interval, because the intercept coefficient also has its own associated CI. Should I use the central estimate of the intercept, or to be conservative should I use whatever limits of the intercept CI make my relative risk CI widest? Or am I barking up the wrong tree entirely?</p>
"
"0.0880450906325624","0.0917662935482247","184137","<p>I would like to get the predicted values (with confidence intervals) for a multinomial logistic regression. I know this could be done with predict but in my case I have clustered standard errors in the following way:</p>

<pre><code>multinom &lt;- mlogit(Y ~0| X1+ X2 , data)
cl.mlogit   &lt;- function(fm, cluster){
  M &lt;- length(unique(cluster))
  N &lt;- length(cluster)
  K &lt;- length(coefficients(fm))
  dfc &lt;- (M/(M-1))
  uj  &lt;- apply(estfun(fm),2, function(x) tapply(x, cluster, sum));
  vcovCL &lt;- dfc*sandwich(fm, meat.=crossprod(uj)/N)
 coeftest(fm, vcovCL) 
}
cl.mlogit(multinom, data$group)
</code></pre>

<p>How I could use these results to get the predicted probabilities (with confidence intervals) for X1=1 and X2=0 for example and compare it with predicted probalities for X1=2 and X2=0. </p>

<p>Also, how could I get a confidence interval for that difference? In Stata prvalue do this last thing by using the delta method to get the confidence interval <a href=""http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf"" rel=""nofollow"">http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf</a>. Is there an easy way to do it in R?</p>
"
"0.062257280636469","0.0324442842261525","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.152498570332605","0.145698559277155","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.0762492851663023","0.0794719414239026","187658","<p>I ran a logistic regression in R using driving data from about 10,000 people. The model included age, years of driving experience, as well as 4 driving test results. The dependent variable was whether or not they had been involved in a crash recently (yes or no, a categorical variable). </p>

<p>The coefficients of the model are given below:</p>

<pre><code>                    Estimate     Std.Err       z value     Pr(&gt;|z|)    
(Intercept)        -1.450041     0.207144      -7.000      2.56e-12 ***
riding experience  -0.014115     0.003697      -3.818      0.000134 ***
age                -0.034544     0.003608      -9.575       &lt; 2e-16 ***
test 1              0.261485     0.088645       2.950      0.003180 ** 
test 2              0.090102     0.051328       1.755      0.079184 .  
test 3              0.228918     0.073666       3.108      0.001887 ** 
test 4              0.070106     0.063652       1.101      0.270729    
</code></pre>

<p>Firstly, with 10,000 people am I right in thinking that p-values aren't going to be that useful?</p>

<p>I calculated the probabilities of being involved in a crash with a 1 unit increase in each variable by doing <code>exp(variable)</code> to get the odds and then, <code>probability = odds/(1+odds)</code>. It gave me:</p>

<pre><code>(Intercept)        0.1899952          
ridingexp          0.4964712
age                0.4913648
test 1             0.5650012
test 2             0.5225104
test 3             0.5569810   
test 4             0.5175193
</code></pre>

<p>These seem awfully high! It is like saying that an increase in age of 1 year makes you 49% less likely to be involved in a crash? Surely that can't be right.</p>
"
"0.0880450906325624","0.0917662935482247","187796","<p>I've been trying to fit exactly the same logistic regression model (same data) in SAS and R. As far as the coefficients are concerned I didn't notice any differences. 
However, when I tried to perform some of the Goodness of fit tests (Pearson residuals and Deviance residuals GOF tests ) I noticed there is huge difference on how they are computed.
It's hard to bring in some reproducible data here but that's my output:</p>

<ol>
<li>R

<blockquote>
  <p>1 - pchisq(deviance(modelx),df.residual(modelx))</p>
</blockquote></li>
</ol>

<p>[1] 0.0003661318</p>

<blockquote>
  <p>1 - pchisq(sum(residuals(modelx, type = ""pearson"")^2),df.residual(modelx))</p>
</blockquote>

<p>[1] 0.4574779</p>

<blockquote>
  <p>deviance(modelx)</p>
</blockquote>

<p>[1] 3284.208</p>

<blockquote>
  <p>df.residual(modelx)</p>
</blockquote>

<p>[1] 3015</p>

<blockquote>
  <p>sum(residuals(modelx, type = ""pearson"")^2)</p>
</blockquote>

<p>[1] 3022.632</p>

<p>While in SAS its:</p>

<p>Criterion | Value | DF | Value/DF | Pr. > chi-sq.</p>

<p>Deviance | 2347.8792 | 2116 | 1.1096 | 0.0003 </p>

<p>Pearson | 2126.1138 | 2116 | 1.0048 | 0.4343 </p>

<p>the probabilities are similar but values and the degrees of freedom are completely different. </p>

<p>I've read that both the statistic and DF in SAS are calculated using ""profiles"" (<a href=""http://support.sas.com/resources/papers/proceedings14/1485-2014.pdf"" rel=""nofollow"">http://support.sas.com/resources/papers/proceedings14/1485-2014.pdf</a>, page 3) but I still don't understand how those profiles are calculated - I have 7 predictors in my data, each with 3,4,5,5,5,6,6 categories - or why one would use profiles at all.</p>

<p>Any ideas?</p>
"
"0.108950241113821","0.12977713690461","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"0.124514561272938","0.12977713690461","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.124514561272938","0.12977713690461","191063","<p>need help.
I use <code>rms</code> and can't understand different between <code>orm</code> and <code>lrm</code> when i used <code>contrasts</code>. For example:</p>

<pre><code>x &lt;- factor(rbinom(100,2,0.6), labels = c(""a"",""b"",""c""), ordered = TRUE)
y &lt;- factor(rbinom(100,1,0.5), labels=c(""no"",""yes""))
l&lt;-lrm(x~y);l
Logistic Regression Model

lrm(formula = x ~ y)
                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       
Obs           100    LR chi2      0.51    R2       0.006    C       0.529    
 a             24    d.f.            1    g        0.133    Dxy     0.059    
 b             40    Pr(&gt; chi2) 0.4764    gr       1.143    gamma   0.117    
 c             36                         gp       0.024    tau-a   0.039    
max |deriv| 1e-10                         Brier    0.181                     

      Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=b   1.0188 0.2988  3.41  0.0007  
y&gt;=c  -0.7162 0.2884 -2.48  0.0130  
y=yes  0.2642 0.3715  0.71  0.4769

o&lt;-orm(x~y);l;o
Logistic (Proportional Odds) Ordinal Regression Model

orm(formula = x ~ y)
                     Model Likelihood          Discrimination          Rank Discrim.    
                        Ratio Test                 Indexes                Indexes       
Obs           100    LR chi2      0.51    R2                  0.006    rho     0.071    
 a             24    d.f.            1    g                   0.133                     
 b             40    Pr(&gt; chi2) 0.4764    gr                  1.143                     
 c             36    Score chi2   0.51    |Pr(Y&gt;=median)-0.5| 0.259                     
Unique Y        3    Pr(&gt; chi2) 0.4766                                                  
Median Y        2                                                                       
max |deriv| 7e-05                                                                       

      Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=b   1.0188 0.2988  3.41  0.0007  
y&gt;=c  -0.7162 0.2884 -2.48  0.0130  
y=yes  0.2642 0.3715  0.71  0.4769  
</code></pre>

<p>We can see, that results <code>orm</code> and <code>lrm</code> are equal. When we use <code>contrasts</code> results are different:</p>

<pre><code>contrast(l,list(y=""no""),list(y=""yes""))
     Contrast      S.E.      Lower     Upper     Z Pr(&gt;|z|)
11 -0.2642454 0.3714673 -0.9923081 0.4638172 -0.71   0.4769
Confidence intervals are 0.95 individual intervals
</code></pre>

<p>and </p>

<pre><code>contrast(o,list(y=""no""),list(y=""yes""))

Contrast      S.E.      Lower   Upper    Z Pr(&gt;|z|)
11 0.7545878 0.3714672 0.02652544 1.48265 2.03   0.0422

Confidence intervals are 0.95 individual intervals
</code></pre>

<p>Why <code>orm</code> contrast aren't equal beta regression coefficient as <code>lrm</code> contrast? p.s. sorry for bad English</p>
"
"NaN","NaN","191434","<p>I want to do a logistic regression simulation using R </p>

<p>I use this code</p>

<pre><code>set.seed(666)
age = rnorm(60)         
blood_pressure = rnorm(60)
race = sample(c(rep(1,30),rep(0,30)))
inactivity = sample(c(rep(1,30),rep(0,30)))
weight = rnorm(60)

z=1+1*age+blood_pressure*2+3*weight+3*inactivity+0*race
pr=exp(z)/(1+exp(z))
y=rbinom(60,1,pr)

df = data.frame(y=y,age,blood_pressure,inactivity,weight,race)
glm(y~age+blood_pressure+inactivity+weight+race,data=df,family=binomial(link='logit'),control = list(maxit = 50))
</code></pre>

<p>I got very strange result from it.</p>

<pre><code>Coefficients:
   (Intercept)             age  blood_pressure      inactivity          weight            race  
        -39.75           46.64          106.65          143.52          229.75          100.87  
</code></pre>

<p>And it says the model doesn't converge.</p>

<p>Does someone know what's wrong and how to fix it?</p>
"
"0.0984374038697697","0.102597835208515","191506","<p>My dependent variable has 4 categories, but when I run the multinomial logistic regression using the package <code>nnet</code> with function <code>multinom</code> the results only show 3 categories. </p>

<p>I've tried changing the category numbers from 0,2,3,4 to 1,2,3,4, and also tried using names instead of numbers for the categories but it still wont show all 4 categories in the results. </p>

<p>Also, when I changed the categories to names instead of numbers, the resulting p values for each category drastically changed. Why is this? 
The p values were acquired using these commands</p>

<pre><code>z &lt;- summary(siglm)$coefficients/summary(siglm)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
p
</code></pre>
"
"0.152951761733879","0.147153441476392","191891","<p>I'm hoping someone can help clarify a few things for me.</p>

<p>I ran some relatively simple logistic regressions in r and am having trouble with interpretation.  I'm interested in the effects of elevation and a species diversity index on the presence/absence of a disease in individual animals.</p>

<p>I ran a simple model of: <code>Result~Elevation+Diversity</code> which gave this result</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation + Simpsons_Diversity, family = binomial, 
    data = XXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8141  -0.6984  -0.5317  -0.4143   2.3337  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.118e+00  1.594e-01 -13.289  &lt; 2e-16 
Elevation           1.316e-04  2.247e-05   5.855 4.76e-09 
Simpsons_Diversity -9.907e-01  2.725e-01  -3.635 0.000278 

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2923.6  on 3297  degrees of freedom
AIC: 2929.6
</code></pre>

<p>I have a strong suspicion that diversity decreases with increasing elevation which I have confirmed although the relationship isn't quite as strong as I thought. When I run a model with an interaction term <code>elevation*diversity</code> I get:</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation_1000 + Simpsons_Diversity_100 + 
    Elevation_1000 * Simpsons_Diversity_100, family = binomial, 
    data = XXXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7908  -0.6959  -0.5437  -0.3963   2.4215  

Coefficients:
                                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                           -2.014422   0.179507 -11.222  &lt; 2e-16 
Elevation_1000                         0.112466   0.027433   4.100 4.14e-05 
Simpsons_Diversity_100                -0.015851   0.005780  -2.743   0.0061  
Elevation_1000:Simpsons_Diversity_100  0.001408   0.001200   1.173   0.2406   

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2922.2  on 3296  degrees of freedom
AIC: 2930.2

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Showing that adding the interaction term doesn't really help the fit of the model (AIC = 2930) and the interaction term itself is not significant (p-value=0.24).</p>

<p>Am I on the right track so far?</p>

<p>If I am, I understand how to convert coefficients to odds ratios and interpret those.  My main question is can I plot the predicted probabilities for a combination of elevation and diversity where each variable is allowed to vary? Or is this essentially plotting the interaction?  </p>

<p>I was able to create a dataframe where I varied elevation and diversity and I used my simple non-interaction model to obtain predicted probabilities using the PREDICT fuction) for those combinations, but I want to make sure that I am doing things correctly.  I've attached the plot of predicted probs for different levels of diversity. </p>

<p><a href=""http://i.stack.imgur.com/NINBE.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NINBE.gif"" alt=""Elevation vs. Predicted Probabilities for various levels of diversity)""></a></p>
"
"0.124514561272938","0.113554994791534","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.146006265130522","0.138342892773215","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0831947907049652","0.0693687975619296","198925","<p>Although there has been some detailed discussions about power analysis on this website (for example <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">here</a> and <a href=""http://stackoverflow.com/questions/27234696/how-do-you-conduct-a-power-analysis-for-logistic-regression-in-r"">here</a>), the answer provided to this question has  outlines the steps to simulating a power analysis, <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>.</p>

<p>Say we take some data (data was linked to a <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">bootstrapping question</a>)</p>

<p>We create a regression that will predict <code>admit</code> based on the two continous variables <code>gpa</code> and <code>gre</code></p>

<ul>
<li>Now we have a <code>n=400</code>. </li>
<li>We can then elect our power level, <code>alpha = 0.5</code></li>
<li>The effect size you would like to detect, e.g., odds ratios  (we obtain this from our regression)</li>
</ul>

<p>So in following the detailed method provided by @gung <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>, I want to run the simulation. Here is the code I have adjusted, but my output is not correct. Can someone outline what I have not understood</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
head(mydata)

set.seed(1234)

my.mod &lt;- glm(admit ~ gre + gpa , data = mydata, family = ""binomial"")


repetitions &lt;- length(mydata$admit)

gre &lt;- mydata$gre
    gpa &lt;- mydata$gpa


significant = matrix(nrow=repetitions, ncol=4)

for(i in 1:repetitions){
  responses          = mydata$admit
      #responses          = rbinom(n=N, size=1, prob=mydata$admit)      # we can interchange this comment
  model              = glm(responses ~ gre + gpa, family = binomial(link=""logit""))
  significant[i,1:2] = (summary(model)$coefficients[2:3,4]&lt;.05)
      significant[i,3]   = sum(significant[i,1:2])
      modelDev           = model$null.deviance-model$deviance
  significant[i,4]   = (1-pchisq(modelDev, 2))&lt;.05
}



sum(significant[,1])/repetitions      # pre-specified effect power for gre

sum(significant[,2])/repetitions      # pre-specified effect power for gpa

sum(significant[,4])/repetitions  # power for likelihood ratio test of model

sum(significant[,3]==2)/repetitions   # all effects power

sum(significant[,3]&gt;0)/repetitions    # any effect power
</code></pre>
"
"0.0984374038697697","0.102597835208515","199912","<p>I have a large dataset with 4000 variables and 15000 observations. I am looking to build a predictive model using logistic regression. I believe that the glmnet package (using elastic net) is the best tool to use with such a large set of variables. Every variable of the 4000 is a moving average. I have split the dataset into two - training and testing.</p>

<p>When I run the code with glmnet I find something unusual happening. As I increase the number of variables for glmnet to select the model probabilities get more and more extreme which causes the misclassification rate to converge to 0%. I realise something is wrong but I cannot figure what it is.</p>

<p>Here is the code I have used:</p>

<pre><code>x &lt;- as.matrix(training[1:4000])
newx &lt;- as.matrix(testing[1:4000])

model &lt;- cv.glmnet(x, y, alpha = 0.5, family = 'binomial')

predict(model, type = ""coefficients"",s = model$lambda.min)
predict(model, newx, type= ""response"",s = model$lambda.min)
</code></pre>

<p>Is this overfitting?
I also read that categorical variables need to worked around with glmnet - none of the 4000 are categorical but they are grouped by external categorical vars.</p>

<p>I'm desperate for some help!</p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.107832773203438","0.112390297389803","200031","<p>I have very easy question that I'm hoping someone can assist me with:</p>

<p>I ran an example logistic regression using this R code:</p>

<pre><code>     hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5)
        pass &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
        data &lt;- data.frame(hours, pass)
        mylogit &lt;- glm(pass ~ hours, data = data, family = ""binomial"") #Activates the logistic regression model
        summary(mylogit) #Summary of the model

    Call:
    glm(formula = pass ~ hours, family = ""binomial"", data = data)

    Deviance Residuals: 
         Min        1Q    Median        3Q       Max  
    -1.70557  -0.57357  -0.04654   0.45470   1.82008  

    Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
    hours         1.5046     0.6287   2.393   0.0167 *
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 27.726  on 19  degrees of freedom
    Residual deviance: 16.060  on 18  degrees of freedom
    AIC: 20.06

    Number of Fisher Scoring iterations: 5

    round(exp(cbind(OR = coef(mylogit), confint(mylogit))),3)

               OR 2.5 % 97.5 %
   (Intercept) 0.017 0.000  0.281
    hours       4.503 1.698 23.223
</code></pre>

<p>I know that by taking the exponent of the log-odds/coefficients for hours the odds of passing increase by a factor of 4.503 for a one-unit change in hours.  However, given that the explanatory variable (hours) is continuous, what is considered a 'one-unit change' i.e. going from 1 to 2 hours as one unit? or from 1.75 to 1.76 hours as one unit?  Also, is this interpretation of one-unit the same for regular OLS regression as well? I'm seeking to better understand the rules R applies to creating its regression coefficients.  </p>
"
"0.0660338179744218","0.0688247201611685","201462","<p>I'm fitting a logistic regression model with <code>patient_group</code> (0,1) as response variable and the explanatory variable being an interaction between two SNPs. When running summary for the model, the alert 'Coefficients: (1 not defined because of singularities)' is shown, and I guess it is due to the fact that the combination AACT has 0 observations. </p>

<p>My question is whether the statistics are still valid, or is there a better way to analyse this kind of data? (The SNPs are located close to each other and are most likely strongly linked.)</p>

<pre><code>&gt; table(data$SNP1, data$SNP2)    
     CC CT
  TT 27  9
  AT 83 14
  AA 47  0
&gt; model &lt;- glm(patient_group ~ SNP1 * SNP2, data=data, family=""binomial"")
&gt; summary(model)
Call:
glm(formula = patient_group ~ SNP1 * SNP2, family = ""binomial"", 
data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2735  -0.9072  -0.7679   1.4742   1.8365  

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    -1.4816     0.4954  -2.991  0.00279 **
SNP1AT          0.8065     0.5471   1.474  0.14048   
SNP1AA          0.4112     0.5978   0.688  0.49158   
SNP2CT          1.7047     0.8339   2.044  0.04093 * 
SNP1AT:SNP2CT  -2.3289     1.0833  -2.150  0.03157 * 
SNP1AA:SNP2CT       NA         NA      NA       NA   

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 218.19  on 179  degrees of freedom
Residual deviance: 212.31  on 175  degrees of freedom
(26 observations deleted due to missingness)
AIC: 222.31

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.141186241600503","0.147153441476392","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.0880450906325624","0.0917662935482247","203929","<p>I have a dataset with the same dependent and independent variables as those for a logistic regression model whose equation has been published in the literature. How do I go about testing whether that equation fits well with my data, since their model was obviously fitted with a different dataset?</p>

<p>In other words I want to know if their model can be generalisable to a different sample/population.</p>

<p>I want to do this in R and all the searches I have done seem to only discuss how to fit a model with my data using the glm() function. I can fit a new model with my data and will therefore get different coefficients to those published, how do I then compare and contrast the two?</p>
"
"0.125290360437684","0.145095250022002","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.0984374038697697","0.102597835208515","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.233084030715699","0.242935266809183","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"0.0984374038697697","0.102597835208515","206702","<p>I have some data to fit a logistic regression, although the data seems quite good, the resulted fit does not look as expected.</p>

<blockquote>
<pre><code>  paramValue      normality
1  3.69             0
2  1.16             0
3  6.12             1
4  2.78             1
5  1.45             1
6  3.56             0
</code></pre>
</blockquote>

<pre><code>mylogit &lt;- glm(normality ~paramValue,  family = binomial(link=""logit""))
summary(mylogit)
</code></pre>

<blockquote>
<pre><code>Call:
glm(formula = normality ~ paramValue, family = binomial(link = ""logit""))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.44994  -0.73312   0.08151   0.63377   1.41140  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.9945     0.9531  -2.093   0.0364 *
paramValue    1.2582     0.5655   2.225   0.0261 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 29.065  on 20  degrees of freedom
Residual deviance: 19.746  on 19  degrees of freedom
AIC: 23.746

Number of Fisher Scoring iterations: 5
</code></pre>
</blockquote>

<pre><code>    plot(paramValue,normality)

    x &lt;- seq(-1, 6, 0.1)

curve(predict(mylogit,data.frame(paramValue=x),type=""response""),add=TRUE, col=""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1f0DY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1f0DY.png"" alt=""enter image description here""></a></p>

<p>Did I do something wrong? Is there any way to force the regression to cross the origin?</p>
"
"0.0440225453162812","0.0458831467741124","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.107832773203438","0.112390297389803","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.0762492851663023","0.0794719414239026","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.0440225453162812","0","208895","<p>I have a logistic regression model where Pstatus (a binary variable is coded as 1,2 (1 being apart, 2 being together in R see code below) has a coefficient of 0.8. I am wondering how I should interprete this coefficient of 0.8. And the dependent variable is alcohol consumption. </p>

<p>Is this positive coefficient telling me that being together increases the likelihood/log(odds ratio) of alcohol consumption? </p>

<pre><code>   logit.model&lt;-glm(alc~sex+age+famsize+Pstatus+Medu+Fedu+studytime+
   activities+romantic+famrel+freetime+goout+health+absences+grades,
   data=h,family=""binomial"")

   **Result** 
   PstatusT       0.800220   0.281027   2.847 0.004407 ** 

   Pstatus   : Factor w/ 2 levels ""A"",""T"": 1 2 2 2 2 2 2 1 1 2 ...
</code></pre>
"
"0.124514561272938","0.12977713690461","210900","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and he second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<ul>
<li>X1= friends and X2= location</li>
<li>X1= friends and X2= time</li>
<li>X1= public and X2= location </li>
<li>X1= public and X2= time</li>
</ul>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way.
If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks.</p>
"
"0.0984374038697697","0.0820782681668123","212213","<p>I have a problem with this ""-inf"" value in my table and I don't understand where it's coming from. As long as it's there, I cannot run a proportional odds assumption check. Here is my model, I used the MASS package:</p>

<pre><code>    m27 &lt;- polr(typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, method=""logistic"", Hess=T)

    summary(m27)
    Call: polr(formula = typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, Hess = T, method = ""logistic"")

    Coefficients:
        Value Std. Error t value
bek2   0.4620     0.2705  1.7080
vst2   0.1169     0.3217  0.3635
verw2 -0.7230     0.2580 -2.8028
nwoe2  1.8877     0.2791  6.7626

Intercepts:
     Value   Std. Error t value
1|2  2.9883  1.1341     2.6349
2|3  5.2964  1.1764     4.5021

Residual Deviance: 373.3716 
AIC: 385.3716 
</code></pre>

<p>Here is my code for the table:</p>

<pre><code>    sf &lt;- function(y) {
      c('Y&gt;=1' = qlogis(mean(y &gt;= 1)),   #typ outcome
        'Y&gt;=2' = qlogis(mean(y &gt;= 2)),
        'Y&gt;=3' = qlogis(mean(y &gt;= 3)))
    }
    s &lt;- with(typmed23, summary(as.numeric(typ) ~ bek2 + vst2 + verw2 + nwoe2, fun=sf))  
</code></pre>

<p>which produces this table:</p>

<pre><code>    as.numeric(typ)    N=244

    +-------+-+---+----+----------+-----------+
    |       | |N  |Y&gt;=1|Y&gt;=2      |Y&gt;=3       |
    +-------+-+---+----+----------+-----------+
    |bek2   |1|104|Inf | 0.6359888|-0.63598877|
    |       |2|128|Inf | 0.7884574|-0.54430155|
    |       |3| 12|Inf | 0.0000000|-1.60943791|
    +-------+-+---+----+----------+-----------+
    |vst2   |1| 55|Inf | 1.7707061| 0.10919929|
    |       |2|148|Inf | 0.8602013|-0.55431074|
    |       |3| 41|Inf |-1.0033021|-2.97041447|
    +-------+-+---+----+----------+-----------+
    |verw2  |1| 77|Inf | 3.6243409| 0.73236789|
    |       |2| 76|Inf | 1.1700713|-0.89794159|
    |       |3| 91|Inf |-0.7598386|-1.98413136|
    +-------+-+---+----+----------+-----------+
    |nwoe2  |1| 58|Inf |-2.3608540|       -Inf|
    |       |2| 40|Inf |-0.1000835|-0.96940056|
    |       |3|146|Inf | 2.8478121| 0.02739897|
    +-------+-+---+----+----------+-----------+
    |Overall| |244|Inf | 0.6808771|-0.62625295|
    +-------+-+---+----+----------+-----------+
</code></pre>

<p>My dataframe is fine: No NA or huge values, no empty cells, only 5 columns containing values between 1 and 3. Can somebody tell me what to do and how to solve the problem? </p>

<p>Here is the variable (column in a df) </p>

<pre><code>nwoe2: typmed23$nwoe2
 [1] 3 3 3 3 3 3 3 1 1 1 3 3 3 2 3 3 2 2 1 2 1 3 3 3 2 2 1 2 3 3 3 1 3 3 3
 [36] 3 2 3 2 3 3 3 2 3 1 3 3 1 3 1 3 3 3 2 1 1 3 3 2 1 1 1 3 3 2 3 3 3 2 1
 [71] 1 3 3 3 3 1 2 3 1 3 1 1 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3
 [106] 3 3 3 1 3 3 3 3 3 1 2 2 1 3 3 2 3 3 3 3 1 1 1 1 3 3 3 2 3 2 1 3 3 3 3
 [141] 1 2 3 1 3 3 3 3 3 1 3 3 3 1 2 2 3 1 3 3 1 2 3 2 2 1 3 3 3 3 3 1 1 3 3
 [176] 1 2 3 3 3 2 1 1 2 3 3 3 1 3 3 1 1 3 3 3 2 2 2 3 2 3 3 1 1 3 1 2 3 3 2
 [211] 3 1 3 3 2 3 3 1 3 3 1 1 3 3 3 3 3 3 3 1 1 3 1 3 3 3 2 3 3 2 1 1 1 3
</code></pre>

<p>Just to avoid any confusion: I'm referring to the lonely ""-Inf"" in column ""Y>=3"", not to the column ""Y>=1"".</p>
"
"0.146006265130522","0.138342892773215","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.186771841909407","0.17303618253948","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"NaN","NaN","214512","<p>I'm trying to run a logistic regression with a L2-Penalty on a dataset I have. For the regression coefficient I also want to have the p-values of the signifance or at least the standard errors.
My plan was to do it with Python but unfortunately none of the package fulfilled my needs. </p>

<p>As I need a solution for this problem really quick, I thought to use R in this case. Unfortunately I don't have the time to read all about the package R offers, so I just liked to ask if there is any way to do the above mentioned in R?</p>
"
"0.0762492851663023","0.0794719414239026","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.124514561272938","0.12977713690461","215447","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and the second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<pre><code>X1= friends and X2= location
X1= friends and X2= time
X1= public and X2= location
X1= public and X2= time
</code></pre>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. </p>

<p>The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way. If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks in advance.</p>
"
"0.124514561272938","0.12977713690461","215532","<p>I am a clinicians (limited statistical knowledge) who is trying to use mlogit pkg in R to analyze a clinical dataset, running logistic regression on it. I am trying to ascertain if there is any correlation seen in patients with many vars and heart block (var = block (0s and 1s))</p>

<p>I have a dataset with these variables</p>

<pre><code> [1] ""Age""                    ""Sex""                    ""Race""                       ""Obesity""               
 [5] ""CAD""                    ""HTN""                    ""DM""                     ""HLD""                   
 [9] ""CHF""                    ""COPD""                   ""Asthma""                 ""Thyroid.disorder""      
[13] ""Smoking""                ""Illicit.drug.use""       ""Alcohol""                ""INR""                   
[17] ""TB""                     ""AST""                    ""ALT""                    ""Cirrhosis""             
[21] ""Adenosine""              ""Amiodarone""             ""Beta.blocker""           ""CCB""                   
[25] ""Digoxin""                ""TCA""                    ""SSRI""                   ""Antipsychotic""         
[29] ""AV.block""               ""Bundle.branch.block""    ""PAC.PVC""                ""Afib""                  
[33] ""Other.Arrythmia""        ""Nonspecific.ST.wave""    ""Anterioseptal..ST.wave"" ""Anteriolateral.ST.wave""
[37] ""Inferior.ST.wave""       ""Posterior.ST.wave""      ""Axis.deviation""         ""Low.voltage""           
[41] ""Qt.prolongation""        ""Hypertrophy""            ""block""
</code></pre>

<p>Now I have used the mlogit pkg in R</p>

<p>My Code is</p>

<pre><code># Reshaping data
mydata &lt;- mlogit.data(data=mydata, shape=""wide"", choice=""block"")

# Creating Model with all Vars
model &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking+Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy+INR+TB+AST+ALT)
</code></pre>

<p>Generates this error :</p>

<pre><code> Error in solve.default(H, g[!fixed]) : 
 Lapack routine dgesv: system is exactly singular: U[43,43] = 0
</code></pre>

<p>Now if I break the variables into different variables like..</p>

<pre><code>model1 &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking)
model2 &lt;- mlogit(data=mydata, formula=block~0|Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy)
model3 &lt;- mlogit(data=mydata, formula=block~0|INR+TB+AST+ALT)
</code></pre>

<p>IT WORKS without any errors.</p>

<p>But here I would like to know,
how breaking into different models would change my Coefficients?
What should I do to avoid the error and try to incorporate all variables in one model?
How should I interpret my results if break into 3 different models as opposed to one?</p>

<p>Any help is highly appreciated.</p>
"
"0.0440225453162812","0.0458831467741124","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.176395628469996","0.194665705356915","216122","<p>As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. </p>

<p>Normally, we can use <code>glm</code> to obtain the logistic models by <code>glm(y ~ x1+x2, data = dat, family = binomial(logit))</code>. </p>

<p>For frm, we change <code>family = binomial(logit)</code> to <code>family = quasibinomial(logit)</code>.  </p>

<p>I noticed we can also use <code>family = binomial(logit)</code> to obtain frm's parameter since it gives the same estimated values. See the following example</p>

<pre><code>library(foreign)
mydata &lt;- read.dta(""k401.dta"")


glm.bin &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = binomial('logit'))
summary(glm.bin)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = binomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.074e+00  8.869e-02  12.110  &lt; 2e-16 ***
mrate        5.734e-01  9.011e-02   6.364 1.97e-10 ***
age          3.089e-02  5.832e-03   5.297 1.17e-07 ***
sole         3.636e-01  9.491e-02   3.831 0.000128 ***
totemp      -5.780e-06  2.207e-06  -2.619 0.008814 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: 1997.6

Number of Fisher Scoring iterations: 6
</code></pre>

<p>And for <code>family = quasibinomial('logit')</code>,</p>

<pre><code>glm.quasi &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = quasibinomial('logit'))
summary(glm.quasi)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = quasibinomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.074e+00  4.788e-02  22.435  &lt; 2e-16 ***
mrate        5.734e-01  4.864e-02  11.789  &lt; 2e-16 ***
age          3.089e-02  3.148e-03   9.814  &lt; 2e-16 ***
sole         3.636e-01  5.123e-02   7.097 1.46e-12 ***
totemp      -5.780e-06  1.191e-06  -4.852 1.26e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2913876)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>The estimated Beta from both <code>family</code> are the same, but the difference is the SE values.  However, to obtain the correct SE, we have to use <code>library(sandwich)</code> as in this <a href=""http://stackoverflow.com/questions/37584715/fractional-response-regression-in-r"">post</a>.</p>

<p>Now, my questions:</p>

<ol>
<li>What is the difference between these two codes?</li>
<li>Is frm about to obtain robust SE?</li>
</ol>

<p>If my understanding is not correct, please give some suggestions.</p>
"
"0.0880450906325624","0.0917662935482247","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.124514561272938","0.12977713690461","218471","<p>Apologies if this has been asked before, I have done a lot of googling but finding it hard to wrap my head around this particular problem.  Also apologies if I use the wrong terminology for things, I am self learning statistics and I am just starting.</p>

<p>I am using R to find a logistical model for a data set consisting of 314 rows.  I am using 3 variables to find a ""risk"" value.  The code I wrote so far is as follows.</p>

<pre><code>model = glm(binary_result ~ var1 + var2 + var3, data = import_data, family = binomial(link = ""logit""))

summary(model)
</code></pre>

<p>The estimated coefficients given were</p>

<pre><code>(Intercept)       -5.83951    
var1              0.06819   
var2              12.14347   
var3              -0.10594   
</code></pre>

<p>I can calculate my risk value as follows</p>

<pre><code>-5.83951 + 0.06819 * var1 + 12.14347 * var2 + -0.10594 * var3 = risk
</code></pre>

<p>In my data 57 out of 314 rows are equal to 1.</p>

<p>I now want to calculate an optimum threshold value for ""risk"" which will reduce my 1 to 0 ratio so that for every 10 rows I should only have 1 row equal to 1.  </p>

<p>Is R capable of doing this sort of thing, if so, how? I looked into 2 functions tfromx and optim.thresh already, but I am not sure if these are what I need or how I can apply them.</p>

<p>Please don't hesitate to ask more questions, I hope I made sense.</p>
"
"0.0984374038697697","0.102597835208515","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.0984374038697697","0.102597835208515","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.116472706986951","0.121395395733377","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.186771841909407","0.17303618253948","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0762492851663023","0.0794719414239026","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.116472706986951","0.121395395733377","221427","<p>I want to understand the interpretation of logistic regression coefficients in terms of an increase in probability of dependent variable being 1. </p>

<p>I tested a logistic regression model in R and got the following coefficients (all statistically significant):</p>

<pre><code>&gt; mud$coefficients
  (Intercept)          var1          var2          var3          var4
-3.557573e+00  1.051031e-01  4.937244e-07 -1.308386e-06  3.937646e-01
</code></pre>

<p>Raising these numbers to the power of e resulted in numbres below. I would interpret them so that a 1 unit increase in var1 would increase the probability of dependent variable being 1 by 11% and 1 000 000 unit decrease in var3 would increase that probability by 1%. </p>

<pre><code>&gt; exp(mud$coefficients)
(Intercept)       var1        var2        var3        var4
0.02850792  1.11082516  1.00000049  0.99999869  1.48255150
</code></pre>

<p>As suggested in a previous a question (<a href=""http://stats.stackexchange.com/a/24422/121763"">http://stats.stackexchange.com/a/24422/121763</a>), below is what I should actually do to find the probabilities.</p>

<pre><code>&gt; exp(mud$coefficients)/(1+exp(mud$coefficients))
(Intercept)       var1        var2        var3        var4 
0.02771774  0.52625162  0.50000012  0.49999967  0.59718862
</code></pre>

<p>So which numbers should I use if I'd like to express the effect of independent variables on the probability of and event occurring? E.g. in case of var1 is it 11% or 53% or something else? </p>
"
"0.166019415030584","0.17303618253948","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.107832773203438","0.112390297389803","221525","<p>I am using the <code>svyglm</code> function in the <code>survey</code> package in <code>R</code> to fit logistic regression models to a stratified, cluster survey. I want to calculate confidence intervals for my regression coefficients. The default method for <code>confint.svyglm</code> says that it creates Wald confidence intervals by adding and subtracting a multiple of the standard error. But the confidence interval this produces is not consistent with the p-value from the model - confidence intervals that do not overlap 0 still have p-values greater than .05.</p>

<p>I tried to replicate the p-value and confidence interval calculations by hand. It appears the p-value is calculated using a t-test, with the df of the t distribution taken from the residual degrees of freedom from the model. So far so good. But the confidence interval provided by <code>confint.svyglm</code> is just coefficient +/- 1.96*standard.error. This seems wrong - for a 95% confidence interval, I think the multiplier for the standard error should be the .975 quantile of a t-distribution with the appropriate degrees of freedom (in my case 10), which can be somewhat different from 1.96 (the .975 quantile of a z-distribution). True? Has anyone else had this problem? I am relatively new to working with survey data. Is there a reason to always use the z-quantile instead of the t-quantile for complex surveys specifically, or is this just a bug in the package?</p>
"
"0.132732968300475","0.138342892773215","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.170498584867618","0.177704663327728","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225645","<p>I am trying to perform a logistic regression with the following code </p>

<blockquote>
  <p><code>Y ~ x1+x2+x3,data=data, family=binomial(link=""logit"")</code>. </p>
</blockquote>

<p>However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? </p>
"
"0.132067635948844","0.1223550580643","225697","<p>Let me give a simple example,</p>

<pre><code>set.seed(100)
disease = sample(c(0,1),100,replace = TRUE)
snp1 = sample(c(""AA"",""AB"",""BB""),100,replace = TRUE)
snp2 = sample(c(""XX"",""XY"",""YY""),100,replace = TRUE)

summary(glm(disease~snp1*snp2, family = binomial))
</code></pre>

<p>output1</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  
snp1AB         5.232e-01  9.718e-01   0.538   0.5903  
snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .
snp2XY         4.074e-16  8.498e-01   0.000   1.0000  
snp2YY         1.504e+00  9.280e-01   1.621   0.1051  
snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  
snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  
snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  
snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71
</code></pre>

<p>Output2</p>

<pre><code>snp12 = interaction(snp1,snp2)
summary(glm(disease~snp12, family = binomial))


Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  
snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  
snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .
snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  
snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  
snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  
snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,</p>

<ol>
<li>Are both output1 and output2 same ? </li>
<li>If same, which one is more appropriate?</li>
<li>How to interpret the coefficients (and odds ratios) in each case?</li>
</ol>
"
"0.165084544936054","0.172061800402921","226330","<p>Overall, I'd like to be able to say that, for the logistic prediction for this row, ColA was more influential in driving up the resultant probability (ie, y_hat) than ColB. (We'll use y_hat as it's usually defined for logistic.) But is this possible? Some data scientists I've talked to say yes, but I've also seen push-back.</p>

<p>From what I've read, it seems that GLMs make it easiest to get at a per-row variable importance (see <a href=""http://stats.stackexchange.com/q/190482"">this limited discussion</a> on logit in particular, including push-back). But can they actually do it?</p>

<p>If B1 and B2 are coefficients and the cols in X represent our features, it would seem that if <em>B1</em>*X1 is greater than <em>B2</em>*X2 then <em>B1</em>*X1 would drive the resultant probability towards 1 more than <em>B2</em>*X2. Here's an example (which brings in a factor col, for a full treatment).</p>

<p>We create features X1 and X2, where X1 is random and X2 (I think we can agree) has a large positive impact on y:</p>

<pre><code>set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(1,0,1,0,1,0,1,0,1,0)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)
</code></pre>

<p>Now we create the logit model:</p>

<pre><code>fit.logit = glm(
formula = y~.,
data = df,
family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:       -1.2353      22.0041
Wald statistic:      -0.267        0.003
</code></pre>

<p>Now if we multiply <em>B1</em> and <em>B2</em> by <em>X1</em> and <em>X2</em> respectively and print the results:</p>

<pre><code>coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)

          X1       X2
1  -0.55087679 22.00411
2  -0.48751729  0.00000
3  -0.59755734 22.00411
4  -1.13510089  0.00000
5  -1.04245907 22.00411
6  -0.63908954  0.00000
7  -0.53998690 22.00411
8  -0.42395777  0.00000
9  -0.01916833 22.00411
10 -0.14575621  0.00000
</code></pre>

<p>We see that in the rows where X2 = 1 then <em>B2</em>*X2 (ie the second column) is much higher than <em>B1</em>*X1 (ie the first column). So it would seem that we could say that for those rows that X2 would be the dominant feature driving up the resultant prediction towards 1.</p>

<p>If one reverses the y dependency on X2 by replacing zeros for ones in X2, then after doing the multiplication, <em>B2</em>*X2 has a much lower value than <em>B1</em>*X1 when X2 = 1, which makes sense (since X2 now pushes y_hat towards 0 when X2 = 1). Thus, for these rows, X1 is actually more ""responsible"" for driving y_hat towards 1. (Note that if both results are negative, then the least negative would be the feature more responsible for y_hat being as high as it is.) Because of this, it would seem that this method of per-row feature ranking still works. What am I missing? </p>

<p>In case it helps, the code for the latter (reversed dependency) case is below: </p>

<pre><code># Reverse y dependency on X2
set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(0,1,0,1,0,1,0,1,0,1)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)

fit.logit = glm(
  formula = y~.,
  data = df,
  family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:        -1.235      -22.004
Wald statistic:      -0.267       -0.003

coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)
multiply_res

            X1        X2
1  -0.55087679   0.00000
2  -0.48751729 -22.00411
3  -0.59755734   0.00000
4  -1.13510089 -22.00411
5  -1.04245907   0.00000
6  -0.63908954 -22.00411
7  -0.53998690   0.00000
8  -0.42395777 -22.00411
9  -0.01916833   0.00000
10 -0.14575621 -22.00411
</code></pre>

<p>Overall, for logistic, can we accurately say (for example) that feature A drives y_hat toward 1 more than feature B, for this individual prediction? </p>

<p>Thanks, all!</p>
"
"0.164717281867254","0.17167901505579","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"NaN","NaN","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.116472706986951","0.121395395733377","229336","<p>I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.</p>

<p>The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.</p>

<p>I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)</p>

<p>Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:</p>

<p>I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciated</p>

<pre><code>Here is the R output and sample of dataset:





 &gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+ 
                                  D2011+D2012,family=binomial, data=orf)
      summary(mod)

    Call:
    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) + 
        D2011 + D2012, family = binomial, data = orf)

    Deviance Residuals: 
       Min      1Q  Median      3Q     Max  
    -1.862  -1.293   1.023   1.065   1.318  

    Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)        0.3917290  0.1626769   2.408    0.016 *
    F2011              0.0003269  0.0002782   1.175    0.240  
    F2012             -0.0003596  0.0002786  -1.291    0.197  
    as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708  
    D2011             -0.0311978  0.0272068  -1.147    0.252  
    D2012              0.0226963  0.0274981   0.825    0.409  
    Small sample data set:

    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012
    155     150     1       0               0               0
    740     760     2       0               1               1
    1000    850     1       0               0               0
    1630    1520    1       1               1               1
    0       460     1       0               0               0
    1300    1335    1       0               1               1
    450     450     1       0               0               0
    390     730     1       1               0               1
    390     380     2       0               0               0
    600     600     2       0               0               0
</code></pre>
"
"0.186771841909407","0.17303618253948","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"0.152498570332605","0.145698559277155","229598","<p>This isn't a problem with correlation between predictors - I have two models, each considers only one of the variables. That is the only difference between the models.  </p>

<p>I'm estimating the probability of an diagnosis given some confounders and a measure of monthly temperature. I have two possible temperature definitions I'm considering: monthly average temperature and monthly average <em>high</em> temperature. I don't expect the response to temperature to be linear, so I broke average temperature into 5 degree bins with bottom and top coding at &lt; 40 and > 90. I did the same with average high temperature but shifted the bins slightly with bottom and top coding &lt; 50 and > 100. </p>

<p>I estimate the first logistic model </p>

<pre><code>event ~ age + sex + ... + mean_temp_group
</code></pre>

<p>and get the response I'd expect from my theorized process. However, I'd prefer to report the results using mean high temperature since average temperature is misleadingly low (average temp of 70, for instance, is pretty warm with highs in the 80s but people think ""70 degrees? That's wonderful!""). So I estimate the same model but instead replace <code>mean_temp_group</code> with <code>mean_high_group</code>:</p>

<pre><code>event ~ age + sex + ... + mean_high_group
</code></pre>

<p>and the results don't match either my theory or what I saw with <code>mean_temp_group</code>. </p>

<p>That seems weird given how similar the two variables are. The average and average high variables have a correlation coefficient of 0.9939. In essence the average high is the average plus a constant (on average, 9.4 degrees with a standard deviation of 2.1). </p>

<p>At first I assumed this was a problem with the code, so I re-pulled the data (still have the same problem and the data extraction seems to be accurate). I also took the model with <code>mean_temp_group</code> and edited the formula in place to read <code>mean_high_group</code> lest I omitted/included a different variable between the models (I didn't). </p>

<p>I assume it has something to do with the binning or something along those lines - any ideas? I'm very confused by two variables that basically appear to be an additive shift of each other giving very different results. </p>
"
"0.107832773203438","0.112390297389803","231777","<p>I have a data frame with 1200 observations and 30 variables and I'am trying to do a multinomial logistic regression to explain the intentions of vote of Tunisian citizens using multinom(). my dependent variable has 10 levels.
When I executed the command multinom () i got this warning 
Warning messages: 1: In sqrt(diag(vc)) : NaNs produced </p>

<p>so i reduced the number of the predictor variables to 13 , the levels of my dependent variable to only 3 and the warning message no longer appears , but once I calculate the p.value the mojority of my predictor variables are non significant.</p>

<pre><code>      &gt; str(k)
    'data.frame':   1081 obs. of  19 variables:
     $ URBRUR    : Factor w/ 2 levels ""Rural"",""Urban"": 2 2 2 2 2 2 2 2 2 2  ...
     $ REGION    : Factor w/ 24 levels ""Ariana"",""Beja"",..: 23 23 23 23 23 23 23 23 23 23 ...
     $ classe_age: Factor w/ 5 levels ""60 ans et plus"",..: 3 5 1 1 3 1 5 4 1 2 ...
       $ Q3A       : Factor w/ 5 levels ""Fairly bad"",""Fairly good"",..: 2 1 1 4 4 4 2 4 1 3 ...
       $ Q3B       : Factor w/ 5 levels ""Fairly bad"",""Fairly good"",..: 2 1 1 3 1 4 2 4 1 3 ...
       $ Q7        : Factor w/ 2 levels ""Going in the right direction"",..: 1 2 2 2 2 2 2 2 2 1 ...
       $ Q14       : Factor w/ 4 levels ""Not at all interested"",..: 4 3 3 2 3 3 3 3 3 4 ...
       $ Q27       : Factor w/ 9 levels ""Did not vote for some other reason"",..: 6 6 6 6 6 3 6 6 6 1 ...
       $ Q46A      : num  9 5 8 0 3 3 4 5 0 3 ...
       $ Q63PT1    : Factor w/ 8 levels "" Services gouvernementaux"",..: 5 5 4 4 4 4 5 4 4 5 ...
       $ Q89A      : Factor w/ 9 levels ""Non"",""Oui, autre"",..: 7 1 1 8 5 1 1 1 1 1 ...
       $ Q96       : Factor w/ 3 levels ""No (looking)"",..: 3 2 2 2 1 2 2 3 2 1 ...
       $ Q96_ARB   : Factor w/ 9 levels ""Agriculteur exploitant"",..: 2 6 4 4 1 6 7 4 6 6 ...
       $ Q97       : Factor w/ 4 levels ""Aucune Ã©ducation formelle "",..: 1 3 1 4 4 3 4 3 1 4 ...
       $ Q98B      : Factor w/ 4 levels ""Not at all important"",..: 4 4 4 4 3 4 4 4 4 4 ...
     #the logistic regression
      library(nnet)
      k$out=relevel(k$Q99,ref = ""Nahdha"")
     fit=multinom(out ~ URBRUR+ REGION +    classe_age+ Q3A +Q3B+ Q7 +  Q14+    Q27+ Q46A+  Q63PT1+ Q96+ Q96_ARB+ Q97   + Q98B,data=k,maxit=3000)

     summary(fit)
     #calculate the p.value
     z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
     p &lt;- (1 - pnorm(abs(z), 0, 1))*2
     p
</code></pre>

<p>this is a part from the output R  </p>

<pre><code>                        (Intercept) URBRUR[T.Urban] REGION[T.Beja] REGION[T.Ben        Arous]
          CPR            0.0000000       0.8006384     0.50724591           0.3490626
          Nahdha         0.6480962       0.9298628     0.09299337           0.2426325
          Nidaa Tounes   0.1547996       0.1210917     0.01340229           0.5486973
                           REGION[T.Bizerte] REGION[T.Gabes] REGION[T.Gafsa]
           CPR                  0.6667980      0.86525482      0.01971166
          Nahdha               0.2933951      0.03008731      0.05240173
          Nidaa Tounes         0.5154798      0.51222561      0.03301253
                         REGION[T.Jendouba] REGION[T.Kairouan] REGION[T.Kasserine]
          CPR                  0.21477728          0.4552543          0.53160327
         Nahdha               0.01548534          0.9322695          0.22102722
         Nidaa Tounes         0.06993081          0.7833111          0.09259959
                       REGION[T.Kebili] REGION[T.Le Kef] REGION[T.Mahdia]
           CPR                0.49607138        0.0000000        0.3084810
           Nahdha             0.09437504        0.6338189        0.1629434
           Nidaa Tounes       0.17968658        0.1360486        0.1955159
</code></pre>

<p>I'm sorry if I am asking a complicated question but I would like an explication for this issue</p>

<pre><code>   &gt; table(k$out)

    Ne pas voter       Nahdha Nidaa Tounes 
     307          292          266 
</code></pre>
"
"0.26423179493102","0.253069795372028","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.176395628469996","0.17303618253948","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.062257280636469","0.064888568452305","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.062257280636469","0.064888568452305","234498","<p>First, many thanks to the developers of <code>stan_glmer</code>. I have fit a mixed logistic model, <code>value ~ (1 | site_no) + Is.mid + sinWeek + cosWeek</code>. When fitted by <code>glmer</code>, it returns just the variance on <code>site_no</code>. From <code>stan_glmer</code>, I get coefficients (intercept effects) on each site, plus a variable named <code>b[(Intercept) site_no:_NEW_site_no]</code> that is reported by <code>stan_summary</code>.</p>

<p>I am after marginal posterior of the random effects. Is that what the <code>_NEW_</code> coefficient is giving me? i.e., Is this a marginal posterior not conditioning on the site in the <code>new = dataset</code> provided?</p>

<p>Also, how do I find a variance on the random effect, or is the model not specified correctly? The call is below. I welcome any other suggestions as well. Thank you in advance for any help in finding documentation for these <code>stan_glmer</code> outputs.</p>

<pre><code>stan.T.mod &lt;- stan_glmer(form, iter = 50000, warmup = 5000, thin = 1,
family = binomial(link = ""logit""),
prior = student_t(location = 0, df = 1, scale = 10),
prior_intercept = student_t(location = 0, df = 7, scale = 1),
prior_covariance = decov(shape = 10, scale = 1),
chains = 4, seed = 0305991, data = sub.data)
</code></pre>
"
"0.0984374038697697","0.102597835208515","234763","<p>I have a fairly large dataset ($\approx 3 \bar{M}$ observations for a dozen candidate predictors) and I would like to perform a logistic regression on that dataset.
I have a problem of separation in that dataset so usual model can't converge. That's why I am using Firth penalization (logistf package for R) to have my model to adjust.</p>

<p>I would like to select the best subset of variables for my final model but I can't find the proper way to do that. I know that stepwise selection is out of question and I usually would use L1 or L2 penalized regression so that some coefficients are reduced to 0.</p>

<p>My problem is : the function I am using to adjust my model doesn't handle extra penalization so no Elasticnet-Firth regression.</p>

<p>Is there, apart from stepwise selection, another way to select my variables ?</p>

<p>Thanks in advance !</p>
"
"0.0984374038697697","0.102597835208515","235402","<p>I am a newbie at R. I am trying to do some logistic regressions. My predictors are categorical, and most have more than two levels.</p>

<p>A couple questions:
1. It looks like R already creates the relevant contrasts for the categorical predictors (I am used to SAS where I need to specify all the contrasts). Is this correct for R? As in, I do <strong>not</strong> need to manually create the contrasts myself?</p>

<ol start=""2"">
<li><p>the ""family=binomial"" step in the glm syntax, will I always need to write this regardless of the number of predictors, and even if the categorical predictors have more than two levels (DV is always binary)?</p></li>
<li><p>For interpreting the results, if I use a categorical predictor (e.g., lettergroup) with different levels (a, b, c, d) as my predictor, with the binary DV as 1=yes and 0=no, if R created the lettergroupb contrast, and it is a significant, positive coefficient, this means b is more likely to say yes compared to a, c, and d? thank you!</p></li>
</ol>
"
