"V1","V2","V3","V4"
"0.0635000635000953","0.0719815750748694","  3845","<p>Please see <a href=""http://stats.stackexchange.com/questions/3708/forecasting-unemployment-rate"">question</a> for the background.</p>

<p>Following the advice of @kwak and @Andy W, I have decided to use the package <strong>plm</strong> in <code>R</code> to fit my model. Here an excerpt of the data <code>df</code> (the numbers are made up, not real data!):</p>

<pre>    reg year          ur         mur cl
1    1 2001 0.000698717 0.012483361  1
2    2 2001 0.008283762 0.011899896  1
3    3 2001 0.001863817 0.012393738  1
4    4 2001 0.005344206 0.012126016  1
5    5 2001 0.007475083 0.011962102  1
6    6 2001 0.002785111 0.012322869  1
</pre>

<p>where <strong>reg</strong> is the region indicator, <strong>year</strong> is the year of measurement, <strong>ur</strong> is the unemployment rate, <strong>mur</strong> is the average unemployment rate in the neighboring regions (given by <strong>cl</strong>) excluding the current region (see @kwak suggestion). Below is the code that I used in <code>R</code></p>

<pre><code>fm &lt;- pgmm(log(ur)~lag(log(ur),1)+log(mur)|lag(log(ur),2:40),data=df)
</code></pre>

<p>I have several question regarding this model: </p>

<ul>
<li>I guess I should choose <code>effect=""individual""</code> to avoid having time dependent intercepts (?). But doing so the code is crashing (Error in names(coefficients) &lt;- c(namesX, namest))!</li>
<li>Which <code>model</code> should I choose, <code>onestep</code> or <code>twosteps</code>?</li>
<li>How do I decide on the numbers of instruments (40 is just a guess)?</li>
</ul>

<p>Assuming that I have fitted this model successfully, how do I simulate from it?  </p>

<p>Regards  </p>
"
"NaN","NaN","  5090","<p>The R package <a href=""http://cran.r-project.org/web/packages/dlm/index.html"" rel=""nofollow"">dlm</a> implements filtering and smoothing (<code>dlmFilter</code> and <code>dlmSmooth</code>) for models with regression effects, but forecasting is not (yet) available for these models:</p>

<pre><code>mod &lt;- dlmModSeas(4)+dlmModReg(cbind(rnorm(100),rnorm(100)))
fi &lt;- dlmFilter(rnorm(100),mod)
f &lt;- dlmForecast(fi,nAhead=12)
Error in dlmForecast(fi, nAhead = 12): 
  dlmForecast only works with constant models
</code></pre>

<p>How can I do this in R?</p>

<p>Thanks for your help!</p>
"
"0.127000127000191","0.143963150149739","  5170","<p>I am new to forecasting in R and am trying to automatically fit an ARIMA model to what I believe is a univariate dataset.</p>

<pre><code>&gt; str(p1.z)
'zoo' series from 2009-04-05 to 2010-10-31
  Data: int [1:83] 360 570 540 585 570 690 495 660 510 690 ...
  Index: Class 'Date'  num [1:83] 14339 14346 14353 14360 14367 ...

&gt;  head(p1.z) 
  2009-04-05 2009-04-12 2009-04-19 2009-04-26 2009-05-03 2009-05-10 
         360        570        540        585        570        690
</code></pre>

<p>But when I try to fit the model, I get the error as seen below.</p>

<pre><code>&gt; p1.arima &lt;- auto.arima(p1.z)
Error in nsdiffs(xx) : Non seasonal data
</code></pre>

<p>It is my understanding that the forecast package and the auto.arima function would be able to fit my data seasonal or not.  I am trying to learn time series forecasting and am using a dataset that appears to be ideal for this sort of task .  Also, the function ets() was able to find a model.</p>

<p>Any help you can provide will be greatly appreciated</p>
"
"0.245934688418982","0.241612315965582","  5479","<p>I am teaching myself DLM's using R's <code>dlm</code> package and have two strange results. I am modeling a time series using three combined elements: a trend (<code>dlmModPoly</code>), seasonality (<code>dlmModTrig</code>), and moving seasonality (<code>dlmModReg</code>).</p>

<p>The first strange result is with the <code>$f</code> (one-step-ahead foreacast) result. Most of this forecast appears to be one month behind the actual data, which I believe I've seen in many examples of one-step-ahead forecasting online and in books. The strange thing is that the moving seasonality is NOT similarly lagged, but hits exactly where it should. Is this normal?</p>

<p>If I use the result's <code>$m</code> to manually assemble the componenet, everything lines up perfectly, so it's weird, though it makes sense in a way: the moving seasonality has exogenous data to help it while the rest of the forecast does not. (Still, it'd be nice to simply <code>lag</code> the resulting <code>$f</code> and see a nice match.)</p>

<p>More troubling is the difference I see if I change the degree of <code>dlmModPoly</code>'s polynomial (from 1 to 2) in an attempt to get a smoother level. This introduces a huge spike in all three components at month 9. The spikes all basically cancel out in the composite, but obviously make each piece, say the level or the seasonality, look rather ridiculous there.</p>

<p>Is this just one of those things that happens and I should be prepared to throw away the result's first year of data as ""break-in""? Or is it an indication that something is wrong? (Even in the degree 1 polynomial case, the first year's moving seasonality's level is a bit unsettled, but no huge spike as when I use a degree 2 polynomial.)</p>

<p>Here is my R code:</p>

<pre><code>lvl0 &lt;- log (my.data[1])
slp0 &lt;- mean (diff (log (my.data)))

buildPTR2 &lt;- function (x)
   {
   pm &lt;- dlmModPoly (order=1, dV=exp (x[1]), dW=exp (x[2]), m0=lvl0)
   tm &lt;- dlmModTrig (s=12, dV=exp (x[1]), q=2, dW=exp (x[3:4]))
   rm &lt;- dlmModReg (moving.season, dV=exp (x[1]))

   ptrm &lt;- pm + tm + rm
   return (ptrm)
   }

mlptr2 &lt;- dlmMLE (log (my.data), rep (1, 6), buildPTR2)
dptr2 &lt;- buildPTR2 (mlptr2$par)
dptrf2 &lt;- dlmFilter (log (my.data), dptr2)

tsdiag (dptrf2)

buildPTR3 &lt;- function (x)
   {
   pm &lt;- dlmModPoly (order=2, dV=exp (x[1]), dW=c(0, exp (x[2])), m0=c(lvl0, slp0))
   tm &lt;- dlmModTrig (s=12, dV=exp (x[1]), q=2, dW=exp (x[3:4]))
   rm &lt;- dlmModReg (moving.season, dV=exp (x[1]))

   ptrm &lt;- pm + tm + rm
   return (ptrm)
   }

mlptr3 &lt;- dlmMLE (log (my.data), rep (1, 8), buildPTR3)
dptr3 &lt;- buildPTR3 (mlptr3$par)
dptrf3 &lt;- dlmFilter (log (my.data), dptr3) 
</code></pre>

<p>Per the follow-on question: the data itself is monthly data for 10 years, with each month being the weekly average attendance at a theatrical production. The data definitely has seasonal and moving seasonal effects. I want to model the trend and the seasonal effects to give the management some insight, and to prepare for forecasting. (Which is not directly possible with <code>dlm</code> when you include a <code>dlmModReg</code> component, though that's the next step.)</p>

<p>(I am trying to use an order=2 polynomial component that I believe creates an IRW trend, which is supposed to be nicely smooth.)</p>

<p>If it matters, my moving seasonality is a yearly Big Bash Gala event that can fall in two different months, and I indicate it with 0 for most months and 1 for months in which the Big Bash falls.</p>
"
"0.168005376258062","0.136032390447736","  6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.179605302026775","0.203594639423715"," 13950","<p>As with my previous question, I'm looking at ways to impute missing data in a hierarchical time series data.</p>

<p>With al my other procedures, including the experimentation of imputation packages (<code>Amelia</code>, <code>HoltWinters</code> from <code>Forecast</code> and <code>MICE</code> imputation) I've only been able to use the time series data prior to the missing gap.</p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2001 220 194 238 190 217 244 242 225 242 259 267 244
2002 212 246 250 236 261 286 265 269 226 267 234 246
2003 202 199 297 272 236 266 235 226 260 183 226 265
2004 211 215 219 213 240 236 273 266 262 244 241 235
2005 212 198 233 251 259 282 305 267 241 264 222 269
2006 182 220 250 287 279 281 286 332 300 272 221 233
2007  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
2008 193 215 235 242 246 315 326 280 279 239 236 258
2009 246 189 257 241 268 223 260 288 234 260 216 195
</code></pre>

<p>I'm trying to do simple imputation procedure that uses forecasting and backcasting estimates from the time series model. Forecasting using prior data to predict the future and backcasting  using the later data to â€œpredictâ€ the past.</p>

<p>I would then like to combine the forecast and backcast value to use as imputation. After which I will look at the fit etc.</p>

<p>How do I go about this in coding? </p>

<p>For example, I'm able to determine what SARIMA model exist for the first period 2001-end2006. But not the full period (because my basic functions I know from R does not support the NA values.)</p>

<p>This is only for the period 2001-end2006:</p>

<pre><code>ARIMA(2,0,2)(1,0,1)[12] with non-zero mean 

Call: auto.arima(x = ts.datt) 

Coefficients:
         ar1      ar2      ma1     ma2    sar1     sma1  intercept
      1.3610  -0.8258  -1.2407  0.9191  0.8982  -0.7560   244.8374
s.e.  0.0884   0.0960   0.0878  0.1127  0.2190   0.3335     6.1894

sigma^2 estimated as 605.9:  log likelihood = -335.01
AIC = 686.02   AICc = 688.3   BIC = 704.23
</code></pre>

<p>Should I just model the first period, predict by <code>forecast</code>; model then the last period separately and then backcast? How will I do this backcasting (ie. 'predicting' the past)?</p>

<p><strong>EDIT:</strong>
What I'm asking:
1) How do I use the data from years 2008 &amp; 2009 to BACKCAST? I already know how to use 2001-2006 to forecast. </p>

<p>2) How do I determine the SARIMA model for the whole period? (2001-2009) ie. </p>
"
"NaN","NaN"," 15039","<p>I'm taking an intro to marketing class this semester and the course requires us to buy a software which looks nothing more than a bunch of excel macros. From the <a href=""http://www.decisionpro.biz/products/marketing-engineering-for-excel"">software's page</a></p>

<blockquote>
  <p>Marketing Engineering for Excel provides tools to help address the most common marketing problems Segmentation, Targeting and Positioning (STP); New Product Decision; Sales Forecasting; Advertising and Communication Decisions; Salesforce and Channel Decisions; Pricing; and Sales Promotion Decisions. Users will be able to harness the power of world-class analytics to run quick and simple analyses... all from within Excel.</p>
</blockquote>

<p>I'll buy the license for the software (40 bucks for 6 months)... but can anyone tell me if there are R packages that do similar stuff?</p>
"
"0.0635000635000953","0"," 19620","<p>I've heard a bit about using <a href=""http://stats.stackexchange.com/questions/9842/getting-started-with-neural-networks-for-forecasting"">neural networks to forecast time series</a>, specifically <a href=""http://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis"">recurrent neural networks</a>.</p>

<p>I was wondering, is there a recurrent neural network package for R?  I can't seem to find one on <a href=""http://cran.r-project.org/web/views/TimeSeries.html"">CRAN</a>.  The closest I've come is the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=tsDyn%3annet"">nnetTs</a> function in the <a href=""http://cran.r-project.org/web/packages/tsDyn/index.html"">tsDyn</a> package, but that just calls the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=nnet%3annet"">nnet</a> function from the <a href=""http://cran.r-project.org/web/packages/nnet/index.html"">nnet</a> package.  There's nothing special or ""reccurant"" about it.</p>
"
"0.127000127000191","0.143963150149739"," 25230","<p>I am presently trying to learn R.  I would like to be able to apply it more in my work environment as I am an analyst in the Health Care industry.  I am presently trying to use R to forecast.  What is the best forecasting package in R?  </p>

<p>I am presently using the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package.  I have tried to fit the <code>ets</code> models to my data but I feel that it is giving me some fairly unreasonable solutions.  The data is flat, meaning that it does not linearly increase and there are some fluctuations, but I have not been able to assess whether or not those fluctuations are seasonal.  I am assuming they are not. </p>

<p>How can I calculate the out of sample error when I am comparing forecasting models?  Also, is there a way to plot my forecasted data against the actual values?  Lastly, how can I determine the model that is generated from the forecast?  </p>

<p>Thanks for all of your help in advance.</p>
"
"0.168005376258062","0.19044534662683"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.155542754209564","0.17631812981527"," 29424","<p>I'm looking for some forecasting advice when dealing with seasonal time series data that has a large number of observations.  By ""large"" I only mean a few thousand --- I'm used to such sizes in Data Mining being considered pretty small, but it seems that in time series modeling that's pretty unwieldy for many of the tools I've tried.</p>

<p>For example, here's a toy data set that records an observation once per minute, for five days:</p>

<pre><code>set.seed(123)
t &lt;- 1:(5*24*60)
x &lt;- ts(15 + 0.001*t + 10*sin(2*pi*t/(length(t)/5)) + rnorm(length(t)), freq=length(t)/5)
plot(x, type='l')
</code></pre>

<p><img src=""http://i.stack.imgur.com/xVSCN.png"" alt=""time series plot""></p>

<p>(In my real operational data set, the values are observed at irregular intervals, but I've regularized them by doing something like <code>x &lt;- approx(d$t, d$x, xout=1:(5*24*60))</code> first.  Advice on whether that's advisable, or alternative approaches, is welcome too.)</p>

<p>So the seasonality in this data set has a lag of 1,440 observations, which seems to be way outside the range that things like <code>auto.arima()</code> (in the <code>forecast</code> package) will find:</p>

<pre><code>m1 &lt;- auto.arima(x)
plot(forecast(m1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/ccnGc.png"" alt=""prediction plot""></p>

<p>And I'm not quite sure how to interpret the <code>ets()</code> function here, but it doesn't seem to be able to handle this size data, and it didn't seem to pick up on the seasonality:</p>

<pre><code>&gt; m2 &lt;- ets(x, 'MAZ')
&gt; plot(forecast(m2))
Error in forecast.ets(m2) : Forecast horizon out of bounds
&gt; m2$method
[1] ""ETS(M,A,N)""
</code></pre>

<p>Where to go from here?  Any suggestions?  Thanks.</p>
"
"0.0898026510133875","0.101797319711858"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.141990458561766","0.160955694994913"," 32694","<p>I'm using R together with the <code>forecast</code> package to set up a ARIMA model, that will be used to predict a energy related variable. I used <code>auto.arima()</code> to fit different models (according to geographic region), and I need to put the model coefficients in our database, so that the IT folks can automate things. That's exactly the problem: I simply don't know how set up the equations by looking at the model:</p>

<pre><code>ARIMA(1,0,1)(2,0,1)[12] with non-zero mean 

Coefficients:

       ar1     ma1    sar1    sar2     sma1   intercept    prec0    prec1
     0.3561  0.3290  0.6857  0.2855  -0.7079  11333.240   15.5291  28.0817

s.e. 0.2079  0.1845  0.2764  0.2251   0.3887   2211.302    6.2147   6.0906
</code></pre>

<p>I have 2 regressor variables (prec0 and prec1). Given the residuals, the ARIMA vector <code>ARIMA(1,0,1)(2,0,1)[12]</code>, the time series up to period $t$, the number $h$ of forecasting periods and the regressor matrix reg, how can I set a function to return the forecast values? I.e:</p>

<pre><code>do.forecast = function(residuals, ARIMA, timeSeries, h, regMatrix)
{
  p = ARIMA[1]
  q = ARIMA[3]

  ## arima equations here...
}
</code></pre>

<p>Thanks!  </p>

<p>PS: I know this is a possible duplicate of <a href=""http://stats.stackexchange.com/questions/23881/reproducing-arima-model-outside-r"">Reproducing ARIMA model outside R</a>, but my model seems very different, and I really don't know how to start with.</p>
"
"0.155542754209564","0.146931774846058"," 34493","<p>I am using both R and SAS for the time series modeling. There is an option in SAS that I could not find so far in any packages developed in R for the time series modeling such as TSA or forecast package, at least to the best of my knowledge! To explain more, if we use the windowing environment in SAS to fit an ARIMA model with a regressor, we basically choose:  </p>

<p>Solution->Analysis->Time series Forecasting System->Develop Models<br>
Then Fit ARIMA model -> Predictors->Dynamic Regressors</p>

<p>If we ask to forecast this model, SAS says â€œThe following regressor(s) do not have any forecasting models. The system will automatically select forecasting models for these regressorsâ€. This means that we have not provided the values of the regressors over the forecasting period, and the system tries to find a model for that.</p>

<p>My questions:</p>

<ol>
<li>Is there any package in R with the same capability (explained above) as in SAS to forecast an ARIMA model?  </li>
<li>How can SAS automatically forecast the regressor(s) and based on what models?</li>
</ol>
"
"0.276790359705331","0.313760411548242"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.0898026510133875","0.0508986598559288"," 45762","<p>I have been working in R the last few weeks and have been tinkering with forecasting/predicting values for the financial data.</p>

<p>Is there a good place to find out what the different variables represent? Such as m in caret package, size. C, degree, sigma, and scale. </p>

<p>For example I am trying to use the Support Vector Machines models in caret but they require some input parameters for the different types.</p>

<pre><code>Support Vector Machines
type, package, variables
svmLinear, kernlab, C
svmRadial, kernlab, C, sigma
svmRadialCost, kernlab, C
svmPoly, kernlab, degree, scale, C
</code></pre>

<p>I found the above in documentation regarding the SVM Models in the caret package. However, no explanation on what the variables mean....</p>

<p>Would there be a good place to figure out what these variables mean? Or are they already meant to be known by even novices? If so where can I find out about them?</p>

<p>I think it would be best if I knew what these were instead of just changing them randomly.</p>
"
"0.179605302026775","0.178145309495751"," 47185","<p>I am impressed by the R <code>forecast</code> package, as well as e.g. the <code>zoo</code> package for irregular time series and interpolation of missing values.</p>

<p>My application is in the area of call center traffic forecasting, so data on weekends is (nearly) always missing, which can be nicely handled by <code>zoo</code>. Also, some discrete points may be missing, I just use R's <code>NA</code> for that.</p>

<p>The thing is: all the nice magic of the forecast package, such as <code>eta()</code>, <code>auto.arima()</code> etc, seem to expect plain <code>ts</code> objects, i.e. equispaced time series not containing any missing data. I think real world applications for equispaced-only time series are definitely existent, but - to my opinion -  v e r y  limited.</p>

<p>The problem of a few discrete <code>NA</code> values can easily be solved by using any of the offered interpolation functions in <code>zoo</code> as well as by <code>forecast::interp</code>. After that, I run the forecast.  </p>

<p>My questions:  </p>

<ol>
<li>Does anyone suggest a better solution?</li>
<li><p><strong>(my main question)</strong> At least in my application domain, call center traffic forecasting (and as far as I can imagine most other problem domains), time series are not equispaced. At least we have recurring ""business days"" scheme or something. What's the best way to handle that and still use all the cool magic of the forecast package?  </p>

<p>Should I just ""compress"" the time series to fill the weekends, do the forecast, and then ""inflate"" the data again to re-insert NA values in the weekends? (That would be a shame, I think?)  </p>

<p>Are there any plans to make the forecast package fully compatible with irregular time series packages like zoo or its? If yes, when and if no, why not?  </p></li>
</ol>

<p>I'm quite new to forecasting (and statistics in general), so I might overlook something important.</p>
"
"0.168005376258062","0.163238868537283"," 58407","<p>I think this is a basic question, but maybe I am confusing the concepts.</p>

<p>Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?</p>

<p>If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.</p>

<p>Thank you.</p>

<p>Bruno</p>

<hr>

<h2>Update 1:</h2>

<p>I added some code below. The variance given by <code>sigma2</code> isn't close to the one calculated from the fitted values. I'm still wondering if <code>sigma2</code> is the right option. See figure below for time series plot.</p>

<pre><code>demand.train &lt;- c(10.06286, 9.56286, 10.51914, 12.39571, 14.72857, 15.89429, 15.89429, 17.06143, 
              17.72857, 16.56286, 14.23000, 15.39571, 13.06286, 15.39571, 15.39571, 16.56286,
              16.21765, 15.93449, 14.74856, 14.46465, 15.38132)
timePoints.train &lt;- c(""Q12006"", ""Q22006"", ""Q32006"", ""Q12007"", ""Q22007"", ""Q32007"", ""Q12008"", ""Q22008"",
                      ""Q32008"", ""Q12009"", ""Q22009"", ""Q32009"", ""Q12010"", ""Q22010"", ""Q32010"", ""Q12011"",
                      ""Q22011"", ""Q32011"", ""Q12012"", ""Q22012"", ""Q32012"")

plot(1:length(timePoints.train), demand.train, type=""o"", xaxt=""n"", ylim=c(0, max(demand.train) + 2), 
     ylab=""Demand"", xlab=""Quadrimestre"")

title(main=""Time Series Demand of Product C"", font.main=4)
axis(1, at=1:length(timePoints.train), labels=timePoints.train)
box()

### ARIMA Fit
library(forecast)

# Time series
demandts.freq &lt;- 3
demandts.train &lt;- ts(demand.train, frequency=demandts.freq, start=c(2006, 1))

# Model fitting
demandts.train.arima &lt;- auto.arima(demandts.train, max.p=10, max.q=10, max.P=10, max.Q=10, max.order=10)
print(demandts.train.arima)
summary(demandts.train.arima)
demandts.train.arima.fit &lt;- fitted(demandts.train.arima)

# Forecast ARIMA (conditional means)
demandts.arima.forecast &lt;- forecast(demandts.train.arima, h = 3, level=95)
print(demandts.arima.forecast)

# Constant variance from ARIMA
demandts.arima.var &lt;- demandts.train.arima$sigma2
print(demandts.arima.var)

# Variance from fitted values
print(var(demandts.train.arima.fit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/E5gv0.png"" alt=""Time Series Plot""></p>
"
"0.109985336266015","0.124675745238507"," 58559","<p>I have just started to learn about forecasting. I thought it would be easy to create forecast models for a daily time series but have encountered a number of difficulties. Firstly most examples and available datasets are either in months or quarters. It is rare to find examples for weeks and days. Secondly it also appears difficult to create a timeseries object for days (365) and weeks (52) as these vary between years. This may just be the way the timeseries object works in R. I have had to use Zoo. I also have a concern that my data may not be properly modeled for use in packages like Forecast and HTS.</p>

<p>I am interested in how best to approach this problem. Any examples of forecasting to daily events that may cycle across years would be greatly appreciated.</p>
"
"0.254000254000381","0.26993090653076"," 58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.228952734944813","0.259533259821102"," 62211","<p>I am working with weather forecasters and have access to historical climatology data. Given current weather conditions in an area of interest (i.e. the current ""map""), we want to try to find the most similar ""map"" from the past data. The idea is to try and make a weather forecast by finding the best <a href=""http://en.wikipedia.org/wiki/Weather_forecasting#Analog_technique"" rel=""nofollow"">analog</a> from past data.</p>

<p>The data is represented as a regular X by Y grid (i.e. a matrix) of points, where X is the horizontal location and Y is the vertical location, and the (X,Y)th value in the matrix represents the response variable Z at that location. In addition, the grid points are evenly spaced out. As an example, Z can be a measure of surface temperature, which is measured at each of the grid points.</p>

<p>We take care of seasonal effects by restricting the search in the past data to a window of +/- 15 days of the test date. For example, if we want to find the best analog for a map from 2013-06-19, we would only consider maps from 2012-06-19 +/- 15 days, 2011-06-19 +/- 15 days, etc. We also restrict the search to observations taken at the same time as the test date. For example, if the test data is an observation taken at noon, then we will only look at the past data taken from the same time.</p>

<p>I have two questions.</p>

<p>(1) Given two grids (or ""maps"" or matrices) of data, how can I best calculate the similarity between them? Are there methods that take into account the spatial nature of the data? For example, point (1,1) will be highly correlated with the nearby point (1,2), etc.</p>

<p>I am currently using a very simple distance metric, where I just take the difference of the two maps and find the Frobenius norm. The map from the past that yields the smallest value is the 'closest' map to the test conditions.</p>

<p>(2) I am new to spatial statistics and I am looking for literature that relates to what I am trying to do. What should I read to become familiar with working with grid data? What resources are there to learn about pattern recognition in spatial or spatio-temporal data?</p>

<p>(I want to mention that I am working in R, so I would welcome package recommendations as well!)</p>
"
"0.109985336266015","0.124675745238507"," 62237","<p>I am working on a data set. After using some model identification techniques, I came out with an ARIMA(0,2,1) model. </p>

<p>I used the <code>detectIO</code> function in the package <code>TSA</code> in R to detect an <em>innovative</em> outlier (IO) at the 48th observation of my original data set. </p>

<p>How do I incorporate this outlier into my model so I can use it for forecasting purposes? I don't want to use the ARIMAX model since I might not be able to make any predictions from that in R. Are there any other ways I could do this?  </p>

<p>Here are my values in order:</p>

<pre><code>VALUE &lt;- scan()
  4.6  4.5  4.4  4.5  4.4  4.6  4.7  4.6  4.7  4.7  4.7  5.0  5.0  4.9  5.1  5.0  5.4
  5.6  5.8  6.1  6.1  6.5  6.8  7.3  7.8  8.3  8.7  9.0  9.4  9.5  9.5  9.6  9.8 10.0
  9.9  9.9  9.8  9.8  9.9  9.9  9.6  9.4  9.5  9.5  9.5  9.5  9.8  9.3  9.1  9.0  8.9
  9.0  9.0  9.1  9.0  9.0  9.0  8.9  8.6  8.5  8.3  8.3  8.2  8.1  8.2  8.2  8.2  8.1
  7.8  7.9  7.8  7.8
</code></pre>

<p>That is actually my data. They are unemployment rates over a period of 6 years. There are 72 observations then . Each value is to at most one decimal place</p>
"
"0.0898026510133875","0.101797319711858"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.0635000635000953","0.0719815750748694"," 64166","<p><img src=""http://i.stack.imgur.com/aWnvS.jpg"" alt=""enter image description here"">I'm using the library <code>vars</code> in <code>R</code> to plot fanchart and predictions through the vector of error correction model. I have used this code:</p>

<pre><code>b&lt;-ts(KtF,start=1921,end=2009,frequency=1)
vecmfemales&lt;-ca.jo(b,type=""trace"",spec=""transitory"")
vecm.level &lt;- vec2var(vecmfemales, r = 3)
vecm.females&lt;-predict(vecm.level,n.head=50,start=1921,frequency=1)
plot(vecm.females)
fanchart(vecm.females)
</code></pre>

<p>My graph comes out on the abcissa with and index of number which start by 0 and ends up with 100. However on my graphs I need years calendars from 1921 to 2009. Furthermore, I want to get also forecasting years up to 2059. I try to read the package from Pfaff(2013) but he presents only the abscissa as an index not as a calendar years.
 Please somebody can help me to improve my codes in order to get only calendar years? These codes above are my codes.</p>

<p>Thank you!</p>
"
"0.127000127000191","0.107972362612304"," 64383","<p>I have a time series data (1 minute and sometimes 5 minute data) data I would like use <code>forecasting</code> package to forecast couple hours ahead.</p>

<p>Here is my data:</p>

<pre><code>dput(head(p,20))
structure(list(time = structure(c(1373889420, 1373889480, 1373889540, 
1373889600, 1373889660, 1373889720, 1373889780, 1373889840, 1373889900, 
1373889960, 1373890020, 1373890080, 1373890140, 1373890200, 1373890260, 
1373890320, 1373890380, 1373890440, 1373890500, 1373890560), class = c(""POSIXct"", 
""POSIXt""), tzone = ""America/New_York""), cpu = c(2.25892, 2.04144, 
5.04823333333333, 4.9947, 1.72982857142857, 4.82655, 3.6168625, 
4.7357, 2.42683333333333, 3.62635, 5.02315714285714, 2.57147142857143, 
7.16005, 2.34253333333333, 2.82315714285714, 5.17668, 2.2899375, 
6.92, 5.172375, 4.63735), name = c(""servers"", ""servers"", ""servers"", 
""servers"", ""servers"", ""servers"", ""servers"", ""servers"", ""servers"", 
""servers"", ""servers"", ""servers"", ""servers"", ""servers"", ""servers"", 
""servers"", ""servers"", ""servers"", ""servers"", ""servers"")), .Names = c(""time"", 
""cpu"", ""name""), row.names = c(1116L, 1411L, 123L, 226L, 1014L, 
435L, 538L, 569L, 1081L, 342L, 74L, 865L, 178L, 890L, 281L, 166L, 
1035L, 143L, 112L, 91L), class = ""data.frame"")

x.xts &lt;- xts(p$cpu, p$time)
x.ts &lt;- as.ts(x.xts)
x.ets &lt;- ets(x.ts)
x.fore &lt;- forecast(x.ets, h=120)
f&lt;-data.frame(x.fore$mean)
    DateTime&lt;-tail(z,1)$time
f$DATE &lt;- DateTime + 60 * (seq_len(nrow(f))-1)
    colnames(f)&lt;-c(""cpu"", ""time"")
    f$name&lt;-c(""forecast"")
</code></pre>

<p>I see that <code>cpu</code> is the same for all future data times:</p>

<pre><code> cpu                time     name
1 6.020207 2013-07-15 11:57:00 forecast
2 6.020207 2013-07-15 11:58:00 forecast
3 6.020207 2013-07-15 11:59:00 forecast
4 6.020207 2013-07-15 12:00:00 forecast
5 6.020207 2013-07-15 12:01:00 forecast
6 6.020207 2013-07-15 12:02:00 forecast
</code></pre>

<p>Is there a better forecasting model besides <code>ets</code> for time series data?</p>
"
"0.127000127000191","0.143963150149739"," 66927","<p>I need to take the output parameters from an ARIMA model fitted in R from the following set (1,0,1), (0,1,0), (1,1,0), (0,1,1), (1,1,1) of models and implement the prediction function in C. I DO NOT HAVE THE OPTION of calling predict or any other R package for that step. </p>

<p>Obviously, I can eventually track down all the source code in predict and figure it out. But I was hoping there is somewhere that will walk me through a simple example of how to map the various output parameters of Arima() with X, Y, a, b, E, t (no upper and lower case thetas and B^t's) since every paper loves to include those already. </p>

<p>I think this request is slightly duplicative except in previous versions the question was retired without an answer or a link.</p>

<p>UPDATE: So, first, I HIGHLY second all recommendations for <a href=""http://otexts.com/fpp/"" rel=""nofollow"">Forecasting: Principles and Practice by Hyndman&amp;Athanasopoulos</a>. </p>

<p>I think what I've been missing is that ""d"" isn't a model parameter -- it changes what is being modeled. So while I'm not all the way to where I want to be, I'm starting to be able to write predictive equations based on R output. I will update with my eventual findings if nobody else posts something better. </p>
"
"0.0898026510133875","0.0508986598559288"," 67707","<p>Is it possible to do stochastic population forecasting with the R package Demography when the population size is between 10k and 20k? I have data from year 1985 to 2012 for population, deaths and births. What do you do when almost half of the death rates are 0? From ages 1 to 30 nearly no one dies in a so small population. Is there a method for imputing some plausible values for the zero's in the death and birth rates?</p>

<p>Is it possible to, for example, multiply all population numbers with 1000 and impute some plausible values for rates (maybe borrow these from a similar population), and then divide all the simulated numbers in output with 1000?</p>

<p>I have tried to do some forecasting with the R package <a href=""http://cran.r-project.org/web/packages/demography/index.html"" rel=""nofollow"">demography</a>, but I get so many warnings and weird results, I can't get it right. Is there a population size limit for using the <code>demography</code> package?</p>

<p>Do anyone have any suggestions for how to solve the problem of stochastic forecasting with small populations?</p>
"
"0.109985336266015","0.124675745238507"," 68261","<p>I started evaluating and comparing some methods in forecasting. I used Price of dozen eggs in US, 1900â€“1993, in constant dollars in the R software FMA package. I held out the last 10 years for assessment of forecast. Below are the results:</p>

<p>I used auto arima method in the R software. Obviously the results are way off. Am I doing something incorrect ? Below is the forecast. It does not recognize the declining trend. </p>

<p><img src=""http://i.stack.imgur.com/KIM9O.jpg"" alt=""auto arima""></p>

<p>I also used an unobserved components model (UCM) and obtained a good forecast,  as below.</p>

<ol>
<li>Without outliers/level shifts there are very large standard errors and therefore wide confidence bands. <img src=""http://i.stack.imgur.com/dlIXM.png"" alt=""UCM without outliers level shifts""></li>
<li>After some iterative work, below is the output with outliers/level shifts (I know I'm overfitting here) but it did a pretty good job in forecasting; there are also narrow confidence bands. <img src=""http://i.stack.imgur.com/5SrYJ.png"" alt=""UCM with outliers level shifts""></li>
</ol>

<p>In looking at just this example the UCM seems to predict the hold-out sample more accurately than auto.arima.</p>

<p><strong>Why is auto.arima not providing a reasonable forecast?</strong></p>

<p><strong>Are state space models/UCMs better for forecasting long range?</strong></p>

<p><strong>Are there any benefits of using one method over other?</strong></p>
"
"0.141990458561766","0.064382277997965"," 70275","<p>When so many warnings, what does it in fact means? Is there a problem with the validity of the stochastic forecasting models?</p>

<p>I am doing a stochastic forecasting of a small population with approx. 50.000 people. I am using one year age intervals 0-90+. Because so small dataset, I am borrowing the mortality rates from a population which is similar in life expectancy. Fertility rates with 0 are replaced with 1/10000. Net migrations are calculated with the function â€netmigrationâ€ in the â€demographyâ€ package.</p>

<p>Then I use the function â€pop.simâ€ for simulating say 1.000 sample paths of population 40 years ahead.</p>

<p>When doing so with set.seed(505) and N=1000, I don't get any warning. But nearly all other values in set.seed and or N=1000 give me the warning messages: NAs produced.</p>

<p>For example when my code is</p>

<pre><code>set.seed(300)
sim300 &lt;- pop.sim(mort=mort.fcast, fert=fert.fcast, mig=mig.fcast, 
                  firstyearpop=mort.fo,N=300, mfratio=mfratio,
                  bootstrap=FALSE)
</code></pre>

<p>The first ten messages looks like this:</p>

<pre><code>Warning messages:

1: In rpois(rep(1, length(fert$age)), lambda) : NAs produced  
    2: In rbinom(1, B, mfratio/(1 + mfratio)) : NAs produced  
    3: In rpois(1, Ef0 * mort.sim$female[1, j, i]) : NAs produced  
4: In rpois(1, Em0 * mort.sim$male[1, j, i]) : NAs produced  
    5: In rpois(rep(1, p), Ef * mort.sim$female[, j, i]) : NAs produced  
6: In rpois(rep(1, p), Em * mort.sim$male[, j, i]) : NAs produced  
    7: In rpois(1, Ef0 * mort.sim$female[1, j, i]) : NAs produced  
8: In rpois(1, Em0 * mort.sim$male[1, j, i]) : NAs produced
    9: In rpois(rep(1, p), Ef * mort.sim$female[, j, i]) : NAs produced
10: In rpois(rep(1, p), Em * mort.sim$male[, j, i]) : NAs produced  
</code></pre>

<p>How shall I interpret these messages?
Does this indicate that something is wrong in the data, models, forecasting or simulation procedures?</p>

<p>My first guess is that, in message nr. 1, a negative value is assigned to â€lambdaâ€?, if so, is there a way to prevent that?</p>

<p>The most important question is whether these NAs produced is an indicator of the validity of the models and or the forecast of the population?</p>

<p>Is anyone out there who can say something about what is going on here?</p>
"
"0.155542754209564","0.146931774846058"," 91675","<p>I have been looking for a function that can make recursive window out-of-sample forecasts, but seems there is none. So I'm thinking about about making a function that can be used for recursive window forecasting in an ARIMA model. However I know little about programming, so I'm seeking for help.</p>

<p>What I want to do is use the function <code>forecast.Arima</code> (<strong>forecast</strong> package) to predict future values in a expanding window. Suppose 20 years is the initial window, and I expand the window by 1 year on each iteration until it is of size  30 years. More specifically, use 20 years data to predict one value, use 21 years data to predict the next value, etc.</p>
"
"0.141990458561766","0.12876455599593"," 92177","<p>I'm doing a project related to identifying sales dynamics. My database contains 26 weeks after launching the product (so 26 time-series observations equally spaced in time). </p>

<p><img src=""http://i.stack.imgur.com/Dquwy.jpg"" alt=""http://imageshack.com/a/img18/5628/l5qg.jpg""></p>

<p><img src=""http://i.stack.imgur.com/8Dh2C.jpg"" alt=""http://imageshack.com/a/img34/8953/yh6i.jpg""></p>

<p>I used two methods of time-series clustering to see which patterns dominate in different groups (clustering by <code>units_sold_that_week</code>). The first method is based on k-medoids and the second one connected with clustering by parameters of growth models.</p>

<p>My next step is to make forecasts based on these clusters. Is there any special method for forecasting based on time-series clusters? In my project, I have to combine the topic of clustering and forecasting on clusters.</p>

<p>I am running my analyses in R, so I would be grateful for any suggestions regarding R procedures.</p>

<p>Please note that I am relatively new to time series analysis so any clarity you could provide, on R or any package you could recommend that would help accomplish this task efficiently, would be appreciated.</p>
"
"0.141990458561766","0.160955694994913","100363","<p>I have a question regarding the use of the dlm CRAN package for forecasting values of a seasonal time series.</p>

<p>I've built a dlm model combining a stochastic local level model with a stochastic trigonometric (Fourier representation) seasonal component of period 96 (measurements every 15 mins with a daily cycle).</p>

<p>I used dlmMLE to estimate the parameters for my data and filtered and smoothed the series which all seems to be working fine.</p>

<p>However, when I try to use the dlmForecast function to predict out-of-sample observations, the predictions stay constant. The value of all ""predictions"" are equal to the sum of the filtered level and filtered seasonal components for the final observation in the series.</p>

<p>I have used dlmForecast with several other models including a model with a seasonal factor component but never before with a trigonometric seasonal component.</p>

<p>I notice in the documentation for dlmForecast it says ""Currently, only constant models are allowed"" so I wonder if this applies to trigonometric seasonal models.</p>
"
"0.127000127000191","0.143963150149739","104304","<p>I'm trying to forecast hourly data for 30 days for a process.</p>

<p>I have used the following code:</p>

<pre><code>#The packages required for projection are loaded
library(""forecast"")
library(""zoo"")
</code></pre>

<h3>Data Preparation steps</h3>

<p>There is an assumption that we have all the data for all the 24 hours of the month of May</p>

<pre><code>time_index &lt;- seq(from = as.POSIXct(""2014-05-01 07:00""),
                  to = as.POSIXct(""2014-05-31 18:00""), by = ""hour"")

value &lt;- round(runif(n = length(time_index),100,500))
</code></pre>

<p>Using <code>zoo</code> function , we merge data with the date and hour to create an extensible time series object</p>

<pre><code>eventdata &lt;- zoo(value, order.by = time_index)
</code></pre>

<p>As forecast package requires all the objects to be time series objects, the below command is used </p>

<pre><code>eventdata &lt;- ts(value, order.by = time_index)
</code></pre>

<p>For forecasting the values for the next 30 days, the below command is used</p>

<pre><code>z&lt;-hw(t,h=30)
plot(z)
</code></pre>

<p>I feel the output of this code, is not working fine.
<img src=""http://i.stack.imgur.com/NdZRM.jpg"" alt=""enter image description here"">
The forecasted line looks wrong and the dates are not getting correcting projected on the chart.</p>

<p>I'm not sure the fault lies in the data preparation or the output is as expected.</p>
"
"0.0898026510133875","0.101797319711858","107730","<p>When using the combinef function from Rob Hyndman's very useful <a href=""http://cran.r-project.org/web/packages/hts/index.html"" rel=""nofollow"">hts package</a> for forecasting hierarchical and grouped time series, there does not seem to be a way to constrain the optimally combined forecasts to be positive- the starting forecasts can be positive, but can go negative through the reconciliation process.</p>

<p>The forecast.gts and forecast.hts functions have an argument to keep forecasts positive, but this does not seem to be an option when using combinef by itself with forecasts obtained by other methods.</p>

<p>Am I correct in this understanding, and if so is there a decent workaround? </p>
"
"0.276790359705331","0.297246705677282","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"NaN","NaN","109999","<p>I'm looking for a Python alternative to R's ETS() from forecast(). </p>

<p>It's my understanding that ETS() is one of the best performing forecasting program and I would like to use it. However I am extremely uncomfortable using R and I already have all my data cleaned and set up in Python.</p>

<p>Does anyone know any other packages similar to ETS?</p>
"
"0.0635000635000953","0.0719815750748694","114006","<p>I am trying to use the ""car"" command in ""cts package"" in R program and I see the ""scale"" parameter there. I wonder whether this can be assumed to be equivalent to time intervals for time series forecasting. 
For example, the code is like this:</p>

<pre><code>car(x, y=NULL, scale = 1.5, order = 3, ctrl=car_control())
</code></pre>

<p>and the official cts package explanation is the following:</p>

<blockquote>
  <p>scale: The kappa value referred to in the paper by Belcher et a.
  (1994).We now recommend selection of kappa along with the model order
  by using AIC. Also, it is suggested to choose kappa close to 2pi times
  1/mean.delta (reciprocal of the mean time between observations),
  though it is a good idea to explore somewhat lower and higher values
  to see whether the spectrum estimates were sensitive to this choice.
  Choosing kappa lower increases the risk of trying to estimate the
  spectrum beyond the effective Nyquist frequency of the data - though
  this does depend on the distribution of intersample times.</p>
</blockquote>

<p>Can anyone have some ideas, please..?</p>
"
"0.34214844250962","0.336135085823807","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.311334675866699","0.310568462706534","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.290993847623689","0.314153348974034","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"NaN","NaN","127337","<p>I am using <code>crost()</code> function of R for analyzing and forecasting intermittent demand/slow
moving items time series. I am having difficulty in understanding the output. Could anypne help in understanding the model in layman's terms.</p>

<p>Below is the code and output of the model:</p>

<pre><code>v &lt;- c(1910,874,1920,350,160,685,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,176,0,16,826,0,66,3798,800,1274,638,192,160,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,0,0,276,0,0,1072,80,1776,240,80,528,3081,566,1483,112,272,120,0,0,0,
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,160,0,808,0,0,608,0,1480,184)
t &lt;- ts(v, f=52)
x&lt;-crost(t,h=52)
x
</code></pre>

<p>I have read the document of the  Package â€˜tsintermittentâ€™. But still did not get what are weights, frc.in, frc.out. I would like to discuss this on thread as I am totally new to this method. Thanks in advance.</p>
"
"0.200804832225625","0.204863154122693","130256","<p>I have a number of time series with strong seasonality and I am using auto.arima() from R's Forecast package along with Fourier and dummy/explanatory variables to address the seasonality to make forecasts for each time series.  In one part of the time series there are two peaks of activity.  I am looking at previous data to see how well my model predicts out-of-sample-error.  For most of my time series, my ARIMA models do a really good job at forecasting the peaks and troughs of activity.   The models will do a good job when estimating the peaks before they happen and also if I were to update the model with recent data during the middle of the first peak.  </p>

<p>My model, however, gets wonky if the first peak was higher than expected.  In this situation, if I estimate the future using only data before the first peak, my model underestimates the first peak but it accurately forecasts the following trough and does a reasonable job at forecasting the second peak.  (See below - Red is estimated activity; Black is observed activity) </p>

<p><img src=""http://i.stack.imgur.com/SXeSp.png"" alt=""Forecast before first peak""></p>

<p>If I try updating the model with recent data during the middle of the first peak, the forecast then substantially overestimates activity during the remainder of the time series. (See below - Red is estimated activity; Black is observed activity)</p>

<p><img src=""http://i.stack.imgur.com/YvP4N.png"" alt=""Forecast made during first peak""></p>

<p>Why does an updated model do this?  And is there a way to address this issue? I know from domain knowledge that even if the first peak is higher than expected the following trough should return back down, more or less, to the originally expected level.  I have tried playing with the Fourier parameter and manually testing out different ARIMA models.  </p>
"
"0.109985336266015","0.124675745238507","135011","<p>I'm having trouble finding a time series technique to deal with a data set I am working on. It contains multiple subjects and multiple variables, not all of which will likely be part of the time series. It looks something like this:</p>

<pre><code>Subject  Date      T1  T2  V1  V2  V3
A        1/1/2012  1   5   9   13  17
A        2/1/2012  2   6   10  14  18
...
B        1/1/2012  3   7   11  15  19
B        2/1/2012  4   8   12  16  20
...
</code></pre>

<p>Where T1, T2 are likely time series, and V1, V2, and V3 are likely not. I'm sure that this distinction is probably unnecessary, since techniques like Box-Jenkins should detect autoregression in any variable.</p>

<p>Ultimately, I want to be able to do forecasting on other subjects that were probably not used to build this model.</p>

<p>If you know of any R package(s) that can take this on, please let me know. Some example code would also be greatly appreciated. Thank you for any insight you can provide.</p>

<p>Edit: I am looking into dynamic linear regression using the <code>dynlm</code> package, but am having trouble coding it to include the dates and subjects.</p>
"
"0.0898026510133875","0.101797319711858","139448","<p>I have a client who has sparse hourly data (by sparse I mean there are too many hours with 0 calls). I used TBATS in R to forecast hourly data for them. Regardless of the point forecast, the actual values are always in the 80% prediction interval. I wonder if there is any specific method/package in R that is specifically used for uni variate forecasting of sparse data.</p>

<p>Thanks</p>
"
"0.141990458561766","0.160955694994913","143358","<p>I'm working on a forecast for the following data:</p>

<pre><code>data &lt;-
c(1932, 4807, 6907, 8650, 10259, 11374, 8809, 6745, 7429, 
8041, 9740, 10971, 11953, 9227, 7401, 8355, 9681, 10438, 
11092, 11543, 9181, 7428, 8358, 10049, 10938, 12280, 
13063, 10022, 8125, 8763, 9330, 9919, 11309, 12169, 11063, 
10112, 10621, 11506, 12425, 12929, 13025, 10938, 9437, 
9910, 11104, 11985, 13024, 13962, 11900, 9576, 9590, 
10740, 11689, 13084, 13829, 11975, 10224, 10493, 11899, 
12697, 13959, 14415, 11650, 9477, 11166, 12327, 13238, 
13801, 13493, 11118, 9073, 9954, 11077, 12509, 12985, 
13380, 11454, 9265, 10053, 11443, 12132, 13733, 13850, 
11560, 9401, 9921, 11401, 12622, 14224, 14289, 12097, 
9623, 10630, 11572, 12816, 14180, 14125, 11667, 9328, 
9936, 11159, 12536, 13953, 13840, 11430, 9313, 9926, 
11557, 12428, 13802, 13041, 9927, 7448, 9143, 10872, 
12331, 14370, 14496, 13237, 11176, 11936, 12661, 14442, 
15005, 15359, 12871, 10505, 11231, 12078, 13307, 14027, 
14368, 12057, 9965, 10121, 11414, 13375, 14525, 14686, 
12243, 9833, 10722, 11778, 13143, 14844, 14856, 12745, 
9134, 7856, 9429, 11539, 13241, 14324, 12102, 10136, 
11107, 12028, 13999, 15130, 15488, 13379, 11028, 11708, 
13280, 14665, 15362, 15600, 12950, 10716, 10988, 12350, 
14163, 15264, 15724, 13374, 11764, 12711, 13239, 14849, 
15455, 15914, 13541, 10570, 9376, 10132, 11725, 12328, 
13105, 11022, 9710, 10659, 12068, 12890, 14242, 14294, 
11847, 9776, 10681, 12413, 13571, 14344, 14500, 12234, 
9961, 10699, 11626, 13135, 14387, 15282, 13028, 11211, 
11992, 13524, 15131, 15741, 15357, 12489, 9985, 10786, 
11492, 13851, 14509, 14751, 12327, 10023, 11315, 12363, 
13487, 14944, 15006, 12290, 9867, 11540, 12179, 14094, 
14941, 15006, 13585, 10769, 11408, 12634, 14073, 15361, 
15236, 13151, 9580, 8934, 10128, 12475, 13890, 14740, 
12617, 10358, 11648, 12418, 14094, 15127, 15775, 13647, 
11281, 11773, 13407, 15441, 15601, 15951, 13865, 11447, 
12422, 13725, 15766, 16389, 16868, 15221, 12503, 12780, 
14525, 16479, 17032, 17403, 14553, 12484, 13204, 13792, 
14896, 15673, 16332, 14196, 11749, 12977, 13886, 14931, 
15955, 16037, 14082, 11271, 12512, 13942, 16362, 17456, 
17446, 15509, 13069, 13524, 14918, 16161, 17524, 18138, 
14604, 12993, 13763, 14945, 16686, 17717, 17947, 15744, 
13388, 13177, 14588, 16075, 16705, 17074, 14415, 12766, 
13372, 14033, 14300, 12508, 11502, 9391, 7689, 9613, 
12291, 14448, 15075, 15670, 13929, 10989, 11875, 13409, 
15203, 15654, 16150, 13387, 10931, 11492, 12479, 13674, 
14519, 14241, 11685, 9486, 9990, 11440, 12415, 13505, 
12103, 10311, 8267, 7510, 8595, 10620, 11664, 3182, 6241, 
9365, 10965, 12372, 9958, 8088, 9290, 10665, 12132, 12827, 
13040, 10692, 8882, 9538, 10027, 12086, 13276, 13107, 
10680, 9136, 10744, 11733, 13334, 14654, 14830, 12189, 
9613, 11399, 12837, 13661, 15007, 15579, 12268, 9703, 
10627, 12077, 13287, 14459, 14825, 11958, 10049, 11512, 
12770, 13869, 14873, 15233, 12056, 9654, 10386, 11465, 
13354, 14601, 15161, 12324, 9782, 10791, 12502, 14111, 
14914, 15250, 12366, 10333, 11638, 12449, 13518, 14637, 
14756, 12011, 9878, 10976, 12464, 13674, 14979, 15312, 
12106, 10127, 11666, 12843, 13910, 15024, 15333, 12308, 
9992, 11278, 13364, 14966, 15231, 15507, 13744, 11417, 
12232, 14414, 15245, 15988, 15168, 11905, 9165, 10536, 
12570, 14106, 15204, 15509, 12821, 10321, 11282, 13133, 
14174, 15099, 14750, 12817, 10384, 11368, 12994, 14591, 
16154, 15904, 12784, 10737, 11865, 13809, 14721, 15202, 
15322, 12722, 10741, 11991, 13546, 14716, 15817, 15879, 
12679, 10390, 11524, 13140, 14426, 15613, 16212, 13088, 
10720, 11730, 13776, 14477, 15758, 15922, 13119, 9220, 
8372, 10239, 12397, 14740, 15550, 13306, 10833, 11892, 
13630, 15186, 16154, 16678, 12898, 10485, 11313, 13705, 
15572, 16086, 16305, 14129, 11066, 12251, 13830, 15345, 
16550, 16518, 13700, 10890, 12301, 14163, 15890, 16985, 
17544, 15337, 12633, 13383, 12813, 12051, 13149, 13636, 
10914, 9617, 10619, 12224, 13954, 15325, 15473, 12418, 
9730, 11214, 12572, 14565, 15287, 15721, 12519, 10689, 
11662, 13139, 14902, 16374, 16392, 13895, 11777, 12948, 
14326, 15625, 16745, 16980, 13946, 11181, 12665, 13678, 
15269, 16279, 16634, 14399, 11142, 11900, 13800, 14783, 
16626, 16861, 13917, 11228, 12531, 14206, 15773, 16344, 
16930, 13945, 11110, 12427, 14085, 15627, 16854, 17106, 
14677, 10410, 8550, 10626, 13366, 15337, 16460, 13619, 
11630, 12582, 13926, 15297, 16715, 17036, 14063, 11368, 
12246, 14111, 15525, 16900, 17272, 14254, 11961, 13155, 
14579, 16260, 17187, 17919, 15493, 13162, 13771, 15231, 
15836, 16880, 16976, 14728, 12106, 13030, 13848, 15344, 
16475, 17122, 13601, 10921, 12043, 14114, 15846, 16190, 
17125, 13769, 10768, 12336, 13849, 16138, 17507, 18050, 
15492, 12905, 12847, 14181, 15967, 16704, 17762, 14882, 
12591, 13807, 14959, 16933, 17369, 17453, 14351, 11582, 
13102, 14328, 16185, 16321, 16843, 13773, 11053, 12199, 
14147, 14470, 12598, 11916, 9185, 7903, 9742, 12691, 
15153, 15945, 16254, 13630, 11437, 12235, 14040, 15161, 
15995, 16291, 12944, 10947, 12055, 13444, 14852, 16029, 
16361, 13658, 10885, 11604, 13030, 13959, 14291, 14786, 
12002, 9014, 7610, 7426, 9602, 11077, 12544, 11334, 5710, 
9874, 11949, 10321, 8945, 10152, 11821, 13434, 15187, 
15269, 12661, 10699, 12040, 13154, 14149, 15472, 16569, 
13008, 10521, 11674, 13272, 14025, 15803, 16791, 13615, 
11043, 12448, 13929, 15158, 16610, 17520, 13900, 11095, 
11735, 13652, 14939, 16001, 16265, 13371, 11198, 11583, 
13377, 15361, 16420, 16765, 13800, 10866, 12026, 13908, 
14902, 16044, 16807, 13694, 11475, 13009, 14453, 16231, 
17093, 17411, 14433, 12242, 13035, 14304, 16309, 17026, 
16811, 13986, 11812, 13216, 14397, 16026, 17780, 17463, 
14717, 12029, 13046, 14820, 16626, 17564, 17802, 14134, 
13158, 15356, 16573, 16887, 17494, 17326, 13525, 11517, 
12410, 13817, 14933, 16399, 17019, 14008, 11808, 12599, 
14639, 16339, 17521, 17820, 14444, 11530, 13352, 14997, 
16038, 17631, 17614, 15601, 15176, 16930, 17979, 18772, 
19728, 19452, 16272, 14006, 15510, 17299, 17774, 18345, 
19080, 16486, 14242, 15465, 16973, 17971, 19068, 19075, 
15606, 13315, 14784, 16505, 17910, 18586, 18315, 15659, 
13621, 14673, 16037, 17467, 17972, 17676, 15452, 11850, 
10959, 13641, 15217, 16813, 17641, 15404, 13102, 14391, 
15764, 17326, 17715, 17947, 15272, 13078, 13962, 15372, 
18292, 18569, 16427, 13374, 14725, 15957, 17425, 18530, 
19251, 17094, 13711, 15275, 16663, 18254, 19023, 19787, 
16636, 14398, 15392, 16302, 15844, 14301, 14559, 11739, 
10080, 11690, 14352, 16702, 17810, 17898, 15159, 12527, 
14250, 15788, 17012, 18219, 17743, 15183, 12633, 14033, 
15528, 16984, 18041, 18388, 15248, 12831, 14289, 16143, 
17340, 18863, 18597, 15984, 13697, 14653, 16143, 17262, 
17805, 18565, 16147, 14734, 16548, 17410, 18044, 18705, 
18462, 15706, 13242, 14977, 16168, 17683, 18224, 18454, 
15784, 14003, 16605, 18013, 19361, 19204, 18970, 16655, 
12928, 11502, 13233, 15211, 16883, 17454, 15043, 12953, 
14515, 15846, 17501, 18922, 18903, 16175, 13492, 14150, 
15710, 18297, 18872, 19490, 15921, 13935, 14943, 16457, 
18425, 19975, 20440, 17716, 15059, 16086, 17290, 18477, 
19896, 20115, 17580, 15001, 15640, 17915, 18951, 20029, 
20221, 16653, 15063, 15726, 16849, 18121, 18843, 19112, 
16516, 13960, 15255, 16910, 18895, 20091, 20663, 17698, 
15441, 16775, 18158, 19897, 20424, 20111, 17784, 15044, 
16869, 17773, 19783, 21255, 20632, 18081, 15891, 17180, 
18143, 20197, 20926, 20639, 18407, 16313, 16998, 17860, 
19177, 19618, 19919, 17662, 16033, 17439, 18741, 18108, 
16641, 16319, 13221, 11160, 12783, 14876, 16831, 18379, 
18858, 16191, 14632, 16089, 16828, 18169, 19512, 18828, 
17364, 15516, 17065, 18245, 18684, 19472, 19235, 16885, 
14854, 14526, 12921, 12675, 14884, 15284, 13492, 11457, 
5938, 9694, 9429, 9142, 10648, 13235, 15610, 16868, 17364, 
16043, 14497, 15329, 16839, 17548, 18818, 19320, 15884, 
13834, 14748, 15784, 16729, 18274, 19138, 17413, 15394, 
16596, 17853, 18934, 20310, 20165, 18870, 16562, 16823, 
18051, 18816, 20410, 21211, 18551, 16274, 17289, 18317, 
20259, 19993, 19831, 18166, 16517, 17114, 17763, 19011, 
20541, 19974, 18105, 16130, 17422, 18472, 20213, 20721, 
20803, 19250, 16246, 16582, 18410, 19559, 20821, 20412, 
18576, 16272, 16917, 19027, 19917, 20418, 21188, 18382, 
16842, 17911, 19126, 20471, 21120, 20756, 18190, 15873, 
16924, 18468, 19579, 20877, 20726, 18525, 16110, 17480, 
19313, 20323, 20661, 20541, 18284, 16124, 17312, 18361, 
19170, 19945, 20548, 17605, 15973, 17488, 17444, 19086, 
19775, 19827, 17269, 14616, 15690, 16469, 18626, 19288, 
20111, 17769, 15738, 17060, 18885, 20010, 21371, 21541, 
18682, 15971, 16714, 18659, 19934, 21499, 22118, 18952, 
16025, 18120, 18897, 20630, 20286, 21077, 17710, 14857, 
16050, 17877, 19928, 21299, 21202, 18858, 14339, 13172, 
15521, 17434, 19823, 20679, 18288, 16798, 18673, 20628, 
21462, 22720, 22241, 20064, 17327, 18720, 19896, 19710, 
21185, 21916, 19661, 17134, 18027, 19449, 20912, 21234, 
21950, 19495, 17023, 18473, 19080, 20875, 21031, 21492, 
20091, 17511, 18834, 19126, 19922, 21215, 19017, 15506, 
12854, 14605, 16279, 18129, 20043, 21248, 18518, 15467, 
16586, 18277, 18915, 20597, 21244, 19024, 16294, 17234, 
18786, 20960, 21345, 22068, 19774, 17491, 18279, 19809, 
20757, 21618, 22131, 20214, 17581, 18321, 19590, 21486, 
22492, 23194, 20020, 16819, 17892, 18948, 20921, 21696, 
22549, 19559, 16404, 17301, 18659, 20430, 22300, 22569, 
19630, 16800, 17898, 19584, 21190, 21926, 22359, 20157, 
15823, 14136, 15930, 18341, 21044, 21204, 18994, 16973, 
18171, 19378, 20794, 22442, 22144, 19874, 17859, 18703, 
19082, 20781, 21860, 21536, 20172, 18429, 19221, 19824, 
21326, 22504, 23381, 21733, 19231, 20312, 21994, 22609, 
23317, 23074, 22005, 19209, 20734, 22513, 23017, 23698, 
24385, 22512, 19471, 20061, 21235, 22351, 22532, 22869, 
20409, 17908, 18722, 19894, 20960, 21999, 22125, 20797, 
19091, 19910, 20463, 22106, 22737, 22827, 21695, 19498, 
20180, 21204, 22272, 22803, 22808, 20979, 18952, 20365, 
20875, 22944, 23022, 22786, 21284, 19302, 20394, 21144, 
22633, 23511, 23355, 21979, 19988, 20143, 21966, 22574, 
19974, 19410, 15641, 13265, 14880, 16838, 19262, 19941, 
20479, 18929, 17760, 18078, 19055, 20553, 21732, 21671, 
19218, 18485, 18864, 20278, 21120, 21747, 21087, 17982, 
15115, 16518, 16282, 15032, 15658, 14966, 12172, 10336, 
12669, 14238, 14031, 12441, 13313, 11047, 10158, 12438, 
14255, 16434, 17873, 18481, 16360, 14479, 15595, 17392, 
18878, 19999, 19958, 16748, 13852, 14931, 16410, 18097, 
19654, 19480, 16387, 14515, 15205, 16854, 18544, 19510, 
20382, 17838, 14878, 15041, 16661, 19008, 20265, 20947, 
18048, 16472, 16434, 18250, 19571, 21148, 20117, 17788, 
14321, 14996, 15779, 17789, 18804, 18934, 17488, 15095, 
15859, 16691, 18369, 20012, 21073, 18029, 15582, 17247, 
18608, 19783, 20322, 20908, 18221, 15919, 17107, 18404, 
19262, 21741, 21514, 19798, 17410, 17973, 18469, 17910, 
14901)
</code></pre>

<p>The <code>ts.plot(data)</code> gives:<img src=""http://i.stack.imgur.com/E6WU0.jpg"" alt=""enter image description here""></p>

<p>With this data, I'm looking to forecast the values for the next year. This data is victim to both weekly and yearly seasonality. Due to this, I first attempted to use <code>tbats</code> from the <code>forecast</code> package but received an improper forecast that mirrors that found at <a href=""http://www.github.com/robjhyndman/forecast/issues/87"" rel=""nofollow"">http://www.github.com/robjhyndman/forecast/issues/87</a></p>

<p>Instead, I used the following code:</p>

<pre><code>n&lt;-length(data)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0
for(i in 1:20)
{
fit &lt;- auto.arima(data, xreg = fourier(1:n,i,m1) + fourier(1:n,i,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
if(fit$aicc &lt; bestfit$aicc)
{
    bestfit &lt;- fit
    bestk &lt;- i
}
}

k &lt;- bestk

bestfit &lt;- auto.arima(data, xreg = fourier(1:n,k,m1) + fourier(1:n,k,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
accuracy(bestfit)
fc &lt;- forecast(bestfit, xreg = fourier((n+1):(n+365),k,m1) + fourier((n+1):(n+365),k,m2), level = c(50,80,90), bootstrap = TRUE)
plot(fc)
</code></pre>

<p>This code is searching for the best ARIMA model through the use of Fourier terms in <code>xreg</code> to capture both seasonality components. This Fourier function is defined (per <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}
</code></pre>

<p>This forecasting gives me the following plot:<img src=""http://i.stack.imgur.com/2IsSD.jpg"" alt=""enter image description here""></p>

<p>In looking at this forecast, it seems by my naked eye to be off. Just by observation it appears that my forecast is not properly catching the small, but visible, increasing trend. Instead of being ""centered"" around the extended trendline, it appears that the forecast is ""centered"" around the mean of the entire dataset.</p>

<p>First off, am I doing something that is just blatantly wrong? (my mind is a little fuzzy this morning)</p>

<p>If my forecast is correct, how is it that it falls so much below the extended trendline?</p>

<p>Lastly, are there any other suggestions which might be beneficial to my forecasting?</p>
"
"0.496015872761897","0.489716605611849","144745","<p>I have 17 years (1995 to 2011) of death certificate data related to suicide deaths for a state in the U.S. There is a lot of mythology out there about suicides and the months/seasons, much of it contradictory, and of the literature I've reviewed, I do not get a clear sense of methods used or confidence in results.</p>

<p>So I've set out to see if I can determine whether suicides are more or less likely to occur in any given month within my data set. All of my analyses are done in R.</p>

<p>The total number of suicides in the data is 13,909.</p>

<p>If you look at the year with the fewest suicides, they occur on 309/365 days (85%). If you look at the year with the most suicides, they occur on 339/365 days (93%).</p>

<p>So there are a fair number of days each year without suicides. However, when aggregated across all 17 years, there are suicides on every day of the year, including February 29 (although only 5 when the average is 38).</p>

<p><img src=""http://i.stack.imgur.com/VMQYa.jpg"" alt=""enter image description here""></p>

<p>Simply adding up the number of suicides on each day of the year doesn't indicate a clear seasonality (to my eye).</p>

<p>Aggregated at the monthly level, average suicides per month range from:</p>

<p>(m=65, sd=7.4, to m=72, sd=11.1)</p>

<p>My first approach was to aggregate the data set by month for all years and do a chi-square test after computing the expected probabilities for the null hypothesis, that there was no systematic variance in suicide counts by month. I computed the probabilities for each month taking into account the number of days (and adjusting February for leap years).</p>

<p>The chi-square results indicated no significant variation by month:</p>

<pre><code># So does the sample match  expected values?
chisq.test(monthDat$suicideCounts, p=monthlyProb)
# Yes, X-squared = 12.7048, df = 11, p-value = 0.3131
</code></pre>

<p>The image below indicates total counts per month. The horizontal red lines are positioned at the expected values for February, 30 day months, and 31 day months respectively. Consistent with the chi-square test, no month is outside the 95% confidence interval for expected counts.
<img src=""http://i.stack.imgur.com/XRCzM.jpg"" alt=""enter image description here""></p>

<p>I thought I was done until I started to investigate time series data. As I imagine many people do, I started with the non-parametric seasonal decomposition method using the <code>stl</code> function in the stats package. </p>

<p>To create the time series data, I started with the aggregated monthly data:</p>

<pre><code>suicideByMonthTs &lt;- ts(suicideByMonth$monthlySuicideCount, start=c(1995, 1), end=c(2011, 12), frequency=12) 

# Plot the monthly suicide count, note the trend, but seasonality?
plot(suicideByMonthTs, xlab=""Year"",
  ylab=""Annual  monthly  suicides"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xSWJm.jpg"" alt=""enter image description here""></p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
1995  62  47  55  74  71  70  67  69  61  76  68  68
1996  64  69  68  53  72  73  62  63  64  72  55  61
1997  71  61  64  63  60  64  67  50  48  49  59  72
1998  67  54  72  69  78  45  59  53  48  65  64  44
1999  69  64  65  58  73  83  70  73  58  75  71  58
2000  60  54  67  59  54  69  62  60  58  61  68  56
2001  67  60  54  57  51  61  67  63  55  70  54  55
2002  65  68  65  72  79  72  64  70  59  66  63  66
2003  69  50  59  67  73  77  64  66  71  68  59  69
2004  68  61  66  62  69  84  73  62  71  64  59  70
2005  67  53  76  65  77  68  65  60  68  71  60  79
2006  65  54  65  68  69  68  81  64  69  71  67  67
2007  77  63  61  78  73  69  92  68  72  61  65  77
2008  67  73  81  73  66  63  96  71  75  74  81  63
2009  80  68  76  65  82  69  74  88  80  86  78  76
2010  80  77  82  80  77  70  81  89  91  82  71  73
2011  93  64  87  75 101  89  87  78 106  84  64  71
</code></pre>

<p>And then performed the <code>stl()</code> decomposition</p>

<pre><code># Seasonal decomposition
suicideByMonthFit &lt;- stl(suicideByMonthTs, s.window=""periodic"")
plot(suicideByMonthFit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/cS5pE.jpg"" alt=""enter image description here""></p>

<p>At this point I became concerned because it appears to me that there is both a seasonal component and a trend. After much internet research I decided to follow the instructions of Rob Hyndman and George AthanaÂ­sopouÂ­los as laid out in their on-line text ""Forecasting: principles and practice"", specifically to apply a seasonal ARIMA model.</p>

<p>I used <code>adf.test()</code> and <code>kpss.test()</code> to assess for <em>stationarity</em> and got conflicting results. They both rejected the null hypothesis (noting that they test the opposite hypothesis).</p>

<pre><code>adfResults &lt;- adf.test(suicideByMonthTs, alternative = ""stationary"") # The p &lt; .05 value 
adfResults

    Augmented Dickey-Fuller Test

data:  suicideByMonthTs
Dickey-Fuller = -4.5033, Lag order = 5, p-value = 0.01
alternative hypothesis: stationary

kpssResults &lt;- kpss.test(suicideByMonthTs)
kpssResults

    KPSS Test for Level Stationarity

data:  suicideByMonthTs
KPSS Level = 2.9954, Truncation lag parameter = 3, p-value = 0.01
</code></pre>

<p>I then used the algorithm in the book to see if I could determine the amount of differencing that needed to be done for both the trend and season. I ended  with 
nd = 1, ns = 0.</p>

<p>I then ran <code>auto.arima</code>, which chose a model that had both a trend and a seasonal component along with a ""drift"" type constant.</p>

<pre><code># Extract the best model, it takes time as I've turned off the shortcuts (results differ with it on)
bestFit &lt;- auto.arima(suicideByMonthTs, stepwise=FALSE, approximation=FALSE)
plot(theForecast &lt;- forecast(bestFit, h=12))
theForecast
</code></pre>

<p><img src=""http://i.stack.imgur.com/qTUi9.jpg"" alt=""enter image description here""></p>

<pre><code>&gt; summary(bestFit)
Series: suicideByMonthFromMonthTs 
ARIMA(0,1,1)(1,0,1)[12] with drift         

Coefficients:
          ma1    sar1     sma1   drift
      -0.9299  0.8930  -0.7728  0.0921
s.e.   0.0278  0.1123   0.1621  0.0700

sigma^2 estimated as 64.95:  log likelihood=-709.55
AIC=1429.1   AICc=1429.4   BIC=1445.67

Training set error measures:
                    ME    RMSE     MAE       MPE     MAPE     MASE       ACF1
Training set 0.2753657 8.01942 6.32144 -1.045278 9.512259 0.707026 0.03813434
</code></pre>

<p>Finally, I looked at the residuals from the fit and if I understand this correctly, since all values are within the threshold limits, they are behaving like white noise and thus the model is fairly reasonable. I ran a <em>portmanteau test</em> as described in the text, which had a p value well above 0.05, but I'm not sure that I have the parameters correct.</p>

<pre><code>Acf(residuals(bestFit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/gso3q.jpg"" alt=""enter image description here""></p>

<pre><code>Box.test(residuals(bestFit), lag=12, fitdf=4, type=""Ljung"")

    Box-Ljung test

data:  residuals(bestFit)
X-squared = 7.5201, df = 8, p-value = 0.4817
</code></pre>

<p>Having gone back and read the chapter on arima modeling again, I realize now that <code>auto.arima</code> did choose to model trend and season. And I'm also realizing that forecasting is not specifically the analysis I should probably be doing. I want to know if a specific month (or more generally time of year) should be flagged as a high risk month. It seems that the tools in the forecasting literature are highly pertinent, but perhaps not the best for my question. Any and all input is much appreciated.</p>

<p>I'm posting a link to a csv file that contains the daily counts. The file looks like this:</p>

<pre><code>head(suicideByDay)

        date year month day_of_month t count
1 1995-01-01 1995    01           01 1     2
2 1995-01-03 1995    01           03 2     1
3 1995-01-04 1995    01           04 3     3
4 1995-01-05 1995    01           05 4     2
5 1995-01-06 1995    01           06 5     3
6 1995-01-07 1995    01           07 6     2
</code></pre>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/daily_suicide_counts.csv"" rel=""nofollow"">daily_suicide_data.csv</a></p>

<p>Count is the number of suicides that happened on that day. ""t"" is a numeric sequence from 1 to the total number of days in the table (5533).</p>

<p>I've taken note of comments below and thought about two things related to modeling suicide and seasons. First, with respect to my question, months are simply proxies for marking change of season, I am not interested in wether or not a particular month is different from others (that of course is an interesting question, but it's not what I set out to investigate). Hence, I think it makes sense to <strong>equalize</strong> the months by simply using the first 28 days of all months. When you do this, you get a slightly worse fit, which I am interpreting as more evidence towards a lack of seasonality. In the output below, the first fit is a reproduction from an answer below using months with their true number of days, followed by a data set <strong>suicideByShortMonth</strong> in which suicide counts were computed from the first 28 days of all months. I'm interested in what people think about wether or not this adjustment is a good idea, not necessary, or harmful?</p>

<pre><code>&gt; summary(seasonFit)

Call:
glm(formula = count ~ t + days_in_month + cos(2 * pi * t/12) + 
    sin(2 * pi * t/12), family = ""poisson"", data = suicideByMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4782  -0.7095  -0.0544   0.6471   3.2236  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         2.8662459  0.3382020   8.475  &lt; 2e-16 ***
t                   0.0013711  0.0001444   9.493  &lt; 2e-16 ***
days_in_month       0.0397990  0.0110877   3.589 0.000331 ***
cos(2 * pi * t/12) -0.0299170  0.0120295  -2.487 0.012884 *  
sin(2 * pi * t/12)  0.0026999  0.0123930   0.218 0.827541    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 302.67  on 203  degrees of freedom
Residual deviance: 190.37  on 199  degrees of freedom
AIC: 1434.9

Number of Fisher Scoring iterations: 4

&gt; summary(shortSeasonFit)

Call:
glm(formula = shortMonthCount ~ t + cos(2 * pi * t/12) + sin(2 * 
    pi * t/12), family = ""poisson"", data = suicideByShortMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.2414  -0.7588  -0.0710   0.7170   3.3074  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         4.0022084  0.0182211 219.647   &lt;2e-16 ***
t                   0.0013738  0.0001501   9.153   &lt;2e-16 ***
cos(2 * pi * t/12) -0.0281767  0.0124693  -2.260   0.0238 *  
sin(2 * pi * t/12)  0.0143912  0.0124712   1.154   0.2485    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 295.41  on 203  degrees of freedom
Residual deviance: 205.30  on 200  degrees of freedom
AIC: 1432

Number of Fisher Scoring iterations: 4
</code></pre>

<p>The second thing I've looked into more is the issue of using month as a proxy for season. Perhaps a better indicator of season is the number of daylight hours an area receives. This data comes from a northern state that has substantial variation in daylight. Below is a graph of the daylight from the year 2002. </p>

<p><img src=""http://i.stack.imgur.com/yvVXl.jpg"" alt=""enter image description here""></p>

<p>When I use this data rather than month of the year, the effect is still significant, but the effect is very, very small. The residual deviance is much larger than the models above. If daylight hours is a better model for seasons, and the fit is not as good, is this more evidence of very small seasonal effect? </p>

<pre><code>&gt; summary(daylightFit)

Call:
glm(formula = aggregatedDailyCount ~ t + daylightMinutes, family = ""poisson"", 
    data = aggregatedDailyNoLeap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0003  -0.6684  -0.0407   0.5930   3.8269  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      3.545e+00  4.759e-02  74.493   &lt;2e-16 ***
t               -5.230e-05  8.216e-05  -0.637   0.5244    
daylightMinutes  1.418e-04  5.720e-05   2.479   0.0132 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 380.22  on 364  degrees of freedom
Residual deviance: 373.01  on 362  degrees of freedom
AIC: 2375

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I'm posting the daylight hours in case anyone wants to play around with this. Note, this is not a leap year, so if you want to put in the minutes for the leap years, either extrapolate or retrieve the data.</p>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/state.daylight.2002.csv"" rel=""nofollow"">state.daylight.2002.csv</a></p>

<p>[<strong>Edit</strong> to add plot from deleted answer (hopefully rnso doesn't mind me moving the plot in the deleted answer up here to the question. svannoy, if you don't want this added after all, you can revert it)]</p>

<p><img src=""http://i.stack.imgur.com/WiuvE.png"" alt=""enter image description here""></p>
"
"0.179605302026775","0.178145309495751","145193","<p>I'm trying to model the responses from a direct mail marketing campaign so that I can use it to forecast for future campaigns. In the code below, I started with the average number of responses by day of a historical campaign (contained in the vector: ""responses""). I was then able to fit a 63-day (8-wk) smooth curve to model the data. But I now need a way to use this curve to help me with forecasting. For example, if I think I'll get x number of total responses from a campaign, I need to know when those responses are most likely to happen. In other words, I need the daily ""factors"" (i.e. the percentage of the total responses that is most likely to respond on each day).  Thanks!</p>

<p>p.s. if anyone has a better way of approaching this I'd love to hear!</p>

<pre><code>#vector of direct mail marketing responses over 63 days 
responses &lt;- c(
24.16093706,
41.59607507,
68.20083052,
85.19109064,
100.0704403,
58.6600221,
86.08475816,
88.97439581,
65.58341418,
49.25588053,
53.63602085,
47.03620672,
29.71552264,
32.85862747,
31.29118096,
23.67961069,
19.81261675,
18.69300933,
17.25738435,
12.01161679,
12.36734071,
14.32360673,
11.02390849,
9.108021409,
9.647965622,
8.815576548,
5.67225654,
5.739220185,
6.233999138,
5.527376627,
5.024065761,
5.565266355,
4.626749364,
3.480761716,
4.621902301,
4.518554271,
4.075985188,
3.204946787,
3.174020873,
2.966915873,
2.129178828,
2.673009031,
2.410429043,
2.331287075,
2.509300578,
2.13820695,
2.53433787,
1.603934405,
1.555813592,
1.834605068,
1.842905685,
1.454045577,
2.08684322,
1.318276487,
0.807666643,
1.333167088,
1.004526525,
1.180110123,
1.078079735,
1.151394678,
1.426747942,
0.699119833,
0.583347236)


set.seed(2)
install.packages(""MASS"")
library(""MASS"")


shape_and_scale &lt;- fitdistr(responses,'weibull')

#check the shape and scale
shape_and_scale

#plug in the shape and scale
#essentially taking the total number of respondants and for each, doing a random simulation for what day they'll respond- according to a weibull distribution
#rweibull makes it a random generation
#also need to create a variable for the total number of responses
total_responses &lt;- 1121
day_response &lt;- round(rweibull(total_responses,0.70730466,13.79467490)+.5)

day_response

day_response_frequency_table &lt;- as.data.frame(table(round(rweibull(total_responses,0.70730466,13.79467490)+.5)))

day_response_frequency_table
#notice that it extends beyond our 63 day limit for modeling a campaign

#create a factor with levels so that we can limit our distribution to 63 days
day_response_with_levels &lt;- factor(day_response, levels=0:63)
day_response_with_levels
response_frequency &lt;- as.data.frame(table(day_response_with_levels))
response_frequency

#now use dweibull and the curve() function to create a curve
?dweibull 
curve(x*dweibull(x,0.70730466,13.79467490),from=0, to=63)
</code></pre>
"
"0.127000127000191","0.143963150149739","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.254000254000381","0.287926300299478","153404","<p>I am fairly new to R so my data manipulation experience isn't as strong as it is with other software packages. I have been primarily using the high level functions that others have written. The forecast package written by Hyndman is fantastic and I have also enjoyed reading his book.</p>

<p>What I am struggling with is how to create ARIMA fitted values that are greater than one step forward. The current function will painlessly create fitted values for a one time period projection on historical data. You can use the accuracy function to determine RMSE, MAPE, MASE to test the accuracy of your modelling forecasts. However, I would like to variably test how well the forecast performs for different projections into the future. </p>

<p>For example, I am currently working with weekly time series data and the current goal is to forecast one month into the future. The fitted values and accuracy function show how well my forecast would have performed if I were only projecting 1 week into the future instead of 4. Is there a quick and efficient way to select how many periods into the future we would like to estimate the accuracy for historical data?</p>

<p>Forgive me if I have missed a fundamental aspect of forecasting. I know that I can simply aggregate my weekly time series data into monthly data and then use my ARIMA model to forecast monthly instead of weekly periods. However, since I have multiple years of data, I am largely concerned with seasonality and it would be nice to see how the data ramps up in each week of the month (also would be nice to account for holidays that may affect weekly results). I also realize that I can simply forecast monthly data and then use cubic spline interpolation.</p>

<p>Thank you in advance!</p>
"
"0.210605884793558","0.23873587634214","160716","<p>I have a set of mortality data that I'd like to smooth so that I can run it through a Lee-Carter model for forecasting. The set of data focuses on the cohorts of aged 1-11 people for the years of 2004-2014.</p>

<p>I've tried to run the smooth.demogdata from the demography package in R, but it produces this error: </p>

<pre><code>""Error in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots) : 
A term has fewer unique covariate combinations than specified maximum degrees of freedom""
</code></pre>

<p>I've also tried to run a smooth.spline function on the data and it hasn't worked either.</p>

<p>The data I'm working with are as follows: </p>

<pre><code>US_counts.age
135618, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
150988, 125213, NA, NA, NA, NA, NA, NA, NA, NA, NA
144797, 144686, 117643, NA, NA, NA, NA, NA, NA, NA, NA
145921, 138953, 136791, 110374, NA, NA, NA, NA, NA, NA, NA
146350, 139452, 131145, 128469, 103897, NA, NA, NA, NA, NA, NA
159301, 139080, 130705, 122500, 120655, 97922, NA, NA, NA, NA, NA
169750, 151355, 130195, 121681, 114789, 113711, 92623, NA, NA, NA, NA
166925, 158914, 142749, 122450, 114941, 108932, 108085, 88116, NA, NA, NA
174177, 158635, 150225, 134504, 116111, 109365, 103898, 103520, 84283, NA, NA
174938, 165078, 149825, 141967, 127656, 110712, 104557, 99706, 99568, 80944, NA
169517, 165777, 155530, 141601, 134922, 121745, 105924, 100205, 95836, 95749, 77716



US_rates.age
0.0034287484, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
0.0031989297, 0.0036178352, NA, NA, NA, NA, NA, NA, NA, NA, NA
0.0029213312, 0.0032415023, 0.0036636264, NA, NA, NA, NA, NA, NA, NA, NA
0.0022957628, 0.0028930646, 0.0031873442, 0.0035334408, NA, NA, NA, NA, NA, NA, NA
0.0017902289, 0.0023162092, 0.0027298029, 0.0030513198, 0.0033013465, NA, NA, NA, NA, NA, NA
0.0015254142, 0.0017328156, 0.0022187369, 0.0025224490, 0.0028925449, 0.0029717530, NA, NA, NA, NA, NA
0.0011310751, 0.0014733573, 0.0016052844, 0.0020052432, 0.0023434301, 0.0025855018, 0.0027207065, NA, NA, NA, NA
0.0008926164, 0.0011389808, 0.0013730394, 0.0014373214, 0.0018531246, 0.0021389491, 0.0022297266, 0.0024286168, NA, NA, NA
0.0004994919, 0.0008068837, 0.0009452488, 0.0011300779, 0.0012057428, 0.0015818589, 0.0017805925, 0.0019513138, 0.0020882028, NA, NA
0.0003029645, 0.0004422152, 0.0006407475, 0.0007466524, 0.0007676882, 0.0008761471, 0.0012815976, 0.0014442461, 0.0014362044, 0.0016184028, NA
0.0001415787, 0.0002050948, 0.0002829036, 0.0003954774, 0.0003705845, 0.0004517639, 0.0004342736, 0.0006885884, 0.0008138904, 0.0008772938, 0.0009264502
</code></pre>

<p>Note: it's cohort data, which is why there are missing values in the upper right of both matrices (which are supposed to be shown as matrices, but I don't know how to display matrices on this forum).</p>

<p>I've taken these matrices and input them into a demogdata(...) call as follows:</p>

<pre><code>US.demog.age = demogdata(US_rates.age, US_counts.age, ages = 1:11, years = 2004:2014, type = ""mortality"", name=""Total"", label = ""US"")
</code></pre>

<p>When I try to then smooth this data using the smooth.demogdata(...) function as I normally would with a larger set of mortality data, I get the error I displayed above. I read somewhere about manually changing the knot values to fix the error and to be able to actually smooth the data, but my attempts have been unsuccessful thus far. The information I read pertained to the s(...) function of the mgcv package where you could set k to some value under the default 10 to override these issues for smaller data sets, but all of the examples I saw were related to linear mixed effects models, not to a matrix specifically, so I'm at a loss for how I could use that method with matrix data and get the smoothing function to work.</p>

<p>Am I barking up the wrong tree trying to smooth the demography data with the smooth.demogdata call? Is there a better way to smooth the data? I need to be to use the Lee-Carter model in the ilc package.</p>

<p>Thanks</p>
"
"0.0898026510133875","0.101797319711858","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.180724349003062","0.159338008762095","163520","<p>As I asked in <a href=""http://stackoverflow.com/questions/31210688/what-is-proper-way-of-forecasting-grouped-time-series-specified-via-hts-package"">here</a> I was trying to forecast grouped time series with two grouping variables and I find some limitation of hierarchical forecasting methods. In particular, using <em>hts</em> package from R, <strong>we can't use top-down methods.</strong> </p>

<p>I consider grouped time series which can be viewed as:</p>

<pre><code>     Total
   |       | 
   A       B
 |   |    |   |
AX  AY   BX  BY

     Total
   |       | 
   X       Y
 |   |   |   |
 AX  BX  AY  BY
</code></pre>

<p>(It's described in more details in this <a href=""http://stats.stackexchange.com/questions/31473/forecasting-hierarchical-time-series-r-package"">post</a> and for example in this <a href=""http://robjhyndman.com/papers/hgts6.pdf"" rel=""nofollow"">paper</a>)</p>

<p>According to the notation specified in <a href=""http://robjhyndman.com/papers/Hierarchical6.pdf"" rel=""nofollow"">this paper</a> we can write such grouped time series as $\mathbf{Y_t} = \mathbf{S} \mathbf{Y_{K,t}}$, where $\mathbf{S}$ is a summing matrix and $\mathbf{Y_{K,t}}$ is a vector of bottom level series (which according to assumption in hts package have to be equal). In this case it looks like:</p>

<p>$$   \begin{bmatrix}
      Y_t \\
      Y_{A,t} \\
      Y_{B,t} \\
      Y_{X,t} \\
      Y_{Y,t} \\
      Y_{AX,t} \\
      Y_{AY,t} \\
      Y_{BX,t} \\
      Y_{BY,t} \\
     \end{bmatrix} = \begin{bmatrix}
     1 &amp; 1 &amp; 1 &amp; 1  \\ 
     1 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 1  \\ 
     1 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 1  \\ 
     1 &amp; 0 &amp; 0 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 0 &amp; 0 &amp; 1  \\ 
     \end{bmatrix} \begin{bmatrix}
      Y_{AX,t} \\
      Y_{AY,t} \\
      Y_{BX,t} \\
      Y_{BY,t} \\
     \end{bmatrix}
$$</p>

<p>Revised forecast (what I am looking for) can be written as $\mathbf{\tilde{Y}_n(h) = SP\hat{Y}_n(h)}$ and in case of top-down method matrix $\mathbf{P}$ is defined as 
$\mathbf{P} = \begin{bmatrix}
     \mathbf{p} | \mathbf{0}_{m_K \times (m-1)} 
\end{bmatrix}$, where $ \mathbf{p} = [p_1, p_2, ..., p_{m_K}]^T$  is a vector of proportions. Not going into more details, in this example $m_K = 4$ and $m=9$, so $\mathbf{P} = \begin{bmatrix}
     \mathbf{p_1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 
\mathbf{p_2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\mathbf{p_3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\mathbf{p_4} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}$ </p>

<p>and revised forecasts can be written as:</p>

<p>$$   \begin{bmatrix}
      \tilde{Y_t} \\
      \tilde{Y}_{A,t} \\
      \tilde{Y}_{B,t} \\
      \tilde{Y}_{X,t} \\
      \tilde{Y}_{Y,t} \\
      \tilde{Y}_{AX,t} \\
      \tilde{Y}_{AY,t} \\
      \tilde{Y}_{BX,t} \\
      \tilde{Y}_{BY,t} \\
     \end{bmatrix} = \begin{bmatrix}
     1 &amp; 1 &amp; 1 &amp; 1  \\ 
     1 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 1  \\ 
     1 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 1  \\ 
     1 &amp; 0 &amp; 0 &amp; 0  \\ 
     0 &amp; 1 &amp; 0 &amp; 0  \\ 
     0 &amp; 0 &amp; 1 &amp; 0  \\ 
     0 &amp; 0 &amp; 0 &amp; 1  \\ 
     \end{bmatrix} \begin{bmatrix}
     p_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 
p_2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
p_3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
p_4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix} \begin{bmatrix}
      \hat{Y_t} \\
      \hat{Y}_{A,t} \\
      \hat{Y}_{B,t} \\
      \hat{Y}_{X,t} \\
      \hat{Y}_{Y,t} \\
      \hat{Y}_{AX,t} \\
      \hat{Y}_{AY,t} \\
      \hat{Y}_{BX,t} \\
      \hat{Y}_{BY,t} \\
     \end{bmatrix}
$$</p>

<p>and after calculations:</p>

<p>$$   \begin{bmatrix}
      \tilde{Y_t} \\
      \tilde{Y}_{A,t} \\
      \tilde{Y}_{B,t} \\
      \tilde{Y}_{X,t} \\
      \tilde{Y}_{Y,t} \\
      \tilde{Y}_{AX,t} \\
      \tilde{Y}_{AY,t} \\
      \tilde{Y}_{BX,t} \\
      \tilde{Y}_{BY,t} \\
     \end{bmatrix} = \begin{bmatrix}
      p_1\hat{Y_t} + p_2\hat{Y_t} + p_3\hat{Y_t} + p_4\hat{Y_t} \\
      p_1\hat{Y_t} + p_2\hat{Y_t} \\
      p_3\hat{Y_t} + p_4\hat{Y_t} \\
      p_1\hat{Y_t} + p_3\hat{Y_t} \\
      p_2\hat{Y_t} + p_4\hat{Y_t} \\
      p_1\hat{Y_t} \\
      p_2\hat{Y_t} \\
      p_3\hat{Y_t} \\
      p_4\hat{Y_t} \\
     \end{bmatrix}
$$</p>

<p>Which seems OK for me. I was hoping that somebody could point out <strong>why this method can't be used in forecasting grouped time series</strong> and point out when my calculations are wrong?</p>
"
"0.0898026510133875","0.101797319711858","164667","<p>I have a bunch of sales data. It is from distributors of 2000 different items, who service big companies and large distributors to a number of small independent stores. They sell some items which do good volume, and others where not even 100 units are sold in a year. What's more is that the method to determine true demand is not perfect - this is because if an item is out of stock, a customer who orders weekly will keep reordering until they get the stock and this will lead to an inflation in true demand as their order will be counted more than once. Conversely, going by sold data will not include demand from customers who did not reorder out of stock items.</p>

<p>Because of all this (as well as some other factors), the data, whilst showing some trend over large periods looks to me to be incredibly ""noisy"". Any trend over a short period of forecasting time would be wiped out.</p>

<p>Here's an example of a couple different plots. 
<a href=""http://i.stack.imgur.com/iaPi6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iaPi6.png"" alt=""Visual representation of noisey data""></a></p>

<p>ETS from the forecast package in R could give some insight, or ARIMA (not that I know how it's fully used yet). Or do you think in such cases where there is so little information in the data that a Simple Exponential Smoothing technique will yield results as good as it gets?</p>
"
"0.141990458561766","0.12876455599593","166485","<p>I created for the following data set a multiple regression. Now I would like to forecast the next 20 data points.</p>

<pre><code>&gt; dput(datSel)
structure(list(oenb_dependent = c(1.0227039, -5.0683144, 0.6657713, 
3.3161374, -2.1586704, -0.7833623, -0.2203209, 2.416144, -1.7625406, 
-0.1565037, -7.9803936, 9.4594715, -4.8104584, 8.4827107, -6.1895262, 
1.4288595, 1.4896459, -0.4198522, -5.1583964, 5.2502294, 1.0567102, 
-1.0923342, -1.5852298, 0.6061936, -0.3752335, 2.5008664, -1.3999729, 
2.2802166, -2.1468756, -1.4890328, -0.79254376, 3.21804705, -0.94407886, 
-0.27802316, -0.20753079, -1.12610048, 2.0883735, -0.7424854, 
0.44203729, -1.48905938, 1.39644424, -3.8917377, 11.25665848, 
-9.22884035, 3.26856762, -0.00179541, -2.39664325, 4.00455574, 
-5.60891295, 4.6556348, -4.40536951, 6.64234497, -7.34787319, 
7.56303006, -8.23083674, 4.43247855, 1.31090412), carReg = c(0.73435946, 
0.24001161, 16.90532537, -14.60281976, 6.47603166, -8.35815849, 
3.55576685, 7.10705794, -4.6955223, 10.9623709, 5.5801857, -6.4499936, 
-9.46196502, 9.36289122, -8.52630424, 5.45070994, -4.5346405, 
-2.26716538, 2.56870398, 0.013737, 5.7750101, -27.1060826, 1.08977179, 
4.94934712, 17.55391859, -13.91160577, 10.38981128, -11.81349246, 
-0.0831467, 2.79748237, 1.84865463, -1.98736934, -6.24191695, 
13.33602659, -3.86527871, 0.78720993, 4.73360651, -4.1674034, 
9.37426802, -5.90660464, -0.4915792, -5.84811629, 9.67648643, 
-6.96872719, -7.6535767, 0.24847595, 0.18685263, -2.28766949, 
1.1544631, -3.87636933, -2.4731545, 4.33876671, 1.08836339, 5.64525271, 
1.90743854, -3.94709355, -0.84611324), cpi = c(1.16, -3.26, 0.22, 
-3.51, 0.84, -2.81, -0.34, -4.57, -0.12, -3.95, -1.37, -2.73, 
0.35, -5.38, -4.43, -3.08, 0.74, -3.03, -1.09, -2, 0.35, -1.52, 
1.28, 0.2, -0.25, -4.55, -2.49, -4.24, -0.31, -2.96, -2.24, -0.46, 
-0.06, -2.67, -1.27, -1.4, -0.7, -0.96, -2.18, -2.53, -0.52, 
-1.74, -2.18, -1.4, -0.34, -0.09, -1.65, -1.15, -0.17, -2.01, 
-1.38, -1.24, 0.09, -2.44, -1.92, -2.61, -0.34), primConstTot = c(-0.33334, 
-0.93333, -0.16667, -0.33333, -0.16667, -0.86666, -0.3, -0.4, 
-0.26667, -1.56667, -0.73333, 0.1, -0.23333, -0.26667, -1.5774, 
-0.19284, 0.38568, -2.42423, -0.93663, 0.08265, -0.63361, 0.0551, 
-0.49587, 2.39668, -1.70798, -3.36085, -2.56196, 0.16529, 0, 
-1.84572, -1.3774, -0.49586, -1.70798, -1.90081, -0.55096, -0.77134, 
-0.16529, -0.30303, -0.17066, -0.23853, -0.64401, -1.52657, -1.57426, 
-0.28623, -0.54861, -1.07336, -0.71558, 0.02385, -0.38164, -1.09721, 
0, 0.14311, -0.38164, -1.02566, -0.42934, -0.35779, -0.4532), 
    resProp.Dwell = c(0.8, -4, -3.2, 2.7, -1.6, -1, -2.4, -0.4, 
    -0.8, 1, -12.1, 0.2, -5.2, 3.7, -2.7, -1.7, 1.5, 0.7, -7.9, 
    0.3, 0.3, 1.4, -3.3, -1, -1.6, 1.5, 0.5, 1.5, -1, -2.2, -3.5, 
    0.5, 0.5, -0.9, -0.4, -3.4, 0.9, 0.1, -0.2, -2.8, -0.8, -6.2, 
    11.3, -4.6, 1, 1.1, -1.7, 4.1, -5, 2.3, -2.3, 4.6, -6.3, 
    6.3, -6.9, 0, 2.4), cbre.office.primeYield = c(0, 0, 0.15, 
    0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.2, 0.15, 0.1, 
    0.05, 0.15, 0.3, 0.35, 0.4, 0.3, 0.2, 0, -0.15, -0.85, -1, 
    -0.85, -0.75, -0.1, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0, 0, 
    0, 0.2, 0.2, 0.2, 0.2, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 
    0, 0, 0, 0, 0, 0, 0), cbre.retail.capitalValue = c(-1882.35294, 
    230.76923, -230.76923, -226.41509, -670.78117, -436.13707, 
    -222.22223, 0, -205.91233, -202.16847, 0, -393.5065, -403.91909, 
    -186.30647, -539.81107, -748.11463, -764.70588, -311.47541, 
    -301.42782, -627.09677, -480, 720, 782.6087, 645.96273, 251.42857, 
    1386.66667, -533.33334, -533.33333, -533.33333, 0, 0, -1024.56141, 
    -192.10526, 0, -730, 0, 0, 0, 0, 0, -834.28571, 0, -1450.93168, 
    0, 0, 0, -700.78261, 0, 0, 0, 0, 0, 0, 0, -1452, 0, 0)), .Names = c(""oenb_dependent"", 
""carReg"", ""cpi"", ""primConstTot"", ""resProp.Dwell"", ""cbre.office.primeYield"", 
""cbre.retail.capitalValue""), row.names = c(NA, -57L), class = ""data.frame"")
&gt; 
&gt; fit &lt;- lm(oenb_dependent ~ carReg + cpi + primConstTot + 
+             resProp.Dwell + cbre.office.primeYield + cbre.retail.capitalValue , data = datSel)
&gt; summary(fit) # show results

Call:
lm(formula = oenb_dependent ~ carReg + cpi + primConstTot + resProp.Dwell + 
    cbre.office.primeYield + cbre.retail.capitalValue, data = datSel)

Residuals:
   Min     1Q Median     3Q    Max 
-5.166 -1.447 -0.162  1.448  7.903 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               0.831630   0.492297    1.69    0.097 .  
carReg                    0.085208   0.039600    2.15    0.036 *  
cpi                      -0.349192   0.212044   -1.65    0.106    
primConstTot              0.752772   0.383810    1.96    0.055 .  
resProp.Dwell             0.994356   0.086812   11.45  1.4e-15 ***
cbre.office.primeYield    1.274734   1.212782    1.05    0.298    
cbre.retail.capitalValue  0.000528   0.000643    0.82    0.416    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.24 on 50 degrees of freedom
Multiple R-squared:  0.754, Adjusted R-squared:  0.725 
F-statistic: 25.6 on 6 and 50 DF,  p-value: 1.2e-13
</code></pre>

<p>I tried the following:</p>

<pre><code>vals.multipleRegr &lt;- forecast(fit, h = 20)
Error: could not find function ""forecast""
</code></pre>

<p>However, this does not work as the function forecast cannot be found. I am using the following packages in my code, <code>library(bootstrap)</code>, <code>library(DAAG)</code> and <code>library(relaimpo)</code>. </p>

<p>Any suggestion how to forecasting using multiple regression?</p>

<p>I appreciate your replies!</p>
"
"0.168005376258062","0.163238868537283","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.276790359705331","0.264219293935362","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.276790359705331","0.280732999806322","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.109985336266015","0.0831171634923383","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"0.155542754209564","0.17631812981527","172550","<p>I want to forecast time-series data using the forecast package methods, but with holidays as dummy variables, as in the following:
<a href=""http://www.r-bloggers.com/forecasting-with-daily-data/"" rel=""nofollow"">http://www.r-bloggers.com/forecasting-with-daily-data/</a>
(see also:)
<a href=""http://stats.stackexchange.com/questions/92743/forecasting-with-holiday-dummy-variables"">Forecasting with holiday dummy variables</a>
I wanted to get the code for finding public holiday dates automatically, so I don't need to upload my data (which the StackExchange user had to do).</p>

<p>The function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/bizdays"" rel=""nofollow"">bizdays</a> can count the number of ""business days"" in a month/or quarter. But its usage example is </p>

<pre><code>bizdays(wineind, FinCenter = ""Sydney"")
</code></pre>

<p>I looked at the source for bizdays, and in  particular the lines:</p>

<pre><code>days.len &lt;- as.timeDate(seq(start, end, by = ""days""), 
                    FinCenter = FinCenter)
biz &lt;- days.len[isBizday(days.len, holidays = unique(format(days.len, 
                                                        ""%Y"")))] 
</code></pre>

<p>However, when I applied the second line to a day.len sequence of dates, it returned the dates from day.len, minus weekends. It did not eliminate Sydney public holidays, as I would expect.</p>

<p>So my question is, what is the point of specifying ""FinCenter"" parameter in biz days, if the function just returns generic 5-day week sequence of dates. How does the FinCenter impact the function?</p>

<p>Also, is there any way of automatically retrieving public holidays for a given financial center, or do I have to load it myself?</p>

<p>Am I better off removing the holidays for e.g. stock exchange data (as weekends are removed), before the forecast? Or is it better to model the public holiday as an extra dummy variable? (As in the example). </p>

<p>I am assuming daily data in this question. </p>
"
"0.200804832225625","0.204863154122693","173111","<p>There are two posts on CV about differentiation between short-term and long-term forecasting. e.g. <a href=""http://stats.stackexchange.com/questions/124843/what-is-the-distinction-between-short-term-and-long-term-forecasting"">here</a>. There is nothing on CV about combining (incorporating) long-term dynamics into short term forecasting. Is this irrelevant?</p>

<p>I wonder if someone can show/explain what is the statistical procedure of incorporating long term time series dynamics into short term forecasting? </p>

<p>I would like to forecast short-term by incorporating the long term: trend etc. Quite often is the case that short term forecasting is insufficient (short sighted) as in number of cases the time series converges to long term mean. (I understand this assumption is data dependent, but this is type of data I'm dealing with). </p>

<p>Is also the model choice in this case important? So the question is how to combine these two (short/long-term dynamics)? </p>

<p><strong>Actual example (fitted model) within <code>R</code> would be desired result. For nice answer I'm offering double the current bounty.</strong>   </p>

<p>EDIT: I'm going to disappoint in term of data and provide data from the <code>forecast</code> R package, since I think (for my purpose) it is going to be sufficient to answer this question. </p>

<p>Demonstration in R:</p>

<pre><code>library(forecast)
data(gold)   

plot(gold)
abline(h=mean(gold, na.rm=T), col=""orange3"", lwd=2)
lines(fitted(lm(gold[1:800]~index(gold[1:800]))), col=""blue3"", lwd=2)
abline(v=800, col=""red3"", lty=2, lwd=3)
</code></pre>

<p>I fitted number of models ""plainly"" to the data <code>(gold[1:800])</code> and all models would blindly forecast the upward trend. Here is example (picture nr. 2):</p>

<pre><code>plot(forecast(auto.arima(gold[1:800]), h=200))
</code></pre>

<p><a href=""http://i.stack.imgur.com/erUB3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/erUB3.png"" alt=""enter image description here""></a></p>

<p>We divide the sample data up to obs <code>800</code>. (what we would observe-""in-sample""). In light of the ""long-term"" mean value (approx. mean 390) and the last observation value just bellow <code>500</code>, I would like to forecast up to step 1000. (<code>index(gold)</code>) taking into account the <code>long-term mean value</code> and hence incorporate it into forecast. </p>

<p>Since here we are dealing with steps rather than time, imagine that steps 1:800 is ""long-term"" (in years) and from <code>800&gt;</code> the steps are of small scale, let say ""hours"". (possible the time representation doesn't matter). </p>

<p>So starting with forecast from the observation <code>800</code> forecast next 200 steps by taking into account the ""long-term"" mean value. </p>
"
"0.0898026510133875","0.101797319711858","175085","<p>I'm using model trees to forecast sales data. I've developed a pretty good model but I'm concerned about some of the models predictions. I'm working with R and using the M5P algo in the RWeka package. </p>

<p>I'm forecasting revenues so I never have negative numbers but my model predicts negative numbers. That makes me think I'm doing something incorrectly. Here is the shape of my data<a href=""http://i.stack.imgur.com/8f7j7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8f7j7.png"" alt=""enter image description here""></a></p>

<p>What should I do to make sure my model doesn't predict meaningless values?</p>
"
"0.109985336266015","0.124675745238507","180217","<p>I'm using time series data containing both trend and seasonality. I also have 2 endogenous predictor variables that I would like to include in my model.</p>

<p>In R I've used the forecast package to develop a dynamic regression model with use of <code>auto.arima()</code> and the <code>xreg</code> argument from the <code>forecast package</code>. I understand this procedure takes a regression and then attempts to fit the residuals with an ARMA Model.</p>

<p>I've also developed what seems to be an appropriate model using the forecasting Module in SPSS by specifying a Seasonal ARIMA model and including my covariates. However, one of the coefficients on one of my endogeneous predictors has a negative sign which makes no sense intuitively. </p>

<p>I've read Dr. Hyndman's article <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">The ARIMAX model muddle</a> and found it to be extremely insightful and useful. However, I have not been able to find any documentation on what type of statistical procedure SPSS uses to fit an ARIMA model with covariates, so I'm not sure how I should interpret the coefficients or how concerned I should be with a flipped sign. Any help clarifying the modelling procedure used by SPSS would be tremendously appreciated. </p>
"
"0.141990458561766","0.160955694994913","184713","<p>I am fairly new to R. I have attempted to read up on time series analysis and have already finished </p>

<ol>
<li>Shumway and Stoffer's <a href=""http://www.stat.pitt.edu/stoffer/tsa3/"" rel=""nofollow"">Time series analysis and its applications 3rd Edition</a>,</li>
<li>Hyndman's excellent <a href=""https://www.otexts.org/fpp"" rel=""nofollow"">Forecasting: principles and practice</a></li>
<li>Avril Coghlan's <a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html"" rel=""nofollow"">Using R for Time Series Analysis</a></li>
<li>A. Ian McLeod et al <a href=""http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf"" rel=""nofollow"">Time Series Analysis with R</a></li>
<li>Dr. Marcel Dettling's <a href=""https://stat.ethz.ch/education/semesters/ss2013/atsa/ATSA-Scriptum-SS2013_130218.pdf"" rel=""nofollow"">Applied Time Series Analysis</a></li>
</ol>

<p>Edit: I'm not sure how to handle this but I found a usefull resource outside of Cross Validated. I wanted to include it here in case anyone stumbles upon this question. </p>

<p><a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a></p>

<p>I have a univariate time series of the number of items consumed (count data) measured daily for 7 years. An intervention was applied to the study population at roughly the middle of the time series. This intervention is not expected to produce an immediate effect and the timing of the onset of effect is essentially unknowable.</p>

<p>Using Hyndman's <code>forecast</code> package I have fitted an ARIMA model to the pre-intervention data using <code>auto.arima()</code>. But I am unsure of how to use this fit to answer whether there has been a statistically significant change in trend and quantify the amount.</p>

<pre><code># for simplification I will aggregate to monthly counts
# I can later generalize any teachings the community supplies
count &lt;- c(2464, 2683, 2426, 2258, 1950, 1548, 1108,  991, 1616, 1809, 1688, 2168, 2226, 2379, 2211, 1925, 1998, 1740, 1305,  924, 1487, 1792, 1485, 1701, 1962, 2896, 2862, 2051, 1776, 1358, 1110,  939, 1446, 1550, 1809, 2370, 2401, 2641, 2301, 1902, 2056, 1798, 1198,  994, 1507, 1604, 1761, 2080, 2069, 2279, 2290, 1758, 1850, 1598, 1032,  916, 1428, 1708, 2067, 2626, 2194, 2046, 1905, 1712, 1672, 1473, 1052,  874, 1358, 1694, 1875, 2220, 2141, 2129, 1920, 1595, 1445, 1308, 1039,  828, 1724, 2045, 1715, 1840)
# for explanatory purposes
# month &lt;- rep(month.name, 7)
# year &lt;- 1999:2005
ts &lt;- ts(count, start(1999, 1))
train_month &lt;- window(ts, start=c(1999,1), end = c(2001,1))
require(forecast)
arima_train &lt;- auto.arima(train_month)
fit_month &lt;- Arima(train_month, order = c(2,0,0), seasonal = c(1,1,0), lambda = 0)
plot(forecast(fit_month, 36)); lines(ts, col=""red"")
</code></pre>

<p>Are there any resources specifically dealing with interrupted time series analysis in R? I have found <a href=""http://epoc.cochrane.org/sites/epoc.cochrane.org/files/uploads/21%20Interrupted%20time%20series%20analyses%202013%2008%2012_1.pdf"" rel=""nofollow"">this</a> dealing with ITS in SPSS but I have not been able to translate this to R. </p>
"
"0.155542754209564","0.146931774846058","185755","<p>I am trying to predict the proportion of due accounts type on a given day. </p>

<p>To elaborate a little, everyday I will have a list containing all the past due accounts on that day and the number of days its past due. (i.e. everyday I will have a bar-chart type of data with x-axis = account due days (from 1 to 60) and y-axis = proportion of today's accounts)</p>

<p>My goal is to predict the chart (or the proportions of the accounts) using historical data. Since this is not a single time series, I suppose I need to use a group/hierarchical time series analysis.</p>

<p>However, there's not too many examples online on how to forecast this type of data. The only useful package in R I found is hts or gts from package <a href=""https://cran.r-project.org/web/packages/hts/index.html"" rel=""nofollow"">hts</a>, but I am not familiar with it and aren't sure how to setup the data to fit the package.</p>

<p>I would imagine the way to fit the data should be something like this:</p>

<p><img src=""http://holland.pk/uptow/i4/a7bff32d16084050af1a805e1d17638d.jpg"" alt=""model""></p>

<p>Since I am new to time-series analysis and forecasting, I am hoping someone can provide some insights on forecasting on such data? and if possible, could you provide a general flow to check and run for forecasting?</p>

<p>Thanks!</p>
"
"0.0635000635000953","0.0719815750748694","185816","<p>I am trying to understand how the ets function of R in the forecast package computes initial level $l_0$, and initial trend $b_0$. I was under the impression that they are set to the intercept and slope of the whole data, but it seems it's not true. I would appreciate any explanation on this.</p>

<p>Also, Excel 2016 introduced <a href=""https://support.office.com/en-us/article/Forecasting-functions-897a2fe9-6595-4680-a0b0-93e0308d5f6e"" rel=""nofollow"">FORECAST.ETS</a> function, whose results don't seem to match those produced by R's ets function. What would be possible explanations for the discrepancies in the results.  </p>

<p>Another thing is that when I apply ets on <a href=""http://homepage.stat.uiowa.edu/~kchan/TSA/Datasets/airpass.dat"" rel=""nofollow"">airpass</a> dataset, it doesn't detect seasonality period (12) correctly. The model it picks is ETS(M,A,N), and when I force it to use model=""AAA"" it shows the following error message: ""Error in ets(airpass, model = ""AAA"") : Nonseasonal data.""</p>

<p>Any help is really appreciated.</p>
"
"0.210605884793558","0.23873587634214","187870","<p>I'm working on a sales forecasting package which should be easy to use for the end user. Given a time series with historical sales data I would like to automatically select one of the three forecasts: Auto.Arima, ETS and STLF. 
The idea is to split historical data into 80% train set and 20% test (holdout) set. Then run Auto.Arima, ETS, STLF and choose the one that has best MAPE on the test set. </p>

<p>Now comes the part that is not entirely clear to me. Once I figured out that e.g. ETS gives me the best result should I now </p>

<ol>
<li>Retrain ETS on the entire set of historical data and generate
forecast using this new model? My reservation here is that after I
run ETS again it may even change the class of the algorithm as well
as the fit parameters which will render the MAPE I got on the test
set irrelevant.  </li>
<li>Just generate the forecast using the model that was trained on the
80% train set? My problem with this approach is that we are ignoring
the last 20% of data which is probably the most important
information for the forecast.</li>
<li>The third idea is to use the same model fit parameters that we got
after training the model on the 80% train set. But then use the
entire set of data for        forecasting. This seems like a
reasonable approach but I cannot figure out how to do it for ETS and
STL (For Arima we can do it by supplying the original fit as the model
parameter of the arima function)</li>
</ol>

<p>Could you please let me know what is the right way to approach this problem?</p>
"
"0.210605884793558","0.23873587634214","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.0635000635000953","0.0719815750748694","191001","<p>I am using the following bayesGARCH <a href=""https://cran.r-project.org/web/packages/bayesGARCH/index.html/"" rel=""nofollow"">here</a> package in R. I am interested in forecasting $h_t$, the model setup is given bellow. </p>

<p>$r_t$ = $\varepsilon_t(\frac{v-2}{v}\omega_th_t)^{1/2}$ $\quad$ with $\quad$ $t=1,...,T$ </p>

<p>$\varepsilon_t \overset{iid}{\sim}N(0,1)$ </p>

<p>$\omega_t \overset{iid}{\sim}IG(\frac{v}{2},\frac{v}{2})$ </p>

<p>$h_t = \alpha_0 + \alpha_1r^{2}_{t-1}+\beta h_{t-1}$</p>

<p>The package only provides simulated estimates of the parameter coefficients, namely $\alpha_0$, $\alpha_1$, $\beta$ and $v$. From my understanding the BayesGARCH does not have a function to forecast $h_t$, so I will have to forecast this manually. Any advice on forecasting this conditional volatility would be much appreciated. </p>
"
"0.155542754209564","0.146931774846058","191852","<p>I am interested in forecasting conditional volatility, $h_t$, and returns, $y_t$, in a Bayesian GARCH framework. I am using the <strong>bayesGARCH</strong> package by Ardia in R (<a href=""https://cran.r-project.org/web/packages/bayesGARCH/index.html"" rel=""nofollow"">https://cran.r-project.org/web/packages/bayesGARCH/index.html</a>) in order to estimate the Bayesian GARCH. The model setup implemented by the bayesGARCH package can be seen below.</p>

<p>$r_t$ = $\varepsilon_t(\frac{v-2}{v}\omega_th_t)^{1/2}$ $\quad$ with $\quad$ $t=1,...,T$ </p>

<p>$\varepsilon_t \overset{iid}{\sim}N(0,1)$ </p>

<p>$\omega_t \overset{iid}{\sim}IG(\frac{v}{2},\frac{v}{2})$ </p>

<p>$h_t = \alpha_0 + \alpha_1r^{2}_{t-1}+\beta h_{t-1}$</p>

<p>This setup represents a GARCH(1,1) model with student-t innovations. Currently I am able to forecast $h_t$. I have however only managed to do so using one estimation window. Ideally, I would like to create a rolling window forecast which essentially allows me to update the posterior distribution of the parameters say every 50 observations instead of using the same posterior parameter estimates for the entire test sample. Does anyone have any ideas as to how to go about this?</p>

<p>I set out my existing R code below. Any advice/critiques/comments would be greatly appreciated.</p>

<ol>
<li><p>Fit bayesGARCH on training data</p>

<pre><code>MCMC.1 = bayesGARCH(train_data, mu.alpha = c(0,0), Sigma.alpha = 10000 * diag(1,2),mu.beta = 0, Sigma.beta = 10000, lambda = 0.01, delta = 4, control = list(n.chain=2,l.chain=nr_iterations,refresh=1000))
</code></pre></li>
<li><p>Extract parameter posterior distribution</p>

<pre><code>smpl &lt;- formSmpl(MCMC.1, l.bi = 10000, batch.size = 2)        
alpha0 = smpl[, 1]
alpha1 = smpl[, 2]
beta   = smpl[, 3]
nu     = smpl[, 4]
</code></pre></li>
<li><p>Create an empty conditional volatility series, with 100 rows (one entry for every percentile) and where the amount of columns equals the length of your test(forecast) sample - i.e. you'll have a sigma distribution for each day in your forecast sample</p>

<pre><code>ind  = seq(1,nr_iterations - burn-in,by=100)
nind = length(ind)
sigma2   = matrix(0,nind, test_length - 1)
</code></pre></li>
<li><p>For the conditional volatility forecast merely substitute the estimated parameters into the equation and use the training data as your return series
for period t and then the test data for every period thereafter.</p>

<pre><code>l    = 0 
for (i in ind){ 
 sigma2_temp = rep(0, test_length - 1)
 sigma2_temp[1] = alpha0[i]+alpha1[i]*train_data[train_length - 1]^2 
  for (t in 2:(test_length - 1))
   sigma2_temp[t] = alpha0[i]+alpha1[i]*test_data[t-1]^2+beta[i]*sigma2_temp[t-1]
   l = l + 1 
 sigma2[l,] = sigma2_temp
 }
</code></pre></li>
</ol>

<p>I am also interested in forecasting $y_t$. While I have seen the math and intuition for forecasting $y_t$ (<a href=""http://stats.stackexchange.com/a/152427/98939"">http://stats.stackexchange.com/a/152427/98939</a>) I am yet to find any code. Any ideas?</p>
"
"0.109985336266015","0.124675745238507","194130","<p>I want to get the cross-correlation of two time series <code>x</code> and <code>y</code> in R. </p>

<p>I have calculated an ARIMA model, and I can get the <code>mod1$residuals</code> from signal <code>x</code>. These residuals almost have no autocorrelation, so that's great. </p>

<pre><code>xts &lt;- ts(x,start=1,frequency=12) #convert to a time series
library(fpp)  #load forecasting package
mod1 &lt;- auto.arima(xts)
</code></pre>

<p>I now did the same procedure on signal <code>y</code>. </p>

<p>My question is: is this correct? Or should I somehow deduct the <code>mod1</code> (based on <code>x</code>) from <code>y</code> to de-trend it? </p>

<pre><code>ccf(mod1$residuals, mod2$residuals)
</code></pre>

<p>Secondly, I am confused about the order of operations. Should I prewhiten the data before calculating the model? </p>

<p>I found this code: </p>

<pre><code>prewhiten(x, y, x.model = ar.res,ylab=""CCF"", ...)
</code></pre>

<p>Should I estimate the <code>mod1</code> first and then supply it to the function <code>prewhiten</code>? And are <code>x</code> and <code>y</code> the two time series? Many thanks!</p>
"
"0.168005376258062","0.19044534662683","194635","<p>I'm having a doubt with a time series. I have to find the best model for it and use it to do some forecast. The data are about the arctic oscillation (AO) from 1950 to 2015.</p>

<p><a href=""http://i.stack.imgur.com/NA84u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NA84u.png"" alt=""Plot of the ts""></a></p>

<p>The series is clearly stationary, and the augmented Dickey-Fuller (ADF) test confirms it.</p>

<p>The ACF and PACF for absolute values of the series are depicted below.</p>

<p><a href=""http://i.stack.imgur.com/DyLWQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DyLWQ.png"" alt=""enter image description here""></a></p>

<p>Running <code>seasonplot</code> from ""forecast"" package in R, I can see that in the summer months the values look clearly more clustered, while during the end/beginning of the year the values are more scattered.</p>

<p>The question is: How can I find the best model? Based on the PACF, before seeing the seasonality, I chose an AR(1), and it was good till I discovered the seasonal thing. How can I find the best model to do forecasting?</p>

<p>Thanks!</p>
"
"0.21997067253203","0.22857219960393","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"NaN","NaN","201397","<p>I have been searching for an R package for TDNN but only found packages that implement RNNs . My intention is to use a Neural Network for time series forecasting. In this context,  are both the same or what are their differences?</p>
"
"0.141990458561766","0.12876455599593","212852","<p>What is the difference between a GARCH simulation and a GARCH Monte Carlo simulation? </p>

<p>I look in the vingette for the ""rugarch"" package in R, <a href=""https://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf"" rel=""nofollow"">Introduction to Rugarch</a>. In section <em>6 Simulation</em> on page 32 it is written: </p>

<blockquote>
  <p>Simulation may be carried out either directly on a fitted object (<code>ugarchsim</code>) else on a GARCH spec with fixed parameters (<code>ugarchpath</code>). The <code>ugarchsim</code> method takes the following arguments: ... </p>
</blockquote>

<p>But Monte Carlo is in section <em>8 Simulated Parameter Distribution and RMSE</em> on page 35. There it is written:</p>

<blockquote>
  <p>It is sometimes instructive to be able to investigate the underlying density of the estimated parameters under different models. The <code>ugarchdistribution</code> method performs a monte carlo experiment by simulating and fitting a model multiple times and for different window sizes. This allows to obtain some insight on the consistency of the parameter estimates as the data window increases by looking at the rate of decrease of the Root Mean Squared Error...</p>
</blockquote>

<p>I ran <code>ugarchpath</code>, <code>ugarchsim</code> and <code>ugarchdistribution</code> on my computer:</p>

<ul>
<li>Here are <strong>two pictures</strong>  from a <code>ugarchpath</code> and <code>ugarchsim</code>: 

<ul>
<li><a href=""http://cl.ly/1S1X0W2W3m2O"" rel=""nofollow"">parameter density plot <code>ugarchpath</code></a> </li>
<li><a href=""http://cl.ly/3S1T0L1O4034"" rel=""nofollow"">parameter plot <code>ugarchsim</code></a></li>
</ul></li>
<li>Here are <strong>two pictures</strong>  from a <code>ugarchdistribution</code>: 

<ul>
<li><a href=""http://cl.ly/421k0U0I1z2p"" rel=""nofollow"">Parameter density plot</a> </li>
<li><a href=""http://cl.ly/1i3z1J3x0X3a"" rel=""nofollow"">likelihood, skew, etc...</a></li>
</ul></li>
</ul>

<p>I wonder what the difference is. I look at the output and I think I understand Monte Carlo but not the simulation, or at least not what the difference is. </p>

<p>P.S. You may also note section <em>5 Forecasting and the GARCH Bootstrap</em> that is related.</p>
"
"0.0898026510133875","0.101797319711858","214602","<p>I use R for time series analysis. I would like to evaluate decomposition algorithms. <code>decompose</code> and <code>stl</code> from ""stats"" package lead to good results but often, the residuals are not meaningless.</p>

<p>Example:</p>

<pre><code>dec &lt;- decompose(AirPassengers)
Box.test(dec$random[7:138], lag = 24, type = ""Ljung"")
&gt; p-value &lt; 2.2e-16
</code></pre>

<p>There is still a lot of autocorrelation in the residuals, the same for <code>decompose</code> with <code>type = ""multiplicative""</code> and for <code>stl</code>. If possible, I would like to extract all meaningful information from the residuals. Thus, I had a look on classical forecasting techniques:</p>

<pre><code>library(forecast)
dec &lt;- auto.arima(AirPassengers)
Box.test(dec$residuals, lag = 24, type = ""Ljung"")
&gt; p-value = 0.01356
</code></pre>

<p>Fitting a SARIMA model leads to less autocorrelation and thus, better ""information extraction"". For p > 0.05, one could argue for a Gaussian error distribution. </p>

<p>Is there a way to decompose the ARIMA fit into slowly varying components and oscillating components like with classical decomposition techniques?</p>
"
"0.168005376258062","0.19044534662683","217507","<p>I am trying to build an R tool for forecasting a (hopefully) wide range of time-series. I have settled on using several models, taking the forecasts from each, and deriving a weighed average of them using some weights.</p>

<p>My approach for arriving at appropriate weights for the averaging is to evaluate each model several times on parts of the historical data. For example, for monthly series I do the following:</p>

<blockquote>
  <p>I evaluate a one-step forecast for each model (five of them) for each of the last 12 months in the historical data $\{a_{i,j}\mid i\in\{1,\ldots,5\},j\in\{1,\ldots,12\}\}$ with $a'_{j}$ the actual observations. I evaluate six non-overlapping (ex. Oct+Nov+Dec, then Jul+Aug+Sep, etc.) three-step forecasts for each model, taking the mean of the forecasts for each of the five models at a time $\{b_{i,j}\mid i\in \{1,\ldots,5\},j\in\{1,\ldots,6\}\}$ with $b'_{j}$ as the mean of the relevant actuals at each time. Finally, I evaluate four six-month-overlapping (ex. Jan through Dec, Jul through Jun, etc.) 12-step forecasts for each model, taking again the mean for each model, getting the final set $\{c_{i,j}\mid i\in\{1,\ldots,5\},j\in\{1,\ldots,4\}\}$ with $c'_{j}$ the means of the relevant actuals.</p>
  
  <p>I put </p>
  
  <p>$$A=\left(\begin{array}{c}a_{i,j}\\b_{i,j}\\c_{i,j}\end{array}\right), x=\left(\begin{array}{c}w_1\\\ldots\\w_5\end{array}\right), b=\left(\begin{array}{c}a'_j\\b'_j\\c'_j\end{array}\right)$$
  ! and use <code>optim</code> from the <code>stats</code> package to optimise $x$ to give the least MAE between the two vectors $Ax$ and $b$.</p>
</blockquote>

<p>So my question is</p>

<blockquote>
  <p><em>Is this approach conceptually valid, considering that this evaluates something like the whether the <strong>model procedure</strong> is approriate for the time series, and not whether a <strong>particular model-with-parameters</strong> is?</em></p>
</blockquote>

<p>EDIT: Question paraphrased significantly to focus on aspects not answered <a href=""http://stats.stackexchange.com/questions/163074/assigning-weights-to-an-averaged-forecast"">here</a>.</p>
"
"0.168005376258062","0.19044534662683","218525","<p>Let say that one wants to fit a model to a daily financial time series for prediction (e.g. ARIMA, SVM). If data are stationary, ideally the longer the time series, the better. In practice, I don't feel comfortable in blindly trusting stationarity tests (e.g. KPSS, ADF). For example, a 90% KPSS and ADF confirm that the following time series is stationary when it qualitatively doesn't seem to be homoscedastic.
<a href=""http://i.stack.imgur.com/Qv8x2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qv8x2.png"" alt=""enter image description here""></a>
Which quantitative methods exist to identify a reasonable starting date of the time series in terms of quality of the prediction (i.e. minimum test error, low variance of the prediction)? Please refer to R packages when possible.</p>

<p>My attempts:</p>

<p>(i) A brute force approach could consist in repeating the fitting for any length of the time series of interest (e.g. 1y, 1y+1d, ..., 5y). Anyway, this approach is too expensive.</p>

<p>(ii) Perform stationarity tests (ADF, KPSS) to the time series of minimum allowed length and extend the length until the tests reject the stationarity. The problem of this approach are multiple:
  (a) extremely dependent to the confidence of the test (e.g. 95% or 80%).
  (b) stationarity tests are not able to identify change of regime that may occurs for long financial time series. </p>

<p>Strictly related topic, but it doesn't provides automatic/quantitative procedures:
<a href=""http://stats.stackexchange.com/questions/188868/length-of-time-series-for-forecasting-modeling"">Length of Time-Series for Forecasting Modeling</a></p>

<p>EDIT (2/Jul/2016): After further thoughts, I think that an optimal approach could be to follow the principle ""the larger the dataset, the better"". After all, a model that is highly dependent on the length of the time series I guess that it could be considered a ""bad"" model. Rather than focusing on the selection of an optimal length, one could focus on the identification of features that are able to work well under different regimes of the time series.</p>
"
"0.141990458561766","0.160955694994913","220299","<p>I'm completely new to forecasting so please correct me if I'm wrong.</p>

<p>I'm trying to forecast sales data using R. My main concern is that when I decompose the data using <code>stl()</code> from <code>stats</code> package, it shows a seasonal component whereas when I use <code>ets()</code> or <code>auto.arima()</code> commands, they do not take a seasonal component into account. Can anyone please suggest to me where I am going wrong? Which method should I prefer?</p>

<p>I would like to do forecast for Aug15-Dec15.</p>

<p>My data are as follows:</p>

<pre><code>Month      Year Amount
January    2010 7632
February   2010 6686
March      2010 3442
April      2010 4556
May        2010 7796
June       2010 1534
July       2010 1466
August     2010 3535
September  2010 2503
October    2010 7534
November   2010 1197
December   2010 5861
January    2011 8846
February   2011 7219
March      2011 5066
April      2011 13177
May        2011 7833
June       2011 5585
July       2011 6392
August     2011 5787
September  2011 13488
October    2011 9413
November   2011 7610
December   2011 11301
January    2012 14912
February   2012 13578
March      2012 12091
April      2012 14628
May        2012 10703
June       2012 7373
July       2012 13638
August     2012 10794
September  2012 12186
October    2012 8137
November   2012 7874
December   2012 7707
January    2013 11569
February   2013 13446
March      2013 10339
April      2013 19086
May        2013 15201
June       2013 11741
July       2013 19368
August     2013 15755
September  2013 12214
October    2013 13859
November   2013 13096
December   2013 14548
January    2014 16191.1
February   2014 23122.3
March      2014 21421.6
April      2014 20904.5
May        2014 19711.5
June       2014 9481.9
July       2014 18699
August     2014 21271.9
September  2014 19515.5
October    2014 19890.6
November   2014 16789
December   2014 31409.3
January    2015 21917.2
February   2015 24911.4
March      2015 26072.4
April      2015 23919.3
May        2015 26980.8
June       2015 41661.2
July       2015 27065.4
August     2015 
September  2015 
October    2015 
November   2015 
December   2015 
</code></pre>

<p>My R code:</p>

<pre><code>x.ts &lt;- structure(c(7632, 6686, 3442, 4556, 7796, 1534, 1466, 3535,
    2503, 7534, 1197, 5861, 8846, 7219, 5066, 13177, 7833, 5585, 6392, 
    5787, 13488, 9413, 7610, 11301, 14912, 13578, 12091, 14628, 10703, 
    7373, 13638, 10794, 12186, 8137, 7874, 7707, 11569, 13446, 10339, 
    19086, 15201, 11741, 19368, 15755, 12214, 13859, 13096, 14548, 
    16191.1, 23122.3, 21421.6, 20904.5, 19711.5, 9481.9, 18699, 21271.9, 
    19515.5, 19890.6, 16789, 31409.3, 21917.2, 24911.4, 26072.4, 
    23919.3, 26980.8, 41661.2, 27065.4, NA, NA, NA, NA, NA),
  .Tsp = c(2010, 2015.91666666667, 12), class = ""ts"")

fit &lt;- stl(x.ts,na.action = na.omit,s.window = ""periodic"",robust = T)
plot(fit)
summary(ets(x.ts)) 
fit2 &lt;- auto.arima(x = x.ts, stepwise = F, approximation = F)
summary(fit2)  
</code></pre>

<p>EDIT:</p>

<pre><code>ets(x.ts)$aicc
[1] 1404.23  


ETS       AICc     
AAN    1404.26631   
ANN    1404.23046   
MNN    1411.95791   
MAN    1404.40096   
MMN    1400.49486   
</code></pre>
"
"0.228952734944813","0.219605066002471","223888","<p>I am interested in forecasting with a vector error correction model (VECM). I am facing a problem of not being able to transform a cointegrated series into a VECM model using the stationary series. </p>

<p>In multivariate forecast like VAR or VECM it is important to see which of the two models to use for forecasting. To decide whether to use a VAR or a VECM:</p>

<ul>
<li>First, we do a cointegration test using the <code>ca.jo</code> function from ""urca"" package in R. </li>
<li>If we find that there is no cointegrating vector suggested by the Johansen procedure, then we can run a VAR model. But if we find evidence of cointegration then we have to use a VECM model in order to incorporate the error correction coefficients in the model. </li>
<li>To test if there is cointegration in the series we use Johansen test on the data <strong>in levels</strong>, i.e. in non-stationary form. But after we find evidence of cointegration we have to incorporate as many cointegrating vectors in the VECM as the number suggested by the Johansen test. But then this time the VECM should have been run on stationary series having made them <strong>differenced</strong>. </li>
<li>But in R I am not getting the option as to how to make a VECM model differenced and then forecast it. R manuals are suggesting that we should use the function <code>vec2var</code> to convert a VECM to a VAR model and then forecast the VAR model thus obtained. </li>
<li>But the VAR model thus obtained from the VECM is <strong>at levels</strong> and <em>not</em> at <strong>differenced</strong> form. Hence, inference from this may be biased. </li>
</ul>

<p>I just want to run a VECM in <strong>differenced</strong> series (not <strong>in levels</strong>) and also to include the error correction term. Please help me with this. </p>
"
"0.155542754209564","0.17631812981527","225094","<p>In the <code>Arima()</code> method, in the <code>forecast</code> package in R, I can provide a vector of parameters to the <code>fixed</code> argument, and the model is estimated while ensuring the provided parameters are fixed to the supplied values.</p>

<p>However, when I do this, the model returns no standard errors for these coefficients. Why is this the case? Is it not possible to estimate standard errors of coefficients that are manually provided? Would love an explanation as to why this might be the case.</p>

<p>Moreover, the <code>forecast</code> method still calculates confidence intervals when forecasting from a model that has fixed parameters. Are these intervals still statistically valid? I would have thought such would rely on the standard errors of the estimated coefficients, which it seems we may not know in the case of manually-entered parameters?</p>
"
"0.109985336266015","0.0831171634923383","225297","<p>Any given ARIMA(p,d,q) model $y^*_t=\sum_{i=1}^pa_iy^*_{t-i}+e_t+\sum_{i=1}^qb_ie_{t-i}$,where $ y^*_t=\Delta_dy_t$ - the difference of d$^{th}$ order, can be re-written as a dynamic linear model in state space (<a href=""http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf"" rel=""nofollow"">1</a>, <a href=""http://radhakrishna.typepad.com/TimeSeries_Analysis_State_Space_Methods.pdf"" rel=""nofollow"">2</a>) in the following way: 
$$\begin{cases} y_t=ZX_t,\\
X_{t+1}=TX_t+Re_t.
\end{cases}
$$
Such representation is used for fitting model to the observed time series and forecasting future values.
What I'm trying to understand is the way function SSMArima from KFAS estimates the covariance matrix of the nondiffuse part of the initial state vector P1. The particularly important to the answer is this piece of code:</p>

<pre><code>nd &lt;- which(diag(P1inf) == 0)
    mnd &lt;- length(nd)
    temp &lt;- try(solve(a = diag(mnd^2) - matrix(kronecker(T[nd, 
        nd], T[nd, nd]), mnd^2, mnd^2), b = c(R[nd, , drop = FALSE] %*% 
        Q %*% t(R[nd, , drop = FALSE]))), TRUE)
    if (class(temp) == ""try-error"") {
        stop(""ARIMA part is numerically too close to non-stationarity."")
    }
    else P1[nd, nd] &lt;- temp
</code></pre>

<p>The main part is </p>

<pre><code>solve(a = diag(mnd^2) - matrix(kronecker(T[nd, 
    nd], T[nd, nd]), mnd^2, mnd^2), b = c(R[nd, , drop = FALSE] %*% 
    Q %*% t(R[nd, , drop = FALSE])))
</code></pre>

<p>In its simplest case when $d = 0, Q = 1$ the covariance matrix P1 is obtained after solving the equation:
$$
(I_{r^2} - T\otimes T)x = vec(RR^T)
$$
and putting the result into square matrix (devectorize it $devec(x)$).
<br>
The closest thing I came to it (considering $\mathbb{E}X_tX_{t}^T = \mathbb{E}X_{t + 1}X_{t + 1}^T = \Sigma $) is solving
$$
\Sigma = T\Sigma T^T + RR^T
$$
But how are these two approaches equivalent and can we really think that $\mathbb{E}X_tX_{t}^T = \mathbb{E}X_{t + 1}X_{t + 1}^T$? There is nothing about it in the specification of the package.</p>
"
"0.109985336266015","0.124675745238507","226934","<p>I have bi-weekly data for an event for which I am trying to build a forecasting model. When I plot the ACF and PACF, I get the following plots:</p>

<p><a href=""http://i.stack.imgur.com/Ak0gL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ak0gL.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/7LAjO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7LAjO.png"" alt=""enter image description here""></a></p>

<p>From what I understand, the plots show that the data are seasonal and seasonality has almost a fixed period of length 13 (as there are 13 bars in each block in the ACF plot). The data also seem to have a downward trend because of the auto correlation diminishes from left to right in the plot. My questions are:</p>

<ol>
<li>Am I interpreting the plots correctly?</li>
<li>What types of models should I try with such data?</li>
</ol>

<p>I have already tried <code>auto.arima()</code> and <code>HoltWinters()</code> from the <code>forecast</code> package without much success. Any guidance is appreciated! Thanks!</p>
"
"0.269407953040162","0.169662199519763","230538","<p>I am using the rugarch package in R to forecast volatility using 5 minute data. The package has an a conditional variance model exclusively written for high frequency data, the Multiplicative Component Garch, I know that. 
The thing is that I invested a whole month to to find tick data and transform them to 5 minute intervals which I then seasonally adjusted using techniques from the extant bibliography. 
I am trying to manipulate the package even though It is not efficient for 5 minute data to extract the conditional variance forecasts. My data span the period from 2015-01-01 17:00:00 to 2015-12-31 17:00. (I am willing to reduce my sample to find meaningful results).</p>

<p>This is how my data look like with five fivemin_returns[,2] being the standardised (deseasonalized) returns and <a href=""https://drive.google.com/file/d/0B9Gcg5pCwYo6OERFMjg3SjNlaVU/view?usp=sharing"" rel=""nofollow"">click this to download my data</a></p>

<pre><code>head(fivemin_returns, 3)

                           DPRICE STD_DPRICE
2015-01-01 17:00:00  0.000000e+00  0.0000000
2015-01-01 17:05:00  9.797714e-05  0.2324451
2015-01-01 17:10:00  2.027022e-04  0.9845100

tail(fivemin_returns, 3)
                           DPRICE STD_DPRICE
2015-12-31 16:50:00 -0.0010186764 -6.0913134
2015-12-31 16:55:00 -0.0006841370 -3.9376338
2015-12-31 17:00:00  0.0002481227  1.1458867
</code></pre>

<p>So I want to create several conditional variance models and extract the forecasted sigma values. My methodology draws heavily from this article <a href=""http://unstarched.net/2013/01/07/does-anything-not-beat-the-garch11/"" rel=""nofollow"">Does anything NOT beat the GARCH(1,1)?</a>. I fit a MA(1) process to my returns without the conditional mean.</p>

<pre><code>library(xts)
library(rugarch)
model = c('sGARCH', 'gjrGARCH', 'eGARCH', 'apARCH', 'csGARCH', 'fGARCH', 'fGARCH', 'fGARCH')
submodel = c(NA, NA, NA, NA, NA, 'AVGARCH', 'NGARCH', 'NAGARCH')
spec1 = vector(mode = 'list', length = 8)
for (i in 1:8) spec1[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL))
spec2 = vector(mode = 'list', length = 8)
for (i in 1:8) spec2[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL), distribution = 'sstd')
spec3 = vector(mode = 'list', length = 8)
for (i in 1:8) spec3[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL), distribution = 'nig')
spec4 = vector(mode = 'list', length = 8)
for (i in 1:8) spec4[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i &gt; 5) submodel[i] else NULL), distribution = 'jsu')
spec = c(spec1, spec2, spec3, spec4)
cluster = makePSOCKcluster(15)
clusterExport(cluster, c('spec', 'fivemin_returns'))
clusterEvalQ(cluster, library(rugarch))
# Out of sample estimation
n = length(spec)
fitlist = vector(mode = 'list', length = n)
for (i in 1:n) {
    tmp = ugarchroll(spec[[i]], fivemin_returns[,2], n.ahead = 1, forecast.length = 8640, refit.every = 50, refit.window = 'moving', windows.size = 1500, solver = 'hybrid', calculate.VaR = FALSE, cluster = cluster, keep.coef = FALSE)
    if (!is.null(tmp@model$noncidx)) {
        tmp = resume(tmp, solver = 'solnp', fit.control = list(scale = 1), solver.control = list(tol = 1e-07, delta = 1e-06), cluster = cluster)
        if (!is.null(tmp@model$noncidx))
            fitlist[[i]] = NA
    } else {
        fitlist[[i]] = as.data.frame(tmp, which = 'density')
    }
}
</code></pre>

<p>I tried to run the code but it is impossible due to the amount of data and the wrong formulation of the <code>ugarchroll(spec[[i]], R, n.ahead = 1, forecast.length = 8640, refit.every = 50, refit.window = 'moving', windows.size = 1500, solver = 'hybrid', calculate.VaR = FALSE, cluster = cluster, keep.coef = FALSE)</code> argument I reckon. I am not really sure about the </p>

<pre><code>refit.every = 50, refit.window = 'moving', windows.size = 1500
</code></pre>

<p>How can I make it work? Decrease the sample size to 4 months maybe and try to forecast the last 15 days?</p>

<p>Edit 1: decreasing the sample and forecasting the condiational variance for the last 15 days is not the most practical thing to do as foreacting evaluation techniques with 15 (aggregated) observations is not a sensible thing to do. That is why I see at least one year of tick data in other papers and makes me think how much computing power have these scientists at their disposal? With my mere I3 processor and 4 gb ram I can't do anything.</p>
"
"0.127000127000191","0.143963150149739","231860","<p>I am trying to compare two forecasts using the Mariano Diebold test in R. Both  forecasts are for 150 days ahead; that is, on day $t$ I forecast $t+1, t+2, \dotsc, t+150$. </p>

<p>I deduced from <a href=""http://stats.stackexchange.com/questions/137261/understanding-forecast-horizon-for-diebold-mariano-tests"">this</a> post that my forecast horizon $h=150$. Using that, the Diebold-Mariano test (implemented using function <code>dm.test</code> in ""forecast"" package in R) gives a p-value of 1 no matter what forecasts I compare. </p>

<p>I looked into the code of this function, and I figured that this is caused by the following 3 lines of code (<code>d</code> is the vector of loss-differential series):</p>

<pre><code>n &lt;- length(d)
k &lt;- ((n + 1 - 2 * h + (h/n) * (h - 1))/n)^(1/2)   
STATISTIC &lt;- STATISTIC * k
</code></pre>

<p>Since in my case <code>n = h</code>, the variable <code>k</code> will always be 0 and therefore the test statistic is always zero. </p>

<p><strong>Questions:</strong></p>

<ol>
<li>Does this mean that we cannot use the Diebold-Mariano test when we are forecasting an entire period at once? I could not find any evidence of this in their paper. </li>
<li>How should I proceed to find a model-based way to compare my forecasts, rather than for example simply taking the MSE?</li>
</ol>
"
"0.0898026510133875","0.101797319711858","233312","<p>am new to data science.i have used forecast package in R and got some accuracy measurements like RMSE,ME,MAPE etc.</p>

<p>can anyone explain me this measurements in a practical approach?</p>

<p>Please see my example code.</p>

<pre><code>&gt; library(forecast)

&gt; data=read.csv(""experiment.csv"")
&gt; head(data)
  BATCH value value1 value2
1     I     5.77  21.32   34.82
2    II     4.46  20.36   46.89
3   III     4.57  22.64   42.95
4    IV     3.54  23.63   48.45
5     V     6.34  19.33   36.43
6    VI     4.56  25.36   39.58

&gt; dataset=data.frame(value=data$value,value1=data$value1,value2=data$value2)
&gt; fit.nn=nnetar(dataset$value)
&gt; modelaccuracy.nn=accuracy(fit.nn)
&gt; modelaccuracy.nn
                         ME             RMSE            MAE             MPE             MAPE        MASE        ACF1   
Training set 0.001978004     0.4052840794   0.3743702393    -0.9462031738   9.191369199 0.3249114902    -0.2064674413
</code></pre>

<p>here some measurements has negative values.what it actually means?whether it's a bad dataset or forecasting accuracy would be not good?</p>

<p>many thanks in advance.</p>
"
"0.228952734944813","0.259533259821102","234076","<p>I am looking to do time series forecasting with multiple variables. For example a data frame (df) of 4 different time series might look like this, where each column is its own time series: </p>

<pre><code>    X1 X2 X3 X4
1   4 13  2 81
2  24 91 86 58
3  21 97 39  1    
4   1 56 79 55
5  63  6 91 79
6  66 96 95 81 
</code></pre>

<p>Let's say X1 is 'cost' and the other variables are things like temperature, volume, and #_of_people.</p>

<p>I would like to forecast 'cost' using the other 3 variables. I imagine using something like <strong>Vector Autoregressive Models (VAR) for Multivariate Time Series</strong> can be used to see how each variable impacts the other in each separate time series (one for each variable). </p>

<p>For example, using the <strong>vars</strong> package in r we can run forecasts against the 4 time series and plot the results: </p>

<pre><code>var.2c &lt;- VAR(df, p = 2, type = ""const"")
var.2c.prd &lt;- predict(var.2c, n.ahead = 8, ci = 0.95)
fanchart(var.2c.prd)
</code></pre>

<p><a href=""http://i.stack.imgur.com/lq4VD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lq4VD.png"" alt=""enter image description here""></a></p>

<p>As I understand it, 4 separate regression models were built, one for each variable, where all the other variables were considered for each one. In other words, 'cost' was forecasted, taking into consideration not just the 'cost' trends, but also the impact the other 3 variables (X2, X3, and X3) had on 'cost.'</p>

<p>My question is, say I wanted to take a date in the future, on the forecast of cost, and see what happens to that forecasted value when temperature is increased (X2). I am assuming I can just take the coefficients in the 'cost' regression model that was used to forecast 'cost' using VAR, and use them as you would normally. For example, if the coefficient says the 'cost' will increase by 5 dollars for every one unit increase in temperature (X2), then I could take the forecasted value at the date of interest and add the $5, to say that is what would happen to the forecasted value if X2 were to change. </p>

<p>Are my intuitions correct here or am I missing something? Are there better ways to run 'what-if' analyses on forecasted multivariate time series?</p>
"
"0.127000127000191","0.143963150149739","234975","<p>I am doing time series forecasting in R using the VAR model implementation in the ""vars"" package. I have a default of six lags for my model, and my time series all have 120+ observations.  I'm doing a method of testing different combinations of series and back-testing them using VAR in order to decide which set of variables to choose.  How many variables can I expect to fit in to my VAR model (including the variable I am interested in forecasting)?</p>
"
