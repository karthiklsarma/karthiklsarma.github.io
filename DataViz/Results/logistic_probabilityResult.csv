"V1","V2","V3","V4"
"0.140224539037626","0.1420313721078","  3841","<p>I have two years of data which looks basically like this:</p>

<p>Date   <strong><em>_</em>__<em></strong>    Violence Y/N? _</em>  Number of patients</p>

<p>1/1/2008    <strong><em>_</em>___<em></strong>    0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 11</p>

<p>2/1/2008 <strong><em>_</em>__<em>_</em></strong>       0  <strong><em>_</em>__<em>_</em>__<em>_</em>__</strong> 11</p>

<p>3/1/2008 <strong><em>_</em>____</strong><em>1  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>4/1/2008 <strong><em>_</em>____</strong><em>0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>...</p>

<p>31/12/2009_<strong><em>_</em>__</strong>      0_<strong><em>_</em>__<em>_</em>__<em>_</em>__</strong>                 14</p>

<p>i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.</p>

<p>We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individualâ€™s probability of violence goes up when there are more patients on the ward.</p>

<p>I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesnâ€™t get above .1 at any lag, although this is above the â€œsignificantâ€ blue dashed line that R draws for me).</p>

<p>Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).</p>

<p>I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation â€œlme4 is not set up to deal with autoregressive structuresâ€. Even if it were, Iâ€™m not sure I grasp how to write the code anyway.</p>

<p>Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).</p>

<p>So Iâ€™m a bit stuck and lost to be honest. Any help gratefully received!</p>
"
"0.108185580601979","0.109579582579729","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.140224539037626","0.1420313721078"," 11679","<p>I have a nested-case control study that I have been using for analysis. At the end of my work I have deduced a set of variables that I use later to to classify new cases. One example of a simple classifier I am using is a naive Bayes, which will output simply a probability. </p>

<p>So here is my question:</p>

<p>Could I make my probabilities reflect the real world? In my specific example, the condition that I am testing for has a prevalence of 33% in my study, but a it has a population prevalence of only 10%.  Bayes factors have been suggested to me as a way to achieve this, however I am little unsure how to set up the problem. </p>

<p>As an example I have seen a Bayes factor as a logit between the true vs. study prevalence of the outcome. The classifier however was a logistic regression, and in that case the Bayes factor was just added to the linear predictors. I think the example there was very specific, and perhaps an inappropriate method for probabilities of a naive Bayes. Instead what I did was add the logit Bayes factor to the logged probabilities, but I am also not convinced this is right either. I also think a simpler solution would be to use Bayes theorem directly, but there I am not sure how to represented my study vs.population prevalences. The method below isn't quite right, but gets at what I want:</p>

<pre><code>        p_final = classier_posterior*(population_prev)/(study_prev)
</code></pre>

<p>I should contextualize that I use the probabilities to establish a threshold for classification down stream.</p>
"
"0.175780762327663","0.17804574744834"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.0917985092043157","0.0619875727373744"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.167600380788498","0.152783963429635"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.131168045574276","0.151837923600225"," 19869","<p>I'm running a predictive model to predict the probability of winning a certain item based on the price that I bid (other factors also). After running the model (ols) in R, I wanted to account for all the variables in my model and develop a graph highlighting the 'predicted probabilities' regarding the primary variables I'm concerned about. So want to have a line graph showing the probability of winning on the y axis, and the bid on the x axis. The following data would result in a graph which shows that the probability of winning decreases as the bid increases.</p>

<pre><code>Bid                  8  6      4
Probability Winning 30% 22%    18%
</code></pre>

<ol>
<li>Are predicted probabilities only relevant for logistic regression models or can be equally relevant for linear regression models?</li>
<li>What is the reasoning and logic behind going from a model to a probability curve which would show the 'trend' in one variable as predicted by another, while accounting for all other factors.</li>
</ol>

<p>Sorry for the elementary question, I'm just a little clueless.
Thanks for the help!</p>
"
"0.118511365784994","0.120038418441836"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.105999788000636","0.107365625419004"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.105999788000636","0.107365625419004"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.105999788000636","0.107365625419004"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.0749531688995861","0.0759189618001123"," 24975","<p>I am working on an assignment involving a logistic regression model, where I need to plot the pearson standardized residuals against one of the predictors. Here's the basic setup:</p>

<pre><code>model &lt;- glm(outcome ~ predictor1 + predictor2, family=binomial(logit))
res &lt;- residuals(model, ""pearson"")
</code></pre>

<p>When looking at the residuals' distribution, I see something totally different than my colleagues who use Stata (using predict and rstandard). Their residuals are more or less normal, whereas in mine there is a gap in the values (not a singe residual is between -0.05 and 1.15). That does make sense in the context of logistic regression, especially that the maximum predicted probability is not so high (38%). </p>

<p>I'd like to understand what's happening here... What is Stata doing that R isn't, with those residuals? </p>
"
"0.0749531688995861","0.0759189618001123"," 26178","<p>I want to create a classification table regarding an ordinal response variable with three levels but I don't know how to do it. Searching on the site I fell on the question posted by Brandon Bertelsen that covers only the case of the binary logistic regression (link at the end of the post).Does anyone knows how I can create such that table in my case?</p>

<p>I don't know if it is important but I used the <code>rms</code> package to run the olr and using the <code>predict(fit,type=""fitted.ind"")</code> command I get the next table with probability for each case</p>

<pre><code>      grade=1    grade=2   grade=3
1  0.08042197 0.28380601 0.6357720
2  0.08086877 0.28475584 0.6343754
3  0.41472656 0.40802584 0.1772476
4  0.39680650 0.41484517 0.1883483
5  0.25402385 0.43644283 0.3095333
6  0.13539881 0.37098177 0.4936194
7  0.12591996 0.35959459 0.5144855
8  0.50489952 0.36489760 0.1302029
9          NA         NA        NA
10 0.34757283 0.42969971 0.2227275
11 0.24690054 0.43539812 0.3177013
12 0.17325212 0.40529586 0.4214520
13 0.45795712 0.38900855 0.1530343
14 0.03594015 0.16033637 0.8037235
15         NA         NA        NA
16 0.50188652 0.36653955 0.1315739
17 0.48710163 0.37441720 0.1384812
18 0.38094725 0.42028884 0.1987639
19 0.04134659 0.17894428 0.7797091
20 0.12844729 0.36275605 0.5087967
21 0.23991274 0.43410413 0.3259831
22 0.20506362 0.42316514 0.3717712
23 0.45457929 0.39061326 0.1548075
24         NA         NA        NA
25 0.31269786 0.43606610 0.2512360
26 0.20905830 0.42483513 0.3661066
27 0.05240710 0.21353381 0.7340591
28 0.26569967 0.43759072 0.2967096
29 0.21258621 0.42621415 0.3611996
30 0.11407246 0.34347156 0.5424560
31 0.34656138 0.42993750 0.2235011
32 0.01813256 0.08978609 0.8920813
33 0.44034224 0.39716470 0.1624931
34 0.12213714 0.35468488 0.5231780
35 0.40888783 0.41032190 0.1807903
36 0.33901842 0.43161582 0.2293658
37 0.13275554 0.36793345 0.4993110
38 0.32091057 0.43492411 0.2441653
39 0.45984161 0.38810515 0.1520532
40 0.55550665 0.33564053 0.1088528
41 0.02812293 0.13122652 0.8406505
42 0.46250424 0.38681892 0.1506768
43 0.07352751 0.26852580 0.6579467
44 0.04330967 0.18541327 0.7712771
45 0.45457929 0.39061326 0.1548075
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r"">Logistic Regression: Classification Tables a la SPSS in R</a></p>
"
"0.0948090926279954","0.120038418441836"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.129822696722375","0.131495499095675"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.052999894000318","0.0536828127095019"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.118511365784994","0.120038418441836"," 30491","<p>Given a dataset: </p>

<pre><code>x &lt;- c(4.9958942,5.9730174,9.8642732,11.5609671,10.1178216,6.6279774,9.2441754,9.9419299,13.4710469,6.0601435,8.2095239,7.9456672,12.7039825,7.4197810,9.5928275,8.2267352,2.8314614,11.5653497,6.0828073,11.3926117,10.5403929,14.9751607,11.7647580,8.2867261,10.0291522,7.7132033,6.3337642,14.6066222,11.3436587,11.2717791,10.8818323,8.0320657,6.7354041,9.1871676,13.4381778,7.4353197,8.9210043,10.2010750,11.9442048,11.0081195,4.3369520,13.2562675,15.9945674,8.7528248,14.4948086,14.3577443,6.7438382,9.1434984,15.4599419,13.1424011,7.0481925,7.4823108,10.5743730,6.4166006,11.8225244,8.9388744,10.3698150,10.3965596,13.5226492,16.0069239,6.1139247,11.0838351,9.1659242,7.9896031,10.7282936,14.2666492,13.6478802,10.6248561,15.3834373,11.5096033,14.5806570,10.7648690,5.3407430,7.7535042,7.1942866,9.8867927,12.7413156,10.8127809,8.1726772,8.3965665)
</code></pre>

<p>..
I would like to determine the most fitting probability distribution (gamma, beta, normal, exponential, poisson, chi-square, etc) with an estimation of the parameters. I am already aware of the question on the following link, where a solution is provided using R:
<a href=""http://stackoverflow.com/questions/2661402/given-a-set-of-random-numbers-drawn-from-a-continuous-univariate-distribution-f"">http://stackoverflow.com/questions/2661402/given-a-set-of-random-numbers-drawn-from-a-continuous-univariate-distribution-f</a>
the best proposed solution is the following:</p>

<pre><code>&gt; library(MASS)
&gt; fitdistr(x, 't')$loglik                                                              #$
&gt; fitdistr(x, 'normal')$loglik                                                         #$
&gt; fitdistr(x, 'logistic')$loglik                                                       #$
&gt; fitdistr(x, 'weibull')$loglik                                                        #$
&gt; fitdistr(x, 'gamma')$loglik                                                          #$
&gt; fitdistr(x, 'lognormal')$loglik                                                      #$
&gt; fitdistr(x, 'exponential')$loglik                                                    #$
</code></pre>

<p>And the distribution with the smallest loglik value is selected.
However, other distrubtions such as beta distribution require the specification of some addition parameters in the fitdistr() function:</p>

<pre><code>   fitdistr(x, 'beta', list(shape1 = some value, shape2= some value)).
</code></pre>

<p>Given that i am trying to determine the best distribution without any prior information, i don't know what the value of the parameters can possibly be for each distribution. 
Is there another solution that takes this requirement into account? 
it does not have to be in R.</p>
"
"0.052999894000318","0.0536828127095019"," 30861","<p>I have fit a simple binary logistic GAM model in R and have used the plot() function to plot the results of this model.  The outputted graph shows a fitted line and a confidence interval, but the scale is clearly not 0-1. Does anyone know what is being plotted? Ideally I would like to get a graph of the predicted probability of the outcome versus the continuous predictor. Does anyone know how to get that out?</p>

<p>Thanks </p>
"
"0.0611990061362105","0.0619875727373744"," 31724","<p>Let's say I have a logistic regression model which predicts whether a consumer will buy an item based on about 10 consumer characteristics. </p>

<p>$$\begin{array}{rcl}Buy &amp;=&amp; B_0 + B_1\times Gender + B_2\times CreditType + B_3\times Education + B_4\times OwnsHome \\\phantom{Buy} &amp;&amp; + B_5\times CarMake + B_6\times CarYear + B_7\times State + B_8\times Income + B_9\times Insurance \\ \phantom{Buy} &amp;&amp;+ B_{10}\times CarAccidents\end{array} $$</p>

<ol>
<li><p>Is there ever an issue with including too many predictors in a logistic regression model? I'm not talking about insignificant variables or ones that may be related, but just the sheer number of variables included in a model. </p></li>
<li><p>With a larger number of predictors, how should one present the regression results in a meaningful manner? Is it just a matter of plotting the probability curve for $Y=1$, or are there ""better"" ways of doing this. I'd be doing this in R, so any help on that end would be appreciated.</p></li>
</ol>
"
"0.0917985092043157","0.0929813591060615"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.129822696722375","0.131495499095675"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.140224539037626","0.1420313721078"," 35269","<p>I'm using R to test some distribution families to my data.
I've done KS, AD tests and determined the loglike.</p>

<p>For one of the data the indications given by KS and AD do not agree with the ones given by the loglike:</p>

<pre><code>Table: p-values
Test    Normal  Log-normal  Gamma   Logistic    Weibull Gumbel
KS      0,16    0,00        0,00    0,26        0,00    0,49
AD      0,17    0,00        0,00    0,27        0,00    -

Measure Normal  Log-normal  Gamma   Logistic    Weibull Gumbel
loglike 282,86  279,54      308,96  284,41      304,00  291,55
</code></pre>

<p>I've read that KS gives more emphasis to the middle part of the distributions and AD to the tails. On the loglike one maximizes the probability of a model fitting the data.
The graphical analysis says to me that Log-normal, Gamma and Weibull only fit the data well at the left tail whereas the other distributions fit data quite well all over the domain....
So why does these three dist. have a larger loglike than the others that seem to fit better the data? Thanks</p>
"
"0.0749531688995861","0.0759189618001123"," 41660","<p>I'm modeling a set of outcome data the depends on two parameters:</p>

<ol>
<li>time, T</li>
<li>-100 &lt; A &lt; 100</li>
</ol>

<p>I've done logistic regression using R with the command:</p>

<pre><code>model &lt;- glm(Outcome ~ A + T, family = ""binomial"", data = myData)
</code></pre>

<p>My expectation (the only thing that makes sense) is that when A &lt; 0, the fit probability should be an increasing function of time approaching 0.5, while when A > 0 it should be a decreasing function of time approaching 0.5.</p>

<p>However, the fit I get is that A &lt; 0, A > 0, and A = 0 all are increasing functions of time.  They in fact appear to be the same curve just shifted (ie same ""shape"").</p>

<p>What am I doing incorrectly?  Any suggestions?</p>
"
"0.105999788000636","0.107365625419004"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.0374765844497931","0.0379594809000562"," 45754","<p>I have the following output from a logistic regression model.</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode10000:14849  0.0019334  0.0009006   2.147 0.031807 *  
our_bid:zipcode14850:19699  0.0022905  0.0009514   2.407 0.016064 *  
our_bid:zipcode19700:29999 -0.0009483  0.0008583  -1.105 0.269231    
our_bid:zipcode30000:31999 -0.0016309  0.0011028  -1.479 0.139161    
our_bid:zipcode32000:34999  0.0016241  0.0007856   2.067 0.038688 *  
our_bid:zipcode35000:42999  0.0023549  0.0008541   2.757 0.005831 ** 
our_bid:zipcode43000:49999  0.0007096  0.0008104   0.876 0.381286    
our_bid:zipcode50000:59999  0.0006533  0.0009269   0.705 0.480942    
our_bid:zipcode60000:69999  0.0030564  0.0008169   3.742 0.000183 ***
our_bid:zipcode7000:9999   -0.0027419  0.0012699  -2.159 0.030847 *  
our_bid:zipcode70000:79999  0.0013243  0.0007809   1.696 0.089921 .  
our_bid:zipcode80000:89999  0.0038726  0.0008006   4.837 1.32e-06 ***
our_bid:zipcode90000:96999  0.0038746  0.0007817   4.957 7.18e-07 ***
our_bid:zipcode97000:99820  0.0009085  0.0010044   0.905 0.365726    
---
</code></pre>

<p>I am using these coefficients to draw the predicted probabilities such that.</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>I realize that interpreting these interaction terms can be challenging. However, I generate the main regression equation and use that to formulate the probability curve. However, I'm not sure how to make sense of any of the ""our_bid:zipcode"" variables? </p>

<p>What about if my model output was: (instead saving zipcode as a factor, I make it a continuous variable)</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode             0.0019334  0.0009006   2.147 0.031807 *  
</code></pre>

<p>Would interpretation being easier with this approach? Keeping with the log-odds, how can I make sense of the log-odds effect that this model expresses for the interaction term?</p>
"
"0.218524161109851","0.221339907081527"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.0749531688995861","0.0759189618001123"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"0.198307444884526","0.172168022329123"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.212367311882161","0.215103725100318"," 48415","<p>I come to you today because I face a huge problem that I cannot explain.</p>

<p>I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing</p>

<pre><code>    mlogit &lt;- mlogit.data(Merge, choice = ""Choice"", shape = ""long"", alt.var = ""Comp"", 
                          drop.index = TRUE)
</code></pre>

<p>on my Merge data.</p>

<p>which gives me the following:</p>

<pre><code>                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice
    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE
    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE
    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE
    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE
    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE
    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
</code></pre>

<p>then I ran my regression :</p>

<pre><code>m1 &lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)
</code></pre>

<p>and it gave me significant results :</p>

<pre><code>                          Estimate  Std. Error  t-value  Pr(&gt;|t|)    
    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***
    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &lt; 2.2e-16 ***
    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &lt; 2.2e-16 ***
    M:Temp              1.5371e-02  9.8680e-04  15.5764 &lt; 2.2e-16 ***
    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &lt; 2.2e-16 ***
    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***
    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &lt; 2.2e-16 ***
    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***
    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &lt; 2.2e-16 ***
    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***
    R:Age               1.9465e-02  1.6479e-03  11.8119 &lt; 2.2e-16 ***
    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***
    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** 
    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &lt; 2.2e-16 ***
    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***
    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &lt; 2.2e-16 ***
    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***
    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &lt; 2.2e-16 ***
</code></pre>

<p>those results were in line with what I expected to find in literature so I was quite happy.</p>

<p>My next step was to plot my results and here is when I have some trouble.</p>

<p>First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.</p>

<p>I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.</p>

<p>So I wanted to use the <code>predict()</code> function but I don't know how to do that. I was hoping to get some help here.</p>
"
"0.105999788000636","0.0805242190642528"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.129822696722375","0.131495499095675"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0611990061362105","0.0929813591060615"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0749531688995861","0.0759189618001123"," 65095","<p>In documentation to <code>glm</code> I read: ""<em>For binomial and quasibinomial families the response can also be specified as a factor (when the first level denotes failure and all others success)</em>"" Does it mean that probability of failure or success is being modeled? </p>

<p>I'm trying to apply simple logistic model to ""german credit scoring"" dataset where there are levels ""good"" and ""bad"". To get correct results (higher probability means higher likelihood of being good) I have to assume that <code>Failure=Good</code> and <code>Success=Bad</code>. This works, but it is really counterintuitive. I interpret this as - this will model probability of Failure (failed to be bad).</p>

<pre><code>require(ggplot2)

german_data &lt;- read.csv(file=""http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"",
              sep="" "", header=FALSE)

names(german_data) &lt;- c('ca_status','mob','credit_history','purpose','credit_amount','savings',
'present_employment_since','status_sex','installment_rate_income','other_debtors',
'present_residence_since','property','age','other_installment','housing','existing_credits',
'job','liable_maintenance_people','telephone','foreign_worker','gb')

str(german_data)

german_data$gb &lt;- factor(german_data$gb,levels=c(2,1),labels=c(""bad"",""good""))

levels(german_data$gb)[1] 

table(german_data$gb)

model &lt;- glm(data=german_data,formula=gb~.,family=binomial(link=""logit""))

german_data$prob &lt;- predict(model,newdata=german_data, type=""response"")

ggplot(data=german_data) + geom_boxplot(aes(y=prob,x=gb))  + coord_flip()
</code></pre>

<p><img src=""http://i.stack.imgur.com/kcbao.png"" alt=""enter image description here""></p>
"
"0.0749531688995861","0.0759189618001123"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.0917985092043157","0.0929813591060615"," 65384","<p>I'm analyzing users' in-game data in order to model whether they're going to be paid user or not. </p>

<p>Here's my model:</p>

<pre><code>Logistic Regression Model

lrm(formula = becomePaid ~ x1 + x2 + 
    x3 + x4 + x5 + x6, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
x1              0.0756 0.0193   3.92 &lt;0.0001 
x2              0.0698 0.0091   7.64 &lt;0.0001 
x3              0.0020 0.0002  11.05 &lt;0.0001 
x4              0.0172 0.0057   3.03 0.0024  
x5              0.0304 0.0045   6.82 &lt;0.0001 
x6             -0.0132 0.0042  -3.17 0.0015  
</code></pre>

<p>And in my model, I created couple of use cases such as:</p>

<pre><code>    test1   test2       
x1  8           9
x2  10          10
x3  250        250
x4  6           6
x5  2           2
x6  0           1
</code></pre>

<p>Then the probability of user test1 is to turn out to be a paid user is %.07 and % 0.84 for test2.</p>

<p>However I want to calculate the cumulative probabilities such as users whose' x1 values are greater than 8, x2 values are between 10 and 20 and so on.</p>

<p>Is there any way to calculate this ?</p>

<p>Thanks ! </p>
"
"0.158999682000954","0.161048438128506"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.118511365784994","0.120038418441836"," 65730","<p>I want to check the probability distribution for my data using R.In some site I found <code>fitdistr</code> function in package <code>MASS</code> can do that. To check that I have generated 105 random Poisson numbers &amp; run the <code>fitdistr</code> function to check whether it is able to identify or not. I used following code</p>

<pre><code>library(MASS)
zpois=rpois(105,0.1)

fitdistr(zpois, 't')$loglik
fitdistr(zpois, 'normal')$loglik
fitdistr(zpois, 'logistic')$loglik
fitdistr(zpois, 'weibull')$loglik
fitdistr(zpois, 'gamma')$loglik
fitdistr(zpois, 'lognormal')$loglik
fitdistr(zpois, 'exponential')$loglik
fitdistr(zpois, 'Poisson')$loglik
fitdistr(zpois, 'negative binomial')$loglik
</code></pre>

<p>I found that it is giving lowest value for normal distribution. I know that the large sample approximation of Poisson distribution is normal distribution but I don't want the large sample approximation. I want to see the exact distribution.</p>

<p>Can you help me to guide the suitable function in R using which I can get the exact distribution fit?</p>
"
"0.191583193026391","0.194051796961375"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.214692691985145","0.206013854261698"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.259645393444749","0.252033039933378"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.140224539037626","0.1420313721078"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0865484644815831","0.0876636660637835"," 83908","<p>I am working in program R. I am modeling the incidence of flight in a seabird in relation to distance to the nearest ship (potential disturbance, range = 0 to 74 km from the bird). 1= flight during observation, 0 = no flight. The bird does fly with some unknown probability when no ships are present or really ""far"" away. I am trying to find this really far distance and associated probability of flight using binary logistic regression.</p>

<p>Model = Flight ~ ship distance. Other variables were explored but fell out with stepwise selection.</p>

<p>During exploratory analysis I truncated the data down only looking at smaller distances from the ship (20, 15, 10 km). These models are highly significant and predict that as the ship gets closer the probability of flight increases. However when I include all the data (out to 74 km) the intercept is significant (and predicts the true % of observed flight events) but the slope term is non-significant. </p>

<p>Can I use a weighting scheme to give more weight to observations when the ship was closer?</p>

<p>Thanks.</p>

<p>Edit: I am working through the suggestions made by @Scortchi and @Underminer. Here is a plot of a loess smooth on the observed data to better help visualize the pattern. </p>

<p><img src=""http://i.stack.imgur.com/ZabQh.jpg"" alt=""Loess smooth of probability of flight as a function of distance to nearest ship""></p>

<p>The distance to the ship data does not discriminate between approaching ships and departing ships it is just a straight line measure to the nearest ship. The dip in the probability of flight at 8.5 I believe can be attributed to ""unaffected"" birds that did not fly as the ship passed by them. So as the ship departs and gets further from the observation site we were more like to be observing birds that for whatever reason did not fly when the ship passed and are less likely to fly for ""naturally occurring"" reason. As additional birds fill back into the observation area the ""baseline"" flushing rate is resumed and birds start to fly at ""normal"" probabilities. </p>
"
"0.0749531688995861","0.0759189618001123"," 83945","<p>How do you solve the following problem?</p>

<blockquote>
  <p>A Simulation Study (Probit Regression).</p>
  
  <p>Assume $y|x\sim {\rm Binary}(p)$, where $p= E(y|x)$, and $Î¦^{-1}(\pi)=-1+5.1x_{1i}-0.3x_{2i}$
  Generate data with $x_{1i}\sim{\rm Unif}(0,1)$, $x_{2i}=1$ for $i$ odd and $x_{2i}=0$ for $i$ even, and sample size $n=500$. Try generalized linear model (GLM) with logistic and probit links.</p>
</blockquote>

<p>Here is what I did, I know there is a problem, but I don't know what:</p>

<pre><code>n         &lt;- 500
beta0     &lt;- -1
beta1     &lt;- 5.1
beta2     &lt;- -0.3
x1        &lt;- runif(n=n, min=0, max=1)
x2        &lt;- (1:n)%%2
y         &lt;- pnorm(beta0 + beta1*x1 + beta2*x2)
prob.glm  &lt;- glm(y~x1+x2, family=binomial(link=probit))
logit.glm &lt;- glm(y~x1+x2, family=binomial)
</code></pre>

<p>I know <code>y</code> is a probability here, but how do you simulate a binary variable from the probability? </p>
"
"0.218524161109851","0.221339907081527"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.120192462032251","0.1217411760924"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.149906337799172","0.132858183150197"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.167600380788498","0.169759959366261"," 94089","<p>I started to use the function <code>multinom</code> of <code>R</code> package <code>nnet</code> in order to fit several conditional probability distributions with the multinomial logistic model. I need the parameters of the fittings in order to pass them to a Java program, which will compute the probabilities and use them.</p>

<p>My problem is that the probabilities computed with the parameters returned by <code>multinom</code>, following the usual <a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_set_of_independent_binary_regressions"" rel=""nofollow"">definition</a> of multinomial logistic model, are not the same as those directly computed in <code>R</code>, which are the correct ones. On Stack Overflow I have already asked a <a href=""http://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"">question</a> about this issue, but I do not still know how the <code>R</code> function <code>multinom</code> computes these probabilities; my guess is that it relies on neural networks, since this function belongs to <code>R</code> package <code>nnet</code>, but I do not have any idea about the details, and an inspection of the code led to nowhere.</p>

<p>Do you know an <code>R</code> package which fits conditional probabilities and returns the corresponding parameters of the model, so that we may easily compute the probabilities in another program? E.g., using the MARS model (<code>R</code> package <code>earth</code>) or Projection Pursuit Regression (<code>R</code> package <code>ppr</code>) is not feasible, since computing the probabilities from the parameters of these models would be a mess. Besides, the function <code>mlogit</code> from the <code>R</code> package with the same name is not applicable as well, since the dataset should be in a certain format (we would also need the predictors corresponding to the alternative, ""non-chosen"" response variable).</p>
"
"0.183597018408631","0.185962718212123"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.211999576001272","0.214731250838008"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.118511365784994","0.120038418441836"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.052999894000318","0.0536828127095019","105301","<p>Given the outcome variable in a dataframe is a factored/categorical variable, when regressing the dependent variable (DV) onto a set of independent variables (IVs), what is the model predicting? The probability that the DV is the first level of the factor? Or the second?</p>

<p>A related question: I know that given a numerical column of $1$s and $0$s, a logistic regression would model the probability of the higher order variable (i.e., value=$1$), so I have been attempting to recode the factor ""character"" variable into a numerical one. I am coming from a SAS background, so I am entirely to used to <code>if var = ""yes"" then var_num = 1; else var_num=0;</code></p>

<p>That's clearly wrong. What's the most efficient way you have found to recode such variables?</p>
"
"0.211999576001272","0.187889844483257","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.129822696722375","0.131495499095675","109737","<p>To generate data with a missing at random (MAR) mechanism, usually we can first generate a complete data set and then model the missing probability for the variable $Y$; <em>i.e.</em> $Pr(Y=\text{missing}|{\bf X})$, using a logistical model, <em>i.e.</em> $$Pr(Y_i=1|{\bf X_i})=\frac{\exp(\beta'{\bf X_i})}{1+\exp(\beta'{\bf X_i})}.$$</p>

<p>Based on this probability we can generate the binary variable indicating if $Y$ is missing:</p>

<p>$$I_i = \text{rbinom}(1,p_i).$$</p>

<p>If $I_i=1$ then we delete the corresponding value of $Y$.</p>

<p><strong>If we would like to control the proportion of missing values of $Y$, how can we do that?</strong> Since by using the above method the proportion of missing data depends on the values of the $X$'s then the generation of an indicator variable is random, so it's hard to control the overall proportion of missing values.</p>
"
"0.0917985092043157","0.0929813591060615","109796","<p>I am currently working on a project using a sales system and trying to come up with a way to use the current pipeline of potential sales to predict the amount of product that will be sold in the future. Iâ€™m looking for advice on how to approach this problem and hopefully some resources to teach me what approach to use and why.</p>

<p>The sales system Iâ€™m using has historical data for opportunities (potential sales). Around 50,000 of the opportunities are â€œclosedâ€ meaning that they are either won or lost. I have around 1,000 â€œopenâ€ opportunities that have not yet been won or lost. Some variables that I have on each sale include the product (which is generally homogenous except for the amount), the amount, the salesman, the date, the time it was input into the system, the customer, and other data about the customer.</p>

<p>I understand that if I want to predict a dichotomous variable like win / lose then I should look at a logistic regression. However, Iâ€™m looking for general advice on how to </p>

<ol>
<li>Predict the probability of each individual opportunity closing as won using the data I have (and how to tell if I've done it correctly).</li>
<li>Estimate the total amount of won opportunities for a period.</li>
</ol>

<p>I found a similar question here <a href=""http://stats.stackexchange.com/questions/66276/using-a-logistic-model-on-the-estimates-of-several-other-classification-models"">Using a logistic model on the estimates of several other classification models</a> but Iâ€™m hoping for a response that gives me a better idea of where to start. Iâ€™m comfortable using R or any other statistical software, but ideally I'd like some kind of book or other reference material that is as low-level as possible.</p>
"
"0.118511365784994","0.120038418441836","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.183597018408631","0.185962718212123","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.105999788000636","0.107365625419004","113309","<p>Objective: To predict time to event of a customer using R
I developed a Non parametric model but as per my understanding we cannot perform predictions from these models, so I have to build a model which can predict for any test data. I hope extended cox model will work for my case (we have time depended co variants)</p>

<p>Questions:</p>

<ol>
<li><p>How the data structure would be for extended cox model?</p></li>
<li><p>What we will predict from Cox model (In logistic model we will predict probability in the similar lines what is for survival model)?</p></li>
<li>Variable Importance</li>
<li><p>Can we predict time when a customer will leave from the study?</p></li>
<li><p>How to validate the model?</p></li>
</ol>

<p>It would be great if someone provide the R code for the above questions 2, 3, and 4</p>

<p>Thanks in advance.</p>
"
"0.129822696722375","0.131495499095675","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.108185580601979","0.109579582579729","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.322385769334912","0.326539801670375","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.254178562378958","0.246260085215264","121120","<p>I am trying to create a logistic regression model with mgcv::gam
with what I think is a simple decision boundary, but the model
I build performs very poorly.  A local regression model built
using locfit::locfit on the same data finds the boundary very easily.
I want to add additional parametric regressors to my real-life model, so
I do not want to switch to a purely local regression.</p>

<p>I want to understand why GAM is having trouble fitting the data,
and whether there was ways of specifying the smooths that
can perform better.</p>

<p>Here's a simplified, reproducible example:</p>

<p>Ground truth is 1 = point lies within the unit circle, 0 if outside</p>

<p>e.g. z = 1 if sqrt(x^2 + y^2) &lt;= 1, 0 otherwise</p>

<p>The observed data is noisy, with both false positives and false negatives</p>

<p>Construct a logistic regression to predict whether a point
is inside the circle or not, based on the point's Cartesian
coordinates.</p>

<p>Local regression can find the boundary well (50% probability contour
is very close to the unit circle), but a logistic GAM consistently 
overestimates the size of the circle for the same probability band.</p>

<pre><code>library(ggplot2)
library(locfit)
library(mgcv)
library(plotrix)

set.seed(0)
radius &lt;- 1 # actual boundary
n &lt;- 10000 # data points
jit &lt;- 0.5 # noise factor

# Simulate random data, add polar coordinates
df &lt;- data.frame(x=runif(n,-3,3), y=runif(n,-3,3))
df$r &lt;- with(df, sqrt(x^2+y^2))
    df$theta &lt;- with(df, atan(y/x))

# Noisy indicator for inside the boundary
df$inside &lt;- with(df, ifelse(r &lt; radius + runif(nrow(df),-jit,jit),1,0))

# Plot data, shows ragged edge
(ggplot(df, aes(x=x, y=y, color=inside)) + geom_point() + coord_fixed() + xlim(-4,4) + ylim(-4,4))
</code></pre>

<p><img src=""http://i.stack.imgur.com/BfzkT.png"" alt=""enter image description here"">    </p>

<pre><code>### Model boundary condition using x,y coordinates

### local regression finds the boundary pretty accurately
m.locfit &lt;- locfit(inside ~ lp(x,y, nn=0.3), data=df, family=""binomial"")
plot(m.locfit, asp=1, xlim=c(-2,-2,2,2))
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/fy6z3.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM fits very poorly, also tried with fx=TRUE but didn't help
m.gam &lt;- gam(inside ~ s(x,y), data=df, family=binomial)
plot(m.gam, trans=plogis, se=FALSE, rug=FALSE)
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/ObeIm.png"" alt=""enter image description here"">  </p>

<pre><code>### gam.check doesn't indicate a problem with the model itself
gam.check(m.gam)

Method: UBRE   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [5.41668e-10,5.41668e-10]
(score -0.815746 &amp; scale 1).
Hessian positive definite, eigenvalue range [0.0002169789,0.0002169789].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

           k'    edf k-index p-value
s(x,y) 29.000 13.795   0.973    0.08

#### Try using polar coordinates

### Again, locfit works well
m.locfit2 &lt;- locfit(inside ~ lp(r, nn=0.3), data=df, family=""binomial"")
plot(m.locfit2)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9zr73.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM misses again
m.gam2 &lt;- gam(inside ~ s(r, k=50), data=df, family=binomial)
plot(m.gam2, se=FALSE, rug=FALSE, trans=plogis)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/55XiQ.png"" alt=""enter image description here"">    </p>

<pre><code>### Can also plot gam on link scale for alternate view
plot(m.gam2, se=FALSE, rug=FALSE)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/voMP4.png"" alt=""enter image description here"">    </p>

<pre><code>gam.check(m.gam2)

Method: UBRE   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-3.29203e-08,-3.29203e-08]
(score -0.8240065 &amp; scale 1).
Hessian positive definite, eigenvalue range [7.290233e-05,7.290233e-05].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

         k'    edf k-index p-value
s(r) 49.000 10.537   0.979    0.06
</code></pre>
"
"0.183597018408631","0.17046582502778","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.0917985092043157","0.0929813591060615","122580","<p>I'm running a logistic regression with backard selection method. I get coefficients with p-values>.10. Here's an example:</p>

<pre><code>    DF   Estimate   Error   Chi-Square  Pr &gt; ChiSq  Estimate    Exp(Est)
Intercept   1   -30,32       11,48       6,97        0,01            -     
v1  1    0,001       0,00        9,70        0,00        0,10        1,00   
v2  1   -0,001       0,00        2,84        0,09       -0,07        1,00   
v3  1    0,000       0,00        0,12        0,73        0,01        1,00   
v4  1   -0,000       0,00        0,11        0,74       -0,01        1,00   
v5  1   -0,000       0,00        0,74        0,39       -0,03        1,00   
v6  1    0,000       0,00        0,58        0,45        0,02        1,00   
v7  1   -0,005       0,00        3,98        0,05       -0,07        1,00   
v8  1    0,002       0,01        0,04        0,84        0,01        1,00   
v9  1   -0,016       0,05        0,09        0,76       -0,02        0,98   
v10 1    0,014       0,03        0,29        0,59        0,03        1,01   
v11 1    0,102       0,03        14,77       0,00        0,09        1,11   
v12 1    0,009       0,01        1,27        0,26        0,05        1,01   
v13 1   -0,017       0,01        2,39        0,12       -0,05        0,98   
v14 1   -0,005       0,01        0,48        0,49       -0,03        1,00   
</code></pre>

<p>My question is, if the algorithm selects best variables, how is it be possible that keeps the variables that have p-values greater than 0.1? I know that the effect is reflected in the value of the coefficient but the pvalue shows the probability that having that value in that coefficient is only a coincidence, and the coefficient is 0 (considering all the other variables). So why is still keeping those?</p>
"
"0.0917985092043157","0.0929813591060615","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"0.129822696722375","0.131495499095675","126338","<p>I'm running a binary prediction using a supervised topic modeling package in R (<code>lda</code> package, using <code>slda.predict</code> function). The result of the prediction returns results in linear space. From Googling around, people say that I need to take a sigmoid  to convert the result to a logical value. I'm not really sure what this means. </p>

<p>Basically I have list of documents, and their corresponding labels. What I am trying to do is set 80% of these documents and their labels, and train them using supervised LDA. The label of the document is 0 or 1. I manage to train the document just fine using this piece of code:</p>

<pre><code>example &lt;- c(""I am the role model"",""I have a major crazy   headache"",""i don't have money"", ""you are money crazy major"")
corpus = lexicalize(example, lower=TRUE)
label = c(1,1,0,0)
params &lt;- sample(c(1, 0), 2, replace=TRUE)
result &lt;- slda.em(documents=corpus$documents,
              K=2,
              vocab=poliblog.vocab,
              num.e.iterations=10,
              num.m.iterations=4,
              alpha=1.0, eta=0.1,
              label,
              params,
              variance=0.25,
              lambda=1.0,
              logistic=TRUE,
              method=""sLDA"")
</code></pre>

<p>for simplicity purpose, i'll try to predict the same document given the model above.</p>

<pre><code>predictions &lt;- slda.predict(corpus$documents,
                            result$topics, 
                        result$model,
                        alpha = 1.0,
                        eta=0.1)
</code></pre>

<p>Now, my problem is, the result of the prediction isn't binary. it's continuous value. I need to convert it back to binary using some sort of sigmoid(according to an <a href=""https://lists.cs.princeton.edu/pipermail/topic-models/2012-June/001912.html"" rel=""nofollow"">article here</a>) </p>

<p>The result i'm getting doesn't seem like a probability. For the 4 documents above, this is the output of the predictions variable</p>

<pre><code>           [,1]
[1,]  44.827420
[2,]  53.895682
[3,] -17.139034
[4,]   1.299764
</code></pre>

<p>How do I do this in R?</p>
"
"0.140224539037626","0.1420313721078","126715","<p>I am doing a logistic regression. The predictor variables are a mixture of categorical and continuous. I ran glm and out of the 80 predictors about 36 came out to be significant based on the p value. The accuracy of the model was also very good.</p>

<ol>
<li><p>I am still stuck with 36 variables but I want to narrow it down further to identify which of the predictors have the greatest impact. I understand that all the 36 predictors are statistically significant but all of these variables do not impact the DV equally. Is their a way to rank these variables based on their influence on the DV? Please feel free to suggest any methods/algorithms you know that does this efficiently.</p></li>
<li><p>Once I narrow down the predictors of my interest, I want to come up with rules based on the variables, much like a decision tree gives. I have tried running <code>rpart</code> and <code>ctree</code> on the 80 variable dataset but the output tree is very small meaning only a few variables appear in the tree, and thus there are very few rules which I can make based on that. I wonder if their is a way to increase the size of my tree to include more variables. Suppose I narrow down to 10-12 predictors, what all modeling techniques can I use to makes rules.</p>

<p>For example, I want something like: when x1 in range (a, b), x2 in range (c, d), ... and so on then the probability of $y(dv) &gt; 0.5$ or the event occurs i.e., $y = 1$ so that the range of values of the predictors can act as rules for determining when the event occurs.</p></li>
</ol>
"
"0.052999894000318","0.0536828127095019","127385","<p>I'm running a logistic regression in which I'm predicted a binary response from a continuous predictor... I'm interested in determining the exact point in which the predicted probability <code>(exponentiated y-hat / (1 + exponentiated y-hat)</code> becomes significantly different than the average y-hat (or average predicted probability of the discrete event...</p>

<p>I was thinking of some sort of Johnson Neyman technique, but am unsure how to begin this exercise...</p>

<p>Would like to do this in <code>R</code> at the end of the day, but not required for an answer.</p>
"
"0.140224539037626","0.1420313721078","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.219744975978132","0.234291002628028","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.0749531688995861","0.0759189618001123","133720","<p><strong>Is it possible to compare probabilities of 2 logistic different models</strong>? For example if I have one model that returns the probability that someone answer a phone call on Mondays, and then I have another model for Tuesday, and another for Wednesday and so on...
Then for the same input I run the first model and I get that the probability for that person of being contacted is .8 while for the model of Tuesdays is .6 and the for the rest of the days is also less than 0.8. 
Would it be ok to compare those, and say that is more probable to contact this person on Tuesdays or those probabilities are not comparable?</p>

<p>I think that they are not because those models might have for example different error rates.
If this is the case, how would you do a model that gives you do the best time to contact someone?
I would really appreciate some light in this subject. thanks</p>
"
"0.052999894000318","0.0536828127095019","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"0.219744975978132","0.222576452496627","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.158999682000954","0.161048438128506","141379","<p>I'm working with a dataset that has multiple categorical classes that cannot be ranked into any relative or priority order.  To approach this, I'm looking at glmnet's multinomial option. </p>

<p>As I understand it (and please correct me if I'm wrong), for this multinomial model, one class is chosen as the ""pivot"" class, and all other classes are modeled relative to that class.  For example, if I have 4 classes, one class is chosen as the pivot, 3 different logistic regression models are made, one for each of the other 3 classes relative to the pivot.  Probabilities are then scaled relatively in order ensure class probabilities sum to one.</p>

<p>The main question that I have is, which class is chosen as the pivot class?</p>

<p>The next question is why this ""pivot"" process is done rather than a 1-versus-all approach? i.e., 4 models, class 1 versus all others, class 2 versus all others, etc.</p>

<p>Further, in reading this page (<a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression"" rel=""nofollow"">http://en.wikipedia.org/wiki/Multinomial_logistic_regression</a>) under the section ""As a set of independent binary regressions"" it discusses the scaling of probabilities to ensure the sum of all probabilities is 1.  But when I use their formulas on the data below, it seems to give an odd result.</p>

<pre><code>Model Prob      Scaled prob
0.72            0.327272727
0.33            0.15
0.15            0.068181818
0.61            0.454545455
</code></pre>

<p>In this case, I chose the last class as the pivot and used it for the scaling formulas.  Notice that the scaled probability for this class is higher than for the first class, despite having a higher Model Probability.</p>

<p>If I use the simpler formula of Scaled Prob = (Model Prob / sum(all Model Probs)), the answer is more intuitive, but perhaps wrong.  Any guidance/explanation there is appreciated.</p>

<p>Finally, any suggestions of other multiclass classification tools is appreciated.</p>
"
"0.118511365784994","0.120038418441836","147060","<p>I am studying an experiment of the kind: 
Let $n_{ij}$ be the number of fetuses, $X_{ij}$ the number of responses i.e. the number of fetuses with a malformation in the jth litter of the ith dose level for j=1,...,25 and i=1,...,5 . 
Then, $p_{ij}$ is the probability of response of in the jth litter of the ith dose level and hence we have:
$$
P(X_{ij} = x_{ij}|p_{ij}) \sim Bin(n_{ij},p_{ij})
$$
But the probability of response $p_{ij}$ follow a beta distribution hence
$$
P(p_{ij})=B^{-1}(\alpha_i , \beta_i )p_{ij}^{x_{ij}}(1-p_{ij})^{n_{ij}-x_{ij}}
$$
and hence, at the end, $X_{ij}$ follow a beta-binomial distribution. </p>

<p>My problem is that I have to generate the number of responses $X_{ij}$ but I'm having some troubles. </p>

<p>The data that I have are all the $n_{ij}$ and $p_1, p_2, p_3, p_4, p_5$ (and this probabilities follow a logistic model) i.e. the probability response for each dose group, hence I don't have $p_ij$. </p>

<p>What should I do? I think that I should first generate $p_{ij}$ using the fact that they follow a beta distribution. But in which way should I do? How to estimate the parameter $\alpha_i$ and $\beta_i$? </p>

<p>Maybe someone has some ideas.. 
Thank you in advance!</p>
"
"0.183597018408631","0.17046582502778","148699","<p>For a current piece of work Iâ€™m trying to model the probability of tree death for beech trees in a woodland in the UK. I have records of whether trees were alive or dead for 3 different census periods along with data on their diameter and growth rate. Each tree has an ID number so it can be identified at each time interval. However, the census intervals vary so that for the time between one survey and another is either 4, 12 or 18 years. Obviously the longer the census period the greater the probability a tree will have died by the time it is next surveyed. <strong>I had problems making a realistic reproducible example so you can find the <a href=""https://github.com/PhilAMartin/Denny_mortality/blob/master/Data/Stack_dead.csv"" rel=""nofollow"">data here</a>.</strong></p>

<p>The variables in the dataset are:</p>

<ol>
<li>ID - Unique ID for tree</li>
<li>Block - the ID for the 20x20m plot in which the tree was located</li>
<li>Dead - Status of tree, either dead (1) or alive (0)</li>
<li>GR - Annual growth rate from previous survey</li>
<li>DBH - diameter of tree at breast height</li>
<li>SL - Length of time between censuses in years</li>
</ol>

<p>Once a tree is recorded as dead it disappears from subsequent surveys.</p>

<p>Ideally I would like to be able to estimate the annual probability of mortality of a tree using information on diameter and growth rate. Having searched around for quite a while I have seen that logistic exposure models appear able to account for differences in census periods by using an altered version of logit link for binomial models as detailed by Ben Bolker <a href=""https://rpubs.com/bbolker/logregexp"" rel=""nofollow"">here</a>. This was originally used by Shaffer to determine the daily probability of bird nest survival where the age (and therefore exposure) of the nest differed. I've not seen it used outside of the context of models of nest survival but it seems like I should be able to use it to model survival/mortality where the exposure differs.</p>

<pre><code>require(MASS)
logexp &lt;- function(exposure = 1)
{
  linkfun &lt;- function(mu) qlogis(mu^(1/exposure))
  ## FIXME: is there some trick we can play here to allow
  ##   evaluation in the context of the 'data' argument?
  linkinv &lt;- function(eta)  plogis(eta)^exposure
  logit_mu_eta &lt;- function(eta) {
    ifelse(abs(eta)&gt;30,.Machine$double.eps,
           exp(eta)/(1+exp(eta))^2)
    ## OR .Call(stats:::C_logit_mu_eta, eta, PACKAGE = ""stats"")
  }
  mu.eta &lt;- function(eta) {       
    exposure * plogis(eta)^(exposure-1) *
      logit_mu_eta(eta)
  }
  valideta &lt;- function(eta) TRUE
  link &lt;- paste(""logexp("", deparse(substitute(exposure)), "")"",
                sep="""")
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, 
                 name = link),
            class = ""link-glm"")
}
</code></pre>

<p>At the moment my model looks like this, but I will incorporate more variables as I go along:</p>

<pre><code>require(lme4)
Dead&lt;-read.csv(""Stack_dead.csv"",)


M1&lt;-glmer(Dead~DBH+(1|ID),data=Dead,family=binomial(logexp(Dead$SL))) 
#I use (1|ID) here to account for the repeated measurements of the same individuals
    summary(M1)

plot(Dead$DBH,plogis(predict(M1,re.form=NA)))
</code></pre>

<p><strong>Primarily I want to know</strong>:</p>

<ol>
<li><strong>Does the statistical technique I am using to control for the difference in time between census seem sensible? If it isn't, can you think of a better way to deal with the problem?</strong></li>
<li><strong>If the answer to the first question is yes, is using the inverse logit (plogis) the correct way to get predictions expressed as probabilities?</strong></li>
</ol>

<p>Thanks in advance for any help!</p>
"
"0.052999894000318","0.0536828127095019","151600","<p>Has anyone written a package for R that can do a logistic regression over categorical variables (like <code>glm</code>) but with the constraint, and I do realize this is weird, that <em>all the residuals must be nonnegative?</em>  (In response space, not link space.  In other words, the predicted probability in each cell must come out less than or equal to the observed probability in that cell.) Alternatively, is there a straightforward way to transform a <code>glm</code> problem so that it will come out with nonnegative residuals?</p>

<p>I know I can probably persuade <code>optim</code> to do what I want but if a shortcut exists that sure would be nice.</p>
"
"0.175780762327663","0.17804574744834","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.052999894000318","0.0536828127095019","152091","<p>I am fairly new with logistic regression. I have a binary response. And did this plot. The binary response is:</p>

<p>Y = 0: The student fails</p>

<p>Y = 1: The student succeed</p>

<pre><code>library(ggplot2)
ggplot(data = both, aes(x = age, y = succeed)) + 
  stat_smooth(method = 'glm', family = 'binomial') +
  theme_bw()+xlab(""X"")+ylab(""The student succeed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xM9wS.png"" alt=""enter image description here""></p>

<p><strong>What is the y-axis? estimated probability that the student succeed? Or estimated log odds of a student succeed?
Feeling a little confused. Could someone explain what they are I have on the y-axis?</strong></p>
"
"0.052999894000318","0.0536828127095019","154448","<p>The logistic regression model is:</p>

<p>$$\log\bigg(\frac{p}{1-p}\bigg) = \ldots$$</p>

<p>The most interesting case (for me) is the case that we have $p=1$ and $p=0$. But in this case, the ratio $p/(1-p)$ doesn't exist</p>

<p>For example: In my model, $p$ is the probability that the customer will come back after the first purchase. We observed that 100% of the clients with the income $&gt; 5000$ euros comes back after the first purchase ($p=1$), and 100% of the clients with the income $&lt; 1000$ don't ($p=0$). </p>

<p>When I treat the income as a continuous explanatory variable, there is no problem (income is a significant variable). But it isn't significant when I segment income into intervals $(0,1000)$, $(1000,3000)$, $(3000,5000)$, $(&gt;5000)$. All the categories become non-significant. I think it's because of the $p=1$ that makes $1-p=0$, then the ratio $p/(1-p)$ is degenerate. What should I do in this case?</p>
"
"0.131168045574276","0.132858183150197","155668","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has: 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>doing this below doesn't specify the individual age group, or can this be worked out from the summary stats? </p>

<pre><code>Model&lt;-glm(Maturity~Lgnth+age, family=binomial(logit)) 
</code></pre>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this. Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.120192462032251","0.1217411760924","155762","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has:
 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this.
Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.0611990061362105","0.0619875727373744","156766","<p>I have been having real issues trying to calculate the length at which certain probabilities of maturing are reached. This is NOT the same as the proportion of individuals that are mature as <code>dose.p</code> would calculate.</p>

<p>I know I have to run a logistic regression with binomial errors, something similar to the below code</p>

<pre><code>mylogit &lt;- glm(Maturity ~ Lngth, data = data, family = ""binomial"")
</code></pre>

<p>But from this how can I work out the length at which the probability of maturing equals 50%.
My data frame consists of Maturity (0,1) data and length data. (I have other data but believe this is all I need at this stage (I could be wrong!)
Any help would be greatly appreciated! </p>
"
"0.052999894000318","0.0536828127095019","156804","<p>I have estimated a mixed-effects logistic regression with glmer
and want to draw a bootstrapped confidence-region for the mean predicted probability for two subgroups of the sample.</p>

<p>I have a $1000 \times 2$ Matrix $X$ containing the bootstrapped mean predicted probabilities for the two groups.
One could now compute the empirical covariance matrix $S$ and draw a
circle around the means using the metric induced by $S^{-1}$,
i.e. drawing a confidence ellipsoid based on normality-assumption.</p>

<p>Are there any widely used alternatives to this approach that do not imply distributional assumptions? </p>

<p>skeletor</p>
"
"0.0611990061362105","0.0619875727373744","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.191093835412303","0.163778267091356","162426","<p>I am trying to create a logistic regression model and a random forest model on the same data to predict probability of default. For the logistic regression model, I have created some dummy variables from categorical variables. Finally, for the input of logistic regression, I have 9 dummy variables and 2 numeric variables (age and level, age takes values from 18 to 60, level from 4 to 10). I want to use same input dataset for the random forest model. When I did so, using ""randomForest"" Package, I get following Variable Importance Plot.</p>

<p><img src=""http://i.stack.imgur.com/qscyb.png"" alt=""enter image description here""></p>

<p>Level seems to be a very good variable both by MSE and Node Purity. Also, level is a very important variable in logistic regression (p value ~ 10^-5). 
However, Age is very important by Node purity, but not by MSE. Also, in logistic regression, age is not a very good variable with p value of 0.026. So I want to understand, Does being numeric increases the node purity importance of a variable by overfitting? Is it not suitable to use numeric and dummy variables together in random forest model? Or is there something I am missing.</p>

<p>I had similar doubts about using numeric and dummy variables in logistic regression, but in logistic regression it did not create any problem. Please help.</p>
"
"0.052999894000318","0.0536828127095019","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.158999682000954","0.161048438128506","164120","<p>I am attempting to conduct a logistic regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. My response variable (service points) is binary in the sense that it can have only two outcomes for each observation - a success (service point win) or a failure (service point loss). </p>

<p>I have an issue with my data: For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>250 matches, each with about 70 dependent variable observations (service points). So for each match I currently have the two variables: Total_Service_Points_Played <strong>and</strong> Total_Service_Points_Won. </p>

<p>Eg - Match 1: Total_Service_Points_Played: 70 ; Total_Service_Points_Won: 47</p>

<p>So my data isn't in 1's and 0's. Is there a way I can implement a logistic regression with my dependent variable observations in their current form? Is there any simple transformation that comes to mind?</p>

<p>What springs to mind for me is to flesh out my match data into 1's and 0's. So following on from Match 1 above I would have: 47 1's followed by 26 0's . My data doesn't provide information as to what sequence these 1's and 0's arrived in, but since the depdendent variable observations are i.i.d this won't cause an issue? Correct me if I'm wrong please. Another issue posed by this technique would be the massive increase in my data - from 250 observations as a ratio (service point wins/service points played) to 250*70=17500 observations or more.</p>

<p>As a side note, the last thing I'm wondering is about the dispersion of my dependent variable data. Specifically, in the ratio of serve wins to total serve points as above, there exists no values &lt; 0.2 or 20% .... In addition, there exists no value > 0.9 ..... Does this fit the bill for the (link=logit) argument? I know this relates to an S shape curve which is undefined at 0 and 1, but approaches both values.... I might be going off track here but is this something to be concerned about? </p>
"
"0.105999788000636","0.107365625419004","164648","<p>I have created a Logistic Regression using the following code:</p>

<pre><code>full.model.f = lm(Ft_45 ~ ., LOG_D)
base.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg)
step(base.model.f, scope=list(upper=full.model.f, lower=~1),
     direction=""forward"", trace=FALSE)
</code></pre>

<p>I have then used the output to create a final model:</p>

<pre><code>final.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg + IP_util_E2_m02_flg + 
                           AE_NumVisit1_flg + OP_NumVisit1_m01_flg + IP_TotLoS_m02 + 
                           Ft1_45 + IP_util_E1_m05_flg + IP_TotPrNonElecLoS_m02 + 
                           IP_util_E2pl_m03_flg + LTC_coding + OP_NumVisit0105_m03_flg +
                           OP_NumVisit11pl_m03_flg + AE_ArrAmb_m02_flg)
</code></pre>

<p>Then I have predicted the outcomes for a different set of data using the predict function:</p>

<pre><code>log.pred.f.v &lt;- predict(final.model.f, newdata=LOG_V)
</code></pre>

<p>I have been able to use establish a pleasing ROC curve and created a table to establish the sensitivity and specificity which gives me responses I would expect. </p>

<p>However What I am trying to do is establish for each row of data what the probability is of Ft_45 being 1. If I look at the output of log.pred.f.v I get, for example,:</p>

<pre><code>1 -0.171739593    
2 -0.049905948    
3 0.141146419    
4 0.11615669    
5 0.07342591    
6 0.093054334    
7 0.957164383    
8 0.098415639    
.
.
.
104 0.196368229    
105 1.045208447    
106 1.05499112
</code></pre>

<p>As I only have a tentative grasp on what I am doing I am struggling to understand how to interpret the negative and higher that 1 values as I would expect a probability to be between 0 and 1.</p>

<p>So my question is am I just missing a step where I need to transform the output or have I gone completely wrong.
Thank you in advance for any help you are able to offer.</p>
"
"0.105999788000636","0.107365625419004","164912","<p>I am modelling invertebrate.biomass ~ habitat.type * calendar.day + habitat.type * calendar.day ^ 2, with a random intercept of transect.id (50 transects were repeated 5 times)</p>

<p>My response is zero-heavy - about 25% are 0s - and the non-zeroes are strongly right-skewed. </p>

<p>I understand a possible way of dealing with this is to construct 2 models - one modelling a binary response in a logistic regression and the other modelling the non-zero response in a (e.g.) Gamma regression. I'm working in R and following the ideas in <a href=""http://seananderson.ca/2014/05/18/gamma-hurdle.html"" rel=""nofollow"">this post</a>.</p>

<p>I want to check the method of combining the results of these 2 models, in order to generate quantitative predictions (ultimately with CI). Am I correct in multiplying the predicted probabilities from the logistic regression with the predicted (non-zero) biomass from the Gamma regression? Thus, the predicted (non-zero) biomass gets down-weighted according to the probability of there actually being an invertebrate present at all. This makes sense in my head, but feels too easy to be true. </p>

<p>See plots below which demonstrate my method in it's current form.
<a href=""http://i.stack.imgur.com/MVmJc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MVmJc.png"" alt=""Gamma hurdle model""></a></p>

<p>Assuming I'm right so far, how would I then go about generating a SE / CI for the predictions combining two models? </p>
"
"0.158999682000954","0.161048438128506","165214","<p>I am attempting to conduct a dynamic or time series regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. 
For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>Numerous matches, for each I have:</p>

<ul>
<li>No. of service points played </li>
<li>No. of service points Nadal won </li>
<li>(Thus) Nadal's Service point win %</li>
<li>The court surface the match was played on (independent variable)</li>
<li><p>Nadal, and his opponent's world ranking points at time of match </p>

<p>I will be using a model like this:</p></li>
</ul>

<p><em>Nadal serve win % = surface + (Nadal rank points - opponent rank points)</em> </p>

<p>However I would like to include an independent variable that accounts for ""form"" or ""hot hands"" in tennis. So I want to include an independent variable : </p>

<ul>
<li>Xi = Avg. Serve % of Last 5 matches </li>
</ul>

<p>i.e. a moving average of sorts </p>

<p>Is this a good idea? Does anyone have any suggestions how this could be implemented in R specifically? </p>

<p>Lastly, since my dependent variable data is binary, and can be used in a logistic regression in R.... Can I run a dynamic logistic regression/logistic time series for the model I discuss above? </p>

<p>Any advice on how I can account for this form/trend would be massively appreciated</p>
"
"0.149906337799172","0.132858183150197","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.052999894000318","0.0536828127095019","167794","<p>I wrote a script that create a logistic model, for Email opening probability, for each user name.</p>

<pre><code>form&lt;-formula(OpenOrNor~as.factor(TimeSend)
              +OpenWithSmartphoneind)
models&lt;- dlply(Data, ""User_id"", 
               function(df) {
                 model&lt;-glm(formula = form,family = binomial(""logit""),data = df,control = glm.control(epsilon = 1e-9, maxit = 500))
                 return(model)})
</code></pre>

<p>for some users it return me </p>

<pre><code>Call:  glm(formula = form, family = binomial(""logit""), data = df, weights = HistoryWeights, 
    control = glm.control(epsilon = 0.000000001, maxit = 500))

Coefficients:
              (Intercept)            as.factor(TimeSend)2      as.factor(TimeSend)3   as.factor(TimeSend)4  
                -22.61106                   20.21853                    0.07738                   20.54737  
          as.factor(TimeSend)5       as.factor(TimeSend)6      as.factor(TimeSend)7   as.factor(TimeSend)9  
                  0.19292                   -0.03624                    0.22013                    0.11837  
          OpenWithSmartphoneind  
                       NA  

Degrees of Freedom: 83 Total (i.e. Null);  76 Residual
Null Deviance:      190.6 
Residual Deviance: 166.8    AIC: 182.8
</code></pre>

<p>We can see that for OpenWithSmartphoneind their  is NA. and this is because their are no Opening With Smartphone at all In this user history.</p>

<p>My question is how it will impact on predict?<br>
And doe's it make different if OpenWithSmartphoneind in the formula will be a factor type or not?</p>
"
"0.218524161109851","0.208319912547319","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.0749531688995861","0.0759189618001123","172889","<p>Recently I'm using R survival package to try to predict the probability of people going to churn. I found some <a href=""http://stackoverflow.com/questions/27408862/how-to-predict-survival-probabilities-in-r"">examples</a> on stack overflow and also tried that to my own data. Here is my prediction model and output:</p>

<pre><code>&gt; Status_by_Time &lt;- Surv(time = Duration, event = Gift_Status_ind)
&gt; model.fit2 &lt;- survreg(Status_by_Time ~ Age
                + Gender_ind 
                + Fundraiser_ind
                + Monthly.Recurring.Amount
                + Frequency_ind
                + Monthly.first.gift.amount
                + Monthly.last.gift.amount
                #+ Duration
                #+ Saved.
                + Upgrade.first.time
                + Upgrade.second.time,
                dist = ""logistic""
)

&gt; summary(model.fit2)
                            Value Std. Error      z         p
(Intercept)                81.525    1.46725  55.56  0.00e+00
Age                         0.156    0.01889   8.27  1.33e-16
Gender_ind2                 2.278    0.55955   4.07  4.68e-05
Gender_ind3                -9.514    1.09689  -8.67  4.18e-18
Fundraiser_ind2            -8.798    0.69303 -12.70  6.25e-37
Fundraiser_ind3             4.028    0.90970   4.43  9.52e-06
Monthly.Recurring.Amount   -1.211    0.04856 -24.95 2.39e-137
Frequency_ind2            257.319    0.00000    Inf  0.00e+00
Frequency_ind3              8.562    2.71423   3.15  1.61e-03
Frequency_ind4            -89.067    1.39379 -63.90  0.00e+00
Monthly.first.gift.amount  -2.538    0.03721 -68.22  0.00e+00
Monthly.last.gift.amount    1.827    0.04981  36.67 2.38e-294
Upgrade.first.time          6.467    0.82381   7.85  4.15e-15
Upgrade.second.time        10.849    2.72927   3.98  7.04e-05
Log(scale)                  2.869    0.00841 341.02  0.00e+00

Scale= 17.6 

Logistic distribution
Loglik(model)= -51841.8   Loglik(intercept only)= -55404
Chisq= 7124.45 on 13 degrees of freedom, p= 0 
Number of Newton-Raphson Iterations: 8 
n= 18097 

&gt; predicted.values &lt;- predict(model.fit2, newdata = churn.df.trim, type = ""quantile"", p = (1:9)/10) # 10 times event???
&gt; head(predicted.values)
            [,1]      [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]
 [1,]   2.219425 16.513993 26.01508 33.80343 40.95072 48.09800 55.88635 65.38744 79.68201
 [2,]  11.088257 25.382825 34.88392 42.67227 49.81955 56.96683 64.75518 74.25627 88.55084
 [3,] -11.996590  2.297977 11.79907 19.58742 26.73470 33.88198 41.67033 51.17143 65.46599
 [4,]   5.456971 19.751539 29.25263 37.04098 44.18826 51.33555 59.12390 68.62499 82.91955
 [5,]  19.690749 33.985316 43.48641 51.27476 58.42204 65.56932 73.35767 82.85876 97.15333
 [6,]  -8.187469  6.107099 15.60819 23.39654 30.54382 37.69111 45.47946 54.98055 69.27511
</code></pre>

<p>I reckon all these numbers are not probabilities. Is there some way to interpret these numbers or turn them into probabilities? Also if I use <code>p = (1:9)/10</code> does that mean I'm calculating the probability for the next 9 or 10 period?</p>

<p>Much appreciate if someone could give me a straight forward explanation (none academic one). </p>
"
"0.0749531688995861","0.0759189618001123","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.129822696722375","0.109579582579729","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.167600380788498","0.169759959366261","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0749531688995861","0.0759189618001123","175956","<p>For a (fictional) <strong>multiple logistic regression</strong>, let's consinder a DV 'hired' (0,1) and <strong>three dichotomous IVs</strong> 'college_degree' (0,1), 'affluent' (0,1) and 'recommendated' (0,1) for <em>N</em> = 1,000 participants.</p>

<p>Running a logistic regression and generating predicted probabilities of being hired using the <code>predict</code> function for a <code>glm</code> object works well. For every respondent I have a probability value ranging continuously from 0 to 1.</p>

<p>Since I do have a base distribution of all three IVs and the DV, I want a kind of simulator that predicts the <strong>percentage/proportion</strong> of the DV using each indivduals predicted probability.</p>

<p>Let's say in the sample 20% are hired, 50% have a college degree, 10% are affluent and 35% are recommendated. I want to use the predicted values to see how much would the <strong>proportion of 'hired' goes</strong> up, when I, e.g., <strong>change the proportion of recommendations to 50%</strong>. I guess, I could also use the equation with the coefficients of the logit model, but would need to run it for every individual.</p>

<p>Is there any way to implement this in R (well or Excel, if that is easier)?</p>
"
"0.183597018408631","0.185962718212123","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.0917985092043157","0.0929813591060615","181463","<p>I made a logistic regression in R statistics, but I don't know how to interpret it with 2 categorical variables (the examples I found on the internet and / or stackoverflow were just with one and I have difficulties to imagine it with two). </p>

<p>So imagine I want to see which factors infuence the fact of having a special desease (1: yes, 0: no) and I have:</p>

<pre><code>City: Manhattan, New York
hospital: St. Mary, Avante, Copperfield
bloodshugar: 1, 28, 7 ... , 66 (numeric)
timetoreact: 113, 423, 334, ... (numeric),
</code></pre>

<p>I give it all in a glm-model glm (desease dependent on: <code>City</code>, <code>hospital</code>, <code>City:hospital</code>, ...)</p>

<p>In the output I have the problem that it's all comprised with the factor level of the first letter of the alphabet, so i.e. ""Manhattan"" and ""Avante"" doesn't appear anymore. </p>

<p>There is just written: </p>

<pre><code>NewYork:Bloodshugar: Coeff.: 0.034 
</code></pre>

<p>and I don't know now what it is... Manhattan:Bloodshugar doesn't appear. Is it the difference of the incline from the probability on bloodshugar in Newyork in comparison to Manhattan? Where can I see if the probability to get the desease sinks or inclines with more bloodshugar in New York? When there's written bloodshugar: Coef.: 0.021, is it the bloodshugar ""mean"" of Manhattan and New York or is it just from Manhattan?  </p>

<p>What is the intercept now? Is it the probability to show the desease when cured in the Avantehospital and raised in Manhattan (because it's always the first letter)?</p>

<p>I hope I explained it well, I still can add some more explanations if you'd like to. </p>
"
"0.158999682000954","0.161048438128506","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.052999894000318","0.0536828127095019","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.105999788000636","0.107365625419004","185495","<p>I am developing some stochastic simulations in which I have four explanatory variables that I named <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. They are all continuous data. I have several responses. One of these responses is a categorical value that can take 5 categories that I just labelled 1 to 5. I called this response an outcome.</p>

<p>I am having lots of trouble figuring out an analysis that I could use to understand the influence of my explanatory variables (all continuous data) on my response (categorical data). For example, I am interested in understanding how the outcome is influenced by <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. </p>

<p>I thought about a multinomial logistic regression, but I am not quite sure whether I can actually apply it to my data.</p>

<hr>

<p><em>Edit</em>: I have followed <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0ahUKEwj30O-9u8zJAhXIqg4KHWlwDjQQFggsMAI&amp;url=https%3A%2F%2Fstatsthewayilikeit.files.wordpress.com%2F2015%2F05%2Fmultinomial-logistic-regression.docx&amp;usg=AFQjCNHj4IdMOAdjfavqcd-Q8HG66vYoag&amp;sig2=ScEoSVJGu1dDvMyIiAyljQ&amp;cad=rja"" rel=""nofollow"">this Word document</a> to do a multinomial logistic regression. An example of one of my plots after performing the regression is reproduced below (the x-axis is in log10 for the <code>Birth Rate</code>):  </p>

<p><a href=""http://i.stack.imgur.com/jjITQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jjITQ.png"" alt=""enter image description here""></a></p>

<p>My code in R:</p>

<pre><code>all.m        &lt;- multinom(code_Final_Info ~ Model+Birth_Rate+Inact_Rate+Del_Rate1+Time, 
                         data=all.mod)
PredProb     &lt;- cbind(preds, predict(all.m, newdata=preds, type='probs', se=TRUE))
PredProbMelt &lt;- melt(PredProb, value.name=""Probability"", 
                     id.vars=c(""Model"",""Birth_Rate"",""Inact_Rate"",""Del_Rate1"",""Time""))

(p &lt;- ggplot(PredProbMelt, aes(x=Del_Rate1, y=Probability, colour=Time)) + 
                           geom_line() +
                           facet_grid(variable ~., scales=""free"") + 
                           theme_bw()) 
</code></pre>

<p>My data.frame looks like:  </p>

<pre><code>&gt; str(all.mod)
'data.frame':   900000 obs. of  6 variables:
 $ Model          : num  0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 ...
 $ Birth_Rate     : num  -4.05 -4.05 -4.05 -4.05 -4.05 ...
 $ Inact_Rate     : num  -3.14 -3.14 -3.14 -3.14 -3.14 ...
 $ Del_Rate1      : num  -4.26 -4.26 -4.26 -4.26 -4.26 ...
 $ code_Final_Info: Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Time           : Factor w/ 3 levels ""0T250"",""1T500"",..: 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>
"
"0.0917985092043157","0.0929813591060615","187658","<p>I ran a logistic regression in R using driving data from about 10,000 people. The model included age, years of driving experience, as well as 4 driving test results. The dependent variable was whether or not they had been involved in a crash recently (yes or no, a categorical variable). </p>

<p>The coefficients of the model are given below:</p>

<pre><code>                    Estimate     Std.Err       z value     Pr(&gt;|z|)    
(Intercept)        -1.450041     0.207144      -7.000      2.56e-12 ***
riding experience  -0.014115     0.003697      -3.818      0.000134 ***
age                -0.034544     0.003608      -9.575       &lt; 2e-16 ***
test 1              0.261485     0.088645       2.950      0.003180 ** 
test 2              0.090102     0.051328       1.755      0.079184 .  
test 3              0.228918     0.073666       3.108      0.001887 ** 
test 4              0.070106     0.063652       1.101      0.270729    
</code></pre>

<p>Firstly, with 10,000 people am I right in thinking that p-values aren't going to be that useful?</p>

<p>I calculated the probabilities of being involved in a crash with a 1 unit increase in each variable by doing <code>exp(variable)</code> to get the odds and then, <code>probability = odds/(1+odds)</code>. It gave me:</p>

<pre><code>(Intercept)        0.1899952          
ridingexp          0.4964712
age                0.4913648
test 1             0.5650012
test 2             0.5225104
test 3             0.5569810   
test 4             0.5175193
</code></pre>

<p>These seem awfully high! It is like saying that an increase in age of 1 year makes you 49% less likely to be involved in a crash? Surely that can't be right.</p>
"
"NaN","NaN","188399","<p>I would like to train a model that has a probability (a success rate between 0 and 1) as outcome.</p>

<p>So the data looks like this:</p>

<pre><code>feature1  feature2   success_rate
0.1       0.3        0.55
0.3       0.6        0.45
</code></pre>

<p>I started using <em>xgboost</em> (gradient boosting machine) with:</p>

<pre><code>""objective"" = ""reg:logistic""
""eval_metric"" = ""auc""
</code></pre>

<p>which means I doing a logistic regression using the Area Under the Curve (AUC) as evaluation function to measure the improvement of the model.</p>

<p>But I understand a logistic regression is usually trained with a categorical target (success or failure), not a probability.
Does this matter? and is this the right approach?</p>
"
"0.105999788000636","0.107365625419004","189627","<p>I am using the <code>survey</code> package and my model is:  </p>

<pre><code>modelsvy &lt;- svydesign(id =~ 1, data=temp12, weights=~WGT) 
model12s &lt;- svyglm(DEPVAR ~ var1 + var2 +... ,  modelsvy, family= quasibinomial)  
</code></pre>

<p>This has been going well for me. </p>

<p>I have a sample of the US population (N=4,343), with fractional weights (<code>WGT</code>) on each observation. The completed model is then scored as a probability: </p>

<pre><code>mdlg2012$DEPVARSCR &lt;- predict(model12s,mdlg2012)  
mdlg2012$DEPVARSCRP &lt;- (1 / (1 + exp(-1 * mdlg2012$DEPVARSCR)))  
</code></pre>

<p>The probabilities are applied to census block-group data to estimate the number of households interested in buying things like Life Insurance.  </p>

<p>In a variation, I built a couple models that did not use all N. For example, I subset to N=2,339, with DEPVAR = 1 (n=178) and DEPVAR = 0 (n=2161). The entire population was not included. The proportion of N was 178 / 2,339 = 0.076. For N overall, it would have been 178 / 4343 = 0.041 (stated on an unweighted basis, I do need to weight them).  </p>

<p>My question is, would you happen to know how I might calibrate the resulting new model probabilities back to the full US population? I down sampled, removing n = 2,004, which inflated the DEPVAR incidence. The logistic transform in this case overstates the probabilities. Is the calibrate(design,...) command useful here?</p>
"
"0.344784576938096","0.334043431920494","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.175780762327663","0.17804574744834","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0749531688995861","0.0759189618001123","199504","<p>I would like to compare an average probability to a fixed probability value in order to determine if there is a significant difference between the two.</p>

<p>My participants had to detect and point a target that appeared among three distractors. The possibility they answered hazardously therefore is 1 out of 4 (i.e., .25). My dependent variable being binary (Correct answer: 1 ; Incorrect answer: 0) I am using logistic regression for my analyses:</p>

<pre><code>model1 &lt;- glm(Accuracy ~ Task * Masking,
              data = DF,
              family = binomial(link = ""logit""))
</code></pre>

<p>âŸ¶ <a href=""http://i.stack.imgur.com/AyPzG.png"" rel=""nofollow"">Plot of results</a></p>

<p>More precisely, I would like to know if the probability of report in the Reach-Masked condition is significantly different from 0.25.</p>

<p>How do I test this possibility in R?</p>

<p>Thank you for your time. Have a very nice day.</p>
"
"0.131168045574276","0.132858183150197","199978","<p>I've been building a logistic regression model (using the ""glm"" method in caret). The training dataset is extremely imbalanced (99% of the observations in the majority class), so I've been trying to optimize the probability threshold during the resampling process using the train function from the caret package as described in this example of a svm model: <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">Illustrative Example 5: Optimizing probability thresholds for class imbalances.</a></p>

<p>The idea is to get the classification parameters for different values of the probability thershold, like this:</p>

<pre><code>threshold   ROC    Sens   Spec   Dist   ROC SD  Sens SD  Spec SD  Dist SD
 0.0100     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.0616     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1132     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1647     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
  ...        ...                  ...                      ...      ...
</code></pre>

<p>I noticed that the 'glm' method in caret uses 0.5 as the probability cutoff value as can be seen in the predict function of the model:</p>

<pre><code>code_glm &lt;- getModelInfo(""glm"", regex = FALSE)[[1]]
code_glm$predict
    function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata)
                    if(modelFit$problemType == ""Classification"") {
                      probs &lt;-  predict(modelFit, newdata, type = ""response"")
                      out &lt;- ifelse(probs &lt; .5,
                                    modelFit$obsLevel[1],
                                    modelFit$obsLevel[2])
                } else {
                  out &lt;- predict(modelFit, newdata, type = ""response"")
                }
                out
              }
</code></pre>

<p>Any ideas about how to pass a grid of probability cutoff values to the predict function shown above to get the optime cutoff value?</p>

<p>I've been trying to adapt the code from the <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">example shown in the caret website</a>, but I haven't been able to make it work. I think I'm finding difficult to understand how caret uses the model's interfaces... </p>

<p>Any help to make this work would be much appreciated... Thanks in advance.</p>
"
"0.0917985092043157","0.0929813591060615","201015","<p>I am currently working on a project where I need to predict a outcome which on average occurs less number of times (for example, let's say the outcome is that a batter reaches the base in a baseball game. Now on average this event occurs with a probability of roughly 0.25-0.39 and seldom ever goes higher than that or lower than that)</p>

<p>Now the questions that I have for this situation:</p>

<ol>
<li><p>Working towards a logistic regression model to predict this, the output of that model on a test set is some kind of a probability which would roughly be in the same range (0.25-0.39). The question is how to come up with a decision threshold probability for such a low probability such that I can predict the class. Is there a specific algorithm, steps, or method that would be give me this?</p></li>
<li><p>Is having a different decision threshold for each observation a possibility?</p></li>
<li><p>Any other algorithm I can use to tackle this problem?</p></li>
</ol>

<p>Apologies if the question is vague or not well formed. Please let me know if I need to provide any additional information. Also, I am using R.</p>
"
"0.118511365784994","0.120038418441836","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0","0.0536828127095019","202211","<p>I have a logistic regression in R whose goal is to predict the probability of default on some test data. </p>

<p><code>glm(default ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>What I'd like to do is 'bin' this data so that bins 1 to n each have a certain rate of default. How can I bin the logistic regression results in this way? For example, the bins on a sample set of 1000 might look like:</p>

<pre><code>Bin# P(Default) Count
1         4%     400
2         2%     300
3         1%     300
</code></pre>

<p>That is, I set in advance the probabilities I want each bin to have (.04,.02,.01) and then bins are created based around those settings.</p>
"
"0.149906337799172","0.151837923600225","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.0917985092043157","0.0929813591060615","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.052999894000318","0.0536828127095019","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.118511365784994","0.120038418441836","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.105999788000636","0.107365625419004","211174","<p>We are oversampling the data to use in logistic regression. Aim  is to predict CTR(click probability) which is rare event scenario.
I have predicted the probabilities of click but CTR results are inflated as we over sampled positive class.</p>

<p>model2&lt;-SMOTE(V61 ~ ., z2, perc.over = 600,perc.under=100, learner = 'glm',family=binomial())</p>

<p>Is there any way to undo oversampling results so that I can get exact probabilities ? Based on research so far, one easiest way to divide the output probability by the multiplier we used in over sampling. I dont feel it would be the exact way as I have used synthetic minority over sampling technique(SMOTE) in R.</p>
"
"0.0749531688995861","0.0759189618001123","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.052999894000318","0.0536828127095019","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.0917985092043157","0.0929813591060615","218276","<p>Imagine a data set with approximately 100 variables and 5000 cases. The outcome is a two-level factor. All variables are factors, most of them three levels (yes, no, or indifferent).</p>

<p>After building a simple logistic regression model I'm in doubt about how to reduce the amount of variables used in the final model (and find a proper validation method).</p>

<p>Before building the model I've used chi-squared to examine individual relations between some predictors and the outcome, but this becomes kind of stupid.</p>

<p>Any advice about how to tackle this issue, preferably in an automated way? I've some understanding of basic probability but would use Lasso or Ridge Regression more as a black box now. </p>

<p>As tools I use R and Python.
Thanks in advance!</p>
"
"0.118511365784994","0.120038418441836","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.052999894000318","0.0536828127095019","218842","<p>I'm using a random forest in R (randomForest) to predict a binary output (1,0) for a dataset that is heavily unbalanced. In this example let's assume the population has 1% 1's and 99% 0's.</p>

<p>Building the random forest on such unbalanced data is difficult and I get much better results when building it on a 50:50 sample.  When predicting a validation set, I obtain the % of trees that predicted that data point to be a 1.  For example, customer A has a 75% probability of being a 1 (based on the # of trees that predicted 1)</p>

<p>If I want to re-scale these predictions back to the original population ratio of 1:99, is there a good way to do this?</p>

<p>In the past I've used logistic regression, and I can adjust the intercept accordingly to down-scale the predicted probability.</p>

<p>Is there a good way to think about this from the RF point of view?  Can I simply just down-weight the predictions from the 50:50 sample by 50 (50% down to 1%)?</p>

<p>Thanks in advance for any thoughts and help</p>
"
"0.140224539037626","0.1420313721078","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.211999576001272","0.201310547660632","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0917985092043157","0.0929813591060615","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.140224539037626","0.1420313721078","221427","<p>I want to understand the interpretation of logistic regression coefficients in terms of an increase in probability of dependent variable being 1. </p>

<p>I tested a logistic regression model in R and got the following coefficients (all statistically significant):</p>

<pre><code>&gt; mud$coefficients
  (Intercept)          var1          var2          var3          var4
-3.557573e+00  1.051031e-01  4.937244e-07 -1.308386e-06  3.937646e-01
</code></pre>

<p>Raising these numbers to the power of e resulted in numbres below. I would interpret them so that a 1 unit increase in var1 would increase the probability of dependent variable being 1 by 11% and 1 000 000 unit decrease in var3 would increase that probability by 1%. </p>

<pre><code>&gt; exp(mud$coefficients)
(Intercept)       var1        var2        var3        var4
0.02850792  1.11082516  1.00000049  0.99999869  1.48255150
</code></pre>

<p>As suggested in a previous a question (<a href=""http://stats.stackexchange.com/a/24422/121763"">http://stats.stackexchange.com/a/24422/121763</a>), below is what I should actually do to find the probabilities.</p>

<pre><code>&gt; exp(mud$coefficients)/(1+exp(mud$coefficients))
(Intercept)       var1        var2        var3        var4 
0.02771774  0.52625162  0.50000012  0.49999967  0.59718862
</code></pre>

<p>So which numbers should I use if I'd like to express the effect of independent variables on the probability of and event occurring? E.g. in case of var1 is it 11% or 53% or something else? </p>
"
"0.183597018408631","0.185962718212123","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.0749531688995861","0.0759189618001123","224438","<p>I've created a multivariate binary logistic regression web app using shiny and R for one of my final projects of the semester. I would like a little critique on it and would love to learn more of just exactly what I'm dealing with. </p>

<p>My model attempts to predict Political Party Affiliation by using the variables of <strong>Age, Sex, Race, Degree of Education, Church Attendance.</strong></p>

<p>I used data collected from the General Social Survey (GSS) and have about 1300 observations in my final data set after everything was cleaned and garbage values taken out. </p>

<p>One problem that I can already see is the distribution of <strong>Race</strong> in my dataset, it's pretty skewed. </p>

<p><a href=""https://rtutorials-portfolio.shinyapps.io/PartyID/"" rel=""nofollow"">My web app can be found here</a></p>

<ul>
<li><p>The probabilities tab shows the expected probability of someone being republican.</p></li>
<li><p>The presentation tab shows a quick presentation on how I went about creating the logistic regression.</p></li>
<li><p>The graphics tab holds all of my graphics of the dataset.</p></li>
<li><p>The model info tab shows the <code>summary()</code> of the glm() and the results of the Hosmer-Lemeshow Test. </p></li>
</ul>

<p>I would love any critique or things you think I could do better with (web design, statistics, coding, etc.)</p>
"
"0.211999576001272","0.201310547660632","226330","<p>Overall, I'd like to be able to say that, for the logistic prediction for this row, ColA was more influential in driving up the resultant probability (ie, y_hat) than ColB. (We'll use y_hat as it's usually defined for logistic.) But is this possible? Some data scientists I've talked to say yes, but I've also seen push-back.</p>

<p>From what I've read, it seems that GLMs make it easiest to get at a per-row variable importance (see <a href=""http://stats.stackexchange.com/q/190482"">this limited discussion</a> on logit in particular, including push-back). But can they actually do it?</p>

<p>If B1 and B2 are coefficients and the cols in X represent our features, it would seem that if <em>B1</em>*X1 is greater than <em>B2</em>*X2 then <em>B1</em>*X1 would drive the resultant probability towards 1 more than <em>B2</em>*X2. Here's an example (which brings in a factor col, for a full treatment).</p>

<p>We create features X1 and X2, where X1 is random and X2 (I think we can agree) has a large positive impact on y:</p>

<pre><code>set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(1,0,1,0,1,0,1,0,1,0)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)
</code></pre>

<p>Now we create the logit model:</p>

<pre><code>fit.logit = glm(
formula = y~.,
data = df,
family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:       -1.2353      22.0041
Wald statistic:      -0.267        0.003
</code></pre>

<p>Now if we multiply <em>B1</em> and <em>B2</em> by <em>X1</em> and <em>X2</em> respectively and print the results:</p>

<pre><code>coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)

          X1       X2
1  -0.55087679 22.00411
2  -0.48751729  0.00000
3  -0.59755734 22.00411
4  -1.13510089  0.00000
5  -1.04245907 22.00411
6  -0.63908954  0.00000
7  -0.53998690 22.00411
8  -0.42395777  0.00000
9  -0.01916833 22.00411
10 -0.14575621  0.00000
</code></pre>

<p>We see that in the rows where X2 = 1 then <em>B2</em>*X2 (ie the second column) is much higher than <em>B1</em>*X1 (ie the first column). So it would seem that we could say that for those rows that X2 would be the dominant feature driving up the resultant prediction towards 1.</p>

<p>If one reverses the y dependency on X2 by replacing zeros for ones in X2, then after doing the multiplication, <em>B2</em>*X2 has a much lower value than <em>B1</em>*X1 when X2 = 1, which makes sense (since X2 now pushes y_hat towards 0 when X2 = 1). Thus, for these rows, X1 is actually more ""responsible"" for driving y_hat towards 1. (Note that if both results are negative, then the least negative would be the feature more responsible for y_hat being as high as it is.) Because of this, it would seem that this method of per-row feature ranking still works. What am I missing? </p>

<p>In case it helps, the code for the latter (reversed dependency) case is below: </p>

<pre><code># Reverse y dependency on X2
set.seed(33)
X1 &lt;- runif(10, 0.0, 1.0)
X2 &lt;- c(0,1,0,1,0,1,0,1,0,1)
y &lt;-  c(1,1,1,0,1,0,1,0,1,0)
df &lt;- data.frame(X1,X2,y)
dforig &lt;- df #Need a copy bc multiplying below doesn't work with factors
df$X2 &lt;- as.factor(df$X2)

fit.logit = glm(
  formula = y~.,
  data = df,
  family = binomial(link = ""logit""))

                         X1          X21  
Coefficients:        -1.235      -22.004
Wald statistic:      -0.267       -0.003

coefftemp &lt;- fit.logit$coefficients
coefficients &lt;- coefftemp[2:length(coefftemp)] # drop intercept
multiply_res &lt;- sweep(dforig[,1:2], 2, coefficients, `*`)
multiply_res

            X1        X2
1  -0.55087679   0.00000
2  -0.48751729 -22.00411
3  -0.59755734   0.00000
4  -1.13510089 -22.00411
5  -1.04245907   0.00000
6  -0.63908954 -22.00411
7  -0.53998690   0.00000
8  -0.42395777 -22.00411
9  -0.01916833   0.00000
10 -0.14575621 -22.00411
</code></pre>

<p>Overall, for logistic, can we accurately say (for example) that feature A drives y_hat toward 1 more than feature B, for this individual prediction? </p>

<p>Thanks, all!</p>
"
"0.16169478381041","0.178667200463298","229598","<p>This isn't a problem with correlation between predictors - I have two models, each considers only one of the variables. That is the only difference between the models.  </p>

<p>I'm estimating the probability of an diagnosis given some confounders and a measure of monthly temperature. I have two possible temperature definitions I'm considering: monthly average temperature and monthly average <em>high</em> temperature. I don't expect the response to temperature to be linear, so I broke average temperature into 5 degree bins with bottom and top coding at &lt; 40 and > 90. I did the same with average high temperature but shifted the bins slightly with bottom and top coding &lt; 50 and > 100. </p>

<p>I estimate the first logistic model </p>

<pre><code>event ~ age + sex + ... + mean_temp_group
</code></pre>

<p>and get the response I'd expect from my theorized process. However, I'd prefer to report the results using mean high temperature since average temperature is misleadingly low (average temp of 70, for instance, is pretty warm with highs in the 80s but people think ""70 degrees? That's wonderful!""). So I estimate the same model but instead replace <code>mean_temp_group</code> with <code>mean_high_group</code>:</p>

<pre><code>event ~ age + sex + ... + mean_high_group
</code></pre>

<p>and the results don't match either my theory or what I saw with <code>mean_temp_group</code>. </p>

<p>That seems weird given how similar the two variables are. The average and average high variables have a correlation coefficient of 0.9939. In essence the average high is the average plus a constant (on average, 9.4 degrees with a standard deviation of 2.1). </p>

<p>At first I assumed this was a problem with the code, so I re-pulled the data (still have the same problem and the data extraction seems to be accurate). I also took the model with <code>mean_temp_group</code> and edited the formula in place to read <code>mean_high_group</code> lest I omitted/included a different variable between the models (I didn't). </p>

<p>I assume it has something to do with the binning or something along those lines - any ideas? I'm very confused by two variables that basically appear to be an additive shift of each other giving very different results. </p>
"
"0.140224539037626","0.1420313721078","230167","<p>I started tinkering with the random forest (RF) model in R recently. I made a long list of coding mistakes on my way to getting a final solution. The output from the mistake that surprised me was when I used the sample operation incorrectly and used the same records for training and validation. The RF algorithm accurately classified all but about 5 of 2000 responders. This shocked me. I later learned how to correctly sample.</p>

<p>The idea that RF could memorize specific records is the focus of my concern. Even though each tree votes on whether the record is responder or non-responder, for example, I think probability has been short-circuited for validation or non-training scoring files by a type of specification bias that ignores maximum likelihood if the validation or scoring record matches a record combination (of fields) that was used in training. This circumstance is how a Relation Database System (RDB) dictionary helps the Oracle or DB2 optimizer (it memorizes performance info to optimize the next exact job/query). The very nature of variability on earth (including statistics) implies that there will be some degree of inconsistencies in observed phenomenon. RF ignores at least some potential variability and the very nature of maximum likelihood probabilities if it correctly classifies almost 100% of records if the validation records are the training records. What happened to the accumulation of (or count of) observed inconsistencies in the data? I think this is a type of specification bias that ignores inherent variability for the validation or non-training scoring data file.</p>

<p>The idea that RF has outperformed Logistic Regression doesn't preempt the issue.</p>

<p>Could we agree that RF is more ""Rule Induction"" than probability?</p>
"
"0.105999788000636","0.107365625419004","230318","<p>I have a simple code to compute the logistic loss but have problem with NaN (not a number). </p>

<pre><code>x=as.matrix(scale(mtcars[,names(mtcars) %in% c(""wt"", ""mpg"")]))
y=mtcars$am

logistic_loss &lt;- function(w){
  p=plogis(x %*% w)
  L=-y*log(p)-(1-y)*log(1-p)
  return(sum(L))
}
</code></pre>

<p><a href=""http://i.stack.imgur.com/aLfNe.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aLfNe.png"" alt=""enter image description here""></a></p>

<p>The reason is $w^Tx$ is too large and after the sigmoid transformation to probability,  <code>R</code> treat it into $1.0$ with finite precision, then we have problem of $\log(0)$.</p>

<p>It seems <code>R</code> optimization toolbox can take care of NaN (using <code>optim</code>, it can find the global minima), but should I fix this? and how should I fix it?</p>

<hr>

<p>EDIT: thanks for  Matthew Drury and  General Abrial 's comments. I think the problem can be generalized to ""how to write a function with some values can be only calculated from math, not simple function evaluation"". For example, suppose I want to write a function $\sin(x)/x$, should I hard code when $x=0$ function return $1$? How about some functions that have a lot of such points? I may not possible to manually do all conditions and the code will be a mess.</p>
"
"0.129822696722375","0.131495499095675","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
