"V1","V2","V3","V4"
"0.276385399196283","0.261386651086967","52206","<p>I have a data set which consists of binomial proportions, let's say the success rate of converting a customer depending on the advertisement, the customer age, and various other factors.</p>

<p>For some common combinations of covariates, I have a lot of data, and therefore the binomial proportion of successes has low variance. For rare combinations of covariates, however, I have very little data, and therefore the variance of the proportion is high.</p>

<p>The magnitude of differences is very large, for example I might have 1 million trials for some combinations of covariates, and only 50 for others. However, I want to include ALL data in my model and weight it appropriately to get the best model fit.</p>

<p>I've tried to use R to do binomial (logistic) regression using a generalized linear model:</p>

<pre><code>lrfit &lt;- glm ( cbind(converted,not_converted) ~ advertisement + age, family = binomial)
</code></pre>

<p>This is a good start because it automatically weights the observations by the number of trials.</p>

<p>However, it's not good enough. Here's why: Let's say you have some observations with 100,000 trials and others with 1,000,000 trials. If you weight by number of trials the latter group is going to receive 10 times the weight. This seems nonsensical, however, because both observations are easily precise enough to receive equal treatment in the model. Clearly you want to penalize groups with only 10 or 100 trials, but as the number of trials gets larger, the weight should stop increasing.</p>

<p>Since in weighted least squares the reciprocal of the error variance is used as the weight, my idea would be to use calculate the posterior variance of the proportion (using Jeffrey's prior), then add some constant term to it (this will make sure the variance stops increasing at a certain number of trials) and then use the reciprocal of the sum as the weight.</p>

<p>Is this approach reasonable? Am I missing something? Can someone give me more information about this method?</p>
"
"0.204124145231932","0.193046835626336","52527","<p>Attempting to replicate the results from the recently published article,</p>

<blockquote>
  <p>Aghion, Philippe, John Van Reenen, and Luigi Zingales. 2013. ""Innovation and Institutional Ownership."" American Economic Review, 103(1): 277-304.</p>
</blockquote>

<p>(Data and stata code is available at <a href=""http://www.aeaweb.org/aer/data/feb2013/20100973_data.zip"">http://www.aeaweb.org/aer/data/feb2013/20100973_data.zip</a>).</p>

<p>Am having no problem recreating the first 5 regressions in R (using OLS and poisson methods), but am simply unable to recreate their negative binomial regression results in R, while in stata the regression works fine.</p>

<p>Specifically, here's the R code I've written, which fails to run a negative binomial regression on the data:</p>

<pre><code>library(foreign)
library(MASS)
data.AVRZ &lt;- read.dta(""results_data2011.dta"",
                  convert.underscore=TRUE)
sicDummies &lt;- grep(""Isic4"", names(data.AVRZ), value=TRUE)
yearDummies &lt;- grep(""Iyear"", names(data.AVRZ), value=TRUE)
data.column.6 &lt;- subset(data.AVRZ, select = c(""cites"",
                                 ""instit.percown"",
                                 ""lk.l"",
                                 ""lsal"",
                                 sicDummies,
                                 yearDummies))
data.column.6 &lt;- na.omit(data.column.6)

glm.nb(cites ~ .,
   data = data.column.6,
   link = log,
   control=glm.control(trace=10,maxit=100))
</code></pre>

<p>Running the above in R, I get the following output:</p>

<pre><code>Initial fit:
Deviance = 1137144 Iterations - 1 
Deviance = 775272.3 Iterations - 2 
Deviance = 725150.7 Iterations - 3 
Deviance = 722911.3 Iterations - 4 
Deviance = 722883.9 Iterations - 5 
Deviance = 722883.3 Iterations - 6 
Deviance = 722883.3 Iterations - 7 
theta.ml: iter 0 'theta = 0.000040'
theta.ml: iter1 theta =7.99248e-05
Initial value for 'theta': 0.000080
Deviance = 24931694 Iterations - 1 
Deviance = NaN Iterations - 2 
Step halved: new deviance = 491946.5 
Error in glm.fitter(x = X, y = Y, w = w, etastart = eta, offset = offset,  : 
NA/NaN/Inf in 'x'
In addition: Warning message:
step size truncated due to divergence
</code></pre>

<p>Have tried using a number of different initial values for theta, as well as varying the maximum number of iterations with no luck. The authors' supplied stata code works fine, but I still can't seem to coerce R into making the model work. Are there alternative fitting methods for glm.nb() that may be more robust to the problem I'm encountering?</p>
"
"0.166666666666667","0.15762208124782","54682","<p>I am modelling a zero-truncated process with a count model, and am trying to determine whether the data are overdispersed. The Poisson distribution has a variance equal to its mean,
$$\newcommand{\Var}{\operatorname{Var}} \Var(y) = E(y) = \lambda $$
The negative binomial model relaxes this assumption by estimating an overdispersion parameter $\alpha$. There are two regimens for this method, <code>NB1</code>
$$\Var(y) = \lambda  \alpha$$
and <code>NB2</code>
$$\Var(y) = \lambda  (1 + \lambda/\alpha)$$
I have seen the second version more often. I have estimated three models with the same covariates but the three different link functions (<code>Poisson</code>, <code>NB1</code>, <code>NB2</code>).</p>

<pre><code>Model   logLik   alpha  p(alpha)
Poisson -7942    1      NA
NB1     -6399    1.001  0
NB2     -7944    403.4  0
</code></pre>

<p>Clearly the overdispersion parameter is significant and I should therefore use one of the <code>NB</code> models. But how can the log-likelihood for <code>NB2</code> be lower than for <code>Poisson</code> when there is an additional parameter in the model?</p>

<p>When I estimate all three models, I get a warning that my variance-covariance matrix may not be positive semi-definite. But I don't have highly collinear variables, and all of the relevant matrices have strictly positive eigenvalues, so I don't see why I'm getting this warning. Could this be related to the likelihood problem?</p>

<p>I am using the <code>glmmadmb</code> function from the <code>R</code> <a href=""http://glmmadmb.r-forge.r-project.org/"" rel=""nofollow"">package</a> of the same name. </p>
"
"0.186338998124982","0.17622684421256","11876","<p>I'm estimating some count data. I have counts for say $m=100$ individuals. Unfortunately when using the Poisson regression overdispersion occurs. So I was thinking to fit a negbin model. But this is not appropriate in my case. So I assume that I can not fit a Poisson regression, because the way the Poisson distribution arises is not appropriate in my case ($n$ is not growing to infinity and $p$ is not converging to zero). So I found the beta-bin model. But quite honestly I'm absolutely not familiar in estimating beta-binomial models using R? </p>

<p>First of all: Does it make sense to fit a beta-bin model when anyone wants to estimate counts? Btw: If it makes sense, does anybody know a good book where the application is discribed?</p>
"
"0.125","0.15762208124782","21024","<p>Plotting a <strong>glm</strong> binomial model is reasonably simple with the <strong>predict</strong> function. I'm having trouble creating a similar plot for a <strong>glmer</strong> model; predict doesn't work:  </p>

<pre><code>id    &lt;- factor(rep(1:20, 3))
age   &lt;- rep(sample(20:50, 20, replace=T), 3)
age   &lt;- age + c(rep(0, 20), rep(3, 20), rep(6, 20))
score &lt;- rbinom(60, 15, 1-age/max(age))
dfx   &lt;- data.frame(id, age, score)

library(lme4)
glmerb  &lt;- glmer(cbind(score, 15-score) ~ age + (1|id), dfx, family=binomial)
ndf     &lt;- expand.grid(age=10:60) #for extensibility, usually also have factors
ndf$fit &lt;- predict(glmerb, ndf, type=""response"")
*Error in UseMethod(""predict"") : no applicable method for 'predict' applied to an object of class ""mer""*
</code></pre>

<ol>
<li>How can I produce the desired plot?</li>
<li>While I'm at it, what other plots would be useful for this kind of model for either diagnostic, presentation or glam purposes?</li>
</ol>
"
"0.206239477846076","0.222911285030141","93328","<p>I'm trying to understand how to show the prediction error of a model fit in R using the non-linear least squares function <code>nls</code>. Although there is an argument <code>se.fit=TRUE in the</code>predict.nls` function, the help explains ""At present se.fit and interval are ignored."" I'm able to calculate the confidence interval of the fitted parameters, but am unsure if this information can be used to calculate prediction std. error. Below is an example:</p>

<h3>Example data:</h3>

<pre><code>df &lt;- structure(list(L = c(13L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 
15L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 
16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L, 17L, 17L, 
17L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 
20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 21L, 21L, 21L, 21L, 
21L, 21L, 21L, 21L, 21L, 22L, 22L, 22L, 22L, 22L, 22L, 23L, 23L, 
23L, 23L, 23L, 23L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 24L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 26L, 
26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 27L, 
27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 
28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 
28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 
28L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 
29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 
30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 
30L, 30L, 30L, 30L, 30L, 30L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 
31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 32L, 
32L, 32L, 32L, 32L, 32L, 33L, 33L, 34L, 34L, 34L, 34L, 34L, 34L, 
34L, 35L, 35L, 35L, 35L, 37L, 38L), mat = c(0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 
1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L)), .Names = c(""L"", ""mat""), class = ""data.frame"", row.names = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", 
""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", 
""25"", ""26"", ""114"", ""115"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", 
""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", 
""44"", ""45"", ""46"", ""47"", ""48"", ""49"", ""50"", ""51"", ""52"", ""53"", ""54"", 
""55"", ""56"", ""57"", ""58"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", 
""116"", ""117"", ""66"", ""67"", ""68"", ""69"", ""70"", ""71"", ""72"", ""118"", 
""119"", ""73"", ""74"", ""75"", ""76"", ""120"", ""121"", ""77"", ""78"", ""79"", 
""122"", ""123"", ""124"", ""80"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", 
""125"", ""126"", ""127"", ""128"", ""129"", ""87"", ""88"", ""89"", ""90"", ""130"", 
""131"", ""132"", ""133"", ""134"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", 
""135"", ""136"", ""137"", ""138"", ""139"", ""140"", ""141"", ""97"", ""98"", 
""99"", ""100"", ""142"", ""143"", ""144"", ""145"", ""146"", ""147"", ""148"", 
""149"", ""150"", ""151"", ""101"", ""102"", ""103"", ""104"", ""105"", ""106"", 
""152"", ""153"", ""154"", ""155"", ""156"", ""157"", ""158"", ""159"", ""160"", 
""161"", ""162"", ""163"", ""164"", ""165"", ""166"", ""167"", ""168"", ""169"", 
""170"", ""171"", ""172"", ""107"", ""108"", ""109"", ""110"", ""111"", ""173"", 
""174"", ""175"", ""176"", ""177"", ""178"", ""179"", ""180"", ""181"", ""182"", 
""183"", ""184"", ""185"", ""186"", ""187"", ""188"", ""189"", ""190"", ""191"", 
""192"", ""112"", ""193"", ""194"", ""195"", ""196"", ""197"", ""198"", ""199"", 
""200"", ""201"", ""202"", ""203"", ""204"", ""205"", ""206"", ""207"", ""208"", 
""209"", ""210"", ""113"", ""211"", ""212"", ""213"", ""214"", ""215"", ""216"", 
""217"", ""218"", ""219"", ""220"", ""221"", ""222"", ""223"", ""224"", ""225"", 
""226"", ""227"", ""228"", ""229"", ""230"", ""231"", ""232"", ""233"", ""234"", 
""235"", ""236"", ""237"", ""238"", ""239"", ""240"", ""241"", ""242"", ""243"", 
""244"", ""245"", ""246"", ""247"", ""248"", ""249""))

df2 &lt;- aggregate(df, by=list(as.factor(df$L)), FUN=""mean"")[,-1]
names(df2) &lt;- c(""L"", ""pmat"")
</code></pre>

<h3>Fit the NLS model</h3>

<pre><code>fmla &lt;- ""mat ~ 1 / (1 + exp(-a*(L-L50)))""
fit &lt;- nls(
  fmla,
  data=df,
  start=c(a=0.3, L50=25)
)
summary(fit)
fit.ci &lt;- confint(fit)
fit.ci

newdat &lt;- data.frame(L=seq(0,50,.1))
pred &lt;- predict(fit, newdat)
</code></pre>

<p>Below, I  compare this fit to a GLM model using a binomial distribution. <code>predict.glm</code> allows me to easily calculate the std. error of the prediction. You can see that the predictions between NLS and GLM are quite similar on this data.</p>

<h3>Comparison to binomial GLM</h3>

<pre><code>fit2 &lt;- glm(mat ~ L, data = df, family=binomial)
pred2 &lt;- predict(fit2, newdat, type=""response"", se.fit=TRUE)
plot(df, xlim=c(0,50))
points(pmat ~ L, df2, pch=20, col=8)
lines(newdat$L, pred)
    lines(newdat$L, pred2$fit, col=2)
    polygon(x=c(newdat$L, rev(newdat$L)), y=c(pred2$fit+pred2$se.fit, rev(pred2$fit-pred2$se.fit)), col=rgb(1,0,0,0.2), border=NA)
legend(""right"", legend=c(""NLS"", ""binom. GLM""), col=1:2, lty=1, bty=""n"")
legend(""left"", legend=c(""raw"", ""binned""), col=c(1,8), pch=c(1,20), bty=""n"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/UZXec.png"" alt=""enter image description here""></p>
"
"0.0833333333333333","0.0788110406239101","80463","<p>I have the following set of model-averaged fixed effects from a set of binomial GLMMs: </p>

<p><img src=""http://i.stack.imgur.com/yN4wR.png"" alt=""model parameters image""></p>

<p>I would like to plot the predicted effect of ""NBT"", along with confidence bands, while holding all the other variables at their baseline levels. My attempt to do this in ggplot:</p>

<pre><code>Xvars &lt;- seq(from=0, to=100, by=0.1)  #NBT range is 0-100
  binomIntercept &lt;- 1.317
  binomSlope &lt;- -0.0076     
  binomSE &lt;- 0.009    
Means &lt;- logistic(binomIntercept + binomSlope*Xvars)              
loCI &lt;- logistic(binomIntercept + (binomSlope - 1.96*binomSE)*Xvars)
upCI &lt;- logistic(binomIntercept + (binomSlope + 1.96*binomSE)*Xvars)
df &lt;- data.frame(Xvars,Means,loCI,upCI)
p &lt;- ggplot(data=df, aes(x = Xvars, y = Means)) + 
geom_line() +          
geom_line(data=df, aes(x = Xvars, y = upCI),col='grey') +
geom_line(data=df, aes(x = Xvars, y = loCI), col='grey')
p                                            
</code></pre>

<p><img src=""http://i.imgur.com/eMJBxQQ.png"" alt=""graph image""></p>

<p>I'm assuming that the confidence bands are cone shaped because I'm not accounting for uncertainty in the estimate for the intercept. Maybe this is okay (?), but it does look different from every regression line I've ever seen with confidence intervals plotted.</p>

<p>Can someone please tell me how I should be writing my equations to get the correct confidence intervals, given the intercept, slope, and standard errors from my model output?</p>

<p>(I know I can use the predict function to do this in R, but would like to know how to do it by hand.)  </p>
"
"0.186338998124982","0.17622684421256","81120","<p>I frequently use this model to test catch efficiency and size selection properties of a given trawl fishing gear:</p>

<p>\begin{equation}
\theta(l)=\frac{s\times r(l)}{(1-s)+s\times r(l)}
\end{equation}</p>

<p>where $\theta(l)$ denotes the expected catch rate in the test gear ($T$), which has been fishing in parallel with a non-selective gear (the control gear with blind meshes,$C$). The parameters affecting $\theta(l)$ are:</p>

<ul>
<li><p><strong>Split parameter</strong> $(s)$: It defines the probability of a fish to enter in $T$ ($s$) or in $C$ ($1-s$), $s\in\{0,1\}$</p></li>
<li><p><strong>Fish size selection</strong> ($r(l)$): It defines the likelihood of fish retention in $T$. This likelihood is conditioned to fish body length, therefore it describes the size selection in $T$. Fish size selection is used to be defined using the logit function:</p></li>
</ul>

<p>\begin{equation}
r(l)=\frac{exp(\beta_1+\beta_2\times l )}{1+exp(\beta_1+\beta_2\times l )} 
\end{equation}</p>

<p>Overall, there is a total of 3 parameters to be estimated ($s$, $\beta_1$, $\beta_2$). We use nonlinear regression techniques to estimate such parameters by maximizing the binomial log-likelihood function:</p>

<p>\begin{equation}
\sum_l(N_{l}^T\times \log\theta(l)+N_{l}^C\times \log(1-\theta(l))) 
\end{equation}</p>

<p>where $N_{l}^T$ is the number of fishes per length-class caught in $T$, and $N_{l}^C$ is the numbers caught in $C$.</p>

<p>During a normal experiment, we deploy the pair of gears ($T$ and $C$) several times to perform parallel fishing. To account for the between-haul variation, we use the bootstrap (using a resampling scheme based on resampling between hauls and fishes within hauls) to estimate the errors of $s$, $\beta_1$ and $\beta_2$.</p>

<p>IÂ´m wondering if itÂ´s possible to shift towards a nonlinear mixed modeling approach, where the hauls are considered as a random component. At the moment I only could find such approach by using least squares as minimization criteria. But I could not find a way to keep using the log-likelihood binomial mass function as target criteria.</p>

<p>Thank you beforehand for any comment or guidance.</p>
"
"0.0833333333333333","0.0788110406239101","24293","<p>I've got a non-linear model that I've been applying to some data of repeated binary outcomes. I have data for multiple years, and I'd like to add random effects (by year) for two of my parameters. Looking at the <code>lme4</code> package in R, it seems that it supports non-Gaussian error structure and nonlinear models, but not simultaneously. Is there a way around this, or another package I could be using? I hear <a href=""http://admb-project.org/"" rel=""nofollow"">ADMB</a> has a steep learning curve, so I'd prefer not to tackle that.</p>

<p>Without random effects, my model is quite simple--only 4 parameters and I've been able to fit it using a formula call to the <code>mle2</code> function from the <code>bbmle</code> package.</p>
"
"0.372677996249965","0.352453688425121","229547","<p>Let's say that you were wanting to model how many times someone had to take a certain test before passing (depending on a range of predictors like practice, mock tests taken, classes attended, etc.). Let's also say that most people pass after the first attempt, but that others have to take the test several times, and that the distribution looks Poisson-ish. </p>

<p>If you were to model the dependent variable as the number of tests taken, your minimum count would be 1. On the other hand, if you wanted to model the dependent variable as the number of resits needed, the minimum count would be 0. Both of these seem to me to a reasonable thing to do, and the latter is just the former minus 1, i.e. is shifted.   </p>

<p>It also <em>seems, conceptually</em> like this difference (tests~x1+x2+x3... vs. resits~x1+x2+x3... or tests-1~x1+x2+x3...) shouldn't really affect your ultimate conclusions: if practice decreases the number of tests, it should also decrease the number of resits, and it seems like it should do so to a similar extent. </p>

<p>My questions are: </p>

<ol>
<li><p>What is the practical effect on the model parameters of using (a) shifted dependent variable (resits) rather than (b) the unshifted one (tests)? For instance, would you generally expect the parameter to be overestimated if using resits, or underestimated? If either, would you generally expect the difference to be substantial or minor? Or would this all depend so much on the particular data set that there's no way to tell? That is, is the conceptual similarity between tests and resits misleading, in as far as it makes me think I should get <em>similar</em> results for both.</p></li>
<li><p>What is the practical effect on the model parameters of using:<br>
(a) a zero-truncated model - e.g., in R, I'd specify:<br>
<code>vglm(tests~x, data, family=pospoisson())</code>  and<br>
(b) a left-shifted model - e.g. in R, <code>glm(resits~x, family=poisson)</code>?</p></li>
</ol>

<p>There is a discussion of shifting vs. truncation <a href=""http://stats.stackexchange.com/questions/69477/difference-between-shifted-distribution-and-zero-truncated-distribution"">here</a> but this discussion doesn't specifically address things like the model parameters and significance. It also focuses on right-shifting rather than left-shifting. </p>

<p>I have tried the various options above on my data and it turned out that the basic Poisson <code>(y~x, fam=poisson)</code> had a lower estimate for the predictor than the zero-truncated <code>(y~x, fam=pospoisson)</code>, which in turn had a lower estimate than the left-shifted model <code>(y-1~x, fam=poisson)</code>. Bootstrapped confidence intervals suggest that these differences are not significant, though. However, doing this hasn't told me whether I can expect this to hold generally, i.e., whether the conceptual similarity between tests and resits should typically translate into similar models. In my case, left-shifting resulted in a higher parameter than the zero-truncation model, but is that generally the case? In my case, the parameters weren't significantly different, but is that generally the case? I realize that someone might be able to derive an answer to all this from first principals, mathematically, but I don't have the mathematical background to do so. </p>

<p>I'm asking this as a prelude to another question, <a href=""http://stats.stackexchange.com/questions/229555/poisson-regression-hurdle-models-and-zero-truncated-models"">here</a>. For reasons that I'll explain in that post, I <em>have</em> to left-shift my response variable and I'm wanting to know whether this is in principle problematic (in which case I've just been lucky that the model parameters are quite similar).</p>

<p><strong>*Edit: My data is not in the form of test-counts vs resit-counts. I'm just using these as illustrations because the conceptual similarity between tests and resits is fairly obvious. So my question is not about what regression someone should use for such variables, but is rather about what the effect of shifting vs truncating is on model parameters - would you expect a trivial difference, a significant difference, or there's no way to tell without data? Since people suggested negative binomials below, though, I'm happy to accept answers to this question concerning either Poisson or negative binomial models.</strong></p>
"
"0.166666666666667","0.15762208124782","228362","<p>I have binomial data (meaning k successes out of n trials) for a set of conditions. </p>

<p>I would like to fit a <code>glm</code> in order to quantify the effect of each condition on the success. Since the data are overdisperesed I thought of using a negative binomial <code>glm</code> (<code>glm.nb</code> from the <code>R</code> <code>MASS</code> package does that).</p>

<p>Code snippet (though not really overdisperesed):</p>

<pre><code>set.seed(1)
df &lt;- data.frame(k = as.integer(runif(200,1,20)),
                 n = as.integer(runif(200,100,200)),
                 cond = rep(LETTERS[1:20],10),
                 stringsAsFactors = F)
df$cond &lt;- as.factor(df$cond)
library(MASS)
fit &lt;- glm.nb(k ~ cond + offset(n), data = df)
</code></pre>

<p>Obviously <code>cond</code> A will be set as baseline and all effects will be relative to it. However, this makes interpretation very difficult for me. Therefore my question is how do I fit a <code>glm.nb</code> model where the effects are relative to the mean across all conditions rather than the dummy variable set as baseline?</p>
"
"0.117851130197758","0.111455642515071","229286","<p>So, I did do a search already and came across <a href=""http://stats.stackexchange.com/questions/60777/what-are-the-assumptions-of-negative-binomial-regression"">this</a> response but, his explanation went over my head a bit. The research i've done online hasn't been any more helpful. I've used this code,</p>

<pre><code>m3 &lt;- glm(daysabs ~ math + prog, family = ""poisson"", data = dat)
X2 &lt;- 2 * (logLik(m1) - logLik(m3))
</code></pre>

<p>to find out whether or not the Poisson model is more applicable but i'm not sure what to do if the value of X2 is close to 0. Also, in the link, he mentions, </p>

<pre><code>Linearity: The model is still linear in the parameters 
(i.e. the linear predictor is XÎ²), but the expected response
is not linearly related to them (unless you use the identity link function!).
</code></pre>

<p>but I'm not sure what that means. Should I be looking at the linearity between Y vs. X? or the Residuals of the regression model vs. X?</p>

<p>Please advise.</p>
"
"0.058925565098879","0.111455642515071","5434","<p>I am doing multiple regression with some data (5 predictors, 1 response). Since the response is discrete and non-negative, I thought I would try Poisson regression. However, the data are significantly overdispersed (variance > mean), so I am now trying negative binomial regression.</p>

<p>I was able to fit the model with this code.</p>

<pre><code>library(MASS)
model.nb &lt;- glm.nb(Response ~ Pred1 + Pred2 + Pred3 + Pred4 + Pred5 - 1, data=d)
</code></pre>

<p>Now I would like to see if I can get a better fit by including interactions between the predictors. However, when I try to do so, I get the following error.</p>

<pre><code>&gt; model.nb.intr &lt;- glm.nb(Response ~ Pred1 * Pred2 * Pred3 * Pred4 * Pred5 - 1, data=d)
Error: no valid set of coefficients has been found: please supply starting values
</code></pre>

<p>Any ideas what may be causing this?</p>
"
"NaN","NaN","230372","<p>I was wondering if someone on here could help</p>

<p>I recently ran a Spatial Durbin Regression model in R which came back with two of my three independent variables had significant beta coefficients. A colleague then advised me that I should run a sensitivity test using a negative binomial model to see if I get the same results. However the results are different as all my beta coefficients become significant.</p>

<p>What I am trying to understand is would this likely be due to the incorporation of the spatially lagged independent variable (which has a significant rho value in the spatial Durbin regression)? And does it make sense to use the negative binomial as a 'sensitivity test' as I would think the assumptions would be different, particularly around spatial autocorrelation. </p>
"
"0.226133508433323","0.261386651086967","61711","<p>In this <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">thread</a>, I laid out a problem involving fitting a model that attempts to use minor league baseball statistics to predict success at the major league level (explained in full in the thread). After doing further research outside of the thread, I have come to the conclusion that a zero-inflated negative binomial model is likely the best fit given that I believe there are two processes generating the data. The first process determines whether a player will make the majors and once the player reaches the majors, a second process governs their success (as measured by WAR - also explained in the linked thread). </p>

<p>I ran the model and the ZINB model appears to be a reasonable fit given the following diagnostic plot of fitted values vs residuals (broken into two plots to make it easier to inspect visually).</p>

<p><img src=""http://i.imgur.com/GO28KA3.jpg"" alt=""image3"">
<img src=""http://i.imgur.com/6J1QGOj.jpg"" alt=""image1"">
<img src=""http://i.imgur.com/Qe3vmGx.jpg"" alt=""image2""></p>

<p><strong>EDIT 2: Here is a plot of the Pearson residuals.</strong></p>

<p><img src=""http://i.imgur.com/eyRBF7C.jpg/"" alt=""image3""></p>

<p><strong>EDIT 3: Here is a plot of the Pearson residuals vs the fitted values.</strong> </p>

<p><img src=""http://i.imgur.com/nNsgFPl.jpg"" alt=""image4""></p>

<p>Although this is a big improvement over my previous models, there is clearly a bias for the model to underestimate a player's career WAR i.e., a majority of the residuals are greater than zero. <strong>EDIT: It turns out that the plot is misleading due to the high number of overlapping residuals. In reality, only 10% of the residuals are greater than zero.</strong> I am guessing that this may be improved by either a) a better specification of the functional form of the covariates or b) using a different yet similar model e.g., ZIP. Given that this type of regression goes well beyond what I have worked with before, I would appreciate any suggestions on further diagnostics I can use to both test this model and compare it to others and how to improve the functional form of the covariates given that the R function I have used, zeroinfl (from the pscl package), does not appear to be that flexible. Thank you!</p>
"
"0.498372054284473","0.540301273215703","62106","<p>I have been working on a baseball model to predict success at the major league level using minor league statistics. After posting multiple threads on this site (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>, <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">2</a>, <a href=""http://stats.stackexchange.com/questions/61711/fitting-a-zero-inflated-negative-binomial-regression-with-r"">3</a>) and receiving valuable feedback, I have settled on a zero-inflated negative binomial model as being the best fit for my data.</p>

<p>For those who do not want to go back through old threads,  I will recap some of the story here. Also, for those who have read the old threads, some of the details regarding the variables I am using have changed. </p>

<p>In my model, the dependent variable, offensive career wins above replacement (oWAR), is a proxy for success at the MLB level and is measured as the sum of offensive contributions for every play the player is involved in over the course of his career (details here - <a href=""http://www.fangraphs.com/library/misc/war/"" rel=""nofollow"">http://www.fangraphs.com/library/misc/war/</a>). The independent variables are z-scored minor league offensive variables for statistics that are thought to be important predictors of success at the major league level including age (players with more success at a younger age tend to be better prospects), strike out rate [SOPct], walk rate [BBPct] and adjusted production (a global measure of offensive production). Additionally, since position is an important determinant of whether a players makes the major leagues (those who play at easier positions will be required to perform at a higher offensive level in order to have the same value as a player at a more difficult position), I have included dummy variables to account for position. Note that I have not included the position dummy in the count portion of the model as the oWAR dependent variable has already been adjusted for the difficulty level of the position played by the player. </p>

<p><strong>EDIT: I have added the paragraphs below in response to the following comments in the answer below:</strong></p>

<p><em>""I see you do not include the same covariates in the Logit and the negative binomial process - why not? Ususally, each relevant predictor would be expected to influence both processes.""</em></p>

<p>I think it would help if I explained the data generating process. A player plays in the minor leagues. At some point, when they have demonstrated enough skill at the minor league level (this is a combination of statistical success and observed traits that scouts believe will allow them to be successful at the major league level), a player is promoted to the major leagues. At this point, they have an opportunity to accrue oWAR. At this point, the first data-generating process  (captured by the logit model) ends. Now, a different data generating process takes over, whereby players accumulate oWAR depending on how they play at the major league level. Some players will not perform well and accumulate zero oWAR, the same as a player who did not make the majors. That is one of the reasons I think this model is appropriate. It is not necessarily easy to separate a player who accumulates zero because they aren't good enough to make the major leagues from a player who makes the major leagues but does not succeed at that level (and still ends his career with zero oWAR). I have not included the positional dummy in the count part of the model because the oWAR measure is already adjusted for the position played by the player whereas the minor league statistics are not. When I tried testing them in the model, they were, not surprisingly, not significant. I omitted the BB Pct statistic from the logit part of the model as it was not significant (p = 0.22)</p>

<p><em>""Since your data appears to be a panel (you observe players/teams repeatedly), you can think about more sophisticated stuff like fixed or random effects. ""</em></p>

<p>In terms of how the data is sampled, a player can be in the dataset once (if they spend only a year at the level of the minor leagues) or multiple times if they take multiple years to advance. After reading up on fixed vs random effects model, I don't see how I can used a fixed model to predict out of sample players. However, I am sure that there are fixed effects (effects determined by the player that are not captured in the dependent variables) so I don't fully understand how to handle that issue.</p>

<p><strong>END EDIT</strong></p>

<p>After trying a linear, Box-Cox transformed and basic GLM model with generally poor results, I was directed to the zero-inflated negative binomial distribution set of models. After trying out different combination of variables and following the steps in this excellent <a href=""http://www.jstatsoft.org/v27/i08/paper"" rel=""nofollow"">step-by-step</a> guide for regression models for count data in R, I settled on the following model (shown below).</p>

<p><strong>Model</strong>
<img src=""http://i.imgur.com/neOP2RE.jpg"" alt=""model1""></p>

<p>Furthermore, when I re-estimate the standard errors using sandwich standard errors, the model still appears to have appropriate independent variables.</p>

<p><strong>Sandwich Standard Errors</strong>
<img src=""http://i.imgur.com/47JjOae.jpg"" alt=""sandwich""></p>

<p>At this point, I do not think I will find a better model type given the dataset I have. However, I am still left with some issues. The first issue may be a function of the dataset I am using. In most examples I have seen that discuss zero-inflated data, there are clearly more zeros than other values (hence, the name). However, the number of zeros still appears to be less than 50% of the total dependent variables and usually not even that high. In my dataset, approximately 87% of the dependent variables are zero i.e., it is hard to have success in major league baseball. I am guessing the model should technically be able to account for this scenario (albeit with less predictive value than a model with more non-zeros) but I am not sure how to check if that is the case. When I create a plot of fitted values and Pearson residuals, and a plot of fitted values and raw residuals, they appear as below:</p>

<p><strong>Fitted vs Pearson residuals</strong>
<img src=""http://i.imgur.com/tlx6ibn.jpg"" alt=""FvP""></p>

<p><strong>Fitted vs raw residuals</strong>
<img src=""http://i.imgur.com/vL8OIcV.jpg"" alt=""FvR""></p>

<p>Not knowing exactly what these plots look like in a good-fitting regression, I decided to take the sample data described <a href=""http://www.ats.ucla.edu/stat/r/dae/zipoisson.htm"" rel=""nofollow"">here</a> and examine the plots in an example where I know the fit has been deemed to be good. </p>

<p><strong>Fitted vs Pearson residuals - Sample problem</strong>
<img src=""http://i.imgur.com/TJh2zHF.jpg"" alt=""FvPEx""></p>

<p><strong>Fitted vs raw residuals - Sample problem</strong>
<img src=""http://i.imgur.com/YJMxJl9.jpg"" alt=""FvREx""></p>

<p>Clearly, these plots do not look that similar to mine. I am not sure how much has to do with a) model misspecification b) the fact that my dependent variable has 87% zeros and c) the fact that this is a simple sample problem designed to perfectly fit this model whereas my data is messy, real world data. Any thoughts on this issue would be appreciated. </p>

<p>My second issue, which I am not sure if I should be tackling after or simultaneously with the first issue, has to do with the functional form specification. I don't know if my independent variables are in the right form. It has been suggested to me by a friend that I could try a) multiple fractional polynomials with loops or b) informally play around with adding polynomials of covariates, interactions, etc. My issue at this point is that I do not know how to implement point a in R and and I am not sure which forms to try for point b besides randomly choosing some. Once again, help on this separate (but related?) issue would be greatly appreciated. </p>

<p>If anyone has any questions, I will do my best to answer them. In my first post (<a href=""http://stats.stackexchange.com/questions/61217/transforming-variables-for-multiple-regression-in-r"">1</a>), I mentioned I could not provide the dataset but I have been given permission to do so if anyone wants to take a look. Thanks again. </p>
"
"0.204124145231932","0.193046835626336","156850","<p>I created a negative binomial model where the final model included 5 quadratic predictors (each with a corresponding linear term). I am considering two ways to interpret the beta coefficients for each predictor: an odds-ratio approach, and reporting the maximum/maxima for the quadratic term using the equation -2a/b where a is the linear term and b is the quadratic term. For example, I may report that animal use was expected to decrease by 10% for every 1 m increase in distance from roads. Or, I may report that animal use was highest at 25 m from roads. </p>

<p>This is simple enough, but prior to model fitting I scaled each response around 0 and with a variance of 1. Two questions: 1) I know how to create odds-ratio estimates using the formula [e(beta)-1]*100 for linear (presumably unscaled) variables. How do I create odds-ratio estimates for scaled quadratic variables? 2) How do I interpret the maximum or maxima for scaled quadratic variables? For example, if I estimate that -2a/b = 25, I assume this doesn't mean that use was highest at 25 m; rather, must I somehow backtransform this estimate to determine the true, unscaled value?</p>
"
"0.301232038038355","0.284884924644768","157575","<p>I'm trying to fit generalized linear models to some sets of count data that might or might not be overdispersed. The two canonical distributions that apply here are the Poisson and Negative Binomial (Negbin), with E.V. $\mu$ and variance </p>

<p>$Var_P = \mu $</p>

<p>$Var_{NB} = \mu + \frac{\mu^2}{\theta} $</p>

<p>which can be fitted in R using <code>glm(..,family=poisson)</code> and <code>glm.nb(...)</code>, respectively. There is also the <code>quasipoisson</code> family, which in my understanding is an adjusted Poisson with the same E.V. and variance</p>

<p>$Var_{QP} = \phi\mu$,</p>

<p>i.e. falling somewhere in between Poisson and Negbin. The main problem with the quasipoisson family is that there is no corresponding likelihood for it, and hence a lot of extremely useful statistical tests and fit measures (AIC, LR etcetera) are unavailable. </p>

<p>If you compare the QP and Negbin variances, you might notice that you could equate them by putting $\phi = 1 + \frac{\mu}{\theta}$. Continuing on this logic, you could try to express the quasipoisson distribution as a special case of the Negbin:</p>

<p>$QP\,(\mu,\phi) = NB\,(\mu,\theta = \frac{\mu}{\phi-1})$,</p>

<p>i.e. a Negbin with $\theta$ linearly dependent on $\mu$. I tried to verify this idea by generating a random sequence of numbers according to the above formula and fitting it with <code>glm</code>:</p>

<pre><code>#fix parameters

phi = 3
a = 1/50
b = 3
x = 1:100

#generating points according to an exp-linear curve
#this way the default log-link recovers the same parameters for comparison

mu = exp(a*x+b) 
y = rnbinom(n = length(mu), mu = mu, size = mu/(phi-1)) #random negbin generator

#fit a generalized linear model y = f(x)  
glmQP = glm(y~x, family=quasipoisson) #quasipoisson
glmNB = glm.nb(y~x) #negative binomial

&gt; glmQP

Call:  glm(formula = y ~ x, family = quasipoisson)

Coefficients:
(Intercept)            x  
    3.11257      0.01854  
(Dispersion parameter for quasipoisson family taken to be 3.613573)

Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
Null Deviance:      2097 
Residual Deviance: 356.8    AIC: NA

&gt; glmNB

Call:  glm.nb(formula = y ~ x, init.theta = 23.36389741, link = log)

Coefficients:
(Intercept)            x  
    3.10182      0.01873  

Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
Null Deviance:      578.1 
Residual Deviance: 107.8    AIC: 824.7
</code></pre>

<p>Both fits reproduce the parameters, and the quasipoisson gives a 'reasonable' estimate for $\phi$. We can now also define an AIC value for the quasipoisson:</p>

<pre><code>df = 3 # three model parameters: a,b, and phi
phi.fit = 3.613573 #fitted phi value copied from summary(glmQP)
mu.fit = glmQP$fitted.values 

#dnbinom = negbin density, log=T returns log probabilities
AIC = 2*df - 2*sum(dnbinom(y, mu=mu.fit, size = mu.fit/(phi.fit - 1), log=T))
&gt; AIC
[1] 819.329
</code></pre>

<p>(I had to manually copy the fitted $\phi$ value from <code>summary(glmQP)</code>, as I couldn't find it in the <code>glmQP</code> object)</p>

<p>Since $AIC_{QP} &lt; AIC_{NB}$, this would indicate that the quasipoisson is, unsurprisingly, the better fit; so at least $AIC_{QP}$ does what it should do, and hence it might be a reasonable definition for the AIC (and by extension, likelihood) of a quasipoisson. The big questions I am left with are then</p>

<ol>
<li>Does this idea make sense? Is my verification based on circular reasoning?</li>
<li>The main question for anyone that 'invents' something that seems to be missing from a well established topic: if this idea makes sense, why isn't it already implemented in <code>glm</code> ?</li>
</ol>

<p>Edit: figure added</p>

<p><img src=""http://i.stack.imgur.com/H6D7A.png"" alt=""glm fits and +-1 sigma bands""></p>
"
"0.149071198499986","0.17622684421256","111549","<p>I am trying to fit a NB GLMM with a gemoetric distribution. I have come across very little information on this form of regression. And would like some pointers/reasurance.</p>

<p>some literature is available for glm method here: <a href=""http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf</a></p>

<p>but I cannot find anything on a mixed model to use with this.</p>

<p>main points:</p>

<p>Is this code correct to run such a model?
Is this the best package for this analysis?
Does model selection and validation follow that for regular Negative binomial models?</p>

<p>sample data</p>

<pre><code>DF&lt;- structure(list(code = structure(c(1L, 1L, 6L, 6L, 7L, 9L, 10L, 
10L, 10L, 10L, 10L, 11L, 11L, 12L, 13L, 14L, 14L, 16L, 16L, 17L, 
17L, 23L, 24L, 26L, 27L, 27L, 27L, 28L, 28L, 29L, 30L, 30L, 31L, 
32L, 34L, 35L, 8L, 8L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 15L, 33L, 33L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 
19L, 19L, 18L, 5L, 5L, 4L, 4L, 22L, 21L, 21L, 20L, 20L), .Label = c(""15010212"", 
""15010213"", ""15010215"", ""15010216"", ""15010220"", ""15010222"", ""15010245"", 
""15010269"", ""15010274"", ""15010284"", ""15010285"", ""15010287"", ""15010290"", 
""15010291"", ""15010292"", ""15010294"", ""15010299"", ""15020313"", ""15020314"", 
""15020315"", ""15020316"", ""15020317"", ""15020326"", ""15020345"", ""15020348"", 
""15020384"", ""15020395"", ""15020396"", ""15020397"", ""15030312"", ""15030317"", 
""15030349"", ""15030392"", ""15030394"", ""15030395""), class = ""factor""), 
flow = c(15.97766667, 14.226, 17.15724762, 14.7465, 39.579, 
23.355, 110.2926923, 71.95709524, 50.283, 66.66754955, 38.9218, 
72.73666667, 32.37466667, 50.34905172, 27.98471429, 49.244, 
109.1759778, 77.71733333, 37.446875, 101.23875, 67.78534615, 
21.359, 36.54257143, 34.13961111, 64.35253333, 80.98554545, 
68.0554, 61.50857143, 48.983, 63.81072727, 26.105, 46.783, 
23.0605, 33.61557143, 46.31042857, 62.37061905, 12.565, 42.31983721, 
15.3982, 14.49625, 16.40853846, 17.84350847, 14.625375, 13.10714286, 
13.35466667, 12.94033333, 13.54236364, 14.10023711, 12.5747807, 
23.77425, 25.626, 15.23888523, 74.62485714, 170.1547778, 
91.292, 71.422, 42.50887568, 53.89983761, 141.7211667, 50.67125, 
48.098, 66.83644444, 76.564875, 80.63189189, 136.0573243, 
136.3484, 86.68688889, 34.82169565, 70.00415385, 64.67233333, 
81.72766667, 57.74522034), success = c(0L, 1L, 0L, 1L, 1L, 
1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 
1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 
0L, 1L, 1L, 0L, 1L, 0L, 1L), length = c(595, 595, 582, 582, 
565, 537, 585, 585, 585, 585, 585, 595, 595, 607, 625, 627, 
627, 607, 607, 644, 644, 620, 560, 567, 615, 615, 615, 595, 
595, 546, 580, 580, 594, 605, 610, 640, 575, 575, 632, 632, 
632, 632, 632, 632, 632, 632, 632, 632, 632, 525, 585, 585, 
624, 624, 624, 624, 624, 624, 624, 608, 635, 635, 655, 670, 
670, 584, 584, 707, 680, 680, 740, 740), attempt = structure(c(1L, 
2L, 1L, 2L, 1L, 1L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 1L, 1L, 1L, 
2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 3L, 1L, 2L, 1L, 1L, 
2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
9L, 10L, 11L, 1L, 1L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 
1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L), .Label = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11""), class = ""factor"")), .Names =         c(""code"", 
""flow"", ""success"", ""length"", ""attempt""), row.names = c(NA, -72L
), class = ""data.frame"")
</code></pre>

<p>model is as follows. setting theta = 1 should determine a geometric distribution.</p>

<pre><code>library(MASS)
M1&lt;-glmmPQL(success ~ length + flow + attempt,
          random = ~ 1|code,
          family = negative.binomial(theta = 1),
          data = DF)

summary(M1)
</code></pre>

<p>Ultimately I am trying predict success (0 = fail, 1 = success). However this is measured for many different individuals (code), essentially a repeated measure and hence should be included as a random effect. each individual may only have one success but can have multiple attempts. Predictors of success come in the form of ""length"" of the individual, ""attempt"" number... a factor of the number of the attempt, and ""flow"" which is river flow at time of the attempt and so a continuous variable.</p>

<p>Thanks in advance</p>
"
"0.220479275922049","0.208514414057075","172394","<p>I have a number of Poisson GLMs relating a daily count to daily weather predictors. They've been assembled in R from time series of counts and observed weather, and they're represented by GLM objects in R like:</p>

<p><code>glm(count ~ offset(log(population)) + dow + temp.max,
    family = poisson, data = myData)
</code></p>

<p>I can send new data, such as new observations or climate model output, to <code>predict.glm()</code> to get predicted daily count estimates. I understand that if I supply <code>se.fit = TRUE</code> to <code>predict.glm()</code>, I can get standard errors for each day's estimateâ€”from which I can derive confidence intervals (for example, <code>+/- 1.96*SE</code>for a 95% CI).</p>

<p>Comparing the estimates for each model is a bit difficult in this form, though, especially as I'll be generating predictions for a number of time periods (2000â€“2009, 2080â€“2089, etc.). So what II'd like to do is aggregate these daily estimates into a mean daily count with an associated combined confidence interval. How would I go about this?</p>

<p>For bonus points, I'm looking at also running Negative Binomial GLMs using the MASS package's <code>glm.nb()</code> to fix overdispersion in my Poisson GLMs. Is the process of obtaining standard errors in R similar for NB GLMs, and could I still calculate aggregate estimates and CIs?</p>

<p><strong>EDIT:</strong> to clarify, I'm not looking to combine the interval around one day's prediction in the Poisson model with the same day's in the NB model. I want to give a model a series of dates, make predictions with associated intervals for each day, and then <em>add up or average</em> those daily counts and come up with an interval for the <em>entire period of dates</em>.</p>
"
"0.251259453814803","0.237624228260879","193000","<p>So I'm trying to fit a hurdle model with the count distribution as negative binomial.  I get the following outputs for assuming negative binomial and poisson:</p>

<pre><code>&gt; hurdle(degree ~ dc, data = data, dist = ""negbin"")

Call:
hurdle(formula = degree ~ dc, data = data, dist = ""negbin"")

Count model coefficients (truncated negbin with log link):
(Intercept)           dc  
     0.2428       0.1035  
Theta = 0.8815 

Zero hurdle model coefficients (binomial with logit link):
(Intercept)           dc  
    -0.8512       0.1649

&gt; hurdle(degree ~ dc, data = data, dist = ""poisson"")

Call:
hurdle(formula = degree ~ dc, data = data, dist = ""poisson"")

Count model coefficients (truncated poisson with log link):
(Intercept)           dc  
    0.68283      0.08584  

Zero hurdle model coefficients (binomial with logit link):
(Intercept)           dc  
    -0.8512       0.1649  
</code></pre>

<p>From a regression in python based on estimates of mean of non zero data vs. regressor, I get:</p>

<p>m = 0.08374289, b =  0.7132967</p>

<p>Which is far from what the negative binomial estimates, but the Poisson gets it pretty close.  However a vuong test tells me that the negative binomial is far better:</p>

<pre><code>&gt; vuong(mod_pois, mod_nb_hurdle)
Vuong Non-Nested Hypothesis Test-Statistic: -114.0873 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
in this case:
model2 &gt; model1, with p-value 0 
</code></pre>

<p>The non-zero data is overdispersed, but it looks like the variance is constant*mean, so I know Poisson shouldn't be used, but why is a Poisson hurdle so much better at predicting log(expected value)? </p>
"
"NaN","NaN","160331","<p>I'd like to use negative binomial regression with caret. However in the <a href=""http://rpackages.ianhowson.com/rforge/caret/man/models.html"" rel=""nofollow"">list of supported models</a> I can't find it.</p>

<p>I tried to use:</p>

<pre><code>train(data=dataset,trControl=trainControl(method = ""none""), method=""glm.nb"", family=binomial()) 
</code></pre>

<p>But this gives me an error too.</p>

<p>So how can I used negative binomial regression with caret?</p>
"
"0.220479275922049","0.208514414057075","89210","<p>I have two data vectors of observed count data: $A$ and $B$, where count $A_n$ and $B_n$ refer to the same observation point.</p>

<p>$A$ is assumed to follow a negative binomial distribution. $B$ is assumed to be the result from two underlying processes. For one again an independent negative binomial distribution and additionally a kind of shadowing effect from $A$, where a certain fraction $f$ of counts from $A$ are also observed in $B$.</p>

<p>Accordingly, one can simulate such data by:</p>

<pre><code>set.seed(13579)
f &lt;- runif(1)
A &lt;- rnbinom(100, mu = 100, size = 1)
B &lt;- floor(f*A) + rnbinom(100, mu = 20, size = 1)
</code></pre>

<p><strong>Now to my question: Since $B$ is a mixture model of two negative binomial distributed variables, how can I estimate the value of factor $f$ explaining the underlying data best?</strong></p>

<p>The final result should be a vector $B^\prime$, correcting $B$ for its shadow portion: $B^\prime = B - (A/f)$. Since I know $A$ and $B$, to gain this I need to estimate $f$ from the data and the underlying assumption of a negative binomial distribution.</p>

<p>I'm neither a statistics nor an R expert, therefore any help is highly appreciated.</p>
"
"0.263523138347365","0.249222393139613","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.144337567297406","0.136504726557987","234947","<p>I'm looking to run a linear mixed effect model using lme4, where my dependent variable <code>one_syllable_words / total_words_generated</code> is a proportion and my random effect <code>(1 | participant_ID)</code> reflects the longitudinal nature of the design. Independent, fixed effect variables of interest include <code>age</code>, <code>group</code>, <code>timepoint</code>, and interactions between them. </p>

<p>I've come across two main ways to deal with the proportional nature of the DV:  </p>

<ol>
<li><p><strong>Standard logistic regression / binomial GLM</strong>  </p>

<p>In my scenario, I envision the lme4 equation looking like this:  </p>

<pre><code>glmer(one_syllable_words / total_words_generated ~ age + group +
timepoint + age:timepoint + age:group + timepoint:group + (1 |
participant_ID), family = ""binomial"", weights =
total_words_generated, data = mydat)  
</code></pre></li>
<li><p><strong>Beta regression</strong>  </p>

<p>I would apply a transformation to my DV <code>(DV * (n - 1) + .5)/ n)</code> so that it cannot equal 0 or 1. (There are a few instances where it equals zero, no instances where it equals one.)  </p></li>
</ol>

<p>I'm unclear whether logistic regression or beta regression is preferred in this example. My DV isn't a clear-cut case of successes and failures (unless we stretch the definition of ""success""), so I'm worried logistic regression might not be appropriate. However, I'm having trouble getting a firm grasp on beta regression &amp; all it entails. If beta regression is preferred:  </p>

<ol>
<li>Why is it preferred?  </li>
<li>What is it doing ""behind the scenes"" to the data?  </li>
<li>How can it be applied in R?  </li>
</ol>
"
"0.322748612183951","0.30523384783368","118141","<p>I'm trying to fit a logistic curve to cumulative data, derived from satellite imagery. Previously, I have point observation data which were either 0s or 1s. Os being 'forest' and 1s being 'non-forest'. These point observations existed for multiple images/dates. So I had one csv file with 'observation date' in once column and 'state' in another. Weights were also included, but these aren't relevant here. </p>

<p>This was the code I used:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Owner\\Desktop\\BV\\Deforestation_Analysis\\Ambaro_Ambanja\\Analysis\\CDM_Table_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%m/%d/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,weights=Weight,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>Now, rather than having multiple 0s and 1s for each date, I have simply number of non-forest pixels (and no weight). Here is the data I have:</p>

<p><img src=""http://i.stack.imgur.com/oFFIe.jpg"" alt=""enter image description here""></p>

<p>The 'State' field is literally the portion of non-forest pixels (18.6% of pixels were non-forest in 2014)</p>

<p>I tried runnning the following adaption of the original script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>I fully expected it to fail, because the data is no longer binomial. But while it did throw up a warning ('In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!'), the coefficients it spat out formed a curve which looked perfect. But I wasn't overly confident in this hash.</p>

<p>I've read that you can still use the binomial glm family if you feed R with a table containing successes (non-forest) and failures. So I came up with this adapted data:</p>

<p><img src=""http://i.stack.imgur.com/CMSd3.jpg"" alt=""enter image description here""></p>

<p>and the following adapted script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=cbind(Deforested, Total-Deforested)~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>It ran fine with no errors, but the trend it generated doesn't fit the data anywhere near as well as the hashed version:</p>

<p><img src=""http://i.stack.imgur.com/FXL6I.jpg"" alt=""Blue line is hased version; Grey line is adapted script; Red points are the data I&#39;m using to fit the model""></p>

<p>The blue line is hased version; grey line is adapted script; red points are the data I'm using to fit the model.</p>

<p>Why does the adapted version fit the point worse than the hashed version? Is R so clever that it just uses my fractional values how I want it to in the glm(family=binomial)?</p>

<p>Any advice greatly appreciated! Am not happy with how I got the blue trend and this work is very important to our study.</p>

<p>THANK YOU!!</p>
"
"0.267261241912424","0.294883912309794","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.201007563051842","0.261386651086967","94248","<p>I'm trying to figure out how to determine to what extent a sample deviates from a negative binomial model fitted to a larger population. </p>

<p>As an example, I generated counts of doctor visits for a population of males and females aged between 20 and 60:</p>

<pre><code>library(""MASS"")
library(""ggplot2"")

set.seed(8)

n &lt;- 1000
dispersion &lt;- 1
baseline &lt;- 1
ageeffect &lt;- 0.02
gendereffect &lt;- 0.5

gender &lt;- sample(c(""male"", ""female""), size=n, replace=TRUE)
age &lt;- round(runif(n, min=20, max=60))
mu &lt;- baseline + age * ageeffect + (gender==""female"") * gendereffect
visits &lt;- rnbinom(n, size=dispersion, mu=mu)
barplot(table(visits), space=0)

data &lt;- data.frame(visits, gender, age)
summary(m &lt;- glm.nb(visits ~ age + gender, data = data))

newdata &lt;- data.frame(
  age = rep(seq(20, 60), 2),
  gender = c(rep(""male"", 41), rep(""female"", 41))
)

p &lt;- predict(m, newdata, type=""link"", se.fit=TRUE)

newdata$visits &lt;- exp(p$fit)
newdata$ll &lt;- exp(p$fit - 1.96 * p$se.fit)
    newdata$ul &lt;- exp(p$fit + 1.96 * p$se.fit)

ggplot(newdata, aes(age, visits)) +
  geom_ribbon(aes(ymin = ll, ymax = ul, fill = gender), alpha = .25) +
  geom_line(aes(colour = gender), size = 2) +
  labs(x = ""Age"", y = ""Prediction"")
</code></pre>

<p>Now I would like to quantify how much a specific group of people deviates from the model. Groups can be of different sizes and composition, and I'm only interested in groups which exhibit a above-normal number of doctor visits. The idea is to calculate some kind of scoring for each group (let's say the inhabitants of a specific neighbourhood) and follow this up over time.</p>

<p>What would be the preferred way of doing this? I did calculate p-values for each observation in the sample, but I'm not sure how to proceed.</p>

<pre><code>samplesize &lt;- 10
s &lt;- data[sample(1:n, samplesize),]
pp &lt;- predict(m, s, type=""link"", se.fit=TRUE)
s$mu &lt;- as.vector(exp(pp$fit))
s$theta &lt;- rep(m$theta, samplesize)
s$p &lt;-  1 - pnbinom(s$visits, m$theta, mu=s$mu)
s

    visits gender age       mu     theta          p
487      0 female  48 2.385442 0.9899275 0.70307422
873      2 female  38 2.115261 0.9899275 0.31264813
19       3   male  29 1.440554 0.9899275 0.12164221
78       2 female  33 1.991872 0.9899275 0.29476639
362      1   male  43 1.704576 0.9899275 0.39648278
120      1 female  48 2.385442 0.9899275 0.49534441
658      1   male  53 1.922300 0.9899275 0.43182233
14       5 female  22 1.745163 0.9899275 0.06636807
663      2 female  57 2.657989 0.9899275 0.38294175
77       1   male  24 1.356523 0.9899275 0.33088457
</code></pre>

<p><strong>Edit 1:</strong> After reading <a href=""http://mikelove.wordpress.com/2012/03/12/combining-p-values-fishers-method-sum-of-p-values-binomial/"" rel=""nofollow"">http://mikelove.wordpress.com/2012/03/12/combining-p-values-fishers-method-sum-of-p-values-binomial/</a>, would Fisher's method be a good approach?</p>

<p><strong>Edit 2:</strong> I'm exploring Pearson residuals now, but something's not clear to me. In the code below I generate a dataset and construct a model. I then calculate Pearson statistics for samples of 20 and 1000 counts. From what I read I expected these statistics to follow a Chi-squared distribution with n-2 degrees of freedom, but that doesn't  really seem to be the case. Am I doing something wrong?</p>

<pre><code>library(""MASS"")

n &lt;- 1000
dispersion &lt;- 1
baseline &lt;- 1
ageeffect &lt;- 0.02
gendereffect &lt;- 0.5

dataset &lt;- function(n) {
  gender &lt;- sample(c(""male"", ""female""), size=n, replace=TRUE)
  age &lt;- round(runif(n, min=20, max=60))
  mu &lt;- baseline + age * ageeffect + (gender==""female"") * gendereffect
  visits &lt;- rnbinom(n, size=dispersion, mu=mu)
  data &lt;- data.frame(visits, gender, age)
  return(data)  
}

model &lt;- function(data) {
  m &lt;- glm.nb(visits ~ age + gender, data = data)
}

pred &lt;- function(data, m) {
  p &lt;- predict(m, data, type=""link"", se.fit=TRUE)
  data$mu &lt;- as.vector(exp(p$fit))
  data$theta &lt;- rep(m$theta, length(data$visits))
      data$pr &lt;- (data$visits - data$mu) / sqrt(data$mu + data$mu^2 / data$theta)
  return(data)
}

pearson.stat &lt;- function(data) {
  return(sum(data$pr^2))
}

test.chisq &lt;- function(data) {
  return(pchisq(pearson.stat(data), df = length(data$pr) - 2, lower=FALSE))
}

# population of 100000 and model

dp &lt;- dataset(100000)
m &lt;- model(dp)

# pearson statistic (samples of 1000)

ps &lt;- NULL
for (i in seq(1, 1000)) {
  d &lt;- dataset(1000)
  p &lt;- pred(d, m)
  ps &lt;- c(ps, pearson.stat(p))
}

# pearson statistic (samples of 20)

ps2 &lt;- NULL
for (i in seq(1, 1000)) {
  d &lt;- dataset(20)
  p &lt;- pred(d, m)
  ps2 &lt;- c(ps2, pearson.stat(p))
}

# plot

dplot &lt;- function(res, df) {
  x &lt;- seq(min(res), max(res), length.out=100)
  y &lt;- dchisq(x, df=df)
  h &lt;- hist(res, prob=T, breaks=30, main=paste(df, ""degrees of freedom""), ylim=c(0, max(y)), xlab=""Pearson statistic"", ylab=""density"")
  lines(x, y, col=""cornflowerblue"", lwd=2)
}

par(mfrow=c(2, 1))
dplot(ps, df=1000-2)
dplot(ps2, df=20-2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/hZyOM.png"" alt=""enter image description here""></p>
"
"0.0833333333333333","0.0788110406239101","206790","<p>I have a count variable that represents the number of new band foundings in a country-year. However, there is zero inflation as there are no foundings for most country-year. There is also overdispersion as the variance is greater than the mean number of foundings. </p>

<p>Under these circumstances, a zero-inflated negative binomial model would fit best however I don't really see how the logistic step fits with the nature of my data. In other words, I don't think the assumptions of ZINB are satisfied. Is there an alternative model that I could use to get around the issue? Any help would be appreciated.</p>

<p>UPDATE 1: </p>

<pre><code>summary(zinb1 &lt;- zeroinfl(foundings ~ disbandings + pop100k | disbandings + pop100k,
           data = casper, dist = ""negbin""))
</code></pre>

<p>However I get the following warning message: </p>

<pre><code>Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred 
2: In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>
"
"0.186338998124982","0.17622684421256","183266","<p>I'm modeling after this <a href=""http://stats.stackexchange.com/a/27445"">answer</a> in order to simulate data from a negative binomial model where both y and x are counts best described by a negative binomial distribution, and am wondering if this was the correct way to go about it in R. If anyone can comment or give advice, I would very much appreciate it. My R code goes like this:</p>

<pre><code>n &lt;- 100
beta0 &lt;- log(5)
beta1 &lt;- 0.01
theta &lt;- 5
set.seed(1)
# generate x as counts
x &lt;- sample(1:100, n, replace = T)
mu &lt;- exp(beta0 + beta1*x)
# generate y as random negative binomial variates
y &lt;- rnegbin(n, mu, theta)

# plot to check
par(mfrow=c(2,2))
hist(x)
hist(y)
plot(y~x)
# summary stats
mean(y)
var(y)

# fit negative binomial model to simulated data
mod1 &lt;- glm.nb(y~x)
summary(mod1)
</code></pre>

<p>Some additional questions:</p>

<p>Firstly, I'm not sure what sort of theta value is a sensible one to use in simulations.</p>

<p>Secondly, <code>plot(y~x)</code> shows considerable scatter about the bivariate relationship, and I understand that this is due to the random variation resulting from the <code>rnegbin(n, mu, theta)</code> step above, but is there a way to control the amount of deviation?</p>

<p>Lastly, if I wanted to also simulate x as a negative binomial distribution, then can I just do something like <code>x &lt;- rnegbin(n, 4, theta) + 1</code> (arbitrary mean of 4 and adding one to avoid 0s) and use this x vector to generate mu and then y?</p>
"
"0.117851130197758","0.111455642515071","77762","<p>I wish to model the number of bugs caused by software development. This is intuitively sort of a Poisson process, however it is overdispersed. One thing we can do in this case is to use a negative binomial distribution (because negative binomial approaches Poisson as <code>r</code> gets larger, or because we might think the parameter $\lambda$ of the poisson is itself gamma-distributed.)</p>

<p>I'm not sure how to do this though. For example, we have that 
$$\lim_{r\to\infty}NB\left(r,\frac{\lambda}{\lambda+r}\right)=\text{Poisson}(\lambda)$$
Given that a poisson process of duration $t$ can be modeled as $\text{Poisson}(\lambda t)$ I guess we could look at $NB\left(r,\frac{\lambda t}{\lambda t+r}\right)$ - is that correct? Given that I know $t$, it seems like I should be setting $r=t$.</p>

<p>At a more technical level, <code>glm.nb</code> from the <code>MASS</code> package seems to fit $r$ not the dispersion parameter and I don't see an obvious parameter to change this.</p>

<p>Any insight at the theoretical or technical level would be appreciated.</p>
"
"0.220479275922049","0.208514414057075","34930","<p>I have a large set of data for 37 different clinical units (all oncology) in their respective 37 hospitals. There are two specific outcome variables that I need to analyse:</p>

<p>First, drug usage for specific drugs types and classes (aggregated drugs) that are expressed as a rate â€“  DDD (Defined Daily Doses) per 100 patient days. There are individual patient drug use figures for this set.</p>

<p>Question1: Which regression approach should I take? From what I can gather I can use a Poisson regression model. IF there is overdispersion in the outcome I could resort to a negative binomial model.</p>

<p>Second: I have antibiotic resistance data that is expressed as proportion in the range 0 â€“ 1.These are not available as individual patient data points but aggregated to each of the 37 hospitals.</p>

<p>Question 2: Again, which approach? From what I have read I can use a logistic regression model. I have been advised by another statistician to initiall use a logit model and then use a probit model and compare goodness of fit for each model.</p>

<p>Does this sound like a reasonable approach? Is there a specific text that you could direct me to in order to upgrade my basic regression modelling skills. I will be using R for the analysis.</p>

<p>Thanks in advance.</p>
"
"0.311804782231162","0.294883912309794","141412","<p>I am very confused with how weight works in glm with family=""binomial"". In my understanding, the likelihood of the glm with  family = ""binomial"" is specified as follows:
$$
f(y) = 
{n\choose{ny}} p^{ny} (1-p)^{n(1-y)} = \exp \left(n \left[ y \log \frac{p}{1-p} - \left(-\log (1-p)\right) \right] + \log {n \choose ny}\right)
$$
where $y$ is the ""proportion of observed success"" and $n$ is the known number of trials.</p>

<p>In my understanding, the probability of success $p$ is parametrized with some linear coefficients $\beta$ as $p=p(\beta)$ and glm function with family = ""binomial"" search for:
$$
\textrm{arg}\max_{\beta} \sum_i \log f(y_i).
$$ 
Then this optimization problem can be simplified as:</p>

<p>$$
\textrm{arg}\max_{\beta} \sum_i \log f(y_i)= 
\textrm{arg}\max_{\beta} \sum_i n_i \left[ y_i \log \frac{p(\beta)}{1-p(\beta)} - \left(-\log (1-p(\beta))\right) 
\right] + \log {n_i \choose n_iy_i}\\
=
\textrm{arg}\max_{\beta} \sum_i n_i \left[ y_i \log \frac{p(\beta)}{1-p(\beta)} - \left(-\log (1-p(\beta))\right) 
\right] \\
$$<br>
Therefore if we let $n_i^*=n_ic$ for all $i=1,...,N$ for some constant $c$, then it must also be true that:
$$
\textrm{arg}\max_{\beta} \sum_i \log f(y_i)
=
\textrm{arg}\max_{\beta} \sum_i n^*_i \left[ y_i \log \frac{p(\beta)}{1-p(\beta)} - \left(-\log (1-p(\beta))\right) 
\right] \\
$$
From this, I thought that <strong>Scaling of the number of trials $n_i$ with a constant does NOT affect the maximum likelihood estimates of $\beta$ given the proportion of success $y_i$</strong>. </p>

<p>The help file of glm says:</p>

<pre><code> ""For a binomial GLM prior weights are used to give the number of trials 
  when the response is the proportion of successes"" 
</code></pre>

<p>Therefore I expected that the scaling of weight would not affect the estimated $\beta$ given the proportion of success as response. However the following two codes return different coefficient values:</p>

<pre><code> Y &lt;- c(1,0,0,0) ## proportion of observed success
 w &lt;- 1:length(Y) ## weight= the number of trials
 glm(Y~1,weights=w,family=binomial)
</code></pre>

<p>This yields:</p>

<pre><code> Call:  glm(formula = Y ~ 1, family = ""binomial"", weights = w)

 Coefficients:
 (Intercept)  
      -2.197     
</code></pre>

<p>while if I multiply all weights by 1000, the estimated coefficients are different:</p>

<pre><code> glm(Y~1,weights=w*1000,family=binomial)

 Call:  glm(formula = Y ~ 1, family = binomial, weights = w * 1000)

 Coefficients:
 (Intercept)  
    -3.153e+15  
</code></pre>

<p>I saw many other examples like this even with some moderate scaling in weights. 
What is going on here?</p>
"
"0.0340206908719886","0.193046835626336","35373","<p>I'd like to do some analysis of shooting efficiency in basketball when a team is leading (AHEAD) or trailing (BEHIND) by less than 8 points and whether they are HOME or AWAY. Here are a few examples of the data:</p>

<pre><code>Ray Allen   HOME    BEHIND  59.4%   134
Ray Allen   HOME    AHEAD   57.13%  132
Ray Allen   AWAY    BEHIND  49.1%   166
Ray Allen   AWAY    AHEAD   48.03%  126
Jason Terry AWAY    BEHIND  56.6%   242
Jason Terry HOME    BEHIND  52.0%   193
Jason Terry AWAY    AHEAD   50.05%  198
Jason Terry HOME    AHEAD   48.73%  207
Jamal Crawford  AWAY    AHEAD   51.65%  82
Jamal Crawford  HOME    AHEAD   42.50%  178
Jamal Crawford  AWAY    BEHIND  35.5%   129
Jamal Crawford  HOME    BEHIND  33.4%   118
Kevin Durant    HOME    BEHIND  48.6%   222
Kevin Durant    HOME    AHEAD   44.05%  248
Kevin Durant    AWAY    BEHIND  41.4%   325
Kevin Durant    AWAY    AHEAD   40.07%  213
</code></pre>

<p>The 4th column is the FG% (i.e. proportion of made shots) and the 5th column is the number of shots (i.e. trials).</p>

<p>You can see even with these 4 players (and there are roughly 200 in the data set), that there is variation of the mean FG% between players, and for each player, there is not a consistent pattern in whether they are ""better"" at HOME or AWAY or AHEAD or BEHIND. So there's a lot of variance between groups and within groups as far as I can tell.</p>

<p>I thought about using lmer, but I wasn't sure how to do that for this problem, because if I just use the FG% as the outcome, I lose the information about how many shots were taken. Eventually, I'd like to put this into BUGS, but I thought there might be a more straightforward way for now, because I'm not quite ready for that yet.</p>

<p>I should just add that what I'm really after is a way to determine whether a player is ""really"" better under one of these conditions, or are the apparent differences just due to noise/variation from small sample sizes.</p>

<p>Thanks for any advice.</p>
"
"0.144337567297406","0.136504726557987","58676","<p>I have non-nested count data that I've interpolated from one area to another based on the proportion of the area that lays in each.  This is ZIP codes to counties, so most nest cleanly, with a few around the edges needing to be distributed in this way.  I'm aware that it would be better to propagate this uncertainty through the model (for instance Bannerjee Carlin and Gelfand, Chapter 6, Section 2), but that's way, way out of scope for this project for now, and I believe the error will be quite small.</p>

<p>This apportionment in proportion to area leaves me with a reasonable distribution of ""counts"" :</p>

<pre><code>         0          1      (1,5]     (5,10]   (10,250] 
      0.30       0.35       0.22       0.06       0.06 
</code></pre>

<p>The NBD models I've run on this have successfully converged, and report parameter estimates in the same basic ballpark as a Poisson regression on the same data.  It does return a warning message about non-integer values, though.</p>

<p>Is there any bias that results from running a negative binomial regression on count data that is non-integer?  Specifically, I'm doing this in R, using  <code>MASS::glm.nb</code>.</p>
"
"0.144337567297406","0.136504726557987","64352","<p>I'm facing a problem with a binomial <code>glmer</code> model. I want to find if differences in flower presence in pine trees is due to procedence of the tree.
My model is as follows: <code>FlorMas ~ Proc + (1|Blq)</code>.
Proc is a factor with nine levels, one of it (<code>TAMR</code>) presents no flower at all (variable value for all <code>TAMR</code> trees is 0).
This model gives me this output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: FlorMas ~ Proc + (1 | Blq) 
   Data: flower.data 
 AIC   BIC logLik deviance
 593 647.7 -285.5      571
Random effects:
 Groups Name        Variance Std.Dev.
 Blq    (Intercept) 0.18476  0.42983 
Number of obs: 1067, groups: Blq, 8

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -1.7668     0.2958  -5.974 2.32e-09 ***
ProcTAMR     -16.8758  1080.5608  -0.016  0.98754    
ProcARMY      -0.3543     0.3910  -0.906  0.36490    
ProcASPE      -1.4891     0.5260  -2.831  0.00464 ** 
ProcCOCA      -2.4947     0.7619  -3.274  0.00106 ** 
ProcMIMI      -1.2040     0.4930  -2.442  0.01459 *  
ProcORIA      -1.5360     0.5739  -2.676  0.00744 ** 
ProcPLEU      -1.9437     1.0538  -1.845  0.06511 .  
ProcPTOV       0.1693     0.3508   0.483  0.62945    
ProcSCRI       0.5060     0.3346   1.512  0.13050    

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I don't understand that values for <code>TAMR</code> procedence, as if it has all zero values it should be different from the others.
Any help will be appreciated.</p>
"
"0.300462606288666","0.284157248042183","86351","<p>I'm quite new on this with binomial data tests, but needed to do one and now IÂ´m not sure how to interpret the outcome. The y-variable, the response variable, is binomial and the explanatory factors are continuous. This is what I got when summarizing the outcome:</p>

<pre><code>glm(formula = leaves.presence ~ Area, family = binomial, data = n)

Deviance Residuals: 
Min      1Q  Median      3Q     Max  
-1.213  -1.044  -1.023   1.312   1.344  

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|) 
(Intercept)           -0.3877697  0.0282178 -13.742  &lt; 2e-16 ***
leaves.presence        0.0008166  0.0002472   3.303 0.000956 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 16662  on 12237  degrees of freedom
Residual deviance: 16651  on 12236  degrees of freedom
(314 observations deleted due to missingness)
AIC: 16655
Number of Fisher Scoring iterations: 4
</code></pre>

<p>There's a number of things I don't get here, what does this really say:</p>

<pre><code>                        Estimate Std. Error z value Pr(&gt;|z|) 
(Intercept)           -0.3877697  0.0282178 -13.742  &lt; 2e-16 ***
leaves.presence        0.0008166  0.0002472   3.303 0.000956 ***
</code></pre>

<p>And what does AIC and Number of Fisher Scoring iterations mean?</p>

<pre><code>&gt; fit
Call:  glm(formula = LÃ¶vfÃ¶rekomst ~ Areal, family = binomial, data = n)

Coefficients:
(Intercept)        Areal  
-0.3877697    0.0008166  

Degrees of Freedom: 12237 Total (i.e. Null);  12236 Residual
(314 observations deleted due to missingness)
Null Deviance:      16660 
Residual Deviance: 16650        AIC: 16650
</code></pre>

<p>And here what does this mean:</p>

<pre><code>Coefficients:
(Intercept)        Areal  
-0.3877697    0.0008166 
</code></pre>
"
"0.144337567297406","0.136504726557987","86470","<p>IÂ´m trying to visualize a <code>glm</code> model with a binomial response variable, I want to put a line in the plots, but neither <code>lines</code> or <code>abline</code>  work and I donÂ´t know why when I have significant responses according to the model I used. Any idea why plotting a line does not work?</p>

<pre><code>&gt; summary(model2)

Call:
glm(formula = LÃ¶vfÃ¶rekomst ~ Areal + Si, family = binomial)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.254  -1.048  -1.012   1.309   1.422  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.2158411  0.0634165  -3.404 0.000665 ***
Areal        0.0009178  0.0002495   3.678 0.000235 ***
Si          -0.0117521  0.0038870  -3.023 0.002499 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 16662  on 12237  degrees of freedom
Residual deviance: 16642  on 12235  degrees of freedom
  (33 observations deleted due to missingness)
AIC: 16648
Number of Fisher Scoring iterations: 4

&gt; mod &lt;- glm(LÃ¶vfÃ¶rekomst~Areal, binomial)
&gt; med &lt;- glm(LÃ¶vfÃ¶rekomst~Si,    binomial)
&gt; x   &lt;- seq(0,9,0.01)
&gt; y   &lt;- predict(mod, list(Areal=x), type=""response"")
&gt; par(mfrow=c(1,2))
&gt; plot(Areal, LÃ¶vfÃ¶rekomst)
&gt; x2 &lt;- seq(0,9,0.01)
&gt; y2 &lt;- predict(med, list(Si=x), type=""response"")
&gt; plot(Si, LÃ¶vfÃ¶rekomst)
&gt; lines(x,  y)
&gt; lines(x2, y2)
</code></pre>
"
"NaN","NaN","86732","<p>I divided my dataset into Test and Validation (50-50 split).</p>"
"NaN","NaN","<p>I ran glm function (link=binomial) on Test dataset and got the parameter estimates.</p>",""
"NaN","NaN","<p>How do I score the Validation dataset based on these parameter estimates (beta) that I got from Test dataset. I know it has something to do with apply () but I am not sure. please advise.</p>",""
"NaN","NaN","","<r><regression><logistic><binomial>"
"NaN","NaN","136137","<p>I am performing a GLM on count data (insurance claims) and I wish to compare Overdispersed Poisson Regression (ODP) against Negative Binomial regression. I would know whether there is a practical index (AIC, logLik) that in standard R could support me in fitting which one to use. I am selecting significant predictors with backard deletion (using anova(fittedModel, test=""Chisq"") type III tests). Theferore it is not assumed the final model within each distribution family to have the same predictor sets.</p>
"
"0.301232038038355","0.30523384783368","46096","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to compare the relative importance of each of my predictor variables regarding their impact on the response variable (note: the predictors each have quite different scales - sometimes by orders of magnitude).  Unfortunately, the output from R gives me results as unstandardized (<em>b</em>) coefficients (""estimates"").  I'm hoping someone can give me a hint as to how to go about getting standardized (<em>beta</em>) coefficients from the NB regression model... or another 'better' way to determine the relative importance of each of my predictors on my response variable.</p>

<p>I've investigated several potential ways like: </p>

<ol>
<li>using the R package 'relimpo' (as suggested in a comment to <a href=""http://stats.stackexchange.com/a/7118"">http://stats.stackexchange.com/a/7118</a>), but it does not work on a NB regression model, thus completely changing the assumptions I should be accounting for and making the outcomes very different; </li>
<li>mean-centering and scaling my data, which changes the interpretation and makes it so that I can't use NB model due to response variables now having negative values; </li>
<li>scaling-only, so that I can still run a NB model... which I <em>thought</em> would only affect the scale of the coefficients without changing their direction (viz., <a href=""http://stats.stackexchange.com/a/29784"">http://stats.stackexchange.com/a/29784</a> ) - but I do get some positive coefficients that flip to neg. and vice-verse... which seems strange to me and makes me wonder whether I'm making a mistake.</li>
</ol>

<p>I've benefited from looking at <a href=""http://stats.stackexchange.com/q/29781"">When should you center your data &amp; when should you standardize?</a> (and the suggested links from comments on the question such as <a href=""http://andrewgelman.com/2009/07/when_to_standar/"" rel=""nofollow"">http://andrewgelman.com/2009/07/when_to_standar/</a> and <a href=""http://stats.stackexchange.com/q/7112"">When and how to use standardized explanatory variables in linear regression</a> and <a href=""http://stats.stackexchange.com/q/19216"">Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?</a>).  </p>

<p>Bottom line: I have not yet found a way to use a NB model in R (which I have statistically confirmed is more appropriate than lm, glm, or poisson for modeling my data) and still get at the relative importance - or at least to the standardized beta coefficients - for my predictors...</p>

<p>The R scripts is something like this:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
  predictor3, data=myData, control=glm.control(maxit=125))
summary(nb)

scaled_nb = glm.nb(scale(responseCountVar, center = FALSE) ~ scale(predictor1, center = FALSE) + scale(predictor2, center = FALSE) + 
  scale(predictor3, center = FALSE), data=myData, control=glm.control(maxit=125))
summary(scaled_nb)
</code></pre>
"
"NaN","NaN","147976","<p>I have a dataset with the dependent variable presence / absence (0 and 1) for a certain species. I have three categorised IV's (2 IV's with 3 categories and 1 with 2 categories). To test the response of the presence / absence to the 3 IV's, I have to perform a binomial regression, right?</p>

<p>I can run a model in R which looks like this:</p>

<pre><code>glm(species.presenceabsence~TREATMENT+WEEK+TYPE, data=speciesdata, family=binomial))
</code></pre>

<p>The outcome looks like this:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -19.4779  2955.2811  -0.007    0.995
TREATMENTHigh  -19.1984  2955.2812  -0.006    0.995
TREATMENTLow    -0.9217     0.8101  -1.138    0.255
WEEK10          18.2767  2955.2812   0.006    0.995
WEEK15          19.1984  2955.2812   0.006    0.995
TYPEF           -0.3083     0.7881  -0.391    0.696
</code></pre>

<p>What I don't understand is why R doesn't give the coefficients of all the IV's. How do I interpret these data? I'm a bit surprised that none of the coefficients has a significant effect.</p>
"
"0.220479275922049","0.208514414057075","46312","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to get the standardized (<em>beta</em>) coefficients from the model, but am given the unstandardized (<em>b</em> ""Estimate"") coefficients.</p>

<p>The R documentation does not seem to show of a way to retrieve the standardized beta weights easily for a negative bionomial regression model.</p>

<p>The R script is something like:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
    predictor3 + predictor4 + predictor5 + predictor6 + 
    predictor7 + predictor8 + predictor9 + predictor10 + 
    predictor11 + predictor12 + predictor13 + predictor14 + 
    predictor15 + predictor16 + predictor17 + predictor18 + 
    predictor19 + predictor20 + predictor21,
    data=myData, control=glm.control(maxit=125))
summary(nb)
</code></pre>

<p>and the output of the above is:</p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.1462  -1.0080  -0.4247   0.2277   3.4336  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -3.059e+00  3.782e-01  -8.088 6.05e-16 ***
predictor1    -2.447e+00  4.930e-01  -4.965 6.88e-07 ***
predictor2    -1.004e+00  1.313e-01  -7.650 2.00e-14 ***
predictor3     1.158e+00  1.440e-01   8.047 8.46e-16 ***
predictor4     1.334e+00  7.034e-02  18.970  &lt; 2e-16 ***
predictor5     9.862e-01  2.006e-01   4.915 8.87e-07 ***
predictor6     1.166e+00  2.378e+00   0.490  0.62392    
predictor7    -1.057e-01  1.494e-01  -0.707  0.47936    
predictor8     4.051e-01  7.318e-02   5.536 3.10e-08 ***
predictor9    -3.320e-01  1.132e-01  -2.933  0.00336 ** 
predictor10    3.761e-01  1.561e-01   2.409  0.01600 *  
predictor11    8.660e-02  4.332e-02   1.999  0.04557 *  
predictor12   -1.583e-01  2.044e-01  -0.774  0.43872    
predictor13    6.404e-02  3.972e-03  16.122  &lt; 2e-16 ***
predictor14    4.264e-03  2.297e-04  18.563  &lt; 2e-16 ***
predictor15    3.279e-03  5.697e-04   5.755 8.68e-09 ***
predictor16    3.487e-03  3.447e-03   1.012  0.31177    
predictor17    1.534e-04  1.647e-04   0.931  0.35182    
predictor18   -7.606e-05  9.021e-05  -0.843  0.39917    
predictor19    2.536e-04  1.733e-05  14.633  &lt; 2e-16 ***
predictor20    2.997e-02  4.977e-03   6.021 1.73e-09 ***
predictor21    2.756e+01  3.508e+00   7.856 3.98e-15 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for Negative Binomial(0.9232) family taken to be 1)

    Null deviance: 5631.1  on 1835  degrees of freedom
Residual deviance: 2120.7  on 1814  degrees of freedom

                                AIC: 19268    
Number of Fisher Scoring iterations: 1    
                              Theta: 0.9232 
                          Std. Err.: 0.0282 
                 2 x log-likelihood: -19221.9910
</code></pre>

<p><strong>My question is</strong>:  Is there a way to get the beta weights, or do I need to try to convert my unstandardized b coefficients to standardized beta coefficients (if so, how would I do that)?</p>
"
"0.344123600805843","0.343529361714903","71094","<p>I am an ecologist that has counted 11 different species of herbivores in about 110 blocks of two habitat types over 18 months, and I am interested in predicting the density across the study area from the model I hope to make. The approach so far is to build a spatially smoothed GLM, accounting for the over-dispersion with either a poisson distribution or the related negative binomial. Using the <code>R</code> software structure, the models looks like this:</p>

<pre><code>herb.pois &lt;-  glm(Spb~F1+X+Y+X2+Y2+Habitat*Month,
offset = lnArea, data = herbivore, family = poisson)

herb.nb &lt;- glm(Spb~F1+X+Y+X2+Y2+Habitat*MonthF,
offset = lnArea, data = herbivore, family = neg.bin(0.23))
</code></pre>

<p><code>F1</code> is a remotely sensed vegetation variable I am hoping to correlate with animal density of some or all species, the offset <code>lnArea</code> is the log of the area surveyed in each block (all different sizes, produces an animals per square kilometre measure when prediction value of this is set to 0). The dispersion parameter for the negative binomial (NB) model is individually calculated for each species elsewhere. </p>

<p>Invariably the negative binomial model has greater traction as per a likelihood ratio test (LRT), or AIC comparison. </p>

<pre><code>Model 1: [Poisson model]
Model 2: [Negative binomial model]
Resid. Df Resid. Dev Df Deviance            AIC
1      1298     1252.9                        2080.394
2      1300      763.3 -2   489.58            1942.656
</code></pre>

<p>However, both model predictions are wildly different, usually the poisson model is very tight (small computed confidence intervals, using the predict functions: <code>se.fit*1.96</code>)</p>

<p><img src=""http://i.stack.imgur.com/mL2VS.png"" alt=""Poisson model - please ignore Y label and blue line, it a representation of the F1 term""></p>

<p>and the negative binomial has larger confidence intervals but importantly fewer important terms. </p>

<p><img src=""http://i.stack.imgur.com/SWaBz.png"" alt=""Negative Binomial model""></p>

<p>The bottom line is I want to choose and use the model which best represents the changing density over time, retains important variables and has reasonable confidence intervals (CIs): The differences between these two models and respective likelihoods is leaving me unconvinced, and stretching my statistical knowledge. My gut is telling me that the CI's are too wide for the NB, and that more terms should be important. Should I trust the likelihood ratio test (LRT) to choose the best model? Should I be worried that the predicted values disagree so much? Am I missing something - would you recommend exploring other distributions, data transformations, model scenarios etc.? What other information might I try get to feel surer about the model selection?</p>

<p>I have explored zero inflated models for which predictions didn't seem reasonable (and NB <a href=""http://www.statisticalhorizons.com/zero-inflated-models"" rel=""nofollow"">seems to be sufficient?</a>), stepwise dropping terms;  for some herbivores spatial terms or the remotely sensed variable would drop out and I need to retain Habitat and Month regardless for prediction purposes. </p>

<hr>

<p><strong>PART II</strong></p>

<p>I thought I had best include this as it may influence the above approach, for instance in multi variate is recommended - my next step is to take the same data, treated a little differently to analyse another aspect of the herbivore ecology. This question is a little more philosophical. On top of an interest in density of herbivores, I am interested in group density (equivalent to the rate at which a predator might encounter herbivores who are clumped) which is exactly equivalent to density in solitary individuals, but ranging to un-correlated in dynamic fission-fusion herding herbivores whose group formation doesn't depend on density alone. Imagine the extremes of a single herd of 500 in the wet season, breaking down into 25 groups of 20 animals in the dry season - density is the same, the first model wouldn't do it justice) The best idea I have had so far is run the models the same way with the dependant term equal to the count of <em>groups</em> of that herbivore in that block (replacing what was the count of animals) i.e. groups per square kilometre. This would allow me to predict another grid for each month of group densities. </p>

<p>Does this seem reasonable? Would I better consider a multi-variate model or something else. Thanks in advance.</p>
"
"0.0833333333333333","0.0788110406239101","48485","<p>Suppose I have 10 students, who each attempt to solve 20 math problems.  The problems are scored correct or incorrect (in longdata) and each student's performance can be summarized by an accuracy measure (in subjdata).  Models 1, 2, and 4 below appear to produce different results, but I understand them to be doing the same thing.  Why are they producing different results?  (I included model 3 for reference.)</p>

<pre><code>library(lme4)

set.seed(1)
nsubjs=10
nprobs=20
subjdata = data.frame('subj'=rep(1:nsubjs),'iq'=rep(seq(80,120,10),nsubjs/5))
longdata = subjdata[rep(seq_len(nrow(subjdata)), each=nprobs), ]
longdata$correct = runif(nsubjs*nprobs)&lt;pnorm(longdata$iq/50-1.4)
subjdata$acc = by(longdata$correct,longdata$subj,mean)
model1 = lm(logit(acc)~iq,subjdata)
model2 = glm(acc~iq,subjdata,family=gaussian(link='logit'))
model3 = glm(acc~iq,subjdata,family=binomial(link='logit'))
model4 = lmer(correct~iq+(1|subj),longdata,family=binomial(link='logit'))
</code></pre>
"
"0.117851130197758","0.111455642515071","48448","<p>I used some categorical variables as predictors to a negative binomial model. The dependent variable is numerical. I used glm.nb in R and the results show relative coefficients of one category respective to another category.</p>

<p>Then I tried using lm.beta to standardize the coefficients, but still the results are relative. How can I interpret a positive versus negative coefficient here?</p>
"
"0.434046146564706","0.424646422629054","96010","<p>I am struggling to fit alternative count models into my data. I guess my problem is just too many zeros.</p>

<p>This is my data</p>

<pre><code>&gt; summary(smpl)
    response        predict1          predict2        
 Min.   :0.000   Min.   :   1.00   Min.   :    22005  
 1st Qu.:0.000   1st Qu.:   3.00   1st Qu.:  4669705  
 Median :0.000   Median :   8.00   Median : 12540318  
 Mean   :0.017   Mean   :  23.27   Mean   : 20382574  
 3rd Qu.:0.000   3rd Qu.:  20.00   3rd Qu.: 25468156  
 Max.   :3.000   Max.   :1584.00   Max.   :145348049

&gt; table(smpl$response)
  0   1   2   3 
987  10   2   1 
</code></pre>

<p>I tried three regressions: basic Poisson, negative binomial and zero-inflated but the only formula returning coefficients without warnings is the Poisson:</p>

<pre><code>&gt; summary(glm(response ~ ., data = smpl, family = poisson))

Call:
glm(formula = response ~ ., family = poisson, data = smpl)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3871  -0.2214  -0.1722  -0.1148   4.7861  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.472e+00  3.521e-01  -9.862  &lt; 2e-16 ***
predict1     3.229e-03  7.271e-04   4.442 8.93e-06 ***
predict2    -6.258e-08  3.060e-08  -2.045   0.0409 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 150.67  on 999  degrees of freedom
Residual deviance: 135.84  on 997  degrees of freedom
AIC: 170.06

Number of Fisher Scoring iterations: 8
</code></pre>

<p>The negative binomial returns a warnings on both the convergence and the alternation limit</p>

<pre><code>summary(glm.nb(response ~ ., data = smpl))

Call:
glm.nb(formula = response ~ ., data = smpl, init.theta = 0.04901296596, 
    link = log)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.28844  -0.17677  -0.14542  -0.09808   2.38314  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.899e+00  4.587e-01  -8.499  &lt; 2e-16 ***
predict1     1.226e-02  2.144e-03   5.720 1.06e-08 ***
predict2    -5.982e-08  3.407e-08  -1.756   0.0791 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.049) family taken to be 1)

    Null deviance: 69.927  on 999  degrees of freedom
Residual deviance: 55.940  on 997  degrees of freedom
AIC: 152.37

Number of Fisher Scoring iterations: 1


              Theta:  0.0490 
          Std. Err.:  0.0251 
Warning while fitting theta: alternation limit reached 

 2 x log-likelihood:  -144.3700 
Warning messages:
1: glm.fit: algorithm did not converge 
2: In glm.nb(response ~ ., data = smpl) : alternation limit reached
</code></pre>

<p>and the zero-inflated (from the <code>pscl</code> package) doesn't return anything at all</p>

<pre><code>&gt; summary(zeroinfl(response ~ ., data = smpl, dist = ""negbin""))

Call:
zeroinfl(formula = response ~ ., data = smpl, dist = ""negbin"")

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-0.45252 -0.08817 -0.05515 -0.04210 19.56118 

Count model coefficients (negbin with log link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.477e+00         NA      NA       NA
predict1     2.678e-03         NA      NA       NA
predict2    -1.160e-07         NA      NA       NA
Log(theta)  -1.241e+00         NA      NA       NA

Zero-inflation model coefficients (binomial with logit link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.869e+00         NA      NA       NA
predict1    -1.329e-01         NA      NA       NA
predict2    -1.346e-07         NA      NA       NA
Error in if (getOption(""show.signif.stars"") &amp; any(rbind(x$coefficients$count,  : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Then my questions are: </p>

<ol>
<li>Is there anything I can do in terms of ""formula tweaking"" with the negative binomial (to avoid the warnings) and with the zero-inflated (to get the coefficients)?</li>
<li>Looking only at the results above (thus including problems with convergence and alternation limit) should I select the negative binomial model since it seems, looking at the AIC, to fit better than the Poisson in my data?</li>
</ol>
"
"0.0833333333333333","0.0788110406239101","137280","<p>While performing betaregression using betareg R package I noticed that the terms in my model are often significant, even with very small sample sizes. I tried the same model using glm with binomial family and logit link function, and I get very similar effect sizes but non-significant terms.</p>

<p>Can someone explain me how should I interpret this? Do the two models test significance in different ways? </p>

<p>NOTE: In my case the response variable is a proportion, so, although extremely unlikely, it could even take values 0 and 1.</p>

<pre><code>library(betareg)

Y=c(0.5283019, 0.4845361, 0.4974874, 0.6884735, 0.5967742, 0.6835443, 0.4152047, 0.4949495,
  0.6478873, 0.7695853, 0.4764398, 0.5780591, 0.5689655)
X=c(0.3616452, -0.4931525,  0.7890441,  0.7890441, -0.9205514,  0.7890441, -0.9205514,
 -0.9205514,  1.2164429,  1.2164429, -1.3479503, -1.3479503,  0.7890441)

summary(glm(Y~X, family=binomial('logit')))
summary(betareg(Y~X))
</code></pre>
"
"0.186338998124982","0.17622684421256","138109","<p>I would like to obtain estimated $\theta$ from glmer.nb function in lme4 package. In my understanding this function fits the model:
$$
Y_{ij}|\boldsymbol{B}_{i}=\boldsymbol{b}_i \overset{ind.}{\sim} NB\Big(mean=\mu,var=\mu + \frac{\mu^2}{\theta}\Big)
$$
where $NB$ refers to the negative binomial distribution and:
$$
\mu = \exp(\boldsymbol{X}_{ij}^T \boldsymbol{\beta} + \boldsymbol{Z}_{ij}^T \boldsymbol{b}_i)
$$
and 
$$
\boldsymbol{B}_i \overset{i.i.d.}{\sim} MVN(mean=\boldsymbol{0},var=\Sigma).
$$
So glmer.nb must be estimating unknown parameters $\boldsymbol{\beta}$, $\Sigma$ and $\theta$ via maximizing its likelihood. The help file of glmer.nb little explains its functionality, and it says ""glmer() for Negative Binomial"". However, the negative binomial is NOT an exponential family when the parameter $\theta$ is unknown. So $\theta$ must be estimated in some other ways that generalized linear mixed effect models (GLMM) do not take, and the estimated $\theta$ must be obtained in some special ways that GLMM do not take. How can I access to the estimate of $\theta$ which should be one of glmer.nb output? </p>
"
"0.276385399196283","0.261386651086967","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
